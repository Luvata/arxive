<!DOCTYPE html>
<html>
<head>
<title>2023-11-28-cs-lg</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2311.13608">Breathing Life Into Sketches Using Text-to-Video Priors. (arXiv:2311.13608v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gal_R/0/1/0/all/0/1">Rinon Gal</a>, <a href="http://arxiv.org/find/cs/1/au:+Vinker_Y/0/1/0/all/0/1">Yael Vinker</a>, <a href="http://arxiv.org/find/cs/1/au:+Alaluf_Y/0/1/0/all/0/1">Yuval Alaluf</a>, <a href="http://arxiv.org/find/cs/1/au:+Bermano_A/0/1/0/all/0/1">Amit H. Bermano</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1">Daniel Cohen-Or</a>, <a href="http://arxiv.org/find/cs/1/au:+Shamir_A/0/1/0/all/0/1">Ariel Shamir</a>, <a href="http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1">Gal Chechik</a></p>
<p>A sketch is one of the most intuitive and versatile tools humans use to
convey their ideas visually. An animated sketch opens another dimension to the
expression of ideas and is widely used by designers for a variety of purposes.
Animating sketches is a laborious process, requiring extensive experience and
professional design skills. In this work, we present a method that
automatically adds motion to a single-subject sketch (hence, "breathing life
into it"), merely by providing a text prompt indicating the desired motion. The
output is a short animation provided in vector representation, which can be
easily edited. Our method does not require extensive training, but instead
leverages the motion prior of a large pretrained text-to-video diffusion model
using a score-distillation loss to guide the placement of strokes. To promote
natural and smooth motion and to better preserve the sketch's appearance, we
model the learned motion through two components. The first governs small local
deformations and the second controls global affine transformations.
Surprisingly, we find that even models that struggle to generate sketch videos
on their own can still serve as a useful backbone for animating abstract
representations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13613">Spanning Training Progress: Temporal Dual-Depth Scoring (TDDS) for Enhanced Dataset Pruning. (arXiv:2311.13613v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1">Jiawei Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yunsong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1">Weiying Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Joey Tianyi Zhou</a></p>
<p>Dataset pruning aims to construct a coreset capable of achieving performance
comparable to the original, full dataset. Most existing dataset pruning methods
rely on snapshot-based criteria to identify representative samples, often
resulting in poor generalization across various pruning and cross-architecture
scenarios. Recent studies have addressed this issue by expanding the scope of
training dynamics considered, including factors such as forgetting event and
probability change, typically using an averaging approach. However, these works
struggle to integrate a broader range of training dynamics without overlooking
well-generalized samples, which may not be sufficiently highlighted in an
averaging manner. In this study, we propose a novel dataset pruning method
termed as Temporal Dual-Depth Scoring (TDDS), to tackle this problem. TDDS
utilizes a dual-depth strategy to achieve a balance between incorporating
extensive training dynamics and identifying representative samples for dataset
pruning. In the first depth, we estimate the series of each sample's individual
contributions spanning the training progress, ensuring comprehensive
integration of training dynamics. In the second depth, we focus on the
variability of the sample-wise contributions identified in the first depth to
highlight well-generalized samples. Extensive experiments conducted on CIFAR
and ImageNet datasets verify the superiority of TDDS over previous SOTA
methods. Specifically on CIFAR-100, our method achieves 54.51% accuracy with
only 10% training data, surpassing random selection by 7.83% and other
comparison methods by at least 12.69%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13623">Density Distribution-based Learning Framework for Addressing Online Continual Learning Challenges. (arXiv:2311.13623v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shilin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiahui Wang</a></p>
<p>In this paper, we address the challenges of online Continual Learning (CL) by
introducing a density distribution-based learning framework. CL, especially the
Class Incremental Learning, enables adaptation to new test distributions while
continuously learning from a single-pass training data stream, which is more in
line with the practical application requirements of real-world scenarios.
However, existing CL methods often suffer from catastrophic forgetting and
higher computing costs due to complex algorithm designs, limiting their
practical use. Our proposed framework overcomes these limitations by achieving
superior average accuracy and time-space efficiency, bridging the performance
gap between CL and classical machine learning. Specifically, we adopt an
independent Generative Kernel Density Estimation (GKDE) model for each CL task.
During the testing stage, the GKDEs utilize a self-reported max probability
density value to determine which one is responsible for predicting incoming
test instances. A GKDE-based learning objective can ensure that samples with
the same label are grouped together, while dissimilar instances are pushed
farther apart. Extensive experiments conducted on multiple CL datasets validate
the effectiveness of our proposed framework. Our method outperforms popular CL
approaches by a significant margin, while maintaining competitive time-space
efficiency, making our framework suitable for real-world applications. Code
will be available at https://github.com/xxxx/xxxx.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13624">A Theoretical Insight into Attack and Defense of Gradient Leakage in Transformer. (arXiv:2311.13624v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chenyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1">Zhao Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Weixin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Chiwun Yang</a></p>
<p>The Deep Leakage from Gradient (DLG) attack has emerged as a prevalent and
highly effective method for extracting sensitive training data by inspecting
exchanged gradients. This approach poses a substantial threat to the privacy of
individuals and organizations alike. This research presents a comprehensive
analysis of the gradient leakage method when applied specifically to
transformer-based models. Through meticulous examination, we showcase the
capability to accurately recover data solely from gradients and rigorously
investigate the conditions under which gradient attacks can be executed,
providing compelling evidence. Furthermore, we reevaluate the approach of
introducing additional noise on gradients as a protective measure against
gradient attacks. To address this, we outline a theoretical proof that analyzes
the associated privacy costs within the framework of differential privacy.
Additionally, we affirm the convergence of the Stochastic Gradient Descent
(SGD) algorithm under perturbed gradients. The primary objective of this study
is to augment the understanding of gradient leakage attack and defense
strategies while actively contributing to the development of privacy-preserving
techniques specifically tailored for transformer-based models. By shedding
light on the vulnerabilities and countermeasures associated with gradient
leakage, this research aims to foster advancements in safeguarding sensitive
data and upholding privacy in the context of transformer-based models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13628">Prompt Risk Control: A Rigorous Framework for Responsible Deployment of Large Language Models. (arXiv:2311.13628v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zollo_T/0/1/0/all/0/1">Thomas P. Zollo</a>, <a href="http://arxiv.org/find/cs/1/au:+Morrill_T/0/1/0/all/0/1">Todd Morrill</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1">Zhun Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Snell_J/0/1/0/all/0/1">Jake C. Snell</a>, <a href="http://arxiv.org/find/cs/1/au:+Pitassi_T/0/1/0/all/0/1">Toniann Pitassi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1">Richard Zemel</a></p>
<p>The recent explosion in the capabilities of large language models has led to
a wave of interest in how best to prompt a model to perform a given task. While
it may be tempting to simply choose a prompt based on average performance on a
validation set, this can lead to a deployment where unexpectedly poor responses
are generated, especially for the worst-off users. To mitigate this prospect,
we propose Prompt Risk Control, a lightweight framework for selecting a prompt
based on rigorous upper bounds on families of informative risk measures. We
offer methods for producing bounds on a diverse set of metrics, including
quantities that measure worst-case responses and disparities in generation
quality across the population of users. In addition, we extend the underlying
statistical bounding techniques to accommodate the possibility of distribution
shifts in deployment. Experiments on applications such as open-ended chat,
medical question summarization, and code generation highlight how such a
framework can foster responsible deployment by reducing the risk of the worst
outcomes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13647">Language Model Inversion. (arXiv:2311.13647v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Morris_J/0/1/0/all/0/1">John X. Morris</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Wenting Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chiu_J/0/1/0/all/0/1">Justin T. Chiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shmatikov_V/0/1/0/all/0/1">Vitaly Shmatikov</a>, <a href="http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1">Alexander M. Rush</a></p>
<p>Language models produce a distribution over the next token; can we use this
information to recover the prompt tokens? We consider the problem of language
model inversion and show that next-token probabilities contain a surprising
amount of information about the preceding text. Often we can recover the text
in cases where it is hidden from the user, motivating a method for recovering
unknown prompts given only the model's current distribution output. We consider
a variety of model access scenarios, and show how even without predictions for
every token in the vocabulary we can recover the probability vector through
search. On Llama-2 7b, our inversion method reconstructs prompts with a BLEU of
$59$ and token-level F1 of $78$ and recovers $27\%$ of prompts exactly. Code
for reproducing all experiments is available at
<a href="http://github.com/jxmorris12/vec2text.">this http URL</a>
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13648">Evaluating Pretrained models for Deployable Lifelong Learning. (arXiv:2311.13648v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lekkala_K/0/1/0/all/0/1">Kiran Lekkala</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhargava_E/0/1/0/all/0/1">Eshan Bhargava</a>, <a href="http://arxiv.org/find/cs/1/au:+Itti_L/0/1/0/all/0/1">Laurent Itti</a></p>
<p>We create a novel benchmark for evaluating a Deployable Lifelong Learning
system for Visual Reinforcement Learning (RL) that is pretrained on a curated
dataset, and propose a novel Scalable Lifelong Learning system capable of
retaining knowledge from the previously learnt RL tasks. Our benchmark measures
the efficacy of a deployable Lifelong Learning system that is evaluated on
scalability, performance and resource utilization. Our proposed system, once
pretrained on the dataset, can be deployed to perform continual learning on
unseen tasks. Our proposed method consists of a Few Shot Class Incremental
Learning (FSCIL) based task-mapper and an encoder/backbone trained entirely
using the pretrain dataset. The policy parameters corresponding to the
recognized task are then loaded to perform the task. We show that this system
can be scaled to incorporate a large number of tasks due to the small memory
footprint and fewer computational resources. We perform experiments on our DeLL
(Deployment for Lifelong Learning) benchmark on the Atari games to determine
the efficacy of the system.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13657">Efficient Transformer Knowledge Distillation: A Performance Review. (arXiv:2311.13657v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Brown_N/0/1/0/all/0/1">Nathan Brown</a>, <a href="http://arxiv.org/find/cs/1/au:+Williamson_A/0/1/0/all/0/1">Ashton Williamson</a>, <a href="http://arxiv.org/find/cs/1/au:+Anderson_T/0/1/0/all/0/1">Tahj Anderson</a>, <a href="http://arxiv.org/find/cs/1/au:+Lawrence_L/0/1/0/all/0/1">Logan Lawrence</a></p>
<p>As pretrained transformer language models continue to achieve
state-of-the-art performance, the Natural Language Processing community has
pushed for advances in model compression and efficient attention mechanisms to
address high computational requirements and limited input sequence length.
Despite these separate efforts, no investigation has been done into the
intersection of these two fields. In this work, we provide an evaluation of
model compression via knowledge distillation on efficient attention
transformers. We provide cost-performance trade-offs for the compression of
state-of-the-art efficient attention architectures and the gains made in
performance in comparison to their full attention counterparts. Furthermore, we
introduce a new long-context Named Entity Recognition dataset, GONERD, to train
and test the performance of NER models on long sequences. We find that
distilled efficient attention transformers can preserve a significant amount of
original model performance, preserving up to 98.6% across short-context tasks
(GLUE, SQUAD, CoNLL-2003), up to 94.6% across long-context
Question-and-Answering tasks (HotpotQA, TriviaQA), and up to 98.8% on
long-context Named Entity Recognition (GONERD), while decreasing inference
times by up to 57.8%. We find that, for most models on most tasks, performing
knowledge distillation is an effective method to yield high-performing
efficient attention models with low costs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13664">Sample as You Infer: Predictive Coding With Langevin Dynamics. (arXiv:2311.13664v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zahid_U/0/1/0/all/0/1">Umais Zahid</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1">Qinghai Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Fountas_Z/0/1/0/all/0/1">Zafeirios Fountas</a></p>
<p>We present a novel algorithm for parameter learning in generic deep
generative models that builds upon the predictive coding (PC) framework of
computational neuroscience. Our approach modifies the standard PC algorithm to
bring performance on-par and exceeding that obtained from standard variational
auto-encoder (VAE) training. By injecting Gaussian noise into the PC inference
procedure we re-envision it as an overdamped Langevin sampling, which
facilitates optimisation with respect to a tight evidence lower bound (ELBO).
We improve the resultant encoder-free training method by incorporating an
encoder network to provide an amortised warm-start to our Langevin sampling and
test three different objectives for doing so. Finally, to increase robustness
to the sampling step size and reduce sensitivity to curvature, we validate a
lightweight and easily computable form of preconditioning, inspired by Riemann
Manifold Langevin and adaptive optimizers from the SGD literature. We compare
against VAEs by training like-for-like generative models using our technique
against those trained with standard reparameterisation-trick-based ELBOs. We
observe our method out-performs or matches performance across a number of
metrics, including sample quality, while converging in a fraction of the number
of SGD training iterations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13665">A Joint Gradient and Loss Based Clustered Federated Learning Design. (arXiv:2311.13665v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1">Licheng Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Mingzhe Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhaohui Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yusen Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuchen Liu</a></p>
<p>In this paper, a novel clustered FL framework that enables distributed edge
devices with non-IID data to independently form several clusters in a
distributed manner and implement FL training within each cluster is proposed.
In particular, our designed clustered FL algorithm must overcome two challenges
associated with FL training. First, the server has limited FL training
information (i.e., the parameter server can only obtain the FL model
information of each device) and limited computational power for finding the
differences among a large amount of devices. Second, each device does not have
the data information of other devices for device clustering and can only use
global FL model parameters received from the server and its data information to
determine its cluster identity, which will increase the difficulty of device
clustering. To overcome these two challenges, we propose a joint gradient and
loss based distributed clustering method in which each device determines its
cluster identity considering the gradient similarity and training loss. The
proposed clustering method not only considers how a local FL model of one
device contributes to each cluster but also the direction of gradient descent
thus improving clustering speed. By delegating clustering decisions to edge
devices, each device can fully leverage its private data information to
determine its own cluster identity, thereby reducing clustering overhead and
improving overall clustering performance. Simulation results demonstrate that
our proposed clustered FL algorithm can reduce clustering iterations by up to
99% compared to the existing baseline.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13687">Beat-Aligned Spectrogram-to-Sequence Generation of Rhythm-Game Charts. (arXiv:2311.13687v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1">Jayeon Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Sungho Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kyogu Lee</a></p>
<p>In the heart of "rhythm games" - games where players must perform actions in
sync with a piece of music - are "charts", the directives to be given to
players. We newly formulate chart generation as a sequence generation task and
train a Transformer using a large dataset. We also introduce tempo-informed
preprocessing and training procedures, some of which are suggested to be
integral for a successful training. Our model is found to outperform the
baselines on a large dataset, and is also found to benefit from pretraining and
finetuning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13688">Masked Conditional Diffusion Models for Image Analysis with Application to Radiographic Diagnosis of Infant Abuse. (arXiv:2311.13688v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Wu_S/0/1/0/all/0/1">Shaoju Wu</a>, <a href="http://arxiv.org/find/eess/1/au:+Kurugol_S/0/1/0/all/0/1">Sila Kurugol</a>, <a href="http://arxiv.org/find/eess/1/au:+Tsai_A/0/1/0/all/0/1">Andy Tsai</a></p>
<p>The classic metaphyseal lesion (CML) is a distinct injury that is highly
specific for infant abuse. It commonly occurs in the distal tibia. To aid
radiologists detect these subtle fractures, we need to develop a model that can
flag abnormal distal tibial radiographs (i.e. those with CMLs). Unfortunately,
the development of such a model requires a large and diverse training database,
which is often not available. To address this limitation, we propose a novel
generative model for data augmentation. Unlike previous models that fail to
generate data that span the diverse radiographic appearance of the distal
tibial CML, our proposed masked conditional diffusion model (MaC-DM) not only
generates realistic-appearing and wide-ranging synthetic images of the distal
tibial radiographs with and without CMLs, it also generates their associated
segmentation labels. To achieve these tasks, MaC-DM combines the weighted
segmentation masks of the tibias and the CML fracture sites as additional
conditions for classifier guidance. The augmented images from our model
improved the performances of ResNet-34 in classifying normal radiographs and
those with CMLs. Further, the augmented images and their associated
segmentation masks enhanced the performance of the U-Net in labeling areas of
the CMLs on distal tibial radiographs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13693">Scalable CP Decomposition for Tensor Learning using GPU Tensor Cores. (arXiv:2311.13693v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zeliang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhuo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1">Susan Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhiyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yifan Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1">Chen Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Chenliang Xu</a></p>
<p>CP decomposition is a powerful tool for data science, especially gene
analysis, deep learning, and quantum computation. However, the application of
tensor decomposition is largely hindered by the exponential increment of the
computational complexity and storage consumption with the size of tensors.
While the data in our real world is usually presented as trillion- or even
exascale-scale tensors, existing work can only support billion-scale scale
tensors. In our work, we propose the Exascale-Tensor to mitigate the
significant gap. Specifically, we propose a compression-based tensor
decomposition framework, namely the exascale-tensor, to support exascale tensor
decomposition. Then, we carefully analyze the inherent parallelism and propose
a bag of strategies to improve computational efficiency. Last, we conduct
experiments to decompose tensors ranging from million-scale to trillion-scale
for evaluation. Compared to the baselines, the exascale-tensor supports 8,000x
larger tensors and a speedup up to 6.95x. We also apply our method to two
real-world applications, including gene analysis and tensor layer neural
networks, of which the numeric results demonstrate the scalability and
effectiveness of our method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13695">BackboneLearn: A Library for Scaling Mixed-Integer Optimization-Based Machine Learning. (arXiv:2311.13695v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Digalakis_V/0/1/0/all/0/1">Vassilis Digalakis Jr</a>, <a href="http://arxiv.org/find/cs/1/au:+Ziakas_C/0/1/0/all/0/1">Christos Ziakas</a></p>
<p>We present BackboneLearn: an open-source software package and framework for
scaling mixed-integer optimization (MIO) problems with indicator variables to
high-dimensional problems. This optimization paradigm can naturally be used to
formulate fundamental problems in interpretable supervised learning (e.g.,
sparse regression and decision trees), in unsupervised learning (e.g.,
clustering), and beyond; BackboneLearn solves the aforementioned problems
faster than exact methods and with higher accuracy than commonly used
heuristics. The package is built in Python and is user-friendly and easily
extensible: users can directly implement a backbone algorithm for their MIO
problem at hand. The source code of BackboneLearn is available on GitHub (link:
https://github.com/chziakas/backbone_learn).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13707">Bayes-xG: Player and Position Correction on Expected Goals (xG) using Bayesian Hierarchical Approach. (arXiv:2311.13707v1 [stat.AP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Scholtes_A/0/1/0/all/0/1">Alexander Scholtes</a>, <a href="http://arxiv.org/find/stat/1/au:+Karakus_O/0/1/0/all/0/1">Oktay Karaku&#x15f;</a></p>
<p>This study employs Bayesian methodologies to explore the influence of player
or positional factors in predicting the probability of a shot resulting in a
goal, measured by the expected goals (xG) metric. Utilising publicly available
data from StatsBomb, Bayesian hierarchical logistic regressions are
constructed, analysing approximately 10,000 shots from the English Premier
League to ascertain whether positional or player-level effects impact xG. The
findings reveal positional effects in a basic model that includes only distance
to goal and shot angle as predictors, highlighting that strikers and attacking
midfielders exhibit a higher likelihood of scoring. However, these effects
diminish when more informative predictors are introduced. Nevertheless, even
with additional predictors, player-level effects persist, indicating that
certain players possess notable positive or negative xG adjustments,
influencing their likelihood of scoring a given chance. The study extends its
analysis to data from Spain's La Liga and Germany's Bundesliga, yielding
comparable results. Additionally, the paper assesses the impact of prior
distribution choices on outcomes, concluding that the priors employed in the
models provide sound results but could be refined to enhance sampling
efficiency for constructing more complex and extensive models feasibly.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13713">A Somewhat Robust Image Watermark against Diffusion-based Editing Models. (arXiv:2311.13713v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1">Mingtian Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tianhao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1">Somesh Jha</a></p>
<p>Recently, diffusion models (DMs) have become the state-of-the-art method for
image synthesis. Editing models based on DMs, known for their high fidelity and
precision, have inadvertently introduced new challenges related to image
copyright infringement and malicious editing. Our work is the first to
formalize and address this issue. After assessing and attempting to enhance
traditional image watermarking techniques, we recognize their limitations in
this emerging context. In response, we develop a novel technique, RIW (Robust
Invisible Watermarking), to embed invisible watermarks leveraging adversarial
example techniques. Our technique ensures a high extraction accuracy of $96\%$
for the invisible watermark after editing, compared to the $0\%$ offered by
conventional methods. We provide access to our code at
https://github.com/BennyTMT/RIW.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13718">A Unified Approach to Count-Based Weakly-Supervised Learning. (arXiv:2311.13718v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shukla_V/0/1/0/all/0/1">Vinay Shukla</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1">Zhe Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_K/0/1/0/all/0/1">Kareem Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Broeck_G/0/1/0/all/0/1">Guy Van den Broeck</a></p>
<p>High-quality labels are often very scarce, whereas unlabeled data with
inferred weak labels occurs more naturally. In many cases, these weak labels
dictate the frequency of each respective class over a set of instances. In this
paper, we develop a unified approach to learning from such weakly-labeled data,
which we call count-based weakly-supervised learning. At the heart of our
approach is the ability to compute the probability of exactly k out of n
outputs being set to true. This computation is differentiable, exact, and
efficient. Building upon the previous computation, we derive a count loss
penalizing the model for deviations in its distribution from an arithmetic
constraint defined over label counts. We evaluate our approach on three common
weakly-supervised learning paradigms and observe that our proposed approach
achieves state-of-the-art or highly competitive results across all three of the
paradigms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13722">Deep Learning as a Method for Inversion of NMR Signals. (arXiv:2311.13722v1 [physics.chem-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Beckmann_J/0/1/0/all/0/1">Julian B. B. Beckmann</a>, <a href="http://arxiv.org/find/physics/1/au:+Mantle_M/0/1/0/all/0/1">Mick D. Mantle</a>, <a href="http://arxiv.org/find/physics/1/au:+Sederman_A/0/1/0/all/0/1">Andrew J. Sederman</a>, <a href="http://arxiv.org/find/physics/1/au:+Gladden_L/0/1/0/all/0/1">Lynn F. Gladden</a></p>
<p>The concept of deep learning is employed for the inversion of NMR signals and
it is shown that NMR signal inversion can be considered as an image-to-image
regression problem, which can be treated with a convolutional neural net. It is
further outlined, that inversion through deep learning provides a clear
efficiency and usability advantage compared to regularization techniques such
as Tikhonov and modified total generalized variation (MTGV), because no
hyperparemeter selection prior to reconstruction is necessary. The inversion
network is applied to simulated NMR signals and the results compared with
Tikhonov- and MTGV-regularization. The comparison shows that inversion via deep
learning is significantly faster than the latter regularization methods and
also outperforms both regularization techniques in nearly all instances.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13743">FinMe: A Performance-Enhanced Large Language Model Trading Agent with Layered Memory and Character Design. (arXiv:2311.13743v1 [q-fin.CP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+Yu_Y/0/1/0/all/0/1">Yangyang Yu</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Li_H/0/1/0/all/0/1">Haohang Li</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Chen_Z/0/1/0/all/0/1">Zhi Chen</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Jiang_Y/0/1/0/all/0/1">Yuechen Jiang</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Li_Y/0/1/0/all/0/1">Yang Li</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Zhang_D/0/1/0/all/0/1">Denghui Zhang</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Liu_R/0/1/0/all/0/1">Rong Liu</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Suchow_J/0/1/0/all/0/1">Jordan W. Suchow</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Khashanah_K/0/1/0/all/0/1">Khaldoun Khashanah</a></p>
<p>Recent advancements in Large Language Models (LLMs) have exhibited notable
efficacy in question-answering (QA) tasks across diverse domains. Their prowess
in integrating extensive web knowledge has fueled interest in developing LLM
autonomous agents. While LLMs are efficient in decoding human instructions and
deriving solutions by holistically processing historical inputs, transitioning
to purpose-driven agents requires a supplementary rational architecture to
process multi-source information, establish reasoning chains, and prioritize
critical tasks. Addressing this, we introduce \textsc{FinMe}, a novel LLM-based
agent framework devised for financial decision-making, encompassing three core
modules: Profiling, to outline the agent's characteristics; Memory, with
layered processing, to aid the agent in assimilating realistic hierarchical
financial data; and Decision-making, to convert insights gained from memories
into investment decisions. Notably, \textsc{FinMe}'s memory module aligns
closely with the cognitive structure of human traders, offering robust
interpretability and real-time tuning. Its adjustable cognitive span allows for
the retention of critical information beyond human perceptual limits, thereby
enhancing trading outcomes. This framework enables the agent to self-evolve its
professional knowledge, react agilely to new investment cues, and continuously
refine trading decisions in the volatile financial environment. We first
compare \textsc{FinMe} with various algorithmic agents on a scalable real-world
financial dataset, underscoring its leading trading performance in stocks and
funds. We then fine-tuned the agent's perceptual spans to achieve a significant
trading performance. Collectively, \textsc{FinMe} presents a cutting-edge LLM
agent framework for automated trading, boosting cumulative investment returns.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13745">Sample-Efficient Training for Diffusion. (arXiv:2311.13745v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1">Shivam Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Parulekar_A/0/1/0/all/0/1">Aditya Parulekar</a>, <a href="http://arxiv.org/find/cs/1/au:+Price_E/0/1/0/all/0/1">Eric Price</a>, <a href="http://arxiv.org/find/cs/1/au:+Xun_Z/0/1/0/all/0/1">Zhiyang Xun</a></p>
<p>Score-based diffusion models have become the most popular approach to deep
generative modeling of images, largely due to their empirical performance and
reliability. Recently, a number of theoretical works \citep{chen2022,
Chen2022ImprovedAO, Chenetal23flowode, benton2023linear} have shown that
diffusion models can efficiently sample, assuming $L^2$-accurate score
estimates. The score-matching objective naturally approximates the true score
in $L^2$, but the sample complexity of existing bounds depends
\emph{polynomially} on the data radius and desired Wasserstein accuracy. By
contrast, the time complexity of sampling is only logarithmic in these
parameters. We show that estimating the score in $L^2$ \emph{requires} this
polynomial dependence, but that a number of samples that scales
polylogarithmically in the Wasserstein accuracy actually do suffice for
sampling. We show that with a polylogarithmic number of samples, the ERM of the
score-matching objective is $L^2$ accurate on all but a probability $\delta$
fraction of the true distribution, and that this weaker guarantee is sufficient
for efficient sampling.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13749">On Principles of Emergent Organization. (arXiv:2311.13749v1 [cond-mat.stat-mech])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Rupe_A/0/1/0/all/0/1">Adam T. Rupe</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Crutchfield_J/0/1/0/all/0/1">James P. Crutchfield</a></p>
<p>After more than a century of concerted effort, physics still lacks basic
principles of spontaneous self-organization. To appreciate why, we first state
the problem, outline historical approaches, and survey the present state of the
physics of self-organization. This frames the particular challenges arising
from mathematical intractability and the resulting need for computational
approaches, as well as those arising from a chronic failure to define
structure. Then, an overview of two modern mathematical formulations of
organization -- intrinsic computation and evolution operators -- lays out a way
to overcome these challenges. Together, the vantage point they afford shows how
to account for the emergence of structured states via a statistical mechanics
of systems arbitrarily far from equilibrium. The result is a constructive path
forward to principles of organization that builds on mathematical
identification of structure.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13750">Towards Transferable Multi-modal Perception Representation Learning for Autonomy: NeRF-Supervised Masked AutoEncoder. (arXiv:2311.13750v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiaohao Xu</a></p>
<p>This work proposes a unified self-supervised pre-training framework for
transferable multi-modal perception representation learning via masked
multi-modal reconstruction in Neural Radiance Field (NeRF), namely
NeRF-Supervised Masked AutoEncoder (NS-MAE). Specifically, conditioned on
certain view directions and locations, multi-modal embeddings extracted from
corrupted multi-modal input signals, i.e., Lidar point clouds and images, are
rendered into projected multi-modal feature maps via neural rendering. Then,
original multi-modal signals serve as reconstruction targets for the rendered
multi-modal feature maps to enable self-supervised representation learning.
Extensive experiments show that the representation learned via NS-MAE shows
promising transferability for diverse multi-modal and single-modal (camera-only
and Lidar-only) perception models on diverse 3D perception downstream tasks (3D
object detection and BEV map segmentation) with diverse amounts of fine-tuning
labeled data. Moreover, we empirically find that NS-MAE enjoys the synergy of
both the mechanism of masked autoencoder and neural radiance field. Our code
shall be released upon acceptance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13763">Extraction of n = 0 pick-up by locked mode detectors based on neural networks in J-TEXT. (arXiv:2311.13763v1 [physics.plasm-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Shen_C/0/1/0/all/0/1">Chengshuo Shen</a>, <a href="http://arxiv.org/find/physics/1/au:+Li_J/0/1/0/all/0/1">Jianchao Li</a>, <a href="http://arxiv.org/find/physics/1/au:+Ding_Y/0/1/0/all/0/1">Yonghua Ding</a>, <a href="http://arxiv.org/find/physics/1/au:+Dong_J/0/1/0/all/0/1">Jiaolong Dong</a>, <a href="http://arxiv.org/find/physics/1/au:+Wang_N/0/1/0/all/0/1">Nengchao Wang</a>, <a href="http://arxiv.org/find/physics/1/au:+Han_D/0/1/0/all/0/1">Dongliang.Han</a>, <a href="http://arxiv.org/find/physics/1/au:+Mao_F/0/1/0/all/0/1">Feiyue Mao</a>, <a href="http://arxiv.org/find/physics/1/au:+Li_D/0/1/0/all/0/1">Da Li</a>, <a href="http://arxiv.org/find/physics/1/au:+Chen_Z/0/1/0/all/0/1">Zhipeng Chen</a>, <a href="http://arxiv.org/find/physics/1/au:+Yang_Z/0/1/0/all/0/1">Zhoujun Yang</a>, <a href="http://arxiv.org/find/physics/1/au:+Chen_Z/0/1/0/all/0/1">Zhongyong Chen</a>, <a href="http://arxiv.org/find/physics/1/au:+Pan_Y/0/1/0/all/0/1">Yuan Pan</a>, <a href="http://arxiv.org/find/physics/1/au:+Team_J_Text/0/1/0/all/0/1">J-Text Team</a></p>
<p>Measurement of locked mode (LM) is important for the physical research of
Magnetohydrodynamic (MHD) instabilities and plasma disruption. The n = 0
pick-up need to be extracted and subtracted to calculate the amplitude and
phase of the LM. A new method to extract this pick-up has been developed by
predicting the n = 0 pick-up brn=0 by the LM detectors based on Neural Networks
(NNs) in J-TEXT. An approach called Power Multiple Time Scale (PMTS) has been
developed with outstanding regressing effect in multiple frequency ranges.
Three models have been progressed based on PMTS NNs. PMTS could fit the brn=0
on the LM detectors with little errors both in time domain and frequency
domain. The n&gt;0 pick-up brn&gt;0 generated by resonant magnetic perturbations
(RMPs) can be obtained after subtracting the extracted brn=0. This new method
uses only one LM instead of 4 LM detectors to extract brn=0. Therefore, the
distribution of the LM detectors can also be optimized based on this new
method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13765">Learning Optimal and Fair Policies for Online Allocation of Scarce Societal Resources from Data Collected in Deployment. (arXiv:2311.13765v1 [math.OC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Tang_B/0/1/0/all/0/1">Bill Tang</a>, <a href="http://arxiv.org/find/math/1/au:+Kocyigit_C/0/1/0/all/0/1">&#xc7;a&#x11f;&#x131;l Ko&#xe7;yi&#x11f;it</a>, <a href="http://arxiv.org/find/math/1/au:+Rice_E/0/1/0/all/0/1">Eric Rice</a>, <a href="http://arxiv.org/find/math/1/au:+Vayanos_P/0/1/0/all/0/1">Phebe Vayanos</a></p>
<p>We study the problem of allocating scarce societal resources of different
types (e.g., permanent housing, deceased donor kidneys for transplantation,
ventilators) to heterogeneous allocatees on a waitlist (e.g., people
experiencing homelessness, individuals suffering from end-stage renal disease,
Covid-19 patients) based on their observed covariates. We leverage
administrative data collected in deployment to design an online policy that
maximizes expected outcomes while satisfying budget constraints, in the long
run. Our proposed policy waitlists each individual for the resource maximizing
the difference between their estimated mean treatment outcome and the estimated
resource dual-price or, roughly, the opportunity cost of using the resource.
Resources are then allocated as they arrive, in a first-come first-serve
fashion. We demonstrate that our data-driven policy almost surely
asymptotically achieves the expected outcome of the optimal out-of-sample
policy under mild technical assumptions. We extend our framework to incorporate
various fairness constraints. We evaluate the performance of our approach on
the problem of designing policies for allocating scarce housing resources to
people experiencing homelessness in Los Angeles based on data from the homeless
management information system. In particular, we show that using our policies
improves rates of exit from homelessness by 1.9% and that policies that are
fair in either allocation or outcomes by race come at a very low price of
fairness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13766">A Unified Framework for Fair Spectral Clustering With Effective Graph Learning. (arXiv:2311.13766v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qiao Wang</a></p>
<p>We consider the problem of spectral clustering under group fairness
constraints, where samples from each sensitive group are approximately
proportionally represented in each cluster. Traditional fair spectral
clustering (FSC) methods consist of two consecutive stages, i.e., performing
fair spectral embedding on a given graph and conducting $k$means to obtain
discrete cluster labels. However, in practice, the graph is usually unknown,
and we need to construct the underlying graph from potentially noisy data, the
quality of which inevitably affects subsequent fair clustering performance.
Furthermore, performing FSC through separate steps breaks the connections among
these steps, leading to suboptimal results. To this end, we first theoretically
analyze the effect of the constructed graph on FSC. Motivated by the analysis,
we propose a novel graph construction method with a node-adaptive graph filter
to learn graphs from noisy data. Then, all independent stages of conventional
FSC are integrated into a single objective function, forming an end-to-end
framework that inputs raw data and outputs discrete cluster labels. An
algorithm is developed to jointly and alternately update the variables in each
stage. Finally, we conduct extensive experiments on synthetic, benchmark, and
real data, which show that our model is superior to state-of-the-art fair
clustering methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13774">Learning Hierarchical Polynomials with Three-Layer Neural Networks. (arXiv:2311.13774v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zihao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Nichani_E/0/1/0/all/0/1">Eshaan Nichani</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jason D. Lee</a></p>
<p>We study the problem of learning hierarchical polynomials over the standard
Gaussian distribution with three-layer neural networks. We specifically
consider target functions of the form $h = g \circ p$ where $p : \mathbb{R}^d
\rightarrow \mathbb{R}$ is a degree $k$ polynomial and $g: \mathbb{R}
\rightarrow \mathbb{R}$ is a degree $q$ polynomial. This function class
generalizes the single-index model, which corresponds to $k=1$, and is a
natural class of functions possessing an underlying hierarchical structure. Our
main result shows that for a large subclass of degree $k$ polynomials $p$, a
three-layer neural network trained via layerwise gradient descent on the square
loss learns the target $h$ up to vanishing test error in
$\widetilde{\mathcal{O}}(d^k)$ samples and polynomial time. This is a strict
improvement over kernel methods, which require $\widetilde \Theta(d^{kq})$
samples, as well as existing guarantees for two-layer networks, which require
the target function to be low-rank. Our result also generalizes prior works on
three-layer neural networks, which were restricted to the case of $p$ being a
quadratic. When $p$ is indeed a quadratic, we achieve the
information-theoretically optimal sample complexity
$\widetilde{\mathcal{O}}(d^2)$, which is an improvement over prior
work~\citep{nichani2023provable} requiring a sample size of
$\widetilde\Theta(d^4)$. Our proof proceeds by showing that during the initial
stage of training the network performs feature learning to recover the feature
$p$ with $\widetilde{\mathcal{O}}(d^k)$ samples. This work demonstrates the
ability of three-layer neural networks to learn complex features and as a
result, learn a broad class of hierarchical functions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13789">Knowledge Distillation Based Semantic Communications For Multiple Users. (arXiv:2311.13789v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1">Chenguang Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1">Yuxin Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1">Yunfei Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1">Shuang-Hua Yang</a></p>
<p>Deep learning (DL) has shown great potential in revolutionizing the
traditional communications system. Many applications in communications have
adopted DL techniques due to their powerful representation ability. However,
the learning-based methods can be dependent on the training dataset and perform
worse on unseen interference due to limited model generalizability and
complexity. In this paper, we consider the semantic communication (SemCom)
system with multiple users, where there is a limited number of training samples
and unexpected interference. To improve the model generalization ability and
reduce the model size, we propose a knowledge distillation (KD) based system
where Transformer based encoder-decoder is implemented as the semantic
encoder-decoder and fully connected neural networks are implemented as the
channel encoder-decoder. Specifically, four types of knowledge transfer and
model compression are analyzed. Important system and model parameters are
considered, including the level of noise and interference, the number of
interfering users and the size of the encoder and decoder. Numerical results
demonstrate that KD significantly improves the robustness and the
generalization ability when applied to unexpected interference, and it reduces
the performance loss when compressing the model size.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13800">Enhancing Intrusion Detection In Internet Of Vehicles Through Federated Learning. (arXiv:2311.13800v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sebastian_A/0/1/0/all/0/1">Abhishek Sebastian</a>, <a href="http://arxiv.org/find/cs/1/au:+R_P/0/1/0/all/0/1">Pragna R</a>, <a href="http://arxiv.org/find/cs/1/au:+G_S/0/1/0/all/0/1">Sudhakaran G</a>, <a href="http://arxiv.org/find/cs/1/au:+N_R/0/1/0/all/0/1">Renjith P N</a>, <a href="http://arxiv.org/find/cs/1/au:+H_L/0/1/0/all/0/1">Leela Karthikeyan H</a></p>
<p>Federated learning is a technique of decentralized machine learning. that
allows multiple parties to collaborate and learn a shared model without sharing
their raw data. Our paper proposes a federated learning framework for intrusion
detection in Internet of Vehicles (IOVs) using the CIC-IDS 2017 dataset. The
proposed framework employs SMOTE for handling class imbalance, outlier
detection for identifying and removing abnormal observations, and
hyperparameter tuning to optimize the model's performance. The authors
evaluated the proposed framework using various performance metrics and
demonstrated its effectiveness in detecting intrusions with other datasets
(KDD-Cup 99 and UNSW- NB-15) and conventional classifiers. Furthermore, the
proposed framework can protect sensitive data while achieving high intrusion
detection performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13806">AdaTyper: Adaptive Semantic Column Type Detection. (arXiv:2311.13806v1 [cs.DB])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hulsebos_M/0/1/0/all/0/1">Madelon Hulsebos</a>, <a href="http://arxiv.org/find/cs/1/au:+Groth_P/0/1/0/all/0/1">Paul Groth</a>, <a href="http://arxiv.org/find/cs/1/au:+Demiralp_C/0/1/0/all/0/1">&#xc7;a&#x11f;atay Demiralp</a></p>
<p>Understanding the semantics of relational tables is instrumental for
automation in data exploration and preparation systems. A key source for
understanding a table is the semantics of its columns. With the rise of deep
learning, learned table representations are now available, which can be applied
for semantic type detection and achieve good performance on benchmarks.
Nevertheless, we observe a gap between this performance and its applicability
in practice. In this paper, we propose AdaTyper to address one of the most
critical deployment challenges: adaptation. AdaTyper uses weak-supervision to
adapt a hybrid type predictor towards new semantic types and shifted data
distributions at inference time, using minimal human feedback. The hybrid type
predictor of AdaTyper combines rule-based methods and a light machine learning
model for semantic column type detection. We evaluate the adaptation
performance of AdaTyper on real-world database tables hand-annotated with
semantic column types through crowdsourcing and find that the f1-score improves
for new and existing types. AdaTyper approaches an average precision of 0.6
after only seeing 5 examples, significantly outperforming existing adaptation
methods based on human-provided regular expressions or dictionaries.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13810">Bridging Classical and Quantum Machine Learning: Knowledge Transfer From Classical to Quantum Neural Networks Using Knowledge Distillation. (arXiv:2311.13810v1 [quant-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Hasan_M/0/1/0/all/0/1">Mohammad Junayed Hasan</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Mahdy_M/0/1/0/all/0/1">M.R.C.Mahdy</a></p>
<p>Very recently, studies have shown that quantum neural networks surpass
classical neural networks in tasks like image classification when a similar
number of learnable parameters are used. However, the development and
optimization of quantum models are currently hindered by issues such as qubit
instability and limited qubit availability, leading to error-prone systems with
weak performance. In contrast, classical models can exhibit high-performance
owing to substantial resource availability. As a result, more studies have been
focusing on hybrid classical-quantum integration. A line of research
particularly focuses on transfer learning through classical-quantum integration
or quantum-quantum approaches. Unlike previous studies, this paper introduces a
new method to transfer knowledge from classical to quantum neural networks
using knowledge distillation, effectively bridging the gap between classical
machine learning and emergent quantum computing techniques. We adapt classical
convolutional neural network (CNN) architectures like LeNet and AlexNet to
serve as teacher networks, facilitating the training of student quantum models
by sending supervisory signals during backpropagation through KL-divergence.
The approach yields significant performance improvements for the quantum models
by solely depending on classical CNNs, with quantum models achieving an average
accuracy improvement of 0.80% on the MNIST dataset and 5.40% on the more
complex Fashion MNIST dataset. Applying this technique eliminates the
cumbersome training of huge quantum models for transfer learning in
resource-constrained settings and enables re-using existing pre-trained
classical models to improve performance.Thus, this study paves the way for
future research in quantum machine learning (QML) by positioning knowledge
distillation as a core technique for advancing QML applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13816">Fairness-Aware Domain Generalization under Covariate and Dependence Shifts. (arXiv:2311.13816v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1">Chen Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1">Kai Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xintao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haoliang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_L/0/1/0/all/0/1">Latifur Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Grant_C/0/1/0/all/0/1">Christan Grant</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1">Feng Chen</a></p>
<p>Achieving the generalization of an invariant classifier from source domains
to shifted target domains while simultaneously considering model fairness is a
substantial and complex challenge in machine learning. Existing domain
generalization research typically attributes domain shifts to concept shift,
which relates to alterations in class labels, and covariate shift, which
pertains to variations in data styles. In this paper, by introducing another
form of distribution shift, known as dependence shift, which involves
variations in fair dependence patterns across domains, we propose a novel
domain generalization approach that addresses domain shifts by considering both
covariate and dependence shifts. We assert the existence of an underlying
transformation model can transform data from one domain to another. By
generating data in synthetic domains through the model, a fairness-aware
invariant classifier is learned that enforces both model accuracy and fairness
in unseen domains. Extensive empirical studies on four benchmark datasets
demonstrate that our approach surpasses state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13817">Molecular Identification and Peak Assignment: Leveraging Multi-Level Multimodal Alignment on NMR. (arXiv:2311.13817v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhengyang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_P/0/1/0/all/0/1">Pengyu Hong</a></p>
<p>Nuclear magnetic resonance (NMR) spectroscopy plays an essential role across
various scientific disciplines, providing valuable insights into molecular
dynamics and interactions. Despite the promise of AI-enhanced NMR prediction
models, challenges persist in the interpretation of spectra for tasks such as
molecular retrieval, isomer recognition, and peak assignment. In response, this
paper introduces Multi-Level Multimodal Alignment with Knowledge-Guided
Instance-Wise Discrimination (K-M3AID) to establish meaningful correspondences
between two heterogeneous modalities: molecular graphs (structures) and NMR
spectra. In particular, K-M3AID employs a dual-coordinated contrastive learning
architecture, and incorporates a graph-level alignment module, a node-level
alignment module, and a communication channel. Notably, the framework
introduces knowledge-guided instance-wise discrimination into contrastive
learning within the node-level alignment module, significantly enhancing
accuracy in cross-modal alignment. Additionally, K-M3AID showcases its
capability of meta-learning by demonstrating that skills acquired during
node-level alignment positively impact graph-level alignment. Empirical
validation underscores K-M3AID's effectiveness in addressing multiple zero-shot
tasks, offering a promising solution to bridge the gap between structural
information and spectral data in complex NMR scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13821">HypUC: Hyperfine Uncertainty Calibration with Gradient-boosted Corrections for Reliable Regression on Imbalanced Electrocardiograms. (arXiv:2311.13821v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Upadhyay_U/0/1/0/all/0/1">Uddeshya Upadhyay</a>, <a href="http://arxiv.org/find/cs/1/au:+Bade_S/0/1/0/all/0/1">Sairam Bade</a>, <a href="http://arxiv.org/find/cs/1/au:+Puranik_A/0/1/0/all/0/1">Arjun Puranik</a>, <a href="http://arxiv.org/find/cs/1/au:+Asfahan_S/0/1/0/all/0/1">Shahir Asfahan</a>, <a href="http://arxiv.org/find/cs/1/au:+Babu_M/0/1/0/all/0/1">Melwin Babu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lopez_Jimenez_F/0/1/0/all/0/1">Francisco Lopez-Jimenez</a>, <a href="http://arxiv.org/find/cs/1/au:+Asirvatham_S/0/1/0/all/0/1">Samuel J. Asirvatham</a>, <a href="http://arxiv.org/find/cs/1/au:+Prasad_A/0/1/0/all/0/1">Ashim Prasad</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajasekharan_A/0/1/0/all/0/1">Ajit Rajasekharan</a>, <a href="http://arxiv.org/find/cs/1/au:+Awasthi_S/0/1/0/all/0/1">Samir Awasthi</a>, <a href="http://arxiv.org/find/cs/1/au:+Barve_R/0/1/0/all/0/1">Rakesh Barve</a></p>
<p>The automated analysis of medical time series, such as the electrocardiogram
(ECG), electroencephalogram (EEG), pulse oximetry, etc, has the potential to
serve as a valuable tool for diagnostic decisions, allowing for remote
monitoring of patients and more efficient use of expensive and time-consuming
medical procedures. Deep neural networks (DNNs) have been demonstrated to
process such signals effectively. However, previous research has primarily
focused on classifying medical time series rather than attempting to regress
the continuous-valued physiological parameters central to diagnosis. One
significant challenge in this regard is the imbalanced nature of the dataset,
as a low prevalence of abnormal conditions can lead to heavily skewed data that
results in inaccurate predictions and a lack of certainty in such predictions
when deployed. To address these challenges, we propose HypUC, a framework for
imbalanced probabilistic regression in medical time series, making several
contributions. (i) We introduce a simple kernel density-based technique to
tackle the imbalanced regression problem with medical time series. (ii)
Moreover, we employ a probabilistic regression framework that allows
uncertainty estimation for the predicted continuous values. (iii) We also
present a new approach to calibrate the predicted uncertainty further. (iv)
Finally, we demonstrate a technique to use calibrated uncertainty estimates to
improve the predicted continuous value and show the efficacy of the calibrated
uncertainty estimates to flag unreliable predictions. HypUC is evaluated on a
large, diverse, real-world dataset of ECGs collected from millions of patients,
outperforming several conventional baselines on various diagnostic tasks,
suggesting a potential use-case for the reliable clinical deployment of deep
learning models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13827">Stability and L2-penalty in Model Averaging. (arXiv:2311.13827v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Zhu_H/0/1/0/all/0/1">Hengkun Zhu</a>, <a href="http://arxiv.org/find/stat/1/au:+Zou_G/0/1/0/all/0/1">Guohua Zou</a></p>
<p>Model averaging has received much attention in the past two decades, which
integrates available information by averaging over potential models. Although
various model averaging methods have been developed, there are few literatures
on the theoretical properties of model averaging from the perspective of
stability, and the majority of these methods constrain model weights to a
simplex. The aim of this paper is to introduce stability from statistical
learning theory into model averaging. Thus, we define the stability, asymptotic
empirical risk minimizer, generalization, and consistency of model averaging
and study the relationship among them. Our results indicate that stability can
ensure that model averaging has good generalization performance and consistency
under reasonable conditions, where consistency means model averaging estimator
can asymptotically minimize the mean squared prediction error. We also propose
a L2-penalty model averaging method without limiting model weights and prove
that it has stability and consistency. In order to reduce the impact of tuning
parameter selection, we use 10-fold cross-validation to select a candidate set
of tuning parameters and perform a weighted average of the estimators of model
weights based on estimation errors. The Monte Carlo simulation and an
illustrative application demonstrate the usefulness of the proposed method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13833">Lego: Learning to Disentangle and Invert Concepts Beyond Object Appearance in Text-to-Image Diffusion Models. (arXiv:2311.13833v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Motamed_S/0/1/0/all/0/1">Saman Motamed</a>, <a href="http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1">Danda Pani Paudel</a>, <a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1">Luc Van Gool</a></p>
<p>Diffusion models have revolutionized generative content creation and
text-to-image (T2I) diffusion models in particular have increased the creative
freedom of users by allowing scene synthesis using natural language. T2I models
excel at synthesizing concepts such as nouns, appearances, and styles. To
enable customized content creation based on a few example images of a concept,
methods such as Textual Inversion and DreamBooth invert the desired concept and
enable synthesizing it in new scenes. However, inverting more general concepts
that go beyond object appearance and style (adjectives and verbs) through
natural language, remains a challenge. Two key characteristics of these
concepts contribute to the limitations of current inversion methods. 1)
Adjectives and verbs are entangled with nouns (subject) and can hinder
appearance-based inversion methods, where the subject appearance leaks into the
concept embedding and 2) describing such concepts often extends beyond single
word embeddings (being frozen in ice, walking on a tightrope, etc.) that
current methods do not handle.
</p>
<p>In this study, we introduce Lego, a textual inversion method designed to
invert subject entangled concepts from a few example images. Lego disentangles
concepts from their associated subjects using a simple yet effective Subject
Separation step and employs a Context Loss that guides the inversion of
single/multi-embedding concepts. In a thorough user study, Lego-generated
concepts were preferred over 70% of the time when compared to the baseline.
Additionally, visual question answering using a large language model suggested
Lego-generated concepts are better aligned with the text description of the
concept.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13843">Exact Combinatorial Optimization with Temporo-Attentional Graph Neural Networks. (arXiv:2311.13843v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Seyfi_M/0/1/0/all/0/1">Mehdi Seyfi</a>, <a href="http://arxiv.org/find/cs/1/au:+Banitalebi_Dehkordi_A/0/1/0/all/0/1">Amin Banitalebi-Dehkordi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zirui Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yong Zhang</a></p>
<p>Combinatorial optimization finds an optimal solution within a discrete set of
variables and constraints. The field has seen tremendous progress both in
research and industry. With the success of deep learning in the past decade, a
recent trend in combinatorial optimization has been to improve state-of-the-art
combinatorial optimization solvers by replacing key heuristic components with
machine learning (ML) models. In this paper, we investigate two essential
aspects of machine learning algorithms for combinatorial optimization: temporal
characteristics and attention. We argue that for the task of variable selection
in the branch-and-bound (B&amp;B) algorithm, incorporating the temporal information
as well as the bipartite graph attention improves the solver's performance. We
support our claims with intuitions and numerical results over several standard
datasets used in the literature and competitions. Code is available at:
https://developer.huaweicloud.com/develop/aigallery/notebook/detail?id=047c6cf2-8463-40d7-b92f-7b2ca998e935
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13845">Touring sampling with pushforward maps. (arXiv:2311.13845v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cabannes_V/0/1/0/all/0/1">Vivien Cabannes</a>, <a href="http://arxiv.org/find/cs/1/au:+Arnal_C/0/1/0/all/0/1">Charles Arnal</a></p>
<p>The number of sampling methods could be daunting for a practitioner looking
to cast powerful machine learning methods to their specific problem. This paper
takes a theoretical stance to review and organize many sampling approaches in
the ``generative modeling'' setting, where one wants to generate new data that
are similar to some training examples. By revealing links between existing
methods, it might prove useful to overcome some of the current challenges in
sampling with diffusion models, such as long inference time due to diffusion
simulation, or the lack of diversity in generated samples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13864">Which Matters Most in Making Fund Investment Decisions? A Multi-granularity Graph Disentangled Learning Framework. (arXiv:2311.13864v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1">Chunjing Gan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1">Binbin Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1">Bo Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1">Tianyu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yingru Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1">Wenliang Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhiqiang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jun Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1">Chuan Shi</a></p>
<p>In this paper, we highlight that both conformity and risk preference matter
in making fund investment decisions beyond personal interest and seek to
jointly characterize these aspects in a disentangled manner. Consequently, we
develop a novel M ulti-granularity Graph Disentangled Learning framework named
MGDL to effectively perform intelligent matching of fund investment products.
Benefiting from the well-established fund graph and the attention module,
multi-granularity user representations are derived from historical behaviors to
separately express personal interest, conformity and risk preference in a
fine-grained way. To attain stronger disentangled representations with specific
semantics, MGDL explicitly involve two self-supervised signals, i.e., fund type
based contrasts and fund popularity. Extensive experiments in offline and
online environments verify the effectiveness of MGDL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13870">L(M)V-IQL: Multiple Intention Inverse Reinforcement Learning for Animal Behavior Characterization. (arXiv:2311.13870v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1">Hao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Crompe_B/0/1/0/all/0/1">Brice De La Crompe</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalweit_G/0/1/0/all/0/1">Gabriel Kalweit</a>, <a href="http://arxiv.org/find/cs/1/au:+Schneider_A/0/1/0/all/0/1">Artur Schneider</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalweit_M/0/1/0/all/0/1">Maria Kalweit</a>, <a href="http://arxiv.org/find/cs/1/au:+Diester_I/0/1/0/all/0/1">Ilka Diester</a>, <a href="http://arxiv.org/find/cs/1/au:+Boedecker_J/0/1/0/all/0/1">Joschka Boedecker</a></p>
<p>In advancing the understanding of decision-making processes, mathematical
models, particularly Inverse Reinforcement Learning (IRL), have proven
instrumental in reconstructing animal's multiple intentions amidst complex
behaviors. Given the recent development of a continuous-time multi-intention
IRL framework, there has been persistent inquiry into inferring discrete
time-varying reward functions with multiple intention IRL approaches. To tackle
the challenge, we introduce the Latent (Markov) Variable Inverse Q-learning
(L(M)V-IQL) algorithms, a novel IRL framework tailored for accommodating
discrete intrinsic rewards. Leveraging an Expectation-Maximization approach, we
cluster observed trajectories into distinct intentions and independently solve
the IRL problem for each. Demonstrating the efficacy of L(M)V-IQL through
simulated experiments and its application to different real mouse behavior
datasets, our approach surpasses current benchmarks in animal behavior
prediction, producing interpretable reward functions. This advancement holds
promise for neuroscience and psychology, contributing to a deeper understanding
of animal decision-making and uncovering underlying brain mechanisms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13877">Locally Optimal Descent for Dynamic Stepsize Scheduling. (arXiv:2311.13877v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yehudai_G/0/1/0/all/0/1">Gilad Yehudai</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_A/0/1/0/all/0/1">Alon Cohen</a>, <a href="http://arxiv.org/find/cs/1/au:+Daniely_A/0/1/0/all/0/1">Amit Daniely</a>, <a href="http://arxiv.org/find/cs/1/au:+Drori_Y/0/1/0/all/0/1">Yoel Drori</a>, <a href="http://arxiv.org/find/cs/1/au:+Koren_T/0/1/0/all/0/1">Tomer Koren</a>, <a href="http://arxiv.org/find/cs/1/au:+Schain_M/0/1/0/all/0/1">Mariano Schain</a></p>
<p>We introduce a novel dynamic learning-rate scheduling scheme grounded in
theory with the goal of simplifying the manual and time-consuming tuning of
schedules in practice. Our approach is based on estimating the locally-optimal
stepsize, guaranteeing maximal descent in the direction of the stochastic
gradient of the current step. We first establish theoretical convergence bounds
for our method within the context of smooth non-convex stochastic optimization,
matching state-of-the-art bounds while only assuming knowledge of the
smoothness parameter. We then present a practical implementation of our
algorithm and conduct systematic experiments across diverse datasets and
optimization algorithms, comparing our scheme with existing state-of-the-art
learning-rate schedulers. Our findings indicate that our method needs minimal
tuning when compared to existing approaches, removing the need for auxiliary
manual schedules and warm-up phases and achieving comparable performance with
drastically reduced parameter tuning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13883">Leveraging Optimal Transport via Projections on Subspaces for Machine Learning Applications. (arXiv:2311.13883v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bonet_C/0/1/0/all/0/1">Cl&#xe9;ment Bonet</a></p>
<p>Optimal Transport has received much attention in Machine Learning as it
allows to compare probability distributions by exploiting the geometry of the
underlying space. However, in its original formulation, solving this problem
suffers from a significant computational burden. Thus, a meaningful line of
work consists at proposing alternatives to reduce this burden while still
enjoying its properties. In this thesis, we focus on alternatives which use
projections on subspaces. The main such alternative is the Sliced-Wasserstein
distance, which we first propose to extend to Riemannian manifolds in order to
use it in Machine Learning applications for which using such spaces has been
shown to be beneficial in the recent years. We also study sliced distances
between positive measures in the so-called unbalanced OT problem. Back to the
original Euclidean Sliced-Wasserstein distance between probability measures, we
study the dynamic of gradient flows when endowing the space with this distance
in place of the usual Wasserstein distance. Then, we investigate the use of the
Busemann function, a generalization of the inner product in metric spaces, in
the space of probability measures. Finally, we extend the subspace detour
approach to incomparable spaces using the Gromov-Wasserstein distance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13885">Can Physics Informed Neural Operators Self Improve?. (arXiv:2311.13885v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Majumdar_R/0/1/0/all/0/1">Ritam Majumdar</a>, <a href="http://arxiv.org/find/cs/1/au:+Varhade_A/0/1/0/all/0/1">Amey Varhade</a>, <a href="http://arxiv.org/find/cs/1/au:+Karande_S/0/1/0/all/0/1">Shirish Karande</a>, <a href="http://arxiv.org/find/cs/1/au:+Vig_L/0/1/0/all/0/1">Lovekesh Vig</a></p>
<p>Self-training techniques have shown remarkable value across many deep
learning models and tasks. However, such techniques remain largely unexplored
when considered in the context of learning fast solvers for systems of partial
differential equations (Eg: Neural Operators). In this work, we explore the use
of self-training for Fourier Neural Operators (FNO). Neural Operators emerged
as a data driven technique, however, data from experiments or traditional
solvers is not always readily available. Physics Informed Neural Operators
(PINO) overcome this constraint by utilizing a physics loss for the training,
however the accuracy of PINO trained without data does not match the
performance obtained by training with data. In this work we show that
self-training can be used to close this gap in performance. We examine
canonical examples, namely the 1D-Burgers and 2D-Darcy PDEs, to showcase the
efficacy of self-training. Specifically, FNOs, when trained exclusively with
physics loss through self-training, approach 1.07x for Burgers and 1.02x for
Darcy, compared to FNOs trained with both data and physics loss. Furthermore,
we discover that pseudo-labels can be used for self-training without
necessarily training to convergence in each iteration. A consequence of this is
that we are able to discover self-training schedules that improve upon the
baseline performance of PINO in terms of accuracy as well as time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13887">Unsupervised Learning for Topological Classification of Transportation Networks. (arXiv:2311.13887v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sabzekar_S/0/1/0/all/0/1">Sina Sabzekar</a>, <a href="http://arxiv.org/find/cs/1/au:+Malakshah_M/0/1/0/all/0/1">Mohammad Reza Valipour Malakshah</a>, <a href="http://arxiv.org/find/cs/1/au:+Amini_Z/0/1/0/all/0/1">Zahra Amini</a></p>
<p>With increasing urbanization, transportation plays an increasingly critical
role in city development. The number of studies on modeling, optimization,
simulation, and data analysis of transportation systems is on the rise. Many of
these studies utilize transportation test networks to represent real-world
transportation systems in urban areas, examining the efficacy of their proposed
approaches. Each of these networks exhibits unique characteristics in their
topology, making their applications distinct for various study objectives.
Despite their widespread use in research, there is a lack of comprehensive
study addressing the classification of these networks based on their
topological characteristics. This study aims to fill this gap by employing
unsupervised learning methods, particularly clustering. We present a
comprehensive framework for evaluating various topological network
characteristics. Additionally, we employ two dimensionality reduction
techniques, namely Principal Component Analysis (PCA) and Isometric Feature
Mapping (ISOMAP), to reduce overlaps of highly correlated features and enhance
the interpretability of the subsequent classification results. We then utilize
two clustering algorithms, K-means and HDBSCAN, to classify 14 transportation
networks. The PCA method, followed by the K-means clustering approach,
outperforms other alternatives with a Silhouette score of $0.510$, enabling the
classification of transportation networks into five clusters. We also provide a
detailed discussion on the resulting classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13912">Expanding the deep-learning model to diagnosis LVNC: Limitations and trade-offs. (arXiv:2311.13912v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Bernabe_G/0/1/0/all/0/1">Gregorio Bernab&#xe9;</a>, <a href="http://arxiv.org/find/eess/1/au:+Gonzalez_Ferez_P/0/1/0/all/0/1">Pilar Gonz&#xe1;lez-F&#xe9;rez</a>, <a href="http://arxiv.org/find/eess/1/au:+Garcia_J/0/1/0/all/0/1">Jos&#xe9; M. Garc&#xed;a</a>, <a href="http://arxiv.org/find/eess/1/au:+Casas_G/0/1/0/all/0/1">Guillem Casas</a>, <a href="http://arxiv.org/find/eess/1/au:+Gonzalez_Carrillo_J/0/1/0/all/0/1">Josefa Gonz&#xe1;lez-Carrillo</a></p>
<p>Hyper-trabeculation or non-compaction in the left ventricle of the myocardium
(LVNC) is a recently classified form of cardiomyopathy. Several methods have
been proposed to quantify the trabeculae accurately in the left ventricle, but
there is no general agreement in the medical community to use a particular
approach. In previous work, we proposed DL-LVTQ, a deep learning approach for
left ventricular trabecular quantification based on a U-Net CNN architecture.
DL-LVTQ was an automatic diagnosis tool developed from a dataset of patients
with the same cardiomyopathy (hypertrophic cardiomyopathy).
</p>
<p>In this work, we have extended and adapted DL-LVTQ to cope with patients with
different cardiomyopathies. The dataset consists of up 379 patients in three
groups with different particularities and cardiomyopathies. Patient images were
taken from different scanners and hospitals. We have modified and adapted the
U-Net convolutional neural network to account for the different particularities
of a heterogeneous group of patients with various unclassifiable or mixed and
inherited cardiomyopathies.
</p>
<p>The inclusion of new groups of patients has increased the accuracy,
specificity and kappa values while maintaining the sensitivity of the automatic
deep learning method proposed. Therefore, a better-prepared diagnosis tool is
ready for various cardiomyopathies with different characteristics.
Cardiologists have considered that 98.9% of the evaluated outputs are verified
clinically for diagnosis. Therefore, the high precision to segment the
different cardiac structures allows us to make a robust diagnostic system
objective and faster, decreasing human error and time spent.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13917">Exploring the impact of social stress on the adaptive dynamics of COVID-19: Typing the behavior of na\&quot;ive populations faced with epidemics. (arXiv:2311.13917v1 [physics.soc-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Kastalskiy_I/0/1/0/all/0/1">Innokentiy Kastalskiy</a>, <a href="http://arxiv.org/find/physics/1/au:+Zinovyev_A/0/1/0/all/0/1">Andrei Zinovyev</a>, <a href="http://arxiv.org/find/physics/1/au:+Mirkes_E/0/1/0/all/0/1">Evgeny Mirkes</a>, <a href="http://arxiv.org/find/physics/1/au:+Kazantsev_V/0/1/0/all/0/1">Victor Kazantsev</a>, <a href="http://arxiv.org/find/physics/1/au:+Gorban_A/0/1/0/all/0/1">Alexander N. Gorban</a></p>
<p>In the context of natural disasters, human responses inevitably intertwine
with natural factors. The COVID-19 pandemic, as a significant stress factor,
has brought to light profound variations among different countries in terms of
their adaptive dynamics in addressing the spread of infection outbreaks across
different regions. This emphasizes the crucial role of cultural characteristics
in natural disaster analysis. The theoretical understanding of large-scale
epidemics primarily relies on mean-field kinetic models. However, conventional
SIR-like models failed to fully explain the observed phenomena at the onset of
the COVID-19 outbreak. These phenomena included the unexpected cessation of
exponential growth, the reaching of plateaus, and the occurrence of multi-wave
dynamics. In situations where an outbreak of a highly virulent and unfamiliar
infection arises, it becomes crucial to respond swiftly at a non-medical level
to mitigate the negative socio-economic impact. Here we present a theoretical
examination of the first wave of the epidemic based on a simple SIRSS model
(SIR with Social Stress). We conduct an analysis of the socio-cultural features
of na\"ive population behaviors across various countries worldwide. The unique
characteristics of each country/territory are encapsulated in only a few
constants within our model, derived from the fitted COVID-19 statistics. These
constants also reflect the societal response dynamics to the external stress
factor, underscoring the importance of studying the mutual behavior of humanity
and natural factors during global social disasters. Based on these distinctive
characteristics of specific regions, local authorities can optimize their
strategies to effectively combat epidemics until vaccines are developed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13925">Predicting Recovery or Decease of COVID-19 Patients with Clinical and RT-PCR Using Machine Learning Classification Algorithms. (arXiv:2311.13925v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Dehghani_M/0/1/0/all/0/1">Mohammad Dehghani</a>, <a href="http://arxiv.org/find/eess/1/au:+Yazdanparast_Z/0/1/0/all/0/1">Zahra Yazdanparast</a></p>
<p>The COVID-19 pandemic has disrupted the global economy and people's daily
lives in unprecedented ways. To make appropriate decisions, it is necessary to
diagnose COVID-19 rapidly and accurately. Clinical decision making is
influenced by data collected from patients. With the aid of artificial
intelligence, COVID-19 has been diagnosed quickly by analyzing symptoms,
polymerase chain reaction (PCR), computed tomography scans, chest X-rays,
routine laboratory blood tests and even cough sounds. Furthermore, these data
can be used to predict a patient's morality, although there is a question about
which data makes the most accurate predictions. Therefore, this study consists
of two parts. Our first objective is to examine whether machine learning
algorithms can predict the outcome of COVID-19 cases (recovery or death), based
on the features present in the dataset. In the second part of the research, we
investigated the impact of clinical and RT-PCR on prediction of recovery and
decease to determine which one is more reliable. We defined four stages with
different feature sets and use six machine learning methods to build prediction
model. With an accuracy of 78.7%, random forest showed promising results for
predicting death and recovery of patients. Based on this, it appears that
recovery and decease of patients are predictable using machine learning. For
second objective, results indicate that clinical alone (without using RT-PCR),
trained with AdaBoost algorithm, is the most accurate with an accuracy of
82.1%. This study can provide guidance for medical professionals in the event
of a crisis or outbreak similar to COVID-19.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13949">Optimal Power Flow in Highly Renewable Power System Based on Attention Neural Networks. (arXiv:2311.13949v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Kies_A/0/1/0/all/0/1">Alexander Kies</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1">Kai Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Schlott_M/0/1/0/all/0/1">Markus Schlott</a>, <a href="http://arxiv.org/find/cs/1/au:+Sayed_O/0/1/0/all/0/1">Omar El Sayed</a>, <a href="http://arxiv.org/find/cs/1/au:+Bilousova_M/0/1/0/all/0/1">Mariia Bilousova</a>, <a href="http://arxiv.org/find/cs/1/au:+Stoecker_H/0/1/0/all/0/1">Horst Stoecker</a></p>
<p>The Optimal Power Flow (OPF) problem is pivotal for power system operations,
guiding generator output and power distribution to meet demand at minimized
costs, while adhering to physical and engineering constraints. The integration
of renewable energy sources, like wind and solar, however, poses challenges due
to their inherent variability. This variability, driven largely by changing
weather conditions, demands frequent recalibrations of power settings, thus
necessitating recurrent OPF resolutions. This task is daunting using
traditional numerical methods, particularly for extensive power systems. In
this work, we present a cutting-edge, physics-informed machine learning
methodology, trained using imitation learning and historical European weather
datasets. Our approach directly correlates electricity demand and weather
patterns with power dispatch and generation, circumventing the iterative
requirements of traditional OPF solvers. This offers a more expedient solution
apt for real-time applications. Rigorous evaluations on aggregated European
power systems validate our method's superiority over existing data-driven
techniques in OPF solving. By presenting a quick, robust, and efficient
solution, this research sets a new standard in real-time OPF resolution, paving
the way for more resilient power systems in the era of renewable energy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13950">Object Location Prediction in Real-time using LSTM Neural Network and Polynomial Regression. (arXiv:2311.13950v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stojkovic_P/0/1/0/all/0/1">Petar Stojkovi&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Tadic_P/0/1/0/all/0/1">Predrag Tadi&#x107;</a></p>
<p>This paper details the design and implementation of a system for predicting
and interpolating object location coordinates. Our solution is based on
processing inertial measurements and global positioning system data through a
Long Short-Term Memory (LSTM) neural network and polynomial regression. LSTM is
a type of recurrent neural network (RNN) particularly suited for processing
data sequences and avoiding the long-term dependency problem. We employed data
from real-world vehicles and the global positioning system (GPS) sensors. A
critical pre-processing step was developed to address varying sensor
frequencies and inconsistent GPS time steps and dropouts. The LSTM-based
system's performance was compared with the Kalman Filter. The system was tuned
to work in real-time with low latency and high precision. We tested our system
on roads under various driving conditions, including acceleration, turns,
deceleration, and straight paths. We tested our proposed solution's accuracy
and inference time and showed that it could perform in real-time. Our
LSTM-based system yielded an average error of 0.11 meters with an inference
time of 2 ms. This represents a 76\% reduction in error compared to the
traditional Kalman filter method, which has an average error of 0.46 meters
with a similar inference time to the LSTM-based system.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13953">Learning Uniform Clusters on Hypersphere for Deep Graph-level Clustering. (arXiv:2311.13953v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1">Mengling Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chaochao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Weiming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xinyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_X/0/1/0/all/0/1">Xinting Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1">Xiaolin Zheng</a></p>
<p>Graph clustering has been popularly studied in recent years. However, most
existing graph clustering methods focus on node-level clustering, i.e.,
grouping nodes in a single graph into clusters. In contrast, graph-level
clustering, i.e., grouping multiple graphs into clusters, remains largely
unexplored. Graph-level clustering is critical in a variety of real-world
applications, such as, properties prediction of molecules and community
analysis in social networks. However, graph-level clustering is challenging due
to the insufficient discriminability of graph-level representations, and the
insufficient discriminability makes deep clustering be more likely to obtain
degenerate solutions (cluster collapse). To address the issue, we propose a
novel deep graph-level clustering method called Uniform Deep Graph Clustering
(UDGC). UDGC assigns instances evenly to different clusters and then scatters
those clusters on unit hypersphere, leading to a more uniform cluster-level
distribution and a slighter cluster collapse. Specifically, we first propose
Augmentation-Consensus Optimal Transport (ACOT) for generating uniformly
distributed and reliable pseudo labels for partitioning clusters. Then we adopt
contrastive learning to scatter those clusters. Besides, we propose Center
Alignment Optimal Transport (CAOT) for guiding the model to learn better
parameters, which further promotes the cluster performance. Our empirical study
on eight well-known datasets demonstrates that UDGC significantly outperforms
the state-of-the-art models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13958">High-Order Tensor Recovery with A Tensor $U_1$ Norm. (arXiv:2311.13958v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Zheng_J/0/1/0/all/0/1">Jingjing Zheng</a>, <a href="http://arxiv.org/find/stat/1/au:+Wang_W/0/1/0/all/0/1">Wenzhe Wang</a>, <a href="http://arxiv.org/find/stat/1/au:+Zhang_X/0/1/0/all/0/1">Xiaoqin Zhang</a>, <a href="http://arxiv.org/find/stat/1/au:+Cao_Y/0/1/0/all/0/1">Yankai Cao</a>, <a href="http://arxiv.org/find/stat/1/au:+Jiang_X/0/1/0/all/0/1">Xianta Jiang</a></p>
<p>Recently, numerous tensor SVD (t-SVD)-based tensor recovery methods have
emerged, showing promise in processing visual data. However, these methods
often suffer from performance degradation when confronted with high-order
tensor data exhibiting non-smooth changes, commonly observed in real-world
scenarios but ignored by the traditional t-SVD-based methods. Our objective in
this study is to provide an effective tensor recovery technique for handling
non-smooth changes in tensor data and efficiently explore the correlations of
high-order tensor data across its various dimensions without introducing
numerous variables and weights. To this end, we introduce a new tensor
decomposition and a new tensor norm called the Tensor $U_1$ norm. We utilize
these novel techniques in solving the problem of high-order tensor completion
problem and provide theoretical guarantees for the exact recovery of the
resulting tensor completion models. An optimization algorithm is proposed to
solve the resulting tensor completion model iteratively by combining the
proximal algorithm with the Alternating Direction Method of Multipliers.
Theoretical analysis showed the convergence of the algorithm to the
Karush-Kuhn-Tucker (KKT) point of the optimization problem. Numerical
experiments demonstrated the effectiveness of the proposed method in high-order
tensor completion, especially for tensor data with non-smooth changes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13959">RankFeat\&amp;RankWeight: Rank-1 Feature/Weight Removal for Out-of-distribution Detection. (arXiv:2311.13959v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Yue Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1">Nicu Sebe</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wei Wang</a></p>
<p>The task of out-of-distribution (OOD) detection is crucial for deploying
machine learning models in real-world settings. In this paper, we observe that
the singular value distributions of the in-distribution (ID) and OOD features
are quite different: the OOD feature matrix tends to have a larger dominant
singular value than the ID feature, and the class predictions of OOD samples
are largely determined by it. This observation motivates us to propose
\texttt{RankFeat}, a simple yet effective \emph{post hoc} approach for OOD
detection by removing the rank-1 matrix composed of the largest singular value
and the associated singular vectors from the high-level feature.
\texttt{RankFeat} achieves \emph{state-of-the-art} performance and reduces the
average false positive rate (FPR95) by 17.90\% compared with the previous best
method. The success of \texttt{RankFeat} motivates us to investigate whether a
similar phenomenon would exist in the parameter matrices of neural networks. We
thus propose \texttt{RankWeight} which removes the rank-1 weight from the
parameter matrices of a single deep layer. Our \texttt{RankWeight}is also
\emph{post hoc} and only requires computing the rank-1 matrix once. As a
standalone approach, \texttt{RankWeight} has very competitive performance
against other methods across various backbones. Moreover, \texttt{RankWeight}
enjoys flexible compatibility with a wide range of OOD detection methods. The
combination of \texttt{RankWeight} and \texttt{RankFeat} refreshes the new
\emph{state-of-the-art} performance, achieving the FPR95 as low as 16.13\% on
the ImageNet-1k benchmark. Extensive ablation studies and comprehensive
theoretical analyses are presented to support the empirical results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13964">Deep Interactive Segmentation of Medical Images: A Systematic Review and Taxonomy. (arXiv:2311.13964v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Marinov_Z/0/1/0/all/0/1">Zdravko Marinov</a>, <a href="http://arxiv.org/find/eess/1/au:+Jager_P/0/1/0/all/0/1">Paul F. J&#xe4;ger</a>, <a href="http://arxiv.org/find/eess/1/au:+Egger_J/0/1/0/all/0/1">Jan Egger</a>, <a href="http://arxiv.org/find/eess/1/au:+Kleesiek_J/0/1/0/all/0/1">Jens Kleesiek</a>, <a href="http://arxiv.org/find/eess/1/au:+Stiefelhagen_R/0/1/0/all/0/1">Rainer Stiefelhagen</a></p>
<p>Interactive segmentation is a crucial research area in medical image analysis
aiming to boost the efficiency of costly annotations by incorporating human
feedback. This feedback takes the form of clicks, scribbles, or masks and
allows for iterative refinement of the model output so as to efficiently guide
the system towards the desired behavior. In recent years, deep learning-based
approaches have propelled results to a new level causing a rapid growth in the
field with 121 methods proposed in the medical imaging domain alone. In this
review, we provide a structured overview of this emerging field featuring a
comprehensive taxonomy, a systematic review of existing methods, and an
in-depth analysis of current practices. Based on these contributions, we
discuss the challenges and opportunities in the field. For instance, we find
that there is a severe lack of comparison across methods which needs to be
tackled by standardized baselines and benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13978">MedISure: Towards Assuring Machine Learning-based Medical Image Classifiers using Mixup Boundary Analysis. (arXiv:2311.13978v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Byfield_A/0/1/0/all/0/1">Adam Byfield</a>, <a href="http://arxiv.org/find/cs/1/au:+Poulett_W/0/1/0/all/0/1">William Poulett</a>, <a href="http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1">Ben Wallace</a>, <a href="http://arxiv.org/find/cs/1/au:+Jose_A/0/1/0/all/0/1">Anusha Jose</a>, <a href="http://arxiv.org/find/cs/1/au:+Tyagi_S/0/1/0/all/0/1">Shatakshi Tyagi</a>, <a href="http://arxiv.org/find/cs/1/au:+Shembekar_S/0/1/0/all/0/1">Smita Shembekar</a>, <a href="http://arxiv.org/find/cs/1/au:+Qayyum_A/0/1/0/all/0/1">Adnan Qayyum</a>, <a href="http://arxiv.org/find/cs/1/au:+Qadir_J/0/1/0/all/0/1">Junaid Qadir</a>, <a href="http://arxiv.org/find/cs/1/au:+Bilal_M/0/1/0/all/0/1">Muhammad Bilal</a></p>
<p>Machine learning (ML) models are becoming integral in healthcare
technologies, presenting a critical need for formal assurance to validate their
safety, fairness, robustness, and trustworthiness. These models are inherently
prone to errors, potentially posing serious risks to patient health and could
even cause irreparable harm. Traditional software assurance techniques rely on
fixed code and do not directly apply to ML models since these algorithms are
adaptable and learn from curated datasets through a training process. However,
adapting established principles, such as boundary testing using synthetic test
data can effectively bridge this gap. To this end, we present a novel technique
called Mix-Up Boundary Analysis (MUBA) that facilitates evaluating image
classifiers in terms of prediction fairness. We evaluated MUBA for two
important medical imaging tasks -- brain tumour classification and breast
cancer classification -- and achieved promising results. This research aims to
showcase the importance of adapting traditional assurance principles for
assessing ML models to enhance the safety and reliability of healthcare
technologies. To facilitate future research, we plan to publicly release our
code for MUBA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13983">Learning Dynamic Selection and Pricing of Out-of-Home Deliveries. (arXiv:2311.13983v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Akkerman_F/0/1/0/all/0/1">Fabian Akkerman</a>, <a href="http://arxiv.org/find/cs/1/au:+Dieter_P/0/1/0/all/0/1">Peter Dieter</a>, <a href="http://arxiv.org/find/cs/1/au:+Mes_M/0/1/0/all/0/1">Martijn Mes</a></p>
<p>Home delivery failures, traffic congestion, and relatively large handling
times have a negative impact on the profitability of last-mile logistics. These
external factors contribute to up to $28\%$ of the overall costs and $25\%$ of
emissions for the home delivery supply chain. A potential solution, showing
annual growth rates up to $36\%$, is the delivery to parcel lockers or parcel
shops, denoted by out-of-home (OOH) delivery. In the academic literature,
models of customer behavior with respect to OOH delivery were so far limited to
deterministic settings, contrasting with the stochastic nature of actual
customer choices. We model the sequential decision-making problem of which OOH
location to offer against what incentive for each incoming customer, taking
into account future customer arrivals and choices. We propose Dynamic Selection
and Pricing of OOH (DSPO), an algorithmic pipeline that uses a novel
spatial-temporal state encoding as input to a convolutional neural network. We
demonstrate the performance of our method by benchmarking it against three
state-of-the-art approaches. Our extensive numerical study, guided by
real-world data, reveals that DSPO can save $20.8\%$ in costs compared to a
situation without OOH locations, $8.1\%$ compared to a static selection and
pricing policy, and $4.6\%$ compared to a state-of-the-art demand management
benchmark. We provide comprehensive insights into the complex interplay between
OOH delivery dynamics and customer behavior influenced by pricing strategies.
The implications of our findings suggest practitioners to adopt dynamic
selection and pricing policies as OOH delivery gains a larger market share.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13987">Jam-ALT: A Formatting-Aware Lyrics Transcription Benchmark. (arXiv:2311.13987v1 [eess.AS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Cifka_O/0/1/0/all/0/1">Ond&#x159;ej C&#xed;fka</a>, <a href="http://arxiv.org/find/eess/1/au:+Dimitriou_C/0/1/0/all/0/1">Constantinos Dimitriou</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1">Cheng-i Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Schreiber_H/0/1/0/all/0/1">Hendrik Schreiber</a>, <a href="http://arxiv.org/find/eess/1/au:+Miner_L/0/1/0/all/0/1">Luke Miner</a>, <a href="http://arxiv.org/find/eess/1/au:+Stoter_F/0/1/0/all/0/1">Fabian-Robert St&#xf6;ter</a></p>
<p>Current automatic lyrics transcription (ALT) benchmarks focus exclusively on
word content and ignore the finer nuances of written lyrics including
formatting and punctuation, which leads to a potential misalignment with the
creative products of musicians and songwriters as well as listeners'
experiences. For example, line breaks are important in conveying information
about rhythm, emotional emphasis, rhyme, and high-level structure. To address
this issue, we introduce Jam-ALT, a new lyrics transcription benchmark based on
the JamendoLyrics dataset. Our contribution is twofold. Firstly, a complete
revision of the transcripts, geared specifically towards ALT evaluation by
following a newly created annotation guide that unifies the music industry's
guidelines, covering aspects such as punctuation, line breaks, spelling,
background vocals, and non-word sounds. Secondly, a suite of evaluation metrics
designed, unlike the traditional word error rate, to capture such phenomena. We
hope that the proposed benchmark contributes to the ALT task, enabling more
precise and reliable assessments of transcription systems and enhancing the
user experience in lyrics applications such as subtitle renderings for live
captioning or karaoke.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13988">Docking Multirotors in Close Proximity using Learnt Downwash Models. (arXiv:2311.13988v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shankar_A/0/1/0/all/0/1">Ajay Shankar</a>, <a href="http://arxiv.org/find/cs/1/au:+Woo_H/0/1/0/all/0/1">Heedo Woo</a>, <a href="http://arxiv.org/find/cs/1/au:+Prorok_A/0/1/0/all/0/1">Amanda Prorok</a></p>
<p>Unmodeled aerodynamic disturbances pose a key challenge for multirotor flight
when multiple vehicles are in close proximity to each other. However, certain
missions \textit{require} two multirotors to approach each other within 1-2
body-lengths of each other and hold formation -- we consider one such practical
instance: vertically docking two multirotors in the air. In this
leader-follower setting, the follower experiences significant downwash
interference from the leader in its final docking stages. To compensate for
this, we employ a learnt downwash model online within an optimal feedback
controller to accurately track a docking maneuver and then hold formation.
Through real-world flights with different maneuvers, we demonstrate that this
compensation is crucial for reducing the large vertical separation otherwise
required by conventional/naive approaches. Our evaluations show a tracking
error of less than 0.06m for the follower (a 3-4x reduction) when approaching
vertically within two body-lengths of the leader. Finally, we deploy the
complete system to effect a successful physical docking between two airborne
multirotors in a single smooth planned trajectory.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14014">On the Hyperparameter Landscapes of Machine Learning Algorithms. (arXiv:2311.14014v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1">Mingyu Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Ke Li</a></p>
<p>Despite the recent success in a plethora of hyperparameter optimization (HPO)
methods for machine learning (ML) models, the intricate interplay between model
hyperparameters (HPs) and predictive losses (a.k.a fitness), which is a key
prerequisite for understanding HPO, remain notably underexplored in our
community. This results in limited explainability in the HPO process, rendering
a lack of human trust and difficulties in pinpointing algorithm bottlenecks. In
this paper, we aim to shed light on this black box by conducting large-scale
fitness landscape analysis (FLA) on 1,500 HP loss landscapes of 6 ML models
with more than 11 model configurations, across 67 datasets and different levels
of fidelities. We reveal the first unified, comprehensive portrait of their
topographies in terms of smoothness, neutrality and modality. We also show that
such properties are highly transferable across datasets and fidelities,
providing fundamental evidence for the success of multi-fidelity and transfer
learning methods. These findings are made possible by developing a dedicated
FLA framework that incorporates a combination of visual and quantitative
measures. We further demonstrate the potential of this framework by analyzing
the NAS-Bench-101 landscape, and we believe it is able to faciliate fundamental
understanding of a broader range of AutoML tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14024">Creating and Benchmarking a Synthetic Dataset for Cloud Optical Thickness Estimation. (arXiv:2311.14024v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pirinen_A/0/1/0/all/0/1">Aleksis Pirinen</a>, <a href="http://arxiv.org/find/cs/1/au:+Abid_N/0/1/0/all/0/1">Nosheen Abid</a>, <a href="http://arxiv.org/find/cs/1/au:+Paszkowsky_N/0/1/0/all/0/1">Nuria Agues Paszkowsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Timoudas_T/0/1/0/all/0/1">Thomas Ohlson Timoudas</a>, <a href="http://arxiv.org/find/cs/1/au:+Scheirer_R/0/1/0/all/0/1">Ronald Scheirer</a>, <a href="http://arxiv.org/find/cs/1/au:+Ceccobello_C/0/1/0/all/0/1">Chiara Ceccobello</a>, <a href="http://arxiv.org/find/cs/1/au:+Kovacs_G/0/1/0/all/0/1">Gy&#xf6;rgy Kov&#xe1;cs</a>, <a href="http://arxiv.org/find/cs/1/au:+Persson_A/0/1/0/all/0/1">Anders Persson</a></p>
<p>Cloud formations often obscure optical satellite-based monitoring of the
Earth's surface, thus limiting Earth observation (EO) activities such as land
cover mapping, ocean color analysis, and cropland monitoring. The integration
of machine learning (ML) methods within the remote sensing domain has
significantly improved performance on a wide range of EO tasks, including cloud
detection and filtering, but there is still much room for improvement. A key
bottleneck is that ML methods typically depend on large amounts of annotated
data for training, which is often difficult to come by in EO contexts. This is
especially true for the task of cloud optical thickness (COT) estimation. A
reliable estimation of COT enables more fine-grained and application-dependent
control compared to using pre-specified cloud categories, as is commonly done
in practice. To alleviate the COT data scarcity problem, in this work we
propose a novel synthetic dataset for COT estimation, where top-of-atmosphere
radiances have been simulated for 12 of the spectral bands of the
Multi-Spectral Instrument (MSI) sensor onboard Sentinel-2 platforms. These data
points have been simulated under consideration of different cloud types, COTs,
and ground surface and atmospheric profiles. Extensive experimentation of
training several ML models to predict COT from the measured reflectivity of the
spectral bands demonstrates the usefulness of our proposed dataset.
Generalization to real data is also demonstrated on two satellite image
datasets -- one that is publicly available, and one which we have collected and
annotated. The synthetic data, the newly collected real dataset, code and
models have been made publicly available at
https://github.com/aleksispi/ml-cloud-opt-thick.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14028">Continual Learning of Diffusion Models with Generative Distillation. (arXiv:2311.14028v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Masip_S/0/1/0/all/0/1">Sergi Masip</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_P/0/1/0/all/0/1">Pau Rodriguez</a>, <a href="http://arxiv.org/find/cs/1/au:+Tuytelaars_T/0/1/0/all/0/1">Tinne Tuytelaars</a>, <a href="http://arxiv.org/find/cs/1/au:+Ven_G/0/1/0/all/0/1">Gido M. van de Ven</a></p>
<p>Diffusion models are powerful generative models that achieve state-of-the-art
performance in tasks such as image synthesis. However, training them demands
substantial amounts of data and computational resources. Continual learning
would allow for incrementally learning new tasks and accumulating knowledge,
thus reusing already trained models would be possible. One potentially suitable
approach is generative replay, where a copy of a generative model trained on
previous tasks produces synthetic data that are interleaved with data from the
current task. However, standard generative replay applied to diffusion models
results in a catastrophic loss in denoising capabilities. In this paper, we
propose generative distillation, an approach that distils the entire reverse
process of a diffusion model. We demonstrate that our approach significantly
improves the continual learning performance of generative replay with only a
moderate increase in the computational costs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14029">Understanding the Vulnerability of CLIP to Image Compression. (arXiv:2311.14029v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Cangxiong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1">Vinay P. Namboodiri</a>, <a href="http://arxiv.org/find/cs/1/au:+Padget_J/0/1/0/all/0/1">Julian Padget</a></p>
<p>CLIP is a widely used foundational vision-language model that is used for
zero-shot image recognition and other image-text alignment tasks. We
demonstrate that CLIP is vulnerable to change in image quality under
compression. This surprising result is further analysed using an attribution
method-Integrated Gradients. Using this attribution method, we are able to
better understand both quantitatively and qualitatively exactly the nature in
which the compression affects the zero-shot recognition accuracy of this model.
We evaluate this extensively on CIFAR-10 and STL-10. Our work provides the
basis to understand this vulnerability of CLIP and can help us develop more
effective methods to improve the robustness of CLIP and other vision-language
models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14033">Multivariate Scenario Generation of Day-Ahead Electricity Prices using Normalizing Flows. (arXiv:2311.14033v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hilger_H/0/1/0/all/0/1">Hannes Hilger</a>, <a href="http://arxiv.org/find/cs/1/au:+Witthaut_D/0/1/0/all/0/1">Dirk Witthaut</a>, <a href="http://arxiv.org/find/cs/1/au:+Dahmen_M/0/1/0/all/0/1">Manuel Dahmen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gorjao_L/0/1/0/all/0/1">Leonardo Rydin Gorjao</a>, <a href="http://arxiv.org/find/cs/1/au:+Trebbien_J/0/1/0/all/0/1">Julius Trebbien</a>, <a href="http://arxiv.org/find/cs/1/au:+Cramer_E/0/1/0/all/0/1">Eike Cramer</a></p>
<p>Trading on electricity markets requires accurate information about the
realization of electricity prices and the uncertainty attached to the
predictions. We present a probabilistic forecasting approach for day-ahead
electricity prices using the fully data-driven deep generative model called
normalizing flows. Our modeling approach generates full-day scenarios of
day-ahead electricity prices based on conditional features such as residual
load forecasts. Furthermore, we propose extended feature sets of prior
realizations and a periodic retraining scheme that allows the normalizing flow
to adapt to the changing conditions of modern electricity markets. In
particular, we investigate the impact of the energy crisis ensuing from the
Russian invasion of Ukraine. Our results highlight that the normalizing flow
generates high-quality scenarios that reproduce the true price distribution and
yield highly accurate forecasts. Additionally, our analysis highlights how our
improvements towards adaptations in changing regimes allow the normalizing flow
to adapt to changing market conditions and enables continued sampling of
high-quality day-ahead price scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14037">AdapterFL: Adaptive Heterogeneous Federated Learning for Resource-constrained Mobile Computing Systems. (arXiv:2311.14037v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1">Ruixuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1">Ming Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_Z/0/1/0/all/0/1">Zeke Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1">Jun Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Pengyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yihao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Mingsong Chen</a></p>
<p>Federated Learning (FL) enables collaborative learning of large-scale
distributed clients without data sharing. However, due to the disparity of
computing resources among massive mobile computing devices, the performance of
traditional homogeneous model-based Federated Learning (FL) is seriously
limited. On the one hand, to achieve model training in all the diverse clients,
mobile computing systems can only use small low-performance models for
collaborative learning. On the other hand, devices with high computing
resources cannot train a high-performance large model with their insufficient
raw data. To address the resource-constrained problem in mobile computing
systems, we present a novel heterogeneous FL approach named AdapterFL, which
uses a model reassemble strategy to facilitate collaborative training of
massive heterogeneous mobile devices adaptively. Specifically, we select
multiple candidate heterogeneous models based on the computing performance of
massive mobile devices and then divide each heterogeneous model into two
partitions. By reassembling the partitions, we can generate models with varied
sizes that are combined by the partial parameters of the large model with the
partial parameters of the small model. Using these reassembled models for FL
training, we can train the partial parameters of the large model using
low-performance devices. In this way, we can alleviate performance degradation
in large models due to resource constraints. The experimental results show that
AdapterFL can achieve up to 12\% accuracy improvement compared to the
state-of-the-art heterogeneous federated learning methods in
resource-constrained scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14056">DPSUR: Accelerating Differentially Private Stochastic Gradient Descent Using Selective Update and Release. (arXiv:2311.14056v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1">Jie Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1">Qingqing Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1">Haibo Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhili Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lulu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kuncan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xun_R/0/1/0/all/0/1">Ran Xun</a></p>
<p>Machine learning models are known to memorize private data to reduce their
training loss, which can be inadvertently exploited by privacy attacks such as
model inversion and membership inference. To protect against these attacks,
differential privacy (DP) has become the de facto standard for
privacy-preserving machine learning, particularly those popular training
algorithms using stochastic gradient descent, such as DPSGD. Nonetheless, DPSGD
still suffers from severe utility loss due to its slow convergence. This is
partially caused by the random sampling, which brings bias and variance to the
gradient, and partially by the Gaussian noise, which leads to fluctuation of
gradient updates.
</p>
<p>Our key idea to address these issues is to apply selective updates to the
model training, while discarding those useless or even harmful updates.
Motivated by this, this paper proposes DPSUR, a Differentially Private training
framework based on Selective Updates and Release, where the gradient from each
iteration is evaluated based on a validation test, and only those updates
leading to convergence are applied to the model. As such, DPSUR ensures the
training in the right direction and thus can achieve faster convergence than
DPSGD. The main challenges lie in two aspects -- privacy concerns arising from
gradient evaluation, and gradient selection strategy for model update. To
address the challenges, DPSUR introduces a clipping strategy for update
randomization and a threshold mechanism for gradient selection. Experiments
conducted on MNIST, FMNIST, CIFAR-10, and IMDB datasets show that DPSUR
significantly outperforms previous works in terms of convergence speed and
model utility.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14063">Do VSR Models Generalize Beyond LRS3?. (arXiv:2311.14063v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Djilali_Y/0/1/0/all/0/1">Yasser Abdelaziz Dahou Djilali</a>, <a href="http://arxiv.org/find/cs/1/au:+Narayan_S/0/1/0/all/0/1">Sanath Narayan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bihan_E/0/1/0/all/0/1">Eustache Le Bihan</a>, <a href="http://arxiv.org/find/cs/1/au:+Boussaid_H/0/1/0/all/0/1">Haithem Boussaid</a>, <a href="http://arxiv.org/find/cs/1/au:+Almazrouei_E/0/1/0/all/0/1">Ebtessam Almazrouei</a>, <a href="http://arxiv.org/find/cs/1/au:+Debbah_M/0/1/0/all/0/1">Merouane Debbah</a></p>
<p>The Lip Reading Sentences-3 (LRS3) benchmark has primarily been the focus of
intense research in visual speech recognition (VSR) during the last few years.
As a result, there is an increased risk of overfitting to its excessively used
test set, which is only one hour duration. To alleviate this issue, we build a
new VSR test set named WildVSR, by closely following the LRS3 dataset creation
processes. We then evaluate and analyse the extent to which the current VSR
models generalize to the new test data. We evaluate a broad range of publicly
available VSR models and find significant drops in performance on our test set,
compared to their corresponding LRS3 results. Our results suggest that the
increase in word error rates is caused by the models inability to generalize to
slightly harder and in the wild lip sequences than those found in the LRS3 test
set. Our new test benchmark is made public in order to enable future research
towards more robust VSR models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14077">RetroDiff: Retrosynthesis as Multi-stage Distribution Interpolation. (arXiv:2311.14077v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yiming Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Yuxuan Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1">Minkai Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Rui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Hao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1">Weiying Ma</a></p>
<p>Retrosynthesis poses a fundamental challenge in biopharmaceuticals, aiming to
aid chemists in finding appropriate reactant molecules and synthetic pathways
given determined product molecules. With the reactant and product represented
as 2D graphs, retrosynthesis constitutes a conditional graph-to-graph
generative task. Inspired by the recent advancements in discrete diffusion
models for graph generation, we introduce Retrosynthesis Diffusion (RetroDiff),
a novel diffusion-based method designed to address this problem. However,
integrating a diffusion-based graph-to-graph framework while retaining
essential chemical reaction template information presents a notable challenge.
Our key innovation is to develop a multi-stage diffusion process. In this
method, we decompose the retrosynthesis procedure to first sample external
groups from the dummy distribution given products and then generate the
external bonds to connect the products and generated groups. Interestingly,
such a generation process is exactly the reverse of the widely adapted
semi-template retrosynthesis procedure, i.e. from reaction center
identification to synthon completion, which significantly reduces the error
accumulation. Experimental results on the benchmark have demonstrated the
superiority of our method over all other semi-template methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14078">Machine learning-based decentralized TDMA for VLC IoT networks. (arXiv:2311.14078v1 [cs.NI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Makvandi_A/0/1/0/all/0/1">Armin Makvandi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kavian_Y/0/1/0/all/0/1">Yousef Seifi Kavian</a></p>
<p>In this paper, a machine learning-based decentralized time division multiple
access (TDMA) algorithm for visible light communication (VLC) Internet of
Things (IoT) networks is proposed. The proposed algorithm is based on
Q-learning, a reinforcement learning algorithm. This paper considers a
decentralized condition in which there is no coordinator node for sending
synchronization frames and assigning transmission time slots to other nodes.
The proposed algorithm uses a decentralized manner for synchronization, and
each node uses the Q-learning algorithm to find the optimal transmission time
slot for sending data without collisions. The proposed algorithm is implemented
on a VLC hardware system, which had been designed and implemented in our
laboratory. Average reward, convergence time, goodput, average delay, and data
packet size are evaluated parameters. The results show that the proposed
algorithm converges quickly and provides collision-free decentralized TDMA for
the network. The proposed algorithm is compared with carrier-sense multiple
access with collision avoidance (CSMA/CA) algorithm as a potential selection
for decentralized VLC IoT networks. The results show that the proposed
algorithm provides up to 61% more goodput and up to 49% less average delay than
CSMA/CA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14079">Empirical Comparison between Cross-Validation and Mutation-Validation in Model Selection. (arXiv:2311.14079v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Jinyang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hamdan_S/0/1/0/all/0/1">Sami Hamdan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sasse_L/0/1/0/all/0/1">Leonard Sasse</a>, <a href="http://arxiv.org/find/cs/1/au:+Morrison_A/0/1/0/all/0/1">Abigail Morrison</a>, <a href="http://arxiv.org/find/cs/1/au:+Patil_K/0/1/0/all/0/1">Kaustubh R. Patil</a></p>
<p>Mutation validation (MV) is a recently proposed approach for model selection,
garnering significant interest due to its unique characteristics and potential
benefits compared to the widely used cross-validation (CV) method. In this
study, we empirically compared MV and $k$-fold CV using benchmark and
real-world datasets. By employing Bayesian tests, we compared generalization
estimates yielding three posterior probabilities: practical equivalence, CV
superiority, and MV superiority. We also evaluated the differences in the
capacity of the selected models and computational efficiency. We found that
both MV and CV select models with practically equivalent generalization
performance across various machine learning algorithms and the majority of
benchmark datasets. MV exhibited advantages in terms of selecting simpler
models and lower computational costs. However, in some cases MV selected overly
simplistic models leading to underfitting and showed instability in
hyperparameter selection. These limitations of MV became more evident in the
evaluation of a real-world neuroscientific task of predicting sex at birth
using brain functional connectivity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14086">Brain MRI Screening Tool with Federated Learning. (arXiv:2311.14086v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Stoklasa_R/0/1/0/all/0/1">Roman Stoklasa</a>, <a href="http://arxiv.org/find/eess/1/au:+Stathopoulos_I/0/1/0/all/0/1">Ioannis Stathopoulos</a>, <a href="http://arxiv.org/find/eess/1/au:+Karavasilis_E/0/1/0/all/0/1">Efstratios Karavasilis</a>, <a href="http://arxiv.org/find/eess/1/au:+Efstathopoulos_E/0/1/0/all/0/1">Efstathios Efstathopoulos</a>, <a href="http://arxiv.org/find/eess/1/au:+Dostal_M/0/1/0/all/0/1">Marek Dost&#xe1;l</a>, <a href="http://arxiv.org/find/eess/1/au:+Kerkovsky_M/0/1/0/all/0/1">Milo&#x161; Ke&#x159;kovsk&#xfd;</a>, <a href="http://arxiv.org/find/eess/1/au:+Kozubek_M/0/1/0/all/0/1">Michal Kozubek</a>, <a href="http://arxiv.org/find/eess/1/au:+Serio_L/0/1/0/all/0/1">Luigi Serio</a></p>
<p>In clinical practice, we often see significant delays between MRI scans and
the diagnosis made by radiologists, even for severe cases. In some cases, this
may be caused by the lack of additional information and clues, so even the
severe cases need to wait in the queue for diagnosis. This can be avoided if
there is an automatic software tool, which would supplement additional
information, alerting radiologists that the particular patient may be a severe
case.
</p>
<p>We are presenting an automatic brain MRI Screening Tool and we are
demonstrating its capabilities for detecting tumor-like pathologies. It is the
first version on the path toward a robust multi-pathology screening solution.
The tool supports Federated Learning, so multiple institutions may contribute
to the model without disclosing their private data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14090">Class Uncertainty: A Measure to Mitigate Class Imbalance. (arXiv:2311.14090v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Baltaci_Z/0/1/0/all/0/1">Z. S. Baltaci</a>, <a href="http://arxiv.org/find/cs/1/au:+Oksuz_K/0/1/0/all/0/1">K. Oksuz</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuzucu_S/0/1/0/all/0/1">S. Kuzucu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tezoren_K/0/1/0/all/0/1">K. Tezoren</a>, <a href="http://arxiv.org/find/cs/1/au:+Konar_B/0/1/0/all/0/1">B. K. Konar</a>, <a href="http://arxiv.org/find/cs/1/au:+Ozkan_A/0/1/0/all/0/1">A. Ozkan</a>, <a href="http://arxiv.org/find/cs/1/au:+Akbas_E/0/1/0/all/0/1">E. Akbas</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalkan_S/0/1/0/all/0/1">S. Kalkan</a></p>
<p>Class-wise characteristics of training examples affect the performance of
deep classifiers. A well-studied example is when the number of training
examples of classes follows a long-tailed distribution, a situation that is
likely to yield sub-optimal performance for under-represented classes. This
class imbalance problem is conventionally addressed by approaches relying on
the class-wise cardinality of training examples, such as data resampling. In
this paper, we demonstrate that considering solely the cardinality of classes
does not cover all issues causing class imbalance. To measure class imbalance,
we propose "Class Uncertainty" as the average predictive uncertainty of the
training examples, and we show that this novel measure captures the differences
across classes better than cardinality. We also curate SVCI-20 as a novel
dataset in which the classes have equal number of training examples but they
differ in terms of their hardness; thereby causing a type of class imbalance
which cannot be addressed by the approaches relying on cardinality. We
incorporate our "Class Uncertainty" measure into a diverse set of ten class
imbalance mitigation methods to demonstrate its effectiveness on long-tailed
datasets as well as on our SVCI-20. Code and datasets will be made available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14094">Robust Decision Aggregation with Second-order Information. (arXiv:2311.14094v1 [cs.GT])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1">Yuqi Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhaohua Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Kong_Y/0/1/0/all/0/1">Yuqing Kong</a></p>
<p>We consider a decision aggregation problem with two experts who each make a
binary recommendation after observing a private signal about an unknown binary
world state. An agent, who does not know the joint information structure
between signals and states, sees the experts' recommendations and aims to match
the action with the true state. Under the scenario, we study whether
supplemented additionally with second-order information (each expert's forecast
on the other's recommendation) could enable a better aggregation.
</p>
<p>We adopt a minimax regret framework to evaluate the aggregator's performance,
by comparing it to an omniscient benchmark that knows the joint information
structure. With general information structures, we show that second-order
information provides no benefit. No aggregator can improve over a trivial
aggregator, which always follows the first expert's recommendation. However,
positive results emerge when we assume experts' signals are conditionally
independent given the world state. When the aggregator is deterministic, we
present a robust aggregator that leverages second-order information, which can
significantly outperform counterparts without it. Second, when two experts are
homogeneous, by adding a non-degenerate assumption on the signals, we
demonstrate that random aggregators using second-order information can surpass
optimal ones without it. In the remaining settings, the second-order
information is not beneficial. We also extend the above results to the setting
when the aggregator's utility function is more general.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14101">Subnetwork Ensembles. (arXiv:2311.14101v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Whitaker_T/0/1/0/all/0/1">Tim Whitaker</a></p>
<p>Neural network ensembles have been effectively used to improve generalization
by combining the predictions of multiple independently trained models. However,
the growing scale and complexity of deep neural networks have led to these
methods becoming prohibitively expensive and time consuming to implement.
Low-cost ensemble methods have become increasingly important as they can
alleviate the need to train multiple models from scratch while retaining the
generalization benefits that traditional ensemble learning methods afford. This
dissertation introduces and formalizes a low-cost framework for constructing
Subnetwork Ensembles, where a collection of child networks are formed by
sampling, perturbing, and optimizing subnetworks from a trained parent model.
We explore several distinct methodologies for generating child networks and we
evaluate their efficacy through a variety of ablation studies and established
benchmarks. Our findings reveal that this approach can greatly improve training
efficiency, parametric utilization, and generalization performance while
minimizing computational cost. Subnetwork Ensembles offer a compelling
framework for exploring how we can build better systems by leveraging the
unrealized potential of deep neural networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14108">MINTY: Rule-based Models that Minimize the Need for Imputing Features with Missing Values. (arXiv:2311.14108v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stempfle_L/0/1/0/all/0/1">Lena Stempfle</a>, <a href="http://arxiv.org/find/cs/1/au:+Johansson_F/0/1/0/all/0/1">Fredrik D. Johansson</a></p>
<p>Rule models are often preferred in prediction tasks with tabular inputs as
they can be easily interpreted using natural language and provide predictive
performance on par with more complex models. However, most rule models'
predictions are undefined or ambiguous when some inputs are missing, forcing
users to rely on statistical imputation models or heuristics like zero
imputation, undermining the interpretability of the models. In this work, we
propose fitting concise yet precise rule models that learn to avoid relying on
features with missing values and, therefore, limit their reliance on imputation
at test time. We develop MINTY, a method that learns rules in the form of
disjunctions between variables that act as replacements for each other when one
or more is missing. This results in a sparse linear rule model, regularized to
have small dependence on features with missing values, that allows a trade-off
between goodness of fit, interpretability, and robustness to missing values at
test time. We demonstrate the value of MINTY in experiments using synthetic and
real-world data sets and find its predictive performance comparable or
favorable to baselines, with smaller reliance on features with missing values.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14110">When is Off-Policy Evaluation Useful? A Data-Centric Perspective. (arXiv:2311.14110v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Hao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1">Alex J. Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Seedat_N/0/1/0/all/0/1">Nabeel Seedat</a>, <a href="http://arxiv.org/find/cs/1/au:+Huyuk_A/0/1/0/all/0/1">Alihan H&#xfc;y&#xfc;k</a>, <a href="http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1">Mihaela van der Schaar</a></p>
<p>Evaluating the value of a hypothetical target policy with only a logged
dataset is important but challenging. On the one hand, it brings opportunities
for safe policy improvement under high-stakes scenarios like clinical
guidelines. On the other hand, such opportunities raise a need for precise
off-policy evaluation (OPE). While previous work on OPE focused on improving
the algorithm in value estimation, in this work, we emphasize the importance of
the offline dataset, hence putting forward a data-centric framework for
evaluating OPE problems. We propose DataCOPE, a data-centric framework for
evaluating OPE, that answers the questions of whether and to what extent we can
evaluate a target policy given a dataset. DataCOPE (1) forecasts the overall
performance of OPE algorithms without access to the environment, which is
especially useful before real-world deployment where evaluating OPE is
impossible; (2) identifies the sub-group in the dataset where OPE can be
inaccurate; (3) permits evaluations of datasets or data-collection strategies
for OPE problems. Our empirical analysis of DataCOPE in the logged contextual
bandit settings using healthcare datasets confirms its ability to evaluate both
machine-learning and human expert policies like clinical guidelines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14114">SySMOL: A Hardware-software Co-design Framework for Ultra-Low and Fine-Grained Mixed-Precision Neural Networks. (arXiv:2311.14114v1 [cs.AR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1">Cyrus Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Richard_V/0/1/0/all/0/1">Vaughn Richard</a>, <a href="http://arxiv.org/find/cs/1/au:+Savarese_P/0/1/0/all/0/1">Pedro Savarese</a>, <a href="http://arxiv.org/find/cs/1/au:+Hassman_Z/0/1/0/all/0/1">Zachary Hassman</a>, <a href="http://arxiv.org/find/cs/1/au:+Maire_M/0/1/0/all/0/1">Michael Maire</a>, <a href="http://arxiv.org/find/cs/1/au:+DiBrino_M/0/1/0/all/0/1">Michael DiBrino</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yanjing Li</a></p>
<p>Recent advancements in quantization and mixed-precision techniques offer
significant promise for improving the run-time and energy efficiency of neural
networks. In this work, we further showed that neural networks, wherein
individual parameters or activations can take on different precisions ranging
between 1 and 4 bits, can achieve accuracies comparable to or exceeding the
full-precision counterparts. However, the deployment of such networks poses
numerous challenges, stemming from the necessity to manage and control the
compute/communication/storage requirements associated with these extremely
fine-grained mixed precisions for each piece of data. There is a lack of
existing efficient hardware and system-level support tailored to these unique
and challenging requirements. Our research introduces the first novel holistic
hardware-software co-design approach for these networks, which enables a
continuous feedback loop between hardware design, training, and inference to
facilitate systematic design exploration. As a proof-of-concept, we illustrate
this co-design approach by designing new, configurable CPU SIMD architectures
tailored for these networks, tightly integrating the architecture with new
system-aware training and inference techniques. We perform systematic design
space exploration using this framework to analyze various tradeoffs. The design
for mixed-precision networks that achieves optimized tradeoffs corresponds to
an architecture that supports 1, 2, and 4-bit fixed-point operations with four
configurable precision patterns, when coupled with system-aware training and
inference optimization -- networks trained for this design achieve accuracies
that closely match full-precision accuracies, while compressing and improving
run-time efficiency of the neural networks drastically by 10-20x, compared to
full-precision networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14115">A density estimation perspective on learning from pairwise human preferences. (arXiv:2311.14115v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dumoulin_V/0/1/0/all/0/1">Vincent Dumoulin</a>, <a href="http://arxiv.org/find/cs/1/au:+Johnson_D/0/1/0/all/0/1">Daniel D. Johnson</a>, <a href="http://arxiv.org/find/cs/1/au:+Castro_P/0/1/0/all/0/1">Pablo Samuel Castro</a>, <a href="http://arxiv.org/find/cs/1/au:+Larochelle_H/0/1/0/all/0/1">Hugo Larochelle</a>, <a href="http://arxiv.org/find/cs/1/au:+Dauphin_Y/0/1/0/all/0/1">Yann Dauphin</a></p>
<p>Learning from human feedback (LHF) -- and in particular learning from
pairwise preferences -- has recently become a crucial ingredient in training
large language models (LLMs), and has been the subject of much research. Most
recent works frame it as a reinforcement learning problem, where a reward
function is learned from pairwise preference data and the LLM is treated as a
policy which is adapted to maximize the rewards, often under additional
regularization constraints. We propose an alternative interpretation which
centers on the generative process for pairwise preferences and treats LHF as a
density estimation problem. We provide theoretical and empirical results
showing that for a family of generative processes defined via preference
behavior distribution equations, training a reward function on pairwise
preferences effectively models an annotator's implicit preference distribution.
Finally, we discuss and present findings on "annotator misspecification" --
failure cases where wrong modeling assumptions are made about annotator
behavior, resulting in poorly-adapted models -- suggesting that approaches that
learn from pairwise human preferences could have trouble learning from a
population of annotators with diverse viewpoints.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14120">Weight fluctuations in (deep) linear neural networks and a derivation of the inverse-variance flatness relation. (arXiv:2311.14120v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gross_M/0/1/0/all/0/1">Markus Gross</a>, <a href="http://arxiv.org/find/cs/1/au:+Raulf_A/0/1/0/all/0/1">Arne P. Raulf</a>, <a href="http://arxiv.org/find/cs/1/au:+Rath_C/0/1/0/all/0/1">Christoph R&#xe4;th</a></p>
<p>We investigate the stationary (late-time) training regime of single- and
two-layer linear neural networks within the continuum limit of stochastic
gradient descent (SGD) for synthetic Gaussian data. In the case of a
single-layer network in the weakly oversampled regime, the spectrum of the
noise covariance matrix deviates notably from the Hessian, which can be
attributed to the broken detailed balance of SGD dynamics. The weight
fluctuations are in this case generally anisotropic, but experience an
isotropic loss. For a two-layer network, we obtain the stochastic dynamics of
the weights in each layer and analyze the associated stationary covariances. We
identify the inter-layer coupling as a new source of anisotropy for the weight
fluctuations. In contrast to the single-layer case, the weight fluctuations
experience an anisotropic loss, the flatness of which is inversely related to
the fluctuation variance. We thereby provide an analytical derivation of the
recently observed inverse variance-flatness relation in a deep linear network
model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14125">Scalable AI Safety via Doubly-Efficient Debate. (arXiv:2311.14125v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Brown_Cohen_J/0/1/0/all/0/1">Jonah Brown-Cohen</a>, <a href="http://arxiv.org/find/cs/1/au:+Irving_G/0/1/0/all/0/1">Geoffrey Irving</a>, <a href="http://arxiv.org/find/cs/1/au:+Piliouras_G/0/1/0/all/0/1">Georgios Piliouras</a></p>
<p>The emergence of pre-trained AI systems with powerful capabilities across a
diverse and ever-increasing set of complex domains has raised a critical
challenge for AI safety as tasks can become too complicated for humans to judge
directly. Irving et al. [2018] proposed a debate method in this direction with
the goal of pitting the power of such AI models against each other until the
problem of identifying (mis)-alignment is broken down into a manageable
subtask. While the promise of this approach is clear, the original framework
was based on the assumption that the honest strategy is able to simulate
deterministic AI systems for an exponential number of steps, limiting its
applicability. In this paper, we show how to address these challenges by
designing a new set of debate protocols where the honest strategy can always
succeed using a simulation of a polynomial number of steps, whilst being able
to verify the alignment of stochastic AI systems, even when the dishonest
strategy is allowed to use exponentially many simulation steps.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14126">Towards Auditing Large Language Models: Improving Text-based Stereotype Detection. (arXiv:2311.14126v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zekun_W/0/1/0/all/0/1">Wu Zekun</a>, <a href="http://arxiv.org/find/cs/1/au:+Bulathwela_S/0/1/0/all/0/1">Sahan Bulathwela</a>, <a href="http://arxiv.org/find/cs/1/au:+Koshiyama_A/0/1/0/all/0/1">Adriano Soares Koshiyama</a></p>
<p>Large Language Models (LLM) have made significant advances in the recent past
becoming more mainstream in Artificial Intelligence (AI) enabled human-facing
applications. However, LLMs often generate stereotypical output inherited from
historical data, amplifying societal biases and raising ethical concerns. This
work introduces i) the Multi-Grain Stereotype Dataset, which includes 52,751
instances of gender, race, profession and religion stereotypic text and ii) a
novel stereotype classifier for English text. We design several experiments to
rigorously test the proposed model trained on the novel dataset. Our
experiments show that training the model in a multi-class setting can
outperform the one-vs-all binary counterpart. Consistent feature importance
signals from different eXplainable AI tools demonstrate that the new model
exploits relevant text features. We utilise the newly created model to assess
the stereotypic behaviour of the popular GPT family of models and observe the
reduction of bias over time. In summary, our work establishes a robust and
practical framework for auditing and evaluating the stereotypic bias in LLM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14127">Byzantine Robustness and Partial Participation Can Be Achieved Simultaneously: Just Clip Gradient Differences. (arXiv:2311.14127v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Malinovsky_G/0/1/0/all/0/1">Grigory Malinovsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Richtarik_P/0/1/0/all/0/1">Peter Richt&#xe1;rik</a>, <a href="http://arxiv.org/find/cs/1/au:+Horvath_S/0/1/0/all/0/1">Samuel Horv&#xe1;th</a>, <a href="http://arxiv.org/find/cs/1/au:+Gorbunov_E/0/1/0/all/0/1">Eduard Gorbunov</a></p>
<p>Distributed learning has emerged as a leading paradigm for training large
machine learning models. However, in real-world scenarios, participants may be
unreliable or malicious, posing a significant challenge to the integrity and
accuracy of the trained models. Byzantine fault tolerance mechanisms have been
proposed to address these issues, but they often assume full participation from
all clients, which is not always practical due to the unavailability of some
clients or communication constraints. In our work, we propose the first
distributed method with client sampling and provable tolerance to Byzantine
workers. The key idea behind the developed method is the use of gradient
clipping to control stochastic gradient differences in recursive variance
reduction. This allows us to bound the potential harm caused by Byzantine
workers, even during iterations when all sampled clients are Byzantine.
Furthermore, we incorporate communication compression into the method to
enhance communication efficiency. Under quite general assumptions, we prove
convergence rates for the proposed method that match the existing
state-of-the-art (SOTA) theoretical results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14131">Exactly conservative physics-informed neural networks and deep operator networks for dynamical systems. (arXiv:2311.14131v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cardoso_Bihlo_E/0/1/0/all/0/1">Elsa Cardoso-Bihlo</a>, <a href="http://arxiv.org/find/cs/1/au:+Bihlo_A/0/1/0/all/0/1">Alex Bihlo</a></p>
<p>We introduce a method for training exactly conservative physics-informed
neural networks and physics-informed deep operator networks for dynamical
systems. The method employs a projection-based technique that maps a candidate
solution learned by the neural network solver for any given dynamical system
possessing at least one first integral onto an invariant manifold. We
illustrate that exactly conservative physics-informed neural network solvers
and physics-informed deep operator networks for dynamical systems vastly
outperform their non-conservative counterparts for several real-world problems
from the mathematical sciences.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14136">A Blockchain Solution for Collaborative Machine Learning over IoT. (arXiv:2311.14136v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Beis_Penedo_C/0/1/0/all/0/1">Carlos Beis-Penedo</a>, <a href="http://arxiv.org/find/cs/1/au:+Troncoso_Pastoriza_F/0/1/0/all/0/1">Francisco Troncoso-Pastoriza</a>, <a href="http://arxiv.org/find/cs/1/au:+Diaz_Redondo_R/0/1/0/all/0/1">Rebeca P. D&#xed;az-Redondo</a>, <a href="http://arxiv.org/find/cs/1/au:+Fernandez_Vilas_A/0/1/0/all/0/1">Ana Fern&#xe1;ndez-Vilas</a>, <a href="http://arxiv.org/find/cs/1/au:+Fernandez_Veiga_M/0/1/0/all/0/1">Manuel Fern&#xe1;ndez-Veiga</a>, <a href="http://arxiv.org/find/cs/1/au:+Soto_M/0/1/0/all/0/1">Mart&#xed;n Gonz&#xe1;lez Soto</a></p>
<p>The rapid growth of Internet of Things (IoT) devices and applications has led
to an increased demand for advanced analytics and machine learning techniques
capable of handling the challenges associated with data privacy, security, and
scalability. Federated learning (FL) and blockchain technologies have emerged
as promising approaches to address these challenges by enabling decentralized,
secure, and privacy-preserving model training on distributed data sources. In
this paper, we present a novel IoT solution that combines the incremental
learning vector quantization algorithm (XuILVQ) with Ethereum blockchain
technology to facilitate secure and efficient data sharing, model training, and
prototype storage in a distributed environment. Our proposed architecture
addresses the shortcomings of existing blockchain-based FL solutions by
reducing computational and communication overheads while maintaining data
privacy and security. We assess the performance of our system through a series
of experiments, showcasing its potential to enhance the accuracy and efficiency
of machine learning tasks in IoT settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14137">Privacy-Preserving Algorithmic Recourse. (arXiv:2311.14137v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pentyala_S/0/1/0/all/0/1">Sikha Pentyala</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1">Shubham Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Kariyappa_S/0/1/0/all/0/1">Sanjay Kariyappa</a>, <a href="http://arxiv.org/find/cs/1/au:+Lecue_F/0/1/0/all/0/1">Freddy Lecue</a>, <a href="http://arxiv.org/find/cs/1/au:+Magazzeni_D/0/1/0/all/0/1">Daniele Magazzeni</a></p>
<p>When individuals are subject to adverse outcomes from machine learning
models, providing a recourse path to help achieve a positive outcome is
desirable. Recent work has shown that counterfactual explanations - which can
be used as a means of single-step recourse - are vulnerable to privacy issues,
putting an individuals' privacy at risk. Providing a sequential multi-step path
for recourse can amplify this risk. Furthermore, simply adding noise to
recourse paths found from existing methods can impact the realism and
actionability of the path for an end-user. In this work, we address privacy
issues when generating realistic recourse paths based on instance-based
counterfactual explanations, and provide PrivRecourse: an end-to-end privacy
preserving pipeline that can provide realistic recourse paths. PrivRecourse
uses differentially private (DP) clustering to represent non-overlapping
subsets of the private dataset. These DP cluster centers are then used to
generate recourse paths by forming a graph with cluster centers as the nodes,
so that we can generate realistic - feasible and actionable - recourse paths.
We empirically evaluate our approach on finance datasets and compare it to
simply adding noise to data instances, and to using DP synthetic data, to
generate the graph. We observe that PrivRecourse can provide paths that are
private and realistic.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14139">Machine Learning For An Explainable Cost Prediction of Medical Insurance. (arXiv:2311.14139v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Orji_U/0/1/0/all/0/1">Ugochukwu Orji</a>, <a href="http://arxiv.org/find/cs/1/au:+Ukwandu_E/0/1/0/all/0/1">Elochukwu Ukwandu</a></p>
<p>Predictive modeling in healthcare continues to be an active actuarial
research topic as more insurance companies aim to maximize the potential of
Machine Learning approaches to increase their productivity and efficiency. In
this paper, the authors deployed three regression-based ensemble ML models that
combine variations of decision trees through Extreme Gradient Boosting,
Gradient-boosting Machine, and Random Forest) methods in predicting medical
insurance costs. Explainable Artificial Intelligence methods SHapley Additive
exPlanations and Individual Conditional Expectation plots were deployed to
discover and explain the key determinant factors that influence medical
insurance premium prices in the dataset. The dataset used comprised 986 records
and is publicly available in the KAGGLE repository. The models were evaluated
using four performance evaluation metrics, including R-squared, Mean Absolute
Error, Root Mean Squared Error, and Mean Absolute Percentage Error. The results
show that all models produced impressive outcomes; however, the XGBoost model
achieved a better overall performance although it also expanded more
computational resources, while the RF model recorded a lesser prediction error
and consumed far fewer computing resources than the XGBoost model. Furthermore,
we compared the outcome of both XAi methods in identifying the key determinant
features that influenced the PremiumPrices for each model and whereas both XAi
methods produced similar outcomes, we found that the ICE plots showed in more
detail the interactions between each variable than the SHAP analysis which
seemed to be more high-level. It is the aim of the authors that the
contributions of this study will help policymakers, insurers, and potential
medical insurance buyers in their decision-making process for selecting the
right policies that meet their specific needs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14148">Automated 3D Tumor Segmentation using Temporal Cubic PatchGAN (TCuP-GAN). (arXiv:2311.14148v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Mantha_K/0/1/0/all/0/1">Kameswara Bharadwaj Mantha</a>, <a href="http://arxiv.org/find/eess/1/au:+Sankar_R/0/1/0/all/0/1">Ramanakumar Sankar</a>, <a href="http://arxiv.org/find/eess/1/au:+Fortson_L/0/1/0/all/0/1">Lucy Fortson</a></p>
<p>Development of robust general purpose 3D segmentation frameworks using the
latest deep learning techniques is one of the active topics in various
bio-medical domains. In this work, we introduce Temporal Cubic PatchGAN
(TCuP-GAN), a volume-to-volume translational model that marries the concepts of
a generative feature learning framework with Convolutional Long Short-Term
Memory Networks (LSTMs), for the task of 3D segmentation. We demonstrate the
capabilities of our TCuP-GAN on the data from four segmentation challenges
(Adult Glioma, Meningioma, Pediatric Tumors, and Sub-Saharan Africa subset)
featured within the 2023 Brain Tumor Segmentation (BraTS) Challenge and
quantify its performance using LesionWise Dice similarity and $95\%$ Hausdorff
Distance metrics. We demonstrate the successful learning of our framework to
predict robust multi-class segmentation masks across all the challenges. This
benchmarking work serves as a stepping stone for future efforts towards
applying TCuP-GAN on other multi-class tasks such as multi-organelle
segmentation in electron microscopy imaging.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14153">Tube-NeRF: Efficient Imitation Learning of Visuomotor Policies from MPC using Tube-Guided Data Augmentation and NeRFs. (arXiv:2311.14153v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tagliabue_A/0/1/0/all/0/1">Andrea Tagliabue</a>, <a href="http://arxiv.org/find/cs/1/au:+How_J/0/1/0/all/0/1">Jonathan P. How</a></p>
<p>Imitation learning (IL) can train computationally-efficient sensorimotor
policies from a resource-intensive Model Predictive Controller (MPC), but it
often requires many samples, leading to long training times or limited
robustness. To address these issues, we combine IL with a variant of robust MPC
that accounts for process and sensing uncertainties, and we design a data
augmentation (DA) strategy that enables efficient learning of vision-based
policies. The proposed DA method, named Tube-NeRF, leverages Neural Radiance
Fields (NeRFs) to generate novel synthetic images, and uses properties of the
robust MPC (the tube) to select relevant views and to efficiently compute the
corresponding actions. We tailor our approach to the task of localization and
trajectory tracking on a multirotor, by learning a visuomotor policy that
generates control actions using images from the onboard camera as only source
of horizontal position. Our evaluations numerically demonstrate learning of a
robust visuomotor policy with an 80-fold increase in demonstration efficiency
and a 50% reduction in training time over current IL methods. Additionally, our
policies successfully transfer to a real multirotor, achieving accurate
localization and low tracking errors despite large disturbances, with an
onboard inference time of only 1.5 ms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14156">Variational Annealing on Graphs for Combinatorial Optimization. (arXiv:2311.14156v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sanokowski_S/0/1/0/all/0/1">Sebastian Sanokowski</a>, <a href="http://arxiv.org/find/cs/1/au:+Berghammer_W/0/1/0/all/0/1">Wilhelm Berghammer</a>, <a href="http://arxiv.org/find/cs/1/au:+Hochreiter_S/0/1/0/all/0/1">Sepp Hochreiter</a>, <a href="http://arxiv.org/find/cs/1/au:+Lehner_S/0/1/0/all/0/1">Sebastian Lehner</a></p>
<p>Several recent unsupervised learning methods use probabilistic approaches to
solve combinatorial optimization (CO) problems based on the assumption of
statistically independent solution variables. We demonstrate that this
assumption imposes performance limitations in particular on difficult problem
instances. Our results corroborate that an autoregressive approach which
captures statistical dependencies among solution variables yields superior
performance on many popular CO problems. We introduce subgraph tokenization in
which the configuration of a set of solution variables is represented by a
single token. This tokenization technique alleviates the drawback of the long
sequential sampling procedure which is inherent to autoregressive methods
without sacrificing expressivity. Importantly, we theoretically motivate an
annealed entropy regularization and show empirically that it is essential for
efficient and stable learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14160">Efficient and Robust Jet Tagging at the LHC with Knowledge Distillation. (arXiv:2311.14160v1 [hep-ex])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/hep-ex/1/au:+Liu_R/0/1/0/all/0/1">Ryan Liu</a>, <a href="http://arxiv.org/find/hep-ex/1/au:+Gandrakota_A/0/1/0/all/0/1">Abhijith Gandrakota</a>, <a href="http://arxiv.org/find/hep-ex/1/au:+Ngadiuba_J/0/1/0/all/0/1">Jennifer Ngadiuba</a>, <a href="http://arxiv.org/find/hep-ex/1/au:+Spiropulu_M/0/1/0/all/0/1">Maria Spiropulu</a>, <a href="http://arxiv.org/find/hep-ex/1/au:+Vlimant_J/0/1/0/all/0/1">Jean-Roch Vlimant</a></p>
<p>The challenging environment of real-time data processing systems at the Large
Hadron Collider (LHC) strictly limits the computational complexity of
algorithms that can be deployed. For deep learning models, this implies that
only models with low computational complexity that have weak inductive bias are
feasible. To address this issue, we utilize knowledge distillation to leverage
both the performance of large models and the reduced computational complexity
of small ones. In this paper, we present an implementation of knowledge
distillation, demonstrating an overall boost in the student models' performance
for the task of classifying jets at the LHC. Furthermore, by using a teacher
model with a strong inductive bias of Lorentz symmetry, we show that we can
induce the same inductive bias in the student model which leads to better
robustness against arbitrary Lorentz boost.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14168">Fast Policy Learning for Linear Quadratic Regulator with Entropy Regularization. (arXiv:2311.14168v1 [math.OC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Guo_X/0/1/0/all/0/1">Xin Guo</a>, <a href="http://arxiv.org/find/math/1/au:+Li_X/0/1/0/all/0/1">Xinyu Li</a>, <a href="http://arxiv.org/find/math/1/au:+Xu_R/0/1/0/all/0/1">Renyuan Xu</a></p>
<p>This paper proposes and analyzes two new policy learning methods: regularized
policy gradient (RPG) and iterative policy optimization (IPO), for a class of
discounted linear-quadratic regulator (LQR) problems over an infinite time
horizon with entropy regularization. Assuming access to the exact policy
evaluation, both proposed approaches are proved to converge linearly in finding
optimal policies of the regularized LQR. Moreover, the IPO method can achieve a
super-linear convergence rate once it enters a local region around the optimal
policy. Finally, when the optimal policy from a well-understood environment in
an RL problem is appropriately transferred as the initial policy to an RL
problem with an unknown environment, the IPO method is shown to enable a
super-linear convergence rate if the latter is sufficiently close to the
former. The performances of these proposed algorithms are supported by
numerical examples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14169">Evaluating GPT-4&#x27;s Vision Capabilities on Brazilian University Admission Exams. (arXiv:2311.14169v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pires_R/0/1/0/all/0/1">Ramon Pires</a>, <a href="http://arxiv.org/find/cs/1/au:+Almeida_T/0/1/0/all/0/1">Thales Sales Almeida</a>, <a href="http://arxiv.org/find/cs/1/au:+Abonizio_H/0/1/0/all/0/1">Hugo Abonizio</a>, <a href="http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1">Rodrigo Nogueira</a></p>
<p>Recent advancements in language models have showcased human-comparable
performance in academic entrance exams. However, existing studies often
overlook questions that require the integration of visual comprehension, thus
compromising the full spectrum and complexity inherent in real-world scenarios.
To address this gap, we present a comprehensive framework to evaluate language
models on entrance exams, which incorporates both textual and visual elements.
We evaluate the two most recent editions of Exame Nacional do Ensino M\'edio
(ENEM), the main standardized entrance examination adopted by Brazilian
universities. Our study not only reaffirms the capabilities of GPT-4 as the
state of the art for handling complex multidisciplinary questions, but also
pioneers in offering a realistic assessment of multimodal language models on
Portuguese examinations. One of the highlights is that text captions
transcribing visual content outperform the direct use of images, suggesting
that the vision model has room for improvement. Yet, despite improvements
afforded by images or captions, mathematical questions remain a challenge for
these state-of-the-art models. The code and data used on experiments are
available at https://github.com/piresramon/gpt-4-enem.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14177">TCuPGAN: A novel framework developed for optimizing human-machine interactions in citizen science. (arXiv:2311.14177v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sankar_R/0/1/0/all/0/1">Ramanakumar Sankar</a>, <a href="http://arxiv.org/find/cs/1/au:+Mantha_K/0/1/0/all/0/1">Kameswara Mantha</a>, <a href="http://arxiv.org/find/cs/1/au:+Fortson_L/0/1/0/all/0/1">Lucy Fortson</a>, <a href="http://arxiv.org/find/cs/1/au:+Spiers_H/0/1/0/all/0/1">Helen Spiers</a>, <a href="http://arxiv.org/find/cs/1/au:+Pengo_T/0/1/0/all/0/1">Thomas Pengo</a>, <a href="http://arxiv.org/find/cs/1/au:+Mashek_D/0/1/0/all/0/1">Douglas Mashek</a>, <a href="http://arxiv.org/find/cs/1/au:+Mo_M/0/1/0/all/0/1">Myat Mo</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanders_M/0/1/0/all/0/1">Mark Sanders</a>, <a href="http://arxiv.org/find/cs/1/au:+Christensen_T/0/1/0/all/0/1">Trace Christensen</a>, <a href="http://arxiv.org/find/cs/1/au:+Salisbury_J/0/1/0/all/0/1">Jeffrey Salisbury</a>, <a href="http://arxiv.org/find/cs/1/au:+Trouille_L/0/1/0/all/0/1">Laura Trouille</a></p>
<p>In the era of big data in scientific research, there is a necessity to
leverage techniques which reduce human effort in labeling and categorizing
large datasets by involving sophisticated machine tools. To combat this
problem, we present a novel, general purpose model for 3D segmentation that
leverages patch-wise adversariality and Long Short-Term Memory to encode
sequential information. Using this model alongside citizen science projects
which use 3D datasets (image cubes) on the Zooniverse platforms, we propose an
iterative human-machine optimization framework where only a fraction of the 2D
slices from these cubes are seen by the volunteers. We leverage the patch-wise
discriminator in our model to provide an estimate of which slices within these
image cubes have poorly generalized feature representations, and
correspondingly poor machine performance. These images with corresponding
machine proposals would be presented to volunteers on Zooniverse for
correction, leading to a drastic reduction in the volunteer effort on citizen
science projects. We trained our model on ~2300 liver tissue 3D electron
micrographs. Lipid droplets were segmented within these images through human
annotation via the `Etch A Cell - Fat Checker' citizen science project, hosted
on the Zooniverse platform. In this work, we demonstrate this framework and the
selection methodology which resulted in a measured reduction in volunteer
effort by more than 60%. We envision this type of joint human-machine
partnership will be of great use on future Zooniverse projects.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14182">Gradient-based bilevel optimization for multi-penalty Ridge regression through matrix differential calculus. (arXiv:2311.14182v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maroni_G/0/1/0/all/0/1">Gabriele Maroni</a>, <a href="http://arxiv.org/find/cs/1/au:+Cannelli_L/0/1/0/all/0/1">Loris Cannelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Piga_D/0/1/0/all/0/1">Dario Piga</a></p>
<p>Common regularization algorithms for linear regression, such as LASSO and
Ridge regression, rely on a regularization hyperparameter that balances the
tradeoff between minimizing the fitting error and the norm of the learned model
coefficients. As this hyperparameter is scalar, it can be easily selected via
random or grid search optimizing a cross-validation criterion. However, using a
scalar hyperparameter limits the algorithm's flexibility and potential for
better generalization. In this paper, we address the problem of linear
regression with l2-regularization, where a different regularization
hyperparameter is associated with each input variable. We optimize these
hyperparameters using a gradient-based approach, wherein the gradient of a
cross-validation criterion with respect to the regularization hyperparameters
is computed analytically through matrix differential calculus. Additionally, we
introduce two strategies tailored for sparse model learning problems aiming at
reducing the risk of overfitting to the validation data. Numerical examples
demonstrate that our multi-hyperparameter regularization approach outperforms
LASSO, Ridge, and Elastic Net regression. Moreover, the analytical computation
of the gradient proves to be more efficient in terms of computational time
compared to automatic differentiation, especially when handling a large number
of input variables. Application to the identification of over-parameterized
Linear Parameter-Varying models is also presented.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/1908.09094">Optimal $\delta$-Correct Best-Arm Selection for Heavy-Tailed Distributions. (arXiv:1908.09094v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1">Shubhada Agrawal</a>, <a href="http://arxiv.org/find/cs/1/au:+Juneja_S/0/1/0/all/0/1">Sandeep Juneja</a>, <a href="http://arxiv.org/find/cs/1/au:+Glynn_P/0/1/0/all/0/1">Peter Glynn</a></p>
<p>Given a finite set of unknown distributions or arms that can be sampled, we
consider the problem of identifying the one with the maximum mean using a
$\delta$-correct algorithm (an adaptive, sequential algorithm that restricts
the probability of error to a specified $\delta$) that has minimum sample
complexity. Lower bounds for $\delta$-correct algorithms are well known.
$\delta$-correct algorithms that match the lower bound asymptotically as
$\delta$ reduces to zero have been previously developed when arm distributions
are restricted to a single parameter exponential family. In this paper, we
first observe a negative result that some restrictions are essential, as
otherwise, under a $\delta$-correct algorithm, distributions with unbounded
support would require an infinite number of samples in expectation. We then
propose a $\delta$-correct algorithm that matches the lower bound as $\delta$
reduces to zero under the mild restriction that a known bound on the
expectation of $(1+\epsilon)^{th}$ moment of the underlying random variables
exists, for $\epsilon &gt; 0$. We also propose batch processing and identify
near-optimal batch sizes to speed up the proposed algorithm substantially. The
best-arm problem has many learning applications, including recommendation
systems and product selection. It is also a well-studied classic problem in the
simulation community.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2102.09030">Proactive DP: A Multple Target Optimization Framework for DP-SGD. (arXiv:2102.09030v9 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dijk_M/0/1/0/all/0/1">Marten van Dijk</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1">Nhuong V. Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Toan N. Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1">Lam M. Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1">Phuong Ha Nguyen</a></p>
<p>We introduce a multiple target optimization framework for DP-SGD referred to
as pro-active DP. In contrast to traditional DP accountants, which are used to
track the expenditure of privacy budgets, the pro-active DP scheme allows one
to {\it a-priori} select parameters of DP-SGD based on a fixed privacy budget
(in terms of $\epsilon$ and $\delta$) in such a way to optimize the anticipated
utility (test accuracy) the most. To achieve this objective, we first propose
significant improvements to the moment account method, presenting a closed-form
$(\epsilon,\delta)$-DP guarantee that connects all parameters in the DP-SGD
setup. Generally, DP-SGD is $(\epsilon\leq 1/2,\delta=1/N)$-DP if
$\sigma=\sqrt{2(\epsilon +\ln(1/\delta))/\epsilon}$ with $T$ at least $\approx
2k^2/\epsilon$ and $(2/e)^2k^2-1/2\geq \ln(N)$, where $T$ is the total number
of rounds, and $K=kN$ is the total number of gradient computations where $k$
measures $K$ in number of epochs of size $N$ of the local data set. We prove
that our expression is close to tight in that if $T$ is more than a constant
factor $\approx 4$ smaller than the lower bound $\approx 2k^2/\epsilon$, then
the $(\epsilon,\delta)$-DP guarantee is violated. Our enhanced DP theory allows
us to create a utility graph and DP calculator. These tools link privacy and
utility objectives and search for optimal experiment setups, efficiently taking
into account both accuracy and privacy objectives, as well as implementation
goals. We furnish a comprehensive implementation flow of our proactive DP, with
rigorous experiments to showcase the proof-of-concept.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2103.01208">Mind the box: $l_1$-APGD for sparse adversarial attacks on image classifiers. (arXiv:2103.01208v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Croce_F/0/1/0/all/0/1">Francesco Croce</a>, <a href="http://arxiv.org/find/cs/1/au:+Hein_M/0/1/0/all/0/1">Matthias Hein</a></p>
<p>We show that when taking into account also the image domain $[0,1]^d$,
established $l_1$-projected gradient descent (PGD) attacks are suboptimal as
they do not consider that the effective threat model is the intersection of the
$l_1$-ball and $[0,1]^d$. We study the expected sparsity of the steepest
descent step for this effective threat model and show that the exact projection
onto this set is computationally feasible and yields better performance.
Moreover, we propose an adaptive form of PGD which is highly effective even
with a small budget of iterations. Our resulting $l_1$-APGD is a strong
white-box attack showing that prior works overestimated their $l_1$-robustness.
Using $l_1$-APGD for adversarial training we get a robust classifier with SOTA
$l_1$-robustness. Finally, we combine $l_1$-APGD and an adaptation of the
Square Attack to $l_1$ into $l_1$-AutoAttack, an ensemble of attacks which
reliably assesses adversarial robustness for the threat model of $l_1$-ball
intersected with $[0,1]^d$.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2103.13192">On Preference Learning Based on Sequential Bayesian Optimization with Pairwise Comparison. (arXiv:2103.13192v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ignatenko_T/0/1/0/all/0/1">Tanya Ignatenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Kondrashov_K/0/1/0/all/0/1">Kirill Kondrashov</a>, <a href="http://arxiv.org/find/cs/1/au:+Cox_M/0/1/0/all/0/1">Marco Cox</a>, <a href="http://arxiv.org/find/cs/1/au:+Vries_B/0/1/0/all/0/1">Bert de Vries</a></p>
<p>User preference learning is generally a hard problem. Individual preferences
are typically unknown even to users themselves, while the space of choices is
infinite. Here we study user preference learning from information-theoretic
perspective. We model preference learning as a system with two interacting
sub-systems, one representing a user with his/her preferences and another one
representing an agent that has to learn these preferences. The user with
his/her behaviour is modeled by a parametric preference function. To
efficiently learn the preferences and reduce search space quickly, we propose
the agent that interacts with the user to collect the most informative data for
learning. The agent presents two proposals to the user for evaluation, and the
user rates them based on his/her preference function. We show that the optimum
agent strategy for data collection and preference learning is a result of
maximin optimization of the normalized weighted Kullback-Leibler (KL)
divergence between true and agent-assigned predictive user response
distributions. The resulting value of KL-divergence, which we also call
remaining system uncertainty (RSU), provides an efficient performance metric in
the absence of the ground truth. This metric characterises how well the agent
can predict user and, thus, the quality of the underlying learned user
(preference) model. Our proposed agent comprises sequential mechanisms for user
model inference and proposal generation. To infer the user model (preference
function), Bayesian approximate inference is used in the agent. The data
collection strategy is to generate proposals, responses to which help resolving
uncertainty associated with prediction of the user responses the most. The
efficiency of our approach is validated by numerical simulations. Also a
real-life example of preference learning application is provided.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2109.00542">Shared Certificates for Neural Network Verification. (arXiv:2109.00542v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fischer_M/0/1/0/all/0/1">Marc Fischer</a>, <a href="http://arxiv.org/find/cs/1/au:+Sprecher_C/0/1/0/all/0/1">Christian Sprecher</a>, <a href="http://arxiv.org/find/cs/1/au:+Dimitrov_D/0/1/0/all/0/1">Dimitar I. Dimitrov</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1">Gagandeep Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Vechev_M/0/1/0/all/0/1">Martin Vechev</a></p>
<p>Existing neural network verifiers compute a proof that each input is handled
correctly under a given perturbation by propagating a symbolic abstraction of
reachable values at each layer. This process is repeated from scratch
independently for each input (e.g., image) and perturbation (e.g., rotation),
leading to an expensive overall proof effort when handling an entire dataset.
In this work, we introduce a new method for reducing this verification cost
without losing precision based on a key insight that abstractions obtained at
intermediate layers for different inputs and perturbations can overlap or
contain each other. Leveraging our insight, we introduce the general concept of
shared certificates, enabling proof effort reuse across multiple inputs to
reduce overall verification costs. We perform an extensive experimental
evaluation to demonstrate the effectiveness of shared certificates in reducing
the verification cost on a range of datasets and attack specifications on image
classifiers including the popular patch and geometric perturbations. We release
our implementation at https://github.com/eth-sri/proof-sharing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2110.02271">Networked Time Series Prediction with Incomplete Data via Generative Adversarial Network. (arXiv:2110.02271v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yichen Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1">Bo Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1">Haiming Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Mengtian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1">Feng Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jianqiang Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1">Tao Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinbing Wang</a></p>
<p>A networked time series (NETS) is a family of time series on a given graph,
one for each node. It has a wide range of applications from intelligent
transportation, environment monitoring to smart grid management. An important
task in such applications is to predict the future values of a NETS based on
its historical values and the underlying graph. Most existing methods require
complete data for training. However, in real-world scenarios, it is not
uncommon to have missing data due to sensor malfunction, incomplete sensing
coverage, etc. In this paper, we study the problem of NETS prediction with
incomplete data. We propose NETS-ImpGAN, a novel deep learning framework that
can be trained on incomplete data with missing values in both history and
future. Furthermore, we propose Graph Temporal Attention Networks, which
incorporate the attention mechanism to capture both inter-time series and
temporal correlations. We conduct extensive experiments on four real-world
datasets under different missing patterns and missing rates. The experimental
results show that NETS-ImpGAN outperforms existing methods, reducing the MAE by
up to 25%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.07628">Training Multi-Layer Over-Parametrized Neural Network in Subquadratic Time. (arXiv:2112.07628v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1">Zhao Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lichen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Ruizhe Zhang</a></p>
<p>We consider the problem of training a multi-layer over-parametrized neural
network to minimize the empirical risk induced by a loss function. In the
typical setting of over-parametrization, the network width $m$ is much larger
than the data dimension $d$ and the number of training samples $n$
($m=\mathrm{poly}(n,d)$), which induces a prohibitive large weight matrix $W\in
\mathbb{R}^{m\times m}$ per layer. Naively, one has to pay $O(m^2)$ time to
read the weight matrix and evaluate the neural network function in both forward
and backward computation. In this work, we show how to reduce the training cost
per iteration. Specifically, we propose a framework that uses $m^2$ cost only
in the initialization phase and achieves \emph{a truly subquadratic cost per
iteration} in terms of $m$, i.e., $m^{2-\Omega(1)}$ per iteration. Our result
has implications beyond standard over-parametrization theory, as it can be
viewed as designing an efficient data structure on top of a pre-trained large
model to further speed up the fine-tuning process, a core procedure to deploy
large language models (LLM).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2201.00292">Fair Data Representation for Machine Learning at the Pareto Frontier. (arXiv:2201.00292v4 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Xu_S/0/1/0/all/0/1">Shizhou Xu</a>, <a href="http://arxiv.org/find/stat/1/au:+Strohmer_T/0/1/0/all/0/1">Thomas Strohmer</a></p>
<p>As machine learning powered decision-making becomes increasingly important in
our daily lives, it is imperative to strive for fairness in the underlying data
processing. We propose a pre-processing algorithm for fair data representation
via which supervised learning results in estimations of the Pareto frontier
between prediction error and statistical disparity. Particularly, the present
work applies the optimal affine transport to approach the post-processing
Wasserstein-2 barycenter characterization of the optimal fair $L^2$-objective
supervised learning via a pre-processing data deformation. Furthermore, we show
that the Wasserstein-2 geodesics from the conditional (on sensitive
information) distributions of the learning outcome to their barycenter
characterizes the Pareto frontier between $L^2$-loss and the average pairwise
Wasserstein-2 distance among sensitive groups on the learning outcome.
Numerical simulations underscore the advantages: (1) the pre-processing step is
compositive with arbitrary conditional expectation estimation supervised
learning methods and unseen data; (2) the fair representation protects the
sensitive information by limiting the inference capability of the remaining
data with respect to the sensitive data; (3) the optimal affine maps are
computationally efficient even for high-dimensional data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.11954">XAutoML: A Visual Analytics Tool for Understanding and Validating Automated Machine Learning. (arXiv:2202.11954v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zoller_M/0/1/0/all/0/1">Marc-Andr&#xe9; Z&#xf6;ller</a>, <a href="http://arxiv.org/find/cs/1/au:+Titov_W/0/1/0/all/0/1">Waldemar Titov</a>, <a href="http://arxiv.org/find/cs/1/au:+Schlegel_T/0/1/0/all/0/1">Thomas Schlegel</a>, <a href="http://arxiv.org/find/cs/1/au:+Huber_M/0/1/0/all/0/1">Marco F. Huber</a></p>
<p>In the last ten years, various automated machine learning (AutoM ) systems
have been proposed to build end-to-end machine learning (ML) pipelines with
minimal human interaction. Even though such automatically synthesized ML
pipelines are able to achieve a competitive performance, recent studies have
shown that users do not trust models constructed by AutoML due to missing
transparency of AutoML systems and missing explanations for the constructed ML
pipelines. In a requirements analysis study with 36 domain experts, data
scientists, and AutoML researchers from different professions with vastly
different expertise in ML, we collect detailed informational needs for AutoML.
We propose XAutoML, an interactive visual analytics tool for explaining
arbitrary AutoML optimization procedures and ML pipelines constructed by
AutoML. XAutoML combines interactive visualizations with established techniques
from explainable artificial intelligence (XAI) to make the complete AutoML
procedure transparent and explainable. By integrating XAutoML with JupyterLab,
experienced users can extend the visual analytics with ad-hoc visualizations
based on information extracted from XAutoML. We validate our approach in a user
study with the same diverse user group from the requirements analysis. All
participants were able to extract useful information from XAutoML, leading to a
significantly increased understanding of ML pipelines produced by AutoML and
the AutoML optimization itself.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.02205">Evaluating Object (mis)Detection from a Safety and Reliability Perspective: Discussion and Measures. (arXiv:2203.02205v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ceccarelli_A/0/1/0/all/0/1">Andrea Ceccarelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Montecchi_L/0/1/0/all/0/1">Leonardo Montecchi</a></p>
<p>We argue that object detectors in the safety critical domain should
prioritize detection of objects that are most likely to interfere with the
actions of the autonomous actor. Especially, this applies to objects that can
impact the actor's safety and reliability. To quantify the impact of object
(mis)detection on safety and reliability in the context of autonomous driving,
we propose new object detection measures that reward the correct identification
of objects that are most dangerous and most likely to affect driving decisions.
To achieve this, we build an object criticality model to reward the detection
of the objects based on proximity, orientation, and relative velocity with
respect to the subject vehicle. Then, we apply our model on the recent
autonomous driving dataset nuScenes, and we compare nine object detectors.
Results show that, in several settings, object detectors that perform best
according to the nuScenes ranking are not the preferable ones when the focus is
shifted on safety and reliability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2204.07485">How to Use K-means for Big Data Clustering?. (arXiv:2204.07485v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mussabayev_R/0/1/0/all/0/1">Rustam Mussabayev</a>, <a href="http://arxiv.org/find/cs/1/au:+Mladenovic_N/0/1/0/all/0/1">Nenad Mladenovic</a>, <a href="http://arxiv.org/find/cs/1/au:+Jarboui_B/0/1/0/all/0/1">Bassem Jarboui</a>, <a href="http://arxiv.org/find/cs/1/au:+Mussabayev_R/0/1/0/all/0/1">Ravil Mussabayev</a></p>
<p>K-means plays a vital role in data mining and is the simplest and most widely
used algorithm under the Euclidean Minimum Sum-of-Squares Clustering (MSSC)
model. However, its performance drastically drops when applied to vast amounts
of data. Therefore, it is crucial to improve K-means by scaling it to big data
using as few of the following computational resources as possible: data, time,
and algorithmic ingredients. We propose a new parallel scheme of using K-means
and K-means++ algorithms for big data clustering that satisfies the properties
of a ``true big data'' algorithm and outperforms the classical and recent
state-of-the-art MSSC approaches in terms of solution quality and runtime. The
new approach naturally implements global search by decomposing the MSSC problem
without using additional metaheuristics. This work shows that data
decomposition is the basic approach to solve the big data clustering problem.
The empirical success of the new algorithm allowed us to challenge the common
belief that more data is required to obtain a good clustering solution.
Moreover, the present work questions the established trend that more
sophisticated hybrid approaches and algorithms are required to obtain a better
clustering solution.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2204.10581">Fused Audio Instance and Representation for Respiratory Disease Detection. (arXiv:2204.10581v4 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1">Tuan Truong</a>, <a href="http://arxiv.org/find/cs/1/au:+Lenga_M/0/1/0/all/0/1">Matthias Lenga</a>, <a href="http://arxiv.org/find/cs/1/au:+Serrurier_A/0/1/0/all/0/1">Antoine Serrurier</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohammadi_S/0/1/0/all/0/1">Sadegh Mohammadi</a></p>
<p>Audio-based classification techniques on body sounds have long been studied
to aid in the diagnosis of respiratory diseases. While most research is
centered on the use of cough as the main biomarker, other body sounds also have
the potential to detect respiratory diseases. Recent studies on COVID-19 have
shown that breath and speech sounds, in addition to cough, correlate with the
disease. Our study proposes Fused Audio Instance and Representation (FAIR) as a
method for respiratory disease detection. FAIR relies on constructing a joint
feature vector from various body sounds represented in waveform and spectrogram
form. We conducted experiments on the use case of COVID-19 detection by
combining waveform and spectrogram representation of body sounds. Our findings
show that the use of self-attention to combine extracted features from cough,
breath, and speech sounds leads to the best performance with an Area Under the
Receiver Operating Characteristic Curve (AUC) score of 0.8658, a sensitivity of
0.8057, and a specificity of 0.7958. Compared to models trained solely on
spectrograms or waveforms, the use of both representations results in an
improved AUC score, demonstrating that combining spectrogram and waveform
representation helps to enrich the extracted features and outperforms the
models that use only one representation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.11952">3D helical CT Reconstruction with a Memory Efficient Learned Primal-Dual Architecture. (arXiv:2205.11952v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Rudzusika_J/0/1/0/all/0/1">Jevgenija Rudzusika</a>, <a href="http://arxiv.org/find/eess/1/au:+Bajic_B/0/1/0/all/0/1">Buda Baji&#x107;</a>, <a href="http://arxiv.org/find/eess/1/au:+Koehler_T/0/1/0/all/0/1">Thomas Koehler</a>, <a href="http://arxiv.org/find/eess/1/au:+Oktem_O/0/1/0/all/0/1">Ozan &#xd6;ktem</a></p>
<p>Deep learning based computed tomography (CT) reconstruction has demonstrated
outstanding performance on simulated 2D low-dose CT data. This applies in
particular to domain adapted neural networks, which incorporate a handcrafted
physics model for CT imaging. Empirical evidence shows that employing such
architectures reduces the demand for training data and improves upon
generalisation. However, their training requires large computational resources
that quickly become prohibitive in 3D helical CT, which is the most common
acquisition geometry used for medical imaging. Furthermore, clinical data also
comes with other challenges not accounted for in simulations, like errors in
flux measurement, resolution mismatch and, most importantly, the absence of the
real ground truth. The necessity to have a computationally feasible training
combined with the need to address these issues has made it difficult to
evaluate deep learning based reconstruction on clinical 3D helical CT. This
paper modifies a domain adapted neural network architecture, the Learned
Primal-Dual (LPD), so that it can be trained and applied to reconstruction in
this setting. We achieve this by splitting the helical trajectory into sections
and applying the unrolled LPD iterations to those sections sequentially. To the
best of our knowledge, this work is the first to apply an unrolled deep
learning architecture for reconstruction on full-sized clinical data, like
those in the Low dose CT image and projection data set (LDCT). Moreover,
training and testing is done on a single GPU card with 24GB of memory.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.14415">Non-stationary Transformers: Exploring the Stationarity in Time Series Forecasting. (arXiv:2205.14415v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Haixu Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jianmin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1">Mingsheng Long</a></p>
<p>Transformers have shown great power in time series forecasting due to their
global-range modeling ability. However, their performance can degenerate
terribly on non-stationary real-world data in which the joint distribution
changes over time. Previous studies primarily adopt stationarization to
attenuate the non-stationarity of original series for better predictability.
But the stationarized series deprived of inherent non-stationarity can be less
instructive for real-world bursty events forecasting. This problem, termed
over-stationarization in this paper, leads Transformers to generate
indistinguishable temporal attentions for different series and impedes the
predictive capability of deep models. To tackle the dilemma between series
predictability and model capability, we propose Non-stationary Transformers as
a generic framework with two interdependent modules: Series Stationarization
and De-stationary Attention. Concretely, Series Stationarization unifies the
statistics of each input and converts the output with restored statistics for
better predictability. To address the over-stationarization problem,
De-stationary Attention is devised to recover the intrinsic non-stationary
information into temporal dependencies by approximating distinguishable
attentions learned from raw series. Our Non-stationary Transformers framework
consistently boosts mainstream Transformers by a large margin, which reduces
MSE by 49.43% on Transformer, 47.34% on Informer, and 46.89% on Reformer,
making them the state-of-the-art in time series forecasting. Code is available
at this repository: https://github.com/thuml/Nonstationary_Transformers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.11161">Sharing pattern submodels for prediction with missing values. (arXiv:2206.11161v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stempfle_L/0/1/0/all/0/1">Lena Stempfle</a>, <a href="http://arxiv.org/find/cs/1/au:+Panahi_A/0/1/0/all/0/1">Ashkan Panahi</a>, <a href="http://arxiv.org/find/cs/1/au:+Johansson_F/0/1/0/all/0/1">Fredrik D. Johansson</a></p>
<p>Missing values are unavoidable in many applications of machine learning and
present challenges both during training and at test time. When variables are
missing in recurring patterns, fitting separate pattern submodels have been
proposed as a solution. However, fitting models independently does not make
efficient use of all available data. Conversely, fitting a single shared model
to the full data set relies on imputation which often leads to biased results
when missingness depends on unobserved factors. We propose an alternative
approach, called sharing pattern submodels, which i) makes predictions that are
robust to missing values at test time, ii) maintains or improves the predictive
power of pattern submodels, and iii) has a short description, enabling improved
interpretability. Parameter sharing is enforced through sparsity-inducing
regularization which we prove leads to consistent estimation. Finally, we give
conditions for when a sharing model is optimal, even when both missingness and
the target outcome depend on unobserved variables. Classification and
regression experiments on synthetic and real-world data sets demonstrate that
our models achieve a favorable tradeoff between pattern specialization and
information sharing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.03048">Benchmarking Multimodal Variational Autoencoders: CdSprites+ Dataset and Toolkit. (arXiv:2209.03048v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sejnova_G/0/1/0/all/0/1">Gabriela Sejnova</a>, <a href="http://arxiv.org/find/cs/1/au:+Vavrecka_M/0/1/0/all/0/1">Michal Vavrecka</a>, <a href="http://arxiv.org/find/cs/1/au:+Stepanova_K/0/1/0/all/0/1">Karla Stepanova</a></p>
<p>Multimodal Variational Autoencoders (VAEs) have been the subject of intense
research in the past years as they can integrate multiple modalities into a
joint representation and can thus serve as a promising tool for both data
classification and generation. Several approaches toward multimodal VAE
learning have been proposed so far, their comparison and evaluation have
however been rather inconsistent. One reason is that the models differ at the
implementation level, another problem is that the datasets commonly used in
these cases were not initially designed to evaluate multimodal generative
models. This paper addresses both mentioned issues. First, we propose a toolkit
for systematic multimodal VAE training and comparison. The toolkit currently
comprises 4 existing multimodal VAEs and 6 commonly used benchmark datasets
along with instructions on how to easily add a new model or a dataset. Second,
we present a disentangled bimodal dataset designed to comprehensively evaluate
the joint generation and cross-generation capabilities across multiple
difficulty levels. We demonstrate the utility of our dataset by comparing the
implemented state-of-the-art models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.05954">Automatically Score Tissue Images Like a Pathologist by Transfer Learning. (arXiv:2209.05954v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yan_I/0/1/0/all/0/1">Iris Yan</a></p>
<p>Cancer is the second leading cause of death in the world. Diagnosing cancer
early on can save many lives. Pathologists have to look at tissue microarray
(TMA) images manually to identify tumors, which can be time-consuming,
inconsistent and subjective. Existing automatic algorithms either have not
achieved the accuracy level of a pathologist or require substantial human
involvements. A major challenge is that TMA images with different shapes,
sizes, and locations can have the same score. Learning staining patterns in TMA
images requires a huge number of images, which are severely limited due to
privacy and regulation concerns in medical organizations. TMA images from
different cancer types may share certain common characteristics, but combining
them directly harms the accuracy due to heterogeneity in their staining
patterns. Transfer learning is an emerging learning paradigm that allows
borrowing strength from similar problems. However, existing approaches
typically require a large sample from similar learning problems, while TMA
images of different cancer types are often available in small sample size and
further existing algorithms are limited to transfer learning from one similar
problem. We propose a new transfer learning algorithm that could learn from
multiple related problems, where each problem has a small sample and can have a
substantially different distribution from the original one. The proposed
algorithm has made it possible to break the critical accuracy barrier (the 75%
accuracy level of pathologists), with a reported accuracy of 75.9% on breast
cancer TMA images from the Stanford Tissue Microarray Database. It is supported
by recent developments in transfer learning theory and empirical evidence in
clustering technology. This will allow pathologists to confidently adopt
automatic algorithms in recognizing tumors consistently with a higher accuracy
in real time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.09154">Physics-Constrained Neural Network for Design and Feature-Based Optimization of Weave Architectures. (arXiv:2209.09154v2 [physics.app-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Feng_H/0/1/0/all/0/1">Haotian Feng</a>, <a href="http://arxiv.org/find/physics/1/au:+Subramaniyan_S/0/1/0/all/0/1">Sabarinathan P Subramaniyan</a>, <a href="http://arxiv.org/find/physics/1/au:+Tewani_H/0/1/0/all/0/1">Hridyesh Tewani</a>, <a href="http://arxiv.org/find/physics/1/au:+Prabhakar_P/0/1/0/all/0/1">Pavana Prabhakar</a></p>
<p>Woven fabrics play an essential role in everyday textiles for
clothing/sportswear, water filtration, and retaining walls, to reinforcements
in stiff composites for lightweight structures like aerospace, sporting,
automotive, and marine industries. Several possible combinations of weave
patterns and material choices, which comprise weave architecture, present a
challenging question about how they could influence the physical and mechanical
properties of woven fabrics and reinforced structures. In this paper, we
present a novel Physics-Constrained Neural Network (PCNN) to predict the
mechanical properties like the modulus of weave architectures and the inverse
problem of predicting pattern/material sequence for a design/target modulus
value. The inverse problem is particularly challenging as it usually requires
many iterations to find the appropriate architecture using traditional
optimization approaches. We show that the proposed PCNN can effectively predict
weave architecture for the desired modulus with higher accuracy than several
baseline models considered. We present a feature-based optimization strategy to
improve the predictions using features in the Grey Level Co-occurrence Matrix
(GLCM) space. We combine PCNN with this feature-based optimization to discover
near-optimal weave architectures to facilitate the initial design of weave
architecture. The proposed frameworks will primarily enable the woven composite
analysis and optimization process, and be a starting point to introduce
Knowledge-guided Neural Networks into the complex structural analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.03488">AlphaFold Distillation for Protein Design. (arXiv:2210.03488v2 [q-bio.BM] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Melnyk_I/0/1/0/all/0/1">Igor Melnyk</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Lozano_A/0/1/0/all/0/1">Aurelie Lozano</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Das_P/0/1/0/all/0/1">Payel Das</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Chenthamarakshan_V/0/1/0/all/0/1">Vijil Chenthamarakshan</a></p>
<p>Inverse protein folding, the process of designing sequences that fold into a
specific 3D structure, is crucial in bio-engineering and drug discovery.
Traditional methods rely on experimentally resolved structures, but these cover
only a small fraction of protein sequences. Forward folding models like
AlphaFold offer a potential solution by accurately predicting structures from
sequences. However, these models are too slow for integration into the
optimization loop of inverse folding models during training. To address this,
we propose using knowledge distillation on folding model confidence metrics,
such as pTM or pLDDT scores, to create a faster and end-to-end differentiable
distilled model. This model can then be used as a structure consistency
regularizer in training the inverse folding model. Our technique is versatile
and can be applied to other design tasks, such as sequence-based protein
infilling. Experimental results show that our method outperforms
non-regularized baselines, yielding up to 3% improvement in sequence recovery
and up to 45% improvement in protein diversity while maintaining structural
consistency in generated sequences. Code is available at
https://github.com/IBM/AFDistill
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.00539">Dungeons and Data: A Large-Scale NetHack Dataset. (arXiv:2211.00539v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hambro_E/0/1/0/all/0/1">Eric Hambro</a>, <a href="http://arxiv.org/find/cs/1/au:+Raileanu_R/0/1/0/all/0/1">Roberta Raileanu</a>, <a href="http://arxiv.org/find/cs/1/au:+Rothermel_D/0/1/0/all/0/1">Danielle Rothermel</a>, <a href="http://arxiv.org/find/cs/1/au:+Mella_V/0/1/0/all/0/1">Vegard Mella</a>, <a href="http://arxiv.org/find/cs/1/au:+Rocktaschel_T/0/1/0/all/0/1">Tim Rockt&#xe4;schel</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuttler_H/0/1/0/all/0/1">Heinrich K&#xfc;ttler</a>, <a href="http://arxiv.org/find/cs/1/au:+Murray_N/0/1/0/all/0/1">Naila Murray</a></p>
<p>Recent breakthroughs in the development of agents to solve challenging
sequential decision making problems such as Go, StarCraft, or DOTA, have relied
on both simulated environments and large-scale datasets. However, progress on
this research has been hindered by the scarcity of open-sourced datasets and
the prohibitive computational cost to work with them. Here we present the
NetHack Learning Dataset (NLD), a large and highly-scalable dataset of
trajectories from the popular game of NetHack, which is both extremely
challenging for current methods and very fast to run. NLD consists of three
parts: 10 billion state transitions from 1.5 million human trajectories
collected on the NAO public NetHack server from 2009 to 2020; 3 billion
state-action-score transitions from 100,000 trajectories collected from the
symbolic bot winner of the NetHack Challenge 2021; and, accompanying code for
users to record, load and stream any collection of such trajectories in a
highly compressed form. We evaluate a wide range of existing algorithms
including online and offline RL, as well as learning from demonstrations,
showing that significant research advances are needed to fully leverage
large-scale datasets for challenging sequential decision making tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.09894">Supervised Feature Compression based on Counterfactual Analysis. (arXiv:2211.09894v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Piccialli_V/0/1/0/all/0/1">Veronica Piccialli</a>, <a href="http://arxiv.org/find/cs/1/au:+Morales_D/0/1/0/all/0/1">Dolores Romero Morales</a>, <a href="http://arxiv.org/find/cs/1/au:+Salvatore_C/0/1/0/all/0/1">Cecilia Salvatore</a></p>
<p>Counterfactual Explanations are becoming a de-facto standard in post-hoc
interpretable machine learning. For a given classifier and an instance
classified in an undesired class, its counterfactual explanation corresponds to
small perturbations of that instance that allows changing the classification
outcome. This work aims to leverage Counterfactual Explanations to detect the
important decision boundaries of a pre-trained black-box model. This
information is used to build a supervised discretization of the features in the
dataset with a tunable granularity. Using the discretized dataset, an optimal
Decision Tree can be trained that resembles the black-box model, but that is
interpretable and compact. Numerical results on real-world datasets show the
effectiveness of the approach in terms of accuracy and sparsity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.11744">Visual Dexterity: In-Hand Reorientation of Novel and Complex Object Shapes. (arXiv:2211.11744v3 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tippur_M/0/1/0/all/0/1">Megha Tippur</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Siyang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1">Vikash Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Adelson_E/0/1/0/all/0/1">Edward Adelson</a>, <a href="http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1">Pulkit Agrawal</a></p>
<p>In-hand object reorientation is necessary for performing many dexterous
manipulation tasks, such as tool use in less structured environments that
remain beyond the reach of current robots. Prior works built reorientation
systems assuming one or many of the following: reorienting only specific
objects with simple shapes, limited range of reorientation, slow or quasistatic
manipulation, simulation-only results, the need for specialized and costly
sensor suites, and other constraints which make the system infeasible for
real-world deployment. We present a general object reorientation controller
that does not make these assumptions. It uses readings from a single commodity
depth camera to dynamically reorient complex and new object shapes by any
rotation in real-time, with the median reorientation time being close to seven
seconds. The controller is trained using reinforcement learning in simulation
and evaluated in the real world on new object shapes not used for training,
including the most challenging scenario of reorienting objects held in the air
by a downward-facing hand that must counteract gravity during reorientation.
Our hardware platform only uses open-source components that cost less than five
thousand dollars. Although we demonstrate the ability to overcome assumptions
in prior work, there is ample scope for improving absolute performance. For
instance, the challenging duck-shaped object not used for training was dropped
in 56 percent of the trials. When it was not dropped, our controller reoriented
the object within 0.4 radians (23 degrees) 75 percent of the time. Videos are
available at: https://taochenshh.github.io/projects/visual-dexterity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.12792">MECCH: Metapath Context Convolution-based Heterogeneous Graph Neural Networks. (arXiv:2211.12792v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1">Xinyu Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1">Irwin King</a></p>
<p>Heterogeneous graph neural networks (HGNNs) were proposed for representation
learning on structural data with multiple types of nodes and edges. To deal
with the performance degradation issue when HGNNs become deep, researchers
combine metapaths into HGNNs to associate nodes closely related in semantics
but far apart in the graph. However, existing metapath-based models suffer from
either information loss or high computation costs. To address these problems,
we present a novel Metapath Context Convolution-based Heterogeneous Graph
Neural Network (MECCH). MECCH leverages metapath contexts, a new kind of graph
structure that facilitates lossless node information aggregation while avoiding
any redundancy. Specifically, MECCH applies three novel components after
feature preprocessing to extract comprehensive information from the input graph
efficiently: (1) metapath context construction, (2) metapath context encoder,
and (3) convolutional metapath fusion. Experiments on five real-world
heterogeneous graph datasets for node classification and link prediction show
that MECCH achieves superior prediction accuracy compared with state-of-the-art
baselines with improved computational efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.11588">Bounding Box-based Multi-objective Bayesian Optimization of Risk Measures under Input Uncertainty. (arXiv:2301.11588v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Inatsu_Y/0/1/0/all/0/1">Yu Inatsu</a>, <a href="http://arxiv.org/find/stat/1/au:+Takeno_S/0/1/0/all/0/1">Shion Takeno</a>, <a href="http://arxiv.org/find/stat/1/au:+Hanada_H/0/1/0/all/0/1">Hiroyuki Hanada</a>, <a href="http://arxiv.org/find/stat/1/au:+Iwata_K/0/1/0/all/0/1">Kazuki Iwata</a>, <a href="http://arxiv.org/find/stat/1/au:+Takeuchi_I/0/1/0/all/0/1">Ichiro Takeuchi</a></p>
<p>In this study, we propose a novel multi-objective Bayesian optimization
(MOBO) method to efficiently identify the Pareto front (PF) defined by risk
measures for black-box functions under the presence of input uncertainty (IU).
Existing BO methods for Pareto optimization in the presence of IU are
risk-specific or without theoretical guarantees, whereas our proposed method
addresses general risk measures and has theoretical guarantees. The basic idea
of the proposed method is to assume a Gaussian process (GP) model for the
black-box function and to construct high-probability bounding boxes for the
risk measures using the GP model. Furthermore, in order to reduce the
uncertainty of non-dominated bounding boxes, we propose a method of selecting
the next evaluation point using a maximin distance defined by the maximum value
of a quasi distance based on bounding boxes. As theoretical analysis, we prove
that the algorithm can return an arbitrary-accurate solution in a finite number
of iterations with high probability, for various risk measures such as Bayes
risk, worst-case risk, and value-at-risk. We also give a theoretical analysis
that takes into account approximation errors because there exist non-negligible
approximation errors (e.g., finite approximation of PFs and sampling-based
approximation of bounding boxes) in practice. We confirm that the proposed
method outperforms compared with existing methods not only in the setting with
IU but also in the setting of ordinary MOBO through numerical experiments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.11873">A Deep Learning Method for Comparing Bayesian Hierarchical Models. (arXiv:2301.11873v4 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Elsemuller_L/0/1/0/all/0/1">Lasse Elsem&#xfc;ller</a>, <a href="http://arxiv.org/find/stat/1/au:+Schnuerch_M/0/1/0/all/0/1">Martin Schnuerch</a>, <a href="http://arxiv.org/find/stat/1/au:+Burkner_P/0/1/0/all/0/1">Paul-Christian B&#xfc;rkner</a>, <a href="http://arxiv.org/find/stat/1/au:+Radev_S/0/1/0/all/0/1">Stefan T. Radev</a></p>
<p>Bayesian model comparison (BMC) offers a principled approach for assessing
the relative merits of competing computational models and propagating
uncertainty into model selection decisions. However, BMC is often intractable
for the popular class of hierarchical models due to their high-dimensional
nested parameter structure. To address this intractability, we propose a deep
learning method for performing BMC on any set of hierarchical models which can
be instantiated as probabilistic programs. Since our method enables amortized
inference, it allows efficient re-estimation of posterior model probabilities
and fast performance validation prior to any real-data application. In a series
of extensive validation studies, we benchmark the performance of our method
against the state-of-the-art bridge sampling method and demonstrate excellent
amortized inference across all BMC settings. We then showcase our method by
comparing four hierarchical evidence accumulation models that have previously
been deemed intractable for BMC due to partly implicit likelihoods.
Additionally, we demonstrate how transfer learning can be leveraged to enhance
training efficiency. We provide reproducible code for all analyses and an
open-source implementation of our method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.13636">Transport with Support: Data-Conditional Diffusion Bridges. (arXiv:2301.13636v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tamir_E/0/1/0/all/0/1">Ella Tamir</a>, <a href="http://arxiv.org/find/cs/1/au:+Trapp_M/0/1/0/all/0/1">Martin Trapp</a>, <a href="http://arxiv.org/find/cs/1/au:+Solin_A/0/1/0/all/0/1">Arno Solin</a></p>
<p>The dynamic Schr\"odinger bridge problem provides an appealing setting for
solving constrained time-series data generation tasks posed as optimal
transport problems. It consists of learning non-linear diffusion processes
using efficient iterative solvers. Recent works have demonstrated
state-of-the-art results (eg. in modelling single-cell embryo RNA sequences or
sampling from complex posteriors) but are limited to learning bridges with only
initial and terminal constraints. Our work extends this paradigm by proposing
the Iterative Smoothing Bridge (ISB). We integrate Bayesian filtering and
optimal control into learning the diffusion process, enabling the generation of
constrained stochastic processes governed by sparse observations at
intermediate stages and terminal constraints. We assess the effectiveness of
our method on synthetic and real-world data generation tasks and we show that
the ISB generalises well to high-dimensional data, is computationally
efficient, and provides accurate estimates of the marginals at intermediate and
terminal times.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.05104">Monte Carlo Neural PDE Solver for Learning PDEs via Probabilistic Representation. (arXiv:2302.05104v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Rui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1">Qi Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1">Rongchan Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yue Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1">Wenlei Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shihua Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1">Zhi-Ming Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tie-Yan Liu</a></p>
<p>In scenarios with limited available or high-quality data, training the
function-to-function neural PDE solver in an unsupervised manner is essential.
However, the efficiency and accuracy of existing methods are constrained by the
properties of numerical algorithms, such as finite difference and
pseudo-spectral methods, integrated during the training stage. These methods
necessitate careful spatiotemporal discretization to achieve reasonable
accuracy, leading to significant computational challenges and inaccurate
simulations, particularly in cases with substantial spatiotemporal variations.
To address these limitations, we propose the Monte Carlo Neural PDE Solver
(MCNP Solver) for training unsupervised neural solvers via the PDEs'
probabilistic representation, which regards macroscopic phenomena as ensembles
of random particles. Compared to other unsupervised methods, MCNP Solver
naturally inherits the advantages of the Monte Carlo method, which is robust
against spatiotemporal variations and can tolerate coarse step size. In
simulating the random walk of particles, we employ Heun's method for the
convection process and calculate the expectation via the probability density
function of neighbouring grid points during the diffusion process. These
techniques enhance accuracy and circumvent the computational memory and time
issues associated with Monte Carlo sampling, offering an improvement over
traditional Monte Carlo methods. Our numerical experiments on
convection-diffusion, Allen-Cahn, and Navier-Stokes equations demonstrate
significant improvements in accuracy and efficiency compared to other
unsupervised baselines. The source code will be publicly available at:
https://github.com/optray/MCNP.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.07400">Score-based Diffusion Models in Function Space. (arXiv:2302.07400v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1">Jae Hyun Lim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kovachki_N/0/1/0/all/0/1">Nikola B. Kovachki</a>, <a href="http://arxiv.org/find/cs/1/au:+Baptista_R/0/1/0/all/0/1">Ricardo Baptista</a>, <a href="http://arxiv.org/find/cs/1/au:+Beckham_C/0/1/0/all/0/1">Christopher Beckham</a>, <a href="http://arxiv.org/find/cs/1/au:+Azizzadenesheli_K/0/1/0/all/0/1">Kamyar Azizzadenesheli</a>, <a href="http://arxiv.org/find/cs/1/au:+Kossaifi_J/0/1/0/all/0/1">Jean Kossaifi</a>, <a href="http://arxiv.org/find/cs/1/au:+Voleti_V/0/1/0/all/0/1">Vikram Voleti</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1">Jiaming Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Kreis_K/0/1/0/all/0/1">Karsten Kreis</a>, <a href="http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1">Jan Kautz</a>, <a href="http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1">Christopher Pal</a>, <a href="http://arxiv.org/find/cs/1/au:+Vahdat_A/0/1/0/all/0/1">Arash Vahdat</a>, <a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1">Anima Anandkumar</a></p>
<p>Diffusion models have recently emerged as a powerful framework for generative
modeling. They consist of a forward process that perturbs input data with
Gaussian white noise and a reverse process that learns a score function to
generate samples by denoising. Despite their tremendous success, they are
mostly formulated on finite-dimensional spaces, e.g. Euclidean, limiting their
applications to many domains where the data has a functional form such as in
scientific computing and 3D geometric data analysis. In this work, we introduce
a mathematically rigorous framework called Denoising Diffusion Operators (DDOs)
for training diffusion models in function space. In DDOs, the forward process
perturbs input functions gradually using a Gaussian process. The generative
process is formulated by integrating a function-valued Langevin dynamic. Our
approach requires an appropriate notion of the score for the perturbed data
distribution, which we obtain by generalizing denoising score matching to
function spaces that can be infinite-dimensional. We show that the
corresponding discretized algorithm generates accurate samples at a fixed cost
that is independent of the data resolution. We theoretically and numerically
verify the applicability of our approach on a set of problems, including
generating solutions to the Navier-Stokes equation viewed as the push-forward
distribution of forcings from a Gaussian Random Field (GRF).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.14460">Interpretable and intervenable ultrasonography-based machine learning models for pediatric appendicitis. (arXiv:2302.14460v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Marcinkevics_R/0/1/0/all/0/1">Ri&#x10d;ards Marcinkevi&#x10d;s</a>, <a href="http://arxiv.org/find/cs/1/au:+Wolfertstetter_P/0/1/0/all/0/1">Patricia Reis Wolfertstetter</a>, <a href="http://arxiv.org/find/cs/1/au:+Klimiene_U/0/1/0/all/0/1">Ugne Klimiene</a>, <a href="http://arxiv.org/find/cs/1/au:+Chin_Cheong_K/0/1/0/all/0/1">Kieran Chin-Cheong</a>, <a href="http://arxiv.org/find/cs/1/au:+Paschke_A/0/1/0/all/0/1">Alyssia Paschke</a>, <a href="http://arxiv.org/find/cs/1/au:+Zerres_J/0/1/0/all/0/1">Julia Zerres</a>, <a href="http://arxiv.org/find/cs/1/au:+Denzinger_M/0/1/0/all/0/1">Markus Denzinger</a>, <a href="http://arxiv.org/find/cs/1/au:+Niederberger_D/0/1/0/all/0/1">David Niederberger</a>, <a href="http://arxiv.org/find/cs/1/au:+Wellmann_S/0/1/0/all/0/1">Sven Wellmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Ozkan_E/0/1/0/all/0/1">Ece Ozkan</a>, <a href="http://arxiv.org/find/cs/1/au:+Knorr_C/0/1/0/all/0/1">Christian Knorr</a>, <a href="http://arxiv.org/find/cs/1/au:+Vogt_J/0/1/0/all/0/1">Julia E. Vogt</a></p>
<p>Appendicitis is among the most frequent reasons for pediatric abdominal
surgeries. Previous decision support systems for appendicitis have focused on
clinical, laboratory, scoring, and computed tomography data and have ignored
abdominal ultrasound, despite its noninvasive nature and widespread
availability. In this work, we present interpretable machine learning models
for predicting the diagnosis, management and severity of suspected appendicitis
using ultrasound images. Our approach utilizes concept bottleneck models (CBM)
that facilitate interpretation and interaction with high-level concepts
understandable to clinicians. Furthermore, we extend CBMs to prediction
problems with multiple views and incomplete concept sets. Our models were
trained on a dataset comprising 579 pediatric patients with 1709 ultrasound
images accompanied by clinical and laboratory data. Results show that our
proposed method enables clinicians to utilize a human-understandable and
intervenable predictive model without compromising performance or requiring
time-consuming image annotation when deployed. For predicting the diagnosis,
the extended multiview CBM attained an AUROC of 0.80 and an AUPR of 0.92,
performing comparably to similar black-box neural networks trained and tested
on the same dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.01538">Feature Perturbation Augmentation for Reliable Evaluation of Importance Estimators in Neural Networks. (arXiv:2303.01538v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Brocki_L/0/1/0/all/0/1">Lennart Brocki</a>, <a href="http://arxiv.org/find/cs/1/au:+Chung_N/0/1/0/all/0/1">Neo Christopher Chung</a></p>
<p>Post-hoc explanation methods attempt to make the inner workings of deep
neural networks more interpretable. However, since a ground truth is in general
lacking, local post-hoc interpretability methods, which assign importance
scores to input features, are challenging to evaluate. One of the most popular
evaluation frameworks is to perturb features deemed important by an
interpretability method and to measure the change in prediction accuracy.
Intuitively, a large decrease in prediction accuracy would indicate that the
explanation has correctly quantified the importance of features with respect to
the prediction outcome (e.g., logits). However, the change in the prediction
outcome may stem from perturbation artifacts, since perturbed samples in the
test dataset are out of distribution (OOD) compared to the training dataset and
can therefore potentially disturb the model in an unexpected manner. To
overcome this challenge, we propose feature perturbation augmentation (FPA)
which creates and adds perturbed images during the model training. Through
extensive computational experiments, we demonstrate that FPA makes deep neural
networks (DNNs) more robust against perturbations. Furthermore, training DNNs
with FPA demonstrate that the sign of importance scores may explain the model
more meaningfully than has previously been assumed. Overall, FPA is an
intuitive data augmentation technique that improves the evaluation of post-hoc
interpretability methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.03982">Structured State Space Models for In-Context Reinforcement Learning. (arXiv:2303.03982v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1">Chris Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Schroecker_Y/0/1/0/all/0/1">Yannick Schroecker</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_A/0/1/0/all/0/1">Albert Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Parisotto_E/0/1/0/all/0/1">Emilio Parisotto</a>, <a href="http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1">Jakob Foerster</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Satinder Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Behbahani_F/0/1/0/all/0/1">Feryal Behbahani</a></p>
<p>Structured state space sequence (S4) models have recently achieved
state-of-the-art performance on long-range sequence modeling tasks. These
models also have fast inference speeds and parallelisable training, making them
potentially useful in many reinforcement learning settings. We propose a
modification to a variant of S4 that enables us to initialise and reset the
hidden state in parallel, allowing us to tackle reinforcement learning tasks.
We show that our modified architecture runs asymptotically faster than
Transformers in sequence length and performs better than RNN's on a simple
memory-based task. We evaluate our modified architecture on a set of
partially-observable environments and find that, in practice, our model
outperforms RNN's while also running over five times faster. Then, by
leveraging the model's ability to handle long-range sequences, we achieve
strong performance on a challenging meta-learning task in which the agent is
given a randomly-sampled continuous control environment, combined with a
randomly-sampled linear projection of the environment's observations and
actions. Furthermore, we show the resulting model can adapt to
out-of-distribution held-out tasks. Overall, the results presented in this
paper show that structured state space models are fast and performant for
in-context reinforcement learning tasks. We provide code at
https://github.com/luchris429/popjaxrl.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.04689">A Privacy Preserving System for Movie Recommendations Using Federated Learning. (arXiv:2303.04689v3 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Neumann_D/0/1/0/all/0/1">David Neumann</a>, <a href="http://arxiv.org/find/cs/1/au:+Lutz_A/0/1/0/all/0/1">Andreas Lutz</a>, <a href="http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1">Karsten M&#xfc;ller</a>, <a href="http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1">Wojciech Samek</a></p>
<p>Recommender systems have become ubiquitous in the past years. They solve the
tyranny of choice problem faced by many users, and are utilized by many online
businesses to drive engagement and sales. Besides other criticisms, like
creating filter bubbles within social networks, recommender systems are often
reproved for collecting considerable amounts of personal data. However, to
personalize recommendations, personal information is fundamentally required. A
recent distributed learning scheme called federated learning has made it
possible to learn from personal user data without its central collection.
Consequently, we present a recommender system for movie recommendations, which
provides privacy and thus trustworthiness on multiple levels: First and
foremost, it is trained using federated learning and thus, by its very nature,
privacy-preserving, while still enabling users to benefit from global insights.
Furthermore, a novel federated learning scheme, called FedQ, is employed, which
not only addresses the problem of non-i.i.d.-ness and small local datasets, but
also prevents input data reconstruction attacks by aggregating client updates
early. Finally, to reduce the communication overhead, compression is applied,
which significantly compresses the exchanged neural network parametrizations to
a fraction of their original size. We conjecture that this may also improve
data privacy through its lossy quantization stage.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.08011">Model scale versus domain knowledge in statistical forecasting of chaotic systems. (arXiv:2303.08011v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gilpin_W/0/1/0/all/0/1">William Gilpin</a></p>
<p>Chaos and unpredictability are traditionally synonymous, yet large-scale
machine learning methods recently have demonstrated a surprising ability to
forecast chaotic systems well beyond typical predictability horizons. However,
recent works disagree on whether specialized methods grounded in dynamical
systems theory, such as reservoir computers or neural ordinary differential
equations, outperform general-purpose large-scale learning methods such as
transformers or recurrent neural networks. These prior studies perform
comparisons on few individually-chosen chaotic systems, thereby precluding
robust quantification of how statistical modeling choices and dynamical
invariants of different chaotic systems jointly determine empirical
predictability. Here, we perform the largest to-date comparative study of
forecasting methods on the classical problem of forecasting chaos: we benchmark
24 state-of-the-art forecasting methods on a crowdsourced database of 135
low-dimensional systems with 17 forecast metrics. We find that large-scale,
domain-agnostic forecasting methods consistently produce predictions that
remain accurate up to two dozen Lyapunov times, thereby accessing a new
long-horizon forecasting regime well beyond classical methods. We find that, in
this regime, accuracy decorrelates with classical invariant measures of
predictability like the Lyapunov exponent. However, in data-limited settings
outside the long-horizon regime, we find that physics-based hybrid methods
retain a comparative advantage due to their strong inductive biases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.14483">Spatio-Temporal Graph Neural Networks for Predictive Learning in Urban Computing: A Survey. (arXiv:2303.14483v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jin_G/0/1/0/all/0/1">Guangyin Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1">Yuxuan Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1">Yuchen Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1">Zezhi Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jincai Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Junbo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1">Yu Zheng</a></p>
<p>With recent advances in sensing technologies, a myriad of spatio-temporal
data has been generated and recorded in smart cities. Forecasting the evolution
patterns of spatio-temporal data is an important yet demanding aspect of urban
computing, which can enhance intelligent management decisions in various
fields, including transportation, environment, climate, public safety,
healthcare, and others. Traditional statistical and deep learning methods
struggle to capture complex correlations in urban spatio-temporal data. To this
end, Spatio-Temporal Graph Neural Networks (STGNN) have been proposed,
achieving great promise in recent years. STGNNs enable the extraction of
complex spatio-temporal dependencies by integrating graph neural networks
(GNNs) and various temporal learning methods. In this manuscript, we provide a
comprehensive survey on recent progress on STGNN technologies for predictive
learning in urban computing. Firstly, we provide a brief introduction to the
construction methods of spatio-temporal graph data and the prevalent
deep-learning architectures used in STGNNs. We then sort out the primary
application domains and specific predictive learning tasks based on existing
literature. Afterward, we scrutinize the design of STGNNs and their combination
with some advanced technologies in recent years. Finally, we conclude the
limitations of existing research and suggest potential directions for future
work.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.16210">Towards Reliable Uncertainty Quantification via Deep Ensembles in Multi-output Regression Task. (arXiv:2303.16210v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Sunwoong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yee_K/0/1/0/all/0/1">Kwanjung Yee</a></p>
<p>This study aims to comprehensively investigate the deep ensemble approach, an
approximate Bayesian inference, in the multi-output regression task for
predicting the aerodynamic performance of a missile configuration. To this end,
the effect of the number of neural networks used in the ensemble, which has
been blindly adopted in previous studies, is scrutinized. As a result, an
obvious trend towards underestimation of uncertainty as it increases is
observed for the first time, and in this context, we propose the deep ensemble
framework that applies the post-hoc calibration method to improve its
uncertainty quantification performance. It is compared with Gaussian process
regression and is shown to have superior performance in terms of regression
accuracy ($\uparrow55\sim56\%$), reliability of estimated uncertainty
($\uparrow38\sim77\%$), and training efficiency ($\uparrow78\%$). Finally, the
potential impact of the suggested framework on the Bayesian optimization is
briefly examined, indicating that deep ensemble without calibration may lead to
unintended exploratory behavior. This UQ framework can be seamlessly applied
and extended to any regression task, as no special assumptions have been made
for the specific problem used in this study.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.00933">Knowledge Accumulation in Continually Learned Representations and the Issue of Feature Forgetting. (arXiv:2304.00933v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hess_T/0/1/0/all/0/1">Timm Hess</a>, <a href="http://arxiv.org/find/cs/1/au:+Verwimp_E/0/1/0/all/0/1">Eli Verwimp</a>, <a href="http://arxiv.org/find/cs/1/au:+Ven_G/0/1/0/all/0/1">Gido M. van de Ven</a>, <a href="http://arxiv.org/find/cs/1/au:+Tuytelaars_T/0/1/0/all/0/1">Tinne Tuytelaars</a></p>
<p>While it is established that neural networks suffer from catastrophic
forgetting ``at the output level'', it is debated whether this is also the case
at the level of representations. Some studies ascribe a certain level of innate
robustness to representations, that they only forget minimally and no critical
information, while others claim that representations are also severely affected
by forgetting. To settle this debate, we first discuss how this apparent
disagreement might stem from the coexistence of two phenomena that affect the
quality of continually learned representations: knowledge accumulation and
feature forgetting. We then show that, even though it is true that feature
forgetting can be small in absolute terms, newly learned information is
forgotten just as catastrophically at the level of representations as it is at
the output level. Next we show that this feature forgetting is problematic as
it substantially slows down knowledge accumulation. We further show that
representations that are continually learned through both supervised and
self-supervised learning suffer from feature forgetting. Finally, we study how
feature forgetting and knowledge accumulation are affected by different types
of continual learning methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.05390">HRS-Bench: Holistic, Reliable and Scalable Benchmark for Text-to-Image Models. (arXiv:2304.05390v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bakr_E/0/1/0/all/0/1">Eslam Mohamed Bakr</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1">Pengzhan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1">Xiaoqian Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1">Faizan Farooq Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Li Erran Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1">Mohamed Elhoseiny</a></p>
<p>In recent years, Text-to-Image (T2I) models have been extensively studied,
especially with the emergence of diffusion models that achieve state-of-the-art
results on T2I synthesis tasks. However, existing benchmarks heavily rely on
subjective human evaluation, limiting their ability to holistically assess the
model's capabilities. Furthermore, there is a significant gap between efforts
in developing new T2I architectures and those in evaluation. To address this,
we introduce HRS-Bench, a concrete evaluation benchmark for T2I models that is
Holistic, Reliable, and Scalable. Unlike existing bench-marks that focus on
limited aspects, HRS-Bench measures 13 skills that can be categorized into five
major categories: accuracy, robustness, generalization, fairness, and bias. In
addition, HRS-Bench covers 50 scenarios, including fashion, animals,
transportation, food, and clothes. We evaluate nine recent large-scale T2I
models using metrics that cover a wide range of skills. A human evaluation
aligned with 95% of our evaluations on average was conducted to probe the
effectiveness of HRS-Bench. Our experiments demonstrate that existing models
often struggle to generate images with the desired count of objects, visual
text, or grounded emotions. We hope that our benchmark help ease future
text-to-image generation research. The code and data are available at
https://eslambakr.github.io/hrsbench.github.io
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.08120">DAS-N2N: Machine learning Distributed Acoustic Sensing (DAS) signal denoising without clean data. (arXiv:2304.08120v2 [physics.geo-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Lapins_S/0/1/0/all/0/1">Sacha Lapins</a>, <a href="http://arxiv.org/find/physics/1/au:+Butcher_A/0/1/0/all/0/1">Antony Butcher</a>, <a href="http://arxiv.org/find/physics/1/au:+Kendall_J/0/1/0/all/0/1">J.-Michael Kendall</a>, <a href="http://arxiv.org/find/physics/1/au:+Hudson_T/0/1/0/all/0/1">Thomas S. Hudson</a>, <a href="http://arxiv.org/find/physics/1/au:+Stork_A/0/1/0/all/0/1">Anna L. Stork</a>, <a href="http://arxiv.org/find/physics/1/au:+Werner_M/0/1/0/all/0/1">Maximilian J. Werner</a>, <a href="http://arxiv.org/find/physics/1/au:+Gunning_J/0/1/0/all/0/1">Jemma Gunning</a>, <a href="http://arxiv.org/find/physics/1/au:+Brisbourne_A/0/1/0/all/0/1">Alex M. Brisbourne</a></p>
<p>This article presents a weakly supervised machine learning method, which we
call DAS-N2N, for suppressing strong random noise in distributed acoustic
sensing (DAS) recordings. DAS-N2N requires no manually produced labels (i.e.,
pre-determined examples of clean event signals or sections of noise) for
training and aims to map random noise processes to a chosen summary statistic,
such as the distribution mean, median or mode, whilst retaining the true
underlying signal. This is achieved by splicing (joining together) two fibres
hosted within a single optical cable, recording two noisy copies of the same
underlying signal corrupted by different independent realizations of random
observational noise. A deep learning model can then be trained using only these
two noisy copies of the data to produce a near fully-denoised copy. Once the
model is trained, only noisy data from a single fibre is required. Using a
dataset from a DAS array deployed on the surface of the Rutford Ice Stream in
Antarctica, we demonstrate that DAS-N2N greatly suppresses incoherent noise and
enhances the signal-to-noise ratios (SNR) of natural microseismic icequake
events. We further show that this approach is inherently more efficient and
effective than standard stop/pass band and white noise (e.g., Wiener) filtering
routines, as well as a comparable self-supervised learning method based on
masking individual DAS channels. Our preferred model for this task is
lightweight, processing 30 seconds of data recorded at a sampling frequency of
1000 Hz over 985 channels (approx. 1 km of fiber) in $&lt;$1 s. Due to the high
noise levels in DAS recordings, efficient data-driven denoising methods, such
as DAS-N2N, will prove essential to time-critical DAS earthquake detection,
particularly in the case of microseismic monitoring.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.00557">Collective Relational Inference for learning heterogeneous interactions. (arXiv:2305.00557v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1">Zhichao Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Fink_O/0/1/0/all/0/1">Olga Fink</a>, <a href="http://arxiv.org/find/cs/1/au:+Kammer_D/0/1/0/all/0/1">David S. Kammer</a></p>
<p>Interacting systems are ubiquitous in nature and engineering, ranging from
particle dynamics in physics to functionally connected brain regions. These
interacting systems can be modeled by graphs where edges correspond to the
interactions between interactive entities. Revealing interaction laws is of
fundamental importance but also particularly challenging due to underlying
configurational complexities. The associated challenges become exacerbated for
heterogeneous systems that are prevalent in reality, where multiple interaction
types coexist simultaneously and relational inference is required. Here, we
propose a novel probabilistic method for relational inference, which possesses
two distinctive characteristics compared to existing methods. First, it infers
the interaction types of different edges collectively, and second, it allows
handling systems with variable topological structure over time. We evaluate the
proposed methodology across several benchmark datasets and demonstrate that it
outperforms existing methods in accurately inferring interaction types. We
further show that when combined with known constraints, it allows us, for
example, to discover physics-consistent interaction laws of particle systems.
Overall the proposed model is data-efficient and generalizable to large systems
when trained on smaller ones. The developed methodology constitutes a key
element for understanding interacting systems and may find application in graph
structure learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16099">FAVANO: Federated AVeraging with Asynchronous NOdes. (arXiv:2305.16099v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Leconte_L/0/1/0/all/0/1">Louis Leconte</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1">Van Minh Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Moulines_E/0/1/0/all/0/1">Eric Moulines</a></p>
<p>In this paper, we propose a novel centralized Asynchronous Federated Learning
(FL) framework, FAVANO, for training Deep Neural Networks (DNNs) in
resource-constrained environments. Despite its popularity, ``classical''
federated learning faces the increasingly difficult task of scaling synchronous
communication over large wireless networks. Moreover, clients typically have
different computing resources and therefore computing speed, which can lead to
a significant bias (in favor of ``fast'' clients) when the updates are
asynchronous. Therefore, practical deployment of FL requires to handle users
with strongly varying computing speed in communication/resource constrained
setting. We provide convergence guarantees for FAVANO in a smooth, non-convex
environment and carefully compare the obtained convergence guarantees with
existing bounds, when they are available. Experimental results show that the
FAVANO algorithm outperforms current methods on standard benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16854">Channel and Gradient-Importance Aware Device Scheduling for Over-the-Air Federated Learning. (arXiv:2305.16854v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yuchang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+lin_Z/0/1/0/all/0/1">Zehong lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1">Yuyi Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1">Shi Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jun Zhang</a></p>
<p>Federated learning (FL) is a popular privacy-preserving distributed training
scheme, where multiple devices collaborate to train machine learning models by
uploading local model updates. To improve communication efficiency,
over-the-air computation (AirComp) has been applied to FL, which leverages
analog modulation to harness the superposition property of radio waves such
that numerous devices can upload their model updates concurrently for
aggregation. However, the uplink channel noise incurs considerable model
aggregation distortion, which is critically determined by the device scheduling
and compromises the learned model performance. In this paper, we propose a
probabilistic device scheduling framework for over-the-air FL, named PO-FL, to
mitigate the negative impact of channel noise, where each device is scheduled
according to a certain probability and its model update is reweighted using
this probability in aggregation. We prove the unbiasedness of this aggregation
scheme and demonstrate the convergence of PO-FL on both convex and non-convex
loss functions. Our convergence bounds unveil that the device scheduling
affects the learning performance through the communication distortion and
global update variance. Based on the convergence analysis, we further develop a
channel and gradient-importance aware algorithm to optimize the device
scheduling probabilities in PO-FL. Extensive simulation results show that the
proposed PO-FL framework with channel and gradient-importance awareness
achieves faster convergence and produces better models than baseline methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.17400">Query-Policy Misalignment in Preference-Based Reinforcement Learning. (arXiv:2305.17400v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xiao Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jianxiong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1">Xianyuan Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_Q/0/1/0/all/0/1">Qing-Shan Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Ya-Qin Zhang</a></p>
<p>Preference-based reinforcement learning (PbRL) provides a natural way to
align RL agents' behavior with human desired outcomes, but is often restrained
by costly human feedback. To improve feedback efficiency, most existing PbRL
methods focus on selecting queries to maximally improve the overall quality of
the reward model, but counter-intuitively, we find that this may not
necessarily lead to improved performance. To unravel this mystery, we identify
a long-neglected issue in the query selection schemes of existing PbRL studies:
Query-Policy Misalignment. We show that the seemingly informative queries
selected to improve the overall quality of reward model actually may not align
with RL agents' interests, thus offering little help on policy learning and
eventually resulting in poor feedback efficiency. We show that this issue can
be effectively addressed via near on-policy query and a specially designed
hybrid experience replay, which together enforce the bidirectional query-policy
alignment. Simple yet elegant, our method can be easily incorporated into
existing approaches by changing only a few lines of code. We showcase in
comprehensive experiments that our method achieves substantial gains in both
human feedback and RL sample efficiency, demonstrating the importance of
addressing query-policy misalignment in PbRL tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.18437">Explainable Machine Learning for Categorical and Mixed Data with Lossless Visualization. (arXiv:2305.18437v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kovalerchuk_B/0/1/0/all/0/1">Boris Kovalerchuk</a>, <a href="http://arxiv.org/find/cs/1/au:+McCoy_E/0/1/0/all/0/1">Elijah McCoy</a></p>
<p>Building accurate and interpretable Machine Learning (ML) models for
heterogeneous/mixed data is a long-standing challenge for algorithms designed
for numeric data. This work focuses on developing numeric coding schemes for
non-numeric attributes for ML algorithms to support accurate and explainable ML
models, methods for lossless visualization of n-D non-numeric categorical data
with visual rule discovery in these visualizations, and accurate and
explainable ML models for categorical data. This study proposes a
classification of mixed data types and analyzes their important role in Machine
Learning. It presents a toolkit for enforcing interpretability of all internal
operations of ML algorithms on mixed data with a visual data exploration on
mixed data. A new Sequential Rule Generation (SRG) algorithm for explainable
rule generation with categorical data is proposed and successfully evaluated in
multiple computational experiments. This work is one of the steps to the full
scope ML algorithms for mixed data supported by lossless visualization of n-D
data in General Line Coordinates beyond Parallel Coordinates.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19190">Inverse Approximation Theory for Nonlinear Recurrent Neural Networks. (arXiv:2305.19190v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shida Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qianxiao Li</a></p>
<p>We prove an inverse approximation theorem for the approximation of nonlinear
sequence-to-sequence relationships using recurrent neural networks (RNNs). This
is a so-called Bernstein-type result in approximation theory, which deduces
properties of a target function under the assumption that it can be effectively
approximated by a hypothesis space. In particular, we show that nonlinear
sequence relationships that can be stably approximated by nonlinear RNNs must
have an exponential decaying memory structure - a notion that can be made
precise. This extends the previously identified curse of memory in linear RNNs
into the general nonlinear setting, and quantifies the essential limitations of
the RNN architecture for learning sequential relationships with long-term
memory. Based on the analysis, we propose a principled reparameterization
method to overcome the limitations. Our theoretical results are confirmed by
numerical experiments. The code has been released in
https://github.com/radarFudan/Curse-of-memory
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.06064">Neural Algorithmic Reasoning for Combinatorial Optimisation. (arXiv:2306.06064v3 [cs.NE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Georgiev_D/0/1/0/all/0/1">Dobrik Georgiev</a>, <a href="http://arxiv.org/find/cs/1/au:+Numeroso_D/0/1/0/all/0/1">Danilo Numeroso</a>, <a href="http://arxiv.org/find/cs/1/au:+Bacciu_D/0/1/0/all/0/1">Davide Bacciu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1">Pietro Li&#xf2;</a></p>
<p>Solving NP-hard/complete combinatorial problems with neural networks is a
challenging research area that aims to surpass classical approximate
algorithms. The long-term objective is to outperform hand-designed heuristics
for NP-hard/complete problems by learning to generate superior solutions solely
from training data. Current neural-based methods for solving CO problems often
overlook the inherent "algorithmic" nature of the problems. In contrast,
heuristics designed for CO problems, e.g. TSP, frequently leverage
well-established algorithms, such as those for finding the minimum spanning
tree. In this paper, we propose leveraging recent advancements in neural
algorithmic reasoning to improve the learning of CO problems. Specifically, we
suggest pre-training our neural model on relevant algorithms before training it
on CO instances. Our results demonstrate that by using this learning setup, we
achieve superior performance compared to non-algorithmically informed deep
learning models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.06394">PEAR: Primitive enabled Adaptive Relabeling for boosting Hierarchical Reinforcement Learning. (arXiv:2306.06394v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singh_U/0/1/0/all/0/1">Utsav Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1">Vinay P Namboodiri</a></p>
<p>Hierarchical reinforcement learning (HRL) has the potential to solve complex
long horizon tasks using temporal abstraction and increased exploration.
However, hierarchical agents are difficult to train due to inherent
non-stationarity. We present primitive enabled adaptive relabeling (PEAR), a
two-phase approach where we first perform adaptive relabeling on a few expert
demonstrations to generate efficient subgoal supervision, and then jointly
optimize HRL agents by employing reinforcement learning (RL) and imitation
learning (IL). We perform theoretical analysis to $(i)$ bound the
sub-optimality of our approach, and $(ii)$ derive a generalized plug-and-play
framework for joint optimization using RL and IL. PEAR uses a handful of expert
demonstrations and makes minimal limiting assumptions on the task structure.
Additionally, it can be easily integrated with typical model free RL algorithms
to produce a practical HRL algorithm. We perform experiments on challenging
robotic environments and show that PEAR is able to solve tasks that require
long term decision making. We empirically show that PEAR exhibits improved
performance and sample efficiency over previous hierarchical and
non-hierarchical approaches. We also perform real world robotic experiments on
complex tasks and demonstrate that PEAR consistently outperforms the baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.10548">MARBLE: Music Audio Representation Benchmark for Universal Evaluation. (arXiv:2306.10548v4 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_R/0/1/0/all/0/1">Ruibin Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yinghao Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yizhi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Ge Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xingran Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1">Hanzhi Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuo_L/0/1/0/all/0/1">Le Zhuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yiqi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jiawen Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1">Zeyue Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1">Binyue Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1">Ningzhi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1">Chenghua Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Benetos_E/0/1/0/all/0/1">Emmanouil Benetos</a>, <a href="http://arxiv.org/find/cs/1/au:+Ragni_A/0/1/0/all/0/1">Anton Ragni</a>, <a href="http://arxiv.org/find/cs/1/au:+Gyenge_N/0/1/0/all/0/1">Norbert Gyenge</a>, <a href="http://arxiv.org/find/cs/1/au:+Dannenberg_R/0/1/0/all/0/1">Roger Dannenberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wenhu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1">Gus Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1">Wei Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Si Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1">Ruibo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yike Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1">Jie Fu</a></p>
<p>In the era of extensive intersection between art and Artificial Intelligence
(AI), such as image generation and fiction co-creation, AI for music remains
relatively nascent, particularly in music understanding. This is evident in the
limited work on deep music representations, the scarcity of large-scale
datasets, and the absence of a universal and community-driven benchmark. To
address this issue, we introduce the Music Audio Representation Benchmark for
universaL Evaluation, termed MARBLE. It aims to provide a benchmark for various
Music Information Retrieval (MIR) tasks by defining a comprehensive taxonomy
with four hierarchy levels, including acoustic, performance, score, and
high-level description. We then establish a unified protocol based on 14 tasks
on 8 public-available datasets, providing a fair and standard assessment of
representations of all open-sourced pre-trained models developed on music
recordings as baselines. Besides, MARBLE offers an easy-to-use, extendable, and
reproducible suite for the community, with a clear statement on copyright
issues on datasets. Results suggest recently proposed large-scale pre-trained
musical language models perform the best in most tasks, with room for further
improvement. The leaderboard and toolkit repository are published at
https://marble-bm.shef.ac.uk to promote future music AI research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.10841">Blockchain-Enabled Federated Learning: A Reference Architecture Design, Implementation, and Verification. (arXiv:2306.10841v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Goh_E/0/1/0/all/0/1">Eunsu Goh</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1">Dae-Yeol Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kwangkee Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1">Suyeong Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Chae_J/0/1/0/all/0/1">Jong-Eui Chae</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1">Do-Yup Kim</a></p>
<p>This paper presents a novel reference architecture for blockchain-enabled
federated learning (BCFL), a state-of-the-art approach that amalgamates the
strengths of federated learning and blockchain technology.We define smart
contract functions, stakeholders and their roles, and the use of interplanetary
file system (IPFS) as key components of BCFL and conduct a comprehensive
analysis. In traditional centralized federated learning, the selection of local
nodes and the collection of learning results for each round are merged under
the control of a central server. In contrast, in BCFL, all these processes are
monitored and managed via smart contracts. Additionally, we propose an
extension architecture to support both crossdevice and cross-silo federated
learning scenarios. Furthermore, we implement and verify the architecture in a
practical real-world Ethereum development environment. Our BCFL reference
architecture provides significant flexibility and extensibility, accommodating
the integration of various additional elements, as per specific requirements
and use cases, thereby rendering it an adaptable solution for a wide range of
BCFL applications. As a prominent example of extensibility, decentralized
identifiers (DIDs) have been employed as an authentication method to introduce
practical utilization within BCFL. This study not only bridges a crucial gap
between research and practical deployment but also lays a solid foundation for
future explorations in the realm of BCFL. The pivotal contribution of this
study is the successful implementation and verification of a realistic BCFL
reference architecture. We intend to make the source code publicly accessible
shortly, fostering further advancements and adaptations within the community.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.11380">A Bayesian Take on Gaussian Process Networks. (arXiv:2306.11380v4 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Giudice_E/0/1/0/all/0/1">Enrico Giudice</a>, <a href="http://arxiv.org/find/stat/1/au:+Kuipers_J/0/1/0/all/0/1">Jack Kuipers</a>, <a href="http://arxiv.org/find/stat/1/au:+Moffa_G/0/1/0/all/0/1">Giusi Moffa</a></p>
<p>Gaussian Process Networks (GPNs) are a class of directed graphical models
which employ Gaussian processes as priors for the conditional expectation of
each variable given its parents in the network. The model allows the
description of continuous joint distributions in a compact but flexible manner
with minimal parametric assumptions on the dependencies between variables.
Bayesian structure learning of GPNs requires computing the posterior over
graphs of the network and is computationally infeasible even in low dimensions.
This work implements Monte Carlo and Markov Chain Monte Carlo methods to sample
from the posterior distribution of network structures. As such, the approach
follows the Bayesian paradigm, comparing models via their marginal likelihood
and computing the posterior probability of the GPN features. Simulation studies
show that our method outperforms state-of-the-art algorithms in recovering the
graphical structure of the network and provides an accurate approximation of
its posterior distribution.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.12070">Task-Robust Pre-Training for Worst-Case Downstream Adaptation. (arXiv:2306.12070v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jianghui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xingyu Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1">Cong Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zhouchen Lin</a></p>
<p>Pre-training has achieved remarkable success when transferred to downstream
tasks. In machine learning, we care about not only the good performance of a
model but also its behavior under reasonable shifts of condition. The same
philosophy holds when pre-training a foundation model. However, the foundation
model may not uniformly behave well for a series of related downstream tasks.
This happens, for example, when conducting mask recovery regression where the
recovery ability or the training instances diverge like pattern features are
extracted dominantly on pre-training, but semantic features are also required
on a downstream task. This paper considers pre-training a model that guarantees
a uniformly good performance over the downstream tasks. We call this goal as
$\textit{downstream-task robustness}$. Our method first separates the upstream
task into several representative ones and applies a simple minimax loss for
pre-training. We then design an efficient algorithm to solve the minimax loss
and prove its convergence in the convex setting. In the experiments, we show
both on large-scale natural language processing and computer vision datasets
our method increases the metrics on worse-case downstream tasks. Additionally,
some theoretical explanations for why our loss is beneficial are provided.
Specifically, we show fewer samples are inherently required for the most
challenging downstream task in some cases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.13686">Broadening the perspective for sustainable AI: Comprehensive sustainability criteria and indicators for AI systems. (arXiv:2306.13686v2 [cs.CY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rohde_F/0/1/0/all/0/1">Friederike Rohde</a>, <a href="http://arxiv.org/find/cs/1/au:+Wagner_J/0/1/0/all/0/1">Josephin Wagner</a>, <a href="http://arxiv.org/find/cs/1/au:+Meyer_A/0/1/0/all/0/1">Andreas Meyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Reinhard_P/0/1/0/all/0/1">Philipp Reinhard</a>, <a href="http://arxiv.org/find/cs/1/au:+Voss_M/0/1/0/all/0/1">Marcus Voss</a>, <a href="http://arxiv.org/find/cs/1/au:+Petschow_U/0/1/0/all/0/1">Ulrich Petschow</a>, <a href="http://arxiv.org/find/cs/1/au:+Mollen_A/0/1/0/all/0/1">Anne Mollen</a></p>
<p>The increased use of AI systems is associated with multi-faceted societal,
environmental, and economic consequences. These include non-transparent
decision-making processes, discrimination, increasing inequalities, rising
energy consumption and greenhouse gas emissions in AI model development and
application, and an increasing concentration of economic power. By considering
the multi-dimensionality of sustainability, this paper takes steps towards
substantiating the call for an overarching perspective on "sustainable AI". It
presents the SCAIS Framework (Sustainability Criteria and Indicators for
Artificial Intelligence Systems) which contains a set 19 sustainability
criteria for sustainable AI and 67 indicators that is based on the results of a
critical review and expert workshops. This interdisciplinary approach
contributes a unique holistic perspective to facilitate and structure the
discourse on sustainable AI. Further, it provides a concrete framework that
lays the foundation for developing standards and tools to support the conscious
development and application of AI systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.13724">Review of compressed embedding layers and their applications for recommender systems. (arXiv:2306.13724v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hajgato_T/0/1/0/all/0/1">Tamas Hajgato</a></p>
<p>We review the literature on trainable, compressed embedding layers and
discuss their applicability for compressing gigantic neural recommender
systems. We also report the results we measured with our compressed embedding
layers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.15030">Equivariant flow matching. (arXiv:2306.15030v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Klein_L/0/1/0/all/0/1">Leon Klein</a>, <a href="http://arxiv.org/find/stat/1/au:+Kramer_A/0/1/0/all/0/1">Andreas Kr&#xe4;mer</a>, <a href="http://arxiv.org/find/stat/1/au:+Noe_F/0/1/0/all/0/1">Frank No&#xe9;</a></p>
<p>Normalizing flows are a class of deep generative models that are especially
interesting for modeling probability distributions in physics, where the exact
likelihood of flows allows reweighting to known target energy functions and
computing unbiased observables. For instance, Boltzmann generators tackle the
long-standing sampling problem in statistical physics by training flows to
produce equilibrium samples of many-body systems such as small molecules and
proteins. To build effective models for such systems, it is crucial to
incorporate the symmetries of the target energy into the model, which can be
achieved by equivariant continuous normalizing flows (CNFs). However, CNFs can
be computationally expensive to train and generate samples from, which has
hampered their scalability and practical application. In this paper, we
introduce equivariant flow matching, a new training objective for equivariant
CNFs that is based on the recently proposed optimal transport flow matching.
Equivariant flow matching exploits the physical symmetries of the target energy
for efficient, simulation-free training of equivariant CNFs. We demonstrate the
effectiveness of flow matching on rotation and permutation invariant
many-particle systems and a small molecule, alanine dipeptide, where for the
first time we obtain a Boltzmann generator with significant sampling efficiency
without relying on tailored internal coordinate featurization. Our results show
that the equivariant flow matching objective yields flows with shorter
integration paths, improved sampling efficiency, and higher scalability
compared to existing methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.15374">LeCo: Lightweight Compression via Learning Serial Correlations. (arXiv:2306.15374v3 [cs.DB] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yihao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1">Xinyu Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Huanchen Zhang</a></p>
<p>Lightweight data compression is a key technique that allows column stores to
exhibit superior performance for analytical queries. Despite a comprehensive
study on dictionary-based encodings to approach Shannon's entropy, few prior
works have systematically exploited the serial correlation in a column for
compression. In this paper, we propose LeCo (i.e., Learned Compression), a
framework that uses machine learning to remove the serial redundancy in a value
sequence automatically to achieve an outstanding compression ratio and
decompression performance simultaneously. LeCo presents a general approach to
this end, making existing (ad-hoc) algorithms such as Frame-of-Reference (FOR),
Delta Encoding, and Run-Length Encoding (RLE) special cases under our
framework. Our microbenchmark with three synthetic and six real-world data sets
shows that a prototype of LeCo achieves a Pareto improvement on both
compression ratio and random access speed over the existing solutions. When
integrating LeCo into widely-used applications, we observe up to 5.2x speed up
in a data analytical query in the Arrow columnar execution engine and a 16%
increase in RocksDB's throughput.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07871">The SocialAI School: Insights from Developmental Psychology Towards Artificial Socio-Cultural Agents. (arXiv:2307.07871v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kovac_G/0/1/0/all/0/1">Grgur Kova&#x10d;</a>, <a href="http://arxiv.org/find/cs/1/au:+Portelas_R/0/1/0/all/0/1">R&#xe9;my Portelas</a>, <a href="http://arxiv.org/find/cs/1/au:+Dominey_P/0/1/0/all/0/1">Peter Ford Dominey</a>, <a href="http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1">Pierre-Yves Oudeyer</a></p>
<p>Developmental psychologists have long-established the importance of
socio-cognitive abilities in human intelligence. These abilities enable us to
enter, participate and benefit from human culture. AI research on social
interactive agents mostly concerns the emergence of culture in a multi-agent
setting (often without a strong grounding in developmental psychology). We
argue that AI research should be informed by psychology and study
socio-cognitive abilities enabling to enter a culture too. We discuss the
theories of Michael Tomasello and Jerome Bruner to introduce some of their
concepts to AI and outline key concepts and socio-cognitive abilities. We
present The SocialAI school - a tool including a customizable parameterized
uite of procedurally generated environments, which simplifies conducting
experiments regarding those concepts. We show examples of such experiments with
RL agents and Large Language Models. The main motivation of this work is to
engage the AI community around the problem of social intelligence informed by
developmental psychology, and to provide a tool to simplify first steps in this
direction. Refer to the project website for code and additional information:
https://sites.google.com/view/socialai-school.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08187">An Empirical Investigation of Pre-trained Model Selection for Out-of-Distribution Generalization and Calibration. (arXiv:2307.08187v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Naganuma_H/0/1/0/all/0/1">Hiroki Naganuma</a>, <a href="http://arxiv.org/find/cs/1/au:+Hataya_R/0/1/0/all/0/1">Ryuichiro Hataya</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitliagkas_I/0/1/0/all/0/1">Ioannis Mitliagkas</a></p>
<p>In the realm of out-of-distribution (OOD) generalization tasks, fine-tuning
pre-trained models has become a prevalent strategy. Different from most prior
work that has focused on advancing learning algorithms, we systematically
examined how pre-trained model size, pre-training data scale, and training
strategies impact downstream generalization and uncertainty calibration. We
evaluated 97 models across diverse pre-trained model sizes, five pre-training
datasets, and five data augmentations through extensive experiments on four
distribution shift datasets totaling over 100,000 GPU hours. Our results
demonstrate the significant impact of pre-trained model selection, with optimal
choices substantially improving OOD accuracy over algorithm improvement alone.
We find larger models and bigger pre-training data improve OOD performance and
calibration, in contrast to some prior studies that found modern deep networks
to calibrate worse than classical shallow models. Our work underscores the
overlooked importance of pre-trained model selection for out-of-distribution
generalization and calibration.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.07741">Real Robot Challenge 2022: Learning Dexterous Manipulation from Offline Data in the Real World. (arXiv:2308.07741v3 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gurtler_N/0/1/0/all/0/1">Nico G&#xfc;rtler</a>, <a href="http://arxiv.org/find/cs/1/au:+Widmaier_F/0/1/0/all/0/1">Felix Widmaier</a>, <a href="http://arxiv.org/find/cs/1/au:+Sancaktar_C/0/1/0/all/0/1">Cansu Sancaktar</a>, <a href="http://arxiv.org/find/cs/1/au:+Blaes_S/0/1/0/all/0/1">Sebastian Blaes</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolev_P/0/1/0/all/0/1">Pavel Kolev</a>, <a href="http://arxiv.org/find/cs/1/au:+Bauer_S/0/1/0/all/0/1">Stefan Bauer</a>, <a href="http://arxiv.org/find/cs/1/au:+Wuthrich_M/0/1/0/all/0/1">Manuel W&#xfc;thrich</a>, <a href="http://arxiv.org/find/cs/1/au:+Wulfmeier_M/0/1/0/all/0/1">Markus Wulfmeier</a>, <a href="http://arxiv.org/find/cs/1/au:+Riedmiller_M/0/1/0/all/0/1">Martin Riedmiller</a>, <a href="http://arxiv.org/find/cs/1/au:+Allshire_A/0/1/0/all/0/1">Arthur Allshire</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+McCarthy_R/0/1/0/all/0/1">Robert McCarthy</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">Hangyeol Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Baek_J/0/1/0/all/0/1">Jongchan Baek</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwon_W/0/1/0/all/0/1">Wookyong Kwon</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1">Shanliang Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Toshimitsu_Y/0/1/0/all/0/1">Yasunori Toshimitsu</a>, <a href="http://arxiv.org/find/cs/1/au:+Michelis_M/0/1/0/all/0/1">Mike Yan Michelis</a>, <a href="http://arxiv.org/find/cs/1/au:+Kazemipour_A/0/1/0/all/0/1">Amirhossein Kazemipour</a>, <a href="http://arxiv.org/find/cs/1/au:+Raayatsanati_A/0/1/0/all/0/1">Arman Raayatsanati</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1">Hehui Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Cangan_B/0/1/0/all/0/1">Barnabas Gavin Cangan</a>, <a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1">Bernhard Sch&#xf6;lkopf</a>, <a href="http://arxiv.org/find/cs/1/au:+Martius_G/0/1/0/all/0/1">Georg Martius</a></p>
<p>Experimentation on real robots is demanding in terms of time and costs. For
this reason, a large part of the reinforcement learning (RL) community uses
simulators to develop and benchmark algorithms. However, insights gained in
simulation do not necessarily translate to real robots, in particular for tasks
involving complex interactions with the environment. The Real Robot Challenge
2022 therefore served as a bridge between the RL and robotics communities by
allowing participants to experiment remotely with a real robot - as easily as
in simulation.
</p>
<p>In the last years, offline reinforcement learning has matured into a
promising paradigm for learning from pre-collected datasets, alleviating the
reliance on expensive online interactions. We therefore asked the participants
to learn two dexterous manipulation tasks involving pushing, grasping, and
in-hand orientation from provided real-robot datasets. An extensive software
documentation and an initial stage based on a simulation of the real set-up
made the competition particularly accessible. By giving each team plenty of
access budget to evaluate their offline-learned policies on a cluster of seven
identical real TriFinger platforms, we organized an exciting competition for
machine learners and roboticists alike.
</p>
<p>In this work we state the rules of the competition, present the methods used
by the winning teams and compare their results with a benchmark of
state-of-the-art offline RL algorithms on the challenge datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.08467">On Neural Quantum Support Vector Machines. (arXiv:2308.08467v2 [quant-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Simon_L/0/1/0/all/0/1">Lars Simon</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Radons_M/0/1/0/all/0/1">Manuel Radons</a></p>
<p>In \cite{simon2023algorithms} we introduced four algorithms for the training
of neural support vector machines (NSVMs) and demonstrated their feasibility.
In this note we introduce neural quantum support vector machines, that is,
NSVMs with a quantum kernel, and extend our results to this setting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.09043">Kernel-Based Tests for Likelihood-Free Hypothesis Testing. (arXiv:2308.09043v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Gerber_P/0/1/0/all/0/1">Patrik R&#xf3;bert Gerber</a>, <a href="http://arxiv.org/find/stat/1/au:+Jiang_T/0/1/0/all/0/1">Tianze Jiang</a>, <a href="http://arxiv.org/find/stat/1/au:+Polyanskiy_Y/0/1/0/all/0/1">Yury Polyanskiy</a>, <a href="http://arxiv.org/find/stat/1/au:+Sun_R/0/1/0/all/0/1">Rui Sun</a></p>
<p>Given $n$ observations from two balanced classes, consider the task of
labeling an additional $m$ inputs that are known to all belong to \emph{one} of
the two classes. Special cases of this problem are well-known: with complete
knowledge of class distributions ($n=\infty$) the problem is solved optimally
by the likelihood-ratio test; when $m=1$ it corresponds to binary
classification; and when $m\approx n$ it is equivalent to two-sample testing.
The intermediate settings occur in the field of likelihood-free inference,
where labeled samples are obtained by running forward simulations and the
unlabeled sample is collected experimentally. In recent work it was discovered
that there is a fundamental trade-off between $m$ and $n$: increasing the data
sample $m$ reduces the amount $n$ of training/simulation data needed. In this
work we (a) introduce a generalization where unlabeled samples come from a
mixture of the two classes -- a case often encountered in practice; (b) study
the minimax sample complexity for non-parametric classes of densities under
\textit{maximum mean discrepancy} (MMD) separation; and (c) investigate the
empirical performance of kernels parameterized by neural networks on two tasks:
detection of the Higgs boson and detection of planted DDPM generated images
amidst CIFAR-10 images. For both problems we confirm the existence of the
theoretically predicted asymmetric $m$ vs $n$ trade-off.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.09687">Graph of Thoughts: Solving Elaborate Problems with Large Language Models. (arXiv:2308.09687v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Besta_M/0/1/0/all/0/1">Maciej Besta</a>, <a href="http://arxiv.org/find/cs/1/au:+Blach_N/0/1/0/all/0/1">Nils Blach</a>, <a href="http://arxiv.org/find/cs/1/au:+Kubicek_A/0/1/0/all/0/1">Ales Kubicek</a>, <a href="http://arxiv.org/find/cs/1/au:+Gerstenberger_R/0/1/0/all/0/1">Robert Gerstenberger</a>, <a href="http://arxiv.org/find/cs/1/au:+Gianinazzi_L/0/1/0/all/0/1">Lukas Gianinazzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gajda_J/0/1/0/all/0/1">Joanna Gajda</a>, <a href="http://arxiv.org/find/cs/1/au:+Lehmann_T/0/1/0/all/0/1">Tomasz Lehmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Podstawski_M/0/1/0/all/0/1">Michal Podstawski</a>, <a href="http://arxiv.org/find/cs/1/au:+Niewiadomski_H/0/1/0/all/0/1">Hubert Niewiadomski</a>, <a href="http://arxiv.org/find/cs/1/au:+Nyczyk_P/0/1/0/all/0/1">Piotr Nyczyk</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoefler_T/0/1/0/all/0/1">Torsten Hoefler</a></p>
<p>We introduce Graph of Thoughts (GoT): a framework that advances prompting
capabilities in large language models (LLMs) beyond those offered by paradigms
such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary
advantage of GoT is the ability to model the information generated by an LLM as
an arbitrary graph, where units of information ("LLM thoughts") are vertices,
and edges correspond to dependencies between these vertices. This approach
enables combining arbitrary LLM thoughts into synergistic outcomes, distilling
the essence of whole networks of thoughts, or enhancing thoughts using feedback
loops. We illustrate that GoT offers advantages over state of the art on
different tasks, for example increasing the quality of sorting by 62% over ToT,
while simultaneously reducing costs by &gt;31%. We ensure that GoT is extensible
with new thought transformations and thus can be used to spearhead new
prompting schemes. This work brings the LLM reasoning closer to human thinking
or brain mechanisms such as recurrence, both of which form complex networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.11792">Karasu: A Collaborative Approach to Efficient Cluster Configuration for Big Data Analytics. (arXiv:2308.11792v2 [cs.DC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Scheinert_D/0/1/0/all/0/1">Dominik Scheinert</a>, <a href="http://arxiv.org/find/cs/1/au:+Wiesner_P/0/1/0/all/0/1">Philipp Wiesner</a>, <a href="http://arxiv.org/find/cs/1/au:+Wittkopp_T/0/1/0/all/0/1">Thorsten Wittkopp</a>, <a href="http://arxiv.org/find/cs/1/au:+Thamsen_L/0/1/0/all/0/1">Lauritz Thamsen</a>, <a href="http://arxiv.org/find/cs/1/au:+Will_J/0/1/0/all/0/1">Jonathan Will</a>, <a href="http://arxiv.org/find/cs/1/au:+Kao_O/0/1/0/all/0/1">Odej Kao</a></p>
<p>Selecting the right resources for big data analytics jobs is hard because of
the wide variety of configuration options like machine type and cluster size.
As poor choices can have a significant impact on resource efficiency, cost, and
energy usage, automated approaches are gaining popularity. Most existing
methods rely on profiling recurring workloads to find near-optimal solutions
over time. Due to the cold-start problem, this often leads to lengthy and
costly profiling phases. However, big data analytics jobs across users can
share many common properties: they often operate on similar infrastructure,
using similar algorithms implemented in similar frameworks. The potential in
sharing aggregated profiling runs to collaboratively address the cold start
problem is largely unexplored.
</p>
<p>We present Karasu, an approach to more efficient resource configuration
profiling that promotes data sharing among users working with similar
infrastructures, frameworks, algorithms, or datasets. Karasu trains lightweight
performance models using aggregated runtime information of collaborators and
combines them into an ensemble method to exploit inherent knowledge of the
configuration search space. Moreover, Karasu allows the optimization of
multiple objectives simultaneously. Our evaluation is based on performance data
from diverse workload executions in a public cloud environment. We show that
Karasu is able to significantly boost existing methods in terms of performance,
search time, and cost, even when few comparable profiling runs are available
that share only partial common characteristics with the target job.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16483">Improving Out-of-Distribution Detection in Echocardiographic View Classication through Enhancing Semantic Features. (arXiv:2308.16483v2 [eess.SP] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Jeon_J/0/1/0/all/0/1">Jaeik Jeon</a>, <a href="http://arxiv.org/find/eess/1/au:+Ha_S/0/1/0/all/0/1">Seongmin Ha</a>, <a href="http://arxiv.org/find/eess/1/au:+Jang_Y/0/1/0/all/0/1">Yeonggul Jang</a>, <a href="http://arxiv.org/find/eess/1/au:+Yoon_Y/0/1/0/all/0/1">Yeonyee E. Yoon</a>, <a href="http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1">Jiyeon Kim</a>, <a href="http://arxiv.org/find/eess/1/au:+Jeong_H/0/1/0/all/0/1">Hyunseok Jeong</a>, <a href="http://arxiv.org/find/eess/1/au:+Jeong_D/0/1/0/all/0/1">Dawun Jeong</a>, <a href="http://arxiv.org/find/eess/1/au:+Hong_Y/0/1/0/all/0/1">Youngtaek Hong</a>, <a href="http://arxiv.org/find/eess/1/au:+Chang_S/0/1/0/all/0/1">Seung-Ah Lee Hyuk-Jae Chang</a></p>
<p>In echocardiographic view classification, accurately detecting
out-of-distribution (OOD) data is essential but challenging, especially given
the subtle differences between in-distribution and OOD data. While conventional
OOD detection methods, such as Mahalanobis distance (MD) are effective in
far-OOD scenarios with clear distinctions between distributions, they struggle
to discern the less obvious variations characteristic of echocardiographic
data. In this study, we introduce a novel use of label smoothing to enhance
semantic feature representation in echocardiographic images, demonstrating that
these enriched semantic features are key for significantly improving near-OOD
instance detection. By combining label smoothing with MD-based OOD detection,
we establish a new benchmark for accuracy in echocardiographic OOD detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.02317">A study on the impact of pre-trained model on Just-In-Time defect prediction. (arXiv:2309.02317v2 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yuxiang Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xiaopeng Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhenyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_W/0/1/0/all/0/1">W.K.Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1">Bo Jiang</a></p>
<p>Previous researchers conducting Just-In-Time (JIT) defect prediction tasks
have primarily focused on the performance of individual pre-trained models,
without exploring the relationship between different pre-trained models as
backbones. In this study, we build six models: RoBERTaJIT, CodeBERTJIT,
BARTJIT, PLBARTJIT, GPT2JIT, and CodeGPTJIT, each with a distinct pre-trained
model as its backbone. We systematically explore the differences and
connections between these models. Specifically, we investigate the performance
of the models when using Commit code and Commit message as inputs, as well as
the relationship between training efficiency and model distribution among these
six models. Additionally, we conduct an ablation experiment to explore the
sensitivity of each model to inputs. Furthermore, we investigate how the models
perform in zero-shot and few-shot scenarios. Our findings indicate that each
model based on different backbones shows improvements, and when the backbone's
pre-training model is similar, the training resources that need to be consumed
are much more closer. We also observe that Commit code plays a significant role
in defect detection, and different pre-trained models demonstrate better defect
detection ability with a balanced dataset under few-shot scenarios. These
results provide new insights for optimizing JIT defect prediction tasks using
pre-trained models and highlight the factors that require more attention when
constructing such models. Additionally, CodeGPTJIT and GPT2JIT achieved better
performance than DeepJIT and CC2Vec on the two datasets respectively under 2000
training samples. These findings emphasize the effectiveness of
transformer-based pre-trained models in JIT defect prediction tasks, especially
in scenarios with limited training data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.04885">Symplectic Structure-Aware Hamiltonian (Graph) Embeddings. (arXiv:2309.04885v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiaxu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1">Xinping Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tianle Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xiaowei Huang</a></p>
<p>In traditional Graph Neural Networks (GNNs), the assumption of a fixed
embedding manifold often limits their adaptability to diverse graph geometries.
Recently, Hamiltonian system-inspired GNNs have been proposed to address the
dynamic nature of such embeddings by incorporating physical laws into node
feature updates. We present Symplectic Structure-Aware Hamiltonian GNN
(SAH-GNN), a novel approach that generalizes Hamiltonian dynamics for more
flexible node feature updates. Unlike existing Hamiltonian approaches, SAH-GNN
employs Riemannian optimization on the symplectic Stiefel manifold to
adaptively learn the underlying symplectic structure, circumventing the
limitations of existing Hamiltonian GNNs that rely on a pre-defined form of
standard symplectic structure. This innovation allows SAH-GNN to automatically
adapt to various graph datasets without extensive hyperparameter tuning.
Moreover, it conserves energy during training meaning the implicit Hamiltonian
system is physically meaningful. Finally, we empirically validate SAH-GNN's
superiority and adaptability in node classification tasks across multiple types
of graph datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.09384">Mitigating Over-Smoothing and Over-Squashing using Augmentations of Forman-Ricci Curvature. (arXiv:2309.09384v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fesser_L/0/1/0/all/0/1">Lukas Fesser</a>, <a href="http://arxiv.org/find/cs/1/au:+Weber_M/0/1/0/all/0/1">Melanie Weber</a></p>
<p>While Graph Neural Networks (GNNs) have been successfully leveraged for
learning on graph-structured data across domains, several potential pitfalls
have been described recently. Those include the inability to accurately
leverage information encoded in long-range connections (over-squashing), as
well as difficulties distinguishing the learned representations of nearby nodes
with growing network depth (over-smoothing). An effective way to characterize
both effects is discrete curvature: Long-range connections that underlie
over-squashing effects have low curvature, whereas edges that contribute to
over-smoothing have high curvature. This observation has given rise to rewiring
techniques, which add or remove edges to mitigate over-smoothing and
over-squashing. Several rewiring approaches utilizing graph characteristics,
such as curvature or the spectrum of the graph Laplacian, have been proposed.
However, existing methods, especially those based on curvature, often require
expensive subroutines and careful hyperparameter tuning, which limits their
applicability to large-scale graphs. Here we propose a rewiring technique based
on Augmented Forman-Ricci curvature (AFRC), a scalable curvature notation,
which can be computed in linear time. We prove that AFRC effectively
characterizes over-smoothing and over-squashing effects in message-passing
GNNs. We complement our theoretical results with experiments, which demonstrate
that the proposed approach achieves state-of-the-art performance while
significantly reducing the computational cost in comparison with other methods.
Utilizing fundamental properties of discrete curvature, we propose effective
heuristics for hyperparameters in curvature-based rewiring, which avoids
expensive hyperparameter searches, further improving the scalability of the
proposed approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.09904">Learning to Generate Lumped Hydrological Models. (arXiv:2309.09904v2 [physics.geo-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Yang_Y/0/1/0/all/0/1">Yang Yang</a>, <a href="http://arxiv.org/find/physics/1/au:+Chui_T/0/1/0/all/0/1">Ting Fong May Chui</a></p>
<p>A lumped hydrological model structure can be considered a generative model
because, given a set of parameter values, it can generate a hydrological
modeling function that accurately predicts the behavior of a catchment under
external forcing. It is implicitly assumed that a small number of variables
(i.e., the model parameters) can sufficiently characterize variations in the
behavioral characteristics of different catchments. This study adopts this
assumption and uses a deep learning method to learn a generative model of
hydrological modeling functions directly from the forcing and runoff data of
multiple catchments. The learned generative model uses a small number of latent
variables to characterize a catchment's behavior, so that assigning values to
these latent variables produces a hydrological modeling function that resembles
a real-world catchment. The learned generative model can be used similarly to a
lumped model structure, i.e., the optimal hydrological modeling function of a
catchment can be derived by estimating optimal parameter values (or latent
variables) with a generic calibration algorithm. In this study, a generative
model was learned from data from over 3,000 catchments worldwide. The model was
then used to derive optimal modeling functions for over 700 different
catchments. The resulting modeling functions generally showed a quality that
was comparable to or better than 36 types of lumped model structures. Overall,
this study demonstrates that the hydrological behavior of a catchment can be
effectively described using a small number of latent variables, and that
well-fitting hydrologic model functions can be reconstructed from these
variables.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.09968">Generating and Imputing Tabular Data via Diffusion and Flow-based Gradient-Boosted Trees. (arXiv:2309.09968v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jolicoeur_Martineau_A/0/1/0/all/0/1">Alexia Jolicoeur-Martineau</a>, <a href="http://arxiv.org/find/cs/1/au:+Fatras_K/0/1/0/all/0/1">Kilian Fatras</a>, <a href="http://arxiv.org/find/cs/1/au:+Kachman_T/0/1/0/all/0/1">Tal Kachman</a></p>
<p>Tabular data is hard to acquire and is subject to missing values. This paper
proposes a novel approach to generate and impute mixed-type (continuous and
categorical) tabular data using score-based diffusion and conditional flow
matching. Contrary to previous work that relies on neural networks to learn the
score function or the vector field, we instead rely on XGBoost, a popular
Gradient-Boosted Tree (GBT) method. We empirically show on 27 different
datasets that our approach i) generates highly realistic synthetic data when
the training dataset is either clean or tainted by missing data and ii)
generates diverse plausible data imputations. Furthermore, our method
outperforms deep-learning generation methods on data generation and is
competitive on data imputation. Finally, it can be trained in parallel using
CPUs without the need for a GPU. To make it easily accessible, we release our
code through a Python library and an R package.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12862">Associative Transformer Is A Sparse Representation Learner. (arXiv:2309.12862v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yuwei Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Ochiai_H/0/1/0/all/0/1">Hideya Ochiai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhirong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1">Stephen Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Kanai_R/0/1/0/all/0/1">Ryota Kanai</a></p>
<p>Emerging from the monolithic pairwise attention mechanism in conventional
Transformer models, there is a growing interest in leveraging sparse
interactions that align more closely with biological principles. Approaches
including the Set Transformer and the Perceiver employ cross-attention
consolidated with a latent space that forms an attention bottleneck with
limited capacity. Building upon recent neuroscience studies of Global Workspace
Theory and associative memory, we propose the Associative Transformer (AiT).
AiT induces low-rank explicit memory that serves as both priors to guide
bottleneck attention in the shared workspace and attractors within associative
memory of a Hopfield network. Through joint end-to-end training, these priors
naturally develop module specialization, each contributing a distinct inductive
bias to form attention bottlenecks. A bottleneck can foster competition among
inputs for writing information into the memory. We show that AiT is a sparse
representation learner, learning distinct priors through the bottlenecks that
are complexity-invariant to input quantities and dimensions. AiT demonstrates
its superiority over methods such as the Set Transformer, Vision Transformer,
and Coordination in various vision tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.14156">Designing and evaluating an online reinforcement learning agent for physical exercise recommendations in N-of-1 trials. (arXiv:2309.14156v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Meier_D/0/1/0/all/0/1">Dominik Meier</a>, <a href="http://arxiv.org/find/cs/1/au:+Ensari_I/0/1/0/all/0/1">Ipek Ensari</a>, <a href="http://arxiv.org/find/cs/1/au:+Konigorski_S/0/1/0/all/0/1">Stefan Konigorski</a></p>
<p>Personalized adaptive interventions offer the opportunity to increase patient
benefits, however, there are challenges in their planning and implementation.
Once implemented, it is an important question whether personalized adaptive
interventions are indeed clinically more effective compared to a fixed gold
standard intervention. In this paper, we present an innovative N-of-1 trial
study design testing whether implementing a personalized intervention by an
online reinforcement learning agent is feasible and effective. Throughout, we
use a new study on physical exercise recommendations to reduce pain in
endometriosis for illustration. We describe the design of a contextual bandit
recommendation agent and evaluate the agent in simulation studies. The results
show that, first, implementing a personalized intervention by an online
reinforcement learning agent is feasible. Second, such adaptive interventions
have the potential to improve patients' benefits even if only few observations
are available. As one challenge, they add complexity to the design and
implementation process. In order to quantify the expected benefit, data from
previous interventional studies is required. We expect our approach to be
transferable to other interventions and clinical interventions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.15643">Why do Angular Margin Losses work well for Semi-Supervised Anomalous Sound Detection?. (arXiv:2309.15643v2 [eess.AS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Wilkinghoff_K/0/1/0/all/0/1">Kevin Wilkinghoff</a>, <a href="http://arxiv.org/find/eess/1/au:+Kurth_F/0/1/0/all/0/1">Frank Kurth</a></p>
<p>State-of-the-art anomalous sound detection systems often utilize angular
margin losses to learn suitable representations of acoustic data using an
auxiliary task, which usually is a supervised or self-supervised classification
task. The underlying idea is that, in order to solve this auxiliary task,
specific information about normal data needs to be captured in the learned
representations and that this information is also sufficient to differentiate
between normal and anomalous samples. Especially in noisy conditions,
discriminative models based on angular margin losses tend to significantly
outperform systems based on generative or one-class models. The goal of this
work is to investigate why using angular margin losses with auxiliary tasks
works well for detecting anomalous sounds. To this end, it is shown, both
theoretically and experimentally, that minimizing angular margin losses also
minimizes compactness loss while inherently preventing learning trivial
solutions. Furthermore, multiple experiments are conducted to show that using a
related classification task as an auxiliary task teaches the model to learn
representations suitable for detecting anomalous sounds in noisy conditions.
Among these experiments are performance evaluations, visualizing the embedding
space with t-SNE and visualizing the input representations with respect to the
anomaly score using randomized input sampling for explanation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.17296">Navigating the Design Space of Equivariant Diffusion-Based Generative Models for De Novo 3D Molecule Generation. (arXiv:2309.17296v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1">Tuan Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Cremer_J/0/1/0/all/0/1">Julian Cremer</a>, <a href="http://arxiv.org/find/cs/1/au:+Noe_F/0/1/0/all/0/1">Frank No&#xe9;</a>, <a href="http://arxiv.org/find/cs/1/au:+Clevert_D/0/1/0/all/0/1">Djork-Arn&#xe9; Clevert</a>, <a href="http://arxiv.org/find/cs/1/au:+Schutt_K/0/1/0/all/0/1">Kristof Sch&#xfc;tt</a></p>
<p>Deep generative diffusion models are a promising avenue for 3D de novo
molecular design in materials science and drug discovery. However, their
utility is still limited by suboptimal performance on large molecular
structures and limited training data. To address this gap, we explore the
design space of E(3)-equivariant diffusion models, focusing on previously
unexplored areas. Our extensive comparative analysis evaluates the interplay
between continuous and discrete state spaces. From this investigation, we
present the EQGAT-diff model, which consistently outperforms established models
for the QM9 and GEOM-Drugs datasets. Significantly, EQGAT-diff takes continuous
atom positions, while chemical elements and bond types are categorical and uses
time-dependent loss weighting, substantially increasing training convergence,
the quality of generated samples, and inference time. We also showcase that
including chemically motivated additional features like hybridization states in
the diffusion process enhances the validity of generated molecules. To further
strengthen the applicability of diffusion models to limited training data, we
investigate the transferability of EQGAT-diff trained on the large PubChem3D
dataset with implicit hydrogen atoms to target different data distributions.
Fine-tuning EQGAT-diff for just a few iterations shows an efficient
distribution shift, further improving performance throughout data sets.
Finally, we test our model on the Crossdocked data set for structure-based de
novo ligand generation, underlining the importance of our findings showing
state-of-the-art performance on Vina docking scores.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00116">Certified Robustness via Dynamic Margin Maximization and Improved Lipschitz Regularization. (arXiv:2310.00116v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fazlyab_M/0/1/0/all/0/1">Mahyar Fazlyab</a>, <a href="http://arxiv.org/find/cs/1/au:+Entesari_T/0/1/0/all/0/1">Taha Entesari</a>, <a href="http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1">Aniket Roy</a>, <a href="http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1">Rama Chellappa</a></p>
<p>To improve the robustness of deep classifiers against adversarial
perturbations, many approaches have been proposed, such as designing new
architectures with better robustness properties (e.g., Lipschitz-capped
networks), or modifying the training process itself (e.g., min-max
optimization, constrained learning, or regularization). These approaches,
however, might not be effective at increasing the margin in the input (feature)
space. As a result, there has been an increasing interest in developing
training procedures that can directly manipulate the decision boundary in the
input space. In this paper, we build upon recent developments in this category
by developing a robust training algorithm whose objective is to increase the
margin in the output (logit) space while regularizing the Lipschitz constant of
the model along vulnerable directions. We show that these two objectives can
directly promote larger margins in the input space. To this end, we develop a
scalable method for calculating guaranteed differentiable upper bounds on the
Lipschitz constant of neural networks accurately and efficiently. The relative
accuracy of the bounds prevents excessive regularization and allows for more
direct manipulation of the decision boundary. Furthermore, our Lipschitz
bounding algorithm exploits the monotonicity and Lipschitz continuity of the
activation layers, and the resulting bounds can be used to design new layers
with controllable bounds on their Lipschitz constant. Experiments on the MNIST,
CIFAR-10, and Tiny-ImageNet data sets verify that our proposed algorithm
obtains competitively improved results compared to the state-of-the-art.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00574">YFlows: Systematic Dataflow Exploration and Code Generation for Efficient Neural Network Inference using SIMD Architectures on CPUs. (arXiv:2310.00574v3 [cs.AR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1">Cyrus Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Hassman_Z/0/1/0/all/0/1">Zack Hassman</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">Ruize Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1">Dhirpal Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Richard_V/0/1/0/all/0/1">Vaugnn Richard</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yanjing Li</a></p>
<p>We address the challenges associated with deploying neural networks on CPUs,
with a particular focus on minimizing inference time while maintaining
accuracy. Our novel approach is to use the dataflow (i.e., computation order)
of a neural network to explore data reuse opportunities using heuristic-guided
analysis and a code generation framework, which enables exploration of various
Single Instruction, Multiple Data (SIMD) implementations to achieve optimized
neural network execution. Our results demonstrate that the dataflow that keeps
outputs in SIMD registers while also maximizing both input and weight reuse
consistently yields the best performance for a wide variety of inference
workloads, achieving up to 3x speedup for 8-bit neural networks, and up to 4.8x
speedup for binary neural networks, respectively, over the optimized
implementations of neural networks today.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00692">The Noise Geometry of Stochastic Gradient Descent: A Quantitative and Analytical Characterization. (arXiv:2310.00692v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Mingze Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1">Lei Wu</a></p>
<p>Empirical studies have demonstrated that the noise in stochastic gradient
descent (SGD) aligns favorably with the local geometry of loss landscape.
However, theoretical and quantitative explanations for this phenomenon remain
sparse. In this paper, we offer a comprehensive theoretical investigation into
the aforementioned {\em noise geometry} for over-parameterized linear (OLMs)
models and two-layer neural networks. We scrutinize both average and
directional alignments, paying special attention to how factors like sample
size and input data degeneracy affect the alignment strength. As a specific
application, we leverage our noise geometry characterizations to study how SGD
escapes from sharp minima, revealing that the escape direction has significant
components along flat directions. This is in stark contrast to GD, which
escapes only along the sharpest directions. To substantiate our theoretical
findings, both synthetic and real-world experiments are provided.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01225">A path-norm toolkit for modern networks: consequences, promises and challenges. (arXiv:2310.01225v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Gonon_A/0/1/0/all/0/1">Antoine Gonon</a>, <a href="http://arxiv.org/find/stat/1/au:+Brisebarre_N/0/1/0/all/0/1">Nicolas Brisebarre</a>, <a href="http://arxiv.org/find/stat/1/au:+Riccietti_E/0/1/0/all/0/1">Elisa Riccietti</a>, <a href="http://arxiv.org/find/stat/1/au:+Gribonval_R/0/1/0/all/0/1">R&#xe9;mi Gribonval</a></p>
<p>This work introduces the first toolkit around path-norms that is fully able
to encompass general DAG ReLU networks with biases, skip connections and any
operation based on the extraction of order statistics: max pooling, GroupSort
etc. This toolkit notably allows us to establish generalization bounds for
modern neural networks that are not only the most widely applicable path-norm
based ones, but also recover or beat the sharpest known bounds of this type.
These extended path-norms further enjoy the usual benefits of path-norms: ease
of computation, invariance under the symmetries of the network, and improved
sharpness on feedforward networks compared to the product of operators' norms,
another complexity measure most commonly used.
</p>
<p>The versatility of the toolkit and its ease of implementation allow us to
challenge the concrete promises of path-norm-based generalization bounds, by
numerically evaluating the sharpest known bounds for ResNets on ImageNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01769">How Over-Parameterization Slows Down Gradient Descent in Matrix Sensing: The Curses of Symmetry and Initialization. (arXiv:2310.01769v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiong_N/0/1/0/all/0/1">Nuoya Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1">Lijun Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1">Simon S. Du</a></p>
<p>This paper rigorously shows how over-parameterization changes the convergence
behaviors of gradient descent (GD) for the matrix sensing problem, where the
goal is to recover an unknown low-rank ground-truth matrix from near-isotropic
linear measurements. First, we consider the symmetric setting with the
symmetric parameterization where $M^* \in \mathbb{R}^{n \times n}$ is a
positive semi-definite unknown matrix of rank $r \ll n$, and one uses a
symmetric parameterization $XX^\top$ to learn $M^*$. Here $X \in \mathbb{R}^{n
\times k}$ with $k &gt; r$ is the factor matrix. We give a novel $\Omega (1/T^2)$
lower bound of randomly initialized GD for the over-parameterized case ($k &gt;r$)
where $T$ is the number of iterations. This is in stark contrast to the
exact-parameterization scenario ($k=r$) where the convergence rate is $\exp
(-\Omega (T))$. Next, we study asymmetric setting where $M^* \in
\mathbb{R}^{n_1 \times n_2}$ is the unknown matrix of rank $r \ll
\min\{n_1,n_2\}$, and one uses an asymmetric parameterization $FG^\top$ to
learn $M^*$ where $F \in \mathbb{R}^{n_1 \times k}$ and $G \in \mathbb{R}^{n_2
\times k}$. Building on prior work, we give a global exact convergence result
of randomly initialized GD for the exact-parameterization case ($k=r$) with an
$\exp (-\Omega(T))$ rate. Furthermore, we give the first global exact
convergence result for the over-parameterization case ($k&gt;r$) with an
$\exp(-\Omega(\alpha^2 T))$ rate where $\alpha$ is the initialization scale.
This linear convergence result in the over-parameterization case is especially
significant because one can apply the asymmetric parameterization to the
symmetric setting to speed up from $\Omega (1/T^2)$ to linear convergence. On
the other hand, we propose a novel method that only modifies one step of GD and
obtains a convergence rate independent of $\alpha$, recovering the rate in the
exact-parameterization case.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02428">EGraFFBench: Evaluation of Equivariant Graph Neural Network Force Fields for Atomistic Simulations. (arXiv:2310.02428v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bihani_V/0/1/0/all/0/1">Vaibhav Bihani</a>, <a href="http://arxiv.org/find/cs/1/au:+Pratiush_U/0/1/0/all/0/1">Utkarsh Pratiush</a>, <a href="http://arxiv.org/find/cs/1/au:+Mannan_S/0/1/0/all/0/1">Sajid Mannan</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_T/0/1/0/all/0/1">Tao Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhimin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Miret_S/0/1/0/all/0/1">Santiago Miret</a>, <a href="http://arxiv.org/find/cs/1/au:+Micoulaut_M/0/1/0/all/0/1">Matthieu Micoulaut</a>, <a href="http://arxiv.org/find/cs/1/au:+Smedskjaer_M/0/1/0/all/0/1">Morten M Smedskjaer</a>, <a href="http://arxiv.org/find/cs/1/au:+Ranu_S/0/1/0/all/0/1">Sayan Ranu</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishnan_N/0/1/0/all/0/1">N M Anoop Krishnan</a></p>
<p>Equivariant graph neural networks force fields (EGraFFs) have shown great
promise in modelling complex interactions in atomic systems by exploiting the
graphs' inherent symmetries. Recent works have led to a surge in the
development of novel architectures that incorporate equivariance-based
inductive biases alongside architectural innovations like graph transformers
and message passing to model atomic interactions. However, thorough evaluations
of these deploying EGraFFs for the downstream task of real-world atomistic
simulations, is lacking. To this end, here we perform a systematic benchmarking
of 6 EGraFF algorithms (NequIP, Allegro, BOTNet, MACE, Equiformer, TorchMDNet),
with the aim of understanding their capabilities and limitations for realistic
atomistic simulations. In addition to our thorough evaluation and analysis on
eight existing datasets based on the benchmarking literature, we release two
new benchmark datasets, propose four new metrics, and three challenging tasks.
The new datasets and tasks evaluate the performance of EGraFF to
out-of-distribution data, in terms of different crystal structures,
temperatures, and new molecules. Interestingly, evaluation of the EGraFF models
based on dynamic simulations reveals that having a lower error on energy or
force does not guarantee stable or reliable simulation or faithful replication
of the atomic structures. Moreover, we find that no model clearly outperforms
other models on all datasets and tasks. Importantly, we show that the
performance of all the models on out-of-distribution datasets is unreliable,
pointing to the need for the development of a foundation model for force fields
that can be used in real-world simulations. In summary, this work establishes a
rigorous framework for evaluating machine learning force fields in the context
of atomic simulations and points to open research challenges within this
domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02970">Fast, Expressive SE$(n)$ Equivariant Networks through Weight-Sharing in Position-Orientation Space. (arXiv:2310.02970v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bekkers_E/0/1/0/all/0/1">Erik J Bekkers</a>, <a href="http://arxiv.org/find/cs/1/au:+Vadgama_S/0/1/0/all/0/1">Sharvaree Vadgama</a>, <a href="http://arxiv.org/find/cs/1/au:+Hesselink_R/0/1/0/all/0/1">Rob D Hesselink</a>, <a href="http://arxiv.org/find/cs/1/au:+Linden_P/0/1/0/all/0/1">Putri A van der Linden</a>, <a href="http://arxiv.org/find/cs/1/au:+Romero_D/0/1/0/all/0/1">David W Romero</a></p>
<p>Based on the theory of homogeneous spaces we derive \textit{geometrically
optimal edge attributes} to be used within the flexible message passing
framework. We formalize the notion of weight sharing in convolutional networks
as the sharing of message functions over point-pairs that should be treated
equally. We define equivalence classes of point-pairs that are identical up to
a transformation in the group and derive attributes that uniquely identify
these classes. Weight sharing is then obtained by conditioning message
functions on these attributes. As an application of the theory, we develop an
efficient equivariant group convolutional network for processing 3D point
clouds. The theory of homogeneous spaces tells us how to do group convolutions
with feature maps over the homogeneous space of positions $\mathbb{R}^3$,
position and orientations $\mathbb{R}^3 {\times} S^2$, and the group SE$(3)$
itself. Among these, $\mathbb{R}^3 {\times} S^2$ is an optimal choice due to
the ability to represent directional information, which $\mathbb{R}^3$ methods
cannot, and it significantly enhances computational efficiency compared to
indexing features on the full SE$(3)$ group. We empirically support this claim
by reaching state-of-the-art results -- in accuracy and speed -- on three
different benchmarks: interatomic potential energy prediction, trajectory
forecasting in N-body systems, and generating molecules via equivariant
diffusion models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04483">Reward Dropout Improves Control: Bi-objective Perspective on Reinforced LM. (arXiv:2310.04483v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1">Changhun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_C/0/1/0/all/0/1">Chiehyeon Lim</a></p>
<p>We study the theoretical aspects of Reinforced Language Models (RLMs) from a
bi-objective optimization perspective. Specifically, we consider the RLMs as a
Pareto optimization problem that maximizes the two conflicting objectives,
i.e., reward objective and likelihood objectives, simultaneously. Our main
contribution consists of three parts. First, we establish the theoretical
foundations of RLM as a Pareto optimization problem by presenting Reward Upper
BOund (RUBO) and Pareto optimality. Our theoretical outcomes are supported by
not only deductive proofs but also empirical results. Second, we propose Reward
Dropout, a simple yet powerful method that guarantees to improve a bi-objective
optimization of RLM. Lastly, we demonstrate that the Reward Dropout is
consistently effective across five benchmark datasets and four benchmark LLMs,
meaning that the Reward Dropout significantly improves the optimization
performance of RLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05052">Accurate battery lifetime prediction across diverse aging conditions with deep learning. (arXiv:2310.05052v3 [eess.SP] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1">Han Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1">Yuqi Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Zheng_S/0/1/0/all/0/1">Shun Zheng</a>, <a href="http://arxiv.org/find/eess/1/au:+Lu_Z/0/1/0/all/0/1">Ziheng Lu</a>, <a href="http://arxiv.org/find/eess/1/au:+Gui_X/0/1/0/all/0/1">Xiaofan Gui</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_W/0/1/0/all/0/1">Wei Xu</a>, <a href="http://arxiv.org/find/eess/1/au:+Bian_J/0/1/0/all/0/1">Jiang Bian</a></p>
<p>Accurately predicting the lifetime of battery cells in early cycles holds
tremendous value for battery research and development as well as numerous
downstream applications. This task is rather challenging because diverse
conditions, such as electrode materials, operating conditions, and working
environments, collectively determine complex capacity-degradation behaviors.
However, current prediction methods are developed and validated under limited
aging conditions, resulting in questionable adaptability to varied aging
conditions and an inability to fully benefit from historical data collected
under different conditions. Here we introduce a universal deep learning
approach that is capable of accommodating various aging conditions and
facilitating effective learning under low-resource conditions by leveraging
data from rich conditions. Our key finding is that incorporating inter-cell
feature differences, rather than solely considering single-cell
characteristics, significantly increases the accuracy of battery lifetime
prediction and its cross-condition robustness. Accordingly, we develop a
holistic learning framework accommodating both single-cell and inter-cell
modeling. A comprehensive benchmark is built for evaluation, encompassing 401
battery cells utilizing 5 prevalent electrode materials across 168 cycling
conditions. We demonstrate remarkable capabilities in learning across diverse
aging conditions, exclusively achieving 10% prediction error using the first
100 cycles, and in facilitating low-resource learning, almost halving the error
of single-cell modeling in many cases. More broadly, by breaking the learning
boundaries among different aging conditions, our approach could significantly
accelerate the development and optimization of lithium-ion batteries.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05898">Lion Secretly Solves Constrained Optimization: As Lyapunov Predicts. (arXiv:2310.05898v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lizhang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1">Kaizhao Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qiang Liu</a></p>
<p>Lion (Evolved Sign Momentum), a new optimizer discovered through program
search, has shown promising results in training large AI models. It performs
comparably or favorably to AdamW but with greater memory efficiency. As we can
expect from the results of a random search program, Lion incorporates elements
from several existing algorithms, including signed momentum, decoupled weight
decay, Polak, and Nesterov momentum, but does not fit into any existing
category of theoretically grounded optimizers. Thus, even though Lion appears
to perform well as a general-purpose optimizer for a wide range of tasks, its
theoretical basis remains uncertain. This lack of theoretical clarity limits
opportunities to further enhance and expand Lion's efficacy.
</p>
<p>This work aims to demystify Lion. Based on both continuous-time and
discrete-time analysis, we demonstrate that Lion is a theoretically novel and
principled approach for minimizing a general loss function $f(x)$ while
enforcing a bound constraint $\|x\|_\infty \leq 1/\lambda$. Lion achieves this
through the incorporation of decoupled weight decay, where $\lambda$ represents
the weight decay coefficient. Our analysis is made possible by the development
of a new Lyapunov function for the Lion updates. It applies to a broader family
of Lion-$\kappa$ algorithms, where the $\text{sign}(\cdot)$ operator in Lion is
replaced by the subgradient of a convex function $\kappa$, leading to the
solution of a general composite optimization problem of $\min_x f(x) +
\kappa^*(x)$. Our findings provide valuable insights into the dynamics of Lion
and pave the way for further improvements and extensions of Lion-related
algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.06110">Grokking as the Transition from Lazy to Rich Training Dynamics. (arXiv:2310.06110v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Kumar_T/0/1/0/all/0/1">Tanishq Kumar</a>, <a href="http://arxiv.org/find/stat/1/au:+Bordelon_B/0/1/0/all/0/1">Blake Bordelon</a>, <a href="http://arxiv.org/find/stat/1/au:+Gershman_S/0/1/0/all/0/1">Samuel J. Gershman</a>, <a href="http://arxiv.org/find/stat/1/au:+Pehlevan_C/0/1/0/all/0/1">Cengiz Pehlevan</a></p>
<p>We propose that the grokking phenomenon, where the train loss of a neural
network decreases much earlier than its test loss, can arise due to a neural
network transitioning from lazy training dynamics to a rich, feature learning
regime. To illustrate this mechanism, we study the simple setting of vanilla
gradient descent on a polynomial regression problem with a two layer neural
network which exhibits grokking without regularization in a way that cannot be
explained by existing theories. We identify sufficient statistics for the test
loss of such a network, and tracking these over training reveals that grokking
arises in this setting when the network first attempts to fit a kernel
regression solution with its initial features, followed by late-time feature
learning where a generalizing solution is identified after train loss is
already low. We provide an asymptotic theoretical description of the grokking
dynamics in this model using dynamical mean field theory (DMFT) for high
dimensional data. We find that the key determinants of grokking are the rate of
feature learning -- which can be controlled precisely by parameters that scale
the network output -- and the alignment of the initial features with the target
function $y(x)$. We argue this delayed generalization arises when (1) the top
eigenvectors of the initial neural tangent kernel and the task labels $y(x)$
are misaligned, but (2) the dataset size is large enough so that it is possible
for the network to generalize eventually, but not so large that train loss
perfectly tracks test loss at all epochs, and (3) the network begins training
in the lazy regime so does not learn features immediately. We conclude with
evidence that this transition from lazy (linear model) to rich training
(feature learning) can control grokking in more general settings, like on
MNIST, one-layer Transformers, and student-teacher networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.10970">Deep Learning based Spatially Dependent Acoustical Properties Recovery. (arXiv:2310.10970v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1">Ruixian Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gerstoft_P/0/1/0/all/0/1">Peter Gerstoft</a></p>
<p>The physics-informed neural network (PINN) is capable of recovering partial
differential equation (PDE) coefficients that remain constant throughout the
spatial domain directly from physical measurements. In this work, we propose a
spatially dependent physics-informed neural network (SD-PINN), which enables
the recovery of coefficients in spatially-dependent PDEs using a single neural
network, eliminating the requirement for domain-specific physical expertise. We
apply the SD-PINN to spatially-dependent wave equation coefficients recovery to
reveal the spatial distribution of acoustical properties in the inhomogeneous
medium. The proposed method exhibits robustness to noise owing to the
incorporation of a loss function for the physical constraint that the assumed
PDE must be satisfied. For the coefficients recovery of spatially
two-dimensional PDEs, we store the PDE coefficients at all locations in the 2D
region of interest into a matrix and incorporate the low-rank assumption for
such a matrix to recover the coefficients at locations without available
measurements.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.11122">Sensitivity-Aware Amortized Bayesian Inference. (arXiv:2310.11122v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Elsemuller_L/0/1/0/all/0/1">Lasse Elsem&#xfc;ller</a>, <a href="http://arxiv.org/find/stat/1/au:+Olischlager_H/0/1/0/all/0/1">Hans Olischl&#xe4;ger</a>, <a href="http://arxiv.org/find/stat/1/au:+Schmitt_M/0/1/0/all/0/1">Marvin Schmitt</a>, <a href="http://arxiv.org/find/stat/1/au:+Burkner_P/0/1/0/all/0/1">Paul-Christian B&#xfc;rkner</a>, <a href="http://arxiv.org/find/stat/1/au:+Kothe_U/0/1/0/all/0/1">Ullrich K&#xf6;the</a>, <a href="http://arxiv.org/find/stat/1/au:+Radev_S/0/1/0/all/0/1">Stefan T. Radev</a></p>
<p>Bayesian inference is a powerful framework for making probabilistic
inferences and decisions under uncertainty. Fundamental choices in modern
Bayesian workflows concern the specification of the likelihood function and
prior distributions, the posterior approximator, and the data. Each choice can
significantly influence model-based inference and subsequent decisions, thereby
necessitating sensitivity analysis. In this work, we propose a multifaceted
approach to integrate sensitivity analyses into amortized Bayesian inference
(ABI, i.e., simulation-based inference with neural networks). First, we utilize
weight sharing to encode the structural similarities between alternative
likelihood and prior specifications in the training process with minimal
computational overhead. Second, we leverage the rapid inference of neural
networks to assess sensitivity to various data perturbations or pre-processing
procedures. In contrast to most other Bayesian approaches, both steps
circumvent the costly bottleneck of refitting the model(s) for each choice of
likelihood, prior, or dataset. Finally, we propose to use neural network
ensembles to evaluate variation in results induced by unreliable approximation
on unseen data. We demonstrate the effectiveness of our method in applied
modeling problems, ranging from the estimation of disease outbreak dynamics and
global warming thresholds to the comparison of human decision-making models.
Our experiments showcase how our approach enables practitioners to effectively
unveil hidden relationships between modeling choices and inferential
conclusions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.13102">Particle Guidance: non-I.I.D. Diverse Sampling with Diffusion Models. (arXiv:2310.13102v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Corso_G/0/1/0/all/0/1">Gabriele Corso</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yilun Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bortoli_V/0/1/0/all/0/1">Valentin de Bortoli</a>, <a href="http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1">Regina Barzilay</a>, <a href="http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1">Tommi Jaakkola</a></p>
<p>In light of the widespread success of generative models, a significant amount
of research has gone into speeding up their sampling time. However, generative
models are often sampled multiple times to obtain a diverse set incurring a
cost that is orthogonal to sampling time. We tackle the question of how to
improve diversity and sample efficiency by moving beyond the common assumption
of independent samples. We propose particle guidance, an extension of
diffusion-based generative sampling where a joint-particle time-evolving
potential enforces diversity. We analyze theoretically the joint distribution
that particle guidance generates, how to learn a potential that achieves
optimal diversity, and the connections with methods in other disciplines.
Empirically, we test the framework both in the setting of conditional image
generation, where we are able to increase diversity without affecting quality,
and molecular conformer generation, where we reduce the state-of-the-art median
error by 13% on average.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.14790">Weighted Joint Maximum Mean Discrepancy Enabled Multi-Source-Multi-Target Unsupervised Domain Adaptation Fault Diagnosis. (arXiv:2310.14790v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zixuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1">Haoran Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haibo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1">Bo Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Butala_M/0/1/0/all/0/1">Mark D. Butala</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1">Weiming Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hongwei Wang</a></p>
<p>Despite the remarkable results that can be achieved by data-driven
intelligent fault diagnosis techniques, they presuppose the same distribution
of training and test data as well as sufficient labeled data. Various operating
states often exist in practical scenarios, leading to the problem of domain
shift that hinders the effectiveness of fault diagnosis. While recent
unsupervised domain adaptation methods enable cross-domain fault diagnosis,
they struggle to effectively utilize information from multiple source domains
and achieve effective diagnosis faults in multiple target domains
simultaneously. In this paper, we innovatively proposed a weighted joint
maximum mean discrepancy enabled multi-source-multi-target unsupervised domain
adaptation (WJMMD-MDA), which realizes domain adaptation under
multi-source-multi-target scenarios in the field of fault diagnosis for the
first time. The proposed method extracts sufficient information from multiple
labeled source domains and achieves domain alignment between source and target
domains through an improved weighted distance loss. As a result,
domain-invariant and discriminative features between multiple source and target
domains are learned with cross-domain fault diagnosis realized. The performance
of the proposed method is evaluated in comprehensive comparative experiments on
three datasets, and the experimental results demonstrate the superiority of
this method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.14948">Physics-Informed Graph Convolutional Networks: Towards a generalized framework for complex geometries. (arXiv:2310.14948v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chenaud_M/0/1/0/all/0/1">Marien Chenaud</a>, <a href="http://arxiv.org/find/cs/1/au:+Alves_J/0/1/0/all/0/1">Jos&#xe9; Alves</a>, <a href="http://arxiv.org/find/cs/1/au:+Magoules_F/0/1/0/all/0/1">Fr&#xe9;d&#xe9;ric Magoul&#xe8;s</a></p>
<p>Since the seminal work of [9] and their Physics-Informed neural networks
(PINNs), many efforts have been conducted towards solving partial differential
equations (PDEs) with Deep Learning models. However, some challenges remain,
for instance the extension of such models to complex three-dimensional
geometries, and a study on how such approaches could be combined to classical
numerical solvers. In this work, we justify the use of graph neural networks
for these problems, based on the similarity between these architectures and the
meshes used in traditional numerical techniques for solving partial
differential equations. After proving an issue with the Physics-Informed
framework for complex geometries, during the computation of PDE residuals, an
alternative procedure is proposed, by combining classical numerical solvers and
the Physics-Informed framework. Finally, we propose an implementation of this
approach, that we test on a three-dimensional problem on an irregular geometry.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.15386">Course Correcting Koopman Representations. (arXiv:2310.15386v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fathi_M/0/1/0/all/0/1">Mahan Fathi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gehring_C/0/1/0/all/0/1">Clement Gehring</a>, <a href="http://arxiv.org/find/cs/1/au:+Pilault_J/0/1/0/all/0/1">Jonathan Pilault</a>, <a href="http://arxiv.org/find/cs/1/au:+Kanaa_D/0/1/0/all/0/1">David Kanaa</a>, <a href="http://arxiv.org/find/cs/1/au:+Bacon_P/0/1/0/all/0/1">Pierre-Luc Bacon</a>, <a href="http://arxiv.org/find/cs/1/au:+Goroshin_R/0/1/0/all/0/1">Ross Goroshin</a></p>
<p>Koopman representations aim to learn features of nonlinear dynamical systems
(NLDS) which lead to linear dynamics in the latent space. Theoretically, such
features can be used to simplify many problems in modeling and control of NLDS.
In this work we study autoencoder formulations of this problem, and different
ways they can be used to model dynamics, specifically for future state
prediction over long horizons. We discover several limitations of predicting
future states in the latent space and propose an inference-time mechanism,
which we refer to as Periodic Reencoding, for faithfully capturing long term
dynamics. We justify this method both analytically and empirically via
experiments in low and high dimensional NLDS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.15975">Data-driven Traffic Simulation: A Comprehensive Review. (arXiv:2310.15975v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Di Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1">Meixin Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Hao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xuesong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yinhai Wang</a></p>
<p>Autonomous vehicles (AVs) have the potential to significantly revolutionize
society by providing a secure and efficient mode of transportation. Recent
years have witnessed notable advancements in autonomous driving perception and
prediction, but the challenge of validating the performance of AVs remains
largely unresolved. Data-driven microscopic traffic simulation has become an
important tool for autonomous driving testing due to 1) availability of
high-fidelity traffic data; 2) its advantages of enabling large-scale testing
and scenario reproducibility; and 3) its potential in reactive and realistic
traffic simulation. However, a comprehensive review of this topic is currently
lacking. This paper aims to fill this gap by summarizing relevant studies. The
primary objective of this paper is to review current research efforts and
provide a futuristic perspective that will benefit future developments in the
field. It introduces the general issues of data-driven traffic simulation and
outlines key concepts and terms. After overviewing traffic simulation, various
datasets and evaluation metrics commonly used are reviewed. The paper then
offers a comprehensive evaluation of imitation learning, reinforcement
learning, deep generative and deep learning methods, summarizing each and
analyzing their advantages and disadvantages in detail. Moreover, it evaluates
the state-of-the-art, existing challenges, and future research directions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17623">Proving Test Set Contamination in Black Box Language Models. (arXiv:2310.17623v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oren_Y/0/1/0/all/0/1">Yonatan Oren</a>, <a href="http://arxiv.org/find/cs/1/au:+Meister_N/0/1/0/all/0/1">Nicole Meister</a>, <a href="http://arxiv.org/find/cs/1/au:+Chatterji_N/0/1/0/all/0/1">Niladri Chatterji</a>, <a href="http://arxiv.org/find/cs/1/au:+Ladhak_F/0/1/0/all/0/1">Faisal Ladhak</a>, <a href="http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1">Tatsunori B. Hashimoto</a></p>
<p>Large language models are trained on vast amounts of internet data, prompting
concerns and speculation that they have memorized public benchmarks. Going from
speculation to proof of contamination is challenging, as the pretraining data
used by proprietary models are often not publicly accessible. We show that it
is possible to provide provable guarantees of test set contamination in
language models without access to pretraining data or model weights. Our
approach leverages the fact that when there is no data contamination, all
orderings of an exchangeable benchmark should be equally likely. In contrast,
the tendency for language models to memorize example order means that a
contaminated language model will find certain canonical orderings to be much
more likely than others. Our test flags potential contamination whenever the
likelihood of a canonically ordered benchmark dataset is significantly higher
than the likelihood after shuffling the examples. We demonstrate that our
procedure is sensitive enough to reliably prove test set contamination in
challenging situations, including models as small as 1.4 billion parameters, on
small test sets of only 1000 examples, and datasets that appear only a few
times in the pretraining corpus. Using our test, we audit five popular publicly
accessible language models for test set contamination and find little evidence
for pervasive contamination.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19653">Upgrading VAE Training With Unlimited Data Plans Provided by Diffusion Models. (arXiv:2310.19653v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Xiao_T/0/1/0/all/0/1">Tim Z. Xiao</a>, <a href="http://arxiv.org/find/stat/1/au:+Zenn_J/0/1/0/all/0/1">Johannes Zenn</a>, <a href="http://arxiv.org/find/stat/1/au:+Bamler_R/0/1/0/all/0/1">Robert Bamler</a></p>
<p>Variational autoencoders (VAEs) are popular models for representation
learning but their encoders are susceptible to overfitting (Cremer et al.,
2018) because they are trained on a finite training set instead of the true
(continuous) data distribution $p_{\mathrm{data}}(\mathbf{x})$. Diffusion
models, on the other hand, avoid this issue by keeping the encoder fixed. This
makes their representations less interpretable, but it simplifies training,
enabling accurate and continuous approximations of
$p_{\mathrm{data}}(\mathbf{x})$. In this paper, we show that overfitting
encoders in VAEs can be effectively mitigated by training on samples from a
pre-trained diffusion model. These results are somewhat unexpected as recent
findings (Alemohammad et al., 2023; Shumailov et al., 2023) observe a decay in
generative performance when models are trained on data generated by another
generative model. We analyze generalization performance, amortization gap, and
robustness of VAEs trained with our proposed method on three different data
sets. We find improvements in all metrics compared to both normal training and
conventional data augmentation methods, and we show that a modest amount of
samples from the diffusion model suffices to obtain these gains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.20457">FlexTrain: A Dynamic Training Framework for Heterogeneous Devices Environments. (arXiv:2310.20457v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Unsal_M/0/1/0/all/0/1">Mert Unsal</a>, <a href="http://arxiv.org/find/cs/1/au:+Maatouk_A/0/1/0/all/0/1">Ali Maatouk</a>, <a href="http://arxiv.org/find/cs/1/au:+Domenico_A/0/1/0/all/0/1">Antonio De Domenico</a>, <a href="http://arxiv.org/find/cs/1/au:+Piovesan_N/0/1/0/all/0/1">Nicola Piovesan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ayed_F/0/1/0/all/0/1">Fadhel Ayed</a></p>
<p>As deep learning models become increasingly large, they pose significant
challenges in heterogeneous devices environments. The size of deep learning
models makes it difficult to deploy them on low-power or resource-constrained
devices, leading to long inference times and high energy consumption. To
address these challenges, we propose FlexTrain, a framework that accommodates
the diverse storage and computational resources available on different devices
during the training phase. FlexTrain enables efficient deployment of deep
learning models, while respecting device constraints, minimizing communication
costs, and ensuring seamless integration with diverse devices. We demonstrate
the effectiveness of FlexTrain on the CIFAR-100 dataset, where a single global
model trained with FlexTrain can be easily deployed on heterogeneous devices,
saving training time and energy consumption. We also extend FlexTrain to the
federated learning setting, showing that our approach outperforms standard
federated learning benchmarks on both CIFAR-10 and CIFAR-100 datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00860">Zero Coordinate Shift: Whetted Automatic Differentiation for Physics-informed Operator Learning. (arXiv:2311.00860v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Leng_K/0/1/0/all/0/1">Kuangdai Leng</a>, <a href="http://arxiv.org/find/cs/1/au:+Shankar_M/0/1/0/all/0/1">Mallikarjun Shankar</a>, <a href="http://arxiv.org/find/cs/1/au:+Thiyagalingam_J/0/1/0/all/0/1">Jeyan Thiyagalingam</a></p>
<p>Automatic differentiation (AD) is a critical step in physics-informed machine
learning, required for computing the high-order derivatives of network output
w.r.t. coordinates of collocation points. In this paper, we present a novel and
lightweight algorithm to conduct AD for physics-informed operator learning,
which we call the trick of Zero Coordinate Shift (ZCS). Instead of making all
sampled coordinates as leaf variables, ZCS introduces only one scalar-valued
leaf variable for each spatial or temporal dimension, simplifying the wanted
derivatives from "many-roots-many-leaves" to "one-root-many-leaves" whereby
reverse-mode AD becomes directly utilisable. It has led to an outstanding
performance leap by avoiding the duplication of the computational graph along
the dimension of functions (physical parameters). ZCS is easy to implement with
current deep learning libraries; our own implementation is achieved by
extending the DeepXDE package. We carry out a comprehensive benchmark analysis
and several case studies, training physics-informed DeepONets to solve partial
differential equations (PDEs) without data. The results show that ZCS has
persistently reduced GPU memory consumption and wall time for training by an
order of magnitude, and such reduction factor scales with the number of
functions. As a low-level optimisation technique, ZCS imposes no restrictions
on data, physics (PDE) or network architecture and does not compromise training
results from any aspect.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01017">Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion. (arXiv:2311.01017v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lunjun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1">Yuwen Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Ze Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Casas_S/0/1/0/all/0/1">Sergio Casas</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1">Rui Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1">Raquel Urtasun</a></p>
<p>Learning world models can teach an agent how the world works in an
unsupervised manner. Even though it can be viewed as a special case of sequence
modeling, progress for scaling world models on robotic applications such as
autonomous driving has been somewhat less rapid than scaling language models
with Generative Pre-trained Transformers (GPT). We identify two reasons as
major bottlenecks: dealing with complex and unstructured observation space, and
having a scalable generative model. Consequently, we propose a novel world
modeling approach that first tokenizes sensor observations with VQVAE, then
predicts the future via discrete diffusion. To efficiently decode and denoise
tokens in parallel, we recast Masked Generative Image Transformer into the
discrete diffusion framework with a few simple changes, resulting in notable
improvement. When applied to learning world models on point cloud observations,
our model reduces prior SOTA Chamfer distance by more than 65% for 1s
prediction, and more than 50% for 3s prediction, across NuScenes, KITTI
Odometry, and Argoverse2 datasets. Our results demonstrate that discrete
diffusion on tokenized agent experience can unlock the power of GPT-like
unsupervised learning for robotic agents.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02198">Imitation Bootstrapped Reinforcement Learning. (arXiv:2311.02198v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1">Hengyuan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mirchandani_S/0/1/0/all/0/1">Suvir Mirchandani</a>, <a href="http://arxiv.org/find/cs/1/au:+Sadigh_D/0/1/0/all/0/1">Dorsa Sadigh</a></p>
<p>Despite the considerable potential of reinforcement learning (RL), robotics
control tasks predominantly rely on imitation learning (IL) owing to its better
sample efficiency. However, given the high cost of collecting extensive
demonstrations, RL is still appealing if it can utilize limited imitation data
for efficient autonomous self-improvement. Existing RL methods that utilize
demonstrations either initialize the replay buffer with demonstrations and
oversample them during RL training, which does not benefit from the
generalization potential of modern IL methods, or pretrain the RL policy with
IL on the demonstrations, which requires additional mechanisms to prevent
catastrophic forgetting during RL fine-tuning. We propose imitation
bootstrapped reinforcement learning (IBRL), a novel framework that first trains
an IL policy on a limited number of demonstrations and then uses it to propose
alternative actions for both online exploration and target value bootstrapping.
IBRL achieves SoTA performance and sample efficiency on 7 challenging sparse
reward continuous control tasks in simulation while learning directly from
pixels. As a highlight of our method, IBRL achieves $6.4\times$ higher success
rate than RLPD, a strong method that combines the idea of oversampling
demonstrations with modern RL improvements, under the budget of 10 demos and
100K interactions in the challenging PickPlaceCan task in the Robomimic
benchmark.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02679">Regret Analysis of Learning-Based Linear Quadratic Gaussian Control with Additive Exploration. (arXiv:2311.02679v2 [eess.SY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Athrey_A/0/1/0/all/0/1">Archith Athrey</a>, <a href="http://arxiv.org/find/eess/1/au:+Mazhar_O/0/1/0/all/0/1">Othmane Mazhar</a>, <a href="http://arxiv.org/find/eess/1/au:+Guo_M/0/1/0/all/0/1">Meichen Guo</a>, <a href="http://arxiv.org/find/eess/1/au:+Schutter_B/0/1/0/all/0/1">Bart De Schutter</a>, <a href="http://arxiv.org/find/eess/1/au:+Shi_S/0/1/0/all/0/1">Shengling Shi</a></p>
<p>In this paper, we analyze the regret incurred by a computationally efficient
exploration strategy, known as naive exploration, for controlling unknown
partially observable systems within the Linear Quadratic Gaussian (LQG)
framework. We introduce a two-phase control algorithm called LQG-NAIVE, which
involves an initial phase of injecting Gaussian input signals to obtain a
system model, followed by a second phase of an interplay between naive
exploration and control in an episodic fashion. We show that LQG-NAIVE achieves
a regret growth rate of $\tilde{\mathcal{O}}(\sqrt{T})$, i.e.,
$\mathcal{O}(\sqrt{T})$ up to logarithmic factors after $T$ time steps, and we
validate its performance through numerical simulations. Additionally, we
propose LQG-IF2E, which extends the exploration signal to a `closed-loop'
setting by incorporating the Fisher Information Matrix (FIM). We provide
compelling numerical evidence of the competitive performance of LQG-IF2E
compared to LQG-NAIVE.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02818">Signal Processing Meets SGD: From Momentum to Filter. (arXiv:2311.02818v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1">Zhipeng Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_G/0/1/0/all/0/1">Guisong Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiaqi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dazhou Li</a></p>
<p>In the field of deep learning, Stochastic Gradient Descent (SGD) and its
momentum-based variants are the predominant choices for optimization
algorithms. Despite all that, these momentum strategies, which accumulate
historical gradients by using a fixed $\beta$ hyperparameter to smooth the
optimization processing, often neglect the potential impact of the variance of
historical gradients on the current gradient estimation. In the gradient
variance during training, fluctuation indicates the objective function does not
meet the Lipschitz continuity condition at all time, which raises the
troublesome optimization problem. This paper aims to explore the potential
benefits of reducing the variance of historical gradients to make optimizer
converge to flat solutions. Moreover, we proposed a new optimization method
based on reducing the variance. We employed the Wiener filter theory to enhance
the first moment estimation of SGD, notably introducing an adaptive weight to
optimizer. Specifically, the adaptive weight dynamically changes along with
temporal fluctuation of gradient variance during deep learning model training.
Experimental results demonstrated our proposed adaptive weight optimizer, SGDF
(Stochastic Gradient Descent With Filter), can achieve satisfactory performance
compared with state-of-the-art optimizers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03348">Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation. (arXiv:2311.03348v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1">Rusheb Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Feuillade__Montixi_Q/0/1/0/all/0/1">Quentin Feuillade--Montixi</a>, <a href="http://arxiv.org/find/cs/1/au:+Pour_S/0/1/0/all/0/1">Soroush Pour</a>, <a href="http://arxiv.org/find/cs/1/au:+Tagade_A/0/1/0/all/0/1">Arush Tagade</a>, <a href="http://arxiv.org/find/cs/1/au:+Casper_S/0/1/0/all/0/1">Stephen Casper</a>, <a href="http://arxiv.org/find/cs/1/au:+Rando_J/0/1/0/all/0/1">Javier Rando</a></p>
<p>Despite efforts to align large language models to produce harmless responses,
they are still vulnerable to jailbreak prompts that elicit unrestricted
behaviour. In this work, we investigate persona modulation as a black-box
jailbreaking method to steer a target model to take on personalities that are
willing to comply with harmful instructions. Rather than manually crafting
prompts for each persona, we automate the generation of jailbreaks using a
language model assistant. We demonstrate a range of harmful completions made
possible by persona modulation, including detailed instructions for
synthesising methamphetamine, building a bomb, and laundering money. These
automated attacks achieve a harmful completion rate of 42.5% in GPT-4, which is
185 times larger than before modulation (0.23%). These prompts also transfer to
Claude 2 and Vicuna with harmful completion rates of 61.0% and 35.9%,
respectively. Our work reveals yet another vulnerability in commercial large
language models and highlights the need for more comprehensive safeguards.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03996">An Initialization Schema for Neuronal Networks on Tabular Data. (arXiv:2311.03996v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fuhl_W/0/1/0/all/0/1">Wolfgang Fuhl</a></p>
<p>Nowadays, many modern applications require heterogeneous tabular data, which
is still a challenging task in terms of regression and classification. Many
approaches have been proposed to adapt neural networks for this task, but
still, boosting and bagging of decision trees are the best-performing methods
for this task. In this paper, we show that a binomial initialized neural
network can be used effectively on tabular data. The proposed approach shows a
simple but effective approach for initializing the first hidden layer in neural
networks. We also show that this initializing schema can be used to jointly
train ensembles by adding gradient masking to batch entries and using the
binomial initialization for the last layer in a neural network. For this
purpose, we modified the hinge binary loss and the soft max loss to make them
applicable for joint ensemble training. We evaluate our approach on multiple
public datasets and showcase the improved performance compared to other neural
network-based approaches. In addition, we discuss the limitations and possible
further research of our approach for improving the applicability of neural
networks to tabular data.
</p>
<p>Link:
https://es-cloud.cs.uni-tuebingen.de/d/8e2ab8c3fdd444e1a135/?p=%2FInitializationNeuronalNetworksTabularData&amp;mode=list
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04517">Strategies for Parallelizing the Big-Means Algorithm: A Comprehensive Tutorial for Effective Big Data Clustering. (arXiv:2311.04517v2 [cs.DC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mussabayev_R/0/1/0/all/0/1">Ravil Mussabayev</a>, <a href="http://arxiv.org/find/cs/1/au:+Mussabayev_R/0/1/0/all/0/1">Rustam Mussabayev</a></p>
<p>This study focuses on the optimization of the Big-means algorithm for
clustering large-scale datasets, exploring four distinct parallelization
strategies. We conducted extensive experiments to assess the computational
efficiency, scalability, and clustering performance of each approach, revealing
their benefits and limitations. The paper also delves into the trade-offs
between computational efficiency and clustering quality, examining the impacts
of various factors. Our insights provide practical guidance on selecting the
best parallelization strategy based on available resources and dataset
characteristics, contributing to a deeper understanding of parallelization
techniques for the Big-means algorithm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04599">Explainable artificial intelligence model for identifying Market Value in Professional Soccer Players. (arXiv:2311.04599v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1">Chunyang Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shaoliang Zhang</a></p>
<p>This study introduces an advanced machine learning method for predicting
soccer players' market values, combining ensemble models and the Shapley
Additive Explanations (SHAP) for interpretability. Utilizing data from about
12,000 players from Sofifa, the Boruta algorithm streamlined feature selection.
The Gradient Boosting Decision Tree (GBDT) model excelled in predictive
accuracy, with an R-squared of 0.901 and a Root Mean Squared Error (RMSE) of
3,221,632.175. Player attributes in skills, fitness, and cognitive areas
significantly influenced market value. These insights aid sports industry
stakeholders in player valuation. However, the study has limitations, like
underestimating superstar players' values and needing larger datasets. Future
research directions include enhancing the model's applicability and exploring
value prediction in various contexts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04760">Towards Open-world Cross-Domain Sequential Recommendation: A Model-Agnostic Contrastive Denoising Approach. (arXiv:2311.04760v2 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Wujiang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ning_X/0/1/0/all/0/1">Xuying Ning</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1">Wenfang Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ha_M/0/1/0/all/0/1">Mingming Ha</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1">Qiongxu Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_Q/0/1/0/all/0/1">Qianqiao Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_X/0/1/0/all/0/1">Xuewen Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Linxun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1">Bing Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1">Minnan Luo</a></p>
<p>Cross-domain sequential recommendation (CDSR) aims to address the data
sparsity problems that exist in traditional sequential recommendation (SR)
systems.
</p>
<p>The existing approaches aim to design a specific cross-domain unit that can
transfer and propagate information across multiple domains by relying on
overlapping users with abundant behaviors. However, in real-world recommender
systems, CDSR scenarios usually consist of a majority of long-tailed users with
sparse behaviors and cold-start users who only exist in one domain. This leads
to a drop in the performance of existing CDSR methods in the real-world
industry platform. Therefore, improving the consistency and effectiveness of
models in open-world CDSR scenarios is crucial for constructing CDSR models
(\textit{1st} CH). Recently, some SR approaches have utilized auxiliary
behaviors to complement the information for long-tailed users. However, these
multi-behavior SR methods cannot deliver promising performance in CDSR, as they
overlook the semantic gap between target and auxiliary behaviors, as well as
user interest deviation across domains (\textit{2nd} CH).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05784">Are &quot;Hierarchical&quot; Visual Representations Hierarchical?. (arXiv:2311.05784v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shen_E/0/1/0/all/0/1">Ethan Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1">Ali Farhadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kusupati_A/0/1/0/all/0/1">Aditya Kusupati</a></p>
<p>Learned visual representations often capture large amounts of semantic
information for accurate downstream applications. Human understanding of the
world is fundamentally grounded in hierarchy. To mimic this and further improve
representation capabilities, the community has explored "hierarchical" visual
representations that aim at modeling the underlying hierarchy of the visual
world. In this work, we set out to investigate if hierarchical visual
representations truly capture the human perceived hierarchy better than
standard learned representations. To this end, we create HierNet, a suite of 12
datasets spanning 3 kinds of hierarchy from the BREEDs subset of ImageNet.
After extensive evaluation of Hyperbolic and Matryoshka Representations across
training setups, we conclude that they do not capture hierarchy any better than
the standard representations but can assist in other aspects like search
efficiency and interpretability. Our benchmark and the datasets are
open-sourced at https://github.com/ethanlshen/HierNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05788">Structured Transforms Across Spaces with Cost-Regularized Optimal Transport. (arXiv:2311.05788v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sebbouh_O/0/1/0/all/0/1">Othmane Sebbouh</a>, <a href="http://arxiv.org/find/cs/1/au:+Cuturi_M/0/1/0/all/0/1">Marco Cuturi</a>, <a href="http://arxiv.org/find/cs/1/au:+Peyre_G/0/1/0/all/0/1">Gabriel Peyr&#xe9;</a></p>
<p>Matching a source to a target probability measure is often solved by
instantiating a linear optimal transport (OT) problem, parameterized by a
ground cost function that quantifies discrepancy between points. When these
measures live in the same metric space, the ground cost often defaults to its
distance. When instantiated across two different spaces, however, choosing that
cost in the absence of aligned data is a conundrum. As a result, practitioners
often resort to solving instead a quadratic Gromow-Wasserstein (GW) problem. We
exploit in this work a parallel between GW and cost-regularized OT, the
regularized minimization of a linear OT objective parameterized by a ground
cost. We use this cost-regularized formulation to match measures across two
different Euclidean spaces, where the cost is evaluated between transformed
source points and target points. We show that several quadratic OT problems
fall in this category, and consider enforcing structure in linear transform
(e.g. sparsity), by introducing structure-inducing regularizers. We provide a
proximal algorithm to extract such transforms from unaligned data, and
demonstrate its applicability to single-cell spatial transcriptomics/multiomics
matching tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06928">Attention for Causal Relationship Discovery from Biological Neural Dynamics. (arXiv:2311.06928v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Ziyu Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tabassum_A/0/1/0/all/0/1">Anika Tabassum</a>, <a href="http://arxiv.org/find/cs/1/au:+Kulkarni_S/0/1/0/all/0/1">Shruti Kulkarni</a>, <a href="http://arxiv.org/find/cs/1/au:+Mi_L/0/1/0/all/0/1">Lu Mi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kutz_J/0/1/0/all/0/1">J. Nathan Kutz</a>, <a href="http://arxiv.org/find/cs/1/au:+Shea_Brown_E/0/1/0/all/0/1">Eric Shea-Brown</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1">Seung-Hwan Lim</a></p>
<p>This paper explores the potential of the transformer models for learning
Granger causality in networks with complex nonlinear dynamics at every node, as
in neurobiological and biophysical networks. Our study primarily focuses on a
proof-of-concept investigation based on simulated neural dynamics, for which
the ground-truth causality is known through the underlying connectivity matrix.
For transformer models trained to forecast neuronal population dynamics, we
show that the cross attention module effectively captures the causal
relationship among neurons, with an accuracy equal or superior to that for the
most popular Granger causality analysis method. While we acknowledge that
real-world neurobiology data will bring further challenges, including dynamic
connectivity and unobserved variability, this research offers an encouraging
preliminary glimpse into the utility of the transformer model for causal
representation learning in neuroscience.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08228">Counterfactual Explanation for Regression via Disentanglement in Latent Space. (arXiv:2311.08228v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xuan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Broelemann_K/0/1/0/all/0/1">Klaus Broelemann</a>, <a href="http://arxiv.org/find/cs/1/au:+Kasneci_G/0/1/0/all/0/1">Gjergji Kasneci</a></p>
<p>Counterfactual Explanations (CEs) help address the question: How can the
factors that influence the prediction of a predictive model be changed to
achieve a more favorable outcome from a user's perspective? Thus, they bear the
potential to guide the user's interaction with AI systems since they represent
easy-to-understand explanations. To be applicable, CEs need to be realistic and
actionable. In the literature, various methods have been proposed to generate
CEs. However, the majority of research on CEs focuses on classification
problems where questions like "What should I do to get my rejected loan
approved?" are raised. In practice, answering questions like "What should I do
to increase my salary?" are of a more regressive nature. In this paper, we
introduce a novel method to generate CEs for a pre-trained regressor by first
disentangling the label-relevant from the label-irrelevant dimensions in the
latent space. CEs are then generated by combining the label-irrelevant
dimensions and the predefined output. The intuition behind this approach is
that the ideal counterfactual search should focus on the label-irrelevant
characteristics of the input and suggest changes toward target-relevant
characteristics. Searching in the latent space could help achieve this goal. We
show that our method maintains the characteristics of the query sample during
the counterfactual search. In various experiments, we demonstrate that the
proposed method is competitive based on different quality measures on image and
tabular datasets in regression problem settings. It efficiently returns results
closer to the original data manifold compared to three state-of-the-art
methods, which is essential for realistic high-dimensional machine learning
applications. Our code will be made available as an open-source package upon
the publication of this work.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08745">Using Stochastic Gradient Descent to Smooth Nonconvex Functions: Analysis of Implicit Graduated Optimization with Optimal Noise Scheduling. (arXiv:2311.08745v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sato_N/0/1/0/all/0/1">Naoki Sato</a>, <a href="http://arxiv.org/find/cs/1/au:+Iiduka_H/0/1/0/all/0/1">Hideaki Iiduka</a></p>
<p>The graduated optimization approach is a heuristic method for finding
globally optimal solutions for nonconvex functions and has been theoretically
analyzed in several studies. This paper defines a new family of nonconvex
functions for graduated optimization, discusses their sufficient conditions,
and provides a convergence analysis of the graduated optimization algorithm for
them. It shows that stochastic gradient descent (SGD) with mini-batch
stochastic gradients has the effect of smoothing the function, the degree of
which is determined by the learning rate and batch size. This finding provides
theoretical insights on why large batch sizes fall into sharp local minima, why
decaying learning rates and increasing batch sizes are superior to fixed
learning rates and batch sizes, and what the optimal learning rate scheduling
is. To the best of our knowledge, this is the first paper to provide a
theoretical explanation for these aspects. Moreover, a new graduated
optimization framework that uses a decaying learning rate and increasing batch
size is analyzed and experimental results of image classification that support
our theoretical findings are reported.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09740">Redefining Super-Resolution: Fine-mesh PDE predictions without classical simulations. (arXiv:2311.09740v2 [physics.flu-dyn] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Sarkar_R/0/1/0/all/0/1">Rajat Kumar Sarkar</a>, <a href="http://arxiv.org/find/physics/1/au:+Majumdar_R/0/1/0/all/0/1">Ritam Majumdar</a>, <a href="http://arxiv.org/find/physics/1/au:+Jadhav_V/0/1/0/all/0/1">Vishal Jadhav</a>, <a href="http://arxiv.org/find/physics/1/au:+Sakhinana_S/0/1/0/all/0/1">Sagar Srinivas Sakhinana</a>, <a href="http://arxiv.org/find/physics/1/au:+Runkana_V/0/1/0/all/0/1">Venkataramana Runkana</a></p>
<p>In Computational Fluid Dynamics (CFD), coarse mesh simulations offer
computational efficiency but often lack precision. Applying conventional
super-resolution to these simulations poses a significant challenge due to the
fundamental contrast between downsampling high-resolution images and
authentically emulating low-resolution physics. The former method conserves
more of the underlying physics, surpassing the usual constraints of real-world
scenarios. We propose a novel definition of super-resolution tailored for
PDE-based problems. Instead of simply downsampling from a high-resolution
dataset, we use coarse-grid simulated data as our input and predict fine-grid
simulated outcomes. Employing a physics-infused UNet upscaling method, we
demonstrate its efficacy across various 2D-CFD problems such as discontinuity
detection in Burger's equation, Methane combustion, and fouling in Industrial
heat exchangers. Our method enables the generation of fine-mesh solutions
bypassing traditional simulation, ensuring considerable computational saving
and fidelity to the original ground truth outcomes. Through diverse boundary
conditions during training, we further establish the robustness of our method,
paving the way for its broad applications in engineering and scientific CFD
solvers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10049">Inherently Interpretable Time Series Classification via Multiple Instance Learning. (arXiv:2311.10049v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Early_J/0/1/0/all/0/1">Joseph Early</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheung_G/0/1/0/all/0/1">Gavin KC Cheung</a>, <a href="http://arxiv.org/find/cs/1/au:+Cutajar_K/0/1/0/all/0/1">Kurt Cutajar</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1">Hanting Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Kandola_J/0/1/0/all/0/1">Jas Kandola</a>, <a href="http://arxiv.org/find/cs/1/au:+Twomey_N/0/1/0/all/0/1">Niall Twomey</a></p>
<p>Conventional Time Series Classification (TSC) methods are often black boxes
that obscure inherent interpretation of their decision-making processes. In
this work, we leverage Multiple Instance Learning (MIL) to overcome this issue,
and propose a new framework called MILLET: Multiple Instance Learning for
Locally Explainable Time series classification. We apply MILLET to existing
deep learning TSC models and show how they become inherently interpretable
without compromising (and in some cases, even improving) predictive
performance. We evaluate MILLET on 85 UCR TSC datasets and also present a novel
synthetic dataset that is specially designed to facilitate interpretability
evaluation. On these datasets, we show MILLET produces sparse explanations
quickly that are of higher quality than other well-known interpretability
methods. To the best of our knowledge, our work with MILLET, which is available
on GitHub (https://github.com/JAEarly/MILTimeSeriesClassification), is the
first to develop general MIL methods for TSC and apply them to an extensive
variety of domains
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10359">FIKIT: Priority-Based Real-time GPU Multi-tasking Scheduling with Kernel Identification. (arXiv:2311.10359v2 [cs.DC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1">Wenqing Wu</a></p>
<p>Highly parallelized workloads like machine learning training, inferences and
general HPC tasks are greatly accelerated using GPU devices. In a cloud
computing cluster, serving a GPU's computation power through multi-tasks
sharing is highly demanded since there are always more task requests than the
number of GPU available. Existing GPU sharing solutions focus on reducing
task-level waiting time or task-level switching costs when multiple jobs
competing for a single GPU. Non-stopped computation requests come with
different priorities, having non-symmetric impact on QoS for sharing a GPU
device. Existing work missed the kernel-level optimization opportunity brought
by this setting. To address this problem, we present a novel kernel-level
scheduling strategy called FIKIT: Filling Inter-kernel Idle Time. FIKIT
incorporates task-level priority information, fine-grained kernel
identification, and kernel measurement, allowing low priorities task's
execution during high priority task's inter-kernel idle time. Thereby, filling
the GPU's device runtime fully, and reduce overall GPU sharing impact to cloud
services. Across a set of ML models, the FIKIT based inference system
accelerated high priority tasks by 1.33 to 14.87 times compared to the JCT in
GPU sharing mode, and more than half of the cases are accelerated by more than
3.5 times. Alternatively, under preemptive sharing, the low-priority tasks have
a comparable to default GPU sharing mode JCT, with a 0.84 to 1 times ratio. We
further limit the kernel measurement and runtime fine-grained kernel scheduling
overhead to less than 10%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10642">Rethinking Attention: Exploring Shallow Feed-Forward Neural Networks as an Alternative to Attention Layers in Transformers. (arXiv:2311.10642v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bozic_V/0/1/0/all/0/1">Vukasin Bozic</a>, <a href="http://arxiv.org/find/cs/1/au:+Dordevic_D/0/1/0/all/0/1">Danilo Dordevic</a>, <a href="http://arxiv.org/find/cs/1/au:+Coppola_D/0/1/0/all/0/1">Daniele Coppola</a>, <a href="http://arxiv.org/find/cs/1/au:+Thommes_J/0/1/0/all/0/1">Joseph Thommes</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Sidak Pal Singh</a></p>
<p>This work presents an analysis of the effectiveness of using standard shallow
feed-forward networks to mimic the behavior of the attention mechanism in the
original Transformer model, a state-of-the-art architecture for
sequence-to-sequence tasks. We substitute key elements of the attention
mechanism in the Transformer with simple feed-forward networks, trained using
the original components via knowledge distillation. Our experiments, conducted
on the IWSLT2017 dataset, reveal the capacity of these "attentionless
Transformers" to rival the performance of the original architecture. Through
rigorous ablation studies, and experimenting with various replacement network
types and sizes, we offer insights that support the viability of our approach.
This not only sheds light on the adaptability of shallow feed-forward networks
in emulating attention mechanisms but also underscores their potential to
streamline complex architectures for sequence-to-sequence tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10986">EdgeFM: Leveraging Foundation Model for Open-set Learning on the Edge. (arXiv:2311.10986v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1">Bufang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1">Lixing He</a>, <a href="http://arxiv.org/find/cs/1/au:+Ling_N/0/1/0/all/0/1">Neiwen Ling</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1">Zhenyu Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_G/0/1/0/all/0/1">Guoliang Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Shuai_X/0/1/0/all/0/1">Xian Shuai</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1">Xiaozhe Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1">Xin Jiang</a></p>
<p>Deep Learning (DL) models have been widely deployed on IoT devices with the
help of advancements in DL algorithms and chips. However, the limited resources
of edge devices make these on-device DL models hard to be generalizable to
diverse environments and tasks. Although the recently emerged foundation models
(FMs) show impressive generalization power, how to effectively leverage the
rich knowledge of FMs on resource-limited edge devices is still not explored.
In this paper, we propose EdgeFM, a novel edge-cloud cooperative system with
open-set recognition capability. EdgeFM selectively uploads unlabeled data to
query the FM on the cloud and customizes the specific knowledge and
architectures for edge models. Meanwhile, EdgeFM conducts dynamic model
switching at run-time taking into account both data uncertainty and dynamic
network variations, which ensures the accuracy always close to the original FM.
We implement EdgeFM using two FMs on two edge platforms. We evaluate EdgeFM on
three public datasets and two self-collected datasets. Results show that EdgeFM
can reduce the end-to-end latency up to 3.2x and achieve 34.3% accuracy
increase compared with the baseline.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10996">BrainZ-BP: A Non-invasive Cuff-less Blood Pressure Estimation Approach Leveraging Brain Bio-impedance and Electrocardiogram. (arXiv:2311.10996v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1">Bufang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Le Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1">Wenxuan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1">Mengliang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hongxing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ning_X/0/1/0/all/0/1">Xinbao Ning</a></p>
<p>Accurate and continuous blood pressure (BP) monitoring is essential to the
early prevention of cardiovascular diseases. Non-invasive and cuff-less BP
estimation algorithm has gained much attention in recent years. Previous
studies have demonstrated that brain bio-impedance (BIOZ) is a promising
technique for non-invasive intracranial pressure (ICP) monitoring. Clinically,
treatment for patients with traumatic brain injuries (TBI) requires monitoring
the ICP and BP of patients simultaneously. Estimating BP by brain BIOZ directly
can reduce the number of sensors attached to the patients, thus improving their
comfort. To address the issues, in this study, we explore the feasibility of
leveraging brain BIOZ for BP estimation and propose a novel cuff-less BP
estimation approach called BrainZ-BP. Two electrodes are placed on the forehead
and occipital bone of the head in the anterior-posterior direction for brain
BIOZ measurement. Various features including pulse transit time and
morphological features of brain BIOZ are extracted and fed into four regression
models for BP estimation. Results show that the mean absolute error, root mean
square error, and correlation coefficient of random forest regression model are
2.17 mmHg, 3.91 mmHg, and 0.90 for systolic pressure estimation, and are 1.71
mmHg, 3.02 mmHg, and 0.89 for diastolic pressure estimation. The presented
BrainZ-BP can be applied in the brain BIOZ-based ICP monitoring scenario to
monitor BP simultaneously.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.11762">MUVO: A Multimodal Generative World Model for Autonomous Driving with Geometric Representations. (arXiv:2311.11762v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bogdoll_D/0/1/0/all/0/1">Daniel Bogdoll</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yitian Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zollner_J/0/1/0/all/0/1">J. Marius Z&#xf6;llner</a></p>
<p>Learning unsupervised world models for autonomous driving has the potential
to improve the reasoning capabilities of today's systems dramatically. However,
most work neglects the physical attributes of the world and focuses on sensor
data alone. We propose MUVO, a MUltimodal World Model with Geometric VOxel
Representations to address this challenge. We utilize raw camera and lidar data
to learn a sensor-agnostic geometric representation of the world, which can
directly be used by downstream tasks, such as planning. We demonstrate
multimodal future predictions and show that our geometric representation
improves the prediction quality of both camera images and lidar point clouds.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.11995">BrainWash: A Poisoning Attack to Forget in Continual Learning. (arXiv:2311.11995v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abbasi_A/0/1/0/all/0/1">Ali Abbasi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nooralinejad_P/0/1/0/all/0/1">Parsa Nooralinejad</a>, <a href="http://arxiv.org/find/cs/1/au:+Pirsiavash_H/0/1/0/all/0/1">Hamed Pirsiavash</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolouri_S/0/1/0/all/0/1">Soheil Kolouri</a></p>
<p>Continual learning has gained substantial attention within the deep learning
community, offering promising solutions to the challenging problem of
sequential learning. Yet, a largely unexplored facet of this paradigm is its
susceptibility to adversarial attacks, especially with the aim of inducing
forgetting. In this paper, we introduce "BrainWash," a novel data poisoning
method tailored to impose forgetting on a continual learner. By adding the
BrainWash noise to a variety of baselines, we demonstrate how a trained
continual learner can be induced to forget its previously learned tasks
catastrophically, even when using these continual learning baselines. An
important feature of our approach is that the attacker requires no access to
previous tasks' data and is armed merely with the model's current parameters
and the data belonging to the most recent task. Our extensive experiments
highlight the efficacy of BrainWash, showcasing degradation in performance
across various regularization-based continual learning methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12255">Exploring Time Granularity on Temporal Graphs for Dynamic Link Prediction in Real-world Networks. (arXiv:2311.12255v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1">Xiangjian Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pu_Y/0/1/0/all/0/1">Yanyi Pu</a></p>
<p>Dynamic Graph Neural Networks (DGNNs) have emerged as the predominant
approach for processing dynamic graph-structured data. However, the influence
of temporal information on model performance and robustness remains
insufficiently explored, particularly regarding how models address prediction
tasks with different time granularities. In this paper, we explore the impact
of time granularity when training DGNNs on dynamic graphs through extensive
experiments. We examine graphs derived from various domains and compare three
different DGNNs to the baseline model across four varied time granularities. We
mainly consider the interplay between time granularities, model architectures,
and negative sampling strategies to obtain general conclusions. Our results
reveal that a sophisticated memory mechanism and proper time granularity are
crucial for a DGNN to deliver competitive and robust performance in the dynamic
link prediction task. We also discuss drawbacks in considered models and
datasets and propose promising directions for future research on the time
granularity of temporal graphs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12379">Infinite forecast combinations based on Dirichlet process. (arXiv:2311.12379v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1">Yinuo Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1">Feng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1">Yanfei Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jue Wang</a></p>
<p>Forecast combination integrates information from various sources by
consolidating multiple forecast results from the target time series. Instead of
the need to select a single optimal forecasting model, this paper introduces a
deep learning ensemble forecasting model based on the Dirichlet process.
Initially, the learning rate is sampled with three basis distributions as
hyperparameters to convert the infinite mixture into a finite one. All
checkpoints are collected to establish a deep learning sub-model pool, and
weight adjustment and diversity strategies are developed during the combination
process. The main advantage of this method is its ability to generate the
required base learners through a single training process, utilizing the
decaying strategy to tackle the challenge posed by the stochastic nature of
gradient descent in determining the optimal learning rate. To ensure the
method's generalizability and competitiveness, this paper conducts an empirical
analysis using the weekly dataset from the M4 competition and explores
sensitivity to the number of models to be combined. The results demonstrate
that the ensemble model proposed offers substantial improvements in prediction
accuracy and stability compared to a single benchmark model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12564">Summary of the DISPLACE Challenge 2023 -- DIarization of SPeaker and LAnguage in Conversational Environments. (arXiv:2311.12564v2 [eess.AS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Baghel_S/0/1/0/all/0/1">Shikha Baghel</a>, <a href="http://arxiv.org/find/eess/1/au:+Ramoji_S/0/1/0/all/0/1">Shreyas Ramoji</a>, <a href="http://arxiv.org/find/eess/1/au:+Jain_S/0/1/0/all/0/1">Somil Jain</a>, <a href="http://arxiv.org/find/eess/1/au:+Chowdhuri_P/0/1/0/all/0/1">Pratik Roy Chowdhuri</a>, <a href="http://arxiv.org/find/eess/1/au:+Singh_P/0/1/0/all/0/1">Prachi Singh</a>, <a href="http://arxiv.org/find/eess/1/au:+Vijayasenan_D/0/1/0/all/0/1">Deepu Vijayasenan</a>, <a href="http://arxiv.org/find/eess/1/au:+Ganapathy_S/0/1/0/all/0/1">Sriram Ganapathy</a></p>
<p>In multi-lingual societies, where multiple languages are spoken in a small
geographic vicinity, informal conversations often involve mix of languages.
Existing speech technologies may be inefficient in extracting information from
such conversations, where the speech data is rich in diversity with multiple
languages and speakers. The DISPLACE (DIarization of SPeaker and LAnguage in
Conversational Environments) challenge constitutes an open-call for evaluating
and bench-marking the speaker and language diarization technologies on this
challenging condition. The challenge entailed two tracks: Track-1 focused on
speaker diarization (SD) in multilingual situations while, Track-2 addressed
the language diarization (LD) in a multi-speaker scenario. Both the tracks were
evaluated using the same underlying audio data. To facilitate this evaluation,
a real-world dataset featuring multilingual, multi-speaker conversational
far-field speech was recorded and distributed. Furthermore, a baseline system
was made available for both SD and LD task which mimicked the state-of-art in
these tasks. The challenge garnered a total of $42$ world-wide registrations
and received a total of $19$ combined submissions for Track-1 and Track-2. This
paper describes the challenge, details of the datasets, tasks, and the baseline
system. Additionally, the paper provides a concise overview of the submitted
systems in both tracks, with an emphasis given to the top performing systems.
The paper also presents insights and future perspectives for SD and LD tasks,
focusing on the key challenges that the systems need to overcome before
wide-spread commercial deployment on such conversations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12612">A New Type Of Upper And Lower Bounds On Right-Tail Probabilities Of Continuous Random Variables. (arXiv:2311.12612v2 [math.PR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Zlatanov_N/0/1/0/all/0/1">Nikola Zlatanov</a></p>
<p>In this paper, I present a completely new type of upper and lower bounds on
the right-tail probabilities of continuous random variables with unbounded
support and with semi-bounded support from the left. The presented upper and
lower right-tail bounds depend only on the probability density function (PDF),
its first derivative, and two parameters that are used for tightening the
bounds. These tail bounds hold under certain conditions that depend on the PDF,
its first and second derivatives, and the two parameters. The new tail bounds
are shown to be tight for a wide range of continuous random variables via
numerical examples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12670">Towards a more inductive world for drug repurposing approaches. (arXiv:2311.12670v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fuente_J/0/1/0/all/0/1">Jesus de la Fuente</a>, <a href="http://arxiv.org/find/cs/1/au:+Serrano_G/0/1/0/all/0/1">Guillermo Serrano</a>, <a href="http://arxiv.org/find/cs/1/au:+Veleiro_U/0/1/0/all/0/1">Ux&#xed;a Veleiro</a>, <a href="http://arxiv.org/find/cs/1/au:+Casals_M/0/1/0/all/0/1">Mikel Casals</a>, <a href="http://arxiv.org/find/cs/1/au:+Vera_L/0/1/0/all/0/1">Laura Vera</a>, <a href="http://arxiv.org/find/cs/1/au:+Pizurica_M/0/1/0/all/0/1">Marija Pizurica</a>, <a href="http://arxiv.org/find/cs/1/au:+Pineda_Lucena_A/0/1/0/all/0/1">Antonio Pineda-Lucena</a>, <a href="http://arxiv.org/find/cs/1/au:+Ochoa_I/0/1/0/all/0/1">Idoia Ochoa</a>, <a href="http://arxiv.org/find/cs/1/au:+Vicent_S/0/1/0/all/0/1">Silve Vicent</a>, <a href="http://arxiv.org/find/cs/1/au:+Gevaert_O/0/1/0/all/0/1">Olivier Gevaert</a>, <a href="http://arxiv.org/find/cs/1/au:+Hernaez_M/0/1/0/all/0/1">Mikel Hernaez</a></p>
<p>Drug-target interaction (DTI) prediction is a challenging, albeit essential
task in drug repurposing. Learning on graph models have drawn special attention
as they can significantly reduce drug repurposing costs and time commitment.
However, many current approaches require high-demanding additional information
besides DTIs that complicates their evaluation process and usability.
Additionally, structural differences in the learning architecture of current
models hinder their fair benchmarking. In this work, we first perform an
in-depth evaluation of current DTI datasets and prediction models through a
robust benchmarking process, and show that DTI prediction methods based on
transductive models lack generalization and lead to inflated performance when
evaluated as previously done in the literature, hence not being suited for drug
repurposing approaches. We then propose a novel biologically-driven strategy
for negative edge subsampling and show through in vitro validation that newly
discovered interactions are indeed true. We envision this work as the
underpinning for future fair benchmarking and robust model design. All
generated resources and tools are publicly available as a python package.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12716">minimax: Efficient Baselines for Autocurricula in JAX. (arXiv:2311.12716v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1">Minqi Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dennis_M/0/1/0/all/0/1">Michael Dennis</a>, <a href="http://arxiv.org/find/cs/1/au:+Grefenstette_E/0/1/0/all/0/1">Edward Grefenstette</a>, <a href="http://arxiv.org/find/cs/1/au:+Rocktaschel_T/0/1/0/all/0/1">Tim Rockt&#xe4;schel</a></p>
<p>Unsupervised environment design (UED) is a form of automatic curriculum
learning for training robust decision-making agents to zero-shot transfer into
unseen environments. Such autocurricula have received much interest from the RL
community. However, UED experiments, based on CPU rollouts and GPU model
updates, have often required several weeks of training. This compute
requirement is a major obstacle to rapid innovation for the field. This work
introduces the minimax library for UED training on accelerated hardware. Using
JAX to implement fully-tensorized environments and autocurriculum algorithms,
minimax allows the entire training loop to be compiled for hardware
acceleration. To provide a petri dish for rapid experimentation, minimax
includes a tensorized grid-world based on MiniGrid, in addition to reusable
abstractions for conducting autocurricula in procedurally-generated
environments. With these components, minimax provides strong UED baselines,
including new parallelized variants, which achieve over 120$\times$ speedups in
wall time compared to previous implementations when training with equal batch
sizes. The minimax library is available under the Apache 2.0 license at
https://github.com/facebookresearch/minimax.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12727">Soft Random Sampling: A Theoretical and Empirical Analysis. (arXiv:2311.12727v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1">Xiaodong Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1">Ashish Mittal</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1">Songtao Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Saon_G/0/1/0/all/0/1">George Saon</a>, <a href="http://arxiv.org/find/cs/1/au:+Kingsbury_B/0/1/0/all/0/1">Brian Kingsbury</a></p>
<p>Soft random sampling (SRS) is a simple yet effective approach for efficient
training of large-scale deep neural networks when dealing with massive data.
SRS selects a subset uniformly at random with replacement from the full data
set in each epoch. In this paper, we conduct a theoretical and empirical
analysis of SRS. First, we analyze its sampling dynamics including data
coverage and occupancy. Next, we investigate its convergence with non-convex
objective functions and give the convergence rate. Finally, we provide its
generalization performance. We empirically evaluate SRS for image recognition
on CIFAR10 and automatic speech recognition on Librispeech and an in-house
payload dataset to demonstrate its effectiveness. Compared to existing
coreset-based data selection methods, SRS offers a better accuracy-efficiency
trade-off. Especially on real-world industrial scale data sets, it is shown to
be a powerful training strategy with significant speedup and competitive
performance with almost no additional computing cost.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12856">Density of States Prediction of Crystalline Materials via Prompt-guided Multi-Modal Transformer. (arXiv:2311.12856v2 [cond-mat.mtrl-sci] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Lee_N/0/1/0/all/0/1">Namkyeong Lee</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Noh_H/0/1/0/all/0/1">Heewoong Noh</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Kim_S/0/1/0/all/0/1">Sungwon Kim</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Hyun_D/0/1/0/all/0/1">Dongmin Hyun</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Na_G/0/1/0/all/0/1">Gyoung S. Na</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Park_C/0/1/0/all/0/1">Chanyoung Park</a></p>
<p>The density of states (DOS) is a spectral property of crystalline materials,
which provides fundamental insights into various characteristics of the
materials. While previous works mainly focus on obtaining high-quality
representations of crystalline materials for DOS prediction, we focus on
predicting the DOS from the obtained representations by reflecting the nature
of DOS: DOS determines the general distribution of states as a function of
energy. That is, DOS is not solely determined by the crystalline material but
also by the energy levels, which has been neglected in previous works. In this
paper, we propose to integrate heterogeneous information obtained from the
crystalline materials and the energies via a multi-modal transformer, thereby
modeling the complex relationships between the atoms in the crystalline
materials and various energy levels for DOS prediction. Moreover, we propose to
utilize prompts to guide the model to learn the crystal structural
system-specific interactions between crystalline materials and energies.
Extensive experiments on two types of DOS, i.e., Phonon DOS and Electron DOS,
with various real-world scenarios demonstrate the superiority of
DOSTransformer. The source code for DOSTransformer is available at
https://github.com/HeewoongNoh/DOSTransformer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13110">White-Box Transformers via Sparse Rate Reduction: Compression Is All There Is?. (arXiv:2311.13110v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yaodong Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Buchanan_S/0/1/0/all/0/1">Sam Buchanan</a>, <a href="http://arxiv.org/find/cs/1/au:+Pai_D/0/1/0/all/0/1">Druv Pai</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_T/0/1/0/all/0/1">Tianzhe Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Ziyang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tong_S/0/1/0/all/0/1">Shengbang Tong</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1">Hao Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1">Yuexiang Zhai</a>, <a href="http://arxiv.org/find/cs/1/au:+Haeffele_B/0/1/0/all/0/1">Benjamin D. Haeffele</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yi Ma</a></p>
<p>In this paper, we contend that a natural objective of representation learning
is to compress and transform the distribution of the data, say sets of tokens,
towards a low-dimensional Gaussian mixture supported on incoherent subspaces.
The goodness of such a representation can be evaluated by a principled measure,
called sparse rate reduction, that simultaneously maximizes the intrinsic
information gain and extrinsic sparsity of the learned representation. From
this perspective, popular deep network architectures, including transformers,
can be viewed as realizing iterative schemes to optimize this measure.
Particularly, we derive a transformer block from alternating optimization on
parts of this objective: the multi-head self-attention operator compresses the
representation by implementing an approximate gradient descent step on the
coding rate of the features, and the subsequent multi-layer perceptron
sparsifies the features. This leads to a family of white-box transformer-like
deep network architectures, named CRATE, which are mathematically fully
interpretable. We show, by way of a novel connection between denoising and
compression, that the inverse to the aforementioned compressive encoding can be
realized by the same class of CRATE architectures. Thus, the so-derived
white-box architectures are universal to both encoders and decoders.
Experiments show that these networks, despite their simplicity, indeed learn to
compress and sparsify representations of large-scale real-world image and text
datasets, and achieve performance very close to highly engineered
transformer-based models: ViT, MAE, DINO, BERT, and GPT2. We believe the
proposed computational framework demonstrates great potential in bridging the
gap between theory and practice of deep learning, from a unified perspective of
data compression. Code is available at: https://ma-lab-berkeley.github.io/CRATE .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13180">Provably Efficient High-Dimensional Bandit Learning with Batched Feedbacks. (arXiv:2311.13180v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Fan_J/0/1/0/all/0/1">Jianqing Fan</a>, <a href="http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1">Zhaoran Wang</a>, <a href="http://arxiv.org/find/stat/1/au:+Yang_Z/0/1/0/all/0/1">Zhuoran Yang</a>, <a href="http://arxiv.org/find/stat/1/au:+Ye_C/0/1/0/all/0/1">Chenlu Ye</a></p>
<p>We study high-dimensional multi-armed contextual bandits with batched
feedback where the $T$ steps of online interactions are divided into $L$
batches. In specific, each batch collects data according to a policy that
depends on previous batches and the rewards are revealed only at the end of the
batch. Such a feedback structure is popular in applications such as
personalized medicine and online advertisement, where the online data often do
not arrive in a fully serial manner. We consider high-dimensional and linear
settings where the reward function of the bandit model admits either a sparse
or low-rank structure and ask how small a number of batches are needed for a
comparable performance with fully dynamic data in which $L = T$. For these
settings, we design a provably sample-efficient algorithm which achieves a $
\mathcal{\tilde O}(s_0^2 \log^2 T)$ regret in the sparse case and $
\mathcal{\tilde O} ( r ^2 \log^2 T)$ regret in the low-rank case, using only $L
= \mathcal{O}( \log T)$ batches. Here $s_0$ and $r$ are the sparsity and rank
of the reward parameter in sparse and low-rank cases, respectively, and $
\mathcal{\tilde O}(\cdot)$ omits logarithmic factors involving the feature
dimensions. In other words, our algorithm achieves regret bounds comparable to
those in fully sequential setting with only $\mathcal{O}( \log T)$ batches. Our
algorithm features a novel batch allocation method that adjusts the batch sizes
according to the estimation accuracy within each batch and cumulative regret.
Furthermore, we also conduct experiments with synthetic and real-world data to
validate our theory.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13231">Using Human Feedback to Fine-tune Diffusion Models without Any Reward Model. (arXiv:2311.13231v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1">Kai Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_J/0/1/0/all/0/1">Jian Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1">Jiafei Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_C/0/1/0/all/0/1">Chunjiang Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiaxin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qimai Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1">Weihan Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiaolong Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiu Li</a></p>
<p>Using reinforcement learning with human feedback (RLHF) has shown significant
promise in fine-tuning diffusion models. Previous methods start by training a
reward model that aligns with human preferences, then leverage RL techniques to
fine-tune the underlying models. However, crafting an efficient reward model
demands extensive datasets, optimal architecture, and manual hyperparameter
tuning, making the process both time and cost-intensive. The direct preference
optimization (DPO) method, effective in fine-tuning large language models,
eliminates the necessity for a reward model. However, the extensive GPU memory
requirement of the diffusion model's denoising process hinders the direct
application of the DPO method. To address this issue, we introduce the Direct
Preference for Denoising Diffusion Policy Optimization (D3PO) method to
directly fine-tune diffusion models. The theoretical analysis demonstrates that
although D3PO omits training a reward model, it effectively functions as the
optimal reward model trained using human feedback data to guide the learning
process. This approach requires no training of a reward model, proving to be
more direct, cost-effective, and minimizing computational overhead. In
experiments, our method uses the relative scale of objectives as a proxy for
human preference, delivering comparable results to methods using ground-truth
rewards. Moreover, D3PO demonstrates the ability to reduce image distortion
rates and generate safer images, overcoming challenges lacking robust reward
models. Our code is publicly available in
https://github.com/yk7333/D3PO/tree/main.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12850">Meticulously Selecting 1% of the Dataset for Pre-training! Generating Differentially Private Images Data with Semantics Query. (arXiv:2311.12850v1 [cs.CV] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Kecen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1">Chen Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhixiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yuzhong Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1">Xinwen Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tianhao Wang</a></p>
<p>Differential Privacy (DP) image data synthesis, which leverages the DP
technique to generate synthetic data to replace the sensitive data, allowing
organizations to share and utilize synthetic images without privacy concerns.
Previous methods incorporate the advanced techniques of generative models and
pre-training on a public dataset to produce exceptional DP image data, but
suffer from problems of unstable training and massive computational resource
demands. This paper proposes a novel DP image synthesis method, termed
PRIVIMAGE, which meticulously selects pre-training data, promoting the
efficient creation of DP datasets with high fidelity and utility. PRIVIMAGE
first establishes a semantic query function using a public dataset. Then, this
function assists in querying the semantic distribution of the sensitive
dataset, facilitating the selection of data from the public dataset with
analogous semantics for pre-training. Finally, we pre-train an image generative
model using the selected data and then fine-tune this model on the sensitive
dataset using Differentially Private Stochastic Gradient Descent (DP-SGD).
PRIVIMAGE allows us to train a lightly parameterized generative model, reducing
the noise in the gradient during DP-SGD training and enhancing training
stability. Extensive experiments demonstrate that PRIVIMAGE uses only 1% of the
public dataset for pre-training and 7.6% of the parameters in the generative
model compared to the state-of-the-art method, whereas achieves superior
synthetic performance and conserves more computational resources. On average,
PRIVIMAGE achieves 30.1% lower FID and 12.6% higher Classification Accuracy
than the state-of-the-art method. The replication package and datasets can be
accessed online.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13060">Training Deep 3D Convolutional Neural Networks to Extract BSM Physics Parameters Directly from HEP Data: a Proof-of-Concept Study Using Monte Carlo Simulations. (arXiv:2311.13060v1 [hep-ex] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/hep-ex/1/au:+Dubey_S/0/1/0/all/0/1">S. Dubey</a>, <a href="http://arxiv.org/find/hep-ex/1/au:+Browder_T/0/1/0/all/0/1">T.E. Browder</a>, <a href="http://arxiv.org/find/hep-ex/1/au:+Kohani_S/0/1/0/all/0/1">S.Kohani</a>, <a href="http://arxiv.org/find/hep-ex/1/au:+Mandal_R/0/1/0/all/0/1">R. Mandal</a>, <a href="http://arxiv.org/find/hep-ex/1/au:+Sibidanov_A/0/1/0/all/0/1">A. Sibidanov</a>, <a href="http://arxiv.org/find/hep-ex/1/au:+Sinha_R/0/1/0/all/0/1">R. Sinha</a></p>
<p>We report on a novel application of computer vision techniques to extract
beyond the Standard Model (BSM) parameters directly from high energy physics
(HEP) flavor data. We develop a method of transforming angular and kinematic
distributions into "quasi-images" that can be used to train a convolutional
neural network to perform regression tasks, similar to fitting. This contrasts
with the usual classification functions performed using ML/AI in HEP. As a
proof-of-concept, we train a 34-layer Residual Neural Network to regress on
these images and determine the Wilson Coefficient $C_{9}$ in MC (Monte Carlo)
simulations of $B \rightarrow K^{*}\mu^{+}\mu^{-}$ decays. The technique
described here can be generalized and may find applicability across various HEP
experiments and elsewhere.
</p>
</p>
</div>

    </div>
    </body>
    