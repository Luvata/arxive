<!DOCTYPE html>
<html>
<head>
<title>2023-12-12-cs-cv</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2312.04584">Towards Sample-specific Backdoor Attack with Clean Labels via Attribute Trigger. (arXiv:2312.04584v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yiming Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1">Mingyan Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Junfeng Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1">Tao Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1">Shu-Tao Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1">Zhan Qin</a></p>
<p>Currently, sample-specific backdoor attacks (SSBAs) are the most advanced and
malicious methods since they can easily circumvent most of the current backdoor
defenses. In this paper, we reveal that SSBAs are not sufficiently stealthy due
to their poisoned-label nature, where users can discover anomalies if they
check the image-label relationship. In particular, we demonstrate that it is
ineffective to directly generalize existing SSBAs to their clean-label variants
by poisoning samples solely from the target class. We reveal that it is
primarily due to two reasons, including \textbf{(1)} the `antagonistic effects'
of ground-truth features and \textbf{(2)} the learning difficulty of
sample-specific features. Accordingly, trigger-related features of existing
SSBAs cannot be effectively learned under the clean-label setting due to their
mild trigger intensity required for ensuring stealthiness. We argue that the
intensity constraint of existing SSBAs is mostly because their trigger patterns
are `content-irrelevant' and therefore act as `noises' for both humans and
DNNs. Motivated by this understanding, we propose to exploit content-relevant
features, $a.k.a.$ (human-relied) attributes, as the trigger patterns to design
clean-label SSBAs. This new attack paradigm is dubbed backdoor attack with
attribute trigger (BAAT). Extensive experiments are conducted on benchmark
datasets, which verify the effectiveness of our BAAT and its resistance to
existing defenses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04590">Reconciling AI Performance and Data Reconstruction Resilience for Medical Imaging. (arXiv:2312.04590v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ziller_A/0/1/0/all/0/1">Alexander Ziller</a>, <a href="http://arxiv.org/find/cs/1/au:+Mueller_T/0/1/0/all/0/1">Tamara T. Mueller</a>, <a href="http://arxiv.org/find/cs/1/au:+Stieger_S/0/1/0/all/0/1">Simon Stieger</a>, <a href="http://arxiv.org/find/cs/1/au:+Feiner_L/0/1/0/all/0/1">Leonhard Feiner</a>, <a href="http://arxiv.org/find/cs/1/au:+Brandt_J/0/1/0/all/0/1">Johannes Brandt</a>, <a href="http://arxiv.org/find/cs/1/au:+Braren_R/0/1/0/all/0/1">Rickmer Braren</a>, <a href="http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1">Daniel Rueckert</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaissis_G/0/1/0/all/0/1">Georgios Kaissis</a></p>
<p>Artificial Intelligence (AI) models are vulnerable to information leakage of
their training data, which can be highly sensitive, for example in medical
imaging. Privacy Enhancing Technologies (PETs), such as Differential Privacy
(DP), aim to circumvent these susceptibilities. DP is the strongest possible
protection for training models while bounding the risks of inferring the
inclusion of training samples or reconstructing the original data. DP achieves
this by setting a quantifiable privacy budget. Although a lower budget
decreases the risk of information leakage, it typically also reduces the
performance of such models. This imposes a trade-off between robust performance
and stringent privacy. Additionally, the interpretation of a privacy budget
remains abstract and challenging to contextualize. In this study, we contrast
the performance of AI models at various privacy budgets against both,
theoretical risk bounds and empirical success of reconstruction attacks. We
show that using very large privacy budgets can render reconstruction attacks
impossible, while drops in performance are negligible. We thus conclude that
not using DP -- at all -- is negligent when applying AI models to sensitive
data. We deem those results to lie a foundation for further debates on striking
a balance between privacy risks and model performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04640">Autoencoding Labeled Interpolator, Inferring Parameters From Image, And Image From Parameters. (arXiv:2312.04640v1 [astro-ph.HE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/astro-ph/1/au:+SaraerToosi_A/0/1/0/all/0/1">Ali SaraerToosi</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Broderick_A/0/1/0/all/0/1">Avery Broderick</a></p>
<p>The Event Horizon Telescope (EHT) provides an avenue to study black hole
accretion flows on event-horizon scales. Fitting a semi-analytical model to EHT
observations requires the construction of synthetic images, which is
computationally expensive. This study presents an image generation tool in the
form of a generative machine learning model, which extends the capabilities of
a variational autoencoder. This tool can rapidly and continuously interpolate
between a training set of images and can retrieve the defining parameters of
those images. Trained on a set of synthetic black hole images, our tool
showcases success in both interpolating black hole images and their associated
physical parameters. By reducing the computational cost of generating an image,
this tool facilitates parameter estimation and model validation for
observations of black hole system.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04651">VOODOO 3D: Volumetric Portrait Disentanglement for One-Shot 3D Head Reenactment. (arXiv:2312.04651v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tran_P/0/1/0/all/0/1">Phong Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Zakharov_E/0/1/0/all/0/1">Egor Zakharov</a>, <a href="http://arxiv.org/find/cs/1/au:+Ho_L/0/1/0/all/0/1">Long-Nhat Ho</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_A/0/1/0/all/0/1">Anh Tuan Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1">Liwen Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hao Li</a></p>
<p>We present a 3D-aware one-shot head reenactment method based on a fully
volumetric neural disentanglement framework for source appearance and driver
expressions. Our method is real-time and produces high-fidelity and
view-consistent output, suitable for 3D teleconferencing systems based on
holographic displays. Existing cutting-edge 3D-aware reenactment methods often
use neural radiance fields or 3D meshes to produce view-consistent appearance
encoding, but, at the same time, they rely on linear face models, such as 3DMM,
to achieve its disentanglement with facial expressions. As a result, their
reenactment results often exhibit identity leakage from the driver or have
unnatural expressions. To address these problems, we propose a neural
self-supervised disentanglement approach that lifts both the source image and
driver video frame into a shared 3D volumetric representation based on
tri-planes. This representation can then be freely manipulated with expression
tri-planes extracted from the driving images and rendered from an arbitrary
view using neural radiance fields. We achieve this disentanglement via
self-supervised learning on a large in-the-wild video dataset. We further
introduce a highly effective fine-tuning approach to improve the
generalizability of the 3D lifting using the same real-world data. We
demonstrate state-of-the-art performance on a wide range of datasets, and also
showcase high-quality 3D-aware head reenactment on highly challenging and
diverse subjects, including non-frontal head poses and complex expressions for
both source and driver.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04654">NeuSD: Surface Completion with Multi-View Text-to-Image Diffusion. (arXiv:2312.04654v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ignatyev_S/0/1/0/all/0/1">Savva Ignatyev</a>, <a href="http://arxiv.org/find/cs/1/au:+Selikhanovych_D/0/1/0/all/0/1">Daniil Selikhanovych</a>, <a href="http://arxiv.org/find/cs/1/au:+Voynov_O/0/1/0/all/0/1">Oleg Voynov</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yiqun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wonka_P/0/1/0/all/0/1">Peter Wonka</a>, <a href="http://arxiv.org/find/cs/1/au:+Lefkimmiatis_S/0/1/0/all/0/1">Stamatios Lefkimmiatis</a>, <a href="http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1">Evgeny Burnaev</a></p>
<p>We present a novel method for 3D surface reconstruction from multiple images
where only a part of the object of interest is captured. Our approach builds on
two recent developments: surface reconstruction using neural radiance fields
for the reconstruction of the visible parts of the surface, and guidance of
pre-trained 2D diffusion models in the form of Score Distillation Sampling
(SDS) to complete the shape in unobserved regions in a plausible manner. We
introduce three components. First, we suggest employing normal maps as a pure
geometric representation for SDS instead of color renderings which are
entangled with the appearance information. Second, we introduce the freezing of
the SDS noise during training which results in more coherent gradients and
better convergence. Third, we propose Multi-View SDS as a way to condition the
generation of the non-observable part of the surface without fine-tuning or
making changes to the underlying 2D Stable Diffusion model. We evaluate our
approach on the BlendedMVS dataset demonstrating significant qualitative and
quantitative improvements over competing methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04655">ECLIPSE: A Resource-Efficient Text-to-Image Prior for Image Generations. (arXiv:2312.04655v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Patel_M/0/1/0/all/0/1">Maitreya Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1">Changhoon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1">Sheng Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1">Chitta Baral</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yezhou Yang</a></p>
<p>Text-to-image (T2I) diffusion models, notably the unCLIP models (e.g.,
DALL-E-2), achieve state-of-the-art (SOTA) performance on various compositional
T2I benchmarks, at the cost of significant computational resources. The unCLIP
stack comprises T2I prior and diffusion image decoder. The T2I prior model
alone adds a billion parameters compared to the Latent Diffusion Models, which
increases the computational and high-quality data requirements. We introduce
ECLIPSE, a novel contrastive learning method that is both parameter and
data-efficient. ECLIPSE leverages pre-trained vision-language models (e.g.,
CLIP) to distill the knowledge into the prior model. We demonstrate that the
ECLIPSE trained prior, with only 3.3% of the parameters and trained on a mere
2.8% of the data, surpasses the baseline T2I priors with an average of 71.6%
preference score under resource-limited setting. It also attains performance on
par with SOTA big models, achieving an average of 63.36% preference score in
terms of the ability to follow the text compositions. Extensive experiments on
two unCLIP diffusion image decoders, Karlo and Kandinsky, affirm that ECLIPSE
priors consistently deliver high performance while significantly reducing
resource dependency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04670">Rapid Motor Adaptation for Robotic Manipulator Arms. (arXiv:2312.04670v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1">Yichao Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ellis_K/0/1/0/all/0/1">Kevin Ellis</a>, <a href="http://arxiv.org/find/cs/1/au:+Henriques_J/0/1/0/all/0/1">Jo&#xe3;o Henriques</a></p>
<p>Developing generalizable manipulation skills is a core challenge in embodied
AI. This includes generalization across diverse task configurations,
encompassing variations in object shape, density, friction coefficient, and
external disturbances such as forces applied to the robot. Rapid Motor
Adaptation (RMA) offers a promising solution to this challenge. It posits that
essential hidden variables influencing an agent's task performance, such as
object mass and shape, can be effectively inferred from the agent's action and
proprioceptive history. Drawing inspiration from RMA in locomotion and in-hand
rotation, we use depth perception to develop agents tailored for rapid motor
adaptation in a variety of manipulation tasks. We evaluated our agents on four
challenging tasks from the Maniskill2 benchmark, namely pick-and-place
operations with hundreds of objects from the YCB and EGAD datasets, peg
insertion with precise position and orientation, and operating a variety of
faucets and handles, with customized environment variations. Empirical results
demonstrate that our agents surpass state-of-the-art methods like automatic
domain randomization and vision-based policies, obtaining better generalization
performance and sample efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04671">The automatic detection of lumber anatomy in epidural injections for ultrasound guidance. (arXiv:2312.04671v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Piri_F/0/1/0/all/0/1">Farhad Piri</a>, <a href="http://arxiv.org/find/eess/1/au:+Sobhiyeh_S/0/1/0/all/0/1">Sima Sobhiyeh</a>, <a href="http://arxiv.org/find/eess/1/au:+Rezaie_A/0/1/0/all/0/1">Amir H. Rezaie</a>, <a href="http://arxiv.org/find/eess/1/au:+Mosaffa_F/0/1/0/all/0/1">Faramarz Mosaffa</a></p>
<p>The purpose of this paper is to help the anesthesiologist to find the
epidural depth automatically to make the first attempt to enter the path of the
needle into the patient's body while it is clogged with bone and avoid causing
a puncture in the surrounding areas of the patient`s back. In this regard, a
morphology-based bone enhancement and detection followed by a
Ramer-Douglas-Peucker algorithm and Hough transform is proposed. The proposed
algorithm is tested on synthetic and real ultrasound images of laminar bone,
and the results are compared with the template matching based Ligamentum Flavum
(LF) detection method. Results indicate that the proposed method can faster
detect the diagonal shape of the laminar bone and its corresponding epidural
depth. Furthermore, the proposed method is reliable enough providing
anesthesiologists with real-time information while an epidural needle insertion
is performed. It has to be noted that using the ultrasound images is to help
anesthesiologists to perform the blind injection, and due to quite a lot of
errors occurred in ultrasound-imaging-based methods, these methods can not
completely replace the tissue pressure-based method. And in the end, when the
needle is injected into the area (dura space) measurements can only be trusted
to the extent of tissue resistance. Despite the fairly limited amount of
training data available in this study, a significant improvement of the
segmentation speed of lumbar bones and epidural depth in ultrasound scans with
a rational accuracy compared to the LF-based detection method was found.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04679">ConVRT: Consistent Video Restoration Through Turbulence with Test-time Optimization of Neural Video Representations. (arXiv:2312.04679v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Cai_H/0/1/0/all/0/1">Haoming Cai</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1">Jingxi Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Feng_B/0/1/0/all/0/1">Brandon Y. Feng</a>, <a href="http://arxiv.org/find/eess/1/au:+Jiang_W/0/1/0/all/0/1">Weiyun Jiang</a>, <a href="http://arxiv.org/find/eess/1/au:+Xie_M/0/1/0/all/0/1">Mingyang Xie</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_K/0/1/0/all/0/1">Kevin Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Veeraraghavan_A/0/1/0/all/0/1">Ashok Veeraraghavan</a>, <a href="http://arxiv.org/find/eess/1/au:+Metzler_C/0/1/0/all/0/1">Christopher Metzler</a></p>
<p>tmospheric turbulence presents a significant challenge in long-range imaging.
Current restoration algorithms often struggle with temporal inconsistency, as
well as limited generalization ability across varying turbulence levels and
scene content different than the training data. To tackle these issues, we
introduce a self-supervised method, Consistent Video Restoration through
Turbulence (ConVRT) a test-time optimization method featuring a neural video
representation designed to enhance temporal consistency in restoration. A key
innovation of ConVRT is the integration of a pretrained vision-language model
(CLIP) for semantic-oriented supervision, which steers the restoration towards
sharp, photorealistic images in the CLIP latent space. We further develop a
principled selection strategy of text prompts, based on their statistical
correlation with a perceptual metric. ConVRT's test-time optimization allows it
to adapt to a wide range of real-world turbulence conditions, effectively
leveraging the insights gained from pre-trained models on simulated data.
ConVRT offers a comprehensive and effective solution for mitigating real-world
turbulence in dynamic videos.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04692">Diffence: Fencing Membership Privacy With Diffusion Models. (arXiv:2312.04692v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1">Yuefeng Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Naseh_A/0/1/0/all/0/1">Ali Naseh</a>, <a href="http://arxiv.org/find/cs/1/au:+Houmansadr_A/0/1/0/all/0/1">Amir Houmansadr</a></p>
<p>Deep learning models, while achieving remarkable performance across various
tasks, are vulnerable to member inference attacks, wherein adversaries identify
if a specific data point was part of a model's training set. This
susceptibility raises substantial privacy concerns, especially when models are
trained on sensitive datasets. Current defense methods often struggle to
provide robust protection without hurting model utility, and they often require
retraining the model or using extra data. In this work, we introduce a novel
defense framework against membership attacks by leveraging generative models.
The key intuition of our defense is to remove the differences between member
and non-member inputs which can be used to perform membership attacks, by
re-generating input samples before feeding them to the target model. Therefore,
our defense works \emph{pre-inference}, which is unlike prior defenses that are
either training-time (modify the model) or post-inference time (modify the
model's output).
</p>
<p>A unique feature of our defense is that it works on input samples only,
without modifying the training or inference phase of the target model.
Therefore, it can be cascaded with other defense mechanisms as we demonstrate
through experiments. Through extensive experimentation, we show that our
approach can serve as a robust plug-n-play defense mechanism, enhancing
membership privacy without compromising model utility in both baseline and
defended settings. For example, our method enhanced the effectiveness of recent
state-of-the-art defenses, reducing attack accuracy by an average of 5.7\% to
12.4\% across three datasets, without any impact on the model's accuracy. By
integrating our method with prior defenses, we achieve new state-of-the-art
performance in the privacy-utility trade-off.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04713">gcDLSeg: Integrating Graph-cut into Deep Learning for Binary Semantic Segmentation. (arXiv:2312.04713v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1">Hui Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Weiyu Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Ya Xing Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Buatti_J/0/1/0/all/0/1">John Buatti</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xiaodong Wu</a></p>
<p>Binary semantic segmentation in computer vision is a fundamental problem. As
a model-based segmentation method, the graph-cut approach was one of the most
successful binary segmentation methods thanks to its global optimality
guarantee of the solutions and its practical polynomial-time complexity.
Recently, many deep learning (DL) based methods have been developed for this
task and yielded remarkable performance, resulting in a paradigm shift in this
field. To combine the strengths of both approaches, we propose in this study to
integrate the graph-cut approach into a deep learning network for end-to-end
learning. Unfortunately, backward propagation through the graph-cut module in
the DL network is challenging due to the combinatorial nature of the graph-cut
algorithm. To tackle this challenge, we propose a novel residual graph-cut loss
and a quasi-residual connection, enabling the backward propagation of the
gradients of the residual graph-cut loss for effective feature learning guided
by the graph-cut segmentation model. In the inference phase, globally optimal
segmentation is achieved with respect to the graph-cut energy defined on the
optimized image features learned from DL networks. Experiments on the public
AZH chronic wound data set and the pancreas cancer data set from the medical
segmentation decathlon (MSD) demonstrated promising segmentation accuracy, and
improved robustness against adversarial attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04727">E2ENet: Dynamic Sparse Feature Fusion for Accurate and Efficient 3D Medical Image Segmentation. (arXiv:2312.04727v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1">Boqian Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_Q/0/1/0/all/0/1">Qiao Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shiwei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1">Lu Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1">Mykola Pechenizkiy</a>, <a href="http://arxiv.org/find/cs/1/au:+Mocanu_D/0/1/0/all/0/1">Decebal Constantin Mocanu</a>, <a href="http://arxiv.org/find/cs/1/au:+Keulen_M/0/1/0/all/0/1">Maurice Van Keulen</a>, <a href="http://arxiv.org/find/cs/1/au:+Mocanu_E/0/1/0/all/0/1">Elena Mocanu</a></p>
<p>Deep neural networks have evolved as the leading approach in 3D medical image
segmentation due to their outstanding performance. However, the ever-increasing
model size and computation cost of deep neural networks have become the primary
barrier to deploying them on real-world resource-limited hardware. In pursuit
of improving performance and efficiency, we propose a 3D medical image
segmentation model, named Efficient to Efficient Network (E2ENet),
incorporating two parametrically and computationally efficient designs. i.
Dynamic sparse feature fusion (DSFF) mechanism: it adaptively learns to fuse
informative multi-scale features while reducing redundancy. ii. Restricted
depth-shift in 3D convolution: it leverages the 3D spatial information while
keeping the model and computational complexity as 2D-based methods. We conduct
extensive experiments on BTCV, AMOS-CT and Brain Tumor Segmentation Challenge,
demonstrating that E2ENet consistently achieves a superior trade-off between
accuracy and efficiency than prior arts across various resource constraints.
E2ENet achieves comparable accuracy on the large-scale challenge AMOS-CT, while
saving over 68\% parameter count and 29\% FLOPs in the inference phase,
compared with the previous best-performing method. Our code has been made
available at: https://github.com/boqian333/E2ENet-Medical.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04744">Fine-Grained Extraction of Road Networks via Joint Learning of Connectivity and Segmentation. (arXiv:2312.04744v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yijia Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Liqiang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wuming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Suhong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jingwen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xingang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuebin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yang Li</a></p>
<p>Road network extraction from satellite images is widely applicated in
intelligent traffic management and autonomous driving fields. The
high-resolution remote sensing images contain complex road areas and distracted
background, which make it a challenge for road extraction. In this study, we
present a stacked multitask network for end-to-end segmenting roads while
preserving connectivity correctness. In the network, a global-aware module is
introduced to enhance pixel-level road feature representation and eliminate
background distraction from overhead images; a road-direction-related
connectivity task is added to ensure that the network preserves the graph-level
relationships of the road segments. We also develop a stacked multihead
structure to jointly learn and effectively utilize the mutual information
between connectivity learning and segmentation learning. We evaluate the
performance of the proposed network on three public remote sensing datasets.
The experimental results demonstrate that the network outperforms the
state-of-the-art methods in terms of road segmentation accuracy and
connectivity maintenance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04746">Quilt-LLaVA: Visual Instruction Tuning by Extracting Localized Narratives from Open-Source Histopathology Videos. (arXiv:2312.04746v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Seyfioglu_M/0/1/0/all/0/1">Mehmet Saygin Seyfioglu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ikezogwo_W/0/1/0/all/0/1">Wisdom O. Ikezogwo</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghezloo_F/0/1/0/all/0/1">Fatemeh Ghezloo</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1">Ranjay Krishna</a>, <a href="http://arxiv.org/find/cs/1/au:+Shapiro_L/0/1/0/all/0/1">Linda Shapiro</a></p>
<p>The gigapixel scale of whole slide images (WSIs) poses a challenge for
histopathology multi-modal chatbots, requiring a global WSI analysis for
diagnosis, compounding evidence from different WSI patches. Current visual
instruction datasets, generated through large language models, focus on
creating question/answer pairs for individual image patches, which may lack
diagnostic capacity on their own in histopathology, further complicated by the
absence of spatial grounding in histopathology image captions. To bridge this
gap, we introduce Quilt-Instruct, a large-scale dataset of 107,131
histopathology-specific instruction question/answer pairs, that is collected by
leveraging educational histopathology videos from YouTube, which provides
spatial localization of captions by automatically extracting narrators' cursor
movements. In addition, we provide contextual reasoning by extracting diagnosis
and supporting facts from the entire video content to guide the extrapolative
reasoning of GPT-4. Using Quilt-Instruct, we train Quilt-LLaVA, which can
reason beyond the given single image patch, enabling diagnostic reasoning and
the capability of spatial awareness. To evaluate Quilt-LLaVA, we propose a
comprehensive evaluation dataset created from 985 images and 1283
human-generated question-answers. We also thoroughly evaluate Quilt-LLaVA using
public histopathology datasets, where Quilt-LLaVA significantly outperforms
SOTA by over 10% on relative GPT-4 score and 4% and 9% on open and closed set
VQA. Our code, data, and model are publicly available at quilt-llava.github.io.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04779">Image Synthesis-based Late Stage Cancer Augmentation and Semi-Supervised Segmentation for MRI Rectal Cancer Staging. (arXiv:2312.04779v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Sasuga_S/0/1/0/all/0/1">Saeko Sasuga</a>, <a href="http://arxiv.org/find/eess/1/au:+Kudo_A/0/1/0/all/0/1">Akira Kudo</a>, <a href="http://arxiv.org/find/eess/1/au:+Kitamura_Y/0/1/0/all/0/1">Yoshiro Kitamura</a>, <a href="http://arxiv.org/find/eess/1/au:+Iizuka_S/0/1/0/all/0/1">Satoshi Iizuka</a>, <a href="http://arxiv.org/find/eess/1/au:+Simo_Serra_E/0/1/0/all/0/1">Edgar Simo-Serra</a>, <a href="http://arxiv.org/find/eess/1/au:+Hamabe_A/0/1/0/all/0/1">Atsushi Hamabe</a>, <a href="http://arxiv.org/find/eess/1/au:+Ishii_M/0/1/0/all/0/1">Masayuki Ishii</a>, <a href="http://arxiv.org/find/eess/1/au:+Takemasa_I/0/1/0/all/0/1">Ichiro Takemasa</a></p>
<p>Rectal cancer is one of the most common diseases and a major cause of
mortality. For deciding rectal cancer treatment plans, T-staging is important.
However, evaluating the index from preoperative MRI images requires high
radiologists' skill and experience. Therefore, the aim of this study is to
segment the mesorectum, rectum, and rectal cancer region so that the system can
predict T-stage from segmentation results. Generally, shortage of large and
diverse dataset and high quality annotation are known to be the bottlenecks in
computer aided diagnostics development. Regarding rectal cancer, advanced
cancer images are very rare, and per-pixel annotation requires high
radiologists' skill and time. Therefore, it is not feasible to collect
comprehensive disease patterns in a training dataset. To tackle this, we
propose two kinds of approaches of image synthesis-based late stage cancer
augmentation and semi-supervised learning which is designed for T-stage
prediction. In the image synthesis data augmentation approach, we generated
advanced cancer images from labels. The real cancer labels were deformed to
resemble advanced cancer labels by artificial cancer progress simulation. Next,
we introduce a T-staging loss which enables us to train segmentation models
from per-image T-stage labels. The loss works to keep inclusion/invasion
relationships between rectum and cancer region consistent to the ground truth
T-stage. The verification tests show that the proposed method obtains the best
sensitivity (0.76) and specificity (0.80) in distinguishing between over T3
stage and underT2. In the ablation studies, our semi-supervised learning
approach with the T-staging loss improved specificity by 0.13. Adding the image
synthesis-based data augmentation improved the DICE score of invasion cancer
area by 0.08 from baseline.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04780">Fine-Tuning InstructPix2Pix for Advanced Image Colorization. (arXiv:2312.04780v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+An_Z/0/1/0/all/0/1">Zifeng An</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zijing Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_E/0/1/0/all/0/1">Eric Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1">Qi Cao</a></p>
<p>This paper presents a novel approach to human image colorization by
fine-tuning the InstructPix2Pix model, which integrates a language model
(GPT-3) with a text-to-image model (Stable Diffusion). Despite the original
InstructPix2Pix model's proficiency in editing images based on textual
instructions, it exhibits limitations in the focused domain of colorization. To
address this, we fine-tuned the model using the IMDB-WIKI dataset, pairing
black-and-white images with a diverse set of colorization prompts generated by
ChatGPT. This paper contributes by (1) applying fine-tuning techniques to
stable diffusion models specifically for colorization tasks, and (2) employing
generative models to create varied conditioning prompts. After finetuning, our
model outperforms the original InstructPix2Pix model on multiple metrics
quantitatively, and we produce more realistically colored images qualitatively.
The code for this project is provided on the GitHub Repository
https://github.com/AllenAnZifeng/DeepLearning282.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04784">Reality&#x27;s Canvas, Language&#x27;s Brush: Crafting 3D Avatars from Monocular Video. (arXiv:2312.04784v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1">Yuchen Rao</a>, <a href="http://arxiv.org/find/cs/1/au:+Pellitero_E/0/1/0/all/0/1">Eduardo Perez Pellitero</a>, <a href="http://arxiv.org/find/cs/1/au:+Busam_B/0/1/0/all/0/1">Benjamin Busam</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yiren Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1">Jifei Song</a></p>
<p>Recent advancements in 3D avatar generation excel with multi-view supervision
for photorealistic models. However, monocular counterparts lag in quality
despite broader applicability. We propose ReCaLab to close this gap. ReCaLab is
a fully-differentiable pipeline that learns high-fidelity 3D human avatars from
just a single RGB video. A pose-conditioned deformable NeRF is optimized to
volumetrically represent a human subject in canonical T-pose. The canonical
representation is then leveraged to efficiently associate viewpoint-agnostic
textures using 2D-3D correspondences. This enables to separately generate
albedo and shading which jointly compose an RGB prediction. The design allows
to control intermediate results for human pose, body shape, texture, and
lighting with text prompts. An image-conditioned diffusion model thereby helps
to animate appearance and pose of the 3D avatar to create video sequences with
previously unseen human motion. Extensive experiments show that ReCaLab
outperforms previous monocular approaches in terms of image quality for image
synthesis tasks. ReCaLab even outperforms multi-view methods that leverage up
to 19x more synchronized videos for the task of novel pose rendering. Moreover,
natural language offers an intuitive user interface for creative manipulation
of 3D human avatars.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04793">User-Aware Prefix-Tuning is a Good Learner for Personalized Image Captioning. (arXiv:2312.04793v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guanhong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chai_W/0/1/0/all/0/1">Wenhao Chai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jiayu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Gaoang Wang</a></p>
<p>Image captioning bridges the gap between vision and language by automatically
generating natural language descriptions for images. Traditional image
captioning methods often overlook the preferences and characteristics of users.
Personalized image captioning solves this problem by incorporating user prior
knowledge into the model, such as writing styles and preferred vocabularies.
Most existing methods emphasize the user context fusion process by memory
networks or transformers. However, these methods ignore the distinct domains of
each dataset. Therefore, they need to update the entire caption model
parameters when meeting new samples, which is time-consuming and
calculation-intensive. To address this challenge, we propose a novel
personalized image captioning framework that leverages user context to consider
personality factors. Additionally, our framework utilizes the prefix-tuning
paradigm to extract knowledge from a frozen large language model, reducing the
gap between different language domains. Specifically, we employ CLIP to extract
the visual features of an image and align the semantic space using a
query-guided mapping network. By incorporating the transformer layer, we merge
the visual features with the user's contextual prior knowledge to generate
informative prefixes. Moreover, we employ GPT-2 as the frozen large language
model. With a small number of parameters to be trained, our model performs
efficiently and effectively. Our model outperforms existing baseline models on
Instagram and YFCC100M datasets across five evaluation metrics, demonstrating
its superiority, including twofold improvements in metrics such as BLEU-4 and
CIDEr.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04794">Visual Grounding of Whole Radiology Reports for 3D CT Images. (arXiv:2312.04794v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ichinose_A/0/1/0/all/0/1">Akimichi Ichinose</a>, <a href="http://arxiv.org/find/cs/1/au:+Hatsutani_T/0/1/0/all/0/1">Taro Hatsutani</a>, <a href="http://arxiv.org/find/cs/1/au:+Nakamura_K/0/1/0/all/0/1">Keigo Nakamura</a>, <a href="http://arxiv.org/find/cs/1/au:+Kitamura_Y/0/1/0/all/0/1">Yoshiro Kitamura</a>, <a href="http://arxiv.org/find/cs/1/au:+Iizuka_S/0/1/0/all/0/1">Satoshi Iizuka</a>, <a href="http://arxiv.org/find/cs/1/au:+Simo_Serra_E/0/1/0/all/0/1">Edgar Simo-Serra</a>, <a href="http://arxiv.org/find/cs/1/au:+Kido_S/0/1/0/all/0/1">Shoji Kido</a>, <a href="http://arxiv.org/find/cs/1/au:+Tomiyama_N/0/1/0/all/0/1">Noriyuki Tomiyama</a></p>
<p>Building a large-scale training dataset is an essential problem in the
development of medical image recognition systems. Visual grounding techniques,
which automatically associate objects in images with corresponding
descriptions, can facilitate labeling of large number of images. However,
visual grounding of radiology reports for CT images remains challenging,
because so many kinds of anomalies are detectable via CT imaging, and resulting
report descriptions are long and complex. In this paper, we present the first
visual grounding framework designed for CT image and report pairs covering
various body parts and diverse anomaly types. Our framework combines two
components of 1) anatomical segmentation of images, and 2) report structuring.
The anatomical segmentation provides multiple organ masks of given CT images,
and helps the grounding model recognize detailed anatomies. The report
structuring helps to accurately extract information regarding the presence,
location, and type of each anomaly described in corresponding reports. Given
the two additional image/report features, the grounding model can achieve
better localization. In the verification process, we constructed a large-scale
dataset with region-description correspondence annotations for 10,410 studies
of 7,321 unique patients. We evaluated our framework using grounding accuracy,
the percentage of correctly localized anomalies, as a metric and demonstrated
that the combination of the anatomical segmentation and the report structuring
improves the performance with a large margin over the baseline model (66.0% vs
77.8%). Comparison with the prior techniques also showed higher performance of
our method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04796">Segmentation of Kidney Tumors on Non-Contrast CT Images using Protuberance Detection Network. (arXiv:2312.04796v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Hatsutani_T/0/1/0/all/0/1">Taro Hatsutani</a>, <a href="http://arxiv.org/find/eess/1/au:+Ichinose_A/0/1/0/all/0/1">Akimichi Ichinose</a>, <a href="http://arxiv.org/find/eess/1/au:+Nakamura_K/0/1/0/all/0/1">Keigo Nakamura</a>, <a href="http://arxiv.org/find/eess/1/au:+Kitamura_Y/0/1/0/all/0/1">Yoshiro Kitamura</a></p>
<p>Many renal cancers are incidentally found on non-contrast CT (NCCT) images.
On contrast-enhanced CT (CECT) images, most kidney tumors, especially renal
cancers, have different intensity values compared to normal tissues. However,
on NCCT images, some tumors called isodensity tumors, have similar intensity
values to the surrounding normal tissues, and can only be detected through a
change in organ shape. Several deep learning methods which segment kidney
tumors from CECT images have been proposed and showed promising results.
However, these methods fail to capture such changes in organ shape on NCCT
images. In this paper, we present a novel framework, which can explicitly
capture protruded regions in kidneys to enable a better segmentation of kidney
tumors. We created a synthetic mask dataset that simulates a protuberance, and
trained a segmentation network to separate the protruded regions from the
normal kidney regions. To achieve the segmentation of whole tumors, our
framework consists of three networks. The first network is a conventional
semantic segmentation network which extracts a kidney region mask and an
initial tumor region mask. The second network, which we name protuberance
detection network, identifies the protruded regions from the kidney region
mask. Given the initial tumor region mask and the protruded region mask, the
last network fuses them and predicts the final kidney tumor mask accurately.
The proposed method was evaluated on a publicly available KiTS19 dataset, which
contains 108 NCCT images, and showed that our method achieved a higher dice
score of 0.615 (+0.097) and sensitivity of 0.721 (+0.103) compared to 3D-UNet.
To the best of our knowledge, this is the first deep learning method that is
specifically designed for kidney tumor segmentation on NCCT images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04802">MimicDiffusion: Purifying Adversarial Perturbation via Mimicking Clean Diffusion Model. (arXiv:2312.04802v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1">Kaiyu Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Lai_H/0/1/0/all/0/1">Hanjiang Lai</a></p>
<p>Deep neural networks (DNNs) are vulnerable to adversarial perturbation, where
an imperceptible perturbation is added to the image that can fool the DNNs.
Diffusion-based adversarial purification focuses on using the diffusion model
to generate a clean image against such adversarial attacks. Unfortunately, the
generative process of the diffusion model is also inevitably affected by
adversarial perturbation since the diffusion model is also a deep network where
its input has adversarial perturbation. In this work, we propose
MimicDiffusion, a new diffusion-based adversarial purification technique, that
directly approximates the generative process of the diffusion model with the
clean image as input. Concretely, we analyze the differences between the guided
terms using the clean image and the adversarial sample. After that, we first
implement MimicDiffusion based on Manhattan distance. Then, we propose two
guidance to purify the adversarial perturbation and approximate the clean
diffusion model. Extensive experiments on three image datasets including
CIFAR-10, CIFAR-100, and ImageNet with three classifier backbones including
WideResNet-70-16, WideResNet-28-10, and ResNet50 demonstrate that
MimicDiffusion significantly performs better than the state-of-the-art
baselines. On CIFAR-10, CIFAR-100, and ImageNet, it achieves 92.67\%, 61.35\%,
and 61.53\% average robust accuracy, which are 18.49\%, 13.23\%, and 17.64\%
higher, respectively. The code is available in the supplementary material.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04803">SuperNormal: Neural Surface Reconstruction via Multi-View Normal Integration. (arXiv:2312.04803v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1">Xu Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Taketomi_T/0/1/0/all/0/1">Takafumi Taketomi</a></p>
<p>We present SuperNormal, a fast, high-fidelity approach to multi-view 3D
reconstruction using surface normal maps. With a few minutes, SuperNormal
produces detailed surfaces on par with 3D scanners. We harness volume rendering
to optimize a neural signed distance function (SDF) powered by multi-resolution
hash encoding. To accelerate training, we propose directional finite difference
and patch-based ray marching to approximate the SDF gradients numerically.
While not compromising reconstruction quality, this strategy is nearly twice as
efficient as analytical gradients and about three times faster than
axis-aligned finite difference. Experiments on the benchmark dataset
demonstrate the superiority of SuperNormal in efficiency and accuracy compared
to existing multi-view photometric stereo methods. On our captured objects,
SuperNormal produces more fine-grained geometry than recent neural 3D
reconstruction methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04806">RL Dreams: Policy Gradient Optimization for Score Distillation based 3D Generation. (arXiv:2312.04806v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mathur_A/0/1/0/all/0/1">Aradhya N. Mathur</a>, <a href="http://arxiv.org/find/cs/1/au:+Pham_P/0/1/0/all/0/1">Phu Pham</a>, <a href="http://arxiv.org/find/cs/1/au:+Bera_A/0/1/0/all/0/1">Aniket Bera</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_O/0/1/0/all/0/1">Ojaswa Sharma</a></p>
<p>3D generation has rapidly accelerated in the past decade owing to the
progress in the field of generative modeling. Score Distillation Sampling (SDS)
based rendering has improved 3D asset generation to a great extent. Further,
the recent work of Denoising Diffusion Policy Optimization (DDPO) demonstrates
that the diffusion process is compatible with policy gradient methods and has
been demonstrated to improve the 2D diffusion models using an aesthetic scoring
function. We first show that this aesthetic scorer acts as a strong guide for a
variety of SDS-based methods and demonstrates its effectiveness in text-to-3D
synthesis. Further, we leverage the DDPO approach to improve the quality of the
3D rendering obtained from 2D diffusion models. Our approach, DDPO3D, employs
the policy gradient method in tandem with aesthetic scoring. To the best of our
knowledge, this is the first method that extends policy gradient methods to 3D
score-based rendering and shows improvement across SDS-based methods such as
DreamGaussian, which are currently driving research in text-to-3D synthesis.
Our approach is compatible with score distillation-based methods, which would
facilitate the integration of diverse reward functions into the generative
process. Our project page can be accessed via https://ddpo3d.github.io.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04808">A Review On Table Recognition Based On Deep Learning. (arXiv:2312.04808v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiyuan_S/0/1/0/all/0/1">Shi Jiyuan</a>, <a href="http://arxiv.org/find/cs/1/au:+chunqi_S/0/1/0/all/0/1">Shi chunqi</a></p>
<p>Table recognition is using the computer to automatically understand the
table, to detect the position of the table from the document or picture, and to
correctly extract and identify the internal structure and content of the table.
After earlier mainstream approaches based on heuristic rules and machine
learning, the development of deep learning techniques has brought a new
paradigm to this field. This review mainly discusses the table recognition
problem from five aspects. The first part introduces data sets, benchmarks, and
commonly used evaluation indicators. This section selects representative data
sets, benchmarks, and evaluation indicators that are frequently used by
researchers. The second part introduces the table recognition model. This
survey introduces the development of the table recognition model, especially
the table recognition model based on deep learning. It is generally accepted
that table recognition is divided into two stages: table detection and table
structure recognition. This section introduces the models that follow this
paradigm (TD and TSR). The third part is the End-to-End method, this section
introduces some scholars' attempts to use an end-to-end approach to solve the
table recognition problem once and for all and the part are Data-centric
methods, such as data augmentation, aligning benchmarks, and other methods. The
fourth part is the data-centric approach, such as data enhancement, alignment
benchmark, and so on. The fifth part summarizes and compares the experimental
data in the field of form recognition, and analyzes the mainstream and more
advantageous methods. Finally, this paper also discusses the possible
development direction and trend of form processing in the future, to provide
some ideas for researchers in the field of table recognition. (Resource will be
released at https://github.com/Wa1den-jy/Topic-on-Table-Recognition .)
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04810">RS-Corrector: Correcting the Racial Stereotypes in Latent Diffusion Models. (arXiv:2312.04810v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yue Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1">Yueming Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1">Tianxiang Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1">Bo Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1">Jing Dong</a></p>
<p>Recent text-conditioned image generation models have demonstrated an
exceptional capacity to produce diverse and creative imagery with high visual
quality. However, when pre-trained on billion-sized datasets randomly collected
from the Internet, where potential biased human preferences exist, these models
tend to produce images with common and recurring stereotypes, particularly for
certain racial groups. In this paper, we conduct an initial analysis of the
publicly available Stable Diffusion model and its derivatives, highlighting the
presence of racial stereotypes. These models often generate distorted or biased
images for certain racial groups, emphasizing stereotypical characteristics. To
address these issues, we propose a framework called "RS-Corrector", designed to
establish an anti-stereotypical preference in the latent space and update the
latent code for refined generated results. The correction process occurs during
the inference stage without requiring fine-tuning of the original model.
Extensive empirical evaluations demonstrate that the introduced \themodel
effectively corrects the racial stereotypes of the well-trained Stable
Diffusion model while leaving the original model unchanged.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04813">DARNet: Bridging Domain Gaps in Cross-Domain Few-Shot Segmentation with Dynamic Adaptation. (arXiv:2312.04813v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1">Haoran Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1">Qi Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Pagnucco_M/0/1/0/all/0/1">Maurice Pagnucco</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Yang Song</a></p>
<p>Few-shot segmentation (FSS) aims to segment novel classes in a query image by
using only a small number of supporting images from base classes. However, in
cross-domain few-shot segmentation (CD-FSS), leveraging features from
label-rich domains for resource-constrained domains poses challenges due to
domain discrepancies. This work presents a Dynamically Adaptive Refine (DARNet)
method that aims to balance generalization and specificity for CD-FSS. Our
method includes the Channel Statistics Disruption (CSD) strategy, which
perturbs feature channel statistics in the source domain, bolstering
generalization to unknown target domains. Moreover, recognizing the variability
across target domains, an Adaptive Refine Self-Matching (ARSM) method is also
proposed to adjust the matching threshold and dynamically refine the prediction
result with the self-matching method, enhancing accuracy. We also present a
Test-Time Adaptation (TTA) method to refine the model's adaptability to diverse
feature distributions. Our approach demonstrates superior performance against
state-of-the-art methods in CD-FSS tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04817">MoVQA: A Benchmark of Versatile Question-Answering for Long-Form Movie Understanding. (arXiv:2312.04817v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongjie Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1">Lu Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yifei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1">Zhen-Hua Ling</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yali Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Limin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a></p>
<p>While several long-form VideoQA datasets have been introduced, the length of
both videos used to curate questions and sub-clips of clues leveraged to answer
those questions have not yet reached the criteria for genuine long-form video
understanding. Moreover, their QAs are unduly narrow and modality-biased,
lacking a wider view of understanding long-term video content with rich
dynamics and complex narratives. To remedy this, we introduce MoVQA, a
long-form movie question-answering dataset, and benchmark to assess the diverse
cognitive capabilities of multimodal systems rely on multi-level temporal
lengths, with considering both video length and clue length. Additionally, to
take a step towards human-level understanding in long-form video, versatile and
multimodal question-answering is designed from the moviegoer-perspective to
assess the model capabilities on various perceptual and cognitive axes.Through
analysis involving various baselines reveals a consistent trend: the
performance of all methods significantly deteriorate with increasing video and
clue length. Meanwhile, our established baseline method has shown some
improvements, but there is still ample scope for enhancement on our challenging
MoVQA dataset. We expect our MoVQA to provide a new perspective and encourage
inspiring works on long-form video understanding research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04820">Learn to Optimize Denoising Scores for 3D Generation: A Unified and Improved Diffusion Prior on NeRF and 3D Gaussian Splatting. (arXiv:2312.04820v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xiaofeng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yiwen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Cheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yi Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xulei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1">Fayao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1">Guosheng Lin</a></p>
<p>We propose a unified framework aimed at enhancing the diffusion priors for 3D
generation tasks. Despite the critical importance of these tasks, existing
methodologies often struggle to generate high-caliber results. We begin by
examining the inherent limitations in previous diffusion priors. We identify a
divergence between the diffusion priors and the training procedures of
diffusion models that substantially impairs the quality of 3D generation. To
address this issue, we propose a novel, unified framework that iteratively
optimizes both the 3D model and the diffusion prior. Leveraging the different
learnable parameters of the diffusion prior, our approach offers multiple
configurations, affording various trade-offs between performance and
implementation complexity. Notably, our experimental results demonstrate that
our method markedly surpasses existing techniques, establishing new
state-of-the-art in the realm of text-to-3D generation. Furthermore, our
approach exhibits impressive performance on both NeRF and the newly introduced
3D Gaussian Splatting backbones. Additionally, our framework yields insightful
contributions to the understanding of recent score distillation methods, such
as the VSD and DDS loss.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04821">Unify Change Point Detection and Segment Classification in a Regression Task for Transportation Mode Identification. (arXiv:2312.04821v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1">Rongsong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Pei_X/0/1/0/all/0/1">Xin Pei</a></p>
<p>Identifying travelers' transportation modes is important in transportation
science and location-based services. It's appealing for researchers to leverage
GPS trajectory data to infer transportation modes with the popularity of
GPS-enabled devices, e.g., smart phones. Existing studies frame this problem as
classification task. The dominant two-stage studies divide the trip into
single-one mode segments first and then categorize these segments. The over
segmentation strategy and inevitable error propagation bring difficulties to
classification stage and make optimizing the whole system hard. The recent
one-stage works throw out trajectory segmentation entirely to avoid these by
directly conducting point-wise classification for the trip, whereas leaving
predictions dis-continuous. To solve above-mentioned problems, inspired by YOLO
and SSD in object detection, we propose to reframe change point detection and
segment classification as a unified regression task instead of the existing
classification task. We directly regress coordinates of change points and
classify associated segments. In this way, our method divides the trip into
segments under a supervised manner and leverage more contextual information,
obtaining predictions with high accuracy and continuity. Two frameworks,
TrajYOLO and TrajSSD, are proposed to solve the regression task and various
feature extraction backbones are exploited. Exhaustive experiments on GeoLife
dataset show that the proposed method has competitive overall identification
accuracy of 0.853 when distinguishing five modes: walk, bike, bus, car, train.
As for change point detection, our method increases precision at the cost of
drop in recall. All codes are available at
https://github.com/RadetzkyLi/TrajYOLO-SSD.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04822">SiCP: Simultaneous Individual and Cooperative Perception for 3D Object Detection in Connected and Automated Vehicles. (arXiv:2312.04822v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qu_D/0/1/0/all/0/1">Deyuan Qu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Qi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_T/0/1/0/all/0/1">Tianyu Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_A/0/1/0/all/0/1">Andy Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1">Hongsheng Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1">Heng Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1">Song Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1">Qing Yang</a></p>
<p>Cooperative perception for connected and automated vehicles is traditionally
achieved through the fusion of feature maps from two or more vehicles. However,
the absence of feature maps shared from other vehicles can lead to a
significant decline in object detection performance for cooperative perception
models compared to standalone 3D detection models. This drawback impedes the
adoption of cooperative perception as vehicle resources are often insufficient
to concurrently employ two perception models. To tackle this issue, we present
Simultaneous Individual and Cooperative Perception (SiCP), a generic framework
that supports a wide range of the state-of-the-art standalone perception
backbones and enhances them with a novel Dual-Perception Network (DP-Net)
designed to facilitate both individual and cooperative perception. In addition
to its lightweight nature with only 0.13M parameters, DP-Net is robust and
retains crucial gradient information during feature map fusion. As demonstrated
in a comprehensive evaluation on the OPV2V dataset, thanks to DP-Net, SiCP
surpasses state-of-the-art cooperative perception solutions while preserving
the performance of standalone perception solutions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04823">Assessing Neural Network Representations During Training Using Noise-Resilient Diffusion Spectral Entropy. (arXiv:2312.04823v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liao_D/0/1/0/all/0/1">Danqi Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Christensen_B/0/1/0/all/0/1">Benjamin W. Christensen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tong_A/0/1/0/all/0/1">Alexander Tong</a>, <a href="http://arxiv.org/find/cs/1/au:+Huguet_G/0/1/0/all/0/1">Guillaume Huguet</a>, <a href="http://arxiv.org/find/cs/1/au:+Wolf_G/0/1/0/all/0/1">Guy Wolf</a>, <a href="http://arxiv.org/find/cs/1/au:+Nickel_M/0/1/0/all/0/1">Maximilian Nickel</a>, <a href="http://arxiv.org/find/cs/1/au:+Adelstein_I/0/1/0/all/0/1">Ian Adelstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishnaswamy_S/0/1/0/all/0/1">Smita Krishnaswamy</a></p>
<p>Entropy and mutual information in neural networks provide rich information on
the learning process, but they have proven difficult to compute reliably in
high dimensions. Indeed, in noisy and high-dimensional data, traditional
estimates in ambient dimensions approach a fixed entropy and are prohibitively
hard to compute. To address these issues, we leverage data geometry to access
the underlying manifold and reliably compute these information-theoretic
measures. Specifically, we define diffusion spectral entropy (DSE) in neural
representations of a dataset as well as diffusion spectral mutual information
(DSMI) between different variables representing data. First, we show that they
form noise-resistant measures of intrinsic dimensionality and relationship
strength in high-dimensional simulated data that outperform classic Shannon
entropy, nonparametric estimation, and mutual information neural estimation
(MINE). We then study the evolution of representations in classification
networks with supervised learning, self-supervision, or overfitting. We observe
that (1) DSE of neural representations increases during training; (2) DSMI with
the class label increases during generalizable learning but stays stagnant
during overfitting; (3) DSMI with the input signal shows differing trends: on
MNIST it increases, while on CIFAR-10 and STL-10 it decreases. Finally, we show
that DSE can be used to guide better network initialization and that DSMI can
be used to predict downstream classification accuracy across 962 models on
ImageNet. The official implementation is available at
https://github.com/ChenLiu-1996/DiffusionSpectralEntropy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04831">Towards Stable and Faithful Inpainting. (arXiv:2312.04831v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yikai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1">Chenjie Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1">Yanwei Fu</a></p>
<p>Recent progress in inpainting increasingly relies on generative models,
leveraging their strong generation capabilities for addressing ill-conditioned
problems. However, this enhanced generation often introduces instability,
leading to arbitrary object generation within masked regions. This paper
proposes a balanced solution, emphasizing the importance of unmasked regions in
guiding inpainting while preserving generative capacity. Our approach, Aligned
Stable Inpainting with UnKnown Areas Prior (ASUKA), employs a
reconstruction-based masked auto-encoder (MAE) as a stable prior. Aligned with
the robust Stable Diffusion inpainting model (SD), ASUKA significantly improves
inpainting stability. ASUKA further aligns masked and unmasked regions through
an inpainting-specialized decoder, ensuring more faithful inpainting. To
validate effectiveness across domains and masking scenarios, we evaluate on
MISATO, a collection of several existing dataset. Results confirm ASUKA's
efficacy in both stability and fidelity compared to SD and other inpainting
algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04837">Localized Symbolic Knowledge Distillation for Visual Commonsense Models. (arXiv:2312.04837v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Jae Sung Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1">Jack Hessel</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandu_K/0/1/0/all/0/1">Khyathi Raghavi Chandu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1">Paul Pu Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1">Ximing Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1">Peter West</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Youngjae Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1">Qiuyuan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jianfeng Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1">Ali Farhadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1">Yejin Choi</a></p>
<p>Instruction following vision-language (VL) models offer a flexible interface
that supports a broad range of multimodal tasks in a zero-shot fashion.
However, interfaces that operate on full images do not directly enable the user
to "point to" and access specific regions within images. This capability is
important not only to support reference-grounded VL benchmarks, but also, for
practical applications that require precise within-image reasoning. We build
Localized Visual Commonsense models, which allow users to specify (multiple)
regions as input. We train our model by sampling localized commonsense
knowledge from a large language model (LLM): specifically, we prompt an LLM to
collect commonsense knowledge given a global literal image description and a
local literal region description automatically generated by a set of VL models.
With a separately trained critic model that selects high-quality examples, we
find that training on the localized commonsense corpus can successfully distill
existing VL models to support a reference-as-input interface. Empirical results
and human evaluations in a zero-shot setup demonstrate that our distillation
method results in more precise VL models of reasoning compared to a baseline of
passing a generated referring expression to an LLM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04838">Learning Generalizable Perceptual Representations for Data-Efficient No-Reference Image Quality Assessment. (arXiv:2312.04838v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Srinath_S/0/1/0/all/0/1">Suhas Srinath</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitra_S/0/1/0/all/0/1">Shankhanil Mitra</a>, <a href="http://arxiv.org/find/cs/1/au:+Rao_S/0/1/0/all/0/1">Shika Rao</a>, <a href="http://arxiv.org/find/cs/1/au:+Soundararajan_R/0/1/0/all/0/1">Rajiv Soundararajan</a></p>
<p>No-reference (NR) image quality assessment (IQA) is an important tool in
enhancing the user experience in diverse visual applications. A major drawback
of state-of-the-art NR-IQA techniques is their reliance on a large number of
human annotations to train models for a target IQA application. To mitigate
this requirement, there is a need for unsupervised learning of generalizable
quality representations that capture diverse distortions. We enable the
learning of low-level quality features agnostic to distortion types by
introducing a novel quality-aware contrastive loss. Further, we leverage the
generalizability of vision-language models by fine-tuning one such model to
extract high-level image quality information through relevant text prompts. The
two sets of features are combined to effectively predict quality by training a
simple regressor with very few samples on a target dataset. Additionally, we
design zero-shot quality predictions from both pathways in a completely blind
setting. Our experiments on diverse datasets encompassing various distortions
show the generalizability of the features and their superior performance in the
data-efficient and zero-shot settings. Code will be made available at
https://github.com/suhas-srinath/GRepQ.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04853">DiffCMR: Fast Cardiac MRI Reconstruction with Diffusion Probabilistic Models. (arXiv:2312.04853v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Xiang_T/0/1/0/all/0/1">Tianqi Xiang</a>, <a href="http://arxiv.org/find/eess/1/au:+Yue_W/0/1/0/all/0/1">Wenjun Yue</a>, <a href="http://arxiv.org/find/eess/1/au:+Lin_Y/0/1/0/all/0/1">Yiqun Lin</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1">Jiewen Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1">Zhenkun Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1">Xiaomeng Li</a></p>
<p>Performing magnetic resonance imaging (MRI) reconstruction from under-sampled
k-space data can accelerate the procedure to acquire MRI scans and reduce
patients' discomfort. The reconstruction problem is usually formulated as a
denoising task that removes the noise in under-sampled MRI image slices.
Although previous GAN-based methods have achieved good performance in image
denoising, they are difficult to train and require careful tuning of
hyperparameters. In this paper, we propose a novel MRI denoising framework
DiffCMR by leveraging conditional denoising diffusion probabilistic models.
Specifically, DiffCMR perceives conditioning signals from the under-sampled MRI
image slice and generates its corresponding fully-sampled MRI image slice.
During inference, we adopt a multi-round ensembling strategy to stabilize the
performance. We validate DiffCMR with cine reconstruction and T1/T2 mapping
tasks on MICCAI 2023 Cardiac MRI Reconstruction Challenge (CMRxRecon) dataset.
Results show that our method achieves state-of-the-art performance, exceeding
previous methods by a significant margin. Code is available at
https://github.com/xmed-lab/DiffCMR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04861">Radar Perception in Autonomous Driving: Exploring Different Data Representations. (arXiv:2312.04861v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1">Shanliang Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_R/0/1/0/all/0/1">Runwei Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1">Zitian Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Chenhang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yilu Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1">Yong Yue</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_E/0/1/0/all/0/1">Eng Gee Lim</a>, <a href="http://arxiv.org/find/cs/1/au:+Seo_H/0/1/0/all/0/1">Hyungjoon Seo</a>, <a href="http://arxiv.org/find/cs/1/au:+Man_K/0/1/0/all/0/1">Ka Lok Man</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiaohui Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1">Yutao Yue</a></p>
<p>With the rapid advancements of sensor technology and deep learning,
autonomous driving systems are providing safe and efficient access to
intelligent vehicles as well as intelligent transportation. Among these
equipped sensors, the radar sensor plays a crucial role in providing robust
perception information in diverse environmental conditions. This review focuses
on exploring different radar data representations utilized in autonomous
driving systems. Firstly, we introduce the capabilities and limitations of the
radar sensor by examining the working principles of radar perception and signal
processing of radar measurements. Then, we delve into the generation process of
five radar representations, including the ADC signal, radar tensor, point
cloud, grid map, and micro-Doppler signature. For each radar representation, we
examine the related datasets, methods, advantages and limitations. Furthermore,
we discuss the challenges faced in these data representations and propose
potential research directions. Above all, this comprehensive review offers an
in-depth insight into how these representations enhance autonomous system
capabilities, providing guidance for radar perception researchers. To
facilitate retrieval and comparison of different data representations, datasets
and methods, we provide an interactive website at
https://radar-camera-fusion.github.io/radar.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04862">Damage GAN: A Generative Model for Imbalanced Data. (arXiv:2312.04862v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Anaissi_A/0/1/0/all/0/1">Ali Anaissi</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1">Yuanzhe Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Braytee_A/0/1/0/all/0/1">Ali Braytee</a>, <a href="http://arxiv.org/find/cs/1/au:+Naji_M/0/1/0/all/0/1">Mohamad Naji</a>, <a href="http://arxiv.org/find/cs/1/au:+Alyassine_W/0/1/0/all/0/1">Widad Alyassine</a></p>
<p>This study delves into the application of Generative Adversarial Networks
(GANs) within the context of imbalanced datasets. Our primary aim is to enhance
the performance and stability of GANs in such datasets. In pursuit of this
objective, we introduce a novel network architecture known as Damage GAN,
building upon the ContraD GAN framework which seamlessly integrates GANs and
contrastive learning. Through the utilization of contrastive learning, the
discriminator is trained to develop an unsupervised representation capable of
distinguishing all provided samples. Our approach draws inspiration from the
straightforward framework for contrastive learning of visual representations
(SimCLR), leading to the formulation of a distinctive loss function. We also
explore the implementation of self-damaging contrastive learning (SDCLR) to
further enhance the optimization of the ContraD GAN model. Comparative
evaluations against baseline models including the deep convolutional GAN
(DCGAN) and ContraD GAN demonstrate the evident superiority of our proposed
model, Damage GAN, in terms of generated image distribution, model stability,
and image quality when applied to imbalanced datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04867">HandDiffuse: Generative Controllers for Two-Hand Interactions via Diffusion Models. (arXiv:2312.04867v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1">Pei Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Sihang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Hongdi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yiran Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jingya Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Jingyi Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1">Lan Xu</a></p>
<p>Existing hands datasets are largely short-range and the interaction is weak
due to the self-occlusion and self-similarity of hands, which can not yet fit
the need for interacting hands motion generation. To rescue the data scarcity,
we propose HandDiffuse12.5M, a novel dataset that consists of temporal
sequences with strong two-hand interactions. HandDiffuse12.5M has the largest
scale and richest interactions among the existing two-hand datasets. We further
present a strong baseline method HandDiffuse for the controllable motion
generation of interacting hands using various controllers. Specifically, we
apply the diffusion model as the backbone and design two motion representations
for different controllers. To reduce artifacts, we also propose Interaction
Loss which explicitly quantifies the dynamic interaction process. Our
HandDiffuse enables various applications with vivid two-hand interactions,
i.e., motion in-betweening and trajectory control. Experiments show that our
method outperforms the state-of-the-art techniques in motion generation and can
also contribute to data augmentation for other datasets. Our dataset,
corresponding codes, and pre-trained models will be disseminated to the
community for future research towards two-hand interaction modeling.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04869">Adapting Vision Transformer for Efficient Change Detection. (arXiv:2312.04869v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuxiang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1">Yanni Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1">Bo Du</a></p>
<p>Most change detection models based on vision transformers currently follow a
"pretraining then fine-tuning" strategy. This involves initializing the model
weights using large scale classification datasets, which can be either natural
images or remote sensing images. However, fully tuning such a model requires
significant time and resources. In this paper, we propose an efficient tuning
approach that involves freezing the parameters of the pretrained image encoder
and introducing additional training parameters. Through this approach, we have
achieved competitive or even better results while maintaining extremely low
resource consumption across six change detection benchmarks. For example,
training time on LEVIR-CD, a change detection benchmark, is only half an hour
with 9 GB memory usage, which could be very convenient for most researchers.
Additionally, the decoupled tuning framework can be extended to any pretrained
model for semantic change detection and multi temporal change detection as
well. We hope that our proposed approach will serve as a part of foundational
model to inspire more unified training approaches on change detection in the
future.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04874">Interpretable Underwater Diver Gesture Recognition. (arXiv:2312.04874v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mangalvedhekar_S/0/1/0/all/0/1">Sudeep Mangalvedhekar</a>, <a href="http://arxiv.org/find/cs/1/au:+Nahar_S/0/1/0/all/0/1">Shreyas Nahar</a>, <a href="http://arxiv.org/find/cs/1/au:+Maskare_S/0/1/0/all/0/1">Sudarshan Maskare</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahajan_K/0/1/0/all/0/1">Kaushal Mahajan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bagade_D/0/1/0/all/0/1">Dr. Anant Bagade</a></p>
<p>In recent years, usage and applications of Autonomous Underwater Vehicles has
grown rapidly. Interaction of divers with the AUVs remains an integral part of
the usage of AUVs for various applications and makes building robust and
efficient underwater gesture recognition systems extremely important. In this
paper, we propose an Underwater Gesture Recognition system trained on the
Cognitive Autonomous Diving Buddy Underwater gesture dataset using deep
learning that achieves 98.01\% accuracy on the dataset, which to the best of
our knowledge is the best performance achieved on this dataset at the time of
writing this paper. We also improve the Gesture Recognition System
Interpretability by using XAI techniques to visualize the model's predictions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04875">MVDD: Multi-View Depth Diffusion Models. (arXiv:2312.04875v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1">Qiangeng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_F/0/1/0/all/0/1">Feitong Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chai_M/0/1/0/all/0/1">Menglei Chai</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shichen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pandey_R/0/1/0/all/0/1">Rohit Pandey</a>, <a href="http://arxiv.org/find/cs/1/au:+Fanello_S/0/1/0/all/0/1">Sean Fanello</a>, <a href="http://arxiv.org/find/cs/1/au:+Kadambi_A/0/1/0/all/0/1">Achuta Kadambi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yinda Zhang</a></p>
<p>Denoising diffusion models have demonstrated outstanding results in 2D image
generation, yet it remains a challenge to replicate its success in 3D shape
generation. In this paper, we propose leveraging multi-view depth, which
represents complex 3D shapes in a 2D data format that is easy to denoise. We
pair this representation with a diffusion model, MVDD, that is capable of
generating high-quality dense point clouds with 20K+ points with fine-grained
details. To enforce 3D consistency in multi-view depth, we introduce an
epipolar line segment attention that conditions the denoising step for a view
on its neighboring views. Additionally, a depth fusion module is incorporated
into diffusion steps to further ensure the alignment of depth maps. When
augmented with surface reconstruction, MVDD can also produce high-quality 3D
meshes. Furthermore, MVDD stands out in other tasks such as depth completion,
and can serve as a 3D prior, significantly boosting many downstream tasks, such
as GAN inversion. State-of-the-art results from extensive experiments
demonstrate MVDD's excellent ability in 3D shape generation, depth completion,
and its potential as a 3D prior for downstream tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04884">UDiffText: A Unified Framework for High-quality Text Synthesis in Arbitrary Images via Character-aware Diffusion Models. (arXiv:2312.04884v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yiming Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lian_Z/0/1/0/all/0/1">Zhouhui Lian</a></p>
<p>Text-to-Image (T2I) generation methods based on diffusion model have garnered
significant attention in the last few years. Although these image synthesis
methods produce visually appealing results, they frequently exhibit spelling
errors when rendering text within the generated images. Such errors manifest as
missing, incorrect or extraneous characters, thereby severely constraining the
performance of text image generation based on diffusion models. To address the
aforementioned issue, this paper proposes a novel approach for text image
generation, utilizing a pre-trained diffusion model (i.e., Stable Diffusion
[27]). Our approach involves the design and training of a light-weight
character-level text encoder, which replaces the original CLIP encoder and
provides more robust text embeddings as conditional guidance. Then, we
fine-tune the diffusion model using a large-scale dataset, incorporating local
attention control under the supervision of character-level segmentation maps.
Finally, by employing an inference stage refinement process, we achieve a
notably high sequence accuracy when synthesizing text in arbitrarily given
images. Both qualitative and quantitative results demonstrate the superiority
of our method to the state of the art. Furthermore, we showcase several
potential applications of the proposed UDiffText, including text-centric image
synthesis, scene text editing, etc. Code and model will be available at
https://github.com/ZYM-PKU/UDiffText .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04885">VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement. (arXiv:2312.04885v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">Hanjung Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1">Jaehyun Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Heo_M/0/1/0/all/0/1">Miran Heo</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1">Sukjun Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1">Seoung Wug Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Seon Joo Kim</a></p>
<p>In recent years, online Video Instance Segmentation (VIS) methods have shown
remarkable advancement with their powerful query-based detectors. Utilizing the
output queries of the detector at the frame level, these methods achieve high
accuracy on challenging benchmarks. However, we observe the heavy reliance of
these methods on the location information that leads to incorrect matching when
positional cues are insufficient for resolving ambiguities. Addressing this
issue, we present VISAGE that enhances instance association by explicitly
leveraging appearance information. Our method involves a generation of queries
that embed appearances from backbone feature maps, which in turn get used in
our suggested simple tracker for robust associations. Finally, enabling
accurate matching in complex scenarios by resolving the issue of over-reliance
on location information, we achieve competitive performance on multiple VIS
benchmarks. For instance, on YTVIS19 and YTVIS21, our method achieves 54.5 AP
and 50.8 AP. Furthermore, to highlight appearance-awareness not fully addressed
by existing benchmarks, we generate a synthetic dataset where our method
outperforms others significantly by leveraging the appearance cue. Code will be
made available at https://github.com/KimHanjung/VISAGE.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04891">Cross-BERT for Point Cloud Pretraining. (arXiv:2312.04891v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Peng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1">Zeyong Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Zhe Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1">Mingqiang Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1">Junhui Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Nan_L/0/1/0/all/0/1">Liangliang Nan</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1">Jing Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1">Haoran Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Fu Lee Wang</a></p>
<p>Introducing BERT into cross-modal settings raises difficulties in its
optimization for handling multiple modalities. Both the BERT architecture and
training objective need to be adapted to incorporate and model information from
different modalities. In this paper, we address these challenges by exploring
the implicit semantic and geometric correlations between 2D and 3D data of the
same objects/scenes. We propose a new cross-modal BERT-style self-supervised
learning paradigm, called Cross-BERT. To facilitate pretraining for irregular
and sparse point clouds, we design two self-supervised tasks to boost
cross-modal interaction. The first task, referred to as Point-Image Alignment,
aims to align features between unimodal and cross-modal representations to
capture the correspondences between the 2D and 3D modalities. The second task,
termed Masked Cross-modal Modeling, further improves mask modeling of BERT by
incorporating high-dimensional semantic information obtained by cross-modal
interaction. By performing cross-modal interaction, Cross-BERT can smoothly
reconstruct the masked tokens during pretraining, leading to notable
performance enhancements for downstream tasks. Through empirical evaluation, we
demonstrate that Cross-BERT outperforms existing state-of-the-art methods in 3D
downstream applications. Our work highlights the effectiveness of leveraging
cross-modal 2D knowledge to strengthen 3D point cloud representation and the
transferable capability of BERT across modalities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04893">Annotation-Free Group Robustness via Loss-Based Resampling. (arXiv:2312.04893v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ghaznavi_M/0/1/0/all/0/1">Mahdi Ghaznavi</a>, <a href="http://arxiv.org/find/cs/1/au:+Asadollahzadeh_H/0/1/0/all/0/1">Hesam Asadollahzadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Araghi_H/0/1/0/all/0/1">HamidReza Yaghoubi Araghi</a>, <a href="http://arxiv.org/find/cs/1/au:+Noohdani_F/0/1/0/all/0/1">Fahimeh Hosseini Noohdani</a>, <a href="http://arxiv.org/find/cs/1/au:+Rohban_M/0/1/0/all/0/1">Mohammad Hossein Rohban</a>, <a href="http://arxiv.org/find/cs/1/au:+Baghshah_M/0/1/0/all/0/1">Mahdieh Soleymani Baghshah</a></p>
<p>It is well-known that training neural networks for image classification with
empirical risk minimization (ERM) makes them vulnerable to relying on spurious
attributes instead of causal ones for prediction. Previously, deep feature
re-weighting (DFR) has proposed retraining the last layer of a pre-trained
network on balanced data concerning spurious attributes, making it robust to
spurious correlation. However, spurious attribute annotations are not always
available. In order to provide group robustness without such annotations, we
propose a new method, called loss-based feature re-weighting (LFR), in which we
infer a grouping of the data by evaluating an ERM-pre-trained model on a small
left-out split of the training data. Then, a balanced number of samples is
chosen by selecting high-loss samples from misclassified data points and
low-loss samples from correctly-classified ones. Finally, we retrain the last
layer on the selected balanced groups to make the model robust to spurious
correlation. For a complete assessment, we evaluate LFR on various versions of
Waterbirds and CelebA datasets with different spurious correlations, which is a
novel technique for observing the model's performance in a wide range of
spuriosity rates. While LFR is extremely fast and straightforward, it
outperforms the previous methods that do not assume group label availability,
as well as the DFR with group annotations provided, in cases of high spurious
correlation in the training data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04913">SA-Attack: Improving Adversarial Transferability of Vision-Language Pre-training Models via Self-Augmentation. (arXiv:2312.04913v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1">Bangyan He</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1">Xiaojun Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1">Siyuan Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lou_T/0/1/0/all/0/1">Tianrui Lou</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1">Xiaochun Cao</a></p>
<p>Current Visual-Language Pre-training (VLP) models are vulnerable to
adversarial examples. These adversarial examples present substantial security
risks to VLP models, as they can leverage inherent weaknesses in the models,
resulting in incorrect predictions. In contrast to white-box adversarial
attacks, transfer attacks (where the adversary crafts adversarial examples on a
white-box model to fool another black-box model) are more reflective of
real-world scenarios, thus making them more meaningful for research. By
summarizing and analyzing existing research, we identified two factors that can
influence the efficacy of transfer attacks on VLP models: inter-modal
interaction and data diversity. Based on these insights, we propose a
self-augment-based transfer attack method, termed SA-Attack. Specifically,
during the generation of adversarial images and adversarial texts, we apply
different data augmentation methods to the image modality and text modality,
respectively, with the aim of improving the adversarial transferability of the
generated adversarial images and texts. Experiments conducted on the FLickr30K
and COCO datasets have validated the effectiveness of our method. Our code will
be available after this paper is accepted.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04926">Accelerating Convolutional Neural Network Pruning via Spatial Aura Entropy. (arXiv:2312.04926v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Musat_B/0/1/0/all/0/1">Bogdan Musat</a>, <a href="http://arxiv.org/find/cs/1/au:+Andonie_R/0/1/0/all/0/1">Razvan Andonie</a></p>
<p>In recent years, pruning has emerged as a popular technique to reduce the
computational complexity and memory footprint of Convolutional Neural Network
(CNN) models. Mutual Information (MI) has been widely used as a criterion for
identifying unimportant filters to prune. However, existing methods for MI
computation suffer from high computational cost and sensitivity to noise,
leading to suboptimal pruning performance. We propose a novel method to improve
MI computation for CNN pruning, using the spatial aura entropy. The spatial
aura entropy is useful for evaluating the heterogeneity in the distribution of
the neural activations over a neighborhood, providing information about local
features. Our method effectively improves the MI computation for CNN pruning,
leading to more robust and efficient pruning. Experimental results on the
CIFAR-10 benchmark dataset demonstrate the superiority of our approach in terms
of pruning performance and computational efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04931">Retrieval-based Video Language Model for Efficient Long Video Question Answering. (arXiv:2312.04931v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jiaqi Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1">Cuiling Lan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1">Wenxuan Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xuejin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yan Lu</a></p>
<p>The remarkable natural language understanding, reasoning, and generation
capabilities of large language models (LLMs) have made them attractive for
application to video question answering (Video QA) tasks, utilizing video
tokens as contextual input. However, employing LLMs for long video
understanding presents significant challenges and remains under-explored. The
extensive number of video tokens leads to considerable computational costs for
LLMs while using aggregated tokens results in loss of vision details. Moreover,
the presence of abundant question-irrelevant tokens introduces noise to the
video QA process. To address these issues, we introduce a simple yet effective
retrieval-based video language model (R-VLM) for efficient and interpretable
long video QA. Specifically, given a question (query) and a long video, our
model identifies and selects the most relevant $K$ video chunks and uses their
associated visual tokens to serve as context for the LLM inference. This
effectively reduces the number of video tokens, eliminates noise interference,
and enhances system performance. Our experimental results validate the
effectiveness of our framework for comprehending long videos. Furthermore,
based on the retrieved chunks, our model is interpretable that provides the
justifications on where we get the answers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04947">Benchmarking and Analysis of Unsupervised Object Segmentation from Real-world Single Images. (arXiv:2312.04947v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yafei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1">Bo Yang</a></p>
<p>In this paper, we study the problem of unsupervised object segmentation from
single images. We do not introduce a new algorithm, but systematically
investigate the effectiveness of existing unsupervised models on challenging
real-world images. We first introduce seven complexity factors to
quantitatively measure the distributions of background and foreground object
biases in appearance and geometry for datasets with human annotations. With the
aid of these factors, we empirically find that, not surprisingly, existing
unsupervised models fail to segment generic objects in real-world images,
although they can easily achieve excellent performance on numerous simple
synthetic datasets, due to the vast gap in objectness biases between synthetic
and real images. By conducting extensive experiments on multiple groups of
ablated real-world datasets, we ultimately find that the key factors underlying
the failure of existing unsupervised models on real-world images are the
challenging distributions of background and foreground object biases in
appearance and geometry. Because of this, the inductive biases introduced in
existing unsupervised models can hardly capture the diverse object
distributions. Our research results suggest that future work should exploit
more explicit objectness biases in the network design.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04948">Scientific Preparation for CSST: Classification of Galaxy and Nebula/Star Cluster Based on Deep Learning. (arXiv:2312.04948v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuquan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1">Zhong Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Feng Wang</a>, Lam, <a href="http://arxiv.org/find/cs/1/au:+I_M/0/1/0/all/0/1">Man I</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1">Hui Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Mei_Y/0/1/0/all/0/1">Ying Mei</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1">Lei Tan</a></p>
<p>The Chinese Space Station Telescope (abbreviated as CSST) is a future
advanced space telescope. Real-time identification of galaxy and nebula/star
cluster (abbreviated as NSC) images is of great value during CSST survey. While
recent research on celestial object recognition has progressed, the rapid and
efficient identification of high-resolution local celestial images remains
challenging. In this study, we conducted galaxy and NSC image classification
research using deep learning methods based on data from the Hubble Space
Telescope. We built a Local Celestial Image Dataset and designed a deep
learning model named HR-CelestialNet for classifying images of the galaxy and
NSC. HR-CelestialNet achieved an accuracy of 89.09% on the testing set,
outperforming models such as AlexNet, VGGNet and ResNet, while demonstrating
faster recognition speeds. Furthermore, we investigated the factors influencing
CSST image quality and evaluated the generalization ability of HR-CelestialNet
on the blurry image dataset, demonstrating its robustness to low image quality.
The proposed method can enable real-time identification of celestial images
during CSST survey mission.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04960">MIMIR: Masked Image Modeling for Mutual Information-based Adversarial Robustness. (arXiv:2312.04960v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiaoyun Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1">Shujian Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jingzheng Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Picek_S/0/1/0/all/0/1">Stjepan Picek</a></p>
<p>Vision Transformers (ViTs) achieve superior performance on various tasks
compared to convolutional neural networks (CNNs), but ViTs are also vulnerable
to adversarial attacks. Adversarial training is one of the most successful
methods to build robust CNN models. Thus, recent works explored new
methodologies for adversarial training of ViTs based on the differences between
ViTs and CNNs, such as better training strategies, preventing attention from
focusing on a single block, or discarding low-attention embeddings. However,
these methods still follow the design of traditional supervised adversarial
training, limiting the potential of adversarial training on ViTs. This paper
proposes a novel defense method, MIMIR, which aims to build a different
adversarial training methodology by utilizing Masked Image Modeling at
pre-training. We create an autoencoder that accepts adversarial examples as
input but takes the clean examples as the modeling target. Then, we create a
mutual information (MI) penalty following the idea of the Information
Bottleneck. Among the two information source inputs and corresponding
adversarial perturbation, the perturbation information is eliminated due to the
constraint of the modeling target. Next, we provide a theoretical analysis of
MIMIR using the bounds of the MI penalty. We also design two adaptive attacks
when the adversary is aware of the MIMIR defense and show that MIMIR still
performs well. The experimental results show that MIMIR improves (natural and
adversarial) accuracy on average by 4.19\% on CIFAR-10 and 5.52\% on
ImageNet-1K, compared to baselines. On Tiny-ImageNet, we obtained improved
natural accuracy of 2.99\% on average and comparable adversarial accuracy. Our
code and trained models are publicly
available\footnote{\url{https://anonymous.4open.science/r/MIMIR-5444/README.md}}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04961">DeepFidelity: Perceptual Forgery Fidelity Assessment for Deepfake Detection. (arXiv:2312.04961v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1">Chunlei Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1">Huiqing Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1">Decheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1">Nannan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1">Ruimin Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xinbo Gao</a></p>
<p>Deepfake detection refers to detecting artificially generated or edited faces
in images or videos, which plays an essential role in visual information
security. Despite promising progress in recent years, Deepfake detection
remains a challenging problem due to the complexity and variability of face
forgery techniques. Existing Deepfake detection methods are often devoted to
extracting features by designing sophisticated networks but ignore the
influence of perceptual quality of faces. Considering the complexity of the
quality distribution of both real and fake faces, we propose a novel Deepfake
detection framework named DeepFidelity to adaptively distinguish real and fake
faces with varying image quality by mining the perceptual forgery fidelity of
face images. Specifically, we improve the model's ability to identify complex
samples by mapping real and fake face data of different qualities to different
scores to distinguish them in a more detailed way. In addition, we propose a
network structure called Symmetric Spatial Attention Augmentation based vision
Transformer (SSAAFormer), which uses the symmetry of face images to promote the
network to model the geographic long-distance relationship at the shallow level
and augment local features. Extensive experiments on multiple benchmark
datasets demonstrate the superiority of the proposed method over
state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04962">Point2CAD: Reverse Engineering CAD Models from 3D Point Clouds. (arXiv:2312.04962v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yujia Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Obukhov_A/0/1/0/all/0/1">Anton Obukhov</a>, <a href="http://arxiv.org/find/cs/1/au:+Wegner_J/0/1/0/all/0/1">Jan Dirk Wegner</a>, <a href="http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1">Konrad Schindler</a></p>
<p>Computer-Aided Design (CAD) model reconstruction from point clouds is an
important problem at the intersection of computer vision, graphics, and machine
learning; it saves the designer significant time when iterating on in-the-wild
objects. Recent advancements in this direction achieve relatively reliable
semantic segmentation but still struggle to produce an adequate topology of the
CAD model. In this work, we analyze the current state of the art for that
ill-posed task and identify shortcomings of existing methods. We propose a
hybrid analytic-neural reconstruction scheme that bridges the gap between
segmented point clouds and structured CAD models and can be readily combined
with different segmentation backbones. Moreover, to power the surface fitting
stage, we propose a novel implicit neural representation of freeform surfaces,
driving up the performance of our overall CAD reconstruction scheme. We
extensively evaluate our method on the popular ABC benchmark of CAD models and
set a new state-of-the-art for that dataset. Project page:
https://www.obukhov.ai/point2cad}{https://www.obukhov.ai/point2cad.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04963">Text-to-3D Generation with Bidirectional Diffusion using both 2D and 3D priors. (arXiv:2312.04963v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1">Lihe Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1">Shaocong Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zhanpeng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zibin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yiyuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_K/0/1/0/all/0/1">Kaixiong Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1">Dan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_T/0/1/0/all/0/1">Tianfan Xue</a></p>
<p>Most 3D generation research focuses on up-projecting 2D foundation models
into the 3D space, either by minimizing 2D Score Distillation Sampling (SDS)
loss or fine-tuning on multi-view datasets. Without explicit 3D priors, these
methods often lead to geometric anomalies and multi-view inconsistency.
Recently, researchers have attempted to improve the genuineness of 3D objects
by directly training on 3D datasets, albeit at the cost of low-quality texture
generation due to the limited texture diversity in 3D datasets. To harness the
advantages of both approaches, we propose Bidirectional Diffusion(BiDiff), a
unified framework that incorporates both a 3D and a 2D diffusion process, to
preserve both 3D fidelity and 2D texture richness, respectively. Moreover, as a
simple combination may yield inconsistent generation results, we further bridge
them with novel bidirectional guidance. In addition, our method can be used as
an initialization of optimization-based models to further improve the quality
of 3D model and efficiency of optimization, reducing the generation process
from 3.4 hours to 20 minutes. Experimental results have shown that our model
achieves high-quality, diverse, and scalable 3D generation. Project website:
https://bidiff.github.io/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04964">ZePT: Zero-Shot Pan-Tumor Segmentation via Query-Disentangling and Self-Prompting. (arXiv:2312.04964v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yankai Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zhongzhen Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Rongzhao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaofan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shaoting Zhang</a></p>
<p>The long-tailed distribution problem in medical image analysis reflects a
high prevalence of common conditions and a low prevalence of rare ones, which
poses a significant challenge in developing a unified model capable of
identifying rare or novel tumor categories not encountered during training. In
this paper, we propose a new zero-shot pan-tumor segmentation framework (ZePT)
based on query-disentangling and self-prompting to segment unseen tumor
categories beyond the training set. ZePT disentangles the object queries into
two subsets and trains them in two stages. Initially, it learns a set of
fundamental queries for organ segmentation through an object-aware feature
grouping strategy, which gathers organ-level visual features. Subsequently, it
refines the other set of advanced queries that focus on the auto-generated
visual prompts for unseen tumor segmentation. Moreover, we introduce
query-knowledge alignment at the feature level to enhance each query's
discriminative representation and generalizability. Extensive experiments on
various tumor segmentation tasks demonstrate the performance superiority of
ZePT, which surpasses the previous counterparts and evidence the promising
ability for zero-shot tumor segmentation in real-world settings. Codes will be
made publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04965">Inversion-Free Image Editing with Natural Language. (arXiv:2312.04965v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Sihan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yidong Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1">Jiayi Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1">Ziqiao Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1">Joyce Chai</a></p>
<p>Despite recent advances in inversion-based editing, text-guided image
manipulation remains challenging for diffusion models. The primary bottlenecks
include 1) the time-consuming nature of the inversion process; 2) the struggle
to balance consistency with accuracy; 3) the lack of compatibility with
efficient consistency sampling methods used in consistency models. To address
the above issues, we start by asking ourselves if the inversion process can be
eliminated for editing. We show that when the initial sample is known, a
special variance schedule reduces the denoising step to the same form as the
multi-step consistency sampling. We name this Denoising Diffusion Consistent
Model (DDCM), and note that it implies a virtual inversion strategy without
explicit inversion in sampling. We further unify the attention control
mechanisms in a tuning-free framework for text-guided editing. Combining them,
we present inversion-free editing (InfEdit), which allows for consistent and
faithful editing for both rigid and non-rigid semantic changes, catering to
intricate modifications without compromising on the image's integrity and
explicit inversion. Through extensive experiments, InfEdit shows strong
performance in various editing tasks and also maintains a seamless workflow
(less than 3 seconds on one single A40), demonstrating the potential for
real-time applications. Project Page: https://sled-group.github.io/InfEdit/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04966">Customizing Motion in Text-to-Video Diffusion Models. (arXiv:2312.04966v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Materzynska_J/0/1/0/all/0/1">Joanna Materzynska</a>, <a href="http://arxiv.org/find/cs/1/au:+Sivic_J/0/1/0/all/0/1">Josef Sivic</a>, <a href="http://arxiv.org/find/cs/1/au:+Shechtman_E/0/1/0/all/0/1">Eli Shechtman</a>, <a href="http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1">Antonio Torralba</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Richard Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Russell_B/0/1/0/all/0/1">Bryan Russell</a></p>
<p>We introduce an approach for augmenting text-to-video generation models with
customized motions, extending their capabilities beyond the motions depicted in
the original training data. By leveraging a few video samples demonstrating
specific movements as input, our method learns and generalizes the input motion
patterns for diverse, text-specified scenarios. Our contributions are
threefold. First, to achieve our results, we finetune an existing text-to-video
model to learn a novel mapping between the depicted motion in the input
examples to a new unique token. To avoid overfitting to the new custom motion,
we introduce an approach for regularization over videos. Second, by leveraging
the motion priors in a pretrained model, our method can produce novel videos
featuring multiple people doing the custom motion, and can invoke the motion in
combination with other motions. Furthermore, our approach extends to the
multimodal customization of motion and appearance of individualized subjects,
enabling the generation of videos featuring unique characters and distinct
motions. Third, to validate our method, we introduce an approach for
quantitatively evaluating the learned custom motion and perform a systematic
ablation study. We show that our method significantly outperforms prior
appearance-based customization approaches when extended to the motion
customization task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05006">Decoupling Degradation and Content Processing for Adverse Weather Image Restoration. (arXiv:2312.05006v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1">Xueyang Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1">Peng-Tao Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jie Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1">Mi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1">Zheng-Jun Zha</a></p>
<p>Adverse weather image restoration strives to recover clear images from those
affected by various weather types, such as rain, haze, and snow. Each weather
type calls for a tailored degradation removal approach due to its unique impact
on images. Conversely, content reconstruction can employ a uniform approach, as
the underlying image content remains consistent. Although previous techniques
can handle multiple weather types within a single network, they neglect the
crucial distinction between these two processes, limiting the quality of
restored images. This work introduces a novel adverse weather image restoration
method, called DDCNet, which decouples the degradation removal and content
reconstruction process at the feature level based on their channel statistics.
Specifically, we exploit the unique advantages of the Fourier transform in both
these two processes: (1) the degradation information is mainly located in the
amplitude component of the Fourier domain, and (2) the Fourier domain contains
global information. The former facilitates channel-dependent degradation
removal operation, allowing the network to tailor responses to various adverse
weather types; the latter, by integrating Fourier's global properties into
channel-independent content features, enhances network capacity for consistent
global content reconstruction. We further augment the degradation removal
process with a degradation mapping loss function. Extensive experiments
demonstrate our method achieves state-of-the-art performance in multiple
adverse weather removal benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05024">A Unified Framework for Unsupervised Domain Adaptation based on Instance Weighting. (arXiv:2312.05024v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jinjing Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1">Feiyang Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_Q/0/1/0/all/0/1">Qiao Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1">Pengxin Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1">Qiang Yang</a></p>
<p>Despite the progress made in domain adaptation, solving Unsupervised Domain
Adaptation (UDA) problems with a general method under complex conditions caused
by label shifts between domains remains a formidable task. In this work, we
comprehensively investigate four distinct UDA settings including closed set
domain adaptation, partial domain adaptation, open set domain adaptation, and
universal domain adaptation, where shared common classes between source and
target domains coexist alongside domain-specific private classes. The prominent
challenges inherent in diverse UDA settings center around the discrimination of
common/private classes and the precise measurement of domain discrepancy. To
surmount these challenges effectively, we propose a novel yet effective method
called Learning Instance Weighting for Unsupervised Domain Adaptation (LIWUDA),
which caters to various UDA settings. Specifically, the proposed LIWUDA method
constructs a weight network to assign weights to each instance based on its
probability of belonging to common classes, and designs Weighted Optimal
Transport (WOT) for domain alignment by leveraging instance weights.
Additionally, the proposed LIWUDA method devises a Separate and Align (SA) loss
to separate instances with low similarities and align instances with high
similarities. To guide the learning of the weight network, Intra-domain Optimal
Transport (IOT) is proposed to enforce the weights of instances in common
classes to follow a uniform distribution. Through the integration of those
three components, the proposed LIWUDA method demonstrates its capability to
address all four UDA settings in a unified manner. Experimental evaluations
conducted on three benchmark datasets substantiate the effectiveness of the
proposed LIWUDA method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05028">Cluster images with AntClust: a clustering algorithm based on the chemical recognition system of ants. (arXiv:2312.05028v1 [cs.DC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oed_W/0/1/0/all/0/1">Winfried Gero Oed</a>, <a href="http://arxiv.org/find/cs/1/au:+Memarmoshrefi_P/0/1/0/all/0/1">Parisa Memarmoshrefi</a></p>
<p>We implement AntClust, a clustering algorithm based on the chemical
recognition system of ants and use it to cluster images of cars. We will give a
short recap summary of the main working principles of the algorithm as devised
by the original paper [1]. Further, we will describe how to define a similarity
function for images and how the implementation is used to cluster images of
cars from the vehicle re-identification data set. We then test the clustering
performance of AntClust against DBSCAN, HDBSCAN and OPTICS. Finally one of the
core parts in AntClust, the rule set can be easily redefined with our
implementation, enabling a way for other bio-inspired algorithms to find rules
in an automated process. The implementation can be found on GitLab [9].
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05031">Synthesizing Traffic Datasets using Graph Neural Networks. (arXiv:2312.05031v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_Criado_D/0/1/0/all/0/1">Daniel Rodriguez-Criado</a>, <a href="http://arxiv.org/find/cs/1/au:+Chli_M/0/1/0/all/0/1">Maria Chli</a>, <a href="http://arxiv.org/find/cs/1/au:+Manso_L/0/1/0/all/0/1">Luis J. Manso</a>, <a href="http://arxiv.org/find/cs/1/au:+Vogiatzis_G/0/1/0/all/0/1">George Vogiatzis</a></p>
<p>Traffic congestion in urban areas presents significant challenges, and
Intelligent Transportation Systems (ITS) have sought to address these via
automated and adaptive controls. However, these systems often struggle to
transfer simulated experiences to real-world scenarios. This paper introduces a
novel methodology for bridging this `sim-real' gap by creating photorealistic
images from 2D traffic simulations and recorded junction footage. We propose a
novel image generation approach, integrating a Conditional Generative
Adversarial Network with a Graph Neural Network (GNN) to facilitate the
creation of realistic urban traffic images. We harness GNNs' ability to process
information at different levels of abstraction alongside segmented images for
preserving locality data. The presented architecture leverages the power of
SPADE and Graph ATtention (GAT) network models to create images based on
simulated traffic scenarios. These images are conditioned by factors such as
entity positions, colors, and time of day. The uniqueness of our approach lies
in its ability to effectively translate structured and human-readable
conditions, encoded as graphs, into realistic images. This advancement
contributes to applications requiring rich traffic image datasets, from data
augmentation to urban traffic solutions. We further provide an application to
test the model's capabilities, including generating images with manually
defined positions for various entities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05038">Prompt-In-Prompt Learning for Universal Image Restoration. (arXiv:2312.05038v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zilong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1">Yiming Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1">Chenglong Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Junping Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shan_H/0/1/0/all/0/1">Hongming Shan</a></p>
<p>Image restoration, which aims to retrieve and enhance degraded images, is
fundamental across a wide range of applications. While conventional deep
learning approaches have notably improved the image quality across various
tasks, they still suffer from (i) the high storage cost needed for various
task-specific models and (ii) the lack of interactivity and flexibility,
hindering their wider application. Drawing inspiration from the pronounced
success of prompts in both linguistic and visual domains, we propose novel
Prompt-In-Prompt learning for universal image restoration, named PIP. First, we
present two novel prompts, a degradation-aware prompt to encode high-level
degradation knowledge and a basic restoration prompt to provide essential
low-level information. Second, we devise a novel prompt-to-prompt interaction
module to fuse these two prompts into a universal restoration prompt. Third, we
introduce a selective prompt-to-feature interaction module to modulate the
degradation-related feature. By doing so, the resultant PIP works as a
plug-and-play module to enhance existing restoration models for universal image
restoration. Extensive experimental results demonstrate the superior
performance of PIP on multiple restoration tasks, including image denoising,
deraining, dehazing, deblurring, and low-light enhancement. Remarkably, PIP is
interpretable, flexible, efficient, and easy-to-use, showing promising
potential for real-world applications. The code is available at
https://github.com/longzilicart/pip_universal.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05039">SmartMask: Context Aware High-Fidelity Mask Generation for Fine-grained Object Insertion and Layout Control. (arXiv:2312.05039v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1">Jaskirat Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jianming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Smith_C/0/1/0/all/0/1">Cameron Smith</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zhe Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1">Liang Zheng</a></p>
<p>The field of generative image inpainting and object insertion has made
significant progress with the recent advent of latent diffusion models.
Utilizing a precise object mask can greatly enhance these applications.
However, due to the challenges users encounter in creating high-fidelity masks,
there is a tendency for these methods to rely on more coarse masks (e.g.,
bounding box) for these applications. This results in limited control and
compromised background content preservation. To overcome these limitations, we
introduce SmartMask, which allows any novice user to create detailed masks for
precise object insertion. Combined with a ControlNet-Inpaint model, our
experiments demonstrate that SmartMask achieves superior object insertion
quality, preserving the background content more effectively than previous
methods. Notably, unlike prior works the proposed approach can also be used
even without user-mask guidance, which allows it to perform mask-free object
insertion at diverse positions and scales. Furthermore, we find that when used
iteratively with a novel instruction-tuning based planning model, SmartMask can
be used to design detailed layouts from scratch. As compared with user-scribble
based layout design, we observe that SmartMask allows for better quality
outputs with layout-to-image generation methods. Project page is available at
https://smartmask-gen.github.io
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05046">MuVieCAST: Multi-View Consistent Artistic Style Transfer. (arXiv:2312.05046v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ibrahimli_N/0/1/0/all/0/1">Nail Ibrahimli</a>, <a href="http://arxiv.org/find/cs/1/au:+Kooij_J/0/1/0/all/0/1">Julian F. P. Kooij</a>, <a href="http://arxiv.org/find/cs/1/au:+Nan_L/0/1/0/all/0/1">Liangliang Nan</a></p>
<p>We introduce MuVieCAST, a modular multi-view consistent style transfer
network architecture that enables consistent style transfer between multiple
viewpoints of the same scene. This network architecture supports both sparse
and dense views, making it versatile enough to handle a wide range of
multi-view image datasets. The approach consists of three modules that perform
specific tasks related to style transfer, namely content preservation, image
transformation, and multi-view consistency enforcement. We extensively evaluate
our approach across multiple application domains including depth-map-based
point cloud fusion, mesh reconstruction, and novel-view synthesis. Our
experiments reveal that the proposed framework achieves an exceptional
generation of stylized images, exhibiting consistent outcomes across
perspectives. A user study focusing on novel-view synthesis further confirms
these results, with approximately 68\% of cases participants expressing a
preference for our generated outputs compared to the recent state-of-the-art
method. Our modular framework is extensible and can easily be integrated with
various backbone architectures, making it a flexible solution for multi-view
style transfer. More results are demonstrated on our project page:
muviecast.github.io
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05086">I Can&#x27;t Believe It&#x27;s Not Better: In-air Movement For Alzheimer Handwriting Synthetic Generation. (arXiv:2312.05086v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bensalah_A/0/1/0/all/0/1">Asma Bensalah</a>, <a href="http://arxiv.org/find/cs/1/au:+Parziale_A/0/1/0/all/0/1">Antonio Parziale</a>, <a href="http://arxiv.org/find/cs/1/au:+Gregorio_G/0/1/0/all/0/1">Giuseppe De Gregorio</a>, <a href="http://arxiv.org/find/cs/1/au:+Marcelli_A/0/1/0/all/0/1">Angelo Marcelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Fornes_A/0/1/0/all/0/1">Alicia Forn&#xe9;s</a>, <a href="http://arxiv.org/find/cs/1/au:+Llad%5C%27os/0/1/0/all/0/1">Llad&#xf3;s</a></p>
<p>During recent years, there here has been a boom in terms of deep learning use
for handwriting analysis and recognition. One main application for handwriting
analysis is early detection and diagnosis in the health field. Unfortunately,
most real case problems still suffer a scarcity of data, which makes difficult
the use of deep learning-based models. To alleviate this problem, some works
resort to synthetic data generation. Lately, more works are directed towards
guided data synthetic generation, a generation that uses the domain and data
knowledge to generate realistic data that can be useful to train deep learning
models. In this work, we combine the domain knowledge about the Alzheimer's
disease for handwriting and use it for a more guided data generation.
Concretely, we have explored the use of in-air movements for synthetic data
generation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05100">Continual learning for surface defect segmentation by subnetwork creation and selection. (arXiv:2312.05100v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dekhovich_A/0/1/0/all/0/1">Aleksandr Dekhovich</a>, <a href="http://arxiv.org/find/cs/1/au:+Bessa_M/0/1/0/all/0/1">Miguel A. Bessa</a></p>
<p>We introduce a new continual (or lifelong) learning algorithm called LDA-CP&amp;S
that performs segmentation tasks without undergoing catastrophic forgetting.
The method is applied to two different surface defect segmentation problems
that are learned incrementally, i.e. providing data about one type of defect at
a time, while still being capable of predicting every defect that was seen
previously. Our method creates a defect-related subnetwork for each defect type
via iterative pruning and trains a classifier based on linear discriminant
analysis (LDA). At the inference stage, we first predict the defect type with
LDA and then predict the surface defects using the selected subnetwork. We
compare our method with other continual learning methods showing a significant
improvement -- mean Intersection over Union better by a factor of two when
compared to existing methods on both datasets. Importantly, our approach shows
comparable results with joint training when all the training data (all defects)
are seen simultaneously
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05107">DreaMoving: A Human Dance Video Generation Framework based on Diffusion Models. (arXiv:2312.05107v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_M/0/1/0/all/0/1">Mengyang Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jinlin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1">Kai Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1">Yuan Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hui_Z/0/1/0/all/0/1">Zheng Hui</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1">Xiefan Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1">Xianhui Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1">Haolan Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1">Chen Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaowen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1">Aojie Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_M/0/1/0/all/0/1">Miaomiao Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1">Peiran Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xuansong Xie</a></p>
<p>In this paper, we present DreaMoving, a diffusion-based controllable video
generation framework to produce high-quality customized human dance videos.
Specifically, given target identity and posture sequences, DreaMoving can
generate a video of the target identity dancing anywhere driven by the posture
sequences. To this end, we propose a Video ControlNet for motion-controlling
and a Content Guider for identity preserving. The proposed model is easy to use
and can be adapted to most stylized diffusion models to generate diverse
results. The project page is available at
https://dreamoving.github.io/dreamoving.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05119">Quantifying white matter hyperintensity and brain volumes in heterogeneous clinical and low-field portable MRI. (arXiv:2312.05119v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Laso_P/0/1/0/all/0/1">Pablo Laso</a>, <a href="http://arxiv.org/find/eess/1/au:+Cerri_S/0/1/0/all/0/1">Stefano Cerri</a>, <a href="http://arxiv.org/find/eess/1/au:+Sorby_Adams_A/0/1/0/all/0/1">Annabel Sorby-Adams</a>, <a href="http://arxiv.org/find/eess/1/au:+Guo_J/0/1/0/all/0/1">Jennifer Guo</a>, <a href="http://arxiv.org/find/eess/1/au:+Mateen_F/0/1/0/all/0/1">Farrah Mateen</a>, <a href="http://arxiv.org/find/eess/1/au:+Goebl_P/0/1/0/all/0/1">Philipp Goebl</a>, <a href="http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1">Jiaming Wu</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_P/0/1/0/all/0/1">Peirong Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1">Hongwei Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Young_S/0/1/0/all/0/1">Sean I. Young</a>, <a href="http://arxiv.org/find/eess/1/au:+Billot_B/0/1/0/all/0/1">Benjamin Billot</a>, <a href="http://arxiv.org/find/eess/1/au:+Puonti_O/0/1/0/all/0/1">Oula Puonti</a>, <a href="http://arxiv.org/find/eess/1/au:+Sze_G/0/1/0/all/0/1">Gordon Sze</a>, <a href="http://arxiv.org/find/eess/1/au:+Payabavash_S/0/1/0/all/0/1">Sam Payabavash</a>, <a href="http://arxiv.org/find/eess/1/au:+DeHavenon_A/0/1/0/all/0/1">Adam DeHavenon</a>, <a href="http://arxiv.org/find/eess/1/au:+Sheth_K/0/1/0/all/0/1">Kevin N. Sheth</a>, <a href="http://arxiv.org/find/eess/1/au:+Rosen_M/0/1/0/all/0/1">Matthew S. Rosen</a>, <a href="http://arxiv.org/find/eess/1/au:+Kirsch_J/0/1/0/all/0/1">John Kirsch</a>, <a href="http://arxiv.org/find/eess/1/au:+Strisciuglio_N/0/1/0/all/0/1">Nicola Strisciuglio</a>, <a href="http://arxiv.org/find/eess/1/au:+Wolterink_J/0/1/0/all/0/1">Jelmer M. Wolterink</a>, <a href="http://arxiv.org/find/eess/1/au:+Eshaghi_A/0/1/0/all/0/1">Arman Eshaghi</a>, <a href="http://arxiv.org/find/eess/1/au:+Barkhof_F/0/1/0/all/0/1">Frederik Barkhof</a>, <a href="http://arxiv.org/find/eess/1/au:+Kimberly_W/0/1/0/all/0/1">W. Taylor Kimberly</a>, <a href="http://arxiv.org/find/eess/1/au:+Iglesias_J/0/1/0/all/0/1">Juan Eugenio Iglesias</a></p>
<p>Brain atrophy and white matter hyperintensity (WMH) are critical neuroimaging
features for ascertaining brain injury in cerebrovascular disease and multiple
sclerosis. Automated segmentation and quantification is desirable but existing
methods require high-resolution MRI with good signal-to-noise ratio (SNR). This
precludes application to clinical and low-field portable MRI (pMRI) scans, thus
hampering large-scale tracking of atrophy and WMH progression, especially in
underserved areas where pMRI has huge potential. Here we present a method that
segments white matter hyperintensity and 36 brain regions from scans of any
resolution and contrast (including pMRI) without retraining. We show results on
six public datasets and on a private dataset with paired high- and low-field
scans (3T and 64mT), where we attain strong correlation between the WMH
($\rho$=.85) and hippocampal volumes (r=.89) estimated at both fields. Our
method is publicly available as part of FreeSurfer, at:
<a href="http://surfer.nmr.mgh.harvard.edu/fswiki/WMH-SynthSeg.">this http URL</a>
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05133">GIR: 3D Gaussian Inverse Rendering for Relightable Scene Factorization. (arXiv:2312.05133v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yahao Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yanmin Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1">Chenming Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1">Chen Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1">Haocheng Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jingtuo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Liangjun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1">Bin Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1">Errui Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jingdong Wang</a></p>
<p>This paper presents GIR, a 3D Gaussian Inverse Rendering method for
relightable scene factorization. Compared to existing methods leveraging
discrete meshes or neural implicit fields for inverse rendering, our method
utilizes 3D Gaussians to estimate the material properties, illumination, and
geometry of an object from multi-view images. Our study is motivated by the
evidence showing that 3D Gaussian is a more promising backbone than neural
fields in terms of performance, versatility, and efficiency. In this paper, we
aim to answer the question: ``How can 3D Gaussian be applied to improve the
performance of inverse rendering?'' To address the complexity of estimating
normals based on discrete and often in-homogeneous distributed 3D Gaussian
representations, we proposed an efficient self-regularization method that
facilitates the modeling of surface normals without the need for additional
supervision. To reconstruct indirect illumination, we propose an approach that
simulates ray tracing. Extensive experiments demonstrate our proposed GIR's
superior performance over existing methods across multiple tasks on a variety
of widely used datasets in inverse rendering. This substantiates its efficacy
and broad applicability, highlighting its potential as an influential tool in
relighting and reconstruction. Project page: https://3dgir.github.io
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05141">Open Domain Generalization with a Single Network by Regularization Exploiting Pre-trained Features. (arXiv:2312.05141v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chung_I/0/1/0/all/0/1">Inseop Chung</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoo_K/0/1/0/all/0/1">KiYoon Yoo</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1">Nojun Kwak</a></p>
<p>Open Domain Generalization (ODG) is a challenging task as it not only deals
with distribution shifts but also category shifts between the source and target
datasets. To handle this task, the model has to learn a generalizable
representation that can be applied to unseen domains while also identify
unknown classes that were not present during training. Previous work has used
multiple source-specific networks, which involve a high computation cost.
Therefore, this paper proposes a method that can handle ODG using only a single
network. The proposed method utilizes a head that is pre-trained by
linear-probing and employs two regularization terms, each targeting the
regularization of feature extractor and the classification head, respectively.
The two regularization terms fully utilize the pre-trained features and
collaborate to modify the head of the model without excessively altering the
feature extractor. This ensures a smoother softmax output and prevents the
model from being biased towards the source domains. The proposed method shows
improved adaptability to unseen domains and increased capability to detect
unseen classes as well. Extensive experiments show that our method achieves
competitive performance in several benchmarks. We also justify our method with
careful analysis of the effect on the logits, features, and the head.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05148">Shape-aware Segmentation of the Placenta in BOLD Fetal MRI Time Series. (arXiv:2312.05148v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Abulnaga_S/0/1/0/all/0/1">S. Mazdak Abulnaga</a>, <a href="http://arxiv.org/find/eess/1/au:+Dey_N/0/1/0/all/0/1">Neel Dey</a>, <a href="http://arxiv.org/find/eess/1/au:+Young_S/0/1/0/all/0/1">Sean I. Young</a>, <a href="http://arxiv.org/find/eess/1/au:+Pan_E/0/1/0/all/0/1">Eileen Pan</a>, <a href="http://arxiv.org/find/eess/1/au:+Hobgood_K/0/1/0/all/0/1">Katherine I. Hobgood</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1">Clinton J. Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Grant_P/0/1/0/all/0/1">P. Ellen Grant</a>, <a href="http://arxiv.org/find/eess/1/au:+Turk_E/0/1/0/all/0/1">Esra Abaci Turk</a>, <a href="http://arxiv.org/find/eess/1/au:+Golland_P/0/1/0/all/0/1">Polina Golland</a></p>
<p>Blood oxygen level dependent (BOLD) MRI time series with maternal hyperoxia
can assess placental oxygenation and function. Measuring precise BOLD changes
in the placenta requires accurate temporal placental segmentation and is
confounded by fetal and maternal motion, contractions, and hyperoxia-induced
intensity changes. Current BOLD placenta segmentation methods warp a manually
annotated subject-specific template to the entire time series. However, as the
placenta is a thin, elongated, and highly non-rigid organ subject to large
deformations and obfuscated edges, existing work cannot accurately segment the
placental shape, especially near boundaries. In this work, we propose a machine
learning segmentation framework for placental BOLD MRI and apply it to
segmenting each volume in a time series. We use a placental-boundary weighted
loss formulation and perform a comprehensive evaluation across several popular
segmentation objectives. Our model is trained and tested on a cohort of 91
subjects containing healthy fetuses, fetuses with fetal growth restriction, and
mothers with high BMI. Biomedically, our model performs reliably in segmenting
volumes in both normoxic and hyperoxic points in the BOLD time series. We
further find that boundary-weighting increases placental segmentation
performance by 8.3% and 6.0% Dice coefficient for the cross-entropy and signed
distance transform objectives, respectively. Our code and trained model is
available at https://github.com/mabulnaga/automatic-placenta-segmentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05161">TriHuman : A Real-time and Controllable Tri-plane Representation for Detailed Human Geometry and Appearance Synthesis. (arXiv:2312.05161v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1">Heming Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1">Fangneng Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1">Christian Theobalt</a>, <a href="http://arxiv.org/find/cs/1/au:+Habermann_M/0/1/0/all/0/1">Marc Habermann</a></p>
<p>Creating controllable, photorealistic, and geometrically detailed digital
doubles of real humans solely from video data is a key challenge in Computer
Graphics and Vision, especially when real-time performance is required. Recent
methods attach a neural radiance field (NeRF) to an articulated structure,
e.g., a body model or a skeleton, to map points into a pose canonical space
while conditioning the NeRF on the skeletal pose. These approaches typically
parameterize the neural field with a multi-layer perceptron (MLP) leading to a
slow runtime. To address this drawback, we propose TriHuman a novel
human-tailored, deformable, and efficient tri-plane representation, which
achieves real-time performance, state-of-the-art pose-controllable geometry
synthesis as well as photorealistic rendering quality. At the core, we
non-rigidly warp global ray samples into our undeformed tri-plane texture
space, which effectively addresses the problem of global points being mapped to
the same tri-plane locations. We then show how such a tri-plane feature
representation can be conditioned on the skeletal motion to account for dynamic
appearance and geometry changes. Our results demonstrate a clear step towards
higher quality in terms of geometry and appearance modeling of humans as well
as runtime performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05176">MRI Scan Synthesis Methods based on Clustering and Pix2Pix. (arXiv:2312.05176v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Baldini_G/0/1/0/all/0/1">Giulia Baldini</a>, <a href="http://arxiv.org/find/eess/1/au:+Schmidt_M/0/1/0/all/0/1">Melanie Schmidt</a>, <a href="http://arxiv.org/find/eess/1/au:+Zaske_C/0/1/0/all/0/1">Charlotte Z&#xe4;ske</a>, <a href="http://arxiv.org/find/eess/1/au:+Caldeira_L/0/1/0/all/0/1">Liliana L. Caldeira</a></p>
<p>We consider a missing data problem in the context of automatic segmentation
methods for Magnetic Resonance Imaging (MRI) brain scans. Usually, automated
MRI scan segmentation is based on multiple scans (e.g., T1-weighted,
T2-weighted, T1CE, FLAIR). However, quite often a scan is blurry, missing or
otherwise unusable. We investigate the question whether a missing scan can be
synthesized. We exemplify that this is in principle possible by synthesizing a
T2-weighted scan from a given T1-weighted scan. Our first aim is to compute a
picture that resembles the missing scan closely, measured by average mean
squared error (MSE). We develop/use several methods for this, including a
random baseline approach, a clustering-based method and pixel-to-pixel
translation method by (Pix2Pix) which is based on conditional GANs. The lowest
MSE is achieved by our clustering-based method. Our second aim is to compare
the methods with respect to the affect that using the synthesized scan has on
the segmentation process. For this, we use a DeepMedic model trained with the
four input scan modalities named above. We replace the T2-weighted scan by the
synthesized picture and evaluate the segmentations with respect to the tumor
identification, using Dice scores as numerical evaluation. The evaluation shows
that the segmentation works well with synthesized scans (in particular, with
Pix2Pix methods) in many cases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05179">Video-Based Rendering Techniques: A Survey. (arXiv:2312.05179v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Anjos_R/0/1/0/all/0/1">Rafael Kuffner dos Anjos</a>, <a href="http://arxiv.org/find/cs/1/au:+Pereira_J/0/1/0/all/0/1">Jo&#xe3;o Madeiras Pereira</a>, <a href="http://arxiv.org/find/cs/1/au:+Gaspar_J/0/1/0/all/0/1">Jos&#xe9; Antonio Gaspar</a></p>
<p>Three-dimensional reconstruction of events recorded on images has been a
common challenge between computer vision and computer graphics for a long time.
Estimating the real position of objects and surfaces using vision as an input
is no trivial task and has been approached in several different ways. Although
huge progress has been made so far, there are several open issues to which an
answer is needed. The use of videos as an input for a rendering process
(video-based rendering, VBR) is something that recently has been started to be
looked upon and has added many other challenges and also solutions to the
classical image-based rendering issue (IBR). This article presents the state of
art on video-based rendering and image-based techniques that can be applied on
this scenario, evaluating the open issues yet to be solved, indicating where
future work should be focused.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05190">Fine Dense Alignment of Image Bursts through Camera Pose and Depth Estimation. (arXiv:2312.05190v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lecouat_B/0/1/0/all/0/1">Bruno Lecouat</a>, <a href="http://arxiv.org/find/cs/1/au:+Mont_Marin_Y/0/1/0/all/0/1">Yann Dubois de Mont-Marin</a>, <a href="http://arxiv.org/find/cs/1/au:+Bodrito_T/0/1/0/all/0/1">Th&#xe9;o Bodrito</a>, <a href="http://arxiv.org/find/cs/1/au:+Mairal_J/0/1/0/all/0/1">Julien Mairal</a>, <a href="http://arxiv.org/find/cs/1/au:+Ponce_J/0/1/0/all/0/1">Jean Ponce</a></p>
<p>This paper introduces a novel approach to the fine alignment of images in a
burst captured by a handheld camera. In contrast to traditional techniques that
estimate two-dimensional transformations between frame pairs or rely on
discrete correspondences, the proposed algorithm establishes dense
correspondences by optimizing both the camera motion and surface depth and
orientation at every pixel. This approach improves alignment, particularly in
scenarios with parallax challenges. Extensive experiments with synthetic bursts
featuring small and even tiny baselines demonstrate that it outperforms the
best optical flow methods available today in this setting, without requiring
any training. Beyond enhanced alignment, our method opens avenues for tasks
beyond simple image restoration, such as depth estimation and 3D
reconstruction, as supported by promising preliminary results. This positions
our approach as a versatile tool for various burst image processing
applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05208">ControlRoom3D: Room Generation using Semantic Proxy Rooms. (arXiv:2312.05208v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schult_J/0/1/0/all/0/1">Jonas Schult</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsai_S/0/1/0/all/0/1">Sam Tsai</a>, <a href="http://arxiv.org/find/cs/1/au:+Hollein_L/0/1/0/all/0/1">Lukas H&#xf6;llein</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1">Bichen Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jialiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1">Chih-Yao Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Kunpeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaofang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wimbauer_F/0/1/0/all/0/1">Felix Wimbauer</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1">Zijian He</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Peizhao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Leibe_B/0/1/0/all/0/1">Bastian Leibe</a>, <a href="http://arxiv.org/find/cs/1/au:+Vajda_P/0/1/0/all/0/1">Peter Vajda</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1">Ji Hou</a></p>
<p>Manually creating 3D environments for AR/VR applications is a complex process
requiring expert knowledge in 3D modeling software. Pioneering works facilitate
this process by generating room meshes conditioned on textual style
descriptions. Yet, many of these automatically generated 3D meshes do not
adhere to typical room layouts, compromising their plausibility, e.g., by
placing several beds in one bedroom. To address these challenges, we present
ControlRoom3D, a novel method to generate high-quality room meshes. Central to
our approach is a user-defined 3D semantic proxy room that outlines a rough
room layout based on semantic bounding boxes and a textual description of the
overall room style. Our key insight is that when rendered to 2D, this 3D
representation provides valuable geometric and semantic information to control
powerful 2D models to generate 3D consistent textures and geometry that aligns
well with the proxy room. Backed up by an extensive study including
quantitative metrics and qualitative user evaluations, our method generates
diverse and globally plausible 3D room meshes, thus empowering users to design
3D rooms effortlessly without specialized knowledge.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05210">IntrinsicAvatar: Physically Based Inverse Rendering of Dynamic Humans from Monocular Videos via Explicit Ray Tracing. (arXiv:2312.05210v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shaofei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Antic_B/0/1/0/all/0/1">Bo&#x17e;idar Anti&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1">Andreas Geiger</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1">Siyu Tang</a></p>
<p>We present IntrinsicAvatar, a novel approach to recovering the intrinsic
properties of clothed human avatars including geometry, albedo, material, and
environment lighting from only monocular videos. Recent advancements in
human-based neural rendering have enabled high-quality geometry and appearance
reconstruction of clothed humans from just monocular videos. However, these
methods bake intrinsic properties such as albedo, material, and environment
lighting into a single entangled neural representation. On the other hand, only
a handful of works tackle the problem of estimating geometry and disentangled
appearance properties of clothed humans from monocular videos. They usually
achieve limited quality and disentanglement due to approximations of secondary
shading effects via learned MLPs. In this work, we propose to model secondary
shading effects explicitly via Monte-Carlo ray tracing. We model the rendering
process of clothed humans as a volumetric scattering process, and combine ray
tracing with body articulation. Our approach can recover high-quality geometry,
albedo, material, and lighting properties of clothed humans from a single
monocular video, without requiring supervised pre-training using ground truth
materials. Furthermore, since we explicitly model the volumetric scattering
process and ray tracing, our model naturally generalizes to novel poses,
enabling animation of the reconstructed avatar in novel lighting conditions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05219">Enhancing Facial Classification and Recognition using 3D Facial Models and Deep Learning. (arXiv:2312.05219v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Houting Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1">Mengxuan Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Lui_L/0/1/0/all/0/1">Lok Ming Lui</a></p>
<p>Accurate analysis and classification of facial attributes are essential in
various applications, from human-computer interaction to security systems. In
this work, a novel approach to enhance facial classification and recognition
tasks through the integration of 3D facial models with deep learning methods
was proposed. We extract the most useful information for various tasks using
the 3D Facial Model, leading to improved classification accuracy. Combining 3D
facial insights with ResNet architecture, our approach achieves notable
results: 100% individual classification, 95.4% gender classification, and 83.5%
expression classification accuracy. This method holds promise for advancing
facial analysis and recognition research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05220">Shape Matters: Detecting Vertebral Fractures Using Differentiable Point-Based Shape Decoding. (arXiv:2312.05220v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Hempe_H/0/1/0/all/0/1">Hellena Hempe</a>, <a href="http://arxiv.org/find/eess/1/au:+Bigalke_A/0/1/0/all/0/1">Alexander Bigalke</a>, <a href="http://arxiv.org/find/eess/1/au:+Heinrich_M/0/1/0/all/0/1">Mattias P. Heinrich</a></p>
<p>Degenerative spinal pathologies are highly prevalent among the elderly
population. Timely diagnosis of osteoporotic fractures and other degenerative
deformities facilitates proactive measures to mitigate the risk of severe back
pain and disability. In this study, we specifically explore the use of shape
auto-encoders for vertebrae, taking advantage of advancements in automated
multi-label segmentation and the availability of large datasets for
unsupervised learning. Our shape auto-encoders are trained on a large set of
vertebrae surface patches, leveraging the vast amount of available data for
vertebra segmentation. This addresses the label scarcity problem faced when
learning shape information of vertebrae from image intensities. Based on the
learned shape features we train an MLP to detect vertebral body fractures.
Using segmentation masks that were automatically generated using the
TotalSegmentator, our proposed method achieves an AUC of 0.901 on the VerSe19
testset. This outperforms image-based and surface-based end-to-end trained
models. Additionally, our results demonstrate that pre-training the models in
an unsupervised manner enhances geometric methods like PointNet and DGCNN. Our
findings emphasise the advantages of explicitly learning shape features for
diagnosing osteoporotic vertebrae fractures. This approach improves the
reliability of classification results and reduces the need for annotated
labels. This study provides novel insights into the effectiveness of various
encoder-decoder models for shape analysis of vertebrae and proposes a new
decoder architecture: the point-based shape decoder.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05229">Few-Shot Class-Incremental Learning via Training-Free Prototype Calibration. (arXiv:2312.05229v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qi-Wei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1">Da-Wei Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yi-Kai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1">De-Chuan Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1">Han-Jia Ye</a></p>
<p>Real-world scenarios are usually accompanied by continuously appearing
classes with scare labeled samples, which require the machine learning model to
incrementally learn new classes and maintain the knowledge of base classes. In
this Few-Shot Class-Incremental Learning (FSCIL) scenario, existing methods
either introduce extra learnable components or rely on a frozen feature
extractor to mitigate catastrophic forgetting and overfitting problems.
However, we find a tendency for existing methods to misclassify the samples of
new classes into base classes, which leads to the poor performance of new
classes. In other words, the strong discriminability of base classes distracts
the classification of new classes. To figure out this intriguing phenomenon, we
observe that although the feature extractor is only trained on base classes, it
can surprisingly represent the semantic similarity between the base and unseen
new classes. Building upon these analyses, we propose a simple yet effective
Training-frEE calibratioN (TEEN) strategy to enhance the discriminability of
new classes by fusing the new prototypes (i.e., mean features of a class) with
weighted base prototypes. In addition to standard benchmarks in FSCIL, TEEN
demonstrates remarkable performance and consistent improvements over baseline
methods in the few-shot learning scenario. Code is available at:
https://github.com/wangkiw/TEEN
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05230">Language Models, Agent Models, and World Models: The LAW for Machine Reasoning and Planning. (arXiv:2312.05230v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1">Zhiting Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shu_T/0/1/0/all/0/1">Tianmin Shu</a></p>
<p>Despite their tremendous success in many applications, large language models
often fall short of consistent reasoning and planning in various (language,
embodied, and social) scenarios, due to inherent limitations in their
inference, learning, and modeling capabilities. In this position paper, we
present a new perspective of machine reasoning, LAW, that connects the concepts
of Language models, Agent models, and World models, for more robust and
versatile reasoning capabilities. In particular, we propose that world and
agent models are a better abstraction of reasoning, that introduces the crucial
elements of deliberate human-like reasoning, including beliefs about the world
and other agents, anticipation of consequences, goals/rewards, and strategic
planning. Crucially, language models in LAW serve as a backend to implement the
system or its elements and hence provide the computational power and
adaptability. We review the recent studies that have made relevant progress and
discuss future research directions towards operationalizing the LAW framework.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05239">SwiftBrush: One-Step Text-to-Image Diffusion Model with Variational Score Distillation. (arXiv:2312.05239v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Thuan Hoang Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_A/0/1/0/all/0/1">Anh Tran</a></p>
<p>Despite their ability to generate high-resolution and diverse images from
text prompts, text-to-image diffusion models often suffer from slow iterative
sampling processes. Model distillation is one of the most effective directions
to accelerate these models. However, previous distillation methods fail to
retain the generation quality while requiring a significant amount of images
for training, either from real data or synthetically generated by the teacher
model. In response to this limitation, we present a novel image-free
distillation scheme named $\textbf{SwiftBrush}$. Drawing inspiration from
text-to-3D synthesis, in which a 3D neural radiance field that aligns with the
input prompt can be obtained from a 2D text-to-image diffusion prior via a
specialized loss without the use of any 3D data ground-truth, our approach
re-purposes that same loss for distilling a pretrained multi-step text-to-image
model to a student network that can generate high-fidelity images with just a
single inference step. In spite of its simplicity, our model stands as one of
the first one-step text-to-image generators that can produce images of
comparable quality to Stable Diffusion without reliance on any training image
data. Remarkably, SwiftBrush achieves an FID score of $\textbf{16.67}$ and a
CLIP score of $\textbf{0.29}$ on the COCO-30K benchmark, achieving competitive
results or even substantially surpassing existing state-of-the-art distillation
techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05247">Dynamic LiDAR Re-simulation using Compositional Neural Fields. (arXiv:2312.05247v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Hanfeng Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1">Xingxing Zuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Leutenegger_S/0/1/0/all/0/1">Stefan Leutenegger</a>, <a href="http://arxiv.org/find/cs/1/au:+Litany_O/0/1/0/all/0/1">Or Litany</a>, <a href="http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1">Konrad Schindler</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Shengyu Huang</a></p>
<p>We introduce DyNFL, a novel neural field-based approach for high-fidelity
re-simulation of LiDAR scans in dynamic driving scenes. DyNFL processes LiDAR
measurements from dynamic environments, accompanied by bounding boxes of moving
objects, to construct an editable neural field. This field, comprising
separately reconstructed static backgrounds and dynamic objects, allows users
to modify viewpoints, adjust object positions, and seamlessly add or remove
objects in the re-simulated scene. A key innovation of our method is the neural
field composition technique, which effectively integrates reconstructed neural
assets from various scenes through a ray drop test, accounting for occlusions
and transparent surfaces. Our evaluation with both synthetic and real-world
environments demonstrates that \ShortName substantial improves dynamic scene
simulation based on LiDAR scans, offering a combination of physical fidelity
and flexible editing capabilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05251">Reconstructing Hands in 3D with Transformers. (arXiv:2312.05251v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pavlakos_G/0/1/0/all/0/1">Georgios Pavlakos</a>, <a href="http://arxiv.org/find/cs/1/au:+Shan_D/0/1/0/all/0/1">Dandan Shan</a>, <a href="http://arxiv.org/find/cs/1/au:+Radosavovic_I/0/1/0/all/0/1">Ilija Radosavovic</a>, <a href="http://arxiv.org/find/cs/1/au:+Kanazawa_A/0/1/0/all/0/1">Angjoo Kanazawa</a>, <a href="http://arxiv.org/find/cs/1/au:+Fouhey_D/0/1/0/all/0/1">David Fouhey</a>, <a href="http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1">Jitendra Malik</a></p>
<p>We present an approach that can reconstruct hands in 3D from monocular input.
Our approach for Hand Mesh Recovery, HaMeR, follows a fully transformer-based
architecture and can analyze hands with significantly increased accuracy and
robustness compared to previous work. The key to HaMeR's success lies in
scaling up both the data used for training and the capacity of the deep network
for hand reconstruction. For training data, we combine multiple datasets that
contain 2D or 3D hand annotations. For the deep model, we use a large scale
Vision Transformer architecture. Our final model consistently outperforms the
previous baselines on popular 3D hand pose benchmarks. To further evaluate the
effect of our design in non-controlled settings, we annotate existing
in-the-wild datasets with 2D hand keypoint annotations. On this newly collected
dataset of annotations, HInt, we demonstrate significant improvements over
existing baselines. We make our code, data and models available on the project
website: https://geopavlakos.github.io/hamer/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2107.01671">Cognitive Visual Commonsense Reasoning Using Dynamic Working Memory. (arXiv:2107.01671v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1">Xuejiao Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenbin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Child_T/0/1/0/all/0/1">Travers B. Child</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1">Qiong Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Ji Zhang</a></p>
<p>Visual Commonsense Reasoning (VCR) predicts an answer with corresponding
rationale, given a question-image input. VCR is a recently introduced visual
scene understanding task with a wide range of applications, including visual
question answering, automated vehicle systems, and clinical decision support.
Previous approaches to solving the VCR task generally rely on pre-training or
exploiting memory with long dependency relationship encoded models. However,
these approaches suffer from a lack of generalizability and prior knowledge. In
this paper we propose a dynamic working memory based cognitive VCR network,
which stores accumulated commonsense between sentences to provide prior
knowledge for inference. Extensive experiments show that the proposed model
yields significant improvements over existing methods on the benchmark VCR
dataset. Moreover, the proposed model provides intuitive interpretation into
visual commonsense reasoning. A Python implementation of our mechanism is
publicly available at https://github.com/tanjatang/DMVCR
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2108.02924">Interpretable Visual Understanding with Cognitive Attention Network. (arXiv:2108.02924v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1">Xuejiao Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenbin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yi Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Turner_K/0/1/0/all/0/1">Kea Turner</a>, <a href="http://arxiv.org/find/cs/1/au:+Derr_T/0/1/0/all/0/1">Tyler Derr</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Mengyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ntoutsi_E/0/1/0/all/0/1">Eirini Ntoutsi</a></p>
<p>While image understanding on recognition-level has achieved remarkable
advancements, reliable visual scene understanding requires comprehensive image
understanding on recognition-level but also cognition-level, which calls for
exploiting the multi-source information as well as learning different levels of
understanding and extensive commonsense knowledge. In this paper, we propose a
novel Cognitive Attention Network (CAN) for visual commonsense reasoning to
achieve interpretable visual understanding. Specifically, we first introduce an
image-text fusion module to fuse information from images and text collectively.
Second, a novel inference module is designed to encode commonsense among image,
query and response. Extensive experiments on large-scale Visual Commonsense
Reasoning (VCR) benchmark dataset demonstrate the effectiveness of our
approach. The implementation is publicly available at
https://github.com/tanjatang/CAN
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.12989">Domain-Aware Continual Zero-Shot Learning. (arXiv:2112.12989v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1">Kai Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Janson_P/0/1/0/all/0/1">Paul Janson</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenxuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1">Mohamed Elhoseiny</a></p>
<p>Continual zero-shot learning involves learning seen classes incrementally
while improving the ability to recognize unseen or yet-to-be-seen classes. It
has a broad range of potential applications in real-world vision tasks, such as
accelerating species discovery. However, in these scenarios, the changes in
environmental conditions cause shifts in the presentation of captured images,
which we refer to as domain shift, and adds complexity to the tasks. In this
paper, we introduce Domain Aware Continual Zero-Shot Learning (DACZSL), a task
that involves visually recognizing images of unseen categories in unseen
domains continually. To address the challenges of DACZSL, we propose a
Domain-Invariant Network (DIN). We empoly a dual network structure to learn
factorized features to alleviate forgetting, where consists of a global shared
net for domian-invirant and task-invariant features, and per-task private nets
for task-specific features. Furthermore, we introduce a class-wise learnable
prompt to obtain better class-level text representation, which enables
zero-shot prediction of future unseen classes. To evaluate DACZSL, we introduce
two benchmarks: DomainNet-CZSL and iWildCam-CZSL. Our results show that DIN
significantly outperforms existing baselines and achieves a new
state-of-the-art.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.04754">Wireless Transmission of Images With The Assistance of Multi-level Semantic Information. (arXiv:2202.04754v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1">Zhenguo Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_Q/0/1/0/all/0/1">Qianqian Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+He_S/0/1/0/all/0/1">Shibo He</a>, <a href="http://arxiv.org/find/eess/1/au:+Sun_M/0/1/0/all/0/1">Mingyang Sun</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1">Jiming Chen</a></p>
<p>Semantic-oriented communication has been considered as a promising to boost
the bandwidth efficiency by only transmitting the semantics of the data. In
this paper, we propose a multi-level semantic aware communication system for
wireless image transmission, named MLSC-image, which is based on the deep
learning techniques and trained in an end to end manner. In particular, the
proposed model includes a multilevel semantic feature extractor, that extracts
both the highlevel semantic information, such as the text semantics and the
segmentation semantics, and the low-level semantic information, such as local
spatial details of the images. We employ a pretrained image caption to capture
the text semantics and a pretrained image segmentation model to obtain the
segmentation semantics. These high-level and low-level semantic features are
then combined and encoded by a joint semantic and channel encoder into symbols
to transmit over the physical channel. The numerical results validate the
effectiveness and efficiency of the proposed semantic communication system,
especially under the limited bandwidth condition, which indicates the
advantages of the high-level semantics in the compression of images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.10705">PointMatch: A Consistency Training Framework for Weakly Supervised Semantic Segmentation of 3D Point Clouds. (arXiv:2202.10705v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yushuang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1">Zizheng Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1">Shengcai Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1">Guanbin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yizhou Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1">Xiaoguang Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1">Shuguang Cui</a></p>
<p>Semantic segmentation of point cloud usually relies on dense annotation that
is exhausting and costly, so it attracts wide attention to investigate
solutions for the weakly supervised scheme with only sparse points annotated.
Existing works start from the given labels and propagate them to highly-related
but unlabeled points, with the guidance of data, e.g. intra-point relation.
However, it suffers from (i) the inefficient exploitation of data information,
and (ii) the strong reliance on labels thus is easily suppressed when given
much fewer annotations. Therefore, we propose a novel framework, PointMatch,
that stands on both data and label, by applying consistency regularization to
sufficiently probe information from data itself and leveraging weak labels as
assistance at the same time. By doing so, meaningful information can be learned
from both data and label for better representation learning, which also enables
the model more robust to the extent of label sparsity. Simple yet effective,
the proposed PointMatch achieves the state-of-the-art performance under various
weakly-supervised schemes on both ScanNet-v2 and S3DIS datasets, especially on
the settings with extremely sparse labels, e.g. surpassing SQN by 21.2% and
17.2% on the 0.01% and 0.1% setting of ScanNet-v2, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.17255">A Cognitive Architecture for Machine Consciousness and Artificial Superintelligence: Thought Is Structured by the Iterative Updating of Working Memory. (arXiv:2203.17255v5 [q-bio.NC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Reser_J/0/1/0/all/0/1">Jared Edward Reser</a></p>
<p>This article provides an analytical framework for how to simulate human-like
thought processes within a computer. It describes how attention and memory
should be structured, updated, and utilized to search for associative additions
to the stream of thought. The focus is on replicating the mammalian working
memory system, which features two forms of persistent activity: sustained
firing (preserving information on the order of seconds) and synaptic
potentiation (preserving information from minutes to hours). The article uses a
series of over 40 original figures to systematically demonstrate how the
iterative updating of these working memory stores provides functional structure
to thought and consciousness. In an AI implementation, these two stores should
be updated continuously and in an iterative fashion, meaning each state should
preserve a proportion of the coactive representations from the state before it.
Thus, the set of concepts in working memory will evolve gradually and
incrementally over time. This makes each state a revised iteration of the
preceding state and causes successive states to overlap and blend with respect
to the information they contain. Transitions between states happen as
persistent activity spreads activation energy throughout the hierarchical
network searching long-term memory for the most appropriate representation to
be added to the global workspace. The result is a chain of associatively linked
intermediate states capable of advancing toward a solution or goal. Iterative
updating is conceptualized here as an information processing strategy, a model
of working memory, a theory of consciousness, and an algorithm for designing
and programming artificial general intelligence.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.13962">Multi-Prior Learning via Neural Architecture Search for Blind Face Restoration. (arXiv:2206.13962v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yanjiang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Puyang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kaihao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1">Wenhan Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Changsheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1">Ye Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guoren Wang</a></p>
<p>Blind Face Restoration (BFR) aims to recover high-quality face images from
low-quality ones and usually resorts to facial priors for improving restoration
performance. However, current methods still suffer from two major difficulties:
1) how to derive a powerful network architecture without extensive hand tuning;
2) how to capture complementary information from multiple facial priors in one
network to improve restoration performance. To this end, we propose a Face
Restoration Searching Network (FRSNet) to adaptively search the suitable
feature extraction architecture within our specified search space, which can
directly contribute to the restoration quality. On the basis of FRSNet, we
further design our Multiple Facial Prior Searching Network (MFPSNet) with a
multi-prior learning scheme. MFPSNet optimally extracts information from
diverse facial priors and fuses the information into image features, ensuring
that both external guidance and internal features are reserved. In this way,
MFPSNet takes full advantage of semantic-level (parsing maps), geometric-level
(facial heatmaps), reference-level (facial dictionaries) and pixel-level
(degraded images) information and thus generates faithful and realistic images.
Quantitative and qualitative experiments show that MFPSNet performs favorably
on both synthetic and real-world datasets against the state-of-the-art BFR
methods. The codes are publicly available at:
https://github.com/YYJ1anG/MFPSNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.14769">Image Quality Assessment: Integrating Model-Centric and Data-Centric Approaches. (arXiv:2207.14769v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cao_P/0/1/0/all/0/1">Peibei Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dingquan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1">Kede Ma</a></p>
<p>Learning-based image quality assessment (IQA) has made remarkable progress in
the past decade, but nearly all consider the two key components -- model and
data -- in isolation. Specifically, model-centric IQA focuses on developing
``better'' objective quality methods on fixed and extensively reused datasets,
with a great danger of overfitting. Data-centric IQA involves conducting
psychophysical experiments to construct ``better'' human-annotated datasets,
which unfortunately ignores current IQA models during dataset creation. In this
paper, we first design a series of experiments to probe computationally that
such isolation of model and data impedes further progress of IQA. We then
describe a computational framework that integrates model-centric and
data-centric IQA. As a specific example, we design computational modules to
quantify the sampling-worthiness of candidate images. Experimental results show
that the proposed sampling-worthiness module successfully spots diverse
failures of the examined blind IQA models, which are indeed worthy samples to
be included in next-generation datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.12464">Dense Depth Distillation with Out-of-Distribution Simulated Images. (arXiv:2208.12464v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1">Junjie Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1">Chenyou Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ozay_M/0/1/0/all/0/1">Mete Ozay</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1">Hualie Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lam_T/0/1/0/all/0/1">Tin Lun Lam</a></p>
<p>We study data-free knowledge distillation (KD) for monocular depth estimation
(MDE), which learns a lightweight model for real-world depth perception tasks
by compressing it from a trained teacher model while lacking training data in
the target domain. Owing to the essential difference between image
classification and dense regression, previous methods of data-free KD are not
applicable to MDE. To strengthen its applicability in real-world tasks, in this
paper, we propose to apply KD with out-of-distribution simulated images. The
major challenges to be resolved are i) lacking prior information about scene
configurations of real-world training data and ii) domain shift between
simulated and real-world images. To cope with these difficulties, we propose a
tailored framework for depth distillation. The framework generates new training
samples for embracing a multitude of possible object arrangements in the target
domain and utilizes a transformation network to efficiently adapt them to the
feature statistics preserved in the teacher model. Through extensive
experiments on various depth estimation models and two different datasets, we
show that our method outperforms the baseline KD by a good margin and even
achieves slightly better performance with as few as 1/6 of training images,
demonstrating a clear superiority.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.15194">Few-shot Image Generation via Masked Discrimination. (arXiv:2210.15194v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jingyuan Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1">Huimin Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiansheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1">Jian Yuan</a></p>
<p>Few-shot image generation aims to generate images of high quality and great
diversity with limited data. However, it is difficult for modern GANs to avoid
overfitting when trained on only a few images. The discriminator can easily
remember all the training samples and guide the generator to replicate them,
leading to severe diversity degradation. Several methods have been proposed to
relieve overfitting by adapting GANs pre-trained on large source domains to
target domains using limited real samples. This work presents a novel approach
to realize few-shot GAN adaptation via masked discrimination. Random masks are
applied to features extracted by the discriminator from input images. We aim to
encourage the discriminator to judge various images which share partially
common features with training samples as realistic. Correspondingly, the
generator is guided to generate diverse images instead of replicating training
samples. In addition, we employ a cross-domain consistency loss for the
discriminator to keep relative distances between generated samples in its
feature space. It strengthens global image discrimination and guides adapted
GANs to preserve more information learned from source domains for higher image
quality. The effectiveness of our approach is demonstrated both qualitatively
and quantitatively with higher quality and greater diversity on a series of
few-shot image generation tasks than prior methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.00385">Behavioral Intention Prediction in Driving Scenes: A Survey. (arXiv:2211.00385v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1">Jianwu Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Fan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1">Jianru Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1">Tat-seng Chua</a></p>
<p>In the driving scene, the road agents usually conduct frequent interactions
and intention understanding of the surroundings. Ego-agent (each road agent
itself) predicts what behavior will be engaged by other road users all the time
and expects a shared and consistent understanding for safe movement. Behavioral
Intention Prediction (BIP) simulates such a human consideration process and
fulfills the early prediction of specific behaviors. Similar to other
prediction tasks, such as trajectory prediction, data-driven deep learning
methods have taken the primary pipeline in research. The rapid development of
BIP inevitably leads to new issues and challenges. To catalyze future research,
this work provides a comprehensive review of BIP from the available datasets,
key factors and challenges, pedestrian-centric and vehicle-centric BIP
approaches, and BIP-aware applications. Based on the investigation, data-driven
deep learning approaches have become the primary pipelines. The behavioral
intention types are still monotonous in most current datasets and methods
(e.g., Crossing (C) and Not Crossing (NC) for pedestrians and Lane Changing
(LC) for vehicles) in this field. In addition, for the safe-critical scenarios
(e.g., near-crashing situations), current research is limited. Through this
investigation, we identify open issues in behavioral intention prediction and
suggest possible insights for future research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.01842">Construction of Hierarchical Neural Architecture Search Spaces based on Context-free Grammars. (arXiv:2211.01842v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schrodi_S/0/1/0/all/0/1">Simon Schrodi</a>, <a href="http://arxiv.org/find/cs/1/au:+Stoll_D/0/1/0/all/0/1">Danny Stoll</a>, <a href="http://arxiv.org/find/cs/1/au:+Ru_B/0/1/0/all/0/1">Binxin Ru</a>, <a href="http://arxiv.org/find/cs/1/au:+Sukthanker_R/0/1/0/all/0/1">Rhea Sukthanker</a>, <a href="http://arxiv.org/find/cs/1/au:+Brox_T/0/1/0/all/0/1">Thomas Brox</a>, <a href="http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1">Frank Hutter</a></p>
<p>The discovery of neural architectures from simple building blocks is a
long-standing goal of Neural Architecture Search (NAS). Hierarchical search
spaces are a promising step towards this goal but lack a unifying search space
design framework and typically only search over some limited aspect of
architectures. In this work, we introduce a unifying search space design
framework based on context-free grammars that can naturally and compactly
generate expressive hierarchical search spaces that are 100s of orders of
magnitude larger than common spaces from the literature. By enhancing and using
their properties, we effectively enable search over the complete architecture
and can foster regularity. Further, we propose an efficient hierarchical kernel
design for a Bayesian Optimization search strategy to efficiently search over
such huge spaces. We demonstrate the versatility of our search space design
framework and show that our search strategy can be superior to existing NAS
approaches. Code is available at
https://github.com/automl/hierarchical_nas_construction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.13220">TetraDiffusion: Tetrahedral Diffusion Models for 3D Shape Generation. (arXiv:2211.13220v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kalischek_N/0/1/0/all/0/1">Nikolai Kalischek</a>, <a href="http://arxiv.org/find/cs/1/au:+Peters_T/0/1/0/all/0/1">Torben Peters</a>, <a href="http://arxiv.org/find/cs/1/au:+Wegner_J/0/1/0/all/0/1">Jan D. Wegner</a>, <a href="http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1">Konrad Schindler</a></p>
<p>Probabilistic denoising diffusion models (DDMs) have set a new standard for
2D image generation. Extending DDMs for 3D content creation is an active field
of research. Here, we propose TetraDiffusion, a diffusion model that operates
on a tetrahedral partitioning of 3D space to enable efficient, high-resolution
3D shape generation. Our model introduces operators for convolution and
transpose convolution that act directly on the tetrahedral partition, and
seamlessly includes additional attributes such as color. Remarkably,
TetraDiffusion enables rapid sampling of detailed 3D objects in nearly
real-time with unprecedented resolution. It's also adaptable for generating 3D
shapes conditioned on 2D images. Compared to existing 3D mesh diffusion
techniques, our method is up to 200 times faster in inference speed, works on
standard consumer hardware, and delivers superior results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.02524">Novel Fundus Image Preprocessing for Retcam Images to Improve Deep Learning Classification of Retinopathy of Prematurity. (arXiv:2302.02524v4 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Rahim_S/0/1/0/all/0/1">Sajid Rahim</a>, <a href="http://arxiv.org/find/eess/1/au:+Sabri_K/0/1/0/all/0/1">Kourosh Sabri</a>, <a href="http://arxiv.org/find/eess/1/au:+Ells_A/0/1/0/all/0/1">Anna Ells</a>, <a href="http://arxiv.org/find/eess/1/au:+Wassyng_A/0/1/0/all/0/1">Alan Wassyng</a>, <a href="http://arxiv.org/find/eess/1/au:+Lawford_M/0/1/0/all/0/1">Mark Lawford</a>, <a href="http://arxiv.org/find/eess/1/au:+Chu_L/0/1/0/all/0/1">Linyang Chu</a>, <a href="http://arxiv.org/find/eess/1/au:+He_W/0/1/0/all/0/1">Wenbo He</a></p>
<p>Retinopathy of Prematurity (ROP) is a potentially blinding eye disorder
because of damage to the eye's retina which can affect babies born prematurely.
Screening of ROP is essential for early detection and treatment. This is a
laborious and manual process which requires trained physician performing
dilated ophthalmological examination which can be subjective resulting in lower
diagnosis success for clinically significant disease. Automated diagnostic
methods can assist ophthalmologists increase diagnosis accuracy using deep
learning. Several research groups have highlighted various approaches. Captured
ROP Retcam images suffer from poor quality. This paper proposes the use of
improved novel fundus preprocessing methods using pretrained transfer learning
frameworks to create hybrid models to give higher diagnosis accuracy. Once
trained and validated, the evaluations showed that these novel methods in
comparison to traditional imaging processing contribute to better and in many
aspects higher accuracy in classifying Plus disease, Stages of ROP and Zones in
comparison to peer papers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.12253">DisCO: Portrait Distortion Correction with Perspective-Aware 3D GANs. (arXiv:2302.12253v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhixiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yu-Lun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jia-Bin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Satoh_S/0/1/0/all/0/1">Shin&#x27;ichi Satoh</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1">Sizhuo Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishnan_G/0/1/0/all/0/1">Gurunandan Krishnan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jian Wang</a></p>
<p>Close-up facial images captured at short distances often suffer from
perspective distortion, resulting in exaggerated facial features and
unnatural/unattractive appearances. We propose a simple yet effective method
for correcting perspective distortions in a single close-up face. We first
perform GAN inversion using a perspective-distorted input facial image by
jointly optimizing the camera intrinsic/extrinsic parameters and face latent
code. To address the ambiguity of joint optimization, we develop starting from
a short distance, optimization scheduling, reparametrizations, and geometric
regularization. Re-rendering the portrait at a proper focal length and camera
distance effectively corrects perspective distortions and produces more
natural-looking results. Our experiments show that our method compares
favorably against previous approaches qualitatively and quantitatively. We
showcase numerous examples validating the applicability of our method on
in-the-wild portrait photos. We will release our code and the evaluation
protocol to facilitate future work.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.05066">Distortion-Disentangled Contrastive Learning. (arXiv:2303.05066v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jinfeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1">Sifan Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1">Jionglong Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1">S. Kevin Zhou</a></p>
<p>Self-supervised learning is well known for its remarkable performance in
representation learning and various downstream computer vision tasks. Recently,
Positive-pair-Only Contrastive Learning (POCL) has achieved reliable
performance without the need to construct positive-negative training sets. It
reduces memory requirements by lessening the dependency on the batch size. The
POCL method typically uses a single loss function to extract the distortion
invariant representation (DIR) which describes the proximity of positive-pair
representations affected by different distortions. This loss function
implicitly enables the model to filter out or ignore the distortion variant
representation (DVR) affected by different distortions. However, existing POCL
methods do not explicitly enforce the disentanglement and exploitation of the
actually valuable DVR. In addition, these POCL methods have been observed to be
sensitive to augmentation strategies. To address these limitations, we propose
a novel POCL framework named Distortion-Disentangled Contrastive Learning
(DDCL) and a Distortion-Disentangled Loss (DDL). Our approach is the first to
explicitly disentangle and exploit the DVR inside the model and feature stream
to improve the overall representation utilization efficiency, robustness and
representation ability. Experiments carried out demonstrate the superiority of
our framework to Barlow Twins and Simsiam in terms of convergence,
representation quality, and robustness on several benchmark datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.14310">Incremental Generalized Category Discovery. (arXiv:2304.14310v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1">Bingchen Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Aodha_O/0/1/0/all/0/1">Oisin Mac Aodha</a></p>
<p>We explore the problem of Incremental Generalized Category Discovery (IGCD).
This is a challenging category incremental learning setting where the goal is
to develop models that can correctly categorize images from previously seen
categories, in addition to discovering novel ones. Learning is performed over a
series of time steps where the model obtains new labeled and unlabeled data,
and discards old data, at each iteration. The difficulty of the problem is
compounded in our generalized setting as the unlabeled data can contain images
from categories that may or may not have been observed before. We present a new
method for IGCD which combines non-parametric categorization with efficient
image sampling to mitigate catastrophic forgetting. To quantify performance, we
propose a new benchmark dataset named iNatIGCD that is motivated by a
real-world fine-grained visual categorization task. In our experiments we
outperform existing related methods
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.10406">Variational Classification. (arXiv:2305.10406v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dhuliawala_S/0/1/0/all/0/1">Shehzaad Dhuliawala</a>, <a href="http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1">Mrinmaya Sachan</a>, <a href="http://arxiv.org/find/cs/1/au:+Allen_C/0/1/0/all/0/1">Carl Allen</a></p>
<p>We present a latent variable model for classification that provides a novel
probabilistic interpretation of neural network softmax classifiers. We derive a
variational objective to train the model, analogous to the evidence lower bound
(ELBO) used to train variational auto-encoders, that generalises the
cross-entropy loss used to train classification models. Treating inputs to the
softmax layer as samples of a latent variable, our abstracted perspective
reveals a potential inconsistency between their anticipated distribution,
required for accurate label predictions, and the empirical distribution they
follow in practice. We then devise a variational objective to mitigate such
inconsistency and encourage a specified latent distribution, instead of the
implicit assumption in off-the-shelf softmax classifiers. Overall, we provide
new theoretical insight into the inner workings of widely-used softmax
classification; and empirical evaluation on image and text classification
datasets demonstrates that our proposed remedy, variational classification,
maintains classification accuracy while the reshaped latent space improves
other desirable classifier properties, such as calibration, adversarial
robustness, robustness to distribution shift and sample efficiency useful in
low data settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.12397">Target-Aware Spatio-Temporal Reasoning via Answering Questions in Dynamics Audio-Visual Scenarios. (arXiv:2305.12397v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yuanyuan Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1">Jianqin Yin</a></p>
<p>Audio-visual question answering (AVQA) is a challenging task that requires
multistep spatio-temporal reasoning over multimodal contexts. Recent works rely
on elaborate target-agnostic parsing of audio-visual scenes for spatial
grounding while mistreating audio and video as separate entities for temporal
grounding. This paper proposes a new target-aware joint spatio-temporal
grounding network for AVQA. It consists of two key components: the target-aware
spatial grounding module (TSG) and the single-stream joint audio-visual
temporal grounding module (JTG). The TSG can focus on audio-visual cues
relevant to the query subject by utilizing explicit semantics from the
question. Unlike previous two-stream temporal grounding modules that required
an additional audio-visual fusion module, JTG incorporates audio-visual fusion
and question-aware temporal grounding into one module with a simpler
single-stream architecture. The temporal synchronization between audio and
video in the JTG is facilitated by our proposed cross-modal synchrony loss
(CSL). Extensive experiments verified the effectiveness of our proposed method
over existing state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.02204">Cycle Consistency Driven Object Discovery. (arXiv:2306.02204v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Didolkar_A/0/1/0/all/0/1">Aniket Didolkar</a>, <a href="http://arxiv.org/find/cs/1/au:+Goyal_A/0/1/0/all/0/1">Anirudh Goyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1">Yoshua Bengio</a></p>
<p>Developing deep learning models that effectively learn object-centric
representations, akin to human cognition, remains a challenging task. Existing
approaches facilitate object discovery by representing objects as fixed-size
vectors, called ``slots'' or ``object files''. While these approaches have
shown promise in certain scenarios, they still exhibit certain limitations.
First, they rely on architectural priors which can be unreliable and usually
require meticulous engineering to identify the correct objects. Second, there
has been a notable gap in investigating the practical utility of these
representations in downstream tasks. To address the first limitation, we
introduce a method that explicitly optimizes the constraint that each object in
a scene should be associated with a distinct slot. We formalize this constraint
by introducing consistency objectives which are cyclic in nature. By
integrating these consistency objectives into various existing slot-based
object-centric methods, we showcase substantial improvements in
object-discovery performance. These enhancements consistently hold true across
both synthetic and real-world scenes, underscoring the effectiveness and
adaptability of the proposed approach. To tackle the second limitation, we
apply the learned object-centric representations from the proposed method to
two downstream reinforcement learning tasks, demonstrating considerable
performance enhancements compared to conventional slot-based and monolithic
representation learning methods. Our results suggest that the proposed approach
not only improves object discovery, but also provides richer features for
downstream tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.04829">Object-Centric Learning for Real-World Videos by Predicting Temporal Feature Similarities. (arXiv:2306.04829v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zadaianchuk_A/0/1/0/all/0/1">Andrii Zadaianchuk</a>, <a href="http://arxiv.org/find/cs/1/au:+Seitzer_M/0/1/0/all/0/1">Maximilian Seitzer</a>, <a href="http://arxiv.org/find/cs/1/au:+Martius_G/0/1/0/all/0/1">Georg Martius</a></p>
<p>Unsupervised video-based object-centric learning is a promising avenue to
learn structured representations from large, unlabeled video collections, but
previous approaches have only managed to scale to real-world datasets in
restricted domains. Recently, it was shown that the reconstruction of
pre-trained self-supervised features leads to object-centric representations on
unconstrained real-world image datasets. Building on this approach, we propose
a novel way to use such pre-trained features in the form of a temporal feature
similarity loss. This loss encodes semantic and temporal correlations between
image patches and is a natural way to introduce a motion bias for object
discovery. We demonstrate that this loss leads to state-of-the-art performance
on the challenging synthetic MOVi datasets. When used in combination with the
feature reconstruction loss, our model is the first object-centric video model
that scales to unconstrained video datasets such as YouTube-VIS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.05668">RePaint-NeRF: NeRF Editting via Semantic Masks and Diffusion Models. (arXiv:2306.05668v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xingchen Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Ying He</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1">F. Richard Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jianqiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">You Li</a></p>
<p>The emergence of Neural Radiance Fields (NeRF) has promoted the development
of synthesized high-fidelity views of the intricate real world. However, it is
still a very demanding task to repaint the content in NeRF. In this paper, we
propose a novel framework that can take RGB images as input and alter the 3D
content in neural scenes. Our work leverages existing diffusion models to guide
changes in the designated 3D content. Specifically, we semantically select the
target object and a pre-trained diffusion model will guide the NeRF model to
generate new 3D objects, which can improve the editability, diversity, and
application range of NeRF. Experiment results show that our algorithm is
effective for editing 3D objects in NeRF under different text prompts,
including editing appearance, shape, and more. We validate our method on both
real-world datasets and synthetic-world datasets for these editing tasks.
Please visit https://starstesla.github.io/repaintnerf for a better view of our
results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.08707">VidEdit: Zero-Shot and Spatially Aware Text-Driven Video Editing. (arXiv:2306.08707v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Couairon_P/0/1/0/all/0/1">Paul Couairon</a>, <a href="http://arxiv.org/find/cs/1/au:+Rambour_C/0/1/0/all/0/1">Cl&#xe9;ment Rambour</a>, <a href="http://arxiv.org/find/cs/1/au:+Haugeard_J/0/1/0/all/0/1">Jean-Emmanuel Haugeard</a>, <a href="http://arxiv.org/find/cs/1/au:+Thome_N/0/1/0/all/0/1">Nicolas Thome</a></p>
<p>Recently, diffusion-based generative models have achieved remarkable success
for image generation and edition. However, their use for video editing still
faces important limitations. This paper introduces VidEdit, a novel method for
zero-shot text-based video editing ensuring strong temporal and spatial
consistency. Firstly, we propose to combine atlas-based and pre-trained
text-to-image diffusion models to provide a training-free and efficient editing
method, which by design fulfills temporal smoothness. Secondly, we leverage
off-the-shelf panoptic segmenters along with edge detectors and adapt their use
for conditioned diffusion-based atlas editing. This ensures a fine spatial
control on targeted regions while strictly preserving the structure of the
original video. Quantitative and qualitative experiments show that VidEdit
outperforms state-of-the-art methods on DAVIS dataset, regarding semantic
faithfulness, image preservation, and temporal consistency metrics. With this
framework, processing a single video only takes approximately one minute, and
it can generate multiple compatible edits based on a unique text prompt.
Project web-page at https://videdit.github.io
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.11290">Habitat Synthetic Scenes Dataset (HSSD-200): An Analysis of 3D Scene Scale and Realism Tradeoffs for ObjectGoal Navigation. (arXiv:2306.11290v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khanna_M/0/1/0/all/0/1">Mukul Khanna</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1">Yongsen Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1">Hanxiao Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Haresh_S/0/1/0/all/0/1">Sanjay Haresh</a>, <a href="http://arxiv.org/find/cs/1/au:+Shacklett_B/0/1/0/all/0/1">Brennan Shacklett</a>, <a href="http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1">Dhruv Batra</a>, <a href="http://arxiv.org/find/cs/1/au:+Clegg_A/0/1/0/all/0/1">Alexander Clegg</a>, <a href="http://arxiv.org/find/cs/1/au:+Undersander_E/0/1/0/all/0/1">Eric Undersander</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_A/0/1/0/all/0/1">Angel X. Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Savva_M/0/1/0/all/0/1">Manolis Savva</a></p>
<p>We contribute the Habitat Synthetic Scene Dataset, a dataset of 211
high-quality 3D scenes, and use it to test navigation agent generalization to
realistic 3D environments. Our dataset represents real interiors and contains a
diverse set of 18,656 models of real-world objects. We investigate the impact
of synthetic 3D scene dataset scale and realism on the task of training
embodied agents to find and navigate to objects (ObjectGoal navigation). By
comparing to synthetic 3D scene datasets from prior work, we find that scale
helps in generalization, but the benefits quickly saturate, making visual
fidelity and correlation to real-world scenes more important. Our experiments
show that agents trained on our smaller-scale dataset can match or outperform
agents trained on much larger datasets. Surprisingly, we observe that agents
trained on just 122 scenes from our dataset outperform agents trained on 10,000
scenes from the ProcTHOR-10K dataset in terms of zero-shot generalization in
real-world scanned environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.02251">RanPAC: Random Projections and Pre-trained Models for Continual Learning. (arXiv:2307.02251v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+McDonnell_M/0/1/0/all/0/1">Mark D. McDonnell</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_D/0/1/0/all/0/1">Dong Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Parveneh_A/0/1/0/all/0/1">Amin Parveneh</a>, <a href="http://arxiv.org/find/cs/1/au:+Abbasnejad_E/0/1/0/all/0/1">Ehsan Abbasnejad</a>, <a href="http://arxiv.org/find/cs/1/au:+Hengel_A/0/1/0/all/0/1">Anton van den Hengel</a></p>
<p>Continual learning (CL) aims to incrementally learn different tasks (such as
classification) in a non-stationary data stream without forgetting old ones.
Most CL works focus on tackling catastrophic forgetting under a
learning-from-scratch paradigm. However, with the increasing prominence of
foundation models, pre-trained models equipped with informative representations
have become available for various downstream requirements. Several CL methods
based on pre-trained models have been explored, either utilizing pre-extracted
features directly (which makes bridging distribution gaps challenging) or
incorporating adaptors (which may be subject to forgetting). In this paper, we
propose a concise and effective approach for CL with pre-trained models. Given
that forgetting occurs during parameter updating, we contemplate an alternative
approach that exploits training-free random projectors and class-prototype
accumulation, which thus bypasses the issue. Specifically, we inject a frozen
Random Projection layer with nonlinear activation between the pre-trained
model's feature representations and output head, which captures interactions
between features with expanded dimensionality, providing enhanced linear
separability for class-prototype-based CL. We also demonstrate the importance
of decorrelating the class-prototypes to reduce the distribution disparity when
using pre-trained representations. These techniques prove to be effective and
circumvent the problem of forgetting for both class- and domain-incremental
continual learning. Compared to previous methods applied to pre-trained
ViT-B/16 models, we reduce final error rates by between 10% and 62% on seven
class-incremental benchmarks, despite not using any rehearsal memory. We
conclude that the full potential of pre-trained models for simple, effective,
and fast CL has not hitherto been fully tapped. Code is at
github.com/RanPAC/RanPAC.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04684">FreeDrag: Feature Dragging for Reliable Point-based Image Editing. (arXiv:2307.04684v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ling_P/0/1/0/all/0/1">Pengyang Ling</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Pan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huaian Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1">Yi Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1">Jinjin Zheng</a></p>
<p>To serve the intricate and varied demands of image editing, precise and
flexible manipulation in image content is indispensable. Recently, Drag-based
editing methods have gained impressive performance. However, these methods
predominantly center on point dragging, resulting in two noteworthy drawbacks,
namely "miss tracking", where difficulties arise in accurately tracking the
predetermined handle points, and "ambiguous tracking", where tracked points are
potentially positioned in wrong regions that closely resemble the handle
points. To address the above issues, we propose FreeDrag, a feature dragging
methodology designed to free the burden on point tracking. The FreeDrag
incorporates two key designs, i.e., template feature via adaptive updating and
line search with backtracking, the former improves the stability against
drastic content change by elaborately controls feature updating scale after
each dragging, while the latter alleviates the misguidance from similar points
by actively restricting the search area in a line. These two technologies
together contribute to a more stable semantic dragging with higher efficiency.
Comprehensive experimental results substantiate that our approach significantly
outperforms pre-existing methodologies, offering reliable point-based editing
even in various complex scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10284">ECSIC: Epipolar Cross Attention for Stereo Image Compression. (arXiv:2307.10284v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Wodlinger_M/0/1/0/all/0/1">Matthias W&#xf6;dlinger</a>, <a href="http://arxiv.org/find/eess/1/au:+Kotera_J/0/1/0/all/0/1">Jan Kotera</a>, <a href="http://arxiv.org/find/eess/1/au:+Keglevic_M/0/1/0/all/0/1">Manuel Keglevic</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1">Jan Xu</a>, <a href="http://arxiv.org/find/eess/1/au:+Sablatnig_R/0/1/0/all/0/1">Robert Sablatnig</a></p>
<p>In this paper, we present ECSIC, a novel learned method for stereo image
compression. Our proposed method compresses the left and right images in a
joint manner by exploiting the mutual information between the images of the
stereo image pair using a novel stereo cross attention (SCA) module and two
stereo context modules. The SCA module performs cross-attention restricted to
the corresponding epipolar lines of the two images and processes them in
parallel. The stereo context modules improve the entropy estimation of the
second encoded image by using the first image as a context. We conduct an
extensive ablation study demonstrating the effectiveness of the proposed
modules and a comprehensive quantitative and qualitative comparison with
existing methods. ECSIC achieves state-of-the-art performance in stereo image
compression on the two popular stereo image datasets Cityscapes and InStereo2k
while allowing for fast encoding and decoding.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.16680">On the Trustworthiness Landscape of State-of-the-art Generative Models: A Survey and Outlook. (arXiv:2307.16680v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1">Mingyuan Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chengyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Cen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jun Huang</a></p>
<p>Diffusion models and large language models have emerged as leading-edge
generative models, revolutionizing various aspects of human life. However, the
practical implementations of these models have also exposed inherent risks,
bringing to the forefront their evil sides and sparking concerns regarding
their trustworthiness. Despite the wealth of literature on this subject, a
comprehensive survey specifically delving into the intersection of large-scale
generative models and their trustworthiness remains largely absent. To bridge
this gap, this paper investigates both the long-standing and emerging threats
associated with these models across four fundamental dimensions: 1) privacy, 2)
security, 3) fairness, and 4) responsibility. Based on the investigation
results, we develop an extensive map outlining the trustworthiness of large
generative models. After that, we provide practical recommendations and
potential research directions for future secure applications equipped with
large generative models, ultimately promoting the trustworthiness of the models
and benefiting the society as a whole.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03321">AFN: Adaptive Fusion Normalization via Encoder-Decoder Framework. (arXiv:2308.03321v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zikai Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huanran Chen</a></p>
<p>The success of deep learning is inseparable from normalization layers.
Researchers have proposed various normalization functions, and each of them has
both advantages and disadvantages. In response, efforts have been made to
design a unified normalization function that combines all normalization
procedures and mitigates their weaknesses. We also proposed a new normalization
function called Adaptive Fusion Normalization. Through experiments, we
demonstrate AFN outperforms the previous normalization techniques in domain
generalization and image classification tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04753">SAfER: Layer-Level Sensitivity Assessment for Efficient and Robust Neural Network Inference. (arXiv:2308.04753v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yvinec_E/0/1/0/all/0/1">Edouard Yvinec</a>, <a href="http://arxiv.org/find/cs/1/au:+Dapogny_A/0/1/0/all/0/1">Arnaud Dapogny</a>, <a href="http://arxiv.org/find/cs/1/au:+Bailly_K/0/1/0/all/0/1">Kevin Bailly</a>, <a href="http://arxiv.org/find/cs/1/au:+Fischer_X/0/1/0/all/0/1">Xavier Fischer</a></p>
<p>Deep neural networks (DNNs) demonstrate outstanding performance across most
computer vision tasks. Some critical applications, such as autonomous driving
or medical imaging, also require investigation into their behavior and the
reasons behind the decisions they make. In this vein, DNN attribution consists
in studying the relationship between the predictions of a DNN and its inputs.
Attribution methods have been adapted to highlight the most relevant weights or
neurons in a DNN, allowing to more efficiently select which weights or neurons
can be pruned. However, a limitation of these approaches is that weights are
typically compared within each layer separately, while some layers might appear
as more critical than others. In this work, we propose to investigate DNN layer
importance, i.e. to estimate the sensitivity of the accuracy w.r.t.
perturbations applied at the layer level. To do so, we propose a novel dataset
to evaluate our method as well as future works. We benchmark a number of
criteria and draw conclusions regarding how to assess DNN layer importance and,
consequently, how to budgetize layers for increased DNN efficiency (with
applications for DNN pruning and quantization), as well as robustness to
hardware failure (e.g. bit swaps).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.09835">Microscopy Image Segmentation via Point and Shape Regularized Data Synthesis. (arXiv:2308.09835v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shijie Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1">Mengwei Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Ach_T/0/1/0/all/0/1">Thomas Ach</a>, <a href="http://arxiv.org/find/cs/1/au:+Gerig_G/0/1/0/all/0/1">Guido Gerig</a></p>
<p>Current deep learning-based approaches for the segmentation of microscopy
images heavily rely on large amount of training data with dense annotation,
which is highly costly and laborious in practice. Compared to full annotation
where the complete contour of objects is depicted, point annotations,
specifically object centroids, are much easier to acquire and still provide
crucial information about the objects for subsequent segmentation. In this
paper, we assume access to point annotations only during training and develop a
unified pipeline for microscopy image segmentation using synthetically
generated training data. Our framework includes three stages: (1) it takes
point annotations and samples a pseudo dense segmentation mask constrained with
shape priors; (2) with an image generative model trained in an unpaired manner,
it translates the mask to a realistic microscopy image regularized by object
level consistency; (3) the pseudo masks along with the synthetic images then
constitute a pairwise dataset for training an ad-hoc segmentation model. On the
public MoNuSeg dataset, our synthesis pipeline produces more diverse and
realistic images than baseline models while maintaining high coherence between
input masks and generated images. When using the identical segmentation
backbones, the models trained on our synthetic dataset significantly outperform
those trained with pseudo-labels or baseline-generated images. Moreover, our
framework achieves comparable results to models trained on authentic microscopy
images with dense labels, demonstrating its potential as a reliable and highly
efficient alternative to labor-intensive manual pixel-wise annotations in
microscopy image segmentation. The code is available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.11471">Dynamic Open Vocabulary Enhanced Safe-landing with Intelligence (DOVESEI). (arXiv:2308.11471v4 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bong_H/0/1/0/all/0/1">Haechan Mark Bong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Rongge Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Azambuja_R/0/1/0/all/0/1">Ricardo de Azambuja</a>, <a href="http://arxiv.org/find/cs/1/au:+Beltrame_G/0/1/0/all/0/1">Giovanni Beltrame</a></p>
<p>This work targets what we consider to be the foundational step for urban
airborne robots, a safe landing. Our attention is directed toward what we deem
the most crucial aspect of the safe landing perception stack: segmentation. We
present a streamlined reactive UAV system that employs visual servoing by
harnessing the capabilities of open vocabulary image segmentation. This
approach can adapt to various scenarios with minimal adjustments, bypassing the
necessity for extensive data accumulation for refining internal models, thanks
to its open vocabulary methodology. Given the limitations imposed by local
authorities, our primary focus centers on operations originating from altitudes
of 100 meters. This choice is deliberate, as numerous preceding works have
dealt with altitudes up to 30 meters, aligning with the capabilities of small
stereo cameras. Consequently, we leave the remaining 20m to be navigated using
conventional 3D path planning methods. Utilizing monocular cameras and image
segmentation, our findings demonstrate the system's capability to successfully
execute landing maneuvers at altitudes as low as 20 meters. However, this
approach is vulnerable to intermittent and occasionally abrupt fluctuations in
the segmentation between frames in a video stream. To address this challenge,
we enhance the image segmentation output by introducing what we call a dynamic
focus: a masking mechanism that self adjusts according to the current landing
stage. This dynamic focus guides the control system to avoid regions beyond the
drone's safety radius projected onto the ground, thus mitigating the problems
with fluctuations. Through the implementation of this supplementary layer, our
experiments have reached improvements in the landing success rate of almost
tenfold when compared to global segmentation. All the source code is open
source and available online (github.com/MISTLab/DOVESEI).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16355">A Recycling Training Strategy for Medical Image Segmentation with Diffusion Denoising Models. (arXiv:2308.16355v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Fu_Y/0/1/0/all/0/1">Yunguan Fu</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1">Yiwen Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Saeed_S/0/1/0/all/0/1">Shaheer U Saeed</a>, <a href="http://arxiv.org/find/eess/1/au:+Clarkson_M/0/1/0/all/0/1">Matthew J Clarkson</a>, <a href="http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1">Yipeng Hu</a></p>
<p>Denoising diffusion models have found applications in image segmentation by
generating segmented masks conditioned on images. Existing studies
predominantly focus on adjusting model architecture or improving inference,
such as test-time sampling strategies. In this work, we focus on improving the
training strategy and propose a novel recycling method. During each training
step, a segmentation mask is first predicted given an image and a random noise.
This predicted mask, which replaces the conventional ground truth mask, is used
for denoising task during training. This approach can be interpreted as
aligning the training strategy with inference by eliminating the dependence on
ground truth masks for generating noisy samples. Our proposed method
significantly outperforms standard diffusion training, self-conditioning, and
existing recycling strategies across multiple medical imaging data sets: muscle
ultrasound, abdominal CT, prostate MR, and brain MR. This holds for two widely
adopted sampling strategies: denoising diffusion probabilistic model and
denoising diffusion implicit model. Importantly, existing diffusion models
often display a declining or unstable performance during inference, whereas our
novel recycling consistently enhances or maintains performance. We show that,
under a fair comparison with the same network architectures and computing
budget, the proposed recycling-based diffusion models achieved on-par
performance with non-diffusion-based supervised training. By ensembling the
proposed diffusion and the non-diffusion models, significant improvements to
the non-diffusion models have been observed across all applications,
demonstrating the value of this novel training method. This paper summarizes
these quantitative results and discusses their values, with a fully
reproducible JAX-based implementation, released at
https://github.com/mathpluscode/ImgX-DiffSeg.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.02833">Image-Object-Specific Prompt Learning for Few-Shot Class-Incremental Learning. (arXiv:2309.02833v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yoon_I/0/1/0/all/0/1">In-Ug Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_T/0/1/0/all/0/1">Tae-Min Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Sun-Kyung Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Young-Min Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jong-Hwan Kim</a></p>
<p>While many FSCIL studies have been undertaken, achieving satisfactory
performance, especially during incremental sessions, has remained challenging.
One prominent challenge is that the encoder, trained with an ample base session
training set, often underperforms in incremental sessions. In this study, we
introduce a novel training framework for FSCIL, capitalizing on the
generalizability of the Contrastive Language-Image Pre-training (CLIP) model to
unseen classes. We achieve this by formulating image-object-specific (IOS)
classifiers for the input images. Here, an IOS classifier refers to one that
targets specific attributes (like wings or wheels) of class objects rather than
the image's background. To create these IOS classifiers, we encode a bias
prompt into the classifiers using our specially designed module, which
harnesses key-prompt pairs to pinpoint the IOS features of classes in each
session. From an FSCIL standpoint, our framework is structured to retain
previous knowledge and swiftly adapt to new sessions without forgetting or
overfitting. This considers the updatability of modules in each session and
some tricks empirically found for fast convergence. Our approach consistently
demonstrates superior performance compared to state-of-the-art methods across
the miniImageNet, CIFAR100, and CUB200 datasets. Further, we provide additional
experiments to validate our learned model's ability to achieve IOS classifiers.
We also conduct ablation studies to analyze the impact of each module within
the architecture.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.05254">Towards Better Data Exploitation in Self-Supervised Monocular Depth Estimation. (arXiv:2309.05254v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jinfeng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1">Lingtong Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jie Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wei Liu</a></p>
<p>Depth estimation plays an important role in the robotic perception system.
Self-supervised monocular paradigm has gained significant attention since it
can free training from the reliance on depth annotations. Despite recent
advancements, existing self-supervised methods still underutilize the available
training data, limiting their generalization ability. In this paper, we take
two data augmentation techniques, namely Resizing-Cropping and
Splitting-Permuting, to fully exploit the potential of training datasets.
Specifically, the original image and the generated two augmented images are fed
into the training pipeline simultaneously and we leverage them to conduct
self-distillation. Additionally, we introduce the detail-enhanced DepthNet with
an extra full-scale branch in the encoder and a grid decoder to enhance the
restoration of fine details in depth maps. Experimental results demonstrate our
method can achieve state-of-the-art performance on the KITTI benchmark, with
both raw ground truth and improved ground truth. Moreover, our models also show
superior generalization performance when transferring to Make3D and NYUv2
datasets. Our codes are available at https://github.com/Sauf4896/BDEdepth.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.06207">SGNet: Salient Geometric Network for Point Cloud Registration. (arXiv:2309.06207v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1">Qianliang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1">Yaqing Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1">Lei Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1">Chuanwei Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1">Jin Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jian Yang</a></p>
<p>Point Cloud Registration (PCR) is a critical and challenging task in computer
vision. One of the primary difficulties in PCR is identifying salient and
meaningful points that exhibit consistent semantic and geometric properties
across different scans. Previous methods have encountered challenges with
ambiguous matching due to the similarity among patch blocks throughout the
entire point cloud and the lack of consideration for efficient global geometric
consistency. To address these issues, we propose a new framework that includes
several novel techniques. Firstly, we introduce a semantic-aware geometric
encoder that combines object-level and patch-level semantic information. This
encoder significantly improves registration recall by reducing ambiguity in
patch-level superpoint matching. Additionally, we incorporate a prior knowledge
approach that utilizes an intrinsic shape signature to identify salient points.
This enables us to extract the most salient super points and meaningful dense
points in the scene. Secondly, we introduce an innovative transformer that
encodes High-Order (HO) geometric features. These features are crucial for
identifying salient points within initial overlap regions while considering
global high-order geometric consistency. To optimize this high-order
transformer further, we introduce an anchor node selection strategy. By
encoding inter-frame triangle or polyhedron consistency features based on these
anchor nodes, we can effectively learn high-order geometric features of salient
super points. These high-order features are then propagated to dense points and
utilized by a Sinkhorn matching module to identify key correspondences for
successful registration. In our experiments conducted on well-known datasets
such as 3DMatch/3DLoMatch and KITTI, our approach has shown promising results,
highlighting the effectiveness of our novel method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11955">A Study of Forward-Forward Algorithm for Self-Supervised Learning. (arXiv:2309.11955v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Brenig_J/0/1/0/all/0/1">Jonas Brenig</a>, <a href="http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1">Radu Timofte</a></p>
<p>Self-supervised representation learning has seen remarkable progress in the
last few years, with some of the recent methods being able to learn useful
image representations without labels. These methods are trained using
backpropagation, the de facto standard. Recently, Geoffrey Hinton proposed the
forward-forward algorithm as an alternative training method. It utilizes two
forward passes and a separate loss function for each layer to train the network
without backpropagation.
</p>
<p>In this study, for the first time, we study the performance of
forward-forward vs. backpropagation for self-supervised representation learning
and provide insights into the learned representation spaces. Our benchmark
employs four standard datasets, namely MNIST, F-MNIST, SVHN and CIFAR-10, and
three commonly used self-supervised representation learning techniques, namely
rotation, flip and jigsaw.
</p>
<p>Our main finding is that while the forward-forward algorithm performs
comparably to backpropagation during (self-)supervised training, the transfer
performance is significantly lagging behind in all the studied settings. This
may be caused by a combination of factors, including having a loss function for
each layer and the way the supervised training is realized in the
forward-forward paradigm. In comparison to backpropagation, the forward-forward
algorithm focuses more on the boundaries and drops part of the information
unnecessary for making decisions which harms the representation learning goal.
Further investigation and research are necessary to stabilize the
forward-forward strategy for self-supervised learning, to work beyond the
datasets and configurations demonstrated by Geoffrey Hinton.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00132">Towards Robust Audiovisual Segmentation in Complex Environments with Quantization-based Semantic Decomposition. (arXiv:2310.00132v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jinglu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiaohao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1">Xiulian Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1">Rita Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yan Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1">Bhiksha Raj</a></p>
<p>Audiovisual segmentation (AVS) is a challenging task that aims to segment
visual objects in videos according to their associated acoustic cues. With
multiple sound sources and background disturbances involved, establishing
robust correspondences between audio and visual contents poses unique
challenges due to (1) complex entanglement across sound sources and (2)
frequent changes in the occurrence of distinct sound events. Assuming sound
events occur independently, the multi-source semantic space can be represented
as the Cartesian product of single-source sub-spaces. We are motivated to
decompose the multi-source audio semantics into single-source semantics for
more effective interactions with visual content. We propose a semantic
decomposition method based on product quantization, where the multi-source
semantics can be decomposed and represented by several disentangled and
noise-suppressed single-source semantics. Furthermore, we introduce a
global-to-local quantization mechanism, which distills knowledge from stable
global (clip-level) features into local (frame-level) ones, to handle frequent
changes in audio semantics. Extensive experiments demonstrate that our
semantically decomposed audio representation significantly improves AVS
performance, e.g., +21.2% mIoU on the challenging AVS-Semantic benchmark with
ResNet50 backbone. https://github.com/lxa9867/QSD.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01994">Understanding Masked Autoencoders From a Local Contrastive Perspective. (arXiv:2310.01994v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1">Xiaoyu Yue</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1">Lei Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1">Meng Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1">Jiangmiao Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xihui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1">Luping Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1">Wanli Ouyang</a></p>
<p>Masked AutoEncoder (MAE) has revolutionized the field of self-supervised
learning with its simple yet effective masking and reconstruction strategies.
However, despite achieving state-of-the-art performance across various
downstream vision tasks, the underlying mechanisms that drive MAE's efficacy
are less well-explored compared to the canonical contrastive learning paradigm.
In this paper, we first propose a local perspective to explicitly extract a
local contrastive form from MAE's reconstructive objective at the patch level.
And then we introduce a new empirical framework, called Local Contrastive MAE
(LC-MAE), to analyze both reconstructive and contrastive aspects of MAE. LC-MAE
reveals that MAE learns invariance to random masking and ensures distribution
consistency between the learned token embeddings and the original images.
Furthermore, we dissect the contribution of the decoder and random masking to
MAE's success, revealing both the decoder's learning mechanism and the dual
role of random masking as data augmentation and effective receptive field
restriction. Our experimental analysis sheds light on the intricacies of MAE
and summarizes some useful design methodologies, which can inspire more
powerful visual self-supervised methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.10624">DynVideo-E: Harnessing Dynamic NeRF for Large-Scale Motion- and View-Change Human-Centric Video Editing. (arXiv:2310.10624v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jia-Wei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yan-Pei Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jay Zhangjie Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1">Weijia Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1">Yuchao Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1">Rui Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Keppo_J/0/1/0/all/0/1">Jussi Keppo</a>, <a href="http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1">Ying Shan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1">Mike Zheng Shou</a></p>
<p>Despite recent progress in diffusion-based video editing, existing methods
are limited to short-length videos due to the contradiction between long-range
consistency and frame-wise editing. Prior attempts to address this challenge by
introducing video-2D representations encounter significant difficulties with
large-scale motion- and view-change videos, especially in human-centric
scenarios. To overcome this, we propose to introduce the dynamic Neural
Radiance Fields (NeRF) as the innovative video representation, where the
editing can be performed in the 3D spaces and propagated to the entire video
via the deformation field. To provide consistent and controllable editing, we
propose the image-based video-NeRF editing pipeline with a set of innovative
designs, including multi-view multi-pose Score Distillation Sampling (SDS) from
both the 2D personalized diffusion prior and 3D diffusion prior, reconstruction
losses, text-guided local parts super-resolution, and style transfer. Extensive
experiments demonstrate that our method, dubbed as DynVideo-E, significantly
outperforms SOTA approaches on two challenging datasets by a large margin of
50% ~ 95% for human preference. Code will be released at
https://showlab.github.io/DynVideo-E/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.12262">Improving SCGAN&#x27;s Similarity Constraint and Learning a Better Disentangled Representation. (arXiv:2310.12262v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yazdanpanah_I/0/1/0/all/0/1">Iman Yazdanpanah</a>, <a href="http://arxiv.org/find/cs/1/au:+Eslamian_A/0/1/0/all/0/1">Ali Eslamian</a></p>
<p>SCGAN adds a similarity constraint between generated images and conditions as
a regularization term on generative adversarial networks. Similarity constraint
works as a tutor to instruct the generator network to comprehend the difference
of representations based on conditions. We understand how SCGAN works on a
deeper level. This understanding makes us realize that the similarity
constraint functions like the contrastive loss function. We believe that a
model with high understanding and intelligence measures the similarity between
images based on their structure and high level features, just like humans do.
Two major changes we applied to SCGAN in order to make a modified model are
using SSIM to measure similarity between images and applying contrastive loss
principles to the similarity constraint. The modified model performs better
using FID and FactorVAE metrics. The modified model also has better
generalisability compared to other models. Keywords Generative Adversarial
Nets, Unsupervised Learning, Disentangled Representation Learning, Contrastive
Disentanglement, SSIM
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03052">MixUp-MIL: A Study on Linear &amp; Multilinear Interpolation-Based Data Augmentation for Whole Slide Image Classification. (arXiv:2311.03052v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gadermayr_M/0/1/0/all/0/1">Michael Gadermayr</a>, <a href="http://arxiv.org/find/cs/1/au:+Koller_L/0/1/0/all/0/1">Lukas Koller</a>, <a href="http://arxiv.org/find/cs/1/au:+Tschuchnig_M/0/1/0/all/0/1">Maximilian Tschuchnig</a>, <a href="http://arxiv.org/find/cs/1/au:+Stangassinger_L/0/1/0/all/0/1">Lea Maria Stangassinger</a>, <a href="http://arxiv.org/find/cs/1/au:+Kreutzer_C/0/1/0/all/0/1">Christina Kreutzer</a>, <a href="http://arxiv.org/find/cs/1/au:+Couillard_Despres_S/0/1/0/all/0/1">Sebastien Couillard-Despres</a>, <a href="http://arxiv.org/find/cs/1/au:+Oostingh_G/0/1/0/all/0/1">Gertie Janneke Oostingh</a>, <a href="http://arxiv.org/find/cs/1/au:+Hittmair_A/0/1/0/all/0/1">Anton Hittmair</a></p>
<p>For classifying digital whole slide images in the absence of pixel level
annotation, typically multiple instance learning methods are applied. Due to
the generic applicability, such methods are currently of very high interest in
the research community, however, the issue of data augmentation in this context
is rarely explored. Here we investigate linear and multilinear interpolation
between feature vectors, a data augmentation technique, which proved to be
capable of improving the generalization performance classification networks and
also for multiple instance learning. Experiments, however, have been performed
on only two rather small data sets and one specific feature extraction approach
so far and a strong dependence on the data set has been identified. Here we
conduct a large study incorporating 10 different data set configurations, two
different feature extraction approaches (supervised and self-supervised), stain
normalization and two multiple instance learning architectures. The results
showed an extraordinarily high variability in the effect of the method. We
identified several interesting aspects to bring light into the darkness and
identified novel promising fields of research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13443">Guided Flows for Generative Modeling and Decision Making. (arXiv:2311.13443v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1">Qinqing Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_M/0/1/0/all/0/1">Matt Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Shaul_N/0/1/0/all/0/1">Neta Shaul</a>, <a href="http://arxiv.org/find/cs/1/au:+Lipman_Y/0/1/0/all/0/1">Yaron Lipman</a>, <a href="http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1">Aditya Grover</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1">Ricky T. Q. Chen</a></p>
<p>Classifier-free guidance is a key component for enhancing the performance of
conditional generative models across diverse tasks. While it has previously
demonstrated remarkable improvements for the sample quality, it has only been
exclusively employed for diffusion models. In this paper, we integrate
classifier-free guidance into Flow Matching (FM) models, an alternative
simulation-free approach that trains Continuous Normalizing Flows (CNFs) based
on regressing vector fields. We explore the usage of \emph{Guided Flows} for a
variety of downstream applications. We show that Guided Flows significantly
improves the sample quality in conditional image generation and zero-shot
text-to-speech synthesis, boasting state-of-the-art performance. Notably, we
are the first to apply flow models for plan generation in the offline
reinforcement learning setting, showcasing a 10x speedup in computation
compared to diffusion models while maintaining comparable performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13713">A Somewhat Robust Image Watermark against Diffusion-based Editing Models. (arXiv:2311.13713v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1">Mingtian Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tianhao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1">Somesh Jha</a></p>
<p>Recently, diffusion models (DMs) have become the state-of-the-art method for
image synthesis. Editing models based on DMs, known for their high fidelity and
precision, have inadvertently introduced new challenges related to image
copyright infringement and malicious editing. Our work is the first to
formalize and address this issue. After assessing and attempting to enhance
traditional image watermarking techniques, we recognize their limitations in
this emerging context. In response, we develop a novel technique, RIW (Robust
Invisible Watermarking), to embed invisible watermarks leveraging adversarial
example techniques. Our technique ensures a high extraction accuracy of $96\%$
for the invisible watermark after editing, compared to the $0\%$ offered by
conventional methods. We provide access to our code at
https://github.com/BennyTMT/RIW.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15167">Self-supervised OCT Image Denoising with Slice-to-Slice Registration and Reconstruction. (arXiv:2311.15167v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1">Shijie Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Alexopoulos_P/0/1/0/all/0/1">Palaiologos Alexopoulos</a>, <a href="http://arxiv.org/find/eess/1/au:+Vellappally_A/0/1/0/all/0/1">Anse Vellappally</a>, <a href="http://arxiv.org/find/eess/1/au:+Zambrano_R/0/1/0/all/0/1">Ronald Zambrano</a>, <a href="http://arxiv.org/find/eess/1/au:+Gadi_W/0/1/0/all/0/1">Wollstein Gadi</a>, <a href="http://arxiv.org/find/eess/1/au:+Gerig_G/0/1/0/all/0/1">Guido Gerig</a></p>
<p>Strong speckle noise is inherent to optical coherence tomography (OCT)
imaging and represents a significant obstacle for accurate quantitative
analysis of retinal structures which is key for advances in clinical diagnosis
and monitoring of disease. Learning-based self-supervised methods for
structure-preserving noise reduction have demonstrated superior performance
over traditional methods but face unique challenges in OCT imaging. The high
correlation of voxels generated by coherent A-scan beams undermines the
efficacy of self-supervised learning methods as it violates the assumption of
independent pixel noise. We conduct experiments demonstrating limitations of
existing models due to this independence assumption. We then introduce a new
end-to-end self-supervised learning framework specifically tailored for OCT
image denoising, integrating slice-by-slice training and registration modules
into one network. An extensive ablation study is conducted for the proposed
approach. Comparison to previously published self-supervised denoising models
demonstrates improved performance of the proposed framework, potentially
serving as a preprocessing step towards superior segmentation performance and
quantitative analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17098">DyRA: Dynamic Resolution Adjustment for Scale-robust Object Detection. (arXiv:2311.17098v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Seo_D/0/1/0/all/0/1">Daeun Seo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Hoeseok Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">Hyungshin Kim</a></p>
<p>In object detection, achieving constant accuracy is challenging due to the
variability of object sizes. One possible solution to this problem is to
optimize the input resolution, known as a multi-resolution strategy. Previous
approaches for optimizing resolution are often based on pre-defined resolutions
or a dynamic neural network, but there is a lack of study for run-time
resolution optimization for existing architecture. In this paper, we propose an
adaptive resolution scaling network called DyRA, which comprises convolutions
and transformer encoder blocks, for existing detectors. Our DyRA returns a
scale factor from an input image, which enables instance-specific scaling. This
network is jointly trained with detectors with specially designed loss
functions, namely ParetoScaleLoss and BalanceLoss. The ParetoScaleLoss produces
an adaptive scale factor from the image, while the BalanceLoss optimizes the
scale factor according to localization power for the dataset. The loss function
is designed to minimize accuracy drop about the contrasting objective of small
and large objects. Our experiments on COCO, RetinaNet, Faster-RCNN, FCOS, and
Mask-RCNN achieved 1.3%, 1.1%, 1.3%, and 0.8% accuracy improvement than a
multi-resolution baseline with solely resolution adjustment. The code is
available at https://github.com/DaEunFullGrace/DyRA.git.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17117">Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation. (arXiv:2311.17117v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1">Li Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xin Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Peng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1">Ke Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Bang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bo_L/0/1/0/all/0/1">Liefeng Bo</a></p>
<p>Character Animation aims to generating character videos from still images
through driving signals. Currently, diffusion models have become the mainstream
in visual generation research, owing to their robust generative capabilities.
However, challenges persist in the realm of image-to-video, especially in
character animation, where temporally maintaining consistency with detailed
information from character remains a formidable problem. In this paper, we
leverage the power of diffusion models and propose a novel framework tailored
for character animation. To preserve consistency of intricate appearance
features from reference image, we design ReferenceNet to merge detail features
via spatial attention. To ensure controllability and continuity, we introduce
an efficient pose guider to direct character's movements and employ an
effective temporal modeling approach to ensure smooth inter-frame transitions
between video frames. By expanding the training data, our approach can animate
arbitrary characters, yielding superior results in character animation compared
to other image-to-video methods. Furthermore, we evaluate our method on
benchmarks for fashion video and human dance synthesis, achieving
state-of-the-art results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17245">LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS. (arXiv:2311.17245v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1">Zhiwen Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kevin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_K/0/1/0/all/0/1">Kairun Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Zehao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1">Dejia Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhangyang Wang</a></p>
<p>Recent advancements in real-time neural rendering using point-based
techniques have paved the way for the widespread adoption of 3D
representations. However, foundational approaches like 3D Gaussian Splatting
come with a substantial storage overhead caused by growing the SfM points to
millions, often demanding gigabyte-level disk space for a single unbounded
scene, posing significant scalability challenges and hindering the splatting
efficiency.
</p>
<p>To address this challenge, we introduce LightGaussian, a novel method
designed to transform 3D Gaussians into a more efficient and compact format.
Drawing inspiration from the concept of Network Pruning, LightGaussian
identifies Gaussians that are insignificant in contributing to the scene
reconstruction and adopts a pruning and recovery process, effectively reducing
redundancy in Gaussian counts while preserving visual effects. Additionally,
LightGaussian employs distillation and pseudo-view augmentation to distill
spherical harmonics to a lower degree, allowing knowledge transfer to more
compact representations while maintaining reflectance. Furthermore, we propose
a hybrid scheme, VecTree Quantization, to quantize all attributes, resulting in
lower bitwidth representations with minimal accuracy losses.
</p>
<p>In summary, LightGaussian achieves an averaged compression rate over 15x
while boosting the FPS from 139 to 215, enabling an efficient representation of
complex scenes on Mip-NeRF 360, Tank and Temple datasets.
</p>
<p>Project website: https://lightgaussian.github.io/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00844">Sparse Beats Dense: Rethinking Supervision in Radar-Camera Depth Completion. (arXiv:2312.00844v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Huadong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Jing_M/0/1/0/all/0/1">Minhao Jing</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1">Jiajun Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1">Haoqiang Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1">Renhe Ji</a></p>
<p>It is widely believed that the dense supervision is better than the sparse
supervision in the field of depth completion, but the underlying reasons for
this are rarely discussed. In this paper, we find that the challenge of using
sparse supervision for training Radar-Camera depth prediction models is the
Projection Transformation Collapse (PTC). The PTC implies that sparse
supervision leads the model to learn unexpected collapsed projection
transformations between Image/Radar/LiDAR spaces. Building on this insight, we
propose a novel ``Disruption-Compensation" framework to handle the PTC, thereby
relighting the use of sparse supervision in depth completion tasks. The
disruption part deliberately discards position correspondences among
Image/Radar/LiDAR, while the compensation part leverages 3D spatial and 2D
semantic information to compensate for the discarded beneficial position
correspondence. Extensive experimental results demonstrate that our framework
(sparse supervision) outperforms the state-of-the-art (dense supervision) with
11.6$\%$ improvement in mean absolute error and $1.6 \times$ speedup. The code
is available at ...
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00858">DeepCache: Accelerating Diffusion Models for Free. (arXiv:2312.00858v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xinyin Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_G/0/1/0/all/0/1">Gongfan Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinchao Wang</a></p>
<p>Diffusion models have recently gained unprecedented attention in the field of
image synthesis due to their remarkable generative capabilities.
Notwithstanding their prowess, these models often incur substantial
computational costs, primarily attributed to the sequential denoising process
and cumbersome model size. Traditional methods for compressing diffusion models
typically involve extensive retraining, presenting cost and feasibility
challenges. In this paper, we introduce DeepCache, a novel training-free
paradigm that accelerates diffusion models from the perspective of model
architecture. DeepCache capitalizes on the inherent temporal redundancy
observed in the sequential denoising steps of diffusion models, which caches
and retrieves features across adjacent denoising stages, thereby curtailing
redundant computations. Utilizing the property of the U-Net, we reuse the
high-level features while updating the low-level features in a very cheap way.
This innovative strategy, in turn, enables a speedup factor of 2.3$\times$ for
Stable Diffusion v1.5 with only a 0.05 decline in CLIP Score, and 4.1$\times$
for LDM-4-G with a slight decrease of 0.22 in FID on ImageNet. Our experiments
also demonstrate DeepCache's superiority over existing pruning and distillation
methods that necessitate retraining and its compatibility with current sampling
techniques. Furthermore, we find that under the same throughput, DeepCache
effectively achieves comparable or even marginally improved results with DDIM
or PLMS. The code is available at https://github.com/horseee/DeepCache
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02366">Towards General Purpose Vision Foundation Models for Medical Image Analysis: An Experimental Study of DINOv2 on Radiology Benchmarks. (arXiv:2312.02366v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Baharoon_M/0/1/0/all/0/1">Mohammed Baharoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Qureshi_W/0/1/0/all/0/1">Waseem Qureshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ouyang_J/0/1/0/all/0/1">Jiahong Ouyang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yanwu Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Aljouie_A/0/1/0/all/0/1">Abdulrhman Aljouie</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1">Wei Peng</a></p>
<p>The integration of deep learning systems into the medical domain has been
hindered by the resource-intensive process of data annotation and the inability
of these systems to generalize to different data distributions. Foundation
models, which are models pre-trained on large datasets, have emerged as a
solution to reduce reliance on annotated data and enhance model
generalizability and robustness. DINOv2, an open-source foundation model
pre-trained with self-supervised learning on 142 million curated natural
images, excels in extracting general-purpose visual representations, exhibiting
promising capabilities across various vision tasks. Nevertheless, a critical
question remains unanswered regarding DINOv2's adaptability to radiological
imaging, and the clarity on whether its features are sufficiently general to
benefit radiology image analysis is yet to be established. Therefore, this
study comprehensively evaluates DINOv2 for radiology, conducting over 100
experiments across diverse modalities (X-ray, CT, and MRI). Tasks include
disease classification and organ segmentation on both 2D and 3D images,
evaluated under different settings like kNN, few-shot learning, linear-probing,
end-to-end fine-tuning, and parameter-efficient fine-tuning, to measure the
effectiveness and generalizability of the DINOv2 feature embeddings.
Comparative analyses with established medical image analysis models, U-Net and
TransUnet for segmentation, and CNN and ViT models pre-trained via supervised,
weakly supervised, and self-supervised learning for classification, reveal
DINOv2's superior performance in segmentation tasks and competitive results in
disease classification. The findings contribute insights to potential avenues
for optimizing pre-training strategies for medical imaging and enhancing the
broader understanding of DINOv2's role in bridging the gap between natural and
radiological image analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02428">FreestyleRet: Retrieving Images from Style-Diversified Queries. (arXiv:2312.02428v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1">Curise Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1">Peng Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1">Zesen Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Kehan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sui_J/0/1/0/all/0/1">Jialu Sui</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1">Li Yuan</a></p>
<p>Image Retrieval aims to retrieve corresponding images based on a given query.
In application scenarios, users intend to express their retrieval intent
through various query styles. However, current retrieval tasks predominantly
focus on text-query retrieval exploration, leading to limited retrieval query
options and potential ambiguity or bias in user intention. In this paper, we
propose the Style-Diversified Query-Based Image Retrieval task, which enables
retrieval based on various query styles. To facilitate the novel setting, we
propose the first Diverse-Style Retrieval dataset, encompassing diverse query
styles including text, sketch, low-resolution, and art. We also propose a
light-weighted style-diversified retrieval framework. For various query style
inputs, we apply the Gram Matrix to extract the query's textural features and
cluster them into a style space with style-specific bases. Then we employ the
style-init prompt tuning module to enable the visual encoder to comprehend the
texture and style information of the query. Experiments demonstrate that our
model, employing the style-init prompt tuning strategy, outperforms existing
retrieval models on the style-diversified retrieval task. Moreover,
style-diversified queries~(sketch+text, art+text, etc) can be simultaneously
retrieved in our model. The auxiliary information from other queries enhances
the retrieval performance within the respective query.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02617">DreaMo: Articulated 3D Reconstruction From A Single Casual Video. (arXiv:2312.02617v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tu_T/0/1/0/all/0/1">Tao Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Ming-Feng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1">Chieh Hubert Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1">Yen-Chi Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1">Min Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Ming-Hsuan Yang</a></p>
<p>Articulated 3D reconstruction has valuable applications in various domains,
yet it remains costly and demands intensive work from domain experts. Recent
advancements in template-free learning methods show promising results with
monocular videos. Nevertheless, these approaches necessitate a comprehensive
coverage of all viewpoints of the subject in the input video, thus limiting
their applicability to casually captured videos from online sources. In this
work, we study articulated 3D shape reconstruction from a single and casually
captured internet video, where the subject's view coverage is incomplete. We
propose DreaMo that jointly performs shape reconstruction while solving the
challenging low-coverage regions with view-conditioned diffusion prior and
several tailored regularizations. In addition, we introduce a skeleton
generation strategy to create human-interpretable skeletons from the learned
neural bones and skinning weights. We conduct our study on a self-collected
internet video collection characterized by incomplete view coverage. DreaMo
shows promising quality in novel-view rendering, detailed articulated shape
reconstruction, and skeleton generation. Extensive qualitative and quantitative
studies validate the efficacy of each proposed component, and show existing
methods are unable to solve correct geometry due to the incomplete view
coverage.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03032">Zero-Shot Point Cloud Registration. (arXiv:2312.03032v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Weijie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mei_G/0/1/0/all/0/1">Guofeng Mei</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1">Bin Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xiaoshui Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Poiesi_F/0/1/0/all/0/1">Fabio Poiesi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1">Luc Van Gool</a>, <a href="http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1">Nicu Sebe</a>, <a href="http://arxiv.org/find/cs/1/au:+Lepri_B/0/1/0/all/0/1">Bruno Lepri</a></p>
<p>Learning-based point cloud registration approaches have significantly
outperformed their traditional counterparts. However, they typically require
extensive training on specific datasets. In this paper, we propose , the first
zero-shot point cloud registration approach that eliminates the need for
training on point cloud datasets. The cornerstone of ZeroReg is the novel
transfer of image features from keypoints to the point cloud, enriched by
aggregating information from 3D geometric neighborhoods. Specifically, we
extract keypoints and features from 2D image pairs using a frozen pretrained 2D
backbone. These features are then projected in 3D, and patches are constructed
by searching for neighboring points. We integrate the geometric and visual
features of each point using our novel parameter-free geometric decoder.
Subsequently, the task of determining correspondences between point clouds is
formulated as an optimal transport problem. Extensive evaluations of ZeroReg
demonstrate its competitive performance against both traditional and
learning-based methods. On benchmarks such as 3DMatch, 3DLoMatch, and ScanNet,
ZeroReg achieves impressive Recall Ratios (RR) of over 84%, 46%, and 75%,
respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03046">Diversified in-domain synthesis with efficient fine-tuning for few-shot classification. (arXiv:2312.03046v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Costa_V/0/1/0/all/0/1">Victor G. Turrisi da Costa</a>, <a href="http://arxiv.org/find/cs/1/au:+DallAsen_N/0/1/0/all/0/1">Nicola Dall&#x27;Asen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yiming Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1">Nicu Sebe</a>, <a href="http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1">Elisa Ricci</a></p>
<p>Few-shot image classification aims to learn an image classifier using only a
small set of labeled examples per class. A recent research direction for
improving few-shot classifiers involves augmenting the labelled samples with
synthetic images created by state-of-the-art text-to-image generation models.
Following this trend, we propose Diversified In-domain Synthesis with Efficient
Fine-tuning (DISEF), a novel approach which addresses the generalization
challenge in few-shot learning using synthetic data. DISEF consists of two main
components. First, we propose a novel text-to-image augmentation pipeline that,
by leveraging the real samples and their rich semantics coming from an advanced
captioning model, promotes in-domain sample diversity for better
generalization. Second, we emphasize the importance of effective model
fine-tuning in few-shot recognition, proposing to use Low-Rank Adaptation
(LoRA) for joint adaptation of the text and image encoders in a Vision Language
Model. We validate our method in ten different benchmarks, consistently
outperforming baselines and establishing a new state-of-the-art for few-shot
classification. Code is available at https://github.com/vturrisi/disef.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03490">PneumoLLM: Harnessing the Power of Large Language Model for Pneumoconiosis Diagnosis. (arXiv:2312.03490v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Song_M/0/1/0/all/0/1">Meiyue Song</a>, <a href="http://arxiv.org/find/eess/1/au:+Yu_Z/0/1/0/all/0/1">Zhihua Yu</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1">Jiaxin Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1">Jiarui Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Lu_Y/0/1/0/all/0/1">Yuting Lu</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1">Baicun Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1">Xiaoxu Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Huang_Q/0/1/0/all/0/1">Qinghua Huang</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1">Zhijun Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Kanellakis_N/0/1/0/all/0/1">Nikolaos I.Kanellakis</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1">Jiangfeng Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1">Jing Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_B/0/1/0/all/0/1">Binglu Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1">Juntao Yang</a></p>
<p>The conventional pretraining-and-finetuning paradigm, while effective for
common diseases with ample data, faces challenges in diagnosing data-scarce
occupational diseases like pneumoconiosis. Recently, large language models
(LLMs) have exhibits unprecedented ability when conducting multiple tasks in
dialogue, bringing opportunities to diagnosis. A common strategy might involve
using adapter layers for vision-language alignment and diagnosis in a dialogic
manner. Yet, this approach often requires optimization of extensive learnable
parameters in the text branch and the dialogue head, potentially diminishing
the LLMs' efficacy, especially with limited training data. In our work, we
innovate by eliminating the text branch and substituting the dialogue head with
a classification head. This approach presents a more effective method for
harnessing LLMs in diagnosis with fewer learnable parameters. Furthermore, to
balance the retention of detailed image information with progression towards
accurate diagnosis, we introduce the contextual multi-token engine. This engine
is specialized in adaptively generating diagnostic tokens. Additionally, we
propose the information emitter module, which unidirectionally emits
information from image tokens to diagnosis tokens. Comprehensive experiments
validate the superiority of our methods and the effectiveness of proposed
modules. Our codes can be found at
https://github.com/CodeMonsterPHD/PneumoLLM/tree/main.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03701">Self-conditioned Image Generation via Generating Representations. (arXiv:2312.03701v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1">Tianhong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Katabi_D/0/1/0/all/0/1">Dina Katabi</a>, <a href="http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1">Kaiming He</a></p>
<p>This paper presents $\textbf{R}$epresentation-$\textbf{C}$onditioned image
$\textbf{G}$eneration (RCG), a simple yet effective image generation framework
which sets a new benchmark in class-unconditional image generation. RCG does
not condition on any human annotations. Instead, it conditions on a
self-supervised representation distribution which is mapped from the image
distribution using a pre-trained encoder. During generation, RCG samples from
such representation distribution using a representation diffusion model (RDM),
and employs a pixel generator to craft image pixels conditioned on the sampled
representation. Such a design provides substantial guidance during the
generative process, resulting in high-quality image generation. Tested on
ImageNet 256$\times$256, RCG achieves a Frechet Inception Distance (FID) of
3.31 and an Inception Score (IS) of 253.4. These results not only significantly
improve the state-of-the-art of class-unconditional image generation but also
rival the current leading methods in class-conditional image generation,
bridging the long-standing performance gap between these two tasks. Code is
available at https://github.com/LTH14/rcg.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03777">On the Robustness of Large Multimodal Models Against Image Adversarial Attacks. (arXiv:2312.03777v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1">Xuanming Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Aparcedo_A/0/1/0/all/0/1">Alejandro Aparcedo</a>, <a href="http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1">Young Kyun Jang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1">Ser-Nam Lim</a></p>
<p>Recent advances in instruction tuning have led to the development of
State-of-the-Art Large Multimodal Models (LMMs). Given the novelty of these
models, the impact of visual adversarial attacks on LMMs has not been
thoroughly examined. We conduct a comprehensive study of the robustness of
various LMMs against different adversarial attacks, evaluated across tasks
including image classification, image captioning, and Visual Question Answer
(VQA). We find that in general LMMs are not robust to visual adversarial
inputs. However, our findings suggest that context provided to the model via
prompts, such as questions in a QA pair helps to mitigate the effects of visual
adversarial inputs. Notably, the LMMs evaluated demonstrated remarkable
resilience to such attacks on the ScienceQA task with only an 8.10% drop in
performance compared to their visual counterparts which dropped 99.73%. We also
propose a new approach to real-world image classification which we term query
decomposition. By incorporating existence queries into our input prompt we
observe diminished attack effectiveness and improvements in image
classification accuracy. This research highlights a previously under-explored
facet of LMM robustness and sets the stage for future work aimed at
strengthening the resilience of multimodal systems in adversarial environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04369">SingingHead: A Large-scale 4D Dataset for Singing Head Animation. (arXiv:2312.04369v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Sijing Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yunhao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Weitian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1">Jun Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yucheng Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1">Yichao Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1">Guangtao Zhai</a></p>
<p>Singing, as a common facial movement second only to talking, can be regarded
as a universal language across ethnicities and cultures, plays an important
role in emotional communication, art, and entertainment. However, it is often
overlooked in the field of audio-driven facial animation due to the lack of
singing head datasets and the domain gap between singing and talking in rhythm
and amplitude. To this end, we collect a high-quality large-scale singing head
dataset, SingingHead, which consists of more than 27 hours of synchronized
singing video, 3D facial motion, singing audio, and background music from 76
individuals and 8 types of music. Along with the SingingHead dataset, we argue
that 3D and 2D facial animation tasks can be solved together, and propose a
unified singing facial animation framework named UniSinger to achieve both
singing audio-driven 3D singing head animation and 2D singing portrait video
synthesis. Extensive comparative experiments with both SOTA 3D facial animation
and 2D portrait animation methods demonstrate the necessity of singing-specific
datasets in singing head animation tasks and the promising performance of our
unified facial animation framework.
</p>
</p>
</div>

    </div>
    </body>
    