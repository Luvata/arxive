<!DOCTYPE html>
<html>
<head>
<title>2023-11-15-cs-lg</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2311.06253">Advancing Parsimonious Deep Learning Weather Prediction using the HEALPix Mesh. (arXiv:2311.06253v1 [physics.ao-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Karlbauer_M/0/1/0/all/0/1">Matthias Karlbauer</a>, <a href="http://arxiv.org/find/physics/1/au:+Cresswell_Clay_N/0/1/0/all/0/1">Nathaniel Cresswell-Clay</a>, <a href="http://arxiv.org/find/physics/1/au:+Moreno_R/0/1/0/all/0/1">Raul A. Moreno</a>, <a href="http://arxiv.org/find/physics/1/au:+Durran_D/0/1/0/all/0/1">Dale R. Durran</a>, <a href="http://arxiv.org/find/physics/1/au:+Kurth_T/0/1/0/all/0/1">Thorsten Kurth</a>, <a href="http://arxiv.org/find/physics/1/au:+Butz_M/0/1/0/all/0/1">Martin V. Butz</a></p>
<p>We present a parsimonious deep learning weather prediction model on the
Hierarchical Equal Area isoLatitude Pixelization (HEALPix) to forecast seven
atmospheric variables for arbitrarily long lead times on a global approximately
110 km mesh at 3h time resolution. In comparison to state-of-the-art machine
learning weather forecast models, such as Pangu-Weather and GraphCast, our
DLWP-HPX model uses coarser resolution and far fewer prognostic variables. Yet,
at one-week lead times its skill is only about one day behind the
state-of-the-art numerical weather prediction model from the European Centre
for Medium-Range Weather Forecasts. We report successive forecast improvements
resulting from model design and data-related decisions, such as switching from
the cubed sphere to the HEALPix mesh, inverting the channel depth of the U-Net,
and introducing gated recurrent units (GRU) on each level of the U-Net
hierarchy. The consistent east-west orientation of all cells on the HEALPix
mesh facilitates the development of location-invariant convolution kernels that
are successfully applied to propagate global weather patterns across our
planet. Without any loss of spectral power after two days, the model can be
unrolled autoregressively for hundreds of steps into the future to generate
stable and realistic states of the atmosphere that respect seasonal trends, as
showcased in one-year simulations. Our parsimonious DLWP-HPX model is
research-friendly and potentially well-suited for sub-seasonal and seasonal
forecasting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06255">Privacy-Engineered Value Decomposition Networks for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2311.06255v1 [cs.MA])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gohari_P/0/1/0/all/0/1">Parham Gohari</a>, <a href="http://arxiv.org/find/cs/1/au:+Hale_M/0/1/0/all/0/1">Matthew Hale</a>, <a href="http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1">Ufuk Topcu</a></p>
<p>In cooperative multi-agent reinforcement learning (Co-MARL), a team of agents
must jointly optimize the team's long-term rewards to learn a designated task.
Optimizing rewards as a team often requires inter-agent communication and data
sharing, leading to potential privacy implications. We assume privacy
considerations prohibit the agents from sharing their environment interaction
data. Accordingly, we propose Privacy-Engineered Value Decomposition Networks
(PE-VDN), a Co-MARL algorithm that models multi-agent coordination while
provably safeguarding the confidentiality of the agents' environment
interaction data. We integrate three privacy-engineering techniques to redesign
the data flows of the VDN algorithm, an existing Co-MARL algorithm that
consolidates the agents' environment interaction data to train a central
controller that models multi-agent coordination, and develop PE-VDN. In the
first technique, we design a distributed computation scheme that eliminates
Vanilla VDN's dependency on sharing environment interaction data. Then, we
utilize a privacy-preserving multi-party computation protocol to guarantee that
the data flows of the distributed computation scheme do not pose new privacy
risks. Finally, we enforce differential privacy to preempt inference threats
against the agents' training data, past environment interactions, when they
take actions based on their neural network predictions. We implement PE-VDN in
StarCraft Multi-Agent Competition (SMAC) and show that it achieves 80% of
Vanilla VDN's win rate while maintaining differential privacy levels that
provide meaningful privacy guarantees. The results demonstrate that PE-VDN can
safeguard the confidentiality of agents' environment interaction data without
sacrificing multi-agent coordination.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06275">Algorithmic Robustness. (arXiv:2311.06275v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jensen_D/0/1/0/all/0/1">David Jensen</a>, <a href="http://arxiv.org/find/cs/1/au:+LaMacchia_B/0/1/0/all/0/1">Brian LaMacchia</a>, <a href="http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1">Ufuk Topcu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wisniewski_P/0/1/0/all/0/1">Pamela Wisniewski</a></p>
<p>Algorithmic robustness refers to the sustained performance of a computational
system in the face of change in the nature of the environment in which that
system operates or in the task that the system is meant to perform. Below, we
motivate the importance of algorithmic robustness, present a conceptual
framework, and highlight the relevant areas of research for which algorithmic
robustness is relevant. Why robustness? Robustness is an important enabler of
other goals that are frequently cited in the context of public policy decisions
about computational systems, including trustworthiness, accountability,
fairness, and safety. Despite this dependence, it tends to be under-recognized
compared to these other concepts. This is unfortunate, because robustness is
often more immediately achievable than these other ultimate goals, which can be
more subjective and exacting. Thus, we highlight robustness as an important
goal for researchers, engineers, regulators, and policymakers when considering
the design, implementation, and deployment of computational systems. We urge
researchers and practitioners to elevate the attention paid to robustness when
designing and evaluating computational systems. For many key systems, the
immediate question after any demonstration of high performance should be: "How
robust is that performance to realistic changes in the task or environment?"
Greater robustness will set the stage for systems that are more trustworthy,
accountable, fair, and safe. Toward that end, this document provides a brief
roadmap to some of the concepts and existing research around the idea of
algorithmic robustness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06278">Boosting Stock Price Prediction with Anticipated Macro Policy Changes. (arXiv:2311.06278v1 [q-fin.ST])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+Haque_M/0/1/0/all/0/1">Md Sabbirul Haque</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Amin_M/0/1/0/all/0/1">Md Shahedul Amin</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Miah_J/0/1/0/all/0/1">Jonayet Miah</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Cao_D/0/1/0/all/0/1">Duc Minh Cao</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Ahmed_A/0/1/0/all/0/1">Ashiqul Haque Ahmed</a></p>
<p>Prediction of stock prices plays a significant role in aiding the
decision-making of investors. Considering its importance, a growing literature
has emerged trying to forecast stock prices with improved accuracy. In this
study, we introduce an innovative approach for forecasting stock prices with
greater accuracy. We incorporate external economic environment-related
information along with stock prices. In our novel approach, we improve the
performance of stock price prediction by taking into account variations due to
future expected macroeconomic policy changes as investors adjust their current
behavior ahead of time based on expected future macroeconomic policy changes.
Furthermore, we incorporate macroeconomic variables along with historical stock
prices to make predictions. Results from this strongly support the inclusion of
future economic policy changes along with current macroeconomic information. We
confirm the supremacy of our method over the conventional approach using
several tree-based machine-learning algorithms. Results are strongly conclusive
across various machine learning models. Our preferred model outperforms the
conventional approach with an RMSE value of 1.61 compared to an RMSE value of
1.75 from the conventional approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06280">A Data-driven Deep Learning Approach for Bitcoin Price Forecasting. (arXiv:2311.06280v1 [q-fin.ST])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+Modi_P/0/1/0/all/0/1">Parth Daxesh Modi</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Arshi_K/0/1/0/all/0/1">Kamyar Arshi</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Kunz_P/0/1/0/all/0/1">Pertami J. Kunz</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Zoubir_A/0/1/0/all/0/1">Abdelhak M. Zoubir</a></p>
<p>Bitcoin as a cryptocurrency has been one of the most important digital coins
and the first decentralized digital currency. Deep neural networks, on the
other hand, has shown promising results recently; however, we require huge
amount of high-quality data to leverage their power. There are some techniques
such as augmentation that can help us with increasing the dataset size, but we
cannot exploit them on historical bitcoin data. As a result, we propose a
shallow Bidirectional-LSTM (Bi-LSTM) model, fed with feature engineered data
using our proposed method to forecast bitcoin closing prices in a daily time
frame. We compare the performance with that of other forecasting methods, and
show that with the help of the proposed feature engineering method, a shallow
deep neural network outperforms other popular price forecasting models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06281">Parallelization of an Ubiquitous Sequential Computation. (arXiv:2311.06281v1 [cs.DS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Heinsen_F/0/1/0/all/0/1">Franz A. Heinsen</a></p>
<p>We show how to compute the elements of a sequence $x_t = a_t x_{t-1} + b_t$
in parallel, given $t = (1, 2, \dots, n)$, $a_t \in \mathbb{R}^n$, $b_t \in
\mathbb{R}^n$, and initial value $x_0 \in \mathbb{R}$. On $n$ parallel
processors, the computation of $n$ elements incurs $\mathcal{O}(\log n)$ time
and $\mathcal{O}(n)$ space. Sequences of this form are ubiquitous in science
and engineering, making their parallelization useful for a vast number of
applications. We implement parallelization in software, test it on parallel
hardware, and verify that it executes faster than sequential computation by a
factor of $\frac{n}{\log n}$.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06285">Sounding Bodies: Modeling 3D Spatial Sound of Humans Using Body Pose and Audio. (arXiv:2311.06285v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xudong Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Markovic_D/0/1/0/all/0/1">Dejan Markovic</a>, <a href="http://arxiv.org/find/cs/1/au:+Sandakly_J/0/1/0/all/0/1">Jacob Sandakly</a>, <a href="http://arxiv.org/find/cs/1/au:+Keebler_T/0/1/0/all/0/1">Todd Keebler</a>, <a href="http://arxiv.org/find/cs/1/au:+Krenn_S/0/1/0/all/0/1">Steven Krenn</a>, <a href="http://arxiv.org/find/cs/1/au:+Richard_A/0/1/0/all/0/1">Alexander Richard</a></p>
<p>While 3D human body modeling has received much attention in computer vision,
modeling the acoustic equivalent, i.e. modeling 3D spatial audio produced by
body motion and speech, has fallen short in the community. To close this gap,
we present a model that can generate accurate 3D spatial audio for full human
bodies. The system consumes, as input, audio signals from headset microphones
and body pose, and produces, as output, a 3D sound field surrounding the
transmitter's body, from which spatial audio can be rendered at any arbitrary
position in the 3D space. We collect a first-of-its-kind multimodal dataset of
human bodies, recorded with multiple cameras and a spherical array of 345
microphones. In an empirical evaluation, we demonstrate that our model can
produce accurate body-induced sound fields when trained with a suitable loss.
Dataset and code are available online.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06292">Towards a data-driven debt collection strategy based on an advanced machine learning framework. (arXiv:2311.06292v1 [q-fin.ST])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+Sancarlos_A/0/1/0/all/0/1">Abel Sancarlos</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Bahilo_E/0/1/0/all/0/1">Edgar Bahilo</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Mozo_P/0/1/0/all/0/1">Pablo Mozo</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Norman_L/0/1/0/all/0/1">Lukas Norman</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Rehma_O/0/1/0/all/0/1">Obaid Ur Rehma</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Anufrijevs_M/0/1/0/all/0/1">Mihails Anufrijevs</a></p>
<p>The European debt purchase market as measured by the total book value of
purchased debt approached 25bn euros in 2020 and it was growing at double-digit
rates. This is an example of how big the debt collection and debt purchase
industry has grown and the important impact it has in the financial sector.
However, in order to ensure an adequate return during the debt collection
process, a good estimation of the propensity to pay and/or the expected
cashflow is crucial. These estimations can be employed, for instance, to create
different strategies during the amicable collection to maximize quality
standards and revenues. And not only that, but also to prioritize the cases in
which a legal process is necessary when debtors are unreachable for an amicable
negotiation. This work offers a solution for these estimations. Specifically, a
new machine learning modelling pipeline is presented showing how outperforms
current strategies employed in the sector. The solution contains a
pre-processing pipeline and a model selector based on the best model
calibration. Performance is validated with real historical data of the debt
industry.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06293">Quantum Neural Networks for Power Flow Analysis. (arXiv:2311.06293v1 [quant-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Kaseb_Z/0/1/0/all/0/1">Zeynab Kaseb</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Moller_M/0/1/0/all/0/1">Matthias Moller</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Balducci_G/0/1/0/all/0/1">Giorgio Tosti Balducci</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Palensky_P/0/1/0/all/0/1">Peter Palensky</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Vergara_P/0/1/0/all/0/1">Pedro P. Vergara</a></p>
<p>This paper explores the potential application of quantum and hybrid
quantum-classical neural networks in power flow analysis. Experiments are
conducted using two small-size datasets based on the IEEE 4-bus and 33-bus test
systems. A systematic performance comparison is also conducted among quantum,
hybrid quantum-classical, and classical neural networks. The comparison is
based on (i) generalization ability, (ii) robustness, (iii) training dataset
size needed, (iv) training error. (v) training computational time, and (vi)
training process stability. The results show that the developed
quantum-classical neural network outperforms both quantum and classical neural
networks, and hence can improve deep learning-based power flow analysis in the
noisy-intermediate-scale quantum (NISQ) era.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06295">Gradual Optimization Learning for Conformational Energy Minimization. (arXiv:2311.06295v1 [physics.chem-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Tsypin_A/0/1/0/all/0/1">Artem Tsypin</a>, <a href="http://arxiv.org/find/physics/1/au:+Ugadiarov_L/0/1/0/all/0/1">Leonid Ugadiarov</a>, <a href="http://arxiv.org/find/physics/1/au:+Khrabrov_K/0/1/0/all/0/1">Kuzma Khrabrov</a>, <a href="http://arxiv.org/find/physics/1/au:+Avetisian_M/0/1/0/all/0/1">Manvel Avetisian</a>, <a href="http://arxiv.org/find/physics/1/au:+Telepov_A/0/1/0/all/0/1">Alexander Telepov</a>, <a href="http://arxiv.org/find/physics/1/au:+Rumiantsev_E/0/1/0/all/0/1">Egor Rumiantsev</a>, <a href="http://arxiv.org/find/physics/1/au:+Skrynnik_A/0/1/0/all/0/1">Alexey Skrynnik</a>, <a href="http://arxiv.org/find/physics/1/au:+Panov_A/0/1/0/all/0/1">Aleksandr I. Panov</a>, <a href="http://arxiv.org/find/physics/1/au:+Vetrov_D/0/1/0/all/0/1">Dmitry Vetrov</a>, <a href="http://arxiv.org/find/physics/1/au:+Tutubalina_E/0/1/0/all/0/1">Elena Tutubalina</a>, <a href="http://arxiv.org/find/physics/1/au:+Kadurin_A/0/1/0/all/0/1">Artur Kadurin</a></p>
<p>Molecular conformation optimization is crucial to computer-aided drug
discovery and materials design. Traditional energy minimization techniques rely
on iterative optimization methods that use molecular forces calculated by a
physical simulator (oracle) as anti-gradients. However, this is a
computationally expensive approach that requires many interactions with a
physical simulator. One way to accelerate this procedure is to replace the
physical simulator with a neural network. Despite recent progress in neural
networks for molecular conformation energy prediction, such models are prone to
distribution shift, leading to inaccurate energy minimization. We find that the
quality of energy minimization with neural networks can be improved by
providing optimization trajectories as additional training data. Still, it
takes around $5 \times 10^5$ additional conformations to match the physical
simulator's optimization quality. In this work, we present the Gradual
Optimization Learning Framework (GOLF) for energy minimization with neural
networks that significantly reduces the required additional data. The framework
consists of an efficient data-collecting scheme and an external optimizer. The
external optimizer utilizes gradients from the energy prediction model to
generate optimization trajectories, and the data-collecting scheme selects
additional training data to be processed by the physical simulator. Our results
demonstrate that the neural network trained with GOLF performs on par with the
oracle on a benchmark of diverse drug-like molecules using $50$x less
additional data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06297">STRIDE: Structure-guided Generation for Inverse Design of Molecules. (arXiv:2311.06297v1 [physics.chem-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Zaman_S/0/1/0/all/0/1">Shehtab Zaman</a>, <a href="http://arxiv.org/find/physics/1/au:+Akhiyarov_D/0/1/0/all/0/1">Denis Akhiyarov</a>, <a href="http://arxiv.org/find/physics/1/au:+Araya_Polo_M/0/1/0/all/0/1">Mauricio Araya-Polo</a>, <a href="http://arxiv.org/find/physics/1/au:+Chiu_K/0/1/0/all/0/1">Kenneth Chiu</a></p>
<p>Machine learning and especially deep learning has had an increasing impact on
molecule and materials design. In particular, given the growing access to an
abundance of high-quality small molecule data for generative modeling for drug
design, results for drug discovery have been promising. However, for many
important classes of materials such as catalysts, antioxidants, and
metal-organic frameworks, such large datasets are not available. Such families
of molecules with limited samples and structural similarities are especially
prevalent for industrial applications. As is well-known, retraining and even
fine-tuning are challenging on such small datasets. Novel, practically
applicable molecules are most often derivatives of well-known molecules,
suggesting approaches to addressing data scarcity. To address this problem, we
introduce $\textbf{STRIDE}$, a generative molecule workflow that generates
novel molecules with an unconditional generative model guided by known
molecules without any retraining. We generate molecules outside of the training
data from a highly specialized set of antioxidant molecules. Our generated
molecules have on average 21.7% lower synthetic accessibility scores and also
reduce ionization potential by 5.9% of generated molecules via guiding.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06304">Retro-BLEU: Quantifying Chemical Plausibility of Retrosynthesis Routes through Reaction Template Sequence Analysis. (arXiv:2311.06304v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Junren Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1">Lei Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1">Jian-Guang Lou</a></p>
<p>Computer-assisted methods have emerged as valuable tools for retrosynthesis
analysis. However, quantifying the plausibility of generated retrosynthesis
routes remains a challenging task. We introduce Retro-BLEU, a statistical
metric adapted from the well-established BLEU score in machine translation, to
evaluate the plausibility of retrosynthesis routes based on reaction template
sequences analysis. We demonstrate the effectiveness of Retro-BLEU by applying
it to a diverse set of retrosynthesis routes generated by state-of-the-art
algorithms and compare the performance with other evaluation metrics. The
results show that Retro-BLEU is capable of differentiating between plausible
and implausible routes. Furthermore, we provide insights into the strengths and
weaknesses of Retro-BLEU, paving the way for future developments and
improvements in this field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06311">Game Theory Solutions in Sensor-Based Human Activity Recognition: A Review. (arXiv:2311.06311v1 [cs.GT])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shayesteh_M/0/1/0/all/0/1">Mohammad Hossein Shayesteh</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharokhzadeh_B/0/1/0/all/0/1">Behrooz Sharokhzadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Masoumi_B/0/1/0/all/0/1">Behrooz Masoumi</a></p>
<p>The Human Activity Recognition (HAR) tasks automatically identify human
activities using the sensor data, which has numerous applications in
healthcare, sports, security, and human-computer interaction. Despite
significant advances in HAR, critical challenges still exist. Game theory has
emerged as a promising solution to address these challenges in machine learning
problems including HAR. However, there is a lack of research work on applying
game theory solutions to the HAR problems. This review paper explores the
potential of game theory as a solution for HAR tasks, and bridges the gap
between game theory and HAR research work by suggesting novel game-theoretic
approaches for HAR problems. The contributions of this work include exploring
how game theory can improve the accuracy and robustness of HAR models,
investigating how game-theoretic concepts can optimize recognition algorithms,
and discussing the game-theoretic approaches against the existing HAR methods.
The objective is to provide insights into the potential of game theory as a
solution for sensor-based HAR, and contribute to develop a more accurate and
efficient recognition system in the future research directions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06315">ShipGen: A Diffusion Model for Parametric Ship Hull Generation with Multiple Objectives and Constraints. (arXiv:2311.06315v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bagazinski_N/0/1/0/all/0/1">Noah J. Bagazinski</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_F/0/1/0/all/0/1">Faez Ahmed</a></p>
<p>Ship design is a years-long process that requires balancing complex design
trade-offs to create a ship that is efficient and effective. Finding new ways
to improve the ship design process can lead to significant cost savings for
ship building and operation. One promising technology is generative artificial
intelligence, which has been shown to reduce design cycle time and create
novel, high-performing designs. In literature review, generative artificial
intelligence has been shown to generate ship hulls; however, ship design is
particularly difficult as the hull of a ship requires the consideration of many
objectives. This paper presents a study on the generation of parametric ship
hull designs using a parametric diffusion model that considers multiple
objectives and constraints for the hulls. This denoising diffusion
probabilistic model (DDPM) generates the tabular parametric design vectors of a
ship hull for evaluation. In addition to a tabular DDPM, this paper details
adding guidance to improve the quality of generated ship hull designs. By
leveraging classifier guidance, the DDPM produced feasible parametric ship
hulls that maintain the coverage of the initial training dataset of ship hulls
with a 99.5% rate, a 149x improvement over random sampling of the design vector
parameters across the design space. Parametric ship hulls produced with
performance guidance saw an average of 91.4% reduction in wave drag
coefficients and an average of a 47.9x relative increase in the total displaced
volume of the hulls compared to the mean performance of the hulls in the
training dataset. The use of a DDPM to generate parametric ship hulls can
reduce design time by generating high-performing hull designs for future
analysis. These generated hulls have low drag and high volume, which can reduce
the cost of operating a ship and increase its potential to generate revenue.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06318">Knowledge-Augmented Large Language Models for Personalized Contextual Query Suggestion. (arXiv:2311.06318v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Baek_J/0/1/0/all/0/1">Jinheon Baek</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandrasekaran_N/0/1/0/all/0/1">Nirupama Chandrasekaran</a>, <a href="http://arxiv.org/find/cs/1/au:+Cucerzan_S/0/1/0/all/0/1">Silviu Cucerzan</a>, <a href="http://arxiv.org/find/cs/1/au:+herring_A/0/1/0/all/0/1">Allen herring</a>, <a href="http://arxiv.org/find/cs/1/au:+Jauhar_S/0/1/0/all/0/1">Sujay Kumar Jauhar</a></p>
<p>Large Language Models (LLMs) excel at tackling various natural language
tasks. However, due to the significant costs involved in re-training or
fine-tuning them, they remain largely static and difficult to personalize.
Nevertheless, a variety of applications could benefit from generations that are
tailored to users' preferences, goals, and knowledge. Among them is web search,
where knowing what a user is trying to accomplish, what they care about, and
what they know can lead to improved search experiences. In this work, we
propose a novel and general approach that augments an LLM with relevant context
from users' interaction histories with a search engine in order to personalize
its outputs. Specifically, we construct an entity-centric knowledge store for
each user based on their search and browsing activities on the web, which is
then leveraged to provide contextually relevant LLM prompt augmentations. This
knowledge store is light-weight, since it only produces user-specific aggregate
projections of interests and knowledge onto public knowledge graphs, and
leverages existing search log infrastructure, thereby mitigating the privacy,
compliance, and scalability concerns associated with building deep user
profiles for personalization. We then validate our approach on the task of
contextual query suggestion, which requires understanding not only the user's
current search context but also what they historically know and care about.
Through a number of experiments based on human evaluation, we show that our
approach is significantly better than several other LLM-powered baselines,
generating query suggestions that are contextually more relevant, personalized,
and useful.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06321">Can Machine Learning Uncover Insights into Vehicle Travel Demand from Our Built Environment?. (arXiv:2311.06321v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zixun Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1">Hao Zheng</a></p>
<p>In this paper, we propose a machine learning-based approach to address the
lack of ability for designers to optimize urban land use planning from the
perspective of vehicle travel demand. Research shows that our computational
model can help designers quickly obtain feedback on the vehicle travel demand,
which includes its total amount and temporal distribution based on the urban
function distribution designed by the designers. It also assists in design
optimization and evaluation of the urban function distribution from the
perspective of vehicle travel. We obtain the city function distribution
information and vehicle hours traveled (VHT) information by collecting the city
point-of-interest (POI) data and online vehicle data. The artificial neural
networks (ANNs) with the best performance in prediction are selected. By using
data sets collected in different regions for mutual prediction and remapping
the predictions onto a map for visualization, we evaluate the extent to which
the computational model sees use across regions in an attempt to reduce the
workload of future urban researchers. Finally, we demonstrate the application
of the computational model to help designers obtain feedback on vehicle travel
demand in the built environment and combine it with genetic algorithms to
optimize the current state of the urban environment to provide recommendations
to designers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06322">Post-training Quantization with Progressive Calibration and Activation Relaxing for Text-to-Image Diffusion Models. (arXiv:2311.06322v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1">Siao Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1">Chaoyu Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zewen Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1">Yansong Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1">Wenwu Zhu</a></p>
<p>Diffusion models have achieved great success due to their remarkable
generation ability. However, their high computational overhead is still a
troublesome problem. Recent studies have leveraged post-training quantization
(PTQ) to compress diffusion models. However, most of them only focus on
unconditional models, leaving the quantization of widely used large pretrained
text-to-image models, e.g., Stable Diffusion, largely unexplored. In this
paper, we propose a novel post-training quantization method PCR (Progressive
Calibration and Relaxing) for text-to-image diffusion models, which consists of
a progressive calibration strategy that considers the accumulated quantization
error across timesteps, and an activation relaxing strategy that improves the
performance with negligible cost. Additionally, we demonstrate the previous
metrics for text-to-image diffusion model quantization are not accurate due to
the distribution gap. To tackle the problem, we propose a novel QDiffBench
benchmark, which utilizes data in the same domain for more accurate evaluation.
Besides, QDiffBench also considers the generalization performance of the
quantized model outside the calibration dataset. Extensive experiments on
Stable Diffusion and Stable Diffusion XL demonstrate the superiority of our
method and benchmark. Moreover, we are the first to achieve quantization for
Stable Diffusion XL while maintaining the performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06323">Reviewing Developments of Graph Convolutional Network Techniques for Recommendation Systems. (arXiv:2311.06323v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1">Haojun Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kapoor_V/0/1/0/all/0/1">Vikram Kapoor</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1">Priya Sharma</a></p>
<p>The Recommender system is a vital information service on today's Internet.
Recently, graph neural networks have emerged as the leading approach for
recommender systems. We try to review recent literature on graph neural
network-based recommender systems, covering the background and development of
both recommender systems and graph neural networks. Then categorizing
recommender systems by their settings and graph neural networks by spectral and
spatial models, we explore the motivation behind incorporating graph neural
networks into recommender systems. We also analyze challenges and open problems
in graph construction, embedding propagation and aggregation, and computation
efficiency. This guides us to better explore the future directions and
developments in this domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06329">A Survey of AI Text-to-Image and AI Text-to-Video Generators. (arXiv:2311.06329v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1">Aditi Singh</a></p>
<p>Text-to-Image and Text-to-Video AI generation models are revolutionary
technologies that use deep learning and natural language processing (NLP)
techniques to create images and videos from textual descriptions. This paper
investigates cutting-edge approaches in the discipline of Text-to-Image and
Text-to-Video AI generations. The survey provides an overview of the existing
literature as well as an analysis of the approaches used in various studies. It
covers data preprocessing techniques, neural network types, and evaluation
metrics used in the field. In addition, the paper discusses the challenges and
limitations of Text-to-Image and Text-to-Video AI generations, as well as
future research directions. Overall, these models have promising potential for
a wide range of applications such as video production, content creation, and
digital marketing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06358">Compact Matrix Quantum Group Equivariant Neural Networks. (arXiv:2311.06358v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pearce_Crump_E/0/1/0/all/0/1">Edward Pearce-Crump</a></p>
<p>We derive the existence of a new type of neural network, called a compact
matrix quantum group equivariant neural network, that learns from data that has
an underlying quantum symmetry. We apply the Woronowicz formulation of
Tannaka-Krein duality to characterise the weight matrices that appear in these
neural networks for any easy compact matrix quantum group. We show that compact
matrix quantum group equivariant neural networks contain, as a subclass, all
compact matrix group equivariant neural networks. Moreover, we obtain
characterisations of the weight matrices for many compact matrix group
equivariant neural networks that have not previously appeared in the machine
learning literature.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06361">CALLOC: Curriculum Adversarial Learning for Secure and Robust Indoor Localization. (arXiv:2311.06361v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gufran_D/0/1/0/all/0/1">Danish Gufran</a>, <a href="http://arxiv.org/find/cs/1/au:+Pasricha_S/0/1/0/all/0/1">Sudeep Pasricha</a></p>
<p>Indoor localization has become increasingly vital for many applications from
tracking assets to delivering personalized services. Yet, achieving pinpoint
accuracy remains a challenge due to variations across indoor environments and
devices used to assist with localization. Another emerging challenge is
adversarial attacks on indoor localization systems that not only threaten
service integrity but also reduce localization accuracy. To combat these
challenges, we introduce CALLOC, a novel framework designed to resist
adversarial attacks and variations across indoor environments and devices that
reduce system accuracy and reliability. CALLOC employs a novel adaptive
curriculum learning approach with a domain specific lightweight scaled-dot
product attention neural network, tailored for adversarial and variation
resilience in practical use cases with resource constrained mobile devices.
Experimental evaluations demonstrate that CALLOC can achieve improvements of up
to 6.03x in mean error and 4.6x in worst-case error against state-of-the-art
indoor localization frameworks, across diverse building floorplans, mobile
devices, and adversarial attacks scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06368">The AeroSonicDB (YPAD-0523) Dataset for Acoustic Detection and Classification of Aircraft. (arXiv:2311.06368v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Downward_B/0/1/0/all/0/1">Blake Downward</a>, <a href="http://arxiv.org/find/cs/1/au:+Nordby_J/0/1/0/all/0/1">Jon Nordby</a></p>
<p>The time and expense required to collect and label audio data has been a
prohibitive factor in the availability of domain specific audio datasets. As
the predictive specificity of a classifier depends on the specificity of the
labels it is trained on, it follows that finely-labelled datasets are crucial
for advances in machine learning. Aiming to stimulate progress in the field of
machine listening, this paper introduces AeroSonicDB (YPAD-0523), a dataset of
low-flying aircraft sounds for training acoustic detection and classification
systems. This paper describes the method of exploiting ADS-B radio
transmissions to passively collect and label audio samples. Provides a summary
of the collated dataset. Presents baseline results from three binary
classification models, then discusses the limitations of the current dataset
and its future potential. The dataset contains 625 aircraft recordings ranging
in event duration from 18 to 60 seconds, for a total of 8.87 hours of aircraft
audio. These 625 samples feature 301 unique aircraft, each of which are
supplied with 14 supplementary (non-acoustic) labels to describe the aircraft.
The dataset also contains 3.52 hours of ambient background audio ("silence"),
as a means to distinguish aircraft noise from other local environmental noises.
Additionally, 6 hours of urban soundscape recordings (with aircraft
annotations) are included as an ancillary method for evaluating model
performance, and to provide a testing ground for real-time applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06372">Blockchain-Enabled Federated Learning Approach for Vehicular Networks. (arXiv:2311.06372v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sultana_S/0/1/0/all/0/1">Shirin Sultana</a>, <a href="http://arxiv.org/find/cs/1/au:+Hossain_J/0/1/0/all/0/1">Jahin Hossain</a>, <a href="http://arxiv.org/find/cs/1/au:+Billah_M/0/1/0/all/0/1">Maruf Billah</a>, <a href="http://arxiv.org/find/cs/1/au:+Shajeeb_H/0/1/0/all/0/1">Hasibul Hossain Shajeeb</a>, <a href="http://arxiv.org/find/cs/1/au:+Rahman_S/0/1/0/all/0/1">Saifur Rahman</a>, <a href="http://arxiv.org/find/cs/1/au:+Ansari_K/0/1/0/all/0/1">Keyvan Ansari</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasan_K/0/1/0/all/0/1">Khondokar Fida Hasan</a></p>
<p>Data from interconnected vehicles may contain sensitive information such as
location, driving behavior, personal identifiers, etc. Without adequate
safeguards, sharing this data jeopardizes data privacy and system security. The
current centralized data-sharing paradigm in these systems raises particular
concerns about data privacy. Recognizing these challenges, the shift towards
decentralized interactions in technology, as echoed by the principles of
Industry 5.0, becomes paramount. This work is closely aligned with these
principles, emphasizing decentralized, human-centric, and secure technological
interactions in an interconnected vehicular ecosystem. To embody this, we
propose a practical approach that merges two emerging technologies: Federated
Learning (FL) and Blockchain. The integration of these technologies enables the
creation of a decentralized vehicular network. In this setting, vehicles can
learn from each other without compromising privacy while also ensuring data
integrity and accountability. Initial experiments show that compared to
conventional decentralized federated learning techniques, our proposed approach
significantly enhances the performance and security of vehicular networks. The
system's accuracy stands at 91.92\%. While this may appear to be low in
comparison to state-of-the-art federated learning models, our work is
noteworthy because, unlike others, it was achieved in a malicious vehicle
setting. Despite the challenging environment, our method maintains high
accuracy, making it a competent solution for preserving data privacy in
vehicular networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06374">Higher-Order Newton Methods with Polynomial Work per Iteration. (arXiv:2311.06374v1 [math.OC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Ahmadi_A/0/1/0/all/0/1">Amir Ali Ahmadi</a>, <a href="http://arxiv.org/find/math/1/au:+Chaudhry_A/0/1/0/all/0/1">Abraar Chaudhry</a>, <a href="http://arxiv.org/find/math/1/au:+Zhang_J/0/1/0/all/0/1">Jeffrey Zhang</a></p>
<p>We present generalizations of Newton's method that incorporate derivatives of
an arbitrary order $d$ but maintain a polynomial dependence on dimension in
their cost per iteration. At each step, our $d^{\text{th}}$-order method uses
semidefinite programming to construct and minimize a sum of squares-convex
approximation to the $d^{\text{th}}$-order Taylor expansion of the function we
wish to minimize. We prove that our $d^{\text{th}}$-order method has local
convergence of order $d$. This results in lower oracle complexity compared to
the classical Newton method. We show on numerical examples that basins of
attraction around local minima can get larger as $d$ increases. Under
additional assumptions, we present a modified algorithm, again with polynomial
cost per iteration, which is globally convergent and has local convergence of
order $d$.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06375">Image Classification using Combination of Topological Features and Neural Networks. (arXiv:2311.06375v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lima_M/0/1/0/all/0/1">Mariana D&#xf3;ria Prata Lima</a>, <a href="http://arxiv.org/find/cs/1/au:+Giraldi_G/0/1/0/all/0/1">Gilson Antonio Giraldi</a>, <a href="http://arxiv.org/find/cs/1/au:+Junior_G/0/1/0/all/0/1">Gast&#xe3;o Flor&#xea;ncio Miranda Junior</a></p>
<p>In this work we use the persistent homology method, a technique in
topological data analysis (TDA), to extract essential topological features from
the data space and combine them with deep learning features for classification
tasks. In TDA, the concepts of complexes and filtration are building blocks.
Firstly, a filtration is constructed from some complex. Then, persistent
homology classes are computed, and their evolution along the filtration is
visualized through the persistence diagram. Additionally, we applied
vectorization techniques to the persistence diagram to make this topological
information compatible with machine learning algorithms. This was carried out
with the aim of classifying images from multiple classes in the MNIST dataset.
Our approach inserts topological features into deep learning approaches
composed by single and two-streams neural networks architectures based on a
multi-layer perceptron (MLP) and a convolutional neral network (CNN) taylored
for multi-class classification in the MNIST dataset. In our analysis, we
evaluated the obtained results and compared them with the outcomes achieved
through the baselines that are available in the TensorFlow library. The main
conclusion is that topological information may increase neural network accuracy
in multi-class classification tasks with the price of computational complexity
of persistent homology calculation. Up to the best of our knowledge, it is the
first work that combines deep learning features and the combination of
topological features for multi-class classification tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06380">Theory and implementation of inelastic Constitutive Artificial Neural Networks. (arXiv:2311.06380v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Holthusen_H/0/1/0/all/0/1">Hagen Holthusen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lamm_L/0/1/0/all/0/1">Lukas Lamm</a>, <a href="http://arxiv.org/find/cs/1/au:+Brepols_T/0/1/0/all/0/1">Tim Brepols</a>, <a href="http://arxiv.org/find/cs/1/au:+Reese_S/0/1/0/all/0/1">Stefanie Reese</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuhl_E/0/1/0/all/0/1">Ellen Kuhl</a></p>
<p>Nature has always been our inspiration in the research, design and
development of materials and has driven us to gain a deep understanding of the
mechanisms that characterize anisotropy and inelastic behavior. All this
knowledge has been accumulated in the principles of thermodynamics. Deduced
from these principles, the multiplicative decomposition combined with pseudo
potentials are powerful and universal concepts. Simultaneously, the tremendous
increase in computational performance enabled us to investigate and rethink our
history-dependent material models to make the most of our predictions. Today,
we have reached a point where materials and their models are becoming
increasingly sophisticated. This raises the question: How do we find the best
model that includes all inelastic effects to explain our complex data?
Constitutive Artificial Neural Networks (CANN) may answer this question. Here,
we extend the CANNs to inelastic materials (iCANN). Rigorous considerations of
objectivity, rigid motion of the reference configuration, multiplicative
decomposition and its inherent non-uniqueness, restrictions of energy and
pseudo potential, and consistent evolution guide us towards the architecture of
the iCANN satisfying thermodynamics per design. We combine feed-forward
networks of the free energy and pseudo potential with a recurrent neural
network approach to take time dependencies into account. We demonstrate that
the iCANN is capable of autonomously discovering models for artificially
generated data, the response of polymers for cyclic loading and the relaxation
behavior of muscle data. As the design of the network is not limited to
visco-elasticity, our vision is that the iCANN will reveal to us new ways to
find the various inelastic phenomena hidden in the data and to understand their
interaction. Our source code, data, and examples are available at
doi.org/10.5281/zenodo.10066805
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06382">Transfer Learning for Structured Pruning under Limited Task Data. (arXiv:2311.06382v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dery_L/0/1/0/all/0/1">Lucio Dery</a>, <a href="http://arxiv.org/find/cs/1/au:+Grangier_D/0/1/0/all/0/1">David Grangier</a>, <a href="http://arxiv.org/find/cs/1/au:+Hannun_A/0/1/0/all/0/1">Awni Hannun</a></p>
<p>Large, pre-trained models are problematic to use in resource constrained
applications. Fortunately, task-aware structured pruning methods offer a
solution. These approaches reduce model size by dropping structural units like
layers and attention heads in a manner that takes into account the end-task.
However, these pruning algorithms require more task-specific data than is
typically available. We propose a framework which combines structured pruning
with transfer learning to reduce the need for task-specific data. Our empirical
results answer questions such as: How should the two tasks be coupled? What
parameters should be transferred? And, when during training should transfer
learning be introduced? Leveraging these insights, we demonstrate that our
framework results in pruned models with improved generalization over strong
baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06383">Distilling Large Language Models using Skill-Occupation Graph Context for HR-Related Tasks. (arXiv:2311.06383v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pezeshkpour_P/0/1/0/all/0/1">Pouya Pezeshkpour</a>, <a href="http://arxiv.org/find/cs/1/au:+Iso_H/0/1/0/all/0/1">Hayate Iso</a>, <a href="http://arxiv.org/find/cs/1/au:+Lake_T/0/1/0/all/0/1">Thom Lake</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhutani_N/0/1/0/all/0/1">Nikita Bhutani</a>, <a href="http://arxiv.org/find/cs/1/au:+Hruschka_E/0/1/0/all/0/1">Estevam Hruschka</a></p>
<p>Numerous HR applications are centered around resumes and job descriptions.
While they can benefit from advancements in NLP, particularly large language
models, their real-world adoption faces challenges due to absence of
comprehensive benchmarks for various HR tasks, and lack of smaller models with
competitive capabilities. In this paper, we aim to bridge this gap by
introducing the Resume-Job Description Benchmark (RJDB). We meticulously craft
this benchmark to cater to a wide array of HR tasks, including matching and
explaining resumes to job descriptions, extracting skills and experiences from
resumes, and editing resumes. To create this benchmark, we propose to distill
domain-specific knowledge from a large language model (LLM). We rely on a
curated skill-occupation graph to ensure diversity and provide context for LLMs
generation. Our benchmark includes over 50 thousand triples of job
descriptions, matched resumes and unmatched resumes. Using RJDB, we train
multiple smaller student models. Our experiments reveal that the student models
achieve near/better performance than the teacher model (GPT-4), affirming the
effectiveness of the benchmark. Additionally, we explore the utility of RJDB on
out-of-distribution data for skill extraction and resume-job description
matching, in zero-shot and weak supervision manner. We release our datasets and
code to foster further research and industry applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06386">Towards A Unified Neural Architecture for Visual Recognition and Reasoning. (arXiv:2311.06386v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1">Calvin Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1">Boqing Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Ting Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1">Chen Sun</a></p>
<p>Recognition and reasoning are two pillars of visual understanding. However,
these tasks have an imbalance in focus; whereas recent advances in neural
networks have shown strong empirical performance in visual recognition, there
has been comparably much less success in solving visual reasoning. Intuitively,
unifying these two tasks under a singular framework is desirable, as they are
mutually dependent and beneficial. Motivated by the recent success of
multi-task transformers for visual recognition and language understanding, we
propose a unified neural architecture for visual recognition and reasoning with
a generic interface (e.g., tokens) for both. Our framework enables the
principled investigation of how different visual recognition tasks, datasets,
and inductive biases can help enable spatiotemporal reasoning capabilities.
Noticeably, we find that object detection, which requires spatial localization
of individual objects, is the most beneficial recognition task for reasoning.
We further demonstrate via probing that implicit object-centric representations
emerge automatically inside our framework. Intriguingly, we discover that
certain architectural choices such as the backbone model of the visual encoder
have a significant impact on visual reasoning, but little on object detection.
Given the results of our experiments, we believe that visual reasoning should
be considered as a first-class citizen alongside visual recognition, as they
are strongly correlated but benefit from potentially different design choices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06395">A statistical perspective on algorithm unrolling models for inverse problems. (arXiv:2311.06395v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Atchade_Y/0/1/0/all/0/1">Yves Atchade</a>, <a href="http://arxiv.org/find/stat/1/au:+Liu_X/0/1/0/all/0/1">Xinru Liu</a>, <a href="http://arxiv.org/find/stat/1/au:+Zhu_Q/0/1/0/all/0/1">Qiuyun Zhu</a></p>
<p>We consider inverse problems where the conditional distribution of the
observation ${\bf y}$ given the latent variable of interest ${\bf x}$ (also
known as the forward model) is known, and we have access to a data set in which
multiple instances of ${\bf x}$ and ${\bf y}$ are both observed. In this
context, algorithm unrolling has become a very popular approach for designing
state-of-the-art deep neural network architectures that effectively exploit the
forward model. We analyze the statistical complexity of the gradient descent
network (GDN), an algorithm unrolling architecture driven by proximal gradient
descent. We show that the unrolling depth needed for the optimal statistical
performance of GDNs is of order $\log(n)/\log(\varrho_n^{-1})$, where $n$ is
the sample size, and $\varrho_n$ is the convergence rate of the corresponding
gradient descent algorithm. We also show that when the negative log-density of
the latent variable ${\bf x}$ has a simple proximal operator, then a GDN
unrolled at depth $D'$ can solve the inverse problem at the parametric rate
$O(D'/\sqrt{n})$. Our results thus also suggest that algorithm unrolling models
are prone to overfitting as the unrolling depth $D'$ increases. We provide
several examples to illustrate these results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06396">A comprehensive analysis of concept drift locality in data streams. (arXiv:2311.06396v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aguiar_G/0/1/0/all/0/1">Gabriel J. Aguiar</a>, <a href="http://arxiv.org/find/cs/1/au:+Cano_A/0/1/0/all/0/1">Alberto Cano</a></p>
<p>Adapting to drifting data streams is a significant challenge in online
learning. Concept drift must be detected for effective model adaptation to
evolving data properties. Concept drift can impact the data distribution
entirely or partially, which makes it difficult for drift detectors to
accurately identify the concept drift. Despite the numerous concept drift
detectors in the literature, standardized procedures and benchmarks for
comprehensive evaluation considering the locality of the drift are lacking. We
present a novel categorization of concept drift based on its locality and
scale. A systematic approach leads to a set of 2,760 benchmark problems,
reflecting various difficulty levels following our proposed categorization. We
conduct a comparative assessment of 9 state-of-the-art drift detectors across
diverse difficulties, highlighting their strengths and weaknesses for future
research. We examine how drift locality influences the classifier performance
and propose strategies for different drift categories to minimize the recovery
time. Lastly, we provide lessons learned and recommendations for future concept
drift research. Our benchmark data streams and experiments are publicly
available at https://github.com/gabrieljaguiar/locality-concept-drift.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06413">Forte: An Interactive Visual Analytic Tool for Trust-Augmented Net Load Forecasting. (arXiv:2311.06413v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bhattacharjee_K/0/1/0/all/0/1">Kaustav Bhattacharjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kundu_S/0/1/0/all/0/1">Soumya Kundu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chakraborty_I/0/1/0/all/0/1">Indrasis Chakraborty</a>, <a href="http://arxiv.org/find/cs/1/au:+Dasgupta_A/0/1/0/all/0/1">Aritra Dasgupta</a></p>
<p>Accurate net load forecasting is vital for energy planning, aiding decisions
on trade and load distribution. However, assessing the performance of
forecasting models across diverse input variables, like temperature and
humidity, remains challenging, particularly for eliciting a high degree of
trust in the model outcomes. In this context, there is a growing need for
data-driven technological interventions to aid scientists in comprehending how
models react to both noisy and clean input variables, thus shedding light on
complex behaviors and fostering confidence in the outcomes. In this paper, we
present Forte, a visual analytics-based application to explore deep
probabilistic net load forecasting models across various input variables and
understand the error rates for different scenarios. With carefully designed
visual interventions, this web-based interface empowers scientists to derive
insights about model performance by simulating diverse scenarios, facilitating
an informed decision-making process. We discuss observations made using Forte
and demonstrate the effectiveness of visualization techniques to provide
valuable insights into the correlation between weather inputs and net load
forecasts, ultimately advancing grid capabilities by improving trust in
forecasting models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06414">Knowledge Graphs are not Created Equal: Exploring the Properties and Structure of Real KGs. (arXiv:2311.06414v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Teneva_N/0/1/0/all/0/1">Nedelina Teneva</a>, <a href="http://arxiv.org/find/cs/1/au:+Hruschka_E/0/1/0/all/0/1">Estevam Hruschka</a></p>
<p>Despite the recent popularity of knowledge graph (KG) related tasks and
benchmarks such as KG embeddings, link prediction, entity alignment and
evaluation of the reasoning abilities of pretrained language models as KGs, the
structure and properties of real KGs are not well studied. In this paper, we
perform a large scale comparative study of 29 real KG datasets from diverse
domains such as the natural sciences, medicine, and NLP to analyze their
properties and structural patterns. Based on our findings, we make several
recommendations regarding KG-based model development and evaluation. We believe
that the rich structural information contained in KGs can benefit the
development of better KG models across fields and we hope this study will
contribute to breaking the existing data silos between different areas of
research (e.g., ML, NLP, AI for sciences).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06423">Flatness-aware Adversarial Attack. (arXiv:2311.06423v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1">Mingyuan Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaodan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Cen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yinggui Wang</a></p>
<p>The transferability of adversarial examples can be exploited to launch
black-box attacks. However, adversarial examples often present poor
transferability. To alleviate this issue, by observing that the diversity of
inputs can boost transferability, input regularization based methods are
proposed, which craft adversarial examples by combining several transformed
inputs. We reveal that input regularization based methods make resultant
adversarial examples biased towards flat extreme regions. Inspired by this, we
propose an attack called flatness-aware adversarial attack (FAA) which
explicitly adds a flatness-aware regularization term in the optimization target
to promote the resultant adversarial examples towards flat extreme regions. The
flatness-aware regularization term involves gradients of samples around the
resultant adversarial examples but optimizing gradients requires the evaluation
of Hessian matrix in high-dimension spaces which generally is intractable. To
address the problem, we derive an approximate solution to circumvent the
construction of Hessian matrix, thereby making FAA practical and cheap.
Extensive experiments show the transferability of adversarial examples crafted
by FAA can be considerably boosted compared with state-of-the-art baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06427">ChatGPT Prompting Cannot Estimate Predictive Uncertainty in High-Resource Languages. (arXiv:2311.06427v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pelucchi_M/0/1/0/all/0/1">Martino Pelucchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Valdenegro_Toro_M/0/1/0/all/0/1">Matias Valdenegro-Toro</a></p>
<p>ChatGPT took the world by storm for its impressive abilities. Due to its
release without documentation, scientists immediately attempted to identify its
limits, mainly through its performance in natural language processing (NLP)
tasks. This paper aims to join the growing literature regarding ChatGPT's
abilities by focusing on its performance in high-resource languages and on its
capacity to predict its answers' accuracy by giving a confidence level. The
analysis of high-resource languages is of interest as studies have shown that
low-resource languages perform worse than English in NLP tasks, but no study so
far has analysed whether high-resource languages perform as well as English.
The analysis of ChatGPT's confidence calibration has not been carried out
before either and is critical to learn about ChatGPT's trustworthiness. In
order to study these two aspects, five high-resource languages and two NLP
tasks were chosen. ChatGPT was asked to perform both tasks in the five
languages and to give a numerical confidence value for each answer. The results
show that all the selected high-resource languages perform similarly and that
ChatGPT does not have a good confidence calibration, often being overconfident
and never giving low confidence values.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06428">A Trichotomy for Transductive Online Learning. (arXiv:2311.06428v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hanneke_S/0/1/0/all/0/1">Steve Hanneke</a>, <a href="http://arxiv.org/find/cs/1/au:+Moran_S/0/1/0/all/0/1">Shay Moran</a>, <a href="http://arxiv.org/find/cs/1/au:+Shafer_J/0/1/0/all/0/1">Jonathan Shafer</a></p>
<p>We present new upper and lower bounds on the number of learner mistakes in
the `transductive' online learning setting of Ben-David, Kushilevitz and
Mansour (1997). This setting is similar to standard online learning, except
that the adversary fixes a sequence of instances $x_1,\dots,x_n$ to be labeled
at the start of the game, and this sequence is known to the learner.
Qualitatively, we prove a trichotomy, stating that the minimal number of
mistakes made by the learner as $n$ grows can take only one of precisely three
possible values: $n$, $\Theta\left(\log (n)\right)$, or $\Theta(1)$.
Furthermore, this behavior is determined by a combination of the VC dimension
and the Littlestone dimension. Quantitatively, we show a variety of bounds
relating the number of mistakes to well-known combinatorial dimensions. In
particular, we improve the known lower bound on the constant in the $\Theta(1)$
case from $\Omega\left(\sqrt{\log(d)}\right)$ to $\Omega(\log(d))$ where $d$ is
the Littlestone dimension. Finally, we extend our results to cover multiclass
classification and the agnostic setting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06440">Separating the Wheat from the Chaff with BREAD: An open-source benchmark and metrics to detect redundancy in text. (arXiv:2311.06440v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Caswell_I/0/1/0/all/0/1">Isaac Caswell</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lisa Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Papadimitriou_I/0/1/0/all/0/1">Isabel Papadimitriou</a></p>
<p>Data quality is a problem that perpetually resurfaces throughout the field of
NLP, regardless of task, domain, or architecture, and remains especially severe
for lower-resource languages. A typical and insidious issue, affecting both
training data and model output, is data that is repetitive and dominated by
linguistically uninteresting boilerplate, such as price catalogs or
computer-generated log files. Though this problem permeates many web-scraped
corpora, there has yet to be a benchmark to test against, or a systematic study
to find simple metrics that generalize across languages and agree with human
judgements of data quality. In the present work, we create and release BREAD, a
human-labeled benchmark on repetitive boilerplate vs. plausible linguistic
content, spanning 360 languages. We release several baseline CRED (Character
REDundancy) scores along with it, and evaluate their effectiveness on BREAD. We
hope that the community will use this resource to develop better filtering
methods, and that our reference implementations of CRED scores can become
standard corpus evaluation tools, driving the development of cleaner language
modeling corpora, especially in low-resource languages.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06444">Mitigating Pooling Bias in E-commerce Search via False Negative Estimation. (arXiv:2311.06444v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaochen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1">Xiao Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Ruhan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Na_T/0/1/0/all/0/1">Taesik Na</a>, <a href="http://arxiv.org/find/cs/1/au:+Tenneti_T/0/1/0/all/0/1">Tejaswi Tenneti</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haixun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1">Fenglong Ma</a></p>
<p>Efficient and accurate product relevance assessment is critical for user
experiences and business success. Training a proficient relevance assessment
model requires high-quality query-product pairs, often obtained through
negative sampling strategies. Unfortunately, current methods introduce pooling
bias by mistakenly sampling false negatives, diminishing performance and
business impact. To address this, we present Bias-mitigating Hard Negative
Sampling (BHNS), a novel negative sampling strategy tailored to identify and
adjust for false negatives, building upon our original False Negative
Estimation algorithm. Our experiments in the Instacart search setting confirm
BHNS as effective for practical e-commerce use. Furthermore, comparative
analyses on public dataset showcase its domain-agnostic potential for diverse
applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06454">A Saliency-based Clustering Framework for Identifying Aberrant Predictions. (arXiv:2311.06454v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Montserrat_A/0/1/0/all/0/1">Aina Tersol Montserrat</a>, <a href="http://arxiv.org/find/cs/1/au:+Loftus_A/0/1/0/all/0/1">Alexander R. Loftus</a>, <a href="http://arxiv.org/find/cs/1/au:+Daihes_Y/0/1/0/all/0/1">Yael Daihes</a></p>
<p>In machine learning, classification tasks serve as the cornerstone of a wide
range of real-world applications. Reliable, trustworthy classification is
particularly intricate in biomedical settings, where the ground truth is often
inherently uncertain and relies on high degrees of human expertise for
labeling. Traditional metrics such as precision and recall, while valuable, are
insufficient for capturing the nuances of these ambiguous scenarios. Here we
introduce the concept of aberrant predictions, emphasizing that the nature of
classification errors is as critical as their frequency. We propose a novel,
efficient training methodology aimed at both reducing the misclassification
rate and discerning aberrant predictions. Our framework demonstrates a
substantial improvement in model performance, achieving a 20\% increase in
precision. We apply this methodology to the less-explored domain of veterinary
radiology, where the stakes are high but have not been as extensively studied
compared to human medicine. By focusing on the identification and mitigation of
aberrant predictions, we enhance the utility and trustworthiness of machine
learning classifiers in high-stakes, real-world scenarios, including new
applications in the veterinary world.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06456">Asymmetric Contrastive Multimodal Learning for Advancing Chemical Understanding. (arXiv:2311.06456v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yifei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yunrui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_P/0/1/0/all/0/1">Pengyu Hong</a></p>
<p>The versatility of multimodal deep learning holds tremendous promise for
advancing scientific research and practical applications. As this field
continues to evolve, the collective power of cross-modal analysis promises to
drive transformative innovations, leading us to new frontiers in chemical
understanding and discovery. Hence, we introduce Asymmetric Contrastive
M}ultimodal Learning (ACML) as a novel approach tailored for molecules,
showcasing its potential to advance the field of chemistry. ACML harnesses the
power of effective asymmetric contrastive learning to seamlessly transfer
information from various chemical modalities to molecular graph
representations. By combining pre-trained chemical unimodal encoders and a
shallow-designed graph encoder, ACML facilitates the assimilation of
coordinated chemical semantics from different modalities, leading to
comprehensive representation learning with efficient training. This innovative
framework enhances the interpretability of learned representations and bolsters
the expressive power of graph neural networks. Through practical tasks such as
isomer discrimination and uncovering crucial chemical properties for drug
discovery, ACML exhibits its capability to revolutionize chemical research and
applications, providing a deeper understanding of chemical semantics of
different modalities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06460">Online Continual Learning via Logit Adjusted Softmax. (arXiv:2311.06460v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zhehao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1">Tao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1">Chenhe Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yingwen Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xiaolin Huang</a></p>
<p>Online continual learning is a challenging problem where models must learn
from a non-stationary data stream while avoiding catastrophic forgetting.
Inter-class imbalance during training has been identified as a major cause of
forgetting, leading to model prediction bias towards recently learned classes.
In this paper, we theoretically analyze that inter-class imbalance is entirely
attributed to imbalanced class-priors, and the function learned from
intra-class intrinsic distributions is the Bayes-optimal classifier. To that
end, we present that a simple adjustment of model logits during training can
effectively resist prior class bias and pursue the corresponding Bayes-optimum.
Our proposed method, Logit Adjusted Softmax, can mitigate the impact of
inter-class imbalance not only in class-incremental but also in realistic
general setups, with little additional computational cost. We evaluate our
approach on various benchmarks and demonstrate significant performance
improvements compared to prior arts. For example, our approach improves the
best baseline by 4.6% on CIFAR10.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06480">Adversarial Fine-tuning using Generated Respiratory Sound to Address Class Imbalance. (arXiv:2311.06480v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">June-Woo Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_C/0/1/0/all/0/1">Chihyeon Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Toikkanen_M/0/1/0/all/0/1">Miika Toikkanen</a>, <a href="http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1">Sangmin Bae</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1">Ho-Young Jung</a></p>
<p>Deep generative models have emerged as a promising approach in the medical
image domain to address data scarcity. However, their use for sequential data
like respiratory sounds is less explored. In this work, we propose a
straightforward approach to augment imbalanced respiratory sound data using an
audio diffusion model as a conditional neural vocoder. We also demonstrate a
simple yet effective adversarial fine-tuning method to align features between
the synthetic and real respiratory sound samples to improve respiratory sound
classification performance. Our experimental results on the ICBHI dataset
demonstrate that the proposed adversarial fine-tuning is effective, while only
using the conventional augmentation method shows performance degradation.
Moreover, our method outperforms the baseline by 2.24% on the ICBHI Score and
improves the accuracy of the minority classes up to 26.58%. For the
supplementary material, we provide the code at
https://github.com/kaen2891/adversarial_fine-tuning_using_generated_respiratory_sound.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06481">Topology-Matching Normalizing Flows for Out-of-Distribution Detection in Robot Learning. (arXiv:2311.06481v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1">Jianxiang Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jongseok Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Geisler_S/0/1/0/all/0/1">Simon Geisler</a>, <a href="http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1">Stephan Gunnemann</a>, <a href="http://arxiv.org/find/cs/1/au:+Triebel_R/0/1/0/all/0/1">Rudolph Triebel</a></p>
<p>To facilitate reliable deployments of autonomous robots in the real world,
Out-of-Distribution (OOD) detection capabilities are often required. A powerful
approach for OOD detection is based on density estimation with Normalizing
Flows (NFs). However, we find that prior work with NFs attempts to match the
complex target distribution topologically with naive base distributions leading
to adverse implications. In this work, we circumvent this topological mismatch
using an expressive class-conditional base distribution trained with an
information-theoretic objective to match the required topology. The proposed
method enjoys the merits of wide compatibility with existing learned models
without any performance degradation and minimum computation overhead while
enhancing OOD detection capabilities. We demonstrate superior results in
density estimation and 2D object detection benchmarks in comparison with
extensive baselines. Moreover, we showcase the applicability of the method with
a real-robot deployment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06483">Stacked networks improve physics-informed training: applications to neural networks and deep operator networks. (arXiv:2311.06483v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Howard_A/0/1/0/all/0/1">Amanda A Howard</a>, <a href="http://arxiv.org/find/cs/1/au:+Murphy_S/0/1/0/all/0/1">Sarah H Murphy</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1">Shady E Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Stinis_P/0/1/0/all/0/1">Panos Stinis</a></p>
<p>Physics-informed neural networks and operator networks have shown promise for
effectively solving equations modeling physical systems. However, these
networks can be difficult or impossible to train accurately for some systems of
equations. We present a novel multifidelity framework for stacking
physics-informed neural networks and operator networks that facilitates
training. We successively build a chain of networks, where the output at one
step can act as a low-fidelity input for training the next step, gradually
increasing the expressivity of the learned model. The equations imposed at each
step of the iterative process can be the same or different (akin to simulated
annealing). The iterative (stacking) nature of the proposed method allows us to
progressively learn features of a solution that are hard to learn directly.
Through benchmark problems including a nonlinear pendulum, the wave equation,
and the viscous Burgers equation, we show how stacking can be used to improve
the accuracy and reduce the required size of physics-informed neural networks
and operator networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06505">CompCodeVet: A Compiler-guided Validation and Enhancement Approach for Code Dataset. (arXiv:2311.06505v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Le Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhattacharjee_A/0/1/0/all/0/1">Arijit Bhattacharjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_N/0/1/0/all/0/1">Nesreen K. Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasabnis_N/0/1/0/all/0/1">Niranjan Hasabnis</a>, <a href="http://arxiv.org/find/cs/1/au:+Oren_G/0/1/0/all/0/1">Gal Oren</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_B/0/1/0/all/0/1">Bin Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Jannesari_A/0/1/0/all/0/1">Ali Jannesari</a></p>
<p>Large language models (LLMs) have become increasingly prominent in academia
and industry due to their remarkable performance in diverse applications. As
these models evolve with increasing parameters, they excel in tasks like
sentiment analysis and machine translation. However, even models with billions
of parameters face challenges in tasks demanding multi-step reasoning. Code
generation and comprehension, especially in C and C++, emerge as significant
challenges. While LLMs trained on code datasets demonstrate competence in many
tasks, they struggle with rectifying non-compilable C and C++ code. Our
investigation attributes this subpar performance to two primary factors: the
quality of the training dataset and the inherent complexity of the problem
which demands intricate reasoning. Existing "Chain of Thought" (CoT) prompting
techniques aim to enhance multi-step reasoning. This approach, however, retains
the limitations associated with the latent drawbacks of LLMs. In this work, we
propose CompCodeVet, a compiler-guided CoT approach to produce compilable code
from non-compilable ones. Diverging from the conventional approach of utilizing
larger LLMs, we employ compilers as a teacher to establish a more robust
zero-shot thought process. The evaluation of CompCodeVet on two open-source
code datasets shows that CompCodeVet has the ability to improve the training
dataset quality for LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06517">BClean: A Bayesian Data Cleaning System. (arXiv:2311.06517v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1">Jianbin Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Sifan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yaoshu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jing Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yifan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Miao_Y/0/1/0/all/0/1">Yukai Miao</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_R/0/1/0/all/0/1">Rui Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Onizuka_M/0/1/0/all/0/1">Makoto Onizuka</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Chuan Xiao</a></p>
<p>There is a considerable body of work on data cleaning which employs various
principles to rectify erroneous data and transform a dirty dataset into a
cleaner one. One of prevalent approaches is probabilistic methods, including
Bayesian methods. However, existing probabilistic methods often assume a
simplistic distribution (e.g., Gaussian distribution), which is frequently
underfitted in practice, or they necessitate experts to provide a complex prior
distribution (e.g., via a programming language). This requirement is both
labor-intensive and costly, rendering these methods less suitable for
real-world applications. In this paper, we propose BClean, a Bayesian Cleaning
system that features automatic Bayesian network construction and user
interaction. We recast the data cleaning problem as a Bayesian inference that
fully exploits the relationships between attributes in the observed dataset and
any prior information provided by users. To this end, we present an automatic
Bayesian network construction method that extends a structure learning-based
functional dependency discovery method with similarity functions to capture the
relationships between attributes. Furthermore, our system allows users to
modify the generated Bayesian network in order to specify prior information or
correct inaccuracies identified by the automatic generation process. We also
design an effective scoring model (called the compensative scoring model)
necessary for the Bayesian inference. To enhance the efficiency of data
cleaning, we propose several approximation strategies for the Bayesian
inference, including graph partitioning, domain pruning, and pre-detection. By
evaluating on both real-world and synthetic datasets, we demonstrate that
BClean is capable of achieving an F-measure of up to 0.9 in data cleaning,
outperforming existing Bayesian methods by 2% and other data cleaning methods
by 15%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06518">Minimum Description Length Hopfield Networks. (arXiv:2311.06518v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abudy_M/0/1/0/all/0/1">Matan Abudy</a>, <a href="http://arxiv.org/find/cs/1/au:+Lan_N/0/1/0/all/0/1">Nur Lan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chemla_E/0/1/0/all/0/1">Emmanuel Chemla</a>, <a href="http://arxiv.org/find/cs/1/au:+Katzir_R/0/1/0/all/0/1">Roni Katzir</a></p>
<p>Associative memory architectures are designed for memorization but also
offer, through their retrieval method, a form of generalization to unseen
inputs: stored memories can be seen as prototypes from this point of view.
Focusing on Modern Hopfield Networks (MHN), we show that a large memorization
capacity undermines the generalization opportunity. We offer a solution to
better optimize this tradeoff. It relies on Minimum Description Length (MDL) to
determine during training which memories to store, as well as how many of them.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06527">TURBO: The Swiss Knife of Auto-Encoders. (arXiv:2311.06527v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Quetant_G/0/1/0/all/0/1">Guillaume Qu&#xe9;tant</a>, <a href="http://arxiv.org/find/cs/1/au:+Belousov_Y/0/1/0/all/0/1">Yury Belousov</a>, <a href="http://arxiv.org/find/cs/1/au:+Kinakh_V/0/1/0/all/0/1">Vitaliy Kinakh</a>, <a href="http://arxiv.org/find/cs/1/au:+Voloshynovskiy_S/0/1/0/all/0/1">Slava Voloshynovskiy</a></p>
<p>We present a novel information-theoretic framework, termed as TURBO, designed
to systematically analyse and generalise auto-encoding methods. We start by
examining the principles of information bottleneck and bottleneck-based
networks in the auto-encoding setting and identifying their inherent
limitations, which become more prominent for data with multiple relevant,
physics-related representations. The TURBO framework is then introduced,
providing a comprehensive derivation of its core concept consisting of the
maximisation of mutual information between various data representations
expressed in two directions reflecting the information flows. We illustrate
that numerous prevalent neural network models are encompassed within this
framework. The paper underscores the insufficiency of the information
bottleneck concept in elucidating all such models, thereby establishing TURBO
as a preferable theoretical reference. The introduction of TURBO contributes to
a richer understanding of data representation and the structure of neural
network models, enabling more efficient and versatile applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06545">Understanding Generalization via Set Theory. (arXiv:2311.06545v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shiqi Liu</a></p>
<p>Generalization is at the core of machine learning models. However, the
definition of generalization is not entirely clear. We employ set theory to
introduce the concepts of algorithms, hypotheses, and dataset generalization.
We analyze the properties of dataset generalization and prove a theorem on
surrogate generalization procedures. This theorem leads to our generalization
method. Through a generalization experiment on the MNIST dataset, we obtain
13,541 sample bases. When we use the entire training set to evaluate the
model's performance, the models achieve an accuracy of 99.945%. However, if we
shift the sample bases or modify the neural network structure, the performance
experiences a significant decline. We also identify consistently mispredicted
samples and find that they are all challenging examples. The experiments
substantiated the accuracy of the generalization definition and the
effectiveness of the proposed methods. Both the set-theoretic deduction and the
experiments help us better understand generalization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06547">From Charts to Atlas: Merging Latent Spaces into One. (arXiv:2311.06547v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Crisostomi_D/0/1/0/all/0/1">Donato Crisostomi</a>, <a href="http://arxiv.org/find/cs/1/au:+Cannistraci_I/0/1/0/all/0/1">Irene Cannistraci</a>, <a href="http://arxiv.org/find/cs/1/au:+Moschella_L/0/1/0/all/0/1">Luca Moschella</a>, <a href="http://arxiv.org/find/cs/1/au:+Barbiero_P/0/1/0/all/0/1">Pietro Barbiero</a>, <a href="http://arxiv.org/find/cs/1/au:+Ciccone_M/0/1/0/all/0/1">Marco Ciccone</a>, <a href="http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1">Pietro Li&#xf2;</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodola_E/0/1/0/all/0/1">Emanuele Rodol&#xe0;</a></p>
<p>Models trained on semantically related datasets and tasks exhibit comparable
inter-sample relations within their latent spaces. We investigate in this study
the aggregation of such latent spaces to create a unified space encompassing
the combined information. To this end, we introduce Relative Latent Space
Aggregation, a two-step approach that first renders the spaces comparable using
relative representations, and then aggregates them via a simple mean. We
carefully divide a classification problem into a series of learning tasks under
three different settings: sharing samples, classes, or neither. We then train a
model on each task and aggregate the resulting latent spaces. We compare the
aggregated space with that derived from an end-to-end model trained over all
tasks and show that the two spaces are similar. We then observe that the
aggregated space is better suited for classification, and empirically
demonstrate that it is due to the unique imprints left by task-specific
embedders within the representations. We finally test our framework in
scenarios where no shared region exists and show that it can still be used to
merge the spaces, albeit with diminished benefits over naive merging.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06552">Stain Consistency Learning: Handling Stain Variation for Automatic Digital Pathology Segmentation. (arXiv:2311.06552v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Yeung_M/0/1/0/all/0/1">Michael Yeung</a>, <a href="http://arxiv.org/find/eess/1/au:+Watts_T/0/1/0/all/0/1">Todd Watts</a>, <a href="http://arxiv.org/find/eess/1/au:+Tan_S/0/1/0/all/0/1">Sean YW Tan</a>, <a href="http://arxiv.org/find/eess/1/au:+Ferreira_P/0/1/0/all/0/1">Pedro F. Ferreira</a>, <a href="http://arxiv.org/find/eess/1/au:+Scott_A/0/1/0/all/0/1">Andrew D. Scott</a>, <a href="http://arxiv.org/find/eess/1/au:+Nielles_Vallespin_S/0/1/0/all/0/1">Sonia Nielles-Vallespin</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1">Guang Yang</a></p>
<p>Stain variation is a unique challenge associated with automated analysis of
digital pathology. Numerous methods have been developed to improve the
robustness of machine learning methods to stain variation, but comparative
studies have demonstrated limited benefits to performance. Moreover, methods to
handle stain variation were largely developed for H&amp;E stained data, with
evaluation generally limited to classification tasks. Here we propose Stain
Consistency Learning, a novel framework combining stain-specific augmentation
with a stain consistency loss function to learn stain colour invariant
features. We perform the first, extensive comparison of methods to handle stain
variation for segmentation tasks, comparing ten methods on Masson's trichrome
and H&amp;E stained cell and nuclei datasets, respectively. We observed that stain
normalisation methods resulted in equivalent or worse performance, while stain
augmentation or stain adversarial methods demonstrated improved performance,
with the best performance consistently achieved by our proposed approach. The
code is available at: https://github.com/mlyg/stain_consistency_learning
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06554">Graph ODE with Factorized Prototypes for Modeling Complicated Interacting Dynamics. (arXiv:2311.06554v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1">Xiao Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1">Yiyang Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1">Huiyu Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jinsheng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ju_W/0/1/0/all/0/1">Wei Ju</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Ming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yizhou Sun</a></p>
<p>This paper studies the problem of modeling interacting dynamical systems,
which is critical for understanding physical dynamics and biological processes.
Recent research predominantly uses geometric graphs to represent these
interactions, which are then captured by powerful graph neural networks (GNNs).
However, predicting interacting dynamics in challenging scenarios such as
out-of-distribution shift and complicated underlying rules remains unsolved. In
this paper, we propose a new approach named Graph ODE with factorized
prototypes (GOAT) to address the problem. The core of GOAT is to incorporate
factorized prototypes from contextual knowledge into a continuous graph ODE
framework. Specifically, GOAT employs representation disentanglement and system
parameters to extract both object-level and system-level contexts from
historical trajectories, which allows us to explicitly model their independent
influence and thus enhances the generalization capability under system changes.
Then, we integrate these disentangled latent representations into a graph ODE
model, which determines a combination of various interacting prototypes for
enhanced model expressivity. The entire model is optimized using an end-to-end
variational inference framework to maximize the likelihood. Extensive
experiments in both in-distribution and out-of-distribution settings validate
the superiority of GOAT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06558">Convolve and Conquer: Data Comparison with Wiener Filters. (arXiv:2311.06558v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cruz_D/0/1/0/all/0/1">Deborah Pelacani Cruz</a>, <a href="http://arxiv.org/find/cs/1/au:+Strong_G/0/1/0/all/0/1">George Strong</a>, <a href="http://arxiv.org/find/cs/1/au:+Bates_O/0/1/0/all/0/1">Oscar Bates</a>, <a href="http://arxiv.org/find/cs/1/au:+Cueto_C/0/1/0/all/0/1">Carlos Cueto</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1">Jiashun Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Guasch_L/0/1/0/all/0/1">Lluis Guasch</a></p>
<p>Quantitative evaluations of differences and/or similarities between data
samples define and shape optimisation problems associated with learning data
distributions. Current methods to compare data often suffer from limitations in
capturing such distributions or lack desirable mathematical properties for
optimisation (e.g. smoothness, differentiability, or convexity). In this paper,
we introduce a new method to measure (dis)similarities between paired samples
inspired by Wiener-filter theory. The convolutional nature of Wiener filters
allows us to comprehensively compare data samples in a globally correlated way.
We validate our approach in four machine learning applications: data
compression, medical imaging imputation, translated classification, and
non-parametric generative modelling. Our results demonstrate increased
resolution in reconstructed images with better perceptual quality and higher
data fidelity, as well as robustness against translations, compared to
conventional mean-squared-error analogue implementations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06567">SCADI: Self-supervised Causal Disentanglement in Latent Variable Models. (arXiv:2311.06567v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nam_H/0/1/0/all/0/1">Heejeong Nam</a></p>
<p>Causal disentanglement has great potential for capturing complex situations.
However, there is a lack of practical and efficient approaches. It is already
known that most unsupervised disentangling methods are unable to produce
identifiable results without additional information, often leading to randomly
disentangled output. Therefore, most existing models for disentangling are
weakly supervised, providing information about intrinsic factors, which incurs
excessive costs. Therefore, we propose a novel model, SCADI(SElf-supervised
CAusal DIsentanglement), that enables the model to discover semantic factors
and learn their causal relationships without any supervision. This model
combines a masked structural causal model (SCM) with a pseudo-label generator
for causal disentanglement, aiming to provide a new direction for
self-supervised causal disentanglement models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06575">Sparse Attention-Based Neural Networks for Code Classification. (arXiv:2311.06575v1 [cs.PL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiang_Z/0/1/0/all/0/1">Ziyang Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zaixi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qi Liu</a></p>
<p>Categorizing source codes accurately and efficiently is a challenging problem
in real-world programming education platform management. In recent years,
model-based approaches utilizing abstract syntax trees (ASTs) have been widely
applied to code classification tasks. We introduce an approach named the Sparse
Attention-based neural network for Code Classification (SACC) in this paper.
The approach involves two main steps: In the first step, source code undergoes
syntax parsing and preprocessing. The generated abstract syntax tree is split
into sequences of subtrees and then encoded using a recursive neural network to
obtain a high-dimensional representation. This step simultaneously considers
both the logical structure and lexical level information contained within the
code. In the second step, the encoded sequences of subtrees are fed into a
Transformer model that incorporates sparse attention mechanisms for the purpose
of classification. This method efficiently reduces the computational cost of
the self-attention mechanisms, thus improving the training speed while
preserving effectiveness. Our work introduces a carefully designed sparse
attention pattern that is specifically designed to meet the unique needs of
code classification tasks. This design helps reduce the influence of redundant
information and enhances the overall performance of the model. Finally, we also
deal with problems in previous related research, which include issues like
incomplete classification labels and a small dataset size. We annotated the
CodeNet dataset with algorithm-related labeling categories, which contains a
significantly large amount of data. Extensive comparative experimental results
demonstrate the effectiveness and efficiency of SACC for the code
classification tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06597">Understanding Grokking Through A Robustness Viewpoint. (arXiv:2311.06597v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1">Zhiquan Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1">Weiran Huang</a></p>
<p>Recently, an unusual phenomenon called grokking has gained much attention,
where sometimes a neural network generalizes long after it perfectly fits the
training data. We try to understand this seemingly strange phenomenon using the
robustness of the neural network. Using a robustness viewpoint, we show that
the popular $l_2$ weight norm (metric) of the neural network is actually a
sufficient condition for grokking. As we also empirically find that $l_2$ norm
correlates with grokking on the test data not in a timely way, we propose new
metrics based on robustness and information theory and find that our new
metrics correlate well with the grokking phenomenon. Based on the previous
observations, we propose methods to speed up the generalization process. In
addition, we examine the standard training process on modulo addition dataset
and find that it hardly learns other basic group operations before grokking,
including the commutative law. Interestingly, the speed up of generalization
when using our proposed method can be partially explained by learning the
commutative law, a necessary condition when the model groks on test dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06613">Computer Vision for Particle Size Analysis of Coarse-Grained Soils. (arXiv:2311.06613v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Youwai_S/0/1/0/all/0/1">Sompote Youwai</a>, <a href="http://arxiv.org/find/cs/1/au:+Makam_P/0/1/0/all/0/1">Parchya Makam</a></p>
<p>Particle size analysis (PSA) is a fundamental technique for evaluating the
physical characteristics of soils. However, traditional methods like sieving
can be time-consuming and labor-intensive. In this study, we present a novel
approach that utilizes computer vision (CV) and the Python programming language
for PSA of coarse-grained soils, employing a standard mobile phone camera. By
eliminating the need for a high-performance camera, our method offers
convenience and cost savings. Our methodology involves using the OPENCV library
to detect and measure soil particles in digital photographs taken under
ordinary lighting conditions. For accurate particle size determination, a
calibration target with known dimensions is placed on a plain paper alongside
20 different sand samples. The proposed method is compared with traditional
sieve analysis and exhibits satisfactory performance for soil particles larger
than 2 mm, with a mean absolute percent error (MAPE) of approximately 6%.
However, particles smaller than 2 mm result in higher MAPE, reaching up to 60%.
To address this limitation, we recommend using a higher-resolution camera to
capture images of the smaller soil particles. Furthermore, we discuss the
advantages, limitations, and potential future improvements of our method.
Remarkably, the program can be executed on a mobile phone, providing immediate
results without the need to send soil samples to a laboratory. This
field-friendly feature makes our approach highly convenient for on-site usage,
outside of a traditional laboratory setting. Ultimately, this novel method
represents an initial disruption to the industry, enabling efficient particle
size analysis of soil without the reliance on laboratory-based sieve analysis.
KEYWORDS: Computer vision, Grain size, ARUCO
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06625">Streamlining Energy Transition Scenarios to Key Policy Decisions. (arXiv:2311.06625v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Baader_F/0/1/0/all/0/1">Florian Joseph Baader</a>, <a href="http://arxiv.org/find/cs/1/au:+Moret_S/0/1/0/all/0/1">Stefano Moret</a>, <a href="http://arxiv.org/find/cs/1/au:+Wiesemann_W/0/1/0/all/0/1">Wolfram Wiesemann</a>, <a href="http://arxiv.org/find/cs/1/au:+Staffell_I/0/1/0/all/0/1">Iain Staffell</a>, <a href="http://arxiv.org/find/cs/1/au:+Bardow_A/0/1/0/all/0/1">Andr&#xe9; Bardow</a></p>
<p>Uncertainties surrounding the energy transition often lead modelers to
present large sets of scenarios that are challenging for policymakers to
interpret and act upon. An alternative approach is to define a few qualitative
storylines from stakeholder discussions, which can be affected by biases and
infeasibilities. Leveraging decision trees, a popular machine-learning
technique, we derive interpretable storylines from many quantitative scenarios
and show how the key decisions in the energy transition are interlinked.
Specifically, our results demonstrate that choosing a high deployment of
renewables and sector coupling makes global decarbonization scenarios robust
against uncertainties in climate sensitivity and demand. Also, the energy
transition to a fossil-free Europe is primarily determined by choices on the
roles of bioenergy, storage, and heat electrification. Our transferrable
approach translates vast energy model results into a small set of critical
decisions, guiding decision-makers in prioritizing the key factors that will
shape the energy transition.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06631">A 3D Conditional Diffusion Model for Image Quality Transfer -- An Application to Low-Field MRI. (arXiv:2311.06631v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1">Seunghoi Kim</a>, <a href="http://arxiv.org/find/eess/1/au:+Tregidgo_H/0/1/0/all/0/1">Henry F. J. Tregidgo</a>, <a href="http://arxiv.org/find/eess/1/au:+Eldaly_A/0/1/0/all/0/1">Ahmed K. Eldaly</a>, <a href="http://arxiv.org/find/eess/1/au:+Figini_M/0/1/0/all/0/1">Matteo Figini</a>, <a href="http://arxiv.org/find/eess/1/au:+Alexander_D/0/1/0/all/0/1">Daniel C. Alexander</a></p>
<p>Low-field (LF) MRI scanners (&lt;1T) are still prevalent in settings with
limited resources or unreliable power supply. However, they often yield images
with lower spatial resolution and contrast than high-field (HF) scanners. This
quality disparity can result in inaccurate clinician interpretations. Image
Quality Transfer (IQT) has been developed to enhance the quality of images by
learning a mapping function between low and high-quality images. Existing IQT
models often fail to restore high-frequency features, leading to blurry output.
In this paper, we propose a 3D conditional diffusion model to improve 3D
volumetric data, specifically LF MR images. Additionally, we incorporate a
cross-batch mechanism into the self-attention and padding of our network,
ensuring broader contextual awareness even under small 3D patches. Experiments
on the publicly available Human Connectome Project (HCP) dataset for IQT and
brain parcellation demonstrate that our model outperforms existing methods both
quantitatively and qualitatively. The code is publicly available at
\url{https://github.com/edshkim98/DiffusionIQT}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06632">The Exact Determinant of a Specific Class of Sparse Positive Definite Matrices. (arXiv:2311.06632v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Molkaraie_M/0/1/0/all/0/1">Mehdi Molkaraie</a></p>
<p>For a specific class of sparse Gaussian graphical models, we provide a
closed-form solution for the determinant of the covariance matrix. In our
framework, the graphical interaction model (i.e., the covariance selection
model) is equal to replacement product of $\mathcal{K}_{n}$ and
$\mathcal{K}_{n-1}$, where $\mathcal{K}_n$ is the complete graph with $n$
vertices. Our analysis is based on taking the Fourier transform of the local
factors of the model, which can be viewed as an application of the Normal
Factor Graph Duality Theorem and holographic algorithms. The closed-form
expression is obtained by applying the Matrix Determinant Lemma on the
transformed graphical model. In this context, we will also define a notion of
equivalence between two Gaussian graphical models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06633">The Pros and Cons of Using Machine Learning and Interpretable Machine Learning Methods in psychiatry detection applications, specifically depression disorder: A Brief Review. (arXiv:2311.06633v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Simchi_H/0/1/0/all/0/1">Hossein Simchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Tajik_S/0/1/0/all/0/1">Samira Tajik</a></p>
<p>The COVID-19 pandemic has forced many people to limit their social
activities, which has resulted in a rise in mental illnesses, particularly
depression. To diagnose these illnesses with accuracy and speed, and prevent
severe outcomes such as suicide, the use of machine learning has become
increasingly important. Additionally, to provide precise and understandable
diagnoses for better treatment, AI scientists and researchers must develop
interpretable AI-based solutions. This article provides an overview of relevant
articles in the field of machine learning and interpretable AI, which helps to
understand the advantages and disadvantages of using AI in psychiatry disorder
detection applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06643">Privacy Risks Analysis and Mitigation in Federated Learning for Medical Images. (arXiv:2311.06643v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Das_B/0/1/0/all/0/1">Badhan Chandra Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Amini_M/0/1/0/all/0/1">M. Hadi Amini</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yanzhao Wu</a></p>
<p>Federated learning (FL) is gaining increasing popularity in the medical
domain for analyzing medical images, which is considered an effective technique
to safeguard sensitive patient data and comply with privacy regulations.
However, several recent studies have revealed that the default settings of FL
may leak private training data under privacy attacks. Thus, it is still unclear
whether and to what extent such privacy risks of FL exist in the medical
domain, and if so, ``how to mitigate such risks?''. In this paper, first, we
propose a holistic framework for Medical data Privacy risk analysis and
mitigation in Federated Learning (MedPFL) to analyze privacy risks and develop
effective mitigation strategies in FL for protecting private medical data.
Second, we demonstrate the substantial privacy risks of using FL to process
medical images, where adversaries can easily perform privacy attacks to
reconstruct private medical images accurately. Third, we show that the defense
approach of adding random noises may not always work effectively to protect
medical images against privacy attacks in FL, which poses unique and pressing
challenges associated with medical data for privacy protection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/1902.11122">Deep Learning in Cardiology. (arXiv:1902.11122v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bizopoulos_P/0/1/0/all/0/1">Paschalis Bizopoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Koutsouris_D/0/1/0/all/0/1">Dimitrios Koutsouris</a></p>
<p>The medical field is creating large amount of data that physicians are unable
to decipher and use efficiently. Moreover, rule-based expert systems are
inefficient in solving complicated medical tasks or for creating insights using
big data. Deep learning has emerged as a more accurate and effective technology
in a wide range of medical problems such as diagnosis, prediction and
intervention. Deep learning is a representation learning method that consists
of layers that transform the data non-linearly, thus, revealing hierarchical
relationships and structures. In this review we survey deep learning
application papers that use structured data, signal and imaging modalities from
cardiology. We discuss the advantages and limitations of applying deep learning
in cardiology that also apply in medicine in general, while proposing certain
directions as the most viable for clinical use.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/1907.06592">Sparsely Activated Networks. (arXiv:1907.06592v10 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bizopoulos_P/0/1/0/all/0/1">Paschalis Bizopoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Koutsouris_D/0/1/0/all/0/1">Dimitrios Koutsouris</a></p>
<p>Previous literature on unsupervised learning focused on designing structural
priors with the aim of learning meaningful features. However, this was done
without considering the description length of the learned representations which
is a direct and unbiased measure of the model complexity. In this paper, first
we introduce the $\varphi$ metric that evaluates unsupervised models based on
their reconstruction accuracy and the degree of compression of their internal
representations. We then present and define two activation functions (Identity,
ReLU) as base of reference and three sparse activation functions (top-k
absolutes, Extrema-Pool indices, Extrema) as candidate structures that minimize
the previously defined $\varphi$. We lastly present Sparsely Activated Networks
(SANs) that consist of kernels with shared weights that, during encoding, are
convolved with the input and then passed through a sparse activation function.
During decoding, the same weights are convolved with the sparse activation map
and subsequently the partial reconstructions from each weight are summed to
reconstruct the input. We compare SANs using the five previously defined
activation functions on a variety of datasets (Physionet, UCI-epilepsy, MNIST,
FMNIST) and show that models that are selected using $\varphi$ have small
description representation length and consist of interpretable kernels.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/1912.00042">Learning Likelihoods with Conditional Normalizing Flows. (arXiv:1912.00042v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Winkler_C/0/1/0/all/0/1">Christina Winkler</a>, <a href="http://arxiv.org/find/cs/1/au:+Worrall_D/0/1/0/all/0/1">Daniel Worrall</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoogeboom_E/0/1/0/all/0/1">Emiel Hoogeboom</a>, <a href="http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1">Max Welling</a></p>
<p>Normalizing Flows (NFs) are able to model complicated distributions p(y) with
strong inter-dimensional correlations and high multimodality by transforming a
simple base density p(z) through an invertible neural network under the change
of variables formula. Such behavior is desirable in multivariate structured
prediction tasks, where handcrafted per-pixel loss-based methods inadequately
capture strong correlations between output dimensions. We present a study of
conditional normalizing flows (CNFs), a class of NFs where the base density to
output space mapping is conditioned on an input x, to model conditional
densities p(y|x). CNFs are efficient in sampling and inference, they can be
trained with a likelihood-based objective, and CNFs, being generative flows, do
not suffer from mode collapse or training instabilities. We provide an
effective method to train continuous CNFs for binary problems and in
particular, we apply these CNFs to super-resolution and vessel segmentation
tasks demonstrating competitive performance on standard benchmark datasets in
terms of likelihood and conventional metrics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2008.12876">Alternating minimization algorithms for graph regularized tensor completion. (arXiv:2008.12876v2 [math.NA] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Guan_Y/0/1/0/all/0/1">Yu Guan</a>, <a href="http://arxiv.org/find/math/1/au:+Dong_S/0/1/0/all/0/1">Shuyu Dong</a>, <a href="http://arxiv.org/find/math/1/au:+Gao_B/0/1/0/all/0/1">Bin Gao</a>, <a href="http://arxiv.org/find/math/1/au:+Absil_P/0/1/0/all/0/1">P.-A. Absil</a>, <a href="http://arxiv.org/find/math/1/au:+Glineur_F/0/1/0/all/0/1">Fran&#xe7;ois Glineur</a></p>
<p>We consider a Canonical Polyadic (CP) decomposition approach to low-rank
tensor completion (LRTC) by incorporating external pairwise similarity
relations through graph Laplacian regularization on the CP factor matrices. The
usage of graph regularization entails benefits in the learning accuracy of
LRTC, but at the same time, induces coupling graph Laplacian terms that hinder
the optimization of the tensor completion model. In order to solve
graph-regularized LRTC, we propose efficient alternating minimization
algorithms by leveraging the block structure of the underlying CP
decomposition-based model. For the subproblems of alternating minimization, a
linear conjugate gradient subroutine is specifically adapted to
graph-regularized LRTC. Alternatively, we circumvent the complicating coupling
effects of graph Laplacian terms by using an alternating directions method of
multipliers. Based on the Kurdyka-{\L}ojasiewicz property, we show that the
sequence generated by the proposed algorithms globally converges to a critical
point of the objective function. Moreover, the complexity and convergence rate
are also derived. In addition, numerical experiments including synthetic data
and real data show that the graph regularized tensor completion model has
improved recovery results compared to those without graph regularization, and
that the proposed algorithms achieve gains in time efficiency over existing
algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2009.06412">Comprehensive Comparison of Deep Learning Models for Lung and COVID-19 Lesion Segmentation in CT scans. (arXiv:2009.06412v7 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Bizopoulos_P/0/1/0/all/0/1">Paschalis Bizopoulos</a>, <a href="http://arxiv.org/find/eess/1/au:+Vretos_N/0/1/0/all/0/1">Nicholas Vretos</a>, <a href="http://arxiv.org/find/eess/1/au:+Daras_P/0/1/0/all/0/1">Petros Daras</a></p>
<p>Recently there has been an explosion in the use of Deep Learning (DL) methods
for medical image segmentation. However the field's reliability is hindered by
the lack of a common base of reference for accuracy/performance evaluation and
the fact that previous research uses different datasets for evaluation. In this
paper, an extensive comparison of DL models for lung and COVID-19 lesion
segmentation in Computerized Tomography (CT) scans is presented, which can also
be used as a benchmark for testing medical image segmentation models. Four DL
architectures (Unet, Linknet, FPN, PSPNet) are combined with 25 randomly
initialized and pretrained encoders (variations of VGG, DenseNet, ResNet,
ResNext, DPN, MobileNet, Xception, Inception-v4, EfficientNet), to construct
200 tested models. Three experimental setups are conducted for lung
segmentation, lesion segmentation and lesion segmentation using the original
lung masks. A public COVID-19 dataset with 100 CT scan images (80 for train, 20
for validation) is used for training/validation and a different public dataset
consisting of 829 images from 9 CT scan volumes for testing. Multiple findings
are provided including the best architecture-encoder models for each experiment
as well as mean Dice results for each experiment, architecture and encoder
independently. Finally, the upper bounds improvements when using lung masks as
a preprocessing step or when using pretrained models are quantified. The source
code and 600 pretrained models for the three experiments are provided, suitable
for fine-tuning in experimental setups without GPU capabilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2011.13132">Generative Learning of Heterogeneous Tail Dependence. (arXiv:2011.13132v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xiangqian Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1">Xing Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1">Qi Wu</a></p>
<p>We propose a multivariate generative model to capture the complex dependence
structure often encountered in business and financial data. Our model features
heterogeneous and asymmetric tail dependence between all pairs of individual
dimensions while also allowing heterogeneity and asymmetry in the tails of the
marginals. A significant merit of our model structure is that it is not prone
to error propagation in the parameter estimation process, hence very scalable,
as the dimensions of datasets grow large. However, the likelihood methods are
infeasible for parameter estimation in our case due to the lack of a
closed-form density function. Instead, we devise a novel moment learning
algorithm to learn the parameters. To demonstrate the effectiveness of the
model and its estimator, we test them on simulated as well as real-world
datasets. Results show that this framework gives better finite-sample
performance compared to the copula-based benchmarks as well as recent similar
models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2102.10773">Slowly Varying Regression under Sparsity. (arXiv:2102.10773v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bertsimas_D/0/1/0/all/0/1">Dimitris Bertsimas</a>, <a href="http://arxiv.org/find/cs/1/au:+Digalakis_V/0/1/0/all/0/1">Vassilis Digalakis Jr</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Michael Linghzi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lami_O/0/1/0/all/0/1">Omar Skali Lami</a></p>
<p>We present the framework of slowly varying regression under sparsity,
allowing sparse regression models to exhibit slow and sparse variations. The
problem of parameter estimation is formulated as a mixed-integer optimization
problem. We demonstrate that it can be precisely reformulated as a binary
convex optimization problem through a novel relaxation technique. This
relaxation involves a new equality on Moore-Penrose inverses, convexifying the
non-convex objective function while matching the original objective on all
feasible binary points. This enables us to efficiently solve the problem to
provable optimality using a cutting plane-type algorithm. We develop a highly
optimized implementation of this algorithm, substantially improving upon the
asymptotic computational complexity of a straightforward implementation.
Additionally, we propose a fast heuristic method that guarantees a feasible
solution and, as empirically illustrated, produces high-quality warm-start
solutions for the binary optimization problem. To tune the framework's
hyperparameters, we suggest a practical procedure relying on binary search
that, under certain assumptions, is guaranteed to recover the true model
parameters. On both synthetic and real-world datasets, we demonstrate that the
resulting algorithm outperforms competing formulations in comparable times
across various metrics, including estimation accuracy, predictive power, and
computational time. The algorithm is highly scalable, allowing us to train
models with thousands of parameters. Our implementation is available
open-source at https://github.com/vvdigalakis/SSVRegression.git.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2106.13823">Quantum Data Compression and Quantum Cross Entropy. (arXiv:2106.13823v3 [quant-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Shangnan_Z/0/1/0/all/0/1">Zhou Shangnan</a></p>
<p>The emerging field of quantum machine learning has the potential of
revolutionizing our perspectives of quantum computing and artificial
intelligence. In the predominantly empirical realm of quantum machine learning,
a theoretical void persists. This paper addresses the gap by highlighting the
quantum cross entropy, a pivotal counterpart to the classical cross entropy. We
establish quantum cross entropy's role in quantum data compression, a
fundamental machine learning task, by demonstrating that it acts as the
compression rate for sub-optimal quantum source coding. Our approach involves a
novel, universal quantum data compression protocol based on the quantum
generalization of variable-length coding and the principle of quantum strong
typicality. This reveals that quantum cross entropy can effectively serve as a
loss function in quantum machine learning algorithms. Furthermore, we
illustrate that the minimum of quantum cross entropy aligns with the von
Neumann entropy, reinforcing its role as the optimal compression rate and
underscoring its significance in advancing our understanding of quantum machine
learning's theoretical framework.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.13254">On Dynamic Pricing with Covariates. (arXiv:2112.13254v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hanzhao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Talluri_K/0/1/0/all/0/1">Kalyan Talluri</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaocheng Li</a></p>
<p>We consider dynamic pricing with covariates under a generalized linear demand
model: a seller can dynamically adjust the price of a product over a horizon of
$T$ time periods, and at each time period $t$, the demand of the product is
jointly determined by the price and an observable covariate vector
$x_t\in\mathbb{R}^d$ through a generalized linear model with unknown
co-efficients. Most of the existing literature assumes the covariate vectors
$x_t$'s are independently and identically distributed (i.i.d.); the few papers
that relax this assumption either sacrifice model generality or yield
sub-optimal regret bounds. In this paper, we show that UCB and Thompson
sampling-based pricing algorithms can achieve an $O(d\sqrt{T}\log T)$ regret
upper bound without assuming any statistical structure on the covariates $x_t$.
Our upper bound on the regret matches the lower bound up to logarithmic
factors. We thus show that (i) the i.i.d. assumption is not necessary for
obtaining low regret, and (ii) the regret bound can be independent of the
(inverse) minimum eigenvalue of the covariance matrix of the $x_t$'s, a
quantity present in previous bounds. Moreover, we consider a constrained
setting of the dynamic pricing problem where there is a limited and
unreplenishable inventory and we develop theoretical results that relate the
best achievable algorithm performance to a variation measure with respect to
the temporal distribution shift of the covariates. We also discuss conditions
under which a better regret is achievable and demonstrate the proposed
algorithms' performance with numerical experiments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.07798">BB-ML: Basic Block Performance Prediction using Machine Learning Techniques. (arXiv:2202.07798v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abdelkhalik_H/0/1/0/all/0/1">Hamdy Abdelkhalik</a>, <a href="http://arxiv.org/find/cs/1/au:+Aktar_S/0/1/0/all/0/1">Shamminuj Aktar</a>, <a href="http://arxiv.org/find/cs/1/au:+Arafa_Y/0/1/0/all/0/1">Yehia Arafa</a>, <a href="http://arxiv.org/find/cs/1/au:+Barai_A/0/1/0/all/0/1">Atanu Barai</a>, <a href="http://arxiv.org/find/cs/1/au:+Chennupati_G/0/1/0/all/0/1">Gopinath Chennupati</a>, <a href="http://arxiv.org/find/cs/1/au:+Santhi_N/0/1/0/all/0/1">Nandakishore Santhi</a>, <a href="http://arxiv.org/find/cs/1/au:+Panda_N/0/1/0/all/0/1">Nishant Panda</a>, <a href="http://arxiv.org/find/cs/1/au:+Prajapati_N/0/1/0/all/0/1">Nirmal Prajapati</a>, <a href="http://arxiv.org/find/cs/1/au:+Turja_N/0/1/0/all/0/1">Nazmul Haque Turja</a>, <a href="http://arxiv.org/find/cs/1/au:+Eidenbenz_S/0/1/0/all/0/1">Stephan Eidenbenz</a>, <a href="http://arxiv.org/find/cs/1/au:+Badawy_A/0/1/0/all/0/1">Abdel-Hameed Badawy</a></p>
<p>Recent years have seen the adoption of Machine Learning (ML) techniques to
predict the performance of large-scale applications, mostly at a coarse level.
In contrast, we propose to use ML techniques for performance prediction at a
much finer granularity, namely at the Basic Block (BB) level, which are single
entry, single exit code blocks that are used for analysis by the compilers to
break down a large code into manageable pieces. We extrapolate the basic block
execution counts of GPU applications and use them for predicting the
performance for large input sizes from the counts of smaller input sizes. We
train a Poisson Neural Network (PNN) model using random input values as well as
the lowest input values of the application to learn the relationship between
inputs and basic block counts. Experimental results show that the model can
accurately predict the basic block execution counts of 16 GPU benchmarks. We
achieve an accuracy of 93.5% in extrapolating the basic block counts for large
input sets when trained on smaller input sets and an accuracy of 97.7% in
predicting basic block counts on random instances. In a case study, we apply
the ML model to CUDA GPU benchmarks for performance prediction across a
spectrum of applications. We use a variety of metrics for evaluation, including
global memory requests and the active cycles of tensor cores, ALU, and FMA
units. Results demonstrate the model's capability of predicting the performance
of large datasets with an average error rate of 0.85% and 0.17% for global and
shared memory requests, respectively. Additionally, to address the utilization
of the main functional units in Ampere architecture GPUs, we calculate the
active cycles for tensor cores, ALU, FMA, and FP64 units and achieve an average
error of 2.3% and 10.66% for ALU and FMA units while the maximum observed error
across all tested applications and units reaches 18.5%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.03691">HyperMixer: An MLP-based Low Cost Alternative to Transformers. (arXiv:2203.03691v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mai_F/0/1/0/all/0/1">Florian Mai</a>, <a href="http://arxiv.org/find/cs/1/au:+Pannatier_A/0/1/0/all/0/1">Arnaud Pannatier</a>, <a href="http://arxiv.org/find/cs/1/au:+Fehr_F/0/1/0/all/0/1">Fabio Fehr</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Haolin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Marelli_F/0/1/0/all/0/1">Francois Marelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Fleuret_F/0/1/0/all/0/1">Francois Fleuret</a>, <a href="http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1">James Henderson</a></p>
<p>Transformer-based architectures are the model of choice for natural language
understanding, but they come at a significant cost, as they have quadratic
complexity in the input length, require a lot of training data, and can be
difficult to tune. In the pursuit of lower costs, we investigate simple
MLP-based architectures. We find that existing architectures such as MLPMixer,
which achieves token mixing through a static MLP applied to each feature
independently, are too detached from the inductive biases required for natural
language understanding. In this paper, we propose a simple variant, HyperMixer,
which forms the token mixing MLP dynamically using hypernetworks. Empirically,
we demonstrate that our model performs better than alternative MLP-based
models, and on par with Transformers. In contrast to Transformers, HyperMixer
achieves these results at substantially lower costs in terms of processing
time, training data, and hyperparameter tuning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2204.00281">i-Razor: A Differentiable Neural Input Razor for Feature Selection and Dimension Search in DNN-Based Recommender Systems. (arXiv:2204.00281v2 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1">Yao Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1">Haoxun He</a>, <a href="http://arxiv.org/find/cs/1/au:+Sheng_D/0/1/0/all/0/1">Dakui Sheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Ke Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1">Li Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1">Huanhuan Cao</a></p>
<p>Input features play a crucial role in DNN-based recommender systems with
thousands of categorical and continuous fields from users, items, contexts, and
interactions. Noisy features and inappropriate embedding dimension assignments
can deteriorate the performance of recommender systems and introduce
unnecessary complexity in model training and online serving. Optimizing the
input configuration of DNN models, including feature selection and embedding
dimension assignment, has become one of the essential topics in feature
engineering. However, in existing industrial practices, feature selection and
dimension search are optimized sequentially, i.e., feature selection is
performed first, followed by dimension search to determine the optimal
dimension size for each selected feature. Such a sequential optimization
mechanism increases training costs and risks generating suboptimal input
configurations. To address this problem, we propose a differentiable neural
input razor (i-Razor) that enables joint optimization of feature selection and
dimension search. Concretely, we introduce an end-to-end differentiable model
to learn the relative importance of different embedding regions of each
feature. Furthermore, a flexible pruning algorithm is proposed to achieve
feature filtering and dimension derivation simultaneously. Extensive
experiments on two large-scale public datasets in the Click-Through-Rate (CTR)
prediction task demonstrate the efficacy and superiority of i-Razor in
balancing model complexity and performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.15680">Simulator-Based Inference with Waldo: Confidence Regions by Leveraging Prediction Algorithms and Posterior Estimators for Inverse Problems. (arXiv:2205.15680v4 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Masserano_L/0/1/0/all/0/1">Luca Masserano</a>, <a href="http://arxiv.org/find/stat/1/au:+Dorigo_T/0/1/0/all/0/1">Tommaso Dorigo</a>, <a href="http://arxiv.org/find/stat/1/au:+Izbicki_R/0/1/0/all/0/1">Rafael Izbicki</a>, <a href="http://arxiv.org/find/stat/1/au:+Kuusela_M/0/1/0/all/0/1">Mikael Kuusela</a>, <a href="http://arxiv.org/find/stat/1/au:+Lee_A/0/1/0/all/0/1">Ann B. Lee</a></p>
<p>Prediction algorithms, such as deep neural networks (DNNs), are used in many
domain sciences to directly estimate internal parameters of interest in
simulator-based models, especially in settings where the observations include
images or complex high-dimensional data. In parallel, modern neural density
estimators, such as normalizing flows, are becoming increasingly popular for
uncertainty quantification, especially when both parameters and observations
are high-dimensional. However, parameter inference is an inverse problem and
not a prediction task; thus, an open challenge is to construct conditionally
valid and precise confidence regions, with a guaranteed probability of covering
the true parameters of the data-generating process, no matter what the
(unknown) parameter values are, and without relying on large-sample theory.
Many simulator-based inference (SBI) methods are indeed known to produce biased
or overly confident parameter regions, yielding misleading uncertainty
estimates. This paper presents WALDO, a novel method to construct confidence
regions with finite-sample conditional validity by leveraging prediction
algorithms or posterior estimators that are currently widely adopted in SBI.
WALDO reframes the well-known Wald test statistic, and uses a computationally
efficient regression-based machinery for classical Neyman inversion of
hypothesis tests. We apply our method to a recent high-energy physics problem,
where prediction with DNNs has previously led to estimates with prediction
bias. We also illustrate how our approach can correct overly confident
posterior regions computed with normalizing flows.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.15935">Bias-inducing geometries: an exactly solvable data model with fairness implications. (arXiv:2205.15935v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mannelli_S/0/1/0/all/0/1">Stefano Sarao Mannelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Gerace_F/0/1/0/all/0/1">Federica Gerace</a>, <a href="http://arxiv.org/find/cs/1/au:+Rostamzadeh_N/0/1/0/all/0/1">Negar Rostamzadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Saglietti_L/0/1/0/all/0/1">Luca Saglietti</a></p>
<p>Machine learning (ML) may be oblivious to human bias but it is not immune to
its perpetuation. Marginalisation and iniquitous group representation are often
traceable in the very data used for training, and may be reflected or even
enhanced by the learning models. In the present work, we aim at clarifying the
role played by data geometry in the emergence of ML bias. We introduce an
exactly solvable high-dimensional model of data imbalance, where parametric
control over the many bias-inducing factors allows for an extensive exploration
of the bias inheritance mechanism. Through the tools of statistical physics, we
analytically characterise the typical properties of learning models trained in
this synthetic framework and obtain exact predictions for the observables that
are commonly employed for fairness assessment. Despite the simplicity of the
data model, we retrace and unpack typical unfairness behaviour observed on
real-world datasets. We also obtain a detailed analytical characterisation of a
class of bias mitigation strategies. We first consider a basic loss-reweighing
scheme, which allows for an implicit minimisation of different unfairness
metrics, and quantify the incompatibilities between some existing fairness
criteria. Then, we consider a novel mitigation strategy based on a matched
inference approach, consisting in the introduction of coupled learning models.
Our theoretical analysis of this approach shows that the coupled strategy can
strike superior fairness-accuracy trade-offs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.07234">Brownian Noise Reduction: Maximizing Privacy Subject to Accuracy Constraints. (arXiv:2206.07234v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Whitehouse_J/0/1/0/all/0/1">Justin Whitehouse</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhiwei Steven Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramdas_A/0/1/0/all/0/1">Aaditya Ramdas</a>, <a href="http://arxiv.org/find/cs/1/au:+Rogers_R/0/1/0/all/0/1">Ryan Rogers</a></p>
<p>There is a disconnect between how researchers and practitioners handle
privacy-utility tradeoffs. Researchers primarily operate from a privacy first
perspective, setting strict privacy requirements and minimizing risk subject to
these constraints. Practitioners often desire an accuracy first perspective,
possibly satisfied with the greatest privacy they can get subject to obtaining
sufficiently small error. Ligett et al. have introduced a "noise reduction"
algorithm to address the latter perspective. The authors show that by adding
correlated Laplace noise and progressively reducing it on demand, it is
possible to produce a sequence of increasingly accurate estimates of a private
parameter while only paying a privacy cost for the least noisy iterate
released. In this work, we generalize noise reduction to the setting of
Gaussian noise, introducing the Brownian mechanism. The Brownian mechanism
works by first adding Gaussian noise of high variance corresponding to the
final point of a simulated Brownian motion. Then, at the practitioner's
discretion, noise is gradually decreased by tracing back along the Brownian
path to an earlier time. Our mechanism is more naturally applicable to the
common setting of bounded $\ell_2$-sensitivity, empirically outperforms
existing work on common statistical tasks, and provides customizable control of
privacy loss over the entire interaction with the practitioner. We complement
our Brownian mechanism with ReducedAboveThreshold, a generalization of the
classical AboveThreshold algorithm that provides adaptive privacy guarantees.
Overall, our results demonstrate that one can meet utility constraints while
still maintaining strong levels of privacy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.11303">Community Recovery in the Geometric Block Model. (arXiv:2206.11303v2 [cs.SI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Galhotra_S/0/1/0/all/0/1">Sainyam Galhotra</a>, <a href="http://arxiv.org/find/cs/1/au:+Mazumdar_A/0/1/0/all/0/1">Arya Mazumdar</a>, <a href="http://arxiv.org/find/cs/1/au:+Pal_S/0/1/0/all/0/1">Soumyabrata Pal</a>, <a href="http://arxiv.org/find/cs/1/au:+Saha_B/0/1/0/all/0/1">Barna Saha</a></p>
<p>To capture inherent geometric features of many community detection problems,
we propose to use a new random graph model of communities that we call a
\textit{Geometric Block Model}. The geometric block model builds on the
\emph{random geometric graphs} (Gilbert, 1961), one of the basic models of
random graphs for spatial networks, in the same way that the well-studied
stochastic block model builds on the Erd\H{o}s-R\'{en}yi random graphs. It is
also a natural extension of random community models inspired by the recent
theoretical and practical advancements in community detection. To analyze the
geometric block model, we first provide new connectivity results for
\emph{random annulus graphs} which are generalizations of random geometric
graphs. The connectivity properties of geometric graphs have been studied since
their introduction, and analyzing them has been difficult due to correlated
edge formation.
</p>
<p>We then use the connectivity results of random annulus graphs to provide
necessary and sufficient conditions for efficient recovery of communities for
the geometric block model. We show that a simple triangle-counting algorithm to
detect communities in the geometric block model is near-optimal. For this we
consider two regimes of graph density.
</p>
<p>In the regime where the average degree of the graph grows logarithmically
with number of vertices, we show that our algorithm performs extremely well,
both theoretically and practically. In contrast, the triangle-counting
algorithm is far from being optimum for the stochastic block model in the
logarithmic degree regime. We also look at the regime where the average degree
of the graph grows linearly with the number of vertices $n$, and hence to store
the graph one needs $\Theta(n^2)$ memory. We show that our algorithm needs to
store only $O(n \log n)$ edges in this regime to recover the latent
communities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.13086">RankSEG: A Consistent Ranking-based Framework for Segmentation. (arXiv:2206.13086v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Dai_B/0/1/0/all/0/1">Ben Dai</a>, <a href="http://arxiv.org/find/stat/1/au:+Li_C/0/1/0/all/0/1">Chunlin Li</a></p>
<p>Segmentation has emerged as a fundamental field of computer vision and
natural language processing, which assigns a label to every pixel/feature to
extract regions of interest from an image/text. To evaluate the performance of
segmentation, the Dice and IoU metrics are used to measure the degree of
overlap between the ground truth and the predicted segmentation. In this paper,
we establish a theoretical foundation of segmentation with respect to the
Dice/IoU metrics, including the Bayes rule and Dice-/IoU-calibration, analogous
to classification-calibration or Fisher consistency in classification. We prove
that the existing thresholding-based framework with most operating losses are
not consistent with respect to the Dice/IoU metrics, and thus may lead to a
suboptimal solution. To address this pitfall, we propose a novel consistent
ranking-based framework, namely RankDice/RankIoU, inspired by plug-in rules of
the Bayes segmentation rule. Three numerical algorithms with GPU parallel
execution are developed to implement the proposed framework in large-scale and
high-dimensional segmentation. We study statistical properties of the proposed
framework. We show it is Dice-/IoU-calibrated, and its excess risk bounds and
the rate of convergence are also provided. The numerical effectiveness of
RankDice/mRankDice is demonstrated in various simulated examples and
Fine-annotated CityScapes, Pascal VOC and Kvasir-SEG datasets with
state-of-the-art deep learning architectures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.15215">Learning nonparametric ordinary differential equations from noisy data. (arXiv:2206.15215v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Lahouel_K/0/1/0/all/0/1">Kamel Lahouel</a>, <a href="http://arxiv.org/find/stat/1/au:+Wells_M/0/1/0/all/0/1">Michael Wells</a>, <a href="http://arxiv.org/find/stat/1/au:+Rielly_V/0/1/0/all/0/1">Victor Rielly</a>, <a href="http://arxiv.org/find/stat/1/au:+Lew_E/0/1/0/all/0/1">Ethan Lew</a>, <a href="http://arxiv.org/find/stat/1/au:+Lovitz_D/0/1/0/all/0/1">David Lovitz</a>, <a href="http://arxiv.org/find/stat/1/au:+Jedynak_B/0/1/0/all/0/1">Bruno M. Jedynak</a></p>
<p>Learning nonparametric systems of Ordinary Differential Equations (ODEs) dot
x = f(t,x) from noisy data is an emerging machine learning topic. We use the
well-developed theory of Reproducing Kernel Hilbert Spaces (RKHS) to define
candidates for f for which the solution of the ODE exists and is unique.
Learning f consists of solving a constrained optimization problem in an RKHS.
We propose a penalty method that iteratively uses the Representer theorem and
Euler approximations to provide a numerical solution. We prove a generalization
bound for the L2 distance between x and its estimator and provide experimental
comparisons with the state-of-the-art.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.01678">FACT: High-Dimensional Random Forests Inference. (arXiv:2207.01678v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Chi_C/0/1/0/all/0/1">Chien-Ming Chi</a>, <a href="http://arxiv.org/find/stat/1/au:+Fan_Y/0/1/0/all/0/1">Yingying Fan</a>, <a href="http://arxiv.org/find/stat/1/au:+Lv_J/0/1/0/all/0/1">Jinchi Lv</a></p>
<p>Quantifying the usefulness of individual features in random forests learning
can greatly enhance its interpretability. Existing studies have shown that some
popularly used feature importance measures for random forests suffer from the
bias issue. In addition, there lack comprehensive size and power analyses for
most of these existing methods. In this paper, we approach the problem via
hypothesis testing, and suggest a framework of the self-normalized
feature-residual correlation test (FACT) for evaluating the significance of a
given feature in the random forests model with bias-resistance property, where
our null hypothesis concerns whether the feature is conditionally independent
of the response given all other features. Such an endeavor on random forests
inference is empowered by some recent developments on high-dimensional random
forests consistency. Under a fairly general high-dimensional nonparametric
model setting with dependent features, we formally establish that FACT can
provide theoretically justified feature importance test with controlled type I
error and enjoy appealing power property. The theoretical results and
finite-sample advantages of the newly suggested method are illustrated with
several simulation examples and an economic forecasting application.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.10434">A simple learning agent interacting with an agent-based market model. (arXiv:2208.10434v4 [q-fin.TR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+Dicks_M/0/1/0/all/0/1">Matthew Dicks</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Paskaramoorthy_A/0/1/0/all/0/1">Andrew Paskaramoorthy</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Gebbie_T/0/1/0/all/0/1">Tim Gebbie</a></p>
<p>We consider the learning dynamics of a single reinforcement learning optimal
execution trading agent when it interacts with an event driven agent-based
financial market model. Trading takes place asynchronously through a matching
engine in event time. The optimal execution agent is considered at different
levels of initial order-sizes and differently sized state spaces. The resulting
impact on the agent-based model and market are considered using a calibration
approach that explores changes in the empirical stylised facts and price impact
curves. Convergence, volume trajectory and action trace plots are used to
visualise the learning dynamics. Here the smaller state space agents had the
number of states they visited converge much faster than the larger state space
agents, and they were able to start learning to trade intuitively using the
spread and volume states. We find that the moments of the model are robust to
the impact of the learning agents except for the Hurst exponent, which was
lowered by the introduction of strategic order-splitting. The introduction of
the learning agent preserves the shape of the price impact curves but can
reduce the trade-sign auto-correlations when their trading volumes increase.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.14673">Incremental Learning in Diagonal Linear Networks. (arXiv:2208.14673v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Berthier_R/0/1/0/all/0/1">Rapha&#xeb;l Berthier</a></p>
<p>Diagonal linear networks (DLNs) are a toy simplification of artificial neural
networks; they consist in a quadratic reparametrization of linear regression
inducing a sparse implicit regularization. In this paper, we describe the
trajectory of the gradient flow of DLNs in the limit of small initialization.
We show that incremental learning is effectively performed in the limit:
coordinates are successively activated, while the iterate is the minimizer of
the loss constrained to have support on the active coordinates only. This shows
that the sparse implicit regularization of DLNs decreases with time. This work
is restricted to the underparametrized regime with anti-correlated features for
technical reasons.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.14750">Non-contrastive representation learning for intervals from well logs. (arXiv:2209.14750v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Marusov_A/0/1/0/all/0/1">Alexander Marusov</a>, <a href="http://arxiv.org/find/cs/1/au:+Zaytsev_A/0/1/0/all/0/1">Alexey Zaytsev</a></p>
<p>The representation learning problem in the oil &amp; gas industry aims to
construct a model that provides a representation based on logging data for a
well interval. Previous attempts are mainly supervised and focus on similarity
task, which estimates closeness between intervals. We desire to build
informative representations without using supervised (labelled) data. One of
the possible approaches is self-supervised learning (SSL). In contrast to the
supervised paradigm, this one requires little or no labels for the data.
Nowadays, most SSL approaches are either contrastive or non-contrastive.
Contrastive methods make representations of similar (positive) objects closer
and distancing different (negative) ones. Due to possible wrong marking of
positive and negative pairs, these methods can provide an inferior performance.
Non-contrastive methods don't rely on such labelling and are widespread in
computer vision. They learn using only pairs of similar objects that are easier
to identify in logging data.
</p>
<p>We are the first to introduce non-contrastive SSL for well-logging data. In
particular, we exploit Bootstrap Your Own Latent (BYOL) and Barlow Twins
methods that avoid using negative pairs and focus only on matching positive
pairs. The crucial part of these methods is an augmentation strategy. Our
augmentation strategies and adaption of BYOL and Barlow Twins together allow us
to achieve superior quality on clusterization and mostly the best performance
on different classification tasks. Our results prove the usefulness of the
proposed non-contrastive self-supervised approaches for representation learning
and interval similarity in particular.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.03461">FastCLIPstyler: Optimisation-free Text-based Image Style Transfer Using Style Representations. (arXiv:2210.03461v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Suresh_A/0/1/0/all/0/1">Ananda Padhmanabhan Suresh</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1">Sanjana Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Noinongyao_P/0/1/0/all/0/1">Pavit Noinongyao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganguly_A/0/1/0/all/0/1">Ankush Ganguly</a></p>
<p>In recent years, language-driven artistic style transfer has emerged as a new
type of style transfer technique, eliminating the need for a reference style
image by using natural language descriptions of the style. The first model to
achieve this, called CLIPstyler, has demonstrated impressive stylisation
results. However, its lengthy optimisation procedure at runtime for each query
limits its suitability for many practical applications. In this work, we
present FastCLIPstyler, a generalised text-based image style transfer model
capable of stylising images in a single forward pass for arbitrary text inputs.
Furthermore, we introduce EdgeCLIPstyler, a lightweight model designed for
compatibility with resource-constrained devices. Through quantitative and
qualitative comparisons with state-of-the-art approaches, we demonstrate that
our models achieve superior stylisation quality based on measurable metrics
while offering significantly improved runtime efficiency, particularly on edge
devices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.06650">Interpreting Neural Policies with Disentangled Tree Representations. (arXiv:2210.06650v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tsun-Hsuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1">Wei Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Seyde_T/0/1/0/all/0/1">Tim Seyde</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasani_R/0/1/0/all/0/1">Ramin Hasani</a>, <a href="http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1">Daniela Rus</a></p>
<p>The advancement of robots, particularly those functioning in complex
human-centric environments, relies on control solutions that are driven by
machine learning. Understanding how learning-based controllers make decisions
is crucial since robots are often safety-critical systems. This urges a formal
and quantitative understanding of the explanatory factors in the
interpretability of robot learning. In this paper, we aim to study
interpretability of compact neural policies through the lens of disentangled
representation. We leverage decision trees to obtain factors of variation [1]
for disentanglement in robot learning; these encapsulate skills, behaviors, or
strategies toward solving tasks. To assess how well networks uncover the
underlying task dynamics, we introduce interpretability metrics that measure
disentanglement of learned neural dynamics from a concentration of decisions,
mutual information and modularity perspective. We showcase the effectiveness of
the connection between interpretability and disentanglement consistently across
extensive experimental analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.16458">Reformulating van Rijsbergen&#x27;s $F_{\beta}$ metric for weighted binary cross-entropy. (arXiv:2210.16458v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Ramdhani_S/0/1/0/all/0/1">Satesh Ramdhani</a></p>
<p>The separation of performance metrics from gradient based loss functions may
not always give optimal results and may miss vital aggregate information. This
paper investigates incorporating a performance metric alongside differentiable
loss functions to inform training outcomes. The goal is to guide model
performance and interpretation by assuming statistical distributions on this
performance metric for dynamic weighting. The focus is on van Rijsbergens
$F_{\beta}$ metric -- a popular choice for gauging classification performance.
Through distributional assumptions on the $F_{\beta}$, an intermediary link can
be established to the standard binary cross-entropy via dynamic penalty
weights. First, the $F_{\beta}$ metric is reformulated to facilitate assuming
statistical distributions with accompanying proofs for the cumulative density
function. These probabilities are used within a knee curve algorithm to find an
optimal $\beta$ or $\beta_{opt}$. This $\beta_{opt}$ is used as a weight or
penalty in the proposed weighted binary cross-entropy. Experimentation on
publicly available data along with benchmark analysis mostly yields better and
interpretable results as compared to the baseline for both imbalanced and
balanced classes. For example, for the IMDB text data with known labeling
errors, a 14% boost in $F_1$ score is shown. The results also reveal
commonalities between the penalty model families derived in this paper and the
suitability of recall-centric or precision-centric parameters used in the
optimization. The flexibility of this methodology can enhance interpretation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.01258">Instance-Dependent Generalization Bounds via Optimal Transport. (arXiv:2211.01258v4 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Hou_S/0/1/0/all/0/1">Songyan Hou</a>, <a href="http://arxiv.org/find/stat/1/au:+Kassraie_P/0/1/0/all/0/1">Parnian Kassraie</a>, <a href="http://arxiv.org/find/stat/1/au:+Kratsios_A/0/1/0/all/0/1">Anastasis Kratsios</a>, <a href="http://arxiv.org/find/stat/1/au:+Krause_A/0/1/0/all/0/1">Andreas Krause</a>, <a href="http://arxiv.org/find/stat/1/au:+Rothfuss_J/0/1/0/all/0/1">Jonas Rothfuss</a></p>
<p>Existing generalization bounds fail to explain crucial factors that drive the
generalization of modern neural networks. Since such bounds often hold
uniformly over all parameters, they suffer from over-parametrization and fail
to account for the strong inductive bias of initialization and stochastic
gradient descent. As an alternative, we propose a novel optimal transport
interpretation of the generalization problem. This allows us to derive
instance-dependent generalization bounds that depend on the local Lipschitz
regularity of the learned prediction function in the data space. Therefore, our
bounds are agnostic to the parametrization of the model and work well when the
number of training samples is much smaller than the number of parameters. With
small modifications, our approach yields accelerated rates for data on
low-dimensional manifolds and guarantees under distribution shifts. We
empirically analyze our generalization bounds for neural networks, showing that
the bound values are meaningful and capture the effect of popular
regularization methods during training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.13308">SciRepEval: A Multi-Format Benchmark for Scientific Document Representations. (arXiv:2211.13308v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1">Amanpreet Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+DArcy_M/0/1/0/all/0/1">Mike D&#x27;Arcy</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1">Arman Cohan</a>, <a href="http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1">Doug Downey</a>, <a href="http://arxiv.org/find/cs/1/au:+Feldman_S/0/1/0/all/0/1">Sergey Feldman</a></p>
<p>Learned representations of scientific documents can serve as valuable input
features for downstream tasks without further fine-tuning. However, existing
benchmarks for evaluating these representations fail to capture the diversity
of relevant tasks. In response, we introduce SciRepEval, the first
comprehensive benchmark for training and evaluating scientific document
representations. It includes 24 challenging and realistic tasks, 8 of which are
new, across four formats: classification, regression, ranking and search. We
then use this benchmark to study and improve the generalization ability of
scientific document representation models. We show how state-of-the-art models
like SPECTER and SciNCL struggle to generalize across the task formats, and
that simple multi-task training fails to improve them. However, a new approach
that learns multiple embeddings per document, each tailored to a different
format, can improve performance. We experiment with task-format-specific
control codes and adapters and find they outperform the existing
single-embedding state-of-the-art by over 2 points absolute. We release the
resulting family of multi-format models, called SPECTER2, for the community to
use and build on.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.14468">Similarity-based cooperative equilibrium. (arXiv:2211.14468v2 [cs.GT] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oesterheld_C/0/1/0/all/0/1">Caspar Oesterheld</a>, <a href="http://arxiv.org/find/cs/1/au:+Treutlein_J/0/1/0/all/0/1">Johannes Treutlein</a>, <a href="http://arxiv.org/find/cs/1/au:+Grosse_R/0/1/0/all/0/1">Roger Grosse</a>, <a href="http://arxiv.org/find/cs/1/au:+Conitzer_V/0/1/0/all/0/1">Vincent Conitzer</a>, <a href="http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1">Jakob Foerster</a></p>
<p>As machine learning agents act more autonomously in the world, they will
increasingly interact with each other. Unfortunately, in many social dilemmas
like the one-shot Prisoner's Dilemma, standard game theory predicts that ML
agents will fail to cooperate with each other. Prior work has shown that one
way to enable cooperative outcomes in the one-shot Prisoner's Dilemma is to
make the agents mutually transparent to each other, i.e., to allow them to
access one another's source code (Rubinstein 1998, Tennenholtz 2004) -- or
weights in the case of ML agents. However, full transparency is often
unrealistic, whereas partial transparency is commonplace. Moreover, it is
challenging for agents to learn their way to cooperation in the full
transparency setting. In this paper, we introduce a more realistic setting in
which agents only observe a single number indicating how similar they are to
each other. We prove that this allows for the same set of cooperative outcomes
as the full transparency setting. We also demonstrate experimentally that
cooperation can be learned using simple ML methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.01368">Fast Non-Rigid Radiance Fields from Monocularized Data. (arXiv:2212.01368v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kappel_M/0/1/0/all/0/1">Moritz Kappel</a>, <a href="http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1">Vladislav Golyanik</a>, <a href="http://arxiv.org/find/cs/1/au:+Castillo_S/0/1/0/all/0/1">Susana Castillo</a>, <a href="http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1">Christian Theobalt</a>, <a href="http://arxiv.org/find/cs/1/au:+Magnor_M/0/1/0/all/0/1">Marcus Magnor</a></p>
<p>The reconstruction and novel view synthesis of dynamic scenes recently gained
increased attention. As reconstruction from large-scale multi-view data
involves immense memory and computational requirements, recent benchmark
datasets provide collections of single monocular views per timestamp sampled
from multiple (virtual) cameras. We refer to this form of inputs as
"monocularized" data. Existing work shows impressive results for synthetic
setups and forward-facing real-world data, but is often limited in the training
speed and angular range for generating novel views. This paper addresses these
limitations and proposes a new method for full 360{\deg} inward-facing novel
view synthesis of non-rigidly deforming scenes. At the core of our method are:
1) An efficient deformation module that decouples the processing of spatial and
temporal information for accelerated training and inference; and 2) A static
module representing the canonical scene as a fast hash-encoded neural radiance
field. In addition to existing synthetic monocularized data, we systematically
analyze the performance on real-world inward-facing scenes using a newly
recorded challenging dataset sampled from a synchronized large-scale multi-view
rig. In both cases, our method is significantly faster than previous methods,
converging in less than 7 minutes and achieving real-time framerates at 1K
resolution, while obtaining a higher visual accuracy for generated novel views.
Our source code and data is available at our project page
https://graphics.tu-bs.de/publications/kappel2022fast.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.01382">Welfare and Fairness in Multi-objective Reinforcement Learning. (arXiv:2212.01382v5 [cs.GT] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1">Zimeng Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1">Nianli Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_M/0/1/0/all/0/1">Muhang Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Fain_B/0/1/0/all/0/1">Brandon Fain</a></p>
<p>We study fair multi-objective reinforcement learning in which an agent must
learn a policy that simultaneously achieves high reward on multiple dimensions
of a vector-valued reward. Motivated by the fair resource allocation
literature, we model this as an expected welfare maximization problem, for some
nonlinear fair welfare function of the vector of long-term cumulative rewards.
One canonical example of such a function is the Nash Social Welfare, or
geometric mean, the log transform of which is also known as the Proportional
Fairness objective. We show that even approximately optimal optimization of the
expected Nash Social Welfare is computationally intractable even in the tabular
case. Nevertheless, we provide a novel adaptation of Q-learning that combines
nonlinear scalarized learning updates and non-stationary action selection to
learn effective policies for optimizing nonlinear welfare functions. We show
that our algorithm is provably convergent, and we demonstrate experimentally
that our approach outperforms techniques based on linear scalarization,
mixtures of optimal linear scalarizations, or stationary action selection for
the Nash Social Welfare Objective.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.05402">Stochastic First-Order Learning for Large-Scale Flexibly Tied Gaussian Mixture Model. (arXiv:2212.05402v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pasande_M/0/1/0/all/0/1">Mohammad Pasande</a>, <a href="http://arxiv.org/find/cs/1/au:+Hosseini_R/0/1/0/all/0/1">Reshad Hosseini</a>, <a href="http://arxiv.org/find/cs/1/au:+Araabi_B/0/1/0/all/0/1">Babak Nadjar Araabi</a></p>
<p>Gaussian Mixture Models (GMMs) are one of the most potent parametric density
models used extensively in many applications. Flexibly-tied factorization of
the covariance matrices in GMMs is a powerful approach for coping with the
challenges of common GMMs when faced with high-dimensional data and complex
densities which often demand a large number of Gaussian components. However,
the expectation-maximization algorithm for fitting flexibly-tied GMMs still
encounters difficulties with streaming and very large dimensional data. To
overcome these challenges, this paper suggests the use of first-order
stochastic optimization algorithms. Specifically, we propose a new stochastic
optimization algorithm on the manifold of orthogonal matrices. Through numerous
empirical results on both synthetic and real datasets, we observe that
stochastic optimization methods can outperform the expectation-maximization
algorithm in terms of attaining better likelihood, needing fewer epochs for
convergence, and consuming less time per each epoch.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.10477">Generalized Simultaneous Perturbation-based Gradient Search with Reduced Estimator Bias. (arXiv:2212.10477v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pachal_S/0/1/0/all/0/1">Soumen Pachal</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhatnagar_S/0/1/0/all/0/1">Shalabh Bhatnagar</a>, <a href="http://arxiv.org/find/cs/1/au:+Prashanth_L/0/1/0/all/0/1">L.A. Prashanth</a></p>
<p>We present in this paper a family of generalized simultaneous
perturbation-based gradient search (GSPGS) estimators that use noisy function
measurements. The number of function measurements required by each estimator is
guided by the desired level of accuracy. We first present in detail unbalanced
generalized simultaneous perturbation stochastic approximation (GSPSA)
estimators and later present the balanced versions (B-GSPSA) of these. We
extend this idea further and present the generalized smoothed functional (GSF)
and generalized random directions stochastic approximation (GRDSA) estimators,
respectively, as well as their balanced variants. We show that estimators
within any specified class requiring more number of function measurements
result in lower estimator bias. We present a detailed analysis of both the
asymptotic and non-asymptotic convergence of the resulting stochastic
approximation schemes. We further present a series of experimental results with
the various GSPGS estimators on the Rastrigin and quadratic function
objectives. Our experiments are seen to validate our theoretical findings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.11146">The Challenges of HTR Model Training: Feedback from the Project Donner le gout de l&#x27;archive a l&#x27;ere numerique. (arXiv:2212.11146v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Couture_B/0/1/0/all/0/1">Beatrice Couture</a> (Universit&#xe9; de Montr&#xe9;al), <a href="http://arxiv.org/find/cs/1/au:+Verret_F/0/1/0/all/0/1">Farah Verret</a> (Universit&#xe9; de Montr&#xe9;al), <a href="http://arxiv.org/find/cs/1/au:+Gohier_M/0/1/0/all/0/1">Maxime Gohier</a> (Universit&#xe9; du Qu&#xe9;bec &#xe0; Rimouski), <a href="http://arxiv.org/find/cs/1/au:+Deslandres_D/0/1/0/all/0/1">Dominique Deslandres</a> (Universit&#xe9; de Montr&#xe9;al)</p>
<p>The arrival of handwriting recognition technologies offers new possibilities
for research in heritage studies. However, it is now necessary to reflect on
the experiences and the practices developed by research teams. Our use of the
Transkribus platform since 2018 has led us to search for the most significant
ways to improve the performance of our handwritten text recognition (HTR)
models which are made to transcribe French handwriting dating from the 17th
century. This article therefore reports on the impacts of creating transcribing
protocols, using the language model at full scale and determining the best way
to use base models in order to help increase the performance of HTR models.
Combining all of these elements can indeed increase the performance of a single
model by more than 20% (reaching a Character Error Rate below 5%). This article
also discusses some challenges regarding the collaborative nature of HTR
platforms such as Transkribus and the way researchers can share their data
generated in the process of creating or training handwritten text recognition
models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.10743">Tighter Bounds on the Expressivity of Transformer Encoders. (arXiv:2301.10743v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chiang_D/0/1/0/all/0/1">David Chiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cholak_P/0/1/0/all/0/1">Peter Cholak</a>, <a href="http://arxiv.org/find/cs/1/au:+Pillay_A/0/1/0/all/0/1">Anand Pillay</a></p>
<p>Characterizing neural networks in terms of better-understood formal systems
has the potential to yield new insights into the power and limitations of these
networks. Doing so for transformers remains an active area of research.
Bhattamishra and others have shown that transformer encoders are at least as
expressive as a certain kind of counter machine, while Merrill and Sabharwal
have shown that fixed-precision transformer encoders recognize only languages
in uniform $TC^0$. We connect and strengthen these results by identifying a
variant of first-order logic with counting quantifiers that is simultaneously
an upper bound for fixed-precision transformer encoders and a lower bound for
transformer encoders. This brings us much closer than before to an exact
characterization of the languages that transformer encoders recognize.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.11975">Byte Pair Encoding for Symbolic Music. (arXiv:2301.11975v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fradet_N/0/1/0/all/0/1">Nathan Fradet</a>, <a href="http://arxiv.org/find/cs/1/au:+Gutowski_N/0/1/0/all/0/1">Nicolas Gutowski</a>, <a href="http://arxiv.org/find/cs/1/au:+Chhel_F/0/1/0/all/0/1">Fabien Chhel</a>, <a href="http://arxiv.org/find/cs/1/au:+Briot_J/0/1/0/all/0/1">Jean-Pierre Briot</a></p>
<p>When used with deep learning, the symbolic music modality is often coupled
with language model architectures. To do so, the music needs to be tokenized,
i.e. converted into a sequence of discrete tokens. This can be achieved by
different approaches, as music can be composed of simultaneous tracks, of
simultaneous notes with several attributes. Until now, the proposed
tokenizations rely on small vocabularies of tokens describing the note
attributes and time events, resulting in fairly long token sequences, and a
sub-optimal use of the embedding space of language models. Recent research has
put efforts on reducing the overall sequence length by merging embeddings or
combining tokens. In this paper, we show that Byte Pair Encoding, a compression
technique widely used for natural language, significantly decreases the
sequence length while increasing the vocabulary size. By doing so, we leverage
the embedding capabilities of such models with more expressive tokens,
resulting in both better results and faster inference in generation and
classification tasks. The source code is shared on Github, along with a
companion website. Finally, BPE is directly implemented in MidiTok, allowing
the reader to easily benefit from this method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.01602">A Feature Selection Method for Driver Stress Detection Using Heart Rate Variability and Breathing Rate. (arXiv:2302.01602v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Parsi_A/0/1/0/all/0/1">Ashkan Parsi</a>, <a href="http://arxiv.org/find/cs/1/au:+OCallaghan_D/0/1/0/all/0/1">David O&#x27;Callaghan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lemley_J/0/1/0/all/0/1">Joseph Lemley</a></p>
<p>Driver stress is a major cause of car accidents and death worldwide.
Furthermore, persistent stress is a health problem, contributing to
hypertension and other diseases of the cardiovascular system. Stress has a
measurable impact on heart and breathing rates and stress levels can be
inferred from such measurements. Galvanic skin response is a common test to
measure the perspiration caused by both physiological and psychological stress,
as well as extreme emotions. In this paper, galvanic skin response is used to
estimate the ground truth stress levels. A feature selection technique based on
the minimal redundancy-maximal relevance method is then applied to multiple
heart rate variability and breathing rate metrics to identify a novel and
optimal combination for use in detecting stress. The support vector machine
algorithm with a radial basis function kernel was used along with these
features to reliably predict stress. The proposed method has achieved a high
level of accuracy on the target dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.02392">Offline Minimax Soft-Q-learning Under Realizability and Partial Coverage. (arXiv:2302.02392v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Uehara_M/0/1/0/all/0/1">Masatoshi Uehara</a>, <a href="http://arxiv.org/find/cs/1/au:+Kallus_N/0/1/0/all/0/1">Nathan Kallus</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jason D. Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1">Wen Sun</a></p>
<p>In offline reinforcement learning (RL) we have no opportunity to explore so
we must make assumptions that the data is sufficient to guide picking a good
policy, taking the form of assuming some coverage, realizability, Bellman
completeness, and/or hard margin (gap). In this work we propose value-based
algorithms for offline RL with PAC guarantees under just partial coverage,
specifically, coverage of just a single comparator policy, and realizability of
soft (entropy-regularized) Q-function of the single policy and a related
function defined as a saddle point of certain minimax optimization problem.
This offers refined and generally more lax conditions for offline RL. We
further show an analogous result for vanilla Q-functions under a soft margin
condition. To attain these guarantees, we leverage novel minimax learning
algorithms to accurately estimate soft or vanilla Q-functions with
$L^2$-convergence guarantees. Our algorithms' loss functions arise from casting
the estimation problems as nonlinear convex optimization problems and
Lagrangifying.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.08624">InstructABSA: Instruction Learning for Aspect Based Sentiment Analysis. (arXiv:2302.08624v6 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Scaria_K/0/1/0/all/0/1">Kevin Scaria</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1">Himanshu Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Goyal_S/0/1/0/all/0/1">Siddharth Goyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Sawant_S/0/1/0/all/0/1">Saurabh Arjun Sawant</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1">Swaroop Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1">Chitta Baral</a></p>
<p>We introduce InstructABSA, an instruction learning paradigm for Aspect-Based
Sentiment Analysis (ABSA) subtasks. Our method introduces positive, negative,
and neutral examples to each training sample, and instruction tune the model
(Tk-Instruct) for ABSA subtasks, yielding significant performance improvements.
Experimental results on the Sem Eval 2014, 15, and 16 datasets demonstrate that
InstructABSA outperforms the previous state-of-the-art (SOTA) approaches on
Term Extraction (ATE), Sentiment Classification(ATSC) and Sentiment Pair
Extraction (ASPE) subtasks. In particular, InstructABSA outperforms the
previous state-of-the-art (SOTA) on the Rest14 ATE subtask by 5.69% points, the
Rest15 ATSC subtask by 9.59% points, and the Lapt14 AOPE subtask by 3.37%
points, surpassing 7x larger models. We also get competitive results on AOOE,
AOPE, and AOSTE subtasks indicating strong generalization ability to all
subtasks. Exploring sample efficiency reveals that just 50% train data is
required to get competitive results with other instruction tuning approaches.
Lastly, we assess the quality of instructions and observe that InstructABSA's
performance experiences a decline of ~10% when adding misleading examples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.09526">Mixed Semi-Supervised Generalized-Linear-Regression with applications to Deep-Learning and Interpolators. (arXiv:2302.09526v2 [stat.ME] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Yuval_O/0/1/0/all/0/1">Oren Yuval</a>, <a href="http://arxiv.org/find/stat/1/au:+Rosset_S/0/1/0/all/0/1">Saharon Rosset</a></p>
<p>We present a methodology for using unlabeled data to design semi supervised
learning (SSL) methods that improve the prediction performance of supervised
learning for regression tasks. The main idea is to design different mechanisms
for integrating the unlabeled data, and include in each of them a mixing
parameter $\alpha$, controlling the weight given to the unlabeled data.
Focusing on Generalized Linear Models (GLM) and linear interpolators classes of
models, we analyze the characteristics of different mixing mechanisms, and
prove that in all cases, it is invariably beneficial to integrate the unlabeled
data with some nonzero mixing ratio $\alpha&gt;0$, in terms of predictive
performance. Moreover, we provide a rigorous framework to estimate the best
mixing ratio $\alpha^*$ where mixed SSL delivers the best predictive
performance, while using the labeled and unlabeled data on hand.
</p>
<p>The effectiveness of our methodology in delivering substantial improvement
compared to the standard supervised models, in a variety of settings, is
demonstrated empirically through extensive simulation, in a manner that
supports the theoretical analysis. We also demonstrate the applicability of our
methodology (with some intuitive modifications) to improve more complex models,
such as deep neural networks, in real-world regression tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.12403">Plume: A Framework for High Performance Deep RL Network Controllers via Prioritized Trace Sampling. (arXiv:2302.12403v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1">Sagar Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Junyang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jyothi_S/0/1/0/all/0/1">Sangeetha Abdu Jyothi</a>, <a href="http://arxiv.org/find/cs/1/au:+Narodytska_N/0/1/0/all/0/1">Nina Narodytska</a></p>
<p>Deep Reinforcement Learning (DRL) has shown promise in various networking
environments. However, these environments present several fundamental
challenges for standard DRL techniques. They are difficult to explore and
exhibit high levels of noise and uncertainty. Although these challenges
complicate the training process, we find that in practice we can substantially
mitigate their effects and even achieve state-of-the-art real-world performance
by addressing a factor that has been previously overlooked: the skewed input
trace distribution in DRL training datasets.
</p>
<p>We introduce a generalized framework, Plume, to automatically identify and
balance the skew using a three-stage process. First, we identify the critical
features that determine the behavior of the traces. Second, we classify the
traces into clusters. Finally, we prioritize the salient clusters to improve
the overall performance of the controller. Plume seamlessly works across DRL
algorithms, without requiring any changes to the DRL workflow. We evaluated
Plume on three networking environments, including Adaptive Bitrate Streaming,
Congestion Control, and Load Balancing. Plume offers superior performance in
both simulation and real-world settings, across different controllers and DRL
algorithms. For example, our novel ABR controller, Gelato trained with Plume
consistently outperforms prior state-of-the-art controllers on the live
streaming platform Puffer for over a year. It is the first controller on the
platform to deliver statistically significant improvements in both video
quality and stalling, decreasing stalls by as much as 75%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.13262">Modulated Neural ODEs. (arXiv:2302.13262v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Auzina_I/0/1/0/all/0/1">Ilze Amanda Auzina</a>, <a href="http://arxiv.org/find/cs/1/au:+Yildiz_C/0/1/0/all/0/1">&#xc7;a&#x11f;atay Y&#x131;ld&#x131;z</a>, <a href="http://arxiv.org/find/cs/1/au:+Magliacane_S/0/1/0/all/0/1">Sara Magliacane</a>, <a href="http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1">Matthias Bethge</a>, <a href="http://arxiv.org/find/cs/1/au:+Gavves_E/0/1/0/all/0/1">Efstratios Gavves</a></p>
<p>Neural ordinary differential equations (NODEs) have been proven useful for
learning non-linear dynamics of arbitrary trajectories. However, current NODE
methods capture variations across trajectories only via the initial state value
or by auto-regressive encoder updates. In this work, we introduce Modulated
Neural ODEs (MoNODEs), a novel framework that sets apart dynamics states from
underlying static factors of variation and improves the existing NODE methods.
In particular, we introduce $\textit{time-invariant modulator variables}$ that
are learned from the data. We incorporate our proposed framework into four
existing NODE variants. We test MoNODE on oscillating systems, videos and human
walking trajectories, where each trajectory has trajectory-specific modulation.
Our framework consistently improves the existing model ability to generalize to
new dynamic parameterizations and to perform far-horizon forecasting. In
addition, we verify that the proposed modulator variables are informative of
the true unknown factors of variation as measured by $R^2$ scores.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.13608">DeepSeq: Deep Sequential Circuit Learning. (arXiv:2302.13608v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1">Sadaf Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1">Zhengyuan Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Min Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1">Qiang Xu</a></p>
<p>Circuit representation learning is a promising research direction in the
electronic design automation (EDA) field. With sufficient data for
pre-training, the learned general yet effective representation can help to
solve multiple downstream EDA tasks by fine-tuning it on a small set of
task-related data. However, existing solutions only target combinational
circuits, significantly limiting their applications. In this work, we propose
DeepSeq, a novel representation learning framework for sequential netlists.
Specifically, we introduce a dedicated graph neural network (GNN) with a
customized propagation scheme to exploit the temporal correlations between
gates in sequential circuits. To ensure effective learning, we propose to use a
multi-task training objective with two sets of strongly related supervision:
logic probability and transition probability at each node. A novel dual
attention aggregation mechanism is introduced to facilitate learning both tasks
efficiently. Experimental results on various benchmark circuits show that
DeepSeq outperforms other GNN models for sequential circuit learning. We
evaluate the generalization capability of DeepSeq on a downstream power
estimation task. After fine-tuning, DeepSeq can accurately estimate power
across various circuits under different workloads.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.14043">Single-Call Stochastic Extragradient Methods for Structured Non-monotone Variational Inequalities: Improved Analysis under Weaker Conditions. (arXiv:2302.14043v2 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Choudhury_S/0/1/0/all/0/1">Sayantan Choudhury</a>, <a href="http://arxiv.org/find/math/1/au:+Gorbunov_E/0/1/0/all/0/1">Eduard Gorbunov</a>, <a href="http://arxiv.org/find/math/1/au:+Loizou_N/0/1/0/all/0/1">Nicolas Loizou</a></p>
<p>Single-call stochastic extragradient methods, like stochastic past
extragradient (SPEG) and stochastic optimistic gradient (SOG), have gained a
lot of interest in recent years and are one of the most efficient algorithms
for solving large-scale min-max optimization and variational inequalities
problems (VIP) appearing in various machine learning tasks. However, despite
their undoubted popularity, current convergence analyses of SPEG and SOG
require a bounded variance assumption. In addition, several important questions
regarding the convergence properties of these methods are still open, including
mini-batching, efficient step-size selection, and convergence guarantees under
different sampling strategies. In this work, we address these questions and
provide convergence guarantees for two large classes of structured non-monotone
VIPs: (i) quasi-strongly monotone problems (a generalization of strongly
monotone problems) and (ii) weak Minty variational inequalities (a
generalization of monotone and Minty VIPs). We introduce the expected residual
condition, explain its benefits, and show how it can be used to obtain a
strictly weaker bound than previously used growth conditions, expected
co-coercivity, or bounded variance assumptions. Equipped with this condition,
we provide theoretical guarantees for the convergence of single-call
extragradient methods for different step-size selections, including constant,
decreasing, and step-size-switching rules. Furthermore, our convergence
analysis holds under the arbitrary sampling paradigm, which includes importance
sampling and various mini-batching strategies as special cases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.04878">DeepGD: A Multi-Objective Black-Box Test Selection Approach for Deep Neural Networks. (arXiv:2303.04878v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aghababaeyan_Z/0/1/0/all/0/1">Zohreh Aghababaeyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdellatif_M/0/1/0/all/0/1">Manel Abdellatif</a>, <a href="http://arxiv.org/find/cs/1/au:+Dadkhah_M/0/1/0/all/0/1">Mahboubeh Dadkhah</a>, <a href="http://arxiv.org/find/cs/1/au:+Briand_L/0/1/0/all/0/1">Lionel Briand</a></p>
<p>Deep neural networks (DNNs) are widely used in various application domains
such as image processing, speech recognition, and natural language processing.
However, testing DNN models may be challenging due to the complexity and size
of their input domain. Particularly, testing DNN models often requires
generating or exploring large unlabeled datasets. In practice, DNN test
oracles, which identify the correct outputs for inputs, often require expensive
manual effort to label test data, possibly involving multiple experts to ensure
labeling correctness. In this paper, we propose DeepGD, a black-box
multi-objective test selection approach for DNN models. It reduces the cost of
labeling by prioritizing the selection of test inputs with high fault revealing
power from large unlabeled datasets. DeepGD not only selects test inputs with
high uncertainty scores to trigger as many mispredicted inputs as possible but
also maximizes the probability of revealing distinct faults in the DNN model by
selecting diverse mispredicted inputs. The experimental results conducted on
four widely used datasets and five DNN models show that in terms of
fault-revealing ability: (1) White-box, coverage-based approaches fare poorly,
(2) DeepGD outperforms existing black-box test selection approaches in terms of
fault detection, and (3) DeepGD also leads to better guidance for DNN model
retraining when using selected inputs to augment the training set.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.06234">Optimal and Private Learning from Human Response Data. (arXiv:2303.06234v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1">Duc Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1">Anderson Y. Zhang</a></p>
<p>Item response theory (IRT) is the study of how people make probabilistic
decisions, with diverse applications in education testing, recommendation
systems, among others. The Rasch model of binary response data, one of the most
fundamental models in IRT, remains an active area of research with important
practical significance. Recently, Nguyen and Zhang (2022) proposed a new
spectral estimation algorithm that is efficient and accurate. In this work, we
extend their results in two important ways. Firstly, we obtain a refined
entrywise error bound for the spectral algorithm, complementing the `average
error' $\ell_2$ bound in their work. Notably, under mild sampling conditions,
the spectral algorithm achieves the minimax optimal error bound (modulo a log
factor). Building on the refined analysis, we also show that the spectral
algorithm enjoys optimal sample complexity for top-$K$ recovery (e.g.,
identifying the best $K$ items from approval/disapproval response data),
explaining the empirical findings in the previous work. Our second contribution
addresses an important but understudied topic in IRT: privacy. Despite the
human-centric applications of IRT, there has not been any proposed
privacy-preserving mechanism in the literature. We develop a private extension
of the spectral algorithm, leveraging its unique Markov chain formulation and
the discrete Gaussian mechanism (Canonne et al., 2020). Experiments show that
our approach is significantly more accurate than the baselines in the
low-to-moderate privacy regime.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.06532">Automated Design of Metaheuristic Algorithms: A Survey. (arXiv:2303.06532v2 [cs.NE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1">Qi Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Duan_Q/0/1/0/all/0/1">Qiqi Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1">Bai Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1">Shi Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yuhui Shi</a></p>
<p>Metaheuristics have gained great success in academia and practice because
their search logic can be applied to any problem with available solution
representation, solution quality evaluation, and certain notions of locality.
Manually designing metaheuristic algorithms for solving a target problem is
criticized for being laborious, error-prone, and requiring intensive
specialized knowledge. This gives rise to increasing interest in automated
design of metaheuristic algorithms. With computing power to fully explore
potential design choices, the automated design could reach and even surpass
human-level design and could make high-performance algorithms accessible to a
much wider range of researchers and practitioners. This paper presents a broad
picture of automated design of metaheuristic algorithms, by conducting a survey
on the common grounds and representative techniques in terms of design space,
design strategies, performance evaluation strategies, and target problems in
this field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.07271">Provably Convergent Plug-and-Play Quasi-Newton Methods. (arXiv:2303.07271v4 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Tan_H/0/1/0/all/0/1">Hong Ye Tan</a>, <a href="http://arxiv.org/find/math/1/au:+Mukherjee_S/0/1/0/all/0/1">Subhadip Mukherjee</a>, <a href="http://arxiv.org/find/math/1/au:+Tang_J/0/1/0/all/0/1">Junqi Tang</a>, <a href="http://arxiv.org/find/math/1/au:+Schonlieb_C/0/1/0/all/0/1">Carola-Bibiane Sch&#xf6;nlieb</a></p>
<p>Plug-and-Play (PnP) methods are a class of efficient iterative methods that
aim to combine data fidelity terms and deep denoisers using classical
optimization algorithms, such as ISTA or ADMM, with applications in inverse
problems and imaging. Provable PnP methods are a subclass of PnP methods with
convergence guarantees, such as fixed point convergence or convergence to
critical points of some energy function. Many existing provable PnP methods
impose heavy restrictions on the denoiser or fidelity function, such as
non-expansiveness or strict convexity, respectively. In this work, we propose a
novel algorithmic approach incorporating quasi-Newton steps into a provable PnP
framework based on proximal denoisers, resulting in greatly accelerated
convergence while retaining light assumptions on the denoiser. By
characterizing the denoiser as the proximal operator of a weakly convex
function, we show that the fixed points of the proposed quasi-Newton PnP
algorithm are critical points of a weakly convex function. Numerical
experiments on image deblurring and super-resolution demonstrate 2--8x faster
convergence as compared to other provable PnP methods with similar
reconstruction quality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.09447">Steering Prototypes with Prompt-tuning for Rehearsal-free Continual Learning. (arXiv:2303.09447v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhuowei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1">Long Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zizhao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Han Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1">Di Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Ting Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1">Dimitris N. Metaxas</a></p>
<p>In the context of continual learning, prototypes-as representative class
embeddings-offer advantages in memory conservation and the mitigation of
catastrophic forgetting. However, challenges related to semantic drift and
prototype interference persist. In this study, we introduce the Contrastive
Prototypical Prompt (CPP) approach. Through task-specific prompt-tuning,
underpinned by a contrastive learning objective, we effectively address both
aforementioned challenges. Our evaluations on four challenging
class-incremental benchmarks reveal that CPP achieves a significant 4% to 6%
improvement over state-of-the-art methods. Importantly, CPP operates without a
rehearsal buffer and narrows the performance divergence between continual and
offline joint-learning, suggesting an innovative scheme for Transformer-based
continual learning systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.10180">Towards Real-World Applications of Personalized Anesthesia Using Policy Constraint Q Learning for Propofol Infusion Control. (arXiv:2303.10180v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1">Xiuding Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yaoyao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Beimin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1">Yu Yao</a></p>
<p>Automated anesthesia promises to enable more precise and personalized
anesthetic administration and free anesthesiologists from repetitive tasks,
allowing them to focus on the most critical aspects of a patient's surgical
care. Current research has typically focused on creating simulated environments
from which agents can learn. These approaches have demonstrated good
experimental results, but are still far from clinical application. In this
paper, Policy Constraint Q-Learning (PCQL), a data-driven reinforcement
learning algorithm for solving the problem of learning anesthesia strategies on
real clinical datasets, is proposed. Conservative Q-Learning was first
introduced to alleviate the problem of Q function overestimation in an offline
context. A policy constraint term is added to agent training to keep the policy
distribution of the agent and the anesthesiologist consistent to ensure safer
decisions made by the agent in anesthesia scenarios. The effectiveness of PCQL
was validated by extensive experiments on a real clinical anesthesia dataset.
Experimental results show that PCQL is predicted to achieve higher gains than
the baseline approach while maintaining good agreement with the reference dose
given by the anesthesiologist, using less total dose, and being more responsive
to the patient's vital signs. In addition, the confidence intervals of the
agent were investigated, which were able to cover most of the clinical
decisions of the anesthesiologist. Finally, an interpretable method, SHAP, was
used to analyze the contributing components of the model predictions to
increase the transparency of the model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.12816">From Wide to Deep: Dimension Lifting Network for Parameter-efficient Knowledge Graph Embedding. (arXiv:2303.12816v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cai_B/0/1/0/all/0/1">Borui Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1">Yong Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1">Longxiang Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1">Di Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">He Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1">Jiong Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Luan_T/0/1/0/all/0/1">Tom Luan</a></p>
<p>Knowledge graph embedding (KGE) that maps entities and relations into vector
representations is essential for downstream applications. Conventional KGE
methods require high-dimensional representations to learn the complex structure
of knowledge graph, but lead to oversized model parameters. Recent advances
reduce parameters by low-dimensional entity representations, while developing
techniques (e.g., knowledge distillation or reinvented representation forms) to
compensate for reduced dimension. However, such operations introduce
complicated computations and model designs that may not benefit large knowledge
graphs. To seek a simple strategy to improve the parameter efficiency of
conventional KGE models, we take inspiration from that deeper neural networks
require exponentially fewer parameters to achieve expressiveness comparable to
wider networks for compositional structures. We view all entity representations
as a single-layer embedding network, and conventional KGE methods that adopt
high-dimensional entity representations equal widening the embedding network to
gain expressiveness. To achieve parameter efficiency, we instead propose a
deeper embedding network for entity representations, i.e., a narrow entity
embedding layer plus a multi-layer dimension lifting network (LiftNet).
Experiments on three public datasets show that by integrating LiftNet, four
conventional KGE methods with 16-dimensional representations achieve comparable
link prediction accuracy as original models that adopt 512-dimensional
representations, saving 68.4% to 96.9% parameters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.13525">Forecasting Workload in Cloud Computing: Towards Uncertainty-Aware Predictions and Transfer Learning. (arXiv:2303.13525v2 [cs.DC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rossi_A/0/1/0/all/0/1">Andrea Rossi</a>, <a href="http://arxiv.org/find/cs/1/au:+Visentin_A/0/1/0/all/0/1">Andrea Visentin</a>, <a href="http://arxiv.org/find/cs/1/au:+Carraro_D/0/1/0/all/0/1">Diego Carraro</a>, <a href="http://arxiv.org/find/cs/1/au:+Prestwich_S/0/1/0/all/0/1">Steven Prestwich</a>, <a href="http://arxiv.org/find/cs/1/au:+Brown_K/0/1/0/all/0/1">Kenneth N. Brown</a></p>
<p>Predicting future resource demand in Cloud Computing is essential for
optimizing the trade-off between serving customers' requests efficiently and
minimizing the provisioning cost. Modelling prediction uncertainty is also
desirable to better inform the resource decision-making process, but research
in this field is under-investigated. In this paper, we propose univariate and
bivariate Bayesian deep learning models that provide predictions of future
workload demand and its uncertainty. We run extensive experiments on Google and
Alibaba clusters, where we first train our models with datasets from different
cloud providers and compare them with LSTM-based baselines. Results show that
modelling the uncertainty of predictions has a positive impact on performance,
especially on service level metrics, because uncertainty quantification can be
tailored to desired target service levels that are critical in cloud
applications. Moreover, we investigate whether our models benefit transfer
learning capabilities across different domains, i.e. dataset distributions.
Experiments on the same workload datasets reveal that acceptable transfer
learning performance can be achieved within the same provider (because
distributions are more similar). Also, domain knowledge does not transfer when
the source and target domains are very different (e.g. from different
providers), but this performance degradation can be mitigated by increasing the
training set size of the source domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.01203">Optimal Goal-Reaching Reinforcement Learning via Quasimetric Learning. (arXiv:2304.01203v6 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tongzhou Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1">Antonio Torralba</a>, <a href="http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1">Phillip Isola</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1">Amy Zhang</a></p>
<p>In goal-reaching reinforcement learning (RL), the optimal value function has
a particular geometry, called quasimetric structure. This paper introduces
Quasimetric Reinforcement Learning (QRL), a new RL method that utilizes
quasimetric models to learn optimal value functions. Distinct from prior
approaches, the QRL objective is specifically designed for quasimetrics, and
provides strong theoretical recovery guarantees. Empirically, we conduct
thorough analyses on a discretized MountainCar environment, identifying
properties of QRL and its advantages over alternatives. On offline and online
goal-reaching benchmarks, QRL also demonstrates improved sample efficiency and
performance, across both state-based and image-based observations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.02725">FMG-Net and W-Net: Multigrid Inspired Deep Learning Architectures For Medical Imaging Segmentation. (arXiv:2304.02725v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Celaya_A/0/1/0/all/0/1">Adrian Celaya</a>, <a href="http://arxiv.org/find/eess/1/au:+Riviere_B/0/1/0/all/0/1">Beatrice Riviere</a>, <a href="http://arxiv.org/find/eess/1/au:+Fuentes_D/0/1/0/all/0/1">David Fuentes</a></p>
<p>Accurate medical imaging segmentation is critical for precise and effective
medical interventions. However, despite the success of convolutional neural
networks (CNNs) in medical image segmentation, they still face challenges in
handling fine-scale features and variations in image scales. These challenges
are particularly evident in complex and challenging segmentation tasks, such as
the BraTS multi-label brain tumor segmentation challenge. In this task,
accurately segmenting the various tumor sub-components, which vary
significantly in size and shape, remains a significant challenge, with even
state-of-the-art methods producing substantial errors. Therefore, we propose
two architectures, FMG-Net and W-Net, that incorporate the principles of
geometric multigrid methods for solving linear systems of equations into CNNs
to address these challenges. Our experiments on the BraTS 2020 dataset
demonstrate that both FMG-Net and W-Net outperform the widely used U-Net
architecture regarding tumor subcomponent segmentation accuracy and training
efficiency. These findings highlight the potential of incorporating the
principles of multigrid methods into CNNs to improve the accuracy and
efficiency of medical imaging segmentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.09402">MixPro: Simple yet Effective Data Augmentation for Prompt-based Learning. (arXiv:2304.09402v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bohan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Dou_L/0/1/0/all/0/1">Longxu Dou</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1">Yutai Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yunlong Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Mu_H/0/1/0/all/0/1">Honglin Mu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1">Qingfu Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1">Qinghua Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1">Wanxiang Che</a></p>
<p>Prompt-based learning has shown considerable promise in reformulating various
downstream tasks as cloze problems by combining original input with a
predetermined template. This approach demonstrates its effectiveness,
especially in few-shot learning scenarios, where the model is trained on a
scarce amount of data. Despite its successes, the limited templates and text in
few-shot prompt-based learning scenarios leave significant room for performance
improvement. Moreover, existing methods sometimes resort to model ensembles,
which, while effective, could potentially hamper model efficiency due to
increased computational demands. To address these issues, we introduce MixPro,
an augmentation method designed to augment both the vanilla input text and the
templates. We implement this through the token-level, the sentence-level, and
the template-level Mixup strategies. The experimental results on five few-shot
datasets show that MixPro outperforms other augmentation baselines, improving
model performance by an average of 5.08% compared to before augmentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.03626">Verifiable Learning for Robust Tree Ensembles. (arXiv:2305.03626v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Calzavara_S/0/1/0/all/0/1">Stefano Calzavara</a>, <a href="http://arxiv.org/find/cs/1/au:+Cazzaro_L/0/1/0/all/0/1">Lorenzo Cazzaro</a>, <a href="http://arxiv.org/find/cs/1/au:+Pibiri_G/0/1/0/all/0/1">Giulio Ermanno Pibiri</a>, <a href="http://arxiv.org/find/cs/1/au:+Prezza_N/0/1/0/all/0/1">Nicola Prezza</a></p>
<p>Verifying the robustness of machine learning models against evasion attacks
at test time is an important research problem. Unfortunately, prior work
established that this problem is NP-hard for decision tree ensembles, hence
bound to be intractable for specific inputs. In this paper, we identify a
restricted class of decision tree ensembles, called large-spread ensembles,
which admit a security verification algorithm running in polynomial time. We
then propose a new approach called verifiable learning, which advocates the
training of such restricted model classes which are amenable for efficient
verification. We show the benefits of this idea by designing a new training
algorithm that automatically learns a large-spread decision tree ensemble from
labelled data, thus enabling its security verification in polynomial time.
Experimental results on public datasets confirm that large-spread ensembles
trained using our algorithm can be verified in a matter of seconds, using
standard commercial hardware. Moreover, large-spread ensembles are more robust
than traditional ensembles against evasion attacks, at the cost of an
acceptable loss of accuracy in the non-adversarial setting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.11650">Moment Matching Denoising Gibbs Sampling. (arXiv:2305.11650v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Zhang_M/0/1/0/all/0/1">Mingtian Zhang</a>, <a href="http://arxiv.org/find/stat/1/au:+Hawkins_Hooker_A/0/1/0/all/0/1">Alex Hawkins-Hooker</a>, <a href="http://arxiv.org/find/stat/1/au:+Paige_B/0/1/0/all/0/1">Brooks Paige</a>, <a href="http://arxiv.org/find/stat/1/au:+Barber_D/0/1/0/all/0/1">David Barber</a></p>
<p>Energy-Based Models (EBMs) offer a versatile framework for modeling complex
data distributions. However, training and sampling from EBMs continue to pose
significant challenges. The widely-used Denoising Score Matching (DSM) method
for scalable EBM training suffers from inconsistency issues, causing the energy
model to learn a `noisy' data distribution. In this work, we propose an
efficient sampling framework: (pseudo)-Gibbs sampling with moment matching,
which enables effective sampling from the underlying clean model when given a
`noisy' model that has been well-trained via DSM. We explore the benefits of
our approach compared to related methods and demonstrate how to scale the
method to high-dimensional datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.12082">SneakyPrompt: Jailbreaking Text-to-image Generative Models. (arXiv:2305.12082v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yuchen Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hui_B/0/1/0/all/0/1">Bo Hui</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1">Haolin Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1">Neil Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yinzhi Cao</a></p>
<p>Text-to-image generative models such as Stable Diffusion and DALL$\cdot$E
raise many ethical concerns due to the generation of harmful images such as
Not-Safe-for-Work (NSFW) ones. To address these ethical concerns, safety
filters are often adopted to prevent the generation of NSFW images. In this
work, we propose SneakyPrompt, the first automated attack framework, to
jailbreak text-to-image generative models such that they generate NSFW images
even if safety filters are adopted. Given a prompt that is blocked by a safety
filter, SneakyPrompt repeatedly queries the text-to-image generative model and
strategically perturbs tokens in the prompt based on the query results to
bypass the safety filter. Specifically, SneakyPrompt utilizes reinforcement
learning to guide the perturbation of tokens. Our evaluation shows that
SneakyPrompt successfully jailbreaks DALL$\cdot$E 2 with closed-box safety
filters to generate NSFW images. Moreover, we also deploy several
state-of-the-art, open-source safety filters on a Stable Diffusion model. Our
evaluation shows that SneakyPrompt not only successfully generates NSFW images,
but also outperforms existing text adversarial attacks when extended to
jailbreak text-to-image generative models, in terms of both the number of
queries and qualities of the generated NSFW images. SneakyPrompt is open-source
and available at this repository:
\url{https://github.com/Yuchen413/text2image_safety}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.12433">ParticleWNN: a Novel Neural Networks Framework for Solving Partial Differential Equations. (arXiv:2305.12433v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zang_Y/0/1/0/all/0/1">Yaohua Zang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bao_G/0/1/0/all/0/1">Gang Bao</a></p>
<p>Deep neural networks (DNNs) have been widely used to solve partial
differential equations (PDEs) in recent years. In this work, a novel deep
learning-based framework named Particle Weak-form based Neural Networks
(ParticleWNN) is developed for solving PDEs in the weak form. In this
framework, the trial space is defined as the space of DNNs, while the test
space consists of functions compactly supported in extremely small regions,
centered around particles. To facilitate the training of neural networks, an
R-adaptive strategy is designed to adaptively modify the radius of regions
during training. The ParticleWNN inherits the benefits of weak/variational
formulation, requiring less regularity of the solution and a small number of
quadrature points for computing integrals. Additionally, due to the special
construction of the test functions, ParticleWNN enables parallel implementation
and integral calculations only in extremely small regions. This framework is
particularly desirable for solving problems with high-dimensional and complex
domains. The efficiency and accuracy of ParticleWNN are demonstrated through
several numerical examples, showcasing its superiority over state-of-the-art
methods. The source code for the numerical examples presented in this paper is
available at https://github.com/yaohua32/ParticleWNN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.12751">Testing of Deep Reinforcement Learning Agents with Surrogate Models. (arXiv:2305.12751v2 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Biagiola_M/0/1/0/all/0/1">Matteo Biagiola</a>, <a href="http://arxiv.org/find/cs/1/au:+Tonella_P/0/1/0/all/0/1">Paolo Tonella</a></p>
<p>Deep Reinforcement Learning (DRL) has received a lot of attention from the
research community in recent years. As the technology moves away from game
playing to practical contexts, such as autonomous vehicles and robotics, it is
crucial to evaluate the quality of DRL agents. In this paper, we propose a
search-based approach to test such agents. Our approach, implemented in a tool
called Indago, trains a classifier on failure and non-failure environment
(i.e., pass) configurations resulting from the DRL training process. The
classifier is used at testing time as a surrogate model for the DRL agent
execution in the environment, predicting the extent to which a given
environment configuration induces a failure of the DRL agent under test. The
failure prediction acts as a fitness function, guiding the generation towards
failure environment configurations, while saving computation time by deferring
the execution of the DRL agent in the environment to those configurations that
are more likely to expose failures. Experimental results show that our
search-based approach finds 50% more failures of the DRL agent than
state-of-the-art techniques. Moreover, such failures are, on average, 78% more
diverse; similarly, the behaviors of the DRL agent induced by failure
configurations are 74% more diverse.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13169">A Pretrainer&#x27;s Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, &amp; Toxicity. (arXiv:2305.13169v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Longpre_S/0/1/0/all/0/1">Shayne Longpre</a>, <a href="http://arxiv.org/find/cs/1/au:+Yauney_G/0/1/0/all/0/1">Gregory Yauney</a>, <a href="http://arxiv.org/find/cs/1/au:+Reif_E/0/1/0/all/0/1">Emily Reif</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Katherine Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1">Adam Roberts</a>, <a href="http://arxiv.org/find/cs/1/au:+Zoph_B/0/1/0/all/0/1">Barret Zoph</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1">Denny Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1">Jason Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Robinson_K/0/1/0/all/0/1">Kevin Robinson</a>, <a href="http://arxiv.org/find/cs/1/au:+Mimno_D/0/1/0/all/0/1">David Mimno</a>, <a href="http://arxiv.org/find/cs/1/au:+Ippolito_D/0/1/0/all/0/1">Daphne Ippolito</a></p>
<p>Pretraining is the preliminary and fundamental step in developing capable
language models (LM). Despite this, pretraining data design is critically
under-documented and often guided by empirically unsupported intuitions. To
address this, we pretrain 28 1.5B parameter decoder-only models, training on
data curated (1) at different times, (2) with varying toxicity and quality
filters, and (3) with different domain compositions. First, we quantify the
effect of pretraining data age. A temporal shift between evaluation data and
pretraining data leads to performance degradation, which is not overcome by
finetuning. Second, we explore the effect of quality and toxicity filters,
showing a trade-off between performance on standard benchmarks and risk of
toxic generations. Our findings indicate there does not exist a
one-size-fits-all solution to filtering training data. We also find that the
effects of different types of filtering are not predictable from text domain
characteristics. Lastly, we empirically validate that the inclusion of
heterogeneous data sources, like books and web, is broadly beneficial and
warrants greater prioritization. These findings constitute the largest set of
experiments to validate, quantify, and expose many undocumented intuitions
about text pretraining, which we hope will help support more informed
data-centric decisions in LM development.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14053">Parts of Speech-Grounded Subspaces in Vision-Language Models. (arXiv:2305.14053v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oldfield_J/0/1/0/all/0/1">James Oldfield</a>, <a href="http://arxiv.org/find/cs/1/au:+Tzelepis_C/0/1/0/all/0/1">Christos Tzelepis</a>, <a href="http://arxiv.org/find/cs/1/au:+Panagakis_Y/0/1/0/all/0/1">Yannis Panagakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Nicolaou_M/0/1/0/all/0/1">Mihalis A. Nicolaou</a>, <a href="http://arxiv.org/find/cs/1/au:+Patras_I/0/1/0/all/0/1">Ioannis Patras</a></p>
<p>Latent image representations arising from vision-language models have proved
immensely useful for a variety of downstream tasks. However, their utility is
limited by their entanglement with respect to different visual attributes. For
instance, recent work has shown that CLIP image representations are often
biased toward specific visual properties (such as objects or actions) in an
unpredictable manner. In this paper, we propose to separate representations of
the different visual modalities in CLIP's joint vision-language space by
leveraging the association between parts of speech and specific visual modes of
variation (e.g. nouns relate to objects, adjectives describe appearance). This
is achieved by formulating an appropriate component analysis model that learns
subspaces capturing variability corresponding to a specific part of speech,
while jointly minimising variability to the rest. Such a subspace yields
disentangled representations of the different visual properties of an image or
text in closed form while respecting the underlying geometry of the manifold on
which the representations lie. What's more, we show the proposed model
additionally facilitates learning subspaces corresponding to specific visual
appearances (e.g. artists' painting styles), which enables the selective
removal of entire visual themes from CLIP-based text-to-image synthesis. We
validate the model both qualitatively, by visualising the subspace projections
with a text-to-image model and by preventing the imitation of artists' styles,
and quantitatively, through class invariance metrics and improvements to
baseline zero-shot classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15616">Reversible and irreversible bracket-based dynamics for deep graph neural networks. (arXiv:2305.15616v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gruber_A/0/1/0/all/0/1">Anthony Gruber</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kookjin Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Trask_N/0/1/0/all/0/1">Nathaniel Trask</a></p>
<p>Recent works have shown that physics-inspired architectures allow the
training of deep graph neural networks (GNNs) without oversmoothing. The role
of these physics is unclear, however, with successful examples of both
reversible (e.g., Hamiltonian) and irreversible (e.g., diffusion) phenomena
producing comparable results despite diametrically opposed mechanisms, and
further complications arising due to empirical departures from mathematical
theory. This work presents a series of novel GNN architectures based upon
structure-preserving bracket-based dynamical systems, which are provably
guaranteed to either conserve energy or generate positive dissipation with
increasing depth. It is shown that the theoretically principled framework
employed here allows for inherently explainable constructions, which
contextualize departures from theory in current architectures and better
elucidate the roles of reversibility and irreversibility in network
performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15889">Quantitatively Measuring and Contrastively Exploring Heterogeneity for Domain Generalization. (arXiv:2305.15889v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1">Yunze Tong</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1">Junkun Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Min Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1">Didi Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Keli Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1">Fei Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1">Kun Kuang</a></p>
<p>Domain generalization (DG) is a prevalent problem in real-world applications,
which aims to train well-generalized models for unseen target domains by
utilizing several source domains. Since domain labels, i.e., which domain each
data point is sampled from, naturally exist, most DG algorithms treat them as a
kind of supervision information to improve the generalization performance.
However, the original domain labels may not be the optimal supervision signal
due to the lack of domain heterogeneity, i.e., the diversity among domains. For
example, a sample in one domain may be closer to another domain, its original
label thus can be the noise to disturb the generalization learning. Although
some methods try to solve it by re-dividing domains and applying the newly
generated dividing pattern, the pattern they choose may not be the most
heterogeneous due to the lack of the metric for heterogeneity. In this paper,
we point out that domain heterogeneity mainly lies in variant features under
the invariant learning framework. With contrastive learning, we propose a
learning potential-guided metric for domain heterogeneity by promoting learning
variant features. Then we notice the differences between seeking variance-based
heterogeneity and training invariance-based generalizable model. We thus
propose a novel method called Heterogeneity-based Two-stage Contrastive
Learning (HTCL) for the DG task. In the first stage, we generate the most
heterogeneous dividing pattern with our contrastive metric. In the second
stage, we employ an invariance-aimed contrastive learning by re-building pairs
with the stable relation hinted by domains and classes, which better utilizes
generated domain labels for generalization learning. Extensive experiments show
HTCL better digs heterogeneity and yields great generalization performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16963">Semantic segmentation of sparse irregular point clouds for leaf/wood discrimination. (arXiv:2305.16963v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1">Yuchen Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Durand_J/0/1/0/all/0/1">Jean-Baptiste Durand</a>, <a href="http://arxiv.org/find/cs/1/au:+Forbes_F/0/1/0/all/0/1">Florence Forbes</a>, <a href="http://arxiv.org/find/cs/1/au:+Vincent_G/0/1/0/all/0/1">Gr&#xe9;goire Vincent</a></p>
<p>LiDAR (Light Detection and Ranging) has become an essential part of the
remote sensing toolbox used for biosphere monitoring. In particular, LiDAR
provides the opportunity to map forest leaf area with unprecedented accuracy,
while leaf area has remained an important source of uncertainty affecting
models of gas exchanges between the vegetation and the atmosphere. Unmanned
Aerial Vehicles (UAV) are easy to mobilize and therefore allow frequent
revisits to track the response of vegetation to climate change. However,
miniature sensors embarked on UAVs usually provide point clouds of limited
density, which are further affected by a strong decrease in density from top to
bottom of the canopy due to progressively stronger occlusion. In such a
context, discriminating leaf points from wood points presents a significant
challenge due in particular to strong class imbalance and spatially irregular
sampling intensity. Here we introduce a neural network model based on the
Pointnet ++ architecture which makes use of point geometry only (excluding any
spectral information). To cope with local data sparsity, we propose an
innovative sampling scheme which strives to preserve local important geometric
information. We also propose a loss function adapted to the severe class
imbalance. We show that our model outperforms state-of-the-art alternatives on
UAV point clouds. We discuss future possible improvements, particularly
regarding much denser point clouds acquired from below the canopy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.17600">NashFormer: Leveraging Local Nash Equilibria for Semantically Diverse Trajectory Prediction. (arXiv:2305.17600v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lidard_J/0/1/0/all/0/1">Justin Lidard</a>, <a href="http://arxiv.org/find/cs/1/au:+So_O/0/1/0/all/0/1">Oswin So</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yanxia Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+DeCastro_J/0/1/0/all/0/1">Jonathan DeCastro</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1">Xiongyi Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuo_Y/0/1/0/all/0/1">Yen-Ling Kuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Leonard_J/0/1/0/all/0/1">John Leonard</a>, <a href="http://arxiv.org/find/cs/1/au:+Balachandran_A/0/1/0/all/0/1">Avinash Balachandran</a>, <a href="http://arxiv.org/find/cs/1/au:+Leonard_N/0/1/0/all/0/1">Naomi Leonard</a>, <a href="http://arxiv.org/find/cs/1/au:+Rosman_G/0/1/0/all/0/1">Guy Rosman</a></p>
<p>Interactions between road agents present a significant challenge in
trajectory prediction, especially in cases involving multiple agents. Because
existing diversity-aware predictors do not account for the interactive nature
of multi-agent predictions, they may miss these important interaction outcomes.
In this paper, we propose NashFormer, a framework for trajectory prediction
that leverages game-theoretic inverse reinforcement learning to improve
coverage of multi-modal predictions. We use a training-time game-theoretic
analysis as an auxiliary loss resulting in improved coverage and accuracy
without presuming a taxonomy of actions for the agents. We demonstrate our
approach on the interactive split of the Waymo Open Motion Dataset, including
four subsets involving scenarios with high interaction complexity. Experiment
results show that our predictor produces accurate predictions while covering
$33\%$ more potential interactions versus a baseline model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.18414">StEik: Stabilizing the Optimization of Neural Signed Distance Functions and Finer Shape Representation. (arXiv:2305.18414v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Huizong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yuxin Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Sundaramoorthi_G/0/1/0/all/0/1">Ganesh Sundaramoorthi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yezzi_A/0/1/0/all/0/1">Anthony Yezzi</a></p>
<p>We present new insights and a novel paradigm (StEik) for learning implicit
neural representations (INR) of shapes. In particular, we shed light on the
popular eikonal loss used for imposing a signed distance function constraint in
INR. We show analytically that as the representation power of the network
increases, the optimization approaches a partial differential equation (PDE) in
the continuum limit that is unstable. We show that this instability can
manifest in existing network optimization, leading to irregularities in the
reconstructed surface and/or convergence to sub-optimal local minima, and thus
fails to capture fine geometric and topological structure. We show analytically
how other terms added to the loss, currently used in the literature for other
purposes, can actually eliminate these instabilities. However, such terms can
over-regularize the surface, preventing the representation of fine shape
detail. Based on a similar PDE theory for the continuum limit, we introduce a
new regularization term that still counteracts the eikonal instability but
without over-regularizing. Furthermore, since stability is now guaranteed in
the continuum limit, this stabilization also allows for considering new network
structures that are able to represent finer shape detail. We introduce such a
structure based on quadratic layers. Experiments on multiple benchmark data
sets show that our new regularization and network are able to capture more
precise shape details and more accurate topology than existing
state-of-the-art.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.18427">Interpretable Reward Redistribution in Reinforcement Learning: A Causal Approach. (arXiv:2305.18427v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yudi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1">Yali Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1">Biwei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Ziyan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1">Meng Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1">Mykola Pechenizkiy</a></p>
<p>A major challenge in reinforcement learning is to determine which
state-action pairs are responsible for future rewards that are delayed. Reward
redistribution serves as a solution to re-assign credits for each time step
from observed sequences. While the majority of current approaches construct the
reward redistribution in an uninterpretable manner, we propose to explicitly
model the contributions of state and action from a causal perspective,
resulting in an interpretable reward redistribution and preserving policy
invariance. In this paper, we start by studying the role of causal generative
models in reward redistribution by characterizing the generation of Markovian
rewards and trajectory-wise long-term return and further propose a framework,
called Generative Return Decomposition (GRD), for policy optimization in
delayed reward scenarios. Specifically, GRD first identifies the unobservable
Markovian rewards and causal relations in the generative process. Then, GRD
makes use of the identified causal generative model to form a compact
representation to train policy over the most favorable subspace of the state
space of the agent. Theoretically, we show that the unobservable Markovian
reward function is identifiable, as well as the underlying causal structure and
causal models. Experimental results show that our method outperforms
state-of-the-art methods and the provided visualization further demonstrates
the interpretability of our method. The project page is located at
https://reedzyd.github.io/GenerativeReturnDecomposition/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.18450">GBG++: A Fast and Stable Granular Ball Generation Method for Classification. (arXiv:2305.18450v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1">Qin Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qinghua Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1">Shuyin Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1">Fan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1">Chengying Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guoyin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1">Weiping Ding</a></p>
<p>Granular ball computing (GBC), as an efficient, robust, and scalable learning
method, has become a popular research topic of granular computing. GBC includes
two stages: granular ball generation (GBG) and multi-granularity learning based
on the granular ball (GB). However, the stability and efficiency of existing
GBG methods need to be further improved due to their strong dependence on
$k$-means or $k$-division. In addition, GB-based classifiers only unilaterally
consider the GB's geometric characteristics to construct classification rules,
but the GB's quality is ignored. Therefore, in this paper, based on the
attention mechanism, a fast and stable GBG (GBG++) method is proposed first.
Specifically, the proposed GBG++ method only needs to calculate the distances
from the data-driven center to the undivided samples when splitting each GB
instead of randomly selecting the center and calculating the distances between
it and all samples. Moreover, an outlier detection method is introduced to
identify local outliers. Consequently, the GBG++ method can significantly
improve effectiveness, robustness, and efficiency while being absolutely
stable. Second, considering the influence of the sample size within the GB on
the GB's quality, based on the GBG++ method, an improved GB-based $k$-nearest
neighbors algorithm (GB$k$NN++) is presented, which can reduce
misclassification at the class boundary. Finally, the experimental results
indicate that the proposed method outperforms several existing GB-based
classifiers and classical machine learning classifiers on $24$ public benchmark
datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.18831">Convolutional Monge Mapping Normalization for learning on sleep data. (arXiv:2305.18831v3 [eess.SP] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Gnassounou_T/0/1/0/all/0/1">Th&#xe9;o Gnassounou</a>, <a href="http://arxiv.org/find/eess/1/au:+Flamary_R/0/1/0/all/0/1">R&#xe9;mi Flamary</a>, <a href="http://arxiv.org/find/eess/1/au:+Gramfort_A/0/1/0/all/0/1">Alexandre Gramfort</a></p>
<p>In many machine learning applications on signals and biomedical data,
especially electroencephalogram (EEG), one major challenge is the variability
of the data across subjects, sessions, and hardware devices. In this work, we
propose a new method called Convolutional Monge Mapping Normalization (CMMN),
which consists in filtering the signals in order to adapt their power spectrum
density (PSD) to a Wasserstein barycenter estimated on training data. CMMN
relies on novel closed-form solutions for optimal transport mappings and
barycenters and provides individual test time adaptation to new data without
needing to retrain a prediction model. Numerical experiments on sleep EEG data
show that CMMN leads to significant and consistent performance gains
independent from the neural network architecture when adapting between
subjects, sessions, and even datasets collected with different hardware.
Notably our performance gain is on par with much more numerically intensive
Domain Adaptation (DA) methods and can be used in conjunction with those for
even better performances.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19452">Bigger, Better, Faster: Human-level Atari with human-level efficiency. (arXiv:2305.19452v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schwarzer_M/0/1/0/all/0/1">Max Schwarzer</a>, <a href="http://arxiv.org/find/cs/1/au:+Obando_Ceron_J/0/1/0/all/0/1">Johan Obando-Ceron</a>, <a href="http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1">Aaron Courville</a>, <a href="http://arxiv.org/find/cs/1/au:+Bellemare_M/0/1/0/all/0/1">Marc Bellemare</a>, <a href="http://arxiv.org/find/cs/1/au:+Agarwal_R/0/1/0/all/0/1">Rishabh Agarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Castro_P/0/1/0/all/0/1">Pablo Samuel Castro</a></p>
<p>We introduce a value-based RL agent, which we call BBF, that achieves
super-human performance in the Atari 100K benchmark. BBF relies on scaling the
neural networks used for value estimation, as well as a number of other design
choices that enable this scaling in a sample-efficient manner. We conduct
extensive analyses of these design choices and provide insights for future
work. We end with a discussion about updating the goalposts for
sample-efficient RL research on the ALE. We make our code and data publicly
available at
https://github.com/google-research/google-research/tree/master/bigger_better_faster.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.00074">Human-Aligned Calibration for AI-Assisted Decision Making. (arXiv:2306.00074v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Benz_N/0/1/0/all/0/1">Nina L. Corvelo Benz</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_M/0/1/0/all/0/1">Manuel Gomez Rodriguez</a></p>
<p>Whenever a binary classifier is used to provide decision support, it
typically provides both a label prediction and a confidence value. Then, the
decision maker is supposed to use the confidence value to calibrate how much to
trust the prediction. In this context, it has been often argued that the
confidence value should correspond to a well calibrated estimate of the
probability that the predicted label matches the ground truth label. However,
multiple lines of empirical evidence suggest that decision makers have
difficulties at developing a good sense on when to trust a prediction using
these confidence values. In this paper, our goal is first to understand why and
then investigate how to construct more useful confidence values. We first argue
that, for a broad class of utility functions, there exist data distributions
for which a rational decision maker is, in general, unlikely to discover the
optimal decision policy using the above confidence values -- an optimal
decision maker would need to sometimes place more (less) trust on predictions
with lower (higher) confidence values. However, we then show that, if the
confidence values satisfy a natural alignment property with respect to the
decision maker's confidence on her own predictions, there always exists an
optimal decision policy under which the level of trust the decision maker would
need to place on predictions is monotone on the confidence values, facilitating
its discoverability. Further, we show that multicalibration with respect to the
decision maker's confidence on her own predictions is a sufficient condition
for alignment. Experiments on four different AI-assisted decision making tasks
where a classifier provides decision support to real human experts validate our
theoretical results and suggest that alignment may lead to better decisions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.04220">Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL. (arXiv:2306.04220v6 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1">Peng Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1">Xianyuan Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhihao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenjia Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1">Shoucheng Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Han Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Youfang Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1">Li Jiang</a></p>
<p>Offline reinforcement learning (RL) offers an appealing approach to
real-world tasks by learning policies from pre-collected datasets without
interacting with the environment. However, the performance of existing offline
RL algorithms heavily depends on the scale and state-action space coverage of
datasets. Real-world data collection is often expensive and uncontrollable,
leading to small and narrowly covered datasets and posing significant
challenges for practical deployments of offline RL. In this paper, we provide a
new insight that leveraging the fundamental symmetry of system dynamics can
substantially enhance offline RL performance under small datasets.
Specifically, we propose a Time-reversal symmetry (T-symmetry) enforced
Dynamics Model (TDM), which establishes consistency between a pair of forward
and reverse latent dynamics. TDM provides both well-behaved representations for
small datasets and a new reliability measure for OOD samples based on
compliance with the T-symmetry. These can be readily used to construct a new
offline RL algorithm (TSRL) with less conservative policy constraints and a
reliable latent space data augmentation procedure. Based on extensive
experiments, we find TSRL achieves great performance on small benchmark
datasets with as few as 1% of the original samples, which significantly
outperforms the recent offline RL algorithms in terms of data efficiency and
generalizability.Code is available at: https://github.com/pcheng2/TSRL
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.06874">VillanDiffusion: A Unified Backdoor Attack Framework for Diffusion Models. (arXiv:2306.06874v4 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chou_S/0/1/0/all/0/1">Sheng-Yen Chou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1">Pin-Yu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ho_T/0/1/0/all/0/1">Tsung-Yi Ho</a></p>
<p>Diffusion Models (DMs) are state-of-the-art generative models that learn a
reversible corruption process from iterative noise addition and denoising. They
are the backbone of many generative AI applications, such as text-to-image
conditional generation. However, recent studies have shown that basic
unconditional DMs (e.g., DDPM and DDIM) are vulnerable to backdoor injection, a
type of output manipulation attack triggered by a maliciously embedded pattern
at model input. This paper presents a unified backdoor attack framework
(VillanDiffusion) to expand the current scope of backdoor analysis for DMs. Our
framework covers mainstream unconditional and conditional DMs (denoising-based
and score-based) and various training-free samplers for holistic evaluations.
Experiments show that our unified framework facilitates the backdoor analysis
of different DM configurations and provides new insights into caption-based
backdoor attacks on DMs. Our code is available on GitHub:
\url{https://github.com/IBM/villandiffusion}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.08698">Phase Transitions of Civil Unrest across Countries and Time. (arXiv:2306.08698v3 [physics.soc-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Braha_D/0/1/0/all/0/1">Dan Braha</a></p>
<p>Phase transitions, characterized by abrupt shifts between macroscopic
patterns of organization, are ubiquitous in complex systems. Despite
considerable research in the physical and natural sciences, the empirical study
of this phenomenon in societal systems is relatively underdeveloped. The goal
of this study is to explore whether the dynamics of collective civil unrest can
be plausibly characterized as a sequence of recurrent phase shifts, with each
phase having measurable and identifiable latent characteristics. Building on
previous efforts to characterize civil unrest as a self-organized critical
system, we introduce a macro-level statistical model of civil unrest and
evaluate its plausibility using a comprehensive dataset of civil unrest events
in 170 countries from 1946 to 2017. Our findings demonstrate that the
macro-level phase model effectively captures the characteristics of civil
unrest data from diverse countries globally and that universal mechanisms may
underlie certain aspects of the dynamics of civil unrest. We also introduce a
scale to quantify a country's long-term unrest per unit of time and show that
civil unrest events tend to cluster geographically, with the magnitude of civil
unrest concentrated in specific regions. Our approach has the potential to
identify and measure phase transitions in various collective human phenomena
beyond civil unrest, contributing to a better understanding of complex social
systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.09338">Understanding Optimization of Deep Learning via Jacobian Matrix and Lipschitz Constant. (arXiv:2306.09338v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1">Xianbiao Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jianan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lei Zhang</a></p>
<p>This article provides a comprehensive understanding of optimization in deep
learning, with a primary focus on the challenges of gradient vanishing and
gradient exploding, which normally lead to diminished model representational
ability and training instability, respectively. We analyze these two challenges
through several strategic measures, including the improvement of gradient flow
and the imposition of constraints on a network's Lipschitz constant. To help
understand the current optimization methodologies, we categorize them into two
classes: explicit optimization and implicit optimization. Explicit optimization
methods involve direct manipulation of optimizer parameters, including weight,
gradient, learning rate, and weight decay. Implicit optimization methods, by
contrast, focus on improving the overall landscape of a network by enhancing
its modules, such as residual shortcuts, normalization methods, attention
mechanisms, and activations. In this article, we provide an in-depth analysis
of these two optimization classes and undertake a thorough examination of the
Jacobian matrices and the Lipschitz constants of many widely used deep learning
modules, highlighting existing issues as well as potential improvements.
Moreover, we also conduct a series of analytical experiments to substantiate
our theoretical discussions. This article does not aim to propose a new
optimizer or network. Rather, our intention is to present a comprehensive
understanding of optimization in deep learning. We hope that this article will
assist readers in gaining a deeper insight in this field and encourages the
development of more robust, efficient, and high-performing models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.11816">Learning to Generate Better Than Your LLM. (arXiv:2306.11816v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1">Jonathan D. Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Brantley_K/0/1/0/all/0/1">Kiante Brantley</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramamurthy_R/0/1/0/all/0/1">Rajkumar Ramamurthy</a>, <a href="http://arxiv.org/find/cs/1/au:+Misra_D/0/1/0/all/0/1">Dipendra Misra</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1">Wen Sun</a></p>
<p>Reinforcement learning (RL) has emerged as a powerful paradigm for
fine-tuning Large Language Models (LLMs) for text generation. In particular,
recent LLMs such as ChatGPT and GPT-4 can engage in fluent conversations with
users after finetuning with RL. Capitalizing on key properties of text
generation, we seek to investigate RL algorithms beyond general purpose
algorithms like Proximal Policy Optimization (PPO). In particular, we extend RL
algorithms to allow them to interact with a dynamic black-box guide LLM and
propose RL with guided feedback (RLGF), a suite of RL algorithms for LLM
fine-tuning. We provide two ways for the guide LLM to interact with the LLM to
be optimized for maximizing rewards. The guide LLM can generate text which
serves as additional starting states for the RL optimization procedure. The
guide LLM can also be used to complete the partial sentences generated by the
LLM that is being optimized, treating the guide LLM as an expert to imitate and
surpass eventually. We experiment on the IMDB positive sentiment, CommonGen,
and TL;DR summarization tasks. We show that our RL algorithms achieve higher
performance than supervised learning (SL) and the RL baseline PPO,
demonstrating the benefit of interaction with the guide LLM. On both CommonGen
and TL;DR, we not only outperform our SL baselines but also improve upon PPO
across a variety of metrics beyond the one we optimized for. Our code can be
found at https://github.com/Cornell-RL/tril.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.13761">CeBed: A Benchmark for Deep Data-Driven OFDM Channel Estimation. (arXiv:2306.13761v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feriani_A/0/1/0/all/0/1">Amal Feriani</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1">Di Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Steve Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dudek_G/0/1/0/all/0/1">Greg Dudek</a></p>
<p>Deep learning has been extensively used in wireless communication problems,
including channel estimation. Although several data-driven approaches exist, a
fair and realistic comparison between them is difficult due to inconsistencies
in the experimental conditions and the lack of a standardized experimental
design. In addition, the performance of data-driven approaches is often
compared based on empirical analysis. The lack of reproducibility and
availability of standardized evaluation tools (e.g., datasets, codebases)
hinder the development and progress of data-driven methods for channel
estimation and wireless communication in general. In this work, we introduce an
initiative to build benchmarks that unify several data-driven OFDM channel
estimation approaches. Specifically, we present CeBed (a testbed for channel
estimation) including different datasets covering various systems models and
propagation conditions along with the implementation of ten deep and
traditional baselines. This benchmark considers different practical aspects
such as the robustness of the data-driven models, the number and the
arrangement of pilots, and the number of receive antennas. This work offers a
comprehensive and unified framework to help researchers evaluate and design
data-driven channel estimation algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.13948">Unleashing Realistic Air Quality Forecasting: Introducing the Ready-to-Use PurpleAirSF Dataset. (arXiv:2306.13948v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zuo_J/0/1/0/all/0/1">Jingwei Zuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wenbin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Baldo_M/0/1/0/all/0/1">Michele Baldo</a>, <a href="http://arxiv.org/find/cs/1/au:+Hacid_H/0/1/0/all/0/1">Hakim Hacid</a></p>
<p>Air quality forecasting has garnered significant attention recently, with
data-driven models taking center stage due to advancements in machine learning
and deep learning models. However, researchers face challenges with complex
data acquisition and the lack of open-sourced datasets, hindering efficient
model validation. This paper introduces PurpleAirSF, a comprehensive and easily
accessible dataset collected from the PurpleAir network. With its high temporal
resolution, various air quality measures, and diverse geographical coverage,
this dataset serves as a useful tool for researchers aiming to develop novel
forecasting models, study air pollution patterns, and investigate their impacts
on health and the environment. We present a detailed account of the data
collection and processing methods employed to build PurpleAirSF. Furthermore,
we conduct preliminary experiments using both classic and modern
spatio-temporal forecasting models, thereby establishing a benchmark for future
air quality forecasting tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.16045">OpenNDD: Open Set Recognition for Neurodevelopmental Disorders Detection. (arXiv:2306.16045v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Jiaming Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_Z/0/1/0/all/0/1">Zihao Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1">Xinyue Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shujie Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1">Zhenshan Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiumei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Changcai Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1">Riqing Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1">Lanyan Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1">Lifang Wei</a></p>
<p>Since the strong comorbid similarity in NDDs, such as attention-deficit
hyperactivity disorder, can interfere with the accurate diagnosis of autism
spectrum disorder (ASD), identifying unknown classes is extremely crucial and
challenging from NDDs. We design a novel open set recognition framework for
ASD-aided diagnosis (OpenNDD), which trains a model by combining autoencoder
and adversarial reciprocal points learning to distinguish in-distribution and
out-of-distribution categories as well as identify ASD accurately. Considering
the strong similarities between NDDs, we present a joint scaling method by
Min-Max scaling combined with Standardization (MMS) to increase the differences
between classes for better distinguishing unknown NDDs. We conduct the
experiments in the hybrid datasets from Autism Brain Imaging Data Exchange I
(ABIDE I) and THE ADHD-200 SAMPLE (ADHD-200) with 791 samples from four sites
and the results demonstrate the superiority on various metrics. Our OpenNDD
achieves promising performance, where the accuracy is 77.38%, AUROC is 75.53%
and the open set classification rate is as high as 59.43%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.16830">Sampling weights of deep neural networks. (arXiv:2306.16830v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bolager_E/0/1/0/all/0/1">Erik Lien Bolager</a>, <a href="http://arxiv.org/find/cs/1/au:+Burak_I/0/1/0/all/0/1">Iryna Burak</a>, <a href="http://arxiv.org/find/cs/1/au:+Datar_C/0/1/0/all/0/1">Chinmay Datar</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1">Qing Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Dietrich_F/0/1/0/all/0/1">Felix Dietrich</a></p>
<p>We introduce a probability distribution, combined with an efficient sampling
algorithm, for weights and biases of fully-connected neural networks. In a
supervised learning context, no iterative optimization or gradient computations
of internal network parameters are needed to obtain a trained network. The
sampling is based on the idea of random feature models. However, instead of a
data-agnostic distribution, e.g., a normal distribution, we use both the input
and the output training data to sample shallow and deep networks. We prove that
sampled networks are universal approximators. For Barron functions, we show
that the $L^2$-approximation error of sampled shallow networks decreases with
the square root of the number of neurons. Our sampling scheme is invariant to
rigid body transformations and scaling of the input data, which implies many
popular pre-processing techniques are not required. In numerical experiments,
we demonstrate that sampled networks achieve accuracy comparable to iteratively
trained ones, but can be constructed orders of magnitude faster. Our test cases
involve a classification benchmark from OpenML, sampling of neural operators to
represent maps in function spaces, and transfer learning using well-known
architectures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.17010">milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing. (arXiv:2306.17010v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1">Fangqiang Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1">Zhen Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1">Peijun Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1">Chris Xiaoxuan Lu</a></p>
<p>Approaching the era of ubiquitous computing, human motion sensing plays a
crucial role in smart systems for decision making, user interaction, and
personalized services. Extensive research has been conducted on human tracking,
pose estimation, gesture recognition, and activity recognition, which are
predominantly based on cameras in traditional methods. However, the intrusive
nature of cameras limits their use in smart home applications. To address this,
mmWave radars have gained popularity due to their privacy-friendly features. In
this work, we propose milliFlow, a novel deep learning method for scene flow
estimation as a complementary motion information for mmWave point cloud,
serving as an intermediate level of features and directly benefiting downstream
human motion sensing tasks. Experimental results demonstrate the superior
performance of our method with an average 3D endpoint error of 4.6cm,
significantly surpassing the competing approaches. Furthermore, by
incorporating scene flow information, we achieve remarkable improvements in
human activity recognition, human parsing, and human body part tracking. To
foster further research in this area, we will provide our codebase and dataset
for open access upon acceptance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01231">A Critical Re-evaluation of Benchmark Datasets for (Deep) Learning-Based Matching Algorithms. (arXiv:2307.01231v2 [cs.DB] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Papadakis_G/0/1/0/all/0/1">George Papadakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Kirielle_N/0/1/0/all/0/1">Nishadi Kirielle</a>, <a href="http://arxiv.org/find/cs/1/au:+Christen_P/0/1/0/all/0/1">Peter Christen</a>, <a href="http://arxiv.org/find/cs/1/au:+Palpanas_T/0/1/0/all/0/1">Themis Palpanas</a></p>
<p>Entity resolution (ER) is the process of identifying records that refer to
the same entities within one or across multiple databases. Numerous techniques
have been developed to tackle ER challenges over the years, with recent
emphasis placed on machine and deep learning methods for the matching phase.
However, the quality of the benchmark datasets typically used in the
experimental evaluations of learning-based matching algorithms has not been
examined in the literature. To cover this gap, we propose four different
approaches to assessing the difficulty and appropriateness of 13 established
datasets: two theoretical approaches, which involve new measures of linearity
and existing measures of complexity, and two practical approaches: the
difference between the best non-linear and linear matchers, as well as the
difference between the best learning-based matcher and the perfect oracle. Our
analysis demonstrates that most of the popular datasets pose rather easy
classification tasks. As a result, they are not suitable for properly
evaluating learning-based matching algorithms. To address this issue, we
propose a new methodology for yielding benchmark datasets. We put it into
practice by creating four new matching tasks, and we verify that these new
benchmarks are more challenging and therefore more suitable for further
advancements in the field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04349">RLTF: Reinforcement Learning from Unit Test Feedback. (arXiv:2307.04349v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiate Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yiqin Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_K/0/1/0/all/0/1">Kaiwen Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1">Qiang Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1">Xiao Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1">Wei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_D/0/1/0/all/0/1">Deheng Ye</a></p>
<p>The goal of program synthesis, or code generation, is to generate executable
code based on given descriptions. Recently, there has been an increasing number
of studies employing reinforcement learning (RL) to improve the performance of
large language models (LLMs) for code. However, current representative works
either rely solely on offline frameworks, limiting the exploration of new
sample spaces, or fall short in the utilization of unit test signals, not
accounting for specific error locations within the code. To address these
issues, we propose RLTF, i.e., Reinforcement Learning from Unit Test Feedback,
a novel online RL framework with unit test feedback of multi-granularity for
refining code LLMs. Our approach generates data in real-time during training
and simultaneously utilizes fine-grained feedback signals to guide the model
towards producing higher-quality code. Extensive experiments show that RLTF
achieves state-of-the-art performance on the APPS and the MBPP benchmarks. Our
code is available at: https://github.com/Zyq-scut/RLTF.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06440">No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models. (arXiv:2307.06440v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kaddour_J/0/1/0/all/0/1">Jean Kaddour</a>, <a href="http://arxiv.org/find/cs/1/au:+Key_O/0/1/0/all/0/1">Oscar Key</a>, <a href="http://arxiv.org/find/cs/1/au:+Nawrot_P/0/1/0/all/0/1">Piotr Nawrot</a>, <a href="http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1">Pasquale Minervini</a>, <a href="http://arxiv.org/find/cs/1/au:+Kusner_M/0/1/0/all/0/1">Matt J. Kusner</a></p>
<p>The computation necessary for training Transformer-based language models has
skyrocketed in recent years. This trend has motivated research on efficient
training algorithms designed to improve training, validation, and downstream
performance faster than standard training. In this work, we revisit three
categories of such algorithms: dynamic architectures (layer stacking, layer
dropping), batch selection (selective backprop, RHO loss), and efficient
optimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed
computation budget using such methods, we find that their training, validation,
and downstream gains vanish compared to a baseline with a fully-decayed
learning rate. We define an evaluation protocol that enables computation to be
done on arbitrary machines by mapping all computation time to a reference
machine which we call reference system time. We discuss the limitations of our
proposed protocol and release our code to encourage rigorous research in
efficient training procedures: https://github.com/JeanKaddour/NoTrainNoGain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07735">Faster Algorithms for Structured Linear and Kernel Support Vector Machines. (arXiv:2307.07735v2 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Gu_Y/0/1/0/all/0/1">Yuzhou Gu</a>, <a href="http://arxiv.org/find/math/1/au:+Song_Z/0/1/0/all/0/1">Zhao Song</a>, <a href="http://arxiv.org/find/math/1/au:+Zhang_L/0/1/0/all/0/1">Lichen Zhang</a></p>
<p>Quadratic programming is a ubiquitous prototype in convex programming. Many
combinatorial optimizations on graphs and machine learning problems can be
formulated as quadratic programming; for example, Support Vector Machines
(SVMs). Linear and kernel SVMs have been among the most popular models in
machine learning over the past three decades, prior to the deep learning era.
</p>
<p>Generally, a quadratic program has an input size of $\Theta(n^2)$, where $n$
is the number of variables. Assuming the Strong Exponential Time Hypothesis
($\textsf{SETH}$), it is known that no $O(n^{2-o(1)})$ algorithm exists
(Backurs, Indyk, and Schmidt, NIPS'17). However, problems such as SVMs usually
feature much smaller input sizes: one is given $n$ data points, each of
dimension $d$, with $d \ll n$. Furthermore, SVMs are variants with only $O(1)$
linear constraints. This suggests that faster algorithms are feasible, provided
the program exhibits certain underlying structures.
</p>
<p>In this work, we design the first nearly-linear time algorithm for solving
quadratic programs whenever the quadratic objective has small treewidth or
admits a low-rank factorization, and the number of linear constraints is small.
Consequently, we obtain a variety of results for SVMs:
</p>
<p>* For linear SVM, where the quadratic constraint matrix has treewidth $\tau$,
we can solve the corresponding program in time $\widetilde
O(n\tau^{(\omega+1)/2}\log(1/\epsilon))$;
</p>
<p>* For linear SVM, where the quadratic constraint matrix admits a low-rank
factorization of rank-$k$, we can solve the corresponding program in time
$\widetilde O(nk^{(\omega+1)/2}\log(1/\epsilon))$;
</p>
<p>* For Gaussian kernel SVM, where the data dimension $d = \Theta(\log n)$ and
the squared dataset radius is small, we can solve it in time
$O(n^{1+o(1)}\log(1/\epsilon))$. We also prove that when the squared dataset
radius is large, then $\Omega(n^{2-o(1)})$ time is required.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08286">Going Beyond Linear Mode Connectivity: The Layerwise Linear Feature Connectivity. (arXiv:2307.08286v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhanpeng Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yongyi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xiaojiang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1">Junchi Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1">Wei Hu</a></p>
<p>Recent work has revealed many intriguing empirical phenomena in neural
network training, despite the poorly understood and highly complex loss
landscapes and training dynamics. One of these phenomena, Linear Mode
Connectivity (LMC), has gained considerable attention due to the intriguing
observation that different solutions can be connected by a linear path in the
parameter space while maintaining near-constant training and test losses. In
this work, we introduce a stronger notion of linear connectivity, Layerwise
Linear Feature Connectivity (LLFC), which says that the feature maps of every
layer in different trained networks are also linearly connected. We provide
comprehensive empirical evidence for LLFC across a wide range of settings,
demonstrating that whenever two trained networks satisfy LMC (via either
spawning or permutation methods), they also satisfy LLFC in nearly all the
layers. Furthermore, we delve deeper into the underlying factors contributing
to LLFC, which reveal new insights into the spawning and permutation
approaches. The study of LLFC transcends and advances our understanding of LMC
by adopting a feature-learning perspective.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10317">FedBug: A Bottom-Up Gradual Unfreezing Framework for Federated Learning. (arXiv:2307.10317v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kao_C/0/1/0/all/0/1">Chia-Hsiang Kao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu-Chiang Frank Wang</a></p>
<p>Federated Learning (FL) offers a collaborative training framework, allowing
multiple clients to contribute to a shared model without compromising data
privacy. Due to the heterogeneous nature of local datasets, updated client
models may overfit and diverge from one another, commonly known as the problem
of client drift. In this paper, we propose FedBug (Federated Learning with
Bottom-Up Gradual Unfreezing), a novel FL framework designed to effectively
mitigate client drift. FedBug adaptively leverages the client model parameters,
distributed by the server at each global round, as the reference points for
cross-client alignment. Specifically, on the client side, FedBug begins by
freezing the entire model, then gradually unfreezes the layers, from the input
layer to the output layer. This bottom-up approach allows models to train the
newly thawed layers to project data into a latent space, wherein the separating
hyperplanes remain consistent across all clients. We theoretically analyze
FedBug in a novel over-parameterization FL setup, revealing its superior
convergence rate compared to FedAvg. Through comprehensive experiments,
spanning various datasets, training conditions, and network architectures, we
validate the efficacy of FedBug. Our contributions encompass a novel FL
framework, theoretical analysis, and empirical validation, demonstrating the
wide potential and applicability of FedBug.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10455">A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset. (arXiv:2307.10455v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gharaee_Z/0/1/0/all/0/1">Zahra Gharaee</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1">ZeMing Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Pellegrino_N/0/1/0/all/0/1">Nicholas Pellegrino</a>, <a href="http://arxiv.org/find/cs/1/au:+Zarubiieva_I/0/1/0/all/0/1">Iuliia Zarubiieva</a>, <a href="http://arxiv.org/find/cs/1/au:+Haurum_J/0/1/0/all/0/1">Joakim Bruslund Haurum</a>, <a href="http://arxiv.org/find/cs/1/au:+Lowe_S/0/1/0/all/0/1">Scott C. Lowe</a>, <a href="http://arxiv.org/find/cs/1/au:+McKeown_J/0/1/0/all/0/1">Jaclyn T.A. McKeown</a>, <a href="http://arxiv.org/find/cs/1/au:+Ho_C/0/1/0/all/0/1">Chris C.Y. Ho</a>, <a href="http://arxiv.org/find/cs/1/au:+McLeod_J/0/1/0/all/0/1">Joschka McLeod</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1">Yi-Yun C Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Agda_J/0/1/0/all/0/1">Jireh Agda</a>, <a href="http://arxiv.org/find/cs/1/au:+Ratnasingham_S/0/1/0/all/0/1">Sujeevan Ratnasingham</a>, <a href="http://arxiv.org/find/cs/1/au:+Steinke_D/0/1/0/all/0/1">Dirk Steinke</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_A/0/1/0/all/0/1">Angel X. Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Taylor_G/0/1/0/all/0/1">Graham W. Taylor</a>, <a href="http://arxiv.org/find/cs/1/au:+Fieguth_P/0/1/0/all/0/1">Paul Fieguth</a></p>
<p>In an effort to catalog insect biodiversity, we propose a new large dataset
of hand-labelled insect images, the BIOSCAN-Insect Dataset. Each record is
taxonomically classified by an expert, and also has associated genetic
information including raw nucleotide barcode sequences and assigned barcode
index numbers, which are genetically-based proxies for species classification.
This paper presents a curated million-image dataset, primarily to train
computer-vision models capable of providing image-based taxonomic assessment,
however, the dataset also presents compelling characteristics, the study of
which would be of interest to the broader machine learning community. Driven by
the biological nature inherent to the dataset, a characteristic long-tailed
class-imbalance distribution is exhibited. Furthermore, taxonomic labelling is
a hierarchical classification scheme, presenting a highly fine-grained
classification problem at lower levels. Beyond spurring interest in
biodiversity research within the machine learning community, progress on
creating an image-based taxonomic classifier will also further the ultimate
goal of all BIOSCAN research: to lay the foundation for a comprehensive survey
of global biodiversity. This paper introduces the dataset and explores the
classification task through the implementation and analysis of a baseline
classifier.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10981">PATROL: Privacy-Oriented Pruning for Collaborative Inference Against Model Inversion Attacks. (arXiv:2307.10981v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1">Shiwei Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_M/0/1/0/all/0/1">Miao Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1">Xiaoyong Yuan</a></p>
<p>Collaborative inference has been a promising solution to enable
resource-constrained edge devices to perform inference using state-of-the-art
deep neural networks (DNNs). In collaborative inference, the edge device first
feeds the input to a partial DNN locally and then uploads the intermediate
result to the cloud to complete the inference. However, recent research
indicates model inversion attacks (MIAs) can reconstruct input data from
intermediate results, posing serious privacy concerns for collaborative
inference. Existing perturbation and cryptography techniques are inefficient
and unreliable in defending against MIAs while performing accurate inference.
This paper provides a viable solution, named PATROL, which develops
privacy-oriented pruning to balance privacy, efficiency, and utility of
collaborative inference. PATROL takes advantage of the fact that later layers
in a DNN can extract more task-specific features. Given limited local resources
for collaborative inference, PATROL intends to deploy more layers at the edge
based on pruning techniques to enforce task-specific features for inference and
reduce task-irrelevant but sensitive features for privacy preservation. To
achieve privacy-oriented pruning, PATROL introduces two key components:
Lipschitz regularization and adversarial reconstruction training, which
increase the reconstruction errors by reducing the stability of MIAs and
enhance the target inference model by adversarial training, respectively. On a
real-world collaborative inference task, vehicle re-identification, we
demonstrate the superior performance of PATROL in terms of against MIAs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.11772">AutoAlign: Fully Automatic and Effective Knowledge Graph Alignment enabled by Large Language Models. (arXiv:2307.11772v3 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Rui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1">Yixin Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Trisedya_B/0/1/0/all/0/1">Bayu Distiawan Trisedya</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xiaoyan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Min Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1">Hong Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1">Jianzhong Qi</a></p>
<p>The task of entity alignment between knowledge graphs (KGs) aims to identify
every pair of entities from two different KGs that represent the same entity.
Many machine learning-based methods have been proposed for this task. However,
to our best knowledge, existing methods all require manually crafted seed
alignments, which are expensive to obtain. In this paper, we propose the first
fully automatic alignment method named AutoAlign, which does not require any
manually crafted seed alignments. Specifically, for predicate embeddings,
AutoAlign constructs a predicate-proximity-graph with the help of large
language models to automatically capture the similarity between predicates
across two KGs. For entity embeddings, AutoAlign first computes the entity
embeddings of each KG independently using TransE, and then shifts the two KGs'
entity embeddings into the same vector space by computing the similarity
between entities based on their attributes. Thus, both predicate alignment and
entity alignment can be done without manually crafted seed alignments.
AutoAlign is not only fully automatic, but also highly effective. Experiments
using real-world KGs show that AutoAlign improves the performance of entity
alignment significantly compared to state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.12306">Tackling the Curse of Dimensionality with Physics-Informed Neural Networks. (arXiv:2307.12306v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1">Zheyuan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shukla_K/0/1/0/all/0/1">Khemraj Shukla</a>, <a href="http://arxiv.org/find/cs/1/au:+Karniadakis_G/0/1/0/all/0/1">George Em Karniadakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1">Kenji Kawaguchi</a></p>
<p>The curse-of-dimensionality taxes computational resources heavily with
exponentially increasing computational cost as the dimension increases. This
poses great challenges in solving high-dimensional PDEs, as Richard E. Bellman
first pointed out over 60 years ago. While there has been some recent success
in solving numerically partial differential equations (PDEs) in high
dimensions, such computations are prohibitively expensive, and true scaling of
general nonlinear PDEs to high dimensions has never been achieved. We develop a
new method of scaling up physics-informed neural networks (PINNs) to solve
arbitrary high-dimensional PDEs. The new method, called Stochastic Dimension
Gradient Descent (SDGD), decomposes a gradient of PDEs into pieces
corresponding to different dimensions and randomly samples a subset of these
dimensional pieces in each iteration of training PINNs. We prove theoretically
the convergence and other desired properties of the proposed method. We
demonstrate in various diverse tests that the proposed method can solve many
notoriously hard high-dimensional PDEs, including the Hamilton-Jacobi-Bellman
(HJB) and the Schr\"{o}dinger equations in tens of thousands of dimensions very
fast on a single GPU using the PINNs mesh-free approach. Notably, we solve
nonlinear PDEs with nontrivial, anisotropic, and inseparable solutions in
100,000 effective dimensions in 12 hours on a single GPU using SDGD with PINNs.
Since SDGD is a general training methodology of PINNs, it can be applied to any
current and future variants of PINNs to scale them up for arbitrary
high-dimensional PDEs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.12897">Anytime Model Selection in Linear Bandits. (arXiv:2307.12897v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Kassraie_P/0/1/0/all/0/1">Parnian Kassraie</a>, <a href="http://arxiv.org/find/stat/1/au:+Emmenegger_N/0/1/0/all/0/1">Nicolas Emmenegger</a>, <a href="http://arxiv.org/find/stat/1/au:+Krause_A/0/1/0/all/0/1">Andreas Krause</a>, <a href="http://arxiv.org/find/stat/1/au:+Pacchiano_A/0/1/0/all/0/1">Aldo Pacchiano</a></p>
<p>Model selection in the context of bandit optimization is a challenging
problem, as it requires balancing exploration and exploitation not only for
action selection, but also for model selection. One natural approach is to rely
on online learning algorithms that treat different models as experts. Existing
methods, however, scale poorly ($\text{poly}M$) with the number of models $M$
in terms of their regret. Our key insight is that, for model selection in
linear bandits, we can emulate full-information feedback to the online learner
with a favorable bias-variance trade-off. This allows us to develop ALEXP,
which has an exponentially improved ($\log M$) dependence on $M$ for its
regret. ALEXP has anytime guarantees on its regret, and neither requires
knowledge of the horizon $n$, nor relies on an initial purely exploratory
stage. Our approach utilizes a novel time-uniform analysis of the Lasso,
establishing a new connection between online learning and high-dimensional
statistics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.15691">ODTlearn: A Package for Learning Optimal Decision Trees for Prediction and Prescription. (arXiv:2307.15691v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Vossler_P/0/1/0/all/0/1">Patrick Vossler</a>, <a href="http://arxiv.org/find/stat/1/au:+Aghaei_S/0/1/0/all/0/1">Sina Aghaei</a>, <a href="http://arxiv.org/find/stat/1/au:+Justin_N/0/1/0/all/0/1">Nathan Justin</a>, <a href="http://arxiv.org/find/stat/1/au:+Jo_N/0/1/0/all/0/1">Nathanael Jo</a>, <a href="http://arxiv.org/find/stat/1/au:+Gomez_A/0/1/0/all/0/1">Andr&#xe9;s G&#xf3;mez</a>, <a href="http://arxiv.org/find/stat/1/au:+Vayanos_P/0/1/0/all/0/1">Phebe Vayanos</a></p>
<p>ODTLearn is an open-source Python package that provides methods for learning
optimal decision trees for high-stakes predictive and prescriptive tasks based
on the mixed-integer optimization (MIO) framework proposed in Aghaei et al.
(2019) and several of its extensions. The current version of the package
provides implementations for learning optimal classification trees, optimal
fair classification trees, optimal classification trees robust to distribution
shifts, and optimal prescriptive trees from observational data. We have
designed the package to be easy to maintain and extend as new optimal decision
tree problem classes, reformulation strategies, and solution algorithms are
introduced. To this end, the package follows object-oriented design principles
and supports both commercial (Gurobi) and open source (COIN-OR branch and cut)
solvers. The package documentation and an extensive user guide can be found at
https://d3m-research-group.github.io/odtlearn/. Additionally, users can view
the package source code and submit feature requests and bug reports by visiting
https://github.com/D3M-Research-Group/odtlearn.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01436">Price-Aware Deep Learning for Electricity Markets. (arXiv:2308.01436v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dvorkin_V/0/1/0/all/0/1">Vladimir Dvorkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Fioretto_F/0/1/0/all/0/1">Ferdinando Fioretto</a></p>
<p>While deep learning gradually penetrates operational planning, its inherent
prediction errors may significantly affect electricity prices. This letter
examines how prediction errors propagate into electricity prices, revealing
notable pricing errors and their spatial disparity in congested power systems.
To improve fairness, we propose to embed electricity market-clearing
optimization as a deep learning layer. Differentiating through this layer
allows for balancing between prediction and pricing errors, as oppose to
minimizing prediction errors alone. This layer implicitly optimizes fairness
and controls the spatial distribution of price errors across the system. We
showcase the price-aware deep learning in the nexus of wind power forecasting
and short-term electricity market clearing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05345">RTLLM: An Open-Source Benchmark for Design RTL Generation with Large Language Model. (arXiv:2308.05345v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yao Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qijun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1">Zhiyao Xie</a></p>
<p>Inspired by the recent success of large language models (LLMs) like ChatGPT,
researchers start to explore the adoption of LLMs for agile hardware design,
such as generating design RTL based on natural-language instructions. However,
in existing works, their target designs are all relatively simple and in a
small scale, and proposed by the authors themselves, making a fair comparison
among different LLM solutions challenging. In addition, many prior works only
focus on the design correctness, without evaluating the design qualities of
generated design RTL. In this work, we propose an open-source benchmark named
RTLLM, for generating design RTL with natural language instructions. To
systematically evaluate the auto-generated design RTL, we summarized three
progressive goals, named syntax goal, functionality goal, and design quality
goal. This benchmark can automatically provide a quantitative evaluation of any
given LLM-based solution. Furthermore, we propose an easy-to-use yet
surprisingly effective prompt engineering technique named self-planning, which
proves to significantly boost the performance of GPT-3.5 in our proposed
benchmark.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.07336">Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic. (arXiv:2308.07336v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Morishita_T/0/1/0/all/0/1">Terufumi Morishita</a>, <a href="http://arxiv.org/find/cs/1/au:+Morio_G/0/1/0/all/0/1">Gaku Morio</a>, <a href="http://arxiv.org/find/cs/1/au:+Yamaguchi_A/0/1/0/all/0/1">Atsuki Yamaguchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Sogawa_Y/0/1/0/all/0/1">Yasuhiro Sogawa</a></p>
<p>We study a synthetic corpus based approach for language models (LMs) to
acquire logical deductive reasoning ability. The previous studies generated
deduction examples using specific sets of deduction rules. However, these rules
were limited or otherwise arbitrary, limiting the generalizability of acquired
reasoning ability. We rethink this and adopt a well-grounded set of deduction
rules based on formal logic theory, which can derive any other deduction rules
when combined in a multistep way. Then, using the proposed corpora, which we
name FLD (Formal Logic Deduction), we first evaluate and analyze the logical
reasoning ability of the latest LLMs. Even GPT-4 can solve only half of the
problems, suggesting that pure logical reasoning isolated from knowledge is
still challenging for the LLMs, and additional training specialized in logical
reasoning is indeed essential. We next empirically verify that LMs trained on
FLD corpora acquire more generalizable reasoning ability. Furthermore, we
identify the aspects of reasoning ability on which deduction corpora can
enhance LMs and those on which they cannot, and discuss future directions on
each aspect. The released corpora serve both as learning resources and as
challenging benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.10248">Activation Addition: Steering Language Models Without Optimization. (arXiv:2308.10248v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Turner_A/0/1/0/all/0/1">Alexander Matt Turner</a>, <a href="http://arxiv.org/find/cs/1/au:+Thiergart_L/0/1/0/all/0/1">Lisa Thiergart</a>, <a href="http://arxiv.org/find/cs/1/au:+Udell_D/0/1/0/all/0/1">David Udell</a>, <a href="http://arxiv.org/find/cs/1/au:+Leech_G/0/1/0/all/0/1">Gavin Leech</a>, <a href="http://arxiv.org/find/cs/1/au:+Mini_U/0/1/0/all/0/1">Ulisse Mini</a>, <a href="http://arxiv.org/find/cs/1/au:+MacDiarmid_M/0/1/0/all/0/1">Monte MacDiarmid</a></p>
<p>Reliably controlling the behavior of large language models is a pressing open
problem. Existing methods include supervised finetuning, reinforcement learning
from human feedback, prompt engineering and guided decoding. We instead
investigate activation engineering: modifying activations at inference-time to
predictably alter model behavior. We bias the forward pass with a 'steering
vector' implicitly specified through natural language. Past work learned these
steering vectors; our Activation Addition (ActAdd) method instead computes them
by taking the activation differences which result from pairs of prompts.
</p>
<p>We demonstrate ActAdd on GPT-2 on OpenWebText and ConceptNet, and replicate
the effect on Llama-13B and GPT-J-6B. Our approach yields inference-time
control over high-level properties of output &amp; preserves performance on
off-target topics. The method requires far less compute and implementation
effort than finetuning and RLHF, allows for natural language specification by
users, and its overhead scales naturally with model size.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.10407">Federated Learning for Connected and Automated Vehicles: A Survey of Existing Approaches and Challenges. (arXiv:2308.10407v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chellapandi_V/0/1/0/all/0/1">Vishnu Pandi Chellapandi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1">Liangqi Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Brinton_C/0/1/0/all/0/1">Christopher G. Brinton</a>, <a href="http://arxiv.org/find/cs/1/au:+Zak_S/0/1/0/all/0/1">Stanislaw H Zak</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Ziran Wang</a></p>
<p>Machine learning (ML) is widely used for key tasks in Connected and Automated
Vehicles (CAV), including perception, planning, and control. However, its
reliance on vehicular data for model training presents significant challenges
related to in-vehicle user privacy and communication overhead generated by
massive data volumes. Federated learning (FL) is a decentralized ML approach
that enables multiple vehicles to collaboratively develop models, broadening
learning from various driving environments, enhancing overall performance, and
simultaneously securing local vehicle data privacy and security. This survey
paper presents a review of the advancements made in the application of FL for
CAV (FL4CAV). First, centralized and decentralized frameworks of FL are
analyzed, highlighting their key characteristics and methodologies. Second,
diverse data sources, models, and data security techniques relevant to FL in
CAVs are reviewed, emphasizing their significance in ensuring privacy and
confidentiality. Third, specific applications of FL are explored, providing
insight into the base models and datasets employed for each application.
Finally, existing challenges for FL4CAV are listed and potential directions for
future investigation to further enhance the effectiveness and efficiency of FL
in the context of CAV are discussed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.11630">Addressing Data Scarcity in Optical Matrix Multiplier Modeling Using Transfer Learning. (arXiv:2308.11630v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cem_A/0/1/0/all/0/1">Ali Cem</a>, <a href="http://arxiv.org/find/cs/1/au:+Jovanovic_O/0/1/0/all/0/1">Ognjen Jovanovic</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1">Siqi Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1">Yunhong Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Zibar_D/0/1/0/all/0/1">Darko Zibar</a>, <a href="http://arxiv.org/find/cs/1/au:+Ros_F/0/1/0/all/0/1">Francesco Da Ros</a></p>
<p>We present and experimentally evaluate using transfer learning to address
experimental data scarcity when training neural network (NN) models for
Mach-Zehnder interferometer mesh-based optical matrix multipliers. Our approach
involves pre-training the model using synthetic data generated from a less
accurate analytical model and fine-tuning with experimental data. Our
investigation demonstrates that this method yields significant reductions in
modeling errors compared to using an analytical model, or a standalone NN model
when training data is limited. Utilizing regularization techniques and ensemble
averaging, we achieve &lt; 1 dB root-mean-square error on the matrix weights
implemented by a 3x3 photonic chip while using only 25% of the available data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.14522">Graph Meets LLMs: Towards Large Graph Models. (arXiv:2308.14522v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Ziwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Haoyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zeyang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1">Yijian Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1">Wenwu Zhu</a></p>
<p>Large models have emerged as the most recent groundbreaking achievements in
artificial intelligence, and particularly machine learning. However, when it
comes to graphs, large models have not achieved the same level of success as in
other fields, such as natural language processing and computer vision. In order
to promote applying large models for graphs forward, we present a perspective
paper to discuss the challenges and opportunities associated with developing
large graph models. First, we discuss the desired characteristics of large
graph models. Then, we present detailed discussions from three key
perspectives: representation basis, graph data, and graph models. In each
category, we provide a brief overview of recent advances and highlight the
remaining challenges together with our visions. Finally, we discuss valuable
applications of large graph models. We believe this perspective can encourage
further investigations into large graph models, ultimately pushing us one step
closer towards artificial general intelligence (AGI). We are the first to
comprehensively study large graph models, to the best of our knowledge.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.03179">SLiMe: Segment Like Me. (arXiv:2309.03179v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khani_A/0/1/0/all/0/1">Aliasghar Khani</a>, <a href="http://arxiv.org/find/cs/1/au:+Taghanaki_S/0/1/0/all/0/1">Saeid Asgari Taghanaki</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanghi_A/0/1/0/all/0/1">Aditya Sanghi</a>, <a href="http://arxiv.org/find/cs/1/au:+Amiri_A/0/1/0/all/0/1">Ali Mahdavi Amiri</a>, <a href="http://arxiv.org/find/cs/1/au:+Hamarneh_G/0/1/0/all/0/1">Ghassan Hamarneh</a></p>
<p>Significant strides have been made using large vision-language models, like
Stable Diffusion (SD), for a variety of downstream tasks, including image
editing, image correspondence, and 3D shape generation. Inspired by these
advancements, we explore leveraging these extensive vision-language models for
segmenting images at any desired granularity using as few as one annotated
sample by proposing SLiMe. SLiMe frames this problem as an optimization task.
Specifically, given a single training image and its segmentation mask, we first
extract attention maps, including our novel "weighted accumulated
self-attention map" from the SD prior. Then, using the extracted attention
maps, the text embeddings of Stable Diffusion are optimized such that, each of
them, learn about a single segmented region from the training image. These
learned embeddings then highlight the segmented region in the attention maps,
which in turn can then be used to derive the segmentation map. This enables
SLiMe to segment any real-world image during inference with the granularity of
the segmented region in the training image, using just one example. Moreover,
leveraging additional training data when available, i.e. few-shot, improves the
performance of SLiMe. We carried out a knowledge-rich set of experiments
examining various design factors and showed that SLiMe outperforms other
existing one-shot and few-shot segmentation methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.08534">Towards Last-layer Retraining for Group Robustness with Fewer Annotations. (arXiv:2309.08534v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+LaBonte_T/0/1/0/all/0/1">Tyler LaBonte</a>, <a href="http://arxiv.org/find/cs/1/au:+Muthukumar_V/0/1/0/all/0/1">Vidya Muthukumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1">Abhishek Kumar</a></p>
<p>Empirical risk minimization (ERM) of neural networks is prone to
over-reliance on spurious correlations and poor generalization on minority
groups. The recent deep feature reweighting (DFR) technique achieves
state-of-the-art group robustness via simple last-layer retraining, but it
requires held-out group and class annotations to construct a group-balanced
reweighting dataset. In this work, we examine this impractical requirement and
find that last-layer retraining can be surprisingly effective with no group
annotations (other than for model selection) and only a handful of class
annotations. We first show that last-layer retraining can greatly improve
worst-group accuracy even when the reweighting dataset has only a small
proportion of worst-group data. This implies a "free lunch" where holding out a
subset of training data to retrain the last layer can substantially outperform
ERM on the entire dataset with no additional data or annotations. To further
improve group robustness, we introduce a lightweight method called selective
last-layer finetuning (SELF), which constructs the reweighting dataset using
misclassifications or disagreements. Our empirical and theoretical results
present the first evidence that model disagreement upsamples worst-group data,
enabling SELF to nearly match DFR on four well-established benchmarks across
vision and language tasks with no group annotations and less than 3% of the
held-out class annotations. Our code is available at
https://github.com/tmlabonte/last-layer-retraining.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.10736">Mixture Weight Estimation and Model Prediction in Multi-source Multi-target Domain Adaptation. (arXiv:2309.10736v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1">Yuyang Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuzborskij_I/0/1/0/all/0/1">Ilja Kuzborskij</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahdavi_M/0/1/0/all/0/1">Mehrdad Mahdavi</a></p>
<p>We consider the problem of learning a model from multiple heterogeneous
sources with the goal of performing well on a new target distribution. The goal
of learner is to mix these data sources in a target-distribution aware way and
simultaneously minimize the empirical risk on the mixed source. The literature
has made some tangible advancements in establishing theory of learning on
mixture domain. However, there are still two unsolved problems. Firstly, how to
estimate the optimal mixture of sources, given a target domain; Secondly, when
there are numerous target domains, how to solve empirical risk minimization
(ERM) for each target using possibly unique mixture of data sources in a
computationally efficient manner. In this paper we address both problems
efficiently and with guarantees. We cast the first problem, mixture weight
estimation, as a convex-nonconcave compositional minimax problem, and propose
an efficient stochastic algorithm with provable stationarity guarantees. Next,
for the second problem, we identify that for certain regimes, solving ERM for
each target domain individually can be avoided, and instead parameters for a
target optimal model can be viewed as a non-linear function on a space of the
mixture coefficients. Building upon this, we show that in the offline setting,
a GD-trained overparameterized neural network can provably learn such function
to predict the model of target domain instead of solving a designated ERM
problem. Finally, we also consider an online setting and propose a label
efficient online algorithm, which predicts parameters for new targets given an
arbitrary sequence of mixing coefficients, while enjoying regret guarantees.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11373">Learning and DiSentangling Patient Static Information from Time-series Electronic HEalth Record (STEER). (arXiv:2309.11373v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1">Wei Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Voldman_J/0/1/0/all/0/1">Joel Voldman</a></p>
<p>Recent work in machine learning for healthcare has raised concerns about
patient privacy and algorithmic fairness. For example, previous work has shown
that patient self-reported race can be predicted from medical data that does
not explicitly contain racial information. However, the extent of data
identification is unknown, and we lack ways to develop models whose outcomes
are minimally affected by such information. Here we systematically investigated
the ability of time-series electronic health record data to predict patient
static information. We found that not only the raw time-series data, but also
learned representations from machine learning models, can be trained to predict
a variety of static information with area under the receiver operating
characteristic curve as high as 0.851 for biological sex, 0.869 for binarized
age and 0.810 for self-reported race. Such high predictive performance can be
extended to a wide range of comorbidity factors and exists even when the model
was trained for different tasks, using different cohorts, using different model
architectures and databases. Given the privacy and fairness concerns these
findings pose, we develop a variational autoencoder-based approach that learns
a structured latent space to disentangle patient-sensitive attributes from
time-series data. Our work thoroughly investigates the ability of machine
learning models to encode patient static information from time-series
electronic health records and introduces a general approach to protect
patient-sensitive attribute information for downstream tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16521">Generating Personalized Insulin Treatments Strategies with Deep Conditional Generative Time Series Models. (arXiv:2309.16521v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Schurch_M/0/1/0/all/0/1">Manuel Sch&#xfc;rch</a>, <a href="http://arxiv.org/find/stat/1/au:+Li_X/0/1/0/all/0/1">Xiang Li</a>, <a href="http://arxiv.org/find/stat/1/au:+Allam_A/0/1/0/all/0/1">Ahmed Allam</a>, <a href="http://arxiv.org/find/stat/1/au:+Rathmes_G/0/1/0/all/0/1">Giulia Rathmes</a>, <a href="http://arxiv.org/find/stat/1/au:+Mollaysa_A/0/1/0/all/0/1">Amina Mollaysa</a>, <a href="http://arxiv.org/find/stat/1/au:+Cavelti_Weder_C/0/1/0/all/0/1">Claudia Cavelti-Weder</a>, <a href="http://arxiv.org/find/stat/1/au:+Krauthammer_M/0/1/0/all/0/1">Michael Krauthammer</a></p>
<p>We propose a novel framework that combines deep generative time series models
with decision theory for generating personalized treatment strategies. It
leverages historical patient trajectory data to jointly learn the generation of
realistic personalized treatment and future outcome trajectories through deep
generative time series models. In particular, our framework enables the
generation of novel multivariate treatment strategies tailored to the
personalized patient history and trained for optimal expected future outcomes
based on conditional expected utility maximization. We demonstrate our
framework by generating personalized insulin treatment strategies and blood
glucose predictions for hospitalized diabetes patients, showcasing the
potential of our approach for generating improved personalized treatment
strategies. Keywords: deep generative model, probabilistic decision support,
personalized treatment generation, insulin and blood glucose prediction
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00023">De-SaTE: Denoising Self-attention Transformer Encoders for Li-ion Battery Health Prognostics. (arXiv:2310.00023v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shinde_G/0/1/0/all/0/1">Gaurav Shinde</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohapatra_R/0/1/0/all/0/1">Rohan Mohapatra</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishan_P/0/1/0/all/0/1">Pooja Krishan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1">Saptarshi Sengupta</a></p>
<p>The usage of Lithium-ion (Li-ion) batteries has gained widespread popularity
across various industries, from powering portable electronic devices to
propelling electric vehicles and supporting energy storage systems. A central
challenge in Li-ion battery reliability lies in accurately predicting their
Remaining Useful Life (RUL), which is a critical measure for proactive
maintenance and predictive analytics. This study presents a novel approach that
harnesses the power of multiple denoising modules, each trained to address
specific types of noise commonly encountered in battery data. Specifically, a
denoising auto-encoder and a wavelet denoiser are used to generate
encoded/decomposed representations, which are subsequently processed through
dedicated self-attention transformer encoders. After extensive experimentation
on NASA and CALCE data, a broad spectrum of health indicator values are
estimated under a set of diverse noise patterns. The reported error metrics on
these data are on par with or better than the state-of-the-art reported in
recent literature.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03059">Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models. (arXiv:2310.03059v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_I/0/1/0/all/0/1">Ivan Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Ray Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Zoey Guo</a></p>
<p>The popularity of pre-trained large models has revolutionized downstream
tasks across diverse fields, such as language, vision, and multi-modality. To
minimize the adaption cost for downstream tasks, many Parameter-Efficient
Fine-Tuning (PEFT) techniques are proposed for language and 2D image
pre-trained models. However, the specialized PEFT method for 3D pre-trained
models is still under-explored. To this end, we introduce Point-PEFT, a novel
framework for adapting point cloud pre-trained models with minimal learnable
parameters. Specifically, for a pre-trained 3D model, we freeze most of its
parameters, and only tune the newly added PEFT modules on downstream tasks,
which consist of a Point-prior Prompt and a Geometry-aware Adapter. The
Point-prior Prompt adopts a set of learnable prompt tokens, for which we
propose to construct a memory bank with domain-specific knowledge, and utilize
a parameter-free attention to enhance the prompt tokens. The Geometry-aware
Adapter aims to aggregate point cloud features within spatial neighborhoods to
capture fine-grained geometric information through local interactions.
Extensive experiments indicate that our Point-PEFT can achieve better
performance than the full fine-tuning on various downstream tasks, while using
only 5% of the trainable parameters, demonstrating the efficiency and
effectiveness of our approach. Code will be released at
https://github.com/Even-JK/PEFT-3D.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03529">Deep Ridgelet Transform: Voice with Koopman Operator Proves Universality of Formal Deep Networks. (arXiv:2310.03529v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sonoda_S/0/1/0/all/0/1">Sho Sonoda</a>, <a href="http://arxiv.org/find/cs/1/au:+Hashimoto_Y/0/1/0/all/0/1">Yuka Hashimoto</a>, <a href="http://arxiv.org/find/cs/1/au:+Ishikawa_I/0/1/0/all/0/1">Isao Ishikawa</a>, <a href="http://arxiv.org/find/cs/1/au:+Ikeda_M/0/1/0/all/0/1">Masahiro Ikeda</a></p>
<p>We identify hidden layers inside a deep neural network (DNN) with group
actions on the data domain, and formulate a formal deep network as a dual voice
transform with respect to the Koopman operator, a linear representation of the
group action. Based on the group theoretic arguments, particularly by using
Schur's lemma, we show a simple proof of the universality of DNNs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03530">Joint Group Invariant Functions on Data-Parameter Domain Induce Universal Neural Networks. (arXiv:2310.03530v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sonoda_S/0/1/0/all/0/1">Sho Sonoda</a>, <a href="http://arxiv.org/find/cs/1/au:+Ishi_H/0/1/0/all/0/1">Hideyuki Ishi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ishikawa_I/0/1/0/all/0/1">Isao Ishikawa</a>, <a href="http://arxiv.org/find/cs/1/au:+Ikeda_M/0/1/0/all/0/1">Masahiro Ikeda</a></p>
<p>The symmetry and geometry of input data are considered to be encoded in the
internal data representation inside the neural network, but the specific
encoding rule has been less investigated. In this study, we present a
systematic method to induce a generalized neural network and its right inverse
operator, called the ridgelet transform, from a joint group invariant function
on the data-parameter domain. Since the ridgelet transform is an inverse, (1)
it can describe the arrangement of parameters for the network to represent a
target function, which is understood as the encoding rule, and (2) it implies
the universality of the network. Based on the group representation theory, we
present a new simple proof of the universality by using Schur's lemma in a
unified manner covering a wide class of networks, for example, the original
ridgelet transform, formal deep networks, and the dual voice transform. Since
traditional universality theorems were demonstrated based on functional
analysis, this study sheds light on the group theoretic aspect of the
approximation theory, connecting geometric deep learning to abstract harmonic
analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04816">Hacking Generative Models with Differentiable Network Bending. (arXiv:2310.04816v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aldegheri_G/0/1/0/all/0/1">Giacomo Aldegheri</a>, <a href="http://arxiv.org/find/cs/1/au:+Rogalska_A/0/1/0/all/0/1">Alina Rogalska</a>, <a href="http://arxiv.org/find/cs/1/au:+Youssef_A/0/1/0/all/0/1">Ahmed Youssef</a>, <a href="http://arxiv.org/find/cs/1/au:+Iofinova_E/0/1/0/all/0/1">Eugenia Iofinova</a></p>
<p>In this work, we propose a method to 'hack' generative models, pushing their
outputs away from the original training distribution towards a new objective.
We inject a small-scale trainable module between the intermediate layers of the
model and train it for a low number of iterations, keeping the rest of the
network frozen. The resulting output images display an uncanny quality, given
by the tension between the original and new objectives that can be exploited
for artistic purposes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07838">Towards the Fundamental Limits of Knowledge Transfer over Finite Domains. (arXiv:2310.07838v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1">Qingyue Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1">Banghua Zhu</a></p>
<p>We characterize the statistical efficiency of knowledge transfer through $n$
samples from a teacher to a probabilistic student classifier with input space
$\mathcal S$ over labels $\mathcal A$. We show that privileged information at
three progressive levels accelerates the transfer. At the first level, only
samples with hard labels are known, via which the maximum likelihood estimator
attains the minimax rate $\sqrt{{|{\mathcal S}||{\mathcal A}|}/{n}}$. The
second level has the teacher probabilities of sampled labels available in
addition, which turns out to boost the convergence rate lower bound to
${{|{\mathcal S}||{\mathcal A}|}/{n}}$. However, under this second data
acquisition protocol, minimizing a naive adaptation of the cross-entropy loss
results in an asymptotically biased student. We overcome this limitation and
achieve the fundamental limit by using a novel empirical variant of the squared
error logit loss. The third level further equips the student with the soft
labels (complete logits) on ${\mathcal A}$ given every sampled input, thereby
provably enables the student to enjoy a rate ${|{\mathcal S}|}/{n}$ free of
$|{\mathcal A}|$. We find any Kullback-Leibler divergence minimizer to be
optimal in the last case. Numerical simulations distinguish the four learners
and corroborate our theory.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08660">Learning RL-Policies for Joint Beamforming Without Exploration: A Batch Constrained Off-Policy Approach. (arXiv:2310.08660v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">Heasung Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Ankireddy_S/0/1/0/all/0/1">Sravan Kumar Ankireddy</a></p>
<p>In this work, we consider the problem of network parameter optimization for
rate maximization. We frame this as a joint optimization problem of power
control, beam forming, and interference cancellation. We consider the setting
where multiple Base Stations (BSs) communicate with multiple user equipment
(UEs). Because of the exponential computational complexity of brute force
search, we instead solve this nonconvex optimization problem using deep
reinforcement learning (RL) techniques. Modern communication systems are
notorious for their difficulty in exactly modeling their behavior. This limits
us in using RL-based algorithms as interaction with the environment is needed
for the agent to explore and learn efficiently. Further, it is ill-advised to
deploy the algorithm in the real world for exploration and learning because of
the high cost of failure. In contrast to the previous RL-based solutions
proposed, such as deep-Q network (DQN) based control, we suggest an offline
model-based approach. We specifically consider discrete batch-constrained deep
Q-learning (BCQ) and show that performance similar to DQN can be achieved with
only a fraction of the data without exploring. This maximizes sample efficiency
and minimizes risk in deploying a new algorithm to commercial networks. We
provide the entire project resource, including code and data, at the following
link: https://github.com/Heasung-Kim/ safe-rl-deployment-for-5g.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.09624">ASSERT: Automated Safety Scenario Red Teaming for Evaluating the Robustness of Large Language Models. (arXiv:2310.09624v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mei_A/0/1/0/all/0/1">Alex Mei</a>, <a href="http://arxiv.org/find/cs/1/au:+Levy_S/0/1/0/all/0/1">Sharon Levy</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">William Yang Wang</a></p>
<p>As large language models are integrated into society, robustness toward a
suite of prompts is increasingly important to maintain reliability in a
high-variance environment.Robustness evaluations must comprehensively
encapsulate the various settings in which a user may invoke an intelligent
system. This paper proposes ASSERT, Automated Safety Scenario Red Teaming,
consisting of three methods -- semantically aligned augmentation, target
bootstrapping, and adversarial knowledge injection. For robust safety
evaluation, we apply these methods in the critical domain of AI safety to
algorithmically generate a test suite of prompts covering diverse robustness
settings -- semantic equivalence, related scenarios, and adversarial. We
partition our prompts into four safety domains for a fine-grained analysis of
how the domain affects model performance. Despite dedicated safeguards in
existing state-of-the-art models, we find statistically significant performance
differences of up to 11% in absolute classification accuracy among semantically
related scenarios and error rates of up to 19% absolute error in zero-shot
adversarial settings, raising concerns for users' physical safety.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.10418">Reading Books is Great, But Not if You Are Driving! Visually Grounded Reasoning about Defeasible Commonsense Norms. (arXiv:2310.10418v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1">Seungju Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Junhyeok Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1">Jack Hessel</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1">Liwei Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1">Jiwan Chung</a>, <a href="http://arxiv.org/find/cs/1/au:+Son_Y/0/1/0/all/0/1">Yejin Son</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1">Yejin Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Youngjae Yu</a></p>
<p>Commonsense norms are defeasible by context: reading books is usually great,
but not when driving a car. While contexts can be explicitly described in
language, in embodied scenarios, contexts are often provided visually. This
type of visually grounded reasoning about defeasible commonsense norms is
generally easy for humans, but (as we show) poses a challenge for machines, as
it necessitates both visual understanding and reasoning about commonsense
norms. We construct a new multimodal benchmark for studying visual-grounded
commonsense norms: NORMLENS. NORMLENS consists of 10K human judgments
accompanied by free-form explanations covering 2K multimodal situations, and
serves as a probe to address two questions: (1) to what extent can models align
with average human judgment? and (2) how well can models explain their
predicted judgments? We find that state-of-the-art model judgments and
explanations are not well-aligned with human annotation. Additionally, we
present a new approach to better align models with humans by distilling social
commonsense knowledge from large language models. The data and code are
released at https://seungjuhan.me/normlens.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.10705">Machine Learning Classification Techniques for Identifying the Defective Patterns in Semiconductor Wafer Maps: A Survey, Empirical, and Experimental Evaluations. (arXiv:2310.10705v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Taha_K/0/1/0/all/0/1">Kamal Taha</a></p>
<p>This survey paper offers a comprehensive review of methodologies utilizing
machine learning (ML) classification techniques for identifying wafer defects
in semiconductor manufacturing. Despite the growing body of research
demonstrating the effectiveness of ML in wafer defect identification, there is
a noticeable absence of comprehensive reviews on this subject. This survey
attempts to fill this void by amalgamating available literature and providing
an in-depth analysis of the advantages, limitations, and potential applications
of various ML classification algorithms in the realm of wafer defect detection.
An innovative taxonomy of methodologies that we present provides a detailed
classification of algorithms into more refined categories and techniques. This
taxonomy follows a four-tier structure, starting from broad methodology
categories and ending with specific sub-techniques. It aids researchers in
comprehending the complex relationships between different algorithms and their
techniques. We employ a rigorous empirical and experimental evaluation to rank
these varying techniques. For the empirical evaluation, we assess techniques
based on a set of four criteria. The experimental evaluation ranks the
algorithms employing the same sub-techniques, techniques, sub-categories, and
categories. This integration of a multi-layered taxonomy, empirical
evaluations, and comparative experiments provides a detailed and holistic
understanding of ML techniques and algorithms for identifying wafer defects.
Additionally, the paper illuminates the future prospects of ML classification
techniques for wafer defect identification, underscoring potential advancements
and opportunities for further research in this field
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.11676">PREM: A Simple Yet Effective Approach for Node-Level Graph Anomaly Detection. (arXiv:2310.11676v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1">Junjun Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yixin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1">Yizhen Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1">Shirui Pan</a></p>
<p>Node-level graph anomaly detection (GAD) plays a critical role in identifying
anomalous nodes from graph-structured data in various domains such as medicine,
social networks, and e-commerce. However, challenges have arisen due to the
diversity of anomalies and the dearth of labeled data. Existing methodologies -
reconstruction-based and contrastive learning - while effective, often suffer
from efficiency issues, stemming from their complex objectives and elaborate
modules. To improve the efficiency of GAD, we introduce a simple method termed
PREprocessing and Matching (PREM for short). Our approach streamlines GAD,
reducing time and memory consumption while maintaining powerful anomaly
detection capabilities. Comprising two modules - a pre-processing module and an
ego-neighbor matching module - PREM eliminates the necessity for
message-passing propagation during training, and employs a simple contrastive
loss, leading to considerable reductions in training time and memory usage.
Moreover, through rigorous evaluations of five real-world datasets, our method
demonstrated robustness and effectiveness. Notably, when validated on the ACM
dataset, PREM achieved a 5% improvement in AUC, a 9-fold increase in training
speed, and sharply reduce memory usage compared to the most efficient baseline.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.11689">Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs. (arXiv:2310.11689v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiefeng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1">Jinsung Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Ebrahimi_S/0/1/0/all/0/1">Sayna Ebrahimi</a>, <a href="http://arxiv.org/find/cs/1/au:+Arik_S/0/1/0/all/0/1">Sercan O Arik</a>, <a href="http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1">Tomas Pfister</a>, <a href="http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1">Somesh Jha</a></p>
<p>Large language models (LLMs) have recently shown great advances in a variety
of tasks, including natural language understanding and generation. However,
their use in high-stakes decision-making scenarios is still limited due to the
potential for errors. Selective prediction is a technique that can be used to
improve the reliability of the LLMs by allowing them to abstain from making
predictions when they are unsure of the answer. In this work, we propose a
novel framework for adaptation with self-evaluation to improve the selective
prediction performance of LLMs. Our framework is based on the idea of using
parameter-efficient tuning to adapt the LLM to the specific task at hand while
improving its ability to perform self-evaluation. We evaluate our method on a
variety of question-answering (QA) datasets and show that it outperforms
state-of-the-art selective prediction methods. For example, on the CoQA
benchmark, our method improves the AUACC from 91.23% to 92.63% and improves the
AUROC from 74.61% to 80.25%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.11864">VQ-NeRF: Neural Reflectance Decomposition and Editing with Vector Quantization. (arXiv:2310.11864v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1">Hongliang Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jingbo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1">Jing Liao</a></p>
<p>We propose VQ-NeRF, a two-branch neural network model that incorporates
Vector Quantization (VQ) to decompose and edit reflectance fields in 3D scenes.
Conventional neural reflectance fields use only continuous representations to
model 3D scenes, despite the fact that objects are typically composed of
discrete materials in reality. This lack of discretization can result in noisy
material decomposition and complicated material editing. To address these
limitations, our model consists of a continuous branch and a discrete branch.
The continuous branch follows the conventional pipeline to predict decomposed
materials, while the discrete branch uses the VQ mechanism to quantize
continuous materials into individual ones. By discretizing the materials, our
model can reduce noise in the decomposition process and generate a segmentation
map of discrete materials. Specific materials can be easily selected for
further editing by clicking on the corresponding area of the segmentation
outcomes. Additionally, we propose a dropout-based VQ codeword ranking strategy
to predict the number of materials in a scene, which reduces redundancy in the
material segmentation process. To improve usability, we also develop an
interactive interface to further assist material editing. We evaluate our model
on both computer-generated and real-world scenes, demonstrating its superior
performance. To the best of our knowledge, our model is the first to enable
discrete material editing in 3D scenes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.14450">TATA: Stance Detection via Topic-Agnostic and Topic-Aware Embeddings. (arXiv:2310.14450v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hanley_H/0/1/0/all/0/1">Hans W. A. Hanley</a>, <a href="http://arxiv.org/find/cs/1/au:+Durumeric_Z/0/1/0/all/0/1">Zakir Durumeric</a></p>
<p>Stance detection is important for understanding different attitudes and
beliefs on the Internet. However, given that a passage's stance toward a given
topic is often highly dependent on that topic, building a stance detection
model that generalizes to unseen topics is difficult. In this work, we propose
using contrastive learning as well as an unlabeled dataset of news articles
that cover a variety of different topics to train topic-agnostic/TAG and
topic-aware/TAW embeddings for use in downstream stance detection. Combining
these embeddings in our full TATA model, we achieve state-of-the-art
performance across several public stance detection datasets (0.771 $F_1$-score
on the Zero-shot VAST dataset). We release our code and data at
https://github.com/hanshanley/tata.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16452">Faithful Path Language Modelling for Explainable Recommendation over Knowledge Graph. (arXiv:2310.16452v2 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Balloccu_G/0/1/0/all/0/1">Giacomo Balloccu</a>, <a href="http://arxiv.org/find/cs/1/au:+Boratto_L/0/1/0/all/0/1">Ludovico Boratto</a>, <a href="http://arxiv.org/find/cs/1/au:+Cancedda_C/0/1/0/all/0/1">Christian Cancedda</a>, <a href="http://arxiv.org/find/cs/1/au:+Fenu_G/0/1/0/all/0/1">Gianni Fenu</a>, <a href="http://arxiv.org/find/cs/1/au:+Marras_M/0/1/0/all/0/1">Mirko Marras</a></p>
<p>Path reasoning methods over knowledge graphs have gained popularity for their
potential to improve transparency in recommender systems. However, the
resulting models still rely on pre-trained knowledge graph embeddings, fail to
fully exploit the interdependence between entities and relations in the KG for
recommendation, and may generate inaccurate explanations. In this paper, we
introduce PEARLM, a novel approach that efficiently captures user behaviour and
product-side knowledge through language modelling. With our approach, knowledge
graph embeddings are directly learned from paths over the KG by the language
model, which also unifies entities and relations in the same optimisation
space. Constraints on the sequence decoding additionally guarantee path
faithfulness with respect to the KG. Experiments on two datasets show the
effectiveness of our approach compared to state-of-the-art baselines. Source
code and datasets: AVAILABLE AFTER GETTING ACCEPTED.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16945">Causal Q-Aggregation for CATE Model Selection. (arXiv:2310.16945v4 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Lan_H/0/1/0/all/0/1">Hui Lan</a>, <a href="http://arxiv.org/find/stat/1/au:+Syrgkanis_V/0/1/0/all/0/1">Vasilis Syrgkanis</a></p>
<p>Accurate estimation of conditional average treatment effects (CATE) is at the
core of personalized decision making. While there is a plethora of models for
CATE estimation, model selection is a nontrivial task, due to the fundamental
problem of causal inference. Recent empirical work provides evidence in favor
of proxy loss metrics with double robust properties and in favor of model
ensembling. However, theoretical understanding is lacking. Direct application
of prior theoretical work leads to suboptimal oracle model selection rates due
to the non-convexity of the model selection problem. We provide regret rates
for the major existing CATE ensembling approaches and propose a new CATE model
ensembling approach based on Q-aggregation using the doubly robust loss. Our
main result shows that causal Q-aggregation achieves statistically optimal
oracle model selection regret rates of $\frac{\log(M)}{n}$ (with $M$ models and
$n$ samples), with the addition of higher-order estimation error terms related
to products of errors in the nuisance functions. Crucially, our regret rate
does not require that any of the candidate CATE models be close to the truth.
We validate our new method on many semi-synthetic datasets and also provide
extensions of our work to CATE model selection with instrumental variables and
unobserved confounding.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17688">Managing AI Risks in an Era of Rapid Progress. (arXiv:2310.17688v2 [cs.CY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1">Yoshua Bengio</a>, <a href="http://arxiv.org/find/cs/1/au:+Hinton_G/0/1/0/all/0/1">Geoffrey Hinton</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1">Andrew Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1">Dawn Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1">Pieter Abbeel</a>, <a href="http://arxiv.org/find/cs/1/au:+Harari_Y/0/1/0/all/0/1">Yuval Noah Harari</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Ya-Qin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1">Lan Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Shalev_Shwartz_S/0/1/0/all/0/1">Shai Shalev-Shwartz</a>, <a href="http://arxiv.org/find/cs/1/au:+Hadfield_G/0/1/0/all/0/1">Gillian Hadfield</a>, <a href="http://arxiv.org/find/cs/1/au:+Clune_J/0/1/0/all/0/1">Jeff Clune</a>, <a href="http://arxiv.org/find/cs/1/au:+Maharaj_T/0/1/0/all/0/1">Tegan Maharaj</a>, <a href="http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1">Frank Hutter</a>, <a href="http://arxiv.org/find/cs/1/au:+Baydin_A/0/1/0/all/0/1">At&#x131;l&#x131;m G&#xfc;ne&#x15f; Baydin</a>, <a href="http://arxiv.org/find/cs/1/au:+McIlraith_S/0/1/0/all/0/1">Sheila McIlraith</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1">Qiqi Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1">Ashwin Acharya</a>, <a href="http://arxiv.org/find/cs/1/au:+Krueger_D/0/1/0/all/0/1">David Krueger</a>, <a href="http://arxiv.org/find/cs/1/au:+Dragan_A/0/1/0/all/0/1">Anca Dragan</a>, <a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1">Philip Torr</a>, <a href="http://arxiv.org/find/cs/1/au:+Russell_S/0/1/0/all/0/1">Stuart Russell</a>, <a href="http://arxiv.org/find/cs/1/au:+Kahneman_D/0/1/0/all/0/1">Daniel Kahneman</a>, <a href="http://arxiv.org/find/cs/1/au:+Brauner_J/0/1/0/all/0/1">Jan Brauner</a>, <a href="http://arxiv.org/find/cs/1/au:+Mindermann_S/0/1/0/all/0/1">S&#xf6;ren Mindermann</a></p>
<p>In this short consensus paper, we outline risks from upcoming, advanced AI
systems. We examine large-scale social harms and malicious uses, as well as an
irreversible loss of human control over autonomous AI systems. In light of
rapid and continuing AI progress, we propose urgent priorities for AI R&amp;D and
governance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.18424">Fast Machine Learning Method with Vector Embedding on Orthonormal Basis and Spectral Transform. (arXiv:2310.18424v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1">Louis Yu Lu</a></p>
<p>This paper presents a novel fast machine learning method that leverages two
techniques: Vector Embedding on Orthonormal Basis (VEOB) and Spectral Transform
(ST). The VEOB converts the original data encoding into a vector embedding with
coordinates projected onto orthonormal bases. The Singular Value Decomposition
(SVD) technique is used to calculate the vector basis and projection
coordinates, leading to an enhanced distance measurement in the embedding space
and facilitating data compression by preserving the projection vectors
associated with the largest singular values. On the other hand, ST transforms
sequence of vector data into spectral space. By applying the Discrete Cosine
Transform (DCT) and selecting the most significant components, it streamlines
the handling of lengthy vector sequences. The paper provides examples of word
embedding, text chunk embedding, and image embedding, implemented in Julia
language with a vector database. It also investigates unsupervised learning and
supervised learning using this method, along with strategies for handling large
data volumes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19680">Integrating Pre-trained Language Model into Neural Machine Translation. (arXiv:2310.19680v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1">Soon-Jae Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeong_C/0/1/0/all/0/1">Chang-Sung Jeong</a></p>
<p>Neural Machine Translation (NMT) has become a significant technology in
natural language processing through extensive research and development.
However, the deficiency of high-quality bilingual language pair data still
poses a major challenge to improving NMT performance. Recent studies are
exploring the use of contextual information from pre-trained language model
(PLM) to address this problem. Yet, the issue of incompatibility between PLM
and NMT model remains unresolved. This study proposes a PLM-integrated NMT
(PiNMT) model to overcome the identified problems. The PiNMT model consists of
three critical components, PLM Multi Layer Converter, Embedding Fusion, and
Cosine Alignment, each playing a vital role in providing effective PLM
information to NMT. Furthermore, two training strategies, Separate Learning
Rates and Dual Step Training, are also introduced in this paper. By
implementing the proposed PiNMT model and training strategy, we achieved
state-of-the-art performance on the IWSLT'14 En$\leftrightarrow$De dataset.
This study's outcomes are noteworthy as they demonstrate a novel approach for
efficiently integrating PLM with NMT to overcome incompatibility and enhance
performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00334">MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows. (arXiv:2311.00334v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stripelis_D/0/1/0/all/0/1">Dimitris Stripelis</a>, <a href="http://arxiv.org/find/cs/1/au:+Anastasiou_C/0/1/0/all/0/1">Chrysovalantis Anastasiou</a>, <a href="http://arxiv.org/find/cs/1/au:+Toral_P/0/1/0/all/0/1">Patrick Toral</a>, <a href="http://arxiv.org/find/cs/1/au:+Asghar_A/0/1/0/all/0/1">Armaghan Asghar</a>, <a href="http://arxiv.org/find/cs/1/au:+Ambite_J/0/1/0/all/0/1">Jose Luis Ambite</a></p>
<p>A Federated Learning (FL) system typically consists of two core processing
entities: the federation controller and the learners. The controller is
responsible for managing the execution of FL workflows across learners and the
learners for training and evaluating federated models over their private
datasets. While executing an FL workflow, the FL system has no control over the
computational resources or data of the participating learners. Still, it is
responsible for other operations, such as model aggregation, task dispatching,
and scheduling. These computationally heavy operations generally need to be
handled by the federation controller. Even though many FL systems have been
recently proposed to facilitate the development of FL workflows, most of these
systems overlook the scalability of the controller. To meet this need, we
designed and developed a novel FL system called MetisFL, where the federation
controller is the first-class citizen. MetisFL re-engineers all the operations
conducted by the federation controller to accelerate the training of
large-scale FL workflows. By quantitatively comparing MetisFL against other
state-of-the-art FL systems, we empirically demonstrate that MetisFL leads to a
10-fold wall-clock time execution boost across a wide range of challenging FL
workflows with increasing model sizes and federation sites.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00567">A Robust Deep Learning Method with Uncertainty Estimation for the Pathological Classification of Renal Cell Carcinoma based on CT Images. (arXiv:2311.00567v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Yao_N/0/1/0/all/0/1">Ni Yao</a>, <a href="http://arxiv.org/find/eess/1/au:+Hu_H/0/1/0/all/0/1">Hang Hu</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_K/0/1/0/all/0/1">Kaicong Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhao_C/0/1/0/all/0/1">Chen Zhao</a>, <a href="http://arxiv.org/find/eess/1/au:+Guo_Y/0/1/0/all/0/1">Yuan Guo</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1">Boya Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Nan_J/0/1/0/all/0/1">Jiaofen Nan</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1">Yanting Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Han_C/0/1/0/all/0/1">Chuang Han</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhu_F/0/1/0/all/0/1">Fubao Zhu</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_W/0/1/0/all/0/1">Weihua Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Tian_L/0/1/0/all/0/1">Li Tian</a></p>
<p>Objectives To develop and validate a deep learning-based diagnostic model
incorporating uncertainty estimation so as to facilitate radiologists in the
preoperative differentiation of the pathological subtypes of renal cell
carcinoma (RCC) based on CT images. Methods Data from 668 consecutive patients,
pathologically proven RCC, were retrospectively collected from Center 1. By
using five-fold cross-validation, a deep learning model incorporating
uncertainty estimation was developed to classify RCC subtypes into clear cell
RCC (ccRCC), papillary RCC (pRCC), and chromophobe RCC (chRCC). An external
validation set of 78 patients from Center 2 further evaluated the model's
performance. Results In the five-fold cross-validation, the model's area under
the receiver operating characteristic curve (AUC) for the classification of
ccRCC, pRCC, and chRCC was 0.868 (95% CI: 0.826-0.923), 0.846 (95% CI:
0.812-0.886), and 0.839 (95% CI: 0.802-0.88), respectively. In the external
validation set, the AUCs were 0.856 (95% CI: 0.838-0.882), 0.787 (95% CI:
0.757-0.818), and 0.793 (95% CI: 0.758-0.831) for ccRCC, pRCC, and chRCC,
respectively. Conclusions The developed deep learning model demonstrated robust
performance in predicting the pathological subtypes of RCC, while the
incorporated uncertainty emphasized the importance of understanding model
confidence, which is crucial for assisting clinical decision-making for
patients with renal tumors. Clinical relevance statement Our deep learning
approach, integrated with uncertainty estimation, offers clinicians a dual
advantage: accurate RCC subtype predictions complemented by diagnostic
confidence references, promoting informed decision-making for patients with
RCC.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00797">Tipping Points of Evolving Epidemiological Networks: Machine Learning-Assisted, Data-Driven Effective Modeling. (arXiv:2311.00797v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Evangelou_N/0/1/0/all/0/1">Nikolaos Evangelou</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_T/0/1/0/all/0/1">Tianqi Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Bello_Rivas_J/0/1/0/all/0/1">Juan M. Bello-Rivas</a>, <a href="http://arxiv.org/find/cs/1/au:+Makeev_A/0/1/0/all/0/1">Alexei Makeev</a>, <a href="http://arxiv.org/find/cs/1/au:+Kevrekidis_I/0/1/0/all/0/1">Ioannis G. Kevrekidis</a></p>
<p>We study the tipping point collective dynamics of an adaptive
susceptible-infected-susceptible (SIS) epidemiological network in a
data-driven, machine learning-assisted manner. We identify a
parameter-dependent effective stochastic differential equation (eSDE) in terms
of physically meaningful coarse mean-field variables through a deep-learning
ResNet architecture inspired by numerical stochastic integrators. We construct
an approximate effective bifurcation diagram based on the identified drift term
of the eSDE and contrast it with the mean-field SIS model bifurcation diagram.
We observe a subcritical Hopf bifurcation in the evolving network's effective
SIS dynamics, that causes the tipping point behavior; this takes the form of
large amplitude collective oscillations that spontaneously -- yet rarely --
arise from the neighborhood of a (noisy) stationary state. We study the
statistics of these rare events both through repeated brute force simulations
and by using established mathematical/computational tools exploiting the
right-hand-side of the identified SDE. We demonstrate that such a collective
SDE can also be identified (and the rare events computations also performed) in
terms of data-driven coarse observables, obtained here via manifold learning
techniques, in particular Diffusion Maps. The workflow of our study is
straightforwardly applicable to other complex dynamics problems exhibiting
tipping point dynamics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01305">AWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language Models. (arXiv:2311.01305v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Baisong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xingwang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Haixiao Xu</a></p>
<p>Large language models(LLMs) exhibit excellent performance across a variety of
tasks, but they come with significant computational and storage costs.
Quantizing these models is an effective way to alleviate this issue. However,
existing methods struggle to strike a balance between model accuracy and
hardware efficiency. This is where we introduce AWEQ, a post-training method
that requires no additional training overhead. AWEQ excels in both
ultra-low-bit quantization and 8-bit weight and activation (W8A8) quantization.
There is an observation that weight quantization is less challenging than
activation quantization. AWEQ transfers the difficulty of activation
quantization to weights using channel equalization, achieving a balance between
the quantization difficulties of both, and thereby maximizing performance. We
have further refined the equalization method to mitigate quantization bias
error, ensuring the robustness of the model. Extensive experiments on popular
models such as LLaMA and OPT demonstrate that AWEQ outperforms all existing
post-training quantization methods for large models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01455">RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation. (arXiv:2311.01455v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yufei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xian_Z/0/1/0/all/0/1">Zhou Xian</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1">Feng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tsun-Hsuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Erickson_Z/0/1/0/all/0/1">Zackory Erickson</a>, <a href="http://arxiv.org/find/cs/1/au:+Held_D/0/1/0/all/0/1">David Held</a>, <a href="http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1">Chuang Gan</a></p>
<p>We present RoboGen, a generative robotic agent that automatically learns
diverse robotic skills at scale via generative simulation. RoboGen leverages
the latest advancements in foundation and generative models. Instead of
directly using or adapting these models to produce policies or low-level
actions, we advocate for a generative scheme, which uses these models to
automatically generate diversified tasks, scenes, and training supervisions,
thereby scaling up robotic skill learning with minimal human supervision. Our
approach equips a robotic agent with a self-guided propose-generate-learn
cycle: the agent first proposes interesting tasks and skills to develop, and
then generates corresponding simulation environments by populating pertinent
objects and assets with proper spatial configurations. Afterwards, the agent
decomposes the proposed high-level task into sub-tasks, selects the optimal
learning approach (reinforcement learning, motion planning, or trajectory
optimization), generates required training supervision, and then learns
policies to acquire the proposed skill. Our work attempts to extract the
extensive and versatile knowledge embedded in large-scale models and transfer
them to the field of robotics. Our fully generative pipeline can be queried
repeatedly, producing an endless stream of skill demonstrations associated with
diverse tasks and environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01544">Divergent Token Metrics: Measuring degradation to prune away LLM components -- and optimize quantization. (arXiv:2311.01544v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Deiseroth_B/0/1/0/all/0/1">Bj&#xf6;rn Deiseroth</a>, <a href="http://arxiv.org/find/cs/1/au:+Meuer_M/0/1/0/all/0/1">Max Meuer</a>, <a href="http://arxiv.org/find/cs/1/au:+Gritsch_N/0/1/0/all/0/1">Nikolas Gritsch</a>, <a href="http://arxiv.org/find/cs/1/au:+Eichenberg_C/0/1/0/all/0/1">Constantin Eichenberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1">Patrick Schramowski</a>, <a href="http://arxiv.org/find/cs/1/au:+Assenmacher_M/0/1/0/all/0/1">Matthias A&#xdf;enmacher</a>, <a href="http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1">Kristian Kersting</a></p>
<p>Large Language Models (LLMs) have reshaped natural language processing with
their impressive capabilities. Their ever-increasing size, however, raised
concerns about their effective deployment and the need for LLM compressions.
This study introduces the Divergent Token metrics (DTMs), a novel approach for
assessing compressed LLMs, addressing the limitations of traditional perplexity
or accuracy measures that fail to accurately reflect text generation quality.
DTMs focus on token divergence, that allow deeper insights into the subtleties
of model compression, i.p. when evaluating component's impacts individually.
Utilizing the First Divergent Token metric (FDTM) in model sparsification
reveals that a quarter of all attention components can be pruned beyond 90% on
the Llama-2 model family, still keeping SOTA performance. For quantization FDTM
suggests that over 80% of parameters can naively be transformed to int8 without
special outlier management. These evaluations indicate the necessity of
choosing appropriate compressions for parameters individually-and that FDTM can
identify those-while standard metrics result in deteriorated outcomes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01568">Anytime-Competitive Reinforcement Learning with Policy Prior. (arXiv:2311.01568v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jianyi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Pengfei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1">Tongxin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wierman_A/0/1/0/all/0/1">Adam Wierman</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1">Shaolei Ren</a></p>
<p>This paper studies the problem of Anytime-Competitive Markov Decision Process
(A-CMDP). Existing works on Constrained Markov Decision Processes (CMDPs) aim
to optimize the expected reward while constraining the expected cost over
random dynamics, but the cost in a specific episode can still be
unsatisfactorily high. In contrast, the goal of A-CMDP is to optimize the
expected reward while guaranteeing a bounded cost in each round of any episode
against a policy prior. We propose a new algorithm, called Anytime-Competitive
Reinforcement Learning (ACRL), which provably guarantees the anytime cost
constraints. The regret analysis shows the policy asymptotically matches the
optimal reward achievable under the anytime competitive constraints.
Experiments on the application of carbon-intelligent computing verify the
reward performance and cost constraint guarantee of ACRL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02332">Multimodal Machine Learning in Image-Based and Clinical Biomedicine: Survey and Prospects. (arXiv:2311.02332v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Warner_E/0/1/0/all/0/1">Elisa Warner</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Joonsang Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1">William Hsu</a>, <a href="http://arxiv.org/find/cs/1/au:+Syeda_Mahmood_T/0/1/0/all/0/1">Tanveer Syeda-Mahmood</a>, <a href="http://arxiv.org/find/cs/1/au:+Kahn_C/0/1/0/all/0/1">Charles Kahn</a>, <a href="http://arxiv.org/find/cs/1/au:+Gevaert_O/0/1/0/all/0/1">Olivier Gevaert</a>, <a href="http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1">Arvind Rao</a></p>
<p>Machine learning (ML) applications in medical artificial intelligence (AI)
systems have shifted from traditional and statistical methods to increasing
application of deep learning models. This survey navigates the current
landscape of multimodal ML, focusing on its profound impact on medical image
analysis and clinical decision support systems. Emphasizing challenges and
innovations in addressing multimodal representation, fusion, translation,
alignment, and co-learning, the paper explores the transformative potential of
multimodal models for clinical predictions. It also questions practical
implementation of such models, bringing attention to the dynamics between
decision support systems and healthcare providers. Despite advancements,
challenges such as data biases and the scarcity of "big data" in many
biomedical domains persist. We conclude with a discussion on effective
innovation and collaborative efforts to further the miss
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02577">Steady-State Analysis and Online Learning for Queues with Hawkes Arrivals. (arXiv:2311.02577v2 [math.PR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Chen_X/0/1/0/all/0/1">Xinyun Chen</a>, <a href="http://arxiv.org/find/math/1/au:+Hong_G/0/1/0/all/0/1">Guiyu Hong</a></p>
<p>We investigate the long-run behavior of single-server queues with Hawkes
arrivals and general service distributions and related optimization problems.
In detail, utilizing novel coupling techniques, we establish finite moment
bounds for the stationary distribution of the workload and busy period
processes. In addition, we are able to show that, those queueing processes
converge exponentially fast to their stationary distribution. Based on these
theoretic results, we develop an efficient numerical algorithm to solve the
optimal staffing problem for the Hawkes queues in a data-driven manner.
Numerical results indicate a sharp difference in staffing for Hawkes queues,
compared to the classic GI/GI/1 model, especially in the heavy-traffic regime.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02775">ChaTA: Towards an Intelligent Question-Answer Teaching Assistant using Open-Source LLMs. (arXiv:2311.02775v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hicke_Y/0/1/0/all/0/1">Yann Hicke</a>, <a href="http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1">Anmol Agarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1">Qianou Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Denny_P/0/1/0/all/0/1">Paul Denny</a></p>
<p>Responding to the thousands of student questions on online QA platforms each
semester has a considerable human cost, particularly in computing courses with
rapidly growing enrollments. To address the challenges of scalable and
intelligent question-answering (QA), we introduce an innovative solution that
leverages open-source Large Language Models (LLMs) from the LLaMA-2 family to
ensure data privacy. Our approach combines augmentation techniques such as
retrieval augmented generation (RAG), supervised fine-tuning (SFT), and
learning from human preferences data using Direct Preference Optimization
(DPO). Through extensive experimentation on a Piazza dataset from an
introductory CS course, comprising 10,000 QA pairs and 1,500 pairs of
preference data, we demonstrate a significant 30% improvement in the quality of
answers, with RAG being a particularly impactful addition. Our contributions
include the development of a novel architecture for educational QA, extensive
evaluations of LLM performance utilizing both human assessments and LLM-based
metrics, and insights into the challenges and future directions of educational
data processing. This work paves the way for the development of CHATA, an
intelligent QA assistant customizable for courses with an online QA platform
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02921">Edge2Node: Reducing Edge Prediction to Node Classification. (arXiv:2311.02921v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rahmati_Z/0/1/0/all/0/1">Zahed Rahmati</a>, <a href="http://arxiv.org/find/cs/1/au:+Rahmati_A/0/1/0/all/0/1">Ali Rahmati</a>, <a href="http://arxiv.org/find/cs/1/au:+Kazemi_D/0/1/0/all/0/1">Dariush Kazemi</a></p>
<p>Despite the success of graph neural network models in node classification,
edge prediction (the task of predicting missing or potential links between
nodes in a graph) remains a challenging problem for these models. A common
approach for edge prediction is to first obtain the embeddings of two nodes,
and then a predefined scoring function is used to predict the existence of an
edge between the two nodes. In this paper, we introduce a new approach called
Edge2Node (E2N) which directly obtains an embedding for each edge, without the
need for a scoring function. To do this, we create a new graph H based on the
graph G given for the edge prediction task, and then reduce the edge prediction
task on G to a node classification task on H. Our E2N method can be easily
applied to any edge prediction task with superior performance and lower
computational costs.
</p>
<p>Our E2N method beats the best-known methods on the leaderboards for ogbl-ppa,
ogbl-collab, and ogbl-ddi datasets by 25.89%, 24.19%, and 0.34% improvements,
respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03764">Neuro-GPT: Developing A Foundation Model for EEG. (arXiv:2311.03764v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cui_W/0/1/0/all/0/1">Wenhui Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeong_W/0/1/0/all/0/1">Woojae Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Tholke_P/0/1/0/all/0/1">Philipp Th&#xf6;lke</a>, <a href="http://arxiv.org/find/cs/1/au:+Medani_T/0/1/0/all/0/1">Takfarinas Medani</a>, <a href="http://arxiv.org/find/cs/1/au:+Jerbi_K/0/1/0/all/0/1">Karim Jerbi</a>, <a href="http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1">Anand A. Joshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Leahy_R/0/1/0/all/0/1">Richard M. Leahy</a></p>
<p>To handle the scarcity and heterogeneity of electroencephalography (EEG) data
for Brain-Computer Interface (BCI) tasks, and to harness the power of large
publicly available data sets, we propose Neuro-GPT, a foundation model
consisting of an EEG encoder and a GPT model. The foundation model is
pre-trained on a large-scale data set using a self-supervised task that learns
how to reconstruct masked EEG segments. We then fine-tune the model on a Motor
Imagery Classification task to validate its performance in a low-data regime (9
subjects). Our experiments demonstrate that applying a foundation model can
significantly improve classification performance compared to a model trained
from scratch, which provides evidence for the generalizability of the
foundation model and its ability to address challenges of data scarcity and
heterogeneity in EEG.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04254">Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation. (arXiv:2311.04254v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1">Ruomeng Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chaoyun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yong Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1">Minghua Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_S/0/1/0/all/0/1">Si Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajmohan_S/0/1/0/all/0/1">Saravan Rajmohan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1">Qingwei Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dongmei Zhang</a></p>
<p>Recent advancements in Large Language Models (LLMs) have revolutionized
decision-making by breaking down complex problems into more manageable language
sequences referred to as ``thoughts''. An effective thought design should
consider three key perspectives: performance, efficiency, and flexibility.
However, existing thought can at most exhibit two of these attributes. To
address these limitations, we introduce a novel thought prompting approach
called ``Everything of Thoughts'' (XoT) to defy the law of ``Penrose triangle
of existing thought paradigms. XoT leverages pretrained reinforcement learning
and Monte Carlo Tree Search (MCTS) to incorporate external domain knowledge
into thoughts, thereby enhancing LLMs' capabilities and enabling them to
generalize to unseen problems efficiently. Through the utilization of the
MCTS-LLM collaborative thought revision framework, this approach autonomously
produces high-quality comprehensive cognitive mappings with minimal LLM
interactions. Additionally, XoT empowers LLMs to engage in unconstrained
thinking, allowing for flexible cognitive mappings for problems with multiple
solutions. We evaluate XoT on several challenging multi-solution
problem-solving tasks, including Game of 24, 8-Puzzle, and Pocket Cube. Our
results demonstrate that XoT significantly outperforms existing approaches.
Notably, XoT can yield multiple solutions with just one LLM call, showcasing
its remarkable proficiency in addressing complex problems across diverse
domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04943">MathNAS: If Blocks Have a Role in Mathematical Architecture Design. (arXiv:2311.04943v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qinsi_W/0/1/0/all/0/1">Wang Qinsi</a>, <a href="http://arxiv.org/find/cs/1/au:+Jinghan_K/0/1/0/all/0/1">Ke Jinghan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhi_L/0/1/0/all/0/1">Liang Zhi</a>, <a href="http://arxiv.org/find/cs/1/au:+Sihai_Z/0/1/0/all/0/1">Zhang Sihai</a></p>
<p>Neural Architecture Search (NAS) has emerged as a favoured method for
unearthing effective neural architectures. Recent development of large models
has intensified the demand for faster search speeds and more accurate search
results. However, designing large models by NAS is challenging due to the
dramatical increase of search space and the associated huge performance
evaluation cost. Consider a typical modular search space widely used in NAS, in
which a neural architecture consists of $m$ block nodes and a block node has
$n$ alternative blocks. Facing the space containing $n^m$ candidate networks,
existing NAS methods attempt to find the best one by searching and evaluating
candidate networks directly.Different from the general strategy that takes
architecture search as a whole problem, we propose a novel divide-and-conquer
strategy by making use of the modular nature of the search space.Here, we
introduce MathNAS, a general NAS framework based on mathematical programming.In
MathNAS, the performances of the $m*n$ possible building blocks in the search
space are calculated first, and then the performance of a network is directly
predicted based on the performances of its building blocks. Although estimating
block performances involves network training, just as what happens for network
performance evaluation in existing NAS methods, predicting network performance
is completely training-free and thus extremely fast. In contrast to the $n^m$
candidate networks to evaluate in existing NAS methods, which require training
and a formidable computational burden, there are only $m*n$ possible blocks to
handle in MathNAS. Therefore, our approach effectively reduces the complexity
of network performance evaluation.Our code is available at
https://github.com/wangqinsi1/MathNAS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05304">Data Valuation and Detections in Federated Learning. (arXiv:2311.05304v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wenqian Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1">Shuran Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1">Fengrui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1">Yan Pang</a></p>
<p>Federated Learning (FL) enables collaborative model training while preserving
the privacy of raw data. A challenge in this framework is the fair and
efficient valuation of data, which is crucial for incentivizing clients to
contribute high-quality data in the FL task. In scenarios involving numerous
data clients within FL, it is often the case that only a subset of clients and
datasets are pertinent to a specific learning task, while others might have
either a negative or negligible impact on the model training process. This
paper introduces a novel privacy-preserving method for evaluating client
contributions and selecting relevant datasets without a pre-specified training
algorithm in an FL task. Our proposed approach FedBary, utilizes Wasserstein
distance within the federated context, offering a new solution for data
valuation in the FL framework. This method ensures transparent data valuation
and efficient computation of the Wasserstein barycenter and reduces the
dependence on validation datasets. Through extensive empirical experiments and
theoretical analyses, we demonstrate the potential of this data valuation
method as a promising avenue for FL research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.08493">Time Travel in LLMs: Tracing Data Contamination in Large Language Models. (arXiv:2308.08493v2 [cs.CL] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Golchin_S/0/1/0/all/0/1">Shahriar Golchin</a>, <a href="http://arxiv.org/find/cs/1/au:+Surdeanu_M/0/1/0/all/0/1">Mihai Surdeanu</a></p>
<p>Data contamination, i.e., the presence of test data from downstream tasks in
the training data of large language models (LLMs), is a potential major issue
in measuring LLMs' real effectiveness on other tasks. We propose a
straightforward yet effective method for identifying data contamination within
LLMs. At its core, our approach starts by identifying potential contamination
at the instance level; using this information, our approach then assesses wider
contamination at the partition level. To estimate contamination of individual
instances, we employ "guided instruction:" a prompt consisting of the dataset
name, partition type, and the random-length initial segment of a reference
instance, asking the LLM to complete it. An instance is flagged as contaminated
if the LLM's output either exactly or nearly matches the latter segment of the
reference. To understand if an entire partition is contaminated, we propose two
ideas. The first idea marks a dataset partition as contaminated if the average
overlap score with the reference instances (as measured by ROUGE-L or BLEURT)
is statistically significantly better with the completions from guided
instruction compared to a "general instruction" that does not include the
dataset and partition name. The second idea marks a dataset partition as
contaminated if a classifier based on GPT-4 with few-shot in-context learning
prompt marks multiple generated completions as exact/near-exact matches of the
corresponding reference instances. Our best method achieves an accuracy between
92% and 100% in detecting if an LLM is contaminated with seven datasets,
containing train and test/validation partitions, when contrasted with manual
evaluation by human experts. Further, our findings indicate that GPT-4 is
contaminated with AG News, WNLI, and XSum datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.17368">Machine Learning for Practical Quantum Error Mitigation. (arXiv:2309.17368v1 [quant-ph] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Liao_H/0/1/0/all/0/1">Haoran Liao</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Wang_D/0/1/0/all/0/1">Derek S. Wang</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Sitdikov_I/0/1/0/all/0/1">Iskandar Sitdikov</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Salcedo_C/0/1/0/all/0/1">Ciro Salcedo</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Seif_A/0/1/0/all/0/1">Alireza Seif</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Minev_Z/0/1/0/all/0/1">Zlatko K. Minev</a></p>
<p>Quantum computers are actively competing to surpass classical supercomputers,
but quantum errors remain their chief obstacle. The key to overcoming these on
near-term devices has emerged through the field of quantum error mitigation,
enabling improved accuracy at the cost of additional runtime. In practice,
however, the success of mitigation is limited by a generally exponential
overhead. Can classical machine learning address this challenge on today's
quantum computers? Here, through both simulations and experiments on
state-of-the-art quantum computers using up to 100 qubits, we demonstrate that
machine learning for quantum error mitigation (ML-QEM) can drastically reduce
overheads, maintain or even surpass the accuracy of conventional methods, and
yield near noise-free results for quantum algorithms. We benchmark a variety of
machine learning models -- linear regression, random forests, multi-layer
perceptrons, and graph neural networks -- on diverse classes of quantum
circuits, over increasingly complex device-noise profiles, under interpolation
and extrapolation, and for small and large quantum circuits. These tests employ
the popular digital zero-noise extrapolation method as an added reference. We
further show how to scale ML-QEM to classically intractable quantum circuits by
mimicking the results of traditional mitigation results, while significantly
reducing overhead. Our results highlight the potential of classical machine
learning for practical quantum computation.
</p>
</p>
</div>

    </div>
    </body>
    