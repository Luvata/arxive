<!DOCTYPE html>
<html>
<head>
<title>2023-07-19-cs-cv</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2307.08716">Enforcing Topological Interaction between Implicit Surfaces via Uniform Sampling. (arXiv:2307.08716v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1">Hieu Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Talabot_N/0/1/0/all/0/1">Nicolas Talabot</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jiancheng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1">Pascal Fua</a></p>
<p>Objects interact with each other in various ways, including containment,
contact, or maintaining fixed distances. Ensuring these topological
interactions is crucial for accurate modeling in many scenarios. In this paper,
we propose a novel method to refine 3D object representations, ensuring that
their surfaces adhere to a topological prior. Our key observation is that the
object interaction can be observed via a stochastic approximation method: the
statistic of signed distances between a large number of random points to the
object surfaces reflect the interaction between them. Thus, the object
interaction can be indirectly manipulated by using choosing a set of points as
anchors to refine the object surfaces. In particular, we show that our method
can be used to enforce two objects to have a specific contact ratio while
having no surface intersection. The conducted experiments show that our
proposed method enables accurate 3D reconstruction of human hearts, ensuring
proper topological connectivity between components. Further, we show that our
proposed method can be used to simulate various ways a hand can interact with
an arbitrary object.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08717">Untrained neural network embedded Fourier phase retrieval from few measurements. (arXiv:2307.08717v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ma_L/0/1/0/all/0/1">Liyuan Ma</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1">Hongxia Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Leng_N/0/1/0/all/0/1">Ningyi Leng</a>, <a href="http://arxiv.org/find/eess/1/au:+Yuan_Z/0/1/0/all/0/1">Ziyang Yuan</a></p>
<p>Fourier phase retrieval (FPR) is a challenging task widely used in various
applications. It involves recovering an unknown signal from its Fourier
phaseless measurements. FPR with few measurements is important for reducing
time and hardware costs, but it suffers from serious ill-posedness. Recently,
untrained neural networks have offered new approaches by introducing learned
priors to alleviate the ill-posedness without requiring any external data.
However, they may not be ideal for reconstructing fine details in images and
can be computationally expensive. This paper proposes an untrained neural
network (NN) embedded algorithm based on the alternating direction method of
multipliers (ADMM) framework to solve FPR with few measurements. Specifically,
we use a generative network to represent the image to be recovered, which
confines the image to the space defined by the network structure. To improve
the ability to represent high-frequency information, total variation (TV)
regularization is imposed to facilitate the recovery of local structures in the
image. Furthermore, to reduce the computational cost mainly caused by the
parameter updates of the untrained NN, we develop an accelerated algorithm that
adaptively trades off between explicit and implicit regularization.
Experimental results indicate that the proposed algorithm outperforms existing
untrained NN-based algorithms with fewer computational resources and even
performs competitively against trained NN-based algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08723">Revisiting Scene Text Recognition: A Data Perspective. (arXiv:2307.08723v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1">Qing Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiapeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1">Dezhi Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chongyu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1">Lianwen Jin</a></p>
<p>This paper aims to re-assess scene text recognition (STR) from a
data-oriented perspective. We begin by revisiting the six commonly used
benchmarks in STR and observe a trend of performance saturation, whereby only
2.91% of the benchmark images cannot be accurately recognized by an ensemble of
13 representative models. While these results are impressive and suggest that
STR could be considered solved, however, we argue that this is primarily due to
the less challenging nature of the common benchmarks, thus concealing the
underlying issues that STR faces. To this end, we consolidate a large-scale
real STR dataset, namely Union14M, which comprises 4 million labeled images and
10 million unlabeled images, to assess the performance of STR models in more
complex real-world scenarios. Our experiments demonstrate that the 13 models
can only achieve an average accuracy of 66.53% on the 4 million labeled images,
indicating that STR still faces numerous challenges in the real world. By
analyzing the error patterns of the 13 models, we identify seven open
challenges in STR and develop a challenge-driven benchmark consisting of eight
distinct subsets to facilitate further progress in the field. Our exploration
demonstrates that STR is far from being solved and leveraging data may be a
promising solution. In this regard, we find that utilizing the 10 million
unlabeled images through self-supervised pre-training can significantly improve
the robustness of STR model in real-world scenarios and leads to
state-of-the-art performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08727">Semantic Counting from Self-Collages. (arXiv:2307.08727v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Knobel_L/0/1/0/all/0/1">Lukas Knobel</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1">Tengda Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Asano_Y/0/1/0/all/0/1">Yuki M. Asano</a></p>
<p>While recent supervised methods for reference-based object counting continue
to improve the performance on benchmark datasets, they have to rely on small
datasets due to the cost associated with manually annotating dozens of objects
in images. We propose Unsupervised Counter (UnCo), a model that can learn this
task without requiring any manual annotations. To this end, we construct
"SelfCollages", images with various pasted objects as training samples, that
provide a rich learning signal covering arbitrary object types and counts. Our
method builds on existing unsupervised representations and segmentation
techniques to successfully demonstrate the ability to count objects without
manual supervision. Our experiments show that our method not only outperforms
simple baselines and generic models such as FasterRCNN, but also matches the
performance of supervised counting models in some domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08758">Linking Physics and Psychology of Bistable Perception Using an Eye Blink Inspired Quantum Harmonic Oscillator Model. (arXiv:2307.08758v1 [q-bio.NC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Maksymov_I/0/1/0/all/0/1">Ivan S. Maksymov</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Pogrebna_G/0/1/0/all/0/1">Ganna Pogrebna</a></p>
<p>This paper introduces a novel quantum-mechanical model that describes
psychological phenomena using the analogy of a harmonic oscillator represented
by an electron trapped in a potential well. Study~1 demonstrates the
application of the proposed model to bistable perception of ambiguous figures
(i.e., optical illusions), exemplified by the Necker cube. While prior research
has theoretically linked quantum mechanics to psychological phenomena, in
Study~2 we demonstrate a viable physiological connection between physics and
bistable perception. To that end, the model draws parallels between quantum
tunneling of an electron through a potential energy barrier and an eye blink,
an action known to trigger perceptual reversals. Finally, we discuss the
ability of the model to capture diverse optical illusions and other
psychological phenomena, including cognitive dissonance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08763">Video-Mined Task Graphs for Keystep Recognition in Instructional Videos. (arXiv:2307.08763v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ashutosh_K/0/1/0/all/0/1">Kumar Ashutosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramakrishnan_S/0/1/0/all/0/1">Santhosh Kumar Ramakrishnan</a>, <a href="http://arxiv.org/find/cs/1/au:+Afouras_T/0/1/0/all/0/1">Triantafyllos Afouras</a>, <a href="http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1">Kristen Grauman</a></p>
<p>Procedural activity understanding requires perceiving human actions in terms
of a broader task, where multiple keysteps are performed in sequence across a
long video to reach a final goal state -- such as the steps of a recipe or a
DIY fix-it task. Prior work largely treats keystep recognition in isolation of
this broader structure, or else rigidly confines keysteps to align with a
predefined sequential script. We propose discovering a task graph automatically
from how-to videos to represent probabilistically how people tend to execute
keysteps, and then leverage this graph to regularize keystep recognition in
novel videos. On multiple datasets of real-world instructional videos, we show
the impact: more reliable zero-shot keystep localization and improved video
representation learning, exceeding the state of the art.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08771">UPSCALE: Unconstrained Channel Pruning. (arXiv:2307.08771v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wan_A/0/1/0/all/0/1">Alvin Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hao_H/0/1/0/all/0/1">Hanxiang Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Patnaik_K/0/1/0/all/0/1">Kaushik Patnaik</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yueyang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hadad_O/0/1/0/all/0/1">Omer Hadad</a>, <a href="http://arxiv.org/find/cs/1/au:+Guera_D/0/1/0/all/0/1">David G&#xfc;era</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1">Zhile Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Shan_Q/0/1/0/all/0/1">Qi Shan</a></p>
<p>As neural networks grow in size and complexity, inference speeds decline. To
combat this, one of the most effective compression techniques -- channel
pruning -- removes channels from weights. However, for multi-branch segments of
a model, channel removal can introduce inference-time memory copies. In turn,
these copies increase inference latency -- so much so that the pruned model can
be slower than the unpruned model. As a workaround, pruners conventionally
constrain certain channels to be pruned together. This fully eliminates memory
copies but, as we show, significantly impairs accuracy. We now have a dilemma:
Remove constraints but increase latency, or add constraints and impair
accuracy. In response, our insight is to reorder channels at export time, (1)
reducing latency by reducing memory copies and (2) improving accuracy by
removing constraints. Using this insight, we design a generic algorithm UPSCALE
to prune models with any pruning pattern. By removing constraints from existing
pruners, we improve ImageNet accuracy for post-training pruned models by 2.1
points on average -- benefiting DenseNet (+16.9), EfficientNetV2 (+7.9), and
ResNet (+6.2). Furthermore, by reordering channels, UPSCALE improves inference
speeds by up to 2x over a baseline export.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08779">Similarity Min-Max: Zero-Shot Day-Night Domain Adaptation. (arXiv:2307.08779v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1">Rundong Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenjing Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1">Wenhan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiaying Liu</a></p>
<p>Low-light conditions not only hamper human visual experience but also degrade
the model's performance on downstream vision tasks. While existing works make
remarkable progress on day-night domain adaptation, they rely heavily on domain
knowledge derived from the task-specific nighttime dataset. This paper
challenges a more complicated scenario with border applicability, i.e.,
zero-shot day-night domain adaptation, which eliminates reliance on any
nighttime data. Unlike prior zero-shot adaptation approaches emphasizing either
image-level translation or model-level adaptation, we propose a similarity
min-max paradigm that considers them under a unified framework. On the image
level, we darken images towards minimum feature similarity to enlarge the
domain gap. Then on the model level, we maximize the feature similarity between
the darkened images and their normal-light counterparts for better model
adaptation. To the best of our knowledge, this work represents the pioneering
effort in jointly optimizing both aspects, resulting in a significant
improvement of model generalizability. Extensive experiments demonstrate our
method's effectiveness and broad applicability on various nighttime vision
tasks, including classification, semantic segmentation, visual place
recognition, and video action recognition. Code and pre-trained models are
available at https://red-fairy.github.io/ZeroShotDayNightDA-Webpage/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08781">The FathomNet2023 Competition Dataset. (arXiv:2307.08781v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Orenstein_E/0/1/0/all/0/1">Eric Orenstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Barnard_K/0/1/0/all/0/1">Kevin Barnard</a>, <a href="http://arxiv.org/find/cs/1/au:+Lundsten_L/0/1/0/all/0/1">Lonny Lundsten</a>, <a href="http://arxiv.org/find/cs/1/au:+Patterson_G/0/1/0/all/0/1">Genevi&#xe8;ve Patterson</a>, <a href="http://arxiv.org/find/cs/1/au:+Woodward_B/0/1/0/all/0/1">Benjamin Woodward</a>, <a href="http://arxiv.org/find/cs/1/au:+Katija_K/0/1/0/all/0/1">Kakani Katija</a></p>
<p>Ocean scientists have been collecting visual data to study marine organisms
for decades. These images and videos are extremely valuable both for basic
science and environmental monitoring tasks. There are tools for automatically
processing these data, but none that are capable of handling the extreme
variability in sample populations, image quality, and habitat characteristics
that are common in visual sampling of the ocean. Such distribution shifts can
occur over very short physical distances and in narrow time windows. Creating
models that are able to recognize when an image or video sequence contains a
new organism, an unusual collection of animals, or is otherwise out-of-sample
is critical to fully leverage visual data in the ocean. The FathomNet2023
competition dataset presents a realistic scenario where the set of animals in
the target data differs from the training data. The challenge is both to
identify the organisms in a target image and assess whether it is
out-of-sample.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08789">Harnessing the Power of AI based Image Generation Model DALLE 2 in Agricultural Settings. (arXiv:2307.08789v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sapkota_R/0/1/0/all/0/1">Ranjan Sapkota</a></p>
<p>This study investigates the potential impact of artificial intelligence (AI)
on the enhancement of visualization processes in the agricultural sector, using
the advanced AI image generator, DALLE 2, developed by OpenAI. By
synergistically utilizing the natural language processing proficiency of
chatGPT and the generative prowess of the DALLE 2 model, which employs a
Generative Adversarial Networks (GANs) framework, our research offers an
innovative method to transform textual descriptors into realistic visual
content. Our rigorously assembled datasets include a broad spectrum of
agricultural elements such as fruits, plants, and scenarios differentiating
crops from weeds, maintained for AI-generated versus original images. The
quality and accuracy of the AI-generated images were evaluated via established
metrics including mean squared error (MSE), peak signal-to-noise ratio (PSNR),
and feature similarity index (FSIM). The results underline the significant role
of the DALLE 2 model in enhancing visualization processes in agriculture,
aiding in more informed decision-making, and improving resource distribution.
The outcomes of this research highlight the imminent rise of an AI-led
transformation in the realm of precision agriculture.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08809">Local or Global: Selective Knowledge Assimilation for Federated Learning with Limited Labels. (arXiv:2307.08809v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1">Yae Jee Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Joshi_G/0/1/0/all/0/1">Gauri Joshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Dimitriadis_D/0/1/0/all/0/1">Dimitrios Dimitriadis</a></p>
<p>Many existing FL methods assume clients with fully-labeled data, while in
realistic settings, clients have limited labels due to the expensive and
laborious process of labeling. Limited labeled local data of the clients often
leads to their local model having poor generalization abilities to their larger
unlabeled local data, such as having class-distribution mismatch with the
unlabeled data. As a result, clients may instead look to benefit from the
global model trained across clients to leverage their unlabeled data, but this
also becomes difficult due to data heterogeneity across clients. In our work,
we propose FedLabel where clients selectively choose the local or global model
to pseudo-label their unlabeled data depending on which is more of an expert of
the data. We further utilize both the local and global models' knowledge via
global-local consistency regularization which minimizes the divergence between
the two models' outputs when they have identical pseudo-labels for the
unlabeled data. Unlike other semi-supervised FL baselines, our method does not
require additional experts other than the local or global model, nor require
additional parameters to be communicated. We also do not assume any
server-labeled data or fully labeled clients. For both cross-device and
cross-silo settings, we show that FedLabel outperforms other semi-supervised FL
baselines by $8$-$24\%$, and even outperforms standard fully supervised FL
baselines ($100\%$ labeled data) with only $5$-$20\%$ of labeled data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08837">DARTS: Double Attention Reference-based Transformer for Super-resolution. (arXiv:2307.08837v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aslahishahri_M/0/1/0/all/0/1">Masoomeh Aslahishahri</a>, <a href="http://arxiv.org/find/cs/1/au:+Ubbens_J/0/1/0/all/0/1">Jordan Ubbens</a>, <a href="http://arxiv.org/find/cs/1/au:+Stavness_I/0/1/0/all/0/1">Ian Stavness</a></p>
<p>We present DARTS, a transformer model for reference-based image
super-resolution. DARTS learns joint representations of two image distributions
to enhance the content of low-resolution input images through matching
correspondences learned from high-resolution reference images. Current
state-of-the-art techniques in reference-based image super-resolution are based
on a multi-network, multi-stage architecture. In this work, we adapt the double
attention block from the GAN literature, processing the two visual streams
separately and combining self-attention and cross-attention blocks through a
gating attention strategy. Our work demonstrates how the attention mechanism
can be adapted for the particular requirements of reference-based image
super-resolution, significantly simplifying the architecture and training
pipeline. We show that our transformer-based model performs competitively with
state-of-the-art models, while maintaining a simpler overall architecture and
training process. In particular, we obtain state-of-the-art on the SUN80
dataset, with a PSNR/SSIM of 29.83 / .809. These results show that attention
alone is sufficient for the RSR task, without multiple purpose-built
subnetworks, knowledge distillation, or multi-stage training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08850">LiDAR-BEVMTN: Real-Time LiDAR Bird&#x27;s-Eye View Multi-Task Perception Network for Autonomous Driving. (arXiv:2307.08850v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mohapatra_S/0/1/0/all/0/1">Sambit Mohapatra</a>, <a href="http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1">Senthil Yogamani</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1">Varun Ravi Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Milz_S/0/1/0/all/0/1">Stefan Milz</a>, <a href="http://arxiv.org/find/cs/1/au:+Gotzig_H/0/1/0/all/0/1">Heinrich Gotzig</a>, <a href="http://arxiv.org/find/cs/1/au:+Mader_P/0/1/0/all/0/1">Patrick M&#xe4;der</a></p>
<p>LiDAR is crucial for robust 3D scene perception in autonomous driving. LiDAR
perception has the largest body of literature after camera perception. However,
multi-task learning across tasks like detection, segmentation, and motion
estimation using LiDAR remains relatively unexplored, especially on
automotive-grade embedded platforms. We present a real-time multi-task
convolutional neural network for LiDAR-based object detection, semantics, and
motion segmentation. The unified architecture comprises a shared encoder and
task-specific decoders, enabling joint representation learning. We propose a
novel Semantic Weighting and Guidance (SWAG) module to transfer semantic
features for improved object detection selectively. Our heterogeneous training
scheme combines diverse datasets and exploits complementary cues between tasks.
The work provides the first embedded implementation unifying these key
perception tasks from LiDAR point clouds achieving 3ms latency on the embedded
NVIDIA Xavier platform. We achieve state-of-the-art results for two tasks,
semantic and motion segmentation, and close to state-of-the-art performance for
3D object detection. By maximizing hardware efficiency and leveraging
multi-task synergies, our method delivers an accurate and efficient solution
tailored for real-world automated driving deployment. Qualitative results can
be seen at https://youtu.be/H-hWRzv2lIY.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08880">Modular Neural Network Approaches for Surgical Image Recognition. (arXiv:2307.08880v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Salem_N/0/1/0/all/0/1">Nosseiba Ben Salem</a>, <a href="http://arxiv.org/find/cs/1/au:+Bennani_Y/0/1/0/all/0/1">Younes Bennani</a>, <a href="http://arxiv.org/find/cs/1/au:+Karkazan_J/0/1/0/all/0/1">Joseph Karkazan</a>, <a href="http://arxiv.org/find/cs/1/au:+Barbara_A/0/1/0/all/0/1">Abir Barbara</a>, <a href="http://arxiv.org/find/cs/1/au:+Dacheux_C/0/1/0/all/0/1">Charles Dacheux</a>, <a href="http://arxiv.org/find/cs/1/au:+Gregory_T/0/1/0/all/0/1">Thomas Gregory</a></p>
<p>Deep learning-based applications have seen a lot of success in recent years.
Text, audio, image, and video have all been explored with great success using
deep learning approaches. The use of convolutional neural networks (CNN) in
computer vision, in particular, has yielded reliable results. In order to
achieve these results, a large amount of data is required. However, the dataset
cannot always be accessible. Moreover, annotating data can be difficult and
time-consuming. Self-training is a semi-supervised approach that managed to
alleviate this problem and achieve state-of-the-art performances. Theoretical
analysis even proved that it may result in a better generalization than a
normal classifier. Another problem neural networks can face is the increasing
complexity of modern problems, requiring a high computational and storage cost.
One way to mitigate this issue, a strategy that has been inspired by human
cognition known as modular learning, can be employed. The principle of the
approach is to decompose a complex problem into simpler sub-tasks. This
approach has several advantages, including faster learning, better
generalization, and enables interpretability.
</p>
<p>In the first part of this paper, we introduce and evaluate different
architectures of modular learning for Dorsal Capsulo-Scapholunate Septum (DCSS)
instability classification. Our experiments have shown that modular learning
improves performances compared to non-modular systems. Moreover, we found that
weighted modular, that is to weight the output using the probabilities from the
gating module, achieved an almost perfect classification. In the second part,
we present our approach for data labeling and segmentation with self-training
applied on shoulder arthroscopy images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08908">What Can Simple Arithmetic Operations Do for Temporal Modeling?. (arXiv:2307.08908v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1">Wenhao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Yuxin Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1">Zhun Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jingdong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Chang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1">Wanli Ouyang</a></p>
<p>Temporal modeling plays a crucial role in understanding video content. To
tackle this problem, previous studies built complicated temporal relations
through time sequence thanks to the development of computationally powerful
devices. In this work, we explore the potential of four simple arithmetic
operations for temporal modeling. Specifically, we first capture auxiliary
temporal cues by computing addition, subtraction, multiplication, and division
between pairs of extracted frame features. Then, we extract corresponding
features from these cues to benefit the original temporal-irrespective domain.
We term such a simple pipeline as an Arithmetic Temporal Module (ATM), which
operates on the stem of a visual backbone with a plug-andplay style. We conduct
comprehensive ablation studies on the instantiation of ATMs and demonstrate
that this module provides powerful temporal modeling capability at a low
computational cost. Moreover, the ATM is compatible with both CNNs- and
ViTs-based architectures. Our results show that ATM achieves superior
performance over several popular video benchmarks. Specifically, on
Something-Something V1, V2 and Kinetics-400, we reach top-1 accuracy of 65.6%,
74.6%, and 89.4% respectively. The code is available at
https://github.com/whwu95/ATM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08913">Towards the Sparseness of Projection Head in Self-Supervised Learning. (arXiv:2307.08913v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1">Zeen Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1">Xingzhe Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jingyao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiang_W/0/1/0/all/0/1">Wenwen Qiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1">Changwen Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1">Fuchun Sun</a></p>
<p>In recent years, self-supervised learning (SSL) has emerged as a promising
approach for extracting valuable representations from unlabeled data. One
successful SSL method is contrastive learning, which aims to bring positive
examples closer while pushing negative examples apart. Many current contrastive
learning approaches utilize a parameterized projection head. Through a
combination of empirical analysis and theoretical investigation, we provide
insights into the internal mechanisms of the projection head and its
relationship with the phenomenon of dimensional collapse. Our findings
demonstrate that the projection head enhances the quality of representations by
performing contrastive loss in a projected subspace. Therefore, we propose an
assumption that only a subset of features is necessary when minimizing the
contrastive loss of a mini-batch of data. Theoretical analysis further suggests
that a sparse projection head can enhance generalization, leading us to
introduce SparseHead - a regularization term that effectively constrains the
sparsity of the projection head, and can be seamlessly integrated with any
self-supervised learning (SSL) approaches. Our experimental results validate
the effectiveness of SparseHead, demonstrating its ability to improve the
performance of existing contrastive methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08919">Accuracy versus time frontiers of semi-supervised and self-supervised learning on medical images. (arXiv:2307.08919v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zhe Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1">Ruijie Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Aeron_S/0/1/0/all/0/1">Shuchin Aeron</a>, <a href="http://arxiv.org/find/cs/1/au:+Hughes_M/0/1/0/all/0/1">Michael C. Hughes</a></p>
<p>For many applications of classifiers to medical images, a trustworthy label
for each image can be difficult or expensive to obtain. In contrast, images
without labels are more readily available. Two major research directions both
promise that additional unlabeled data can improve classifier performance:
self-supervised learning pretrains useful representations on unlabeled data
only, then fine-tunes a classifier on these representations via the labeled
set; semi-supervised learning directly trains a classifier on labeled and
unlabeled data simultaneously. Recent methods from both directions have claimed
significant gains on non-medical tasks, but do not systematically assess
medical images and mostly compare only to methods in the same direction. This
study contributes a carefully-designed benchmark to help answer a
practitioner's key question: given a small labeled dataset and a limited budget
of hours to spend on training, what gains from additional unlabeled images are
possible and which methods best achieve them? Unlike previous benchmarks, ours
uses realistic-sized validation sets to select hyperparameters, assesses
runtime-performance tradeoffs, and bridges two research fields. By comparing 6
semi-supervised methods and 5 self-supervised methods to strong labeled-only
baselines on 3 medical datasets with 30-1000 labels per class, we offer
insights to resource-constrained, results-focused practitioners: MixMatch,
SimCLR, and BYOL represent strong choices that were not surpassed by more
recent methods. After much effort selecting hyperparameters on one dataset, we
publish settings that enable strong methods to perform well on new medical
tasks within a few hours, with further search over dozens of hours delivering
modest additional gains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08924">Learning to Sample Tasks for Meta Learning. (arXiv:2307.08924v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jingyao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1">Zeen Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1">Xingzhe Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1">Lingyu Si</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1">Hongwei Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiang_W/0/1/0/all/0/1">Wenwen Qiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1">Changwen Zheng</a></p>
<p>Through experiments on various meta-learning methods, task samplers, and
few-shot learning tasks, this paper arrives at three conclusions. Firstly,
there are no universal task sampling strategies to guarantee the performance of
meta-learning models. Secondly, task diversity can cause the models to either
underfit or overfit during training. Lastly, the generalization performance of
the models are influenced by task divergence, task entropy, and task
difficulty. In response to these findings, we propose a novel task sampler
called Adaptive Sampler (ASr). ASr is a plug-and-play task sampler that takes
task divergence, task entropy, and task difficulty to sample tasks. To optimize
ASr, we rethink and propose a simple and general meta-learning algorithm.
Finally, a large number of empirical experiments demonstrate the effectiveness
of the proposed ASr.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08930">Unsupervised Deep Graph Matching Based on Cycle Consistency. (arXiv:2307.08930v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tourani_S/0/1/0/all/0/1">Siddharth Tourani</a>, <a href="http://arxiv.org/find/cs/1/au:+Rother_C/0/1/0/all/0/1">Carsten Rother</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1">Muhammad Haris Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Savchynskkyy_B/0/1/0/all/0/1">Bogdan Savchynskkyy</a></p>
<p>We contribute to the sparsely populated area of unsupervised deep graph
matching with application to keypoint matching in images. Contrary to the
standard \emph{supervised} approach, our method does not require ground truth
correspondences between keypoint pairs. Instead, it is self-supervised by
enforcing consistency of matchings between images of the same object category.
As the matching and the consistency loss are discrete, their derivatives cannot
be straightforwardly used for learning. We address this issue in a principled
way by building our method upon the recent results on black-box differentiation
of combinatorial solvers. This makes our method exceptionally flexible, as it
is compatible with arbitrary network architectures and combinatorial solvers.
Our experimental evaluation suggests that our technique sets a new
state-of-the-art for unsupervised graph matching.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08939">Experimental Security Analysis of DNN-based Adaptive Cruise Control under Context-Aware Perception Attacks. (arXiv:2307.08939v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xugui Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1">Anqi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Kouzel_M/0/1/0/all/0/1">Maxfield Kouzel</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1">Haotian Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+McCarty_M/0/1/0/all/0/1">Morgan McCarty</a>, <a href="http://arxiv.org/find/cs/1/au:+Nita_Rotaru_C/0/1/0/all/0/1">Cristina Nita-Rotaru</a>, <a href="http://arxiv.org/find/cs/1/au:+Alemzadeh_H/0/1/0/all/0/1">Homa Alemzadeh</a></p>
<p>Adaptive Cruise Control (ACC) is a widely used driver assistance feature for
maintaining desired speed and safe distance to the leading vehicles. This paper
evaluates the security of the deep neural network (DNN) based ACC systems under
stealthy perception attacks that strategically inject perturbations into camera
data to cause forward collisions. We present a combined
knowledge-and-data-driven approach to design a context-aware strategy for the
selection of the most critical times for triggering the attacks and a novel
optimization-based method for the adaptive generation of image perturbations at
run-time. We evaluate the effectiveness of the proposed attack using an actual
driving dataset and a realistic simulation platform with the control software
from a production ACC system and a physical-world driving simulator while
considering interventions by the driver and safety features such as Automatic
Emergency Braking (AEB) and Forward Collision Warning (FCW). Experimental
results show that the proposed attack achieves 142.9x higher success rate in
causing accidents than random attacks and is mitigated 89.6% less by the safety
features while being stealthy and robust to real-world factors and dynamic
changes in the environment. This study provides insights into the role of human
operators and basic safety interventions in preventing attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08950">Deep Physics-Guided Unrolling Generalization for Compressed Sensing. (arXiv:2307.08950v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Bin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1">Jiechong Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1">Jingfen Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jian Zhang</a></p>
<p>By absorbing the merits of both the model- and data-driven methods, deep
physics-engaged learning scheme achieves high-accuracy and interpretable image
reconstruction. It has attracted growing attention and become the mainstream
for inverse imaging tasks. Focusing on the image compressed sensing (CS)
problem, we find the intrinsic defect of this emerging paradigm, widely
implemented by deep algorithm-unrolled networks, in which more plain iterations
involving real physics will bring enormous computation cost and long inference
time, hindering their practical application. A novel deep
$\textbf{P}$hysics-guided un$\textbf{R}$olled recovery $\textbf{L}$earning
($\textbf{PRL}$) framework is proposed by generalizing the traditional
iterative recovery model from image domain (ID) to the high-dimensional feature
domain (FD). A compact multiscale unrolling architecture is then developed to
enhance the network capacity and keep real-time inference speeds. Taking two
different perspectives of optimization and range-nullspace decomposition,
instead of building an algorithm-specific unrolled network, we provide two
implementations: $\textbf{PRL-PGD}$ and $\textbf{PRL-RND}$. Experiments exhibit
the significant performance and efficiency leading of PRL networks over other
state-of-the-art methods with a large potential for further improvement and
real application to other inverse imaging problems or optimization models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08978">Learned Scalable Video Coding For Humans and Machines. (arXiv:2307.08978v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Hadizadeh_H/0/1/0/all/0/1">Hadi Hadizadeh</a>, <a href="http://arxiv.org/find/eess/1/au:+Bajic_I/0/1/0/all/0/1">Ivan V. Baji&#x107;</a></p>
<p>Video coding has traditionally been developed to support services such as
video streaming, videoconferencing, digital TV, and so on. The main intent was
to enable human viewing of the encoded content. However, with the advances in
deep neural networks (DNNs), encoded video is increasingly being used for
automatic video analytics performed by machines. In applications such as
automatic traffic monitoring, analytics such as vehicle detection, tracking and
counting, would run continuously, while human viewing could be required
occasionally to review potential incidents. To support such applications, a new
paradigm for video coding is needed that will facilitate efficient
representation and compression of video for both machine and human use in a
scalable manner. In this manuscript, we introduce the first end-to-end
learnable video codec that supports a machine vision task in its base layer,
while its enhancement layer supports input reconstruction for human viewing.
The proposed system is constructed based on the concept of conditional coding
to achieve better compression gains. Comprehensive experimental evaluations
conducted on four standard video datasets demonstrate that our framework
outperforms both state-of-the-art learned and conventional video codecs in its
base layer, while maintaining comparable performance on the human vision task
in its enhancement layer. We will provide the implementation of the proposed
system at www.github.com upon completion of the review process.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08984">In Defense of Clip-based Video Relation Detection. (arXiv:2307.08984v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1">Meng Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Long Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1">Wei Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1">Xiaoyu Yue</a>, <a href="http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1">Roger Zimmermann</a></p>
<p>Video Visual Relation Detection (VidVRD) aims to detect visual relationship
triplets in videos using spatial bounding boxes and temporal boundaries.
Existing VidVRD methods can be broadly categorized into bottom-up and top-down
paradigms, depending on their approach to classifying relations. Bottom-up
methods follow a clip-based approach where they classify relations of short
clip tubelet pairs and then merge them into long video relations. On the other
hand, top-down methods directly classify long video tubelet pairs. While recent
video-based methods utilizing video tubelets have shown promising results, we
argue that the effective modeling of spatial and temporal context plays a more
significant role than the choice between clip tubelets and video tubelets. This
motivates us to revisit the clip-based paradigm and explore the key success
factors in VidVRD. In this paper, we propose a Hierarchical Context Model (HCM)
that enriches the object-based spatial context and relation-based temporal
context based on clips. We demonstrate that using clip tubelets can achieve
superior performance compared to most video-based methods. Additionally, using
clip tubelets offers more flexibility in model designs and helps alleviate the
limitations associated with video tubelets, such as the challenging long-term
object tracking problem and the loss of temporal information in long-term
tubelet feature compression. Extensive experiments conducted on two challenging
VidVRD benchmarks validate that our HCM achieves a new state-of-the-art
performance, highlighting the effectiveness of incorporating advanced spatial
and temporal context modeling within the clip-based paradigm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08988">EVIL: Evidential Inference Learning for Trustworthy Semi-supervised Medical Image Segmentation. (arXiv:2307.08988v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yingyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Ziyuan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1">Chenyu Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhiwen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1">Yang Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yi Zhang</a></p>
<p>Recently, uncertainty-aware methods have attracted increasing attention in
semi-supervised medical image segmentation. However, current methods usually
suffer from the drawback that it is difficult to balance the computational
cost, estimation accuracy, and theoretical support in a unified framework. To
alleviate this problem, we introduce the Dempster-Shafer Theory of Evidence
(DST) into semi-supervised medical image segmentation, dubbed Evidential
Inference Learning (EVIL). EVIL provides a theoretically guaranteed solution to
infer accurate uncertainty quantification in a single forward pass. Trustworthy
pseudo labels on unlabeled data are generated after uncertainty estimation. The
recently proposed consistency regularization-based training paradigm is adopted
in our framework, which enforces the consistency on the perturbed predictions
to enhance the generalization with few labeled data. Experimental results show
that EVIL achieves competitive performance in comparison with several
state-of-the-art methods on the public dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08991">EgoVM: Achieving Precise Ego-Localization using Lightweight Vectorized Maps. (arXiv:2307.08991v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yuzhe He</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1">Shuang Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Rui_X/0/1/0/all/0/1">Xiaofei Rui</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1">Chengying Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_G/0/1/0/all/0/1">Guowei Wan</a></p>
<p>Accurate and reliable ego-localization is critical for autonomous driving. In
this paper, we present EgoVM, an end-to-end localization network that achieves
comparable localization accuracy to prior state-of-the-art methods, but uses
lightweight vectorized maps instead of heavy point-based maps. To begin with,
we extract BEV features from online multi-view images and LiDAR point cloud.
Then, we employ a set of learnable semantic embeddings to encode the semantic
types of map elements and supervise them with semantic segmentation, to make
their feature representation consistent with BEV features. After that, we feed
map queries, composed of learnable semantic embeddings and coordinates of map
elements, into a transformer decoder to perform cross-modality matching with
BEV features. Finally, we adopt a robust histogram-based pose solver to
estimate the optimal pose by searching exhaustively over candidate poses. We
comprehensively validate the effectiveness of our method using both the
nuScenes dataset and a newly collected dataset. The experimental results show
that our method achieves centimeter-level localization accuracy, and
outperforms existing methods using vectorized maps by a large margin.
Furthermore, our model has been extensively tested in a large fleet of
autonomous vehicles under various challenging urban scenes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08992">Arbitrary point cloud upsampling via Dual Back-Projection Network. (arXiv:2307.08992v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhi-Song Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zijia Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1">Zhen Jia</a></p>
<p>Point clouds acquired from 3D sensors are usually sparse and noisy. Point
cloud upsampling is an approach to increase the density of the point cloud so
that detailed geometric information can be restored. In this paper, we propose
a Dual Back-Projection network for point cloud upsampling (DBPnet). A Dual
Back-Projection is formulated in an up-down-up manner for point cloud
upsampling. It not only back projects feature residues but also coordinates
residues so that the network better captures the point correlations in the
feature and space domains, achieving lower reconstruction errors on both
uniform and non-uniform sparse point clouds. Our proposed method is also
generalizable for arbitrary upsampling tasks (e.g. 4x, 5.5x). Experimental
results show that the proposed method achieves the lowest point set matching
losses with respect to the benchmark. In addition, the success of our approach
demonstrates that generative networks are not necessarily needed for
non-uniform point clouds.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08994">Human Action Recognition in Still Images Using ConViT. (arXiv:2307.08994v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hosseyni_S/0/1/0/all/0/1">Seyed Rohollah Hosseyni</a>, <a href="http://arxiv.org/find/cs/1/au:+Taheri_H/0/1/0/all/0/1">Hasan Taheri</a>, <a href="http://arxiv.org/find/cs/1/au:+Seyedin_S/0/1/0/all/0/1">Sanaz Seyedin</a>, <a href="http://arxiv.org/find/cs/1/au:+Rahmani_A/0/1/0/all/0/1">Ali Ahmad Rahmani</a></p>
<p>Understanding the relationship between different parts of the image plays a
crucial role in many visual recognition tasks. Despite the fact that
Convolutional Neural Networks (CNNs) have demonstrated impressive results in
detecting single objects, they lack the capability to extract the relationship
between various regions of an image, which is a crucial factor in human action
recognition. To address this problem, this paper proposes a new module that
functions like a convolutional layer using Vision Transformer (ViT). The
proposed action recognition model comprises two components: the first part is a
deep convolutional network that extracts high-level spatial features from the
image, and the second component of the model utilizes a Vision Transformer that
extracts the relationship between various regions of the image using the
feature map generated by the CNN output. The proposed model has been evaluated
on the Stanford40 and PASCAL VOC 2012 action datasets and has achieved 95.5%
mAP and 91.5% mAP results, respectively, which are promising compared to other
state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08995">Revisiting Latent Space of GAN Inversion for Real Image Editing. (arXiv:2307.08995v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Katsumata_K/0/1/0/all/0/1">Kai Katsumata</a>, <a href="http://arxiv.org/find/cs/1/au:+Vo_D/0/1/0/all/0/1">Duc Minh Vo</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Nakayama_H/0/1/0/all/0/1">Hideki Nakayama</a></p>
<p>The exploration of the latent space in StyleGANs and GAN inversion exemplify
impressive real-world image editing, yet the trade-off between reconstruction
quality and editing quality remains an open problem. In this study, we revisit
StyleGANs' hyperspherical prior $\mathcal{Z}$ and combine it with highly
capable latent spaces to build combined spaces that faithfully invert real
images while maintaining the quality of edited images. More specifically, we
propose $\mathcal{F}/\mathcal{Z}^{+}$ space consisting of two subspaces:
$\mathcal{F}$ space of an intermediate feature map of StyleGANs enabling
faithful reconstruction and $\mathcal{Z}^{+}$ space of an extended StyleGAN
prior supporting high editing quality. We project the real images into the
proposed space to obtain the inverted codes, by which we then move along
$\mathcal{Z}^{+}$, enabling semantic editing without sacrificing image quality.
Comprehensive experiments show that $\mathcal{Z}^{+}$ can replace the most
commonly-used $\mathcal{W}$, $\mathcal{W}^{+}$, and $\mathcal{S}$ spaces while
preserving reconstruction quality, resulting in reduced distortion of edited
images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08996">Towards Authentic Face Restoration with Iterative Diffusion Models and Beyond. (arXiv:2307.08996v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_T/0/1/0/all/0/1">Tingbo Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1">Yu-Chuan Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xuhui Jia. Yandong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Grundmann_M/0/1/0/all/0/1">Matthias Grundmann</a></p>
<p>An authentic face restoration system is becoming increasingly demanding in
many computer vision applications, e.g., image enhancement, video
communication, and taking portrait. Most of the advanced face restoration
models can recover high-quality faces from low-quality ones but usually fail to
faithfully generate realistic and high-frequency details that are favored by
users. To achieve authentic restoration, we propose $\textbf{IDM}$, an
$\textbf{I}$teratively learned face restoration system based on denoising
$\textbf{D}$iffusion $\textbf{M}$odels (DDMs). We define the criterion of an
authentic face restoration system, and argue that denoising diffusion models
are naturally endowed with this property from two aspects: intrinsic iterative
refinement and extrinsic iterative enhancement. Intrinsic learning can preserve
the content well and gradually refine the high-quality details, while extrinsic
enhancement helps clean the data and improve the restoration task one step
further. We demonstrate superior performance on blind face restoration tasks.
Beyond restoration, we find the authentically cleaned data by the proposed
restoration system is also helpful to image generation tasks in terms of
training stabilization and sample quality. Without modifying the models, we
achieve better quality than state-of-the-art on FFHQ and ImageNet generation
using either GANs or diffusion models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09000">TractCloud: Registration-free tractography parcellation with a novel local-global streamline point cloud representation. (arXiv:2307.09000v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xue_T/0/1/0/all/0/1">Tengfei Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuqian Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chaoyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Golby_A/0/1/0/all/0/1">Alexandra J. Golby</a>, <a href="http://arxiv.org/find/cs/1/au:+Makris_N/0/1/0/all/0/1">Nikos Makris</a>, <a href="http://arxiv.org/find/cs/1/au:+Rathi_Y/0/1/0/all/0/1">Yogesh Rathi</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1">Weidong Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1">Fan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+ODonnell_L/0/1/0/all/0/1">Lauren J. O&#x27;Donnell</a></p>
<p>Diffusion MRI tractography parcellation classifies streamlines into
anatomical fiber tracts to enable quantification and visualization for clinical
and scientific applications. Current tractography parcellation methods rely
heavily on registration, but registration inaccuracies can affect parcellation
and the computational cost of registration is high for large-scale datasets.
Recently, deep-learning-based methods have been proposed for tractography
parcellation using various types of representations for streamlines. However,
these methods only focus on the information from a single streamline, ignoring
geometric relationships between the streamlines in the brain. We propose
TractCloud, a registration-free framework that performs whole-brain
tractography parcellation directly in individual subject space. We propose a
novel, learnable, local-global streamline representation that leverages
information from neighboring and whole-brain streamlines to describe the local
anatomy and global pose of the brain. We train our framework on a large-scale
labeled tractography dataset, which we augment by applying synthetic transforms
including rotation, scaling, and translations. We test our framework on five
independently acquired datasets across populations and health conditions.
TractCloud significantly outperforms several state-of-the-art methods on all
testing datasets. TractCloud achieves efficient and consistent whole-brain
white matter parcellation across the lifespan (from neonates to elderly
subjects, including brain tumor patients) without the need for registration.
The robustness and high inference speed of TractCloud make it suitable for
large-scale tractography data analysis. Our project page is available at
https://tractcloud.github.io/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09004">Ord2Seq: Regard Ordinal Regression as Label Sequence Prediction. (arXiv:2307.09004v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jinhong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1">Yi Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jintai Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tingting Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Danny Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jian Wu</a></p>
<p>Ordinal regression refers to classifying object instances into ordinal
categories. It has been widely studied in many scenarios, such as medical
disease grading, movie rating, etc. Known methods focused only on learning
inter-class ordinal relationships, but still incur limitations in
distinguishing adjacent categories thus far. In this paper, we propose a simple
sequence prediction framework for ordinal regression called Ord2Seq, which, for
the first time, transforms each ordinal category label into a special label
sequence and thus regards an ordinal regression task as a sequence prediction
process. In this way, we decompose an ordinal regression task into a series of
recursive binary classification steps, so as to subtly distinguish adjacent
categories. Comprehensive experiments show the effectiveness of distinguishing
adjacent categories for performance improvement and our new approach exceeds
state-of-the-art performances in four different scenarios. Codes will be
available upon acceptance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09005">Frequency-mixed Single-source Domain Generalization for Medical Image Segmentation. (arXiv:2307.09005v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1">Heng Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1">Haojin Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhao_W/0/1/0/all/0/1">Wei Zhao</a>, <a href="http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1">Huazhu Fu</a>, <a href="http://arxiv.org/find/eess/1/au:+Su_X/0/1/0/all/0/1">Xiuyun Su</a>, <a href="http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1">Yan Hu</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1">Jiang Liu</a></p>
<p>The annotation scarcity of medical image segmentation poses challenges in
collecting sufficient training data for deep learning models. Specifically,
models trained on limited data may not generalize well to other unseen data
domains, resulting in a domain shift issue. Consequently, domain generalization
(DG) is developed to boost the performance of segmentation models on unseen
domains. However, the DG setup requires multiple source domains, which impedes
the efficient deployment of segmentation algorithms in clinical scenarios. To
address this challenge and improve the segmentation model's generalizability,
we propose a novel approach called the Frequency-mixed Single-source Domain
Generalization method (FreeSDG). By analyzing the frequency's effect on domain
discrepancy, FreeSDG leverages a mixed frequency spectrum to augment the
single-source domain. Additionally, self-supervision is constructed in the
domain augmentation to learn robust context-aware representations for the
segmentation task. Experimental results on five datasets of three modalities
demonstrate the effectiveness of the proposed algorithm. FreeSDG outperforms
state-of-the-art methods and significantly improves the segmentation model's
generalizability. Therefore, FreeSDG provides a promising solution for
enhancing the generalization of medical image segmentation models, especially
when annotated data is scarce. The code is available at
https://github.com/liamheng/Non-IID_Medical_Image_Segmentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09008">Soft-IntroVAE for Continuous Latent space Image Super-Resolution. (arXiv:2307.09008v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1">Zhi-Song Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1">Zijia Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Jia_Z/0/1/0/all/0/1">Zhen Jia</a></p>
<p>Continuous image super-resolution (SR) recently receives a lot of attention
from researchers, for its practical and flexible image scaling for various
displays. Local implicit image representation is one of the methods that can
map the coordinates and 2D features for latent space interpolation. Inspired by
Variational AutoEncoder, we propose a Soft-introVAE for continuous latent space
image super-resolution (SVAE-SR). A novel latent space adversarial training is
achieved for photo-realistic image restoration. To further improve the quality,
a positional encoding scheme is used to extend the original pixel coordinates
by aggregating frequency information over the pixel areas. We show the
effectiveness of the proposed SVAE-SR through quantitative and qualitative
comparisons, and further, illustrate its generalization in denoising and
real-image super-resolution.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09019">U-shaped Transformer: Retain High Frequency Context in Time Series Analysis. (arXiv:2307.09019v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Qingkui Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yiqin Zhang</a></p>
<p>Time series prediction plays a crucial role in various industrial fields. In
recent years, neural networks with a transformer backbone have achieved
remarkable success in many domains, including computer vision and NLP. In time
series analysis domain, some studies have suggested that even the simplest MLP
networks outperform advanced transformer-based networks on time series forecast
tasks. However, we believe these findings indicate there to be low-rank
properties in time series sequences. In this paper, we consider the low-pass
characteristics of transformers and try to incorporate the advantages of MLP.
We adopt skip-layer connections inspired by Unet into traditional transformer
backbone, thus preserving high-frequency context from input to output, namely
U-shaped Transformer. We introduce patch merge and split operation to extract
features with different scales and use larger datasets to fully make use of the
transformer backbone. Our experiments demonstrate that the model performs at an
advanced level across multiple datasets with relatively low cost.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09020">Face-PAST: Facial Pose Awareness and Style Transfer Networks. (arXiv:2307.09020v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khowaja_S/0/1/0/all/0/1">Sunder Ali Khowaja</a>, <a href="http://arxiv.org/find/cs/1/au:+Mujtaba_G/0/1/0/all/0/1">Ghulam Mujtaba</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1">Jiseok Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1">Ik Hyun Lee</a></p>
<p>Facial style transfer has been quite popular among researchers due to the
rise of emerging technologies such as eXtended Reality (XR), Metaverse, and
Non-Fungible Tokens (NFTs). Furthermore, StyleGAN methods along with
transfer-learning strategies have reduced the problem of limited data to some
extent. However, most of the StyleGAN methods overfit the styles while adding
artifacts to facial images. In this paper, we propose a facial pose awareness
and style transfer (Face-PAST) network that preserves facial details and
structures while generating high-quality stylized images. Dual StyleGAN
inspires our work, but in contrast, our work uses a pre-trained style
generation network in an external style pass with a residual modulation block
instead of a transform coding block. Furthermore, we use the gated mapping unit
and facial structure, identity, and segmentation losses to preserve the facial
structure and details. This enables us to train the network with a very limited
amount of data while generating high-quality stylized images. Our training
process adapts curriculum learning strategy to perform efficient and flexible
style mixing in the generative space. We perform extensive experiments to show
the superiority of Face-PAST in comparison to existing state-of-the-art
methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09023">LA-Net: Landmark-Aware Learning for Reliable Facial Expression Recognition under Label Noise. (arXiv:2307.09023v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhiyu Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1">Jinshi Cui</a></p>
<p>Facial expression recognition (FER) remains a challenging task due to the
ambiguity of expressions. The derived noisy labels significantly harm the
performance in real-world scenarios. To address this issue, we present a new
FER model named Landmark-Aware Net~(LA-Net), which leverages facial landmarks
to mitigate the impact of label noise from two perspectives. Firstly, LA-Net
uses landmark information to suppress the uncertainty in expression space and
constructs the label distribution of each sample by neighborhood aggregation,
which in turn improves the quality of training supervision. Secondly, the model
incorporates landmark information into expression representations using the
devised expression-landmark contrastive loss. The enhanced expression feature
extractor can be less susceptible to label noise. Our method can be integrated
with any deep neural network for better training supervision without
introducing extra inference costs. We conduct extensive experiments on both
in-the-wild datasets and synthetic noisy datasets and demonstrate that LA-Net
achieves state-of-the-art performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09026">ActionPrompt: Action-Guided 3D Human Pose Estimation With Text and Pose Prompting. (arXiv:2307.09026v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1">Hongwei Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Han Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1">Bowen Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1">Wenrui Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_B/0/1/0/all/0/1">Botao Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yu Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1">Min Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1">Hongkai Xiong</a></p>
<p>Recent 2D-to-3D human pose estimation (HPE) utilizes temporal consistency
across sequences to alleviate the depth ambiguity problem but ignore the action
related prior knowledge hidden in the pose sequence. In this paper, we propose
a plug-and-play module named Action Prompt Module (APM) that effectively mines
different kinds of action clues for 3D HPE. The highlight is that, the mining
scheme of APM can be widely adapted to different frameworks and bring
consistent benefits. Specifically, we first present a novel Action-related Text
Prompt module (ATP) that directly embeds action labels and transfers the rich
language information in the label to the pose sequence. Besides, we further
introduce Action-specific Pose Prompt module (APP) to mine the position-aware
pose pattern of each action, and exploit the correlation between the mined
patterns and input pose sequence for further pose refinement. Experiments show
that APM can improve the performance of most video-based 2D-to-3D HPE
frameworks by a large margin.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09027">Online Self-Supervised Thermal Water Segmentation for Aerial Vehicles. (arXiv:2307.09027v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1">Connor Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Frennert_J/0/1/0/all/0/1">Jonathan Gustafsson Frennert</a>, <a href="http://arxiv.org/find/cs/1/au:+Gan_L/0/1/0/all/0/1">Lu Gan</a>, <a href="http://arxiv.org/find/cs/1/au:+Anderson_M/0/1/0/all/0/1">Matthew Anderson</a>, <a href="http://arxiv.org/find/cs/1/au:+Chung_S/0/1/0/all/0/1">Soon-Jo Chung</a></p>
<p>We present a new method to adapt an RGB-trained water segmentation network to
target-domain aerial thermal imagery using online self-supervision by
leveraging texture and motion cues as supervisory signals. This new thermal
capability enables current autonomous aerial robots operating in near-shore
environments to perform tasks such as visual navigation, bathymetry, and flow
tracking at night. Our method overcomes the problem of scarce and
difficult-to-obtain near-shore thermal data that prevents the application of
conventional supervised and unsupervised methods. In this work, we curate the
first aerial thermal near-shore dataset, show that our approach outperforms
fully-supervised segmentation models trained on limited target-domain thermal
data, and demonstrate real-time capabilities onboard an Nvidia Jetson embedded
computing platform. Code and datasets used in this work will be available at:
https://github.com/connorlee77/uav-thermal-water-segmentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09039">PottsMGNet: A Mathematical Explanation of Encoder-Decoder Based Neural Networks. (arXiv:2307.09039v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tai_X/0/1/0/all/0/1">Xue-Cheng Tai</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_R/0/1/0/all/0/1">Raymond Chan</a></p>
<p>For problems in image processing and many other fields, a large class of
effective neural networks has encoder-decoder-based architectures. Although
these networks have made impressive performances, mathematical explanations of
their architectures are still underdeveloped. In this paper, we study the
encoder-decoder-based network architecture from the algorithmic perspective and
provide a mathematical explanation. We use the two-phase Potts model for image
segmentation as an example for our explanations. We associate the segmentation
problem with a control problem in the continuous setting. Then, multigrid
method and operator splitting scheme, the PottsMGNet, are used to discretize
the continuous control model. We show that the resulting discrete PottsMGNet is
equivalent to an encoder-decoder-based network. With minor modifications, it is
shown that a number of the popular encoder-decoder-based neural networks are
just instances of the proposed PottsMGNet. By incorporating the
Soft-Threshold-Dynamics into the PottsMGNet as a regularizer, the PottsMGNet
has shown to be robust with the network parameters such as network width and
depth and achieved remarkable performance on datasets with very large noise. In
nearly all our experiments, the new network always performs better or as good
on accuracy and dice score than existing networks for image segmentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09050">R-Cut: Enhancing Explainability in Vision Transformers with Relationship Weighted Out and Cut. (arXiv:2307.09050v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1">Yingjie Niu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1">Ming Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_M/0/1/0/all/0/1">Maoning Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Karlsson_R/0/1/0/all/0/1">Robin Karlsson</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuxiao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Takeda_K/0/1/0/all/0/1">Kazuya Takeda</a></p>
<p>Transformer-based models have gained popularity in the field of natural
language processing (NLP) and are extensively utilized in computer vision tasks
and multi-modal models such as GPT4. This paper presents a novel method to
enhance the explainability of Transformer-based image classification models.
Our method aims to improve trust in classification results and empower users to
gain a deeper understanding of the model for downstream tasks by providing
visualizations of class-specific maps. We introduce two modules: the
``Relationship Weighted Out" and the ``Cut" modules. The ``Relationship
Weighted Out" module focuses on extracting class-specific information from
intermediate layers, enabling us to highlight relevant features. Additionally,
the ``Cut" module performs fine-grained feature decomposition, taking into
account factors such as position, texture, and color. By integrating these
modules, we generate dense class-specific visual explainability maps. We
validate our method with extensive qualitative and quantitative experiments on
the ImageNet dataset. Furthermore, we conduct a large number of experiments on
the LRN dataset, specifically designed for automatic driving danger alerts, to
evaluate the explainability of our method in complex backgrounds. The results
demonstrate a significant improvement over previous methods. Moreover, we
conduct ablation experiments to validate the effectiveness of each module.
Through these experiments, we are able to confirm the respective contributions
of each module, thus solidifying the overall effectiveness of our proposed
approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09052">Connections between Operator-splitting Methods and Deep Neural Networks with Applications in Image Segmentation. (arXiv:2307.09052v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tai_X/0/1/0/all/0/1">Xue-Cheng Tai</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_R/0/1/0/all/0/1">Raymond Chan</a></p>
<p>Deep neural network is a powerful tool for many tasks. Understanding why it
is so successful and providing a mathematical explanation is an important
problem and has been one popular research direction in past years. In the
literature of mathematical analysis of deep deep neural networks, a lot of
works are dedicated to establishing representation theories. How to make
connections between deep neural networks and mathematical algorithms is still
under development. In this paper, we give an algorithmic explanation for deep
neural networks, especially in their connection with operator splitting and
multigrid methods. We show that with certain splitting strategies,
operator-splitting methods have the same structure as networks. Utilizing this
connection and the Potts model for image segmentation, two networks inspired by
operator-splitting methods are proposed. The two networks are essentially two
operator-splitting algorithms solving the Potts model. Numerical experiments
are presented to demonstrate the effectiveness of the proposed networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09055">Outlier-Robust Tensor Low-Rank Representation for Data Clustering. (arXiv:2307.09055v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Wu_T/0/1/0/all/0/1">Tong Wu</a></p>
<p>Low-rank tensor analysis has received widespread attention with many
practical applications. However, the tensor data are often contaminated by
outliers or sample-specific corruptions. How to recover the tensor data that
are corrupted by outliers and perform data clustering remains a challenging
problem. This paper develops an outlier-robust tensor low-rank representation
(OR-TLRR) method for simultaneous outlier detection and tensor data clustering
based on the tensor singular value decomposition (t-SVD) algebraic framework.
It is motivated by the recently proposed tensor-tensor product induced by
invertible linear transforms that satisfy certain conditions. For tensor
observations with arbitrary outlier corruptions, OR-TLRR has provable
performance guarantee for exactly recovering the row space of clean data and
detecting outliers under mild conditions. Moreover, an extension of OR-TLRR is
also proposed to handle the case when parts of the data are missing. Finally,
extensive experimental results on both synthetic and real data demonstrate the
effectiveness of the proposed algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09059">Unleashing the Imagination of Text: A Novel Framework for Text-to-image Person Retrieval via Exploring the Power of Words. (arXiv:2307.09059v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1">Delong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Haiwen Li</a></p>
<p>The goal of Text-to-image person retrieval is to retrieve person images from
a large gallery that match the given textual descriptions. The main challenge
of this task lies in the significant differences in information representation
between the visual and textual modalities. The textual modality conveys
abstract and precise information through vocabulary and grammatical structures,
while the visual modality conveys concrete and intuitive information through
images. To fully leverage the expressive power of textual representations, it
is essential to accurately map abstract textual descriptions to specific
images.
</p>
<p>To address this issue, we propose a novel framework to Unleash the
Imagination of Text (UIT) in text-to-image person retrieval, aiming to fully
explore the power of words in sentences. Specifically, the framework employs
the pre-trained full CLIP model as a dual encoder for the images and texts ,
taking advantage of prior cross-modal alignment knowledge. The Text-guided
Image Restoration auxiliary task is proposed with the aim of implicitly mapping
abstract textual entities to specific image regions, facilitating alignment
between textual and visual embeddings. Additionally, we introduce a cross-modal
triplet loss tailored for handling hard samples, enhancing the model's ability
to distinguish minor differences.
</p>
<p>To focus the model on the key components within sentences, we propose a novel
text data augmentation technique. Our proposed methods achieve state-of-the-art
results on three popular benchmark datasets, and the source code will be made
publicly available shortly.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09065">Learning Adaptive Neighborhoods for Graph Neural Networks. (arXiv:2307.09065v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1">Avishkar Saha</a>, <a href="http://arxiv.org/find/cs/1/au:+Mendez_O/0/1/0/all/0/1">Oscar Mendez</a>, <a href="http://arxiv.org/find/cs/1/au:+Russell_C/0/1/0/all/0/1">Chris Russell</a>, <a href="http://arxiv.org/find/cs/1/au:+Bowden_R/0/1/0/all/0/1">Richard Bowden</a></p>
<p>Graph convolutional networks (GCNs) enable end-to-end learning on graph
structured data. However, many works assume a given graph structure. When the
input graph is noisy or unavailable, one approach is to construct or learn a
latent graph structure. These methods typically fix the choice of node degree
for the entire graph, which is suboptimal. Instead, we propose a novel
end-to-end differentiable graph generator which builds graph topologies where
each node selects both its neighborhood and its size. Our module can be readily
integrated into existing pipelines involving graph convolution operations,
replacing the predetermined or existing adjacency matrix with one that is
learned, and optimized, as part of the general objective. As such it is
applicable to any GCN. We integrate our module into trajectory prediction,
point cloud classification and node classification pipelines resulting in
improved accuracy over other structure-learning methods across a wide range of
datasets and GCN backbones.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09066">PatchCT: Aligning Patch Set and Label Set with Conditional Transport for Multi-Label Image Classification. (arXiv:2307.09066v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Miaoge Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dongsheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xinyang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1">Zequn Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1">Ruiying Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Bo Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1">Mingyuan Zhou</a></p>
<p>Multi-label image classification is a prediction task that aims to identify
more than one label from a given image. This paper considers the semantic
consistency of the latent space between the visual patch and linguistic label
domains and introduces the conditional transport (CT) theory to bridge the
acknowledged gap. While recent cross-modal attention-based studies have
attempted to align such two representations and achieved impressive
performance, they required carefully-designed alignment modules and extra
complex operations in the attention computation. We find that by formulating
the multi-label classification as a CT problem, we can exploit the interactions
between the image and label efficiently by minimizing the bidirectional CT
cost. Specifically, after feeding the images and textual labels into the
modality-specific encoders, we view each image as a mixture of patch embeddings
and a mixture of label embeddings, which capture the local region features and
the class prototypes, respectively. CT is then employed to learn and align
those two semantic sets by defining the forward and backward navigators.
Importantly, the defined navigators in CT distance model the similarities
between patches and labels, which provides an interpretable tool to visualize
the learned prototypes. Extensive experiments on three public image benchmarks
show that the proposed model consistently outperforms the previous methods. Our
code is available at https://github.com/keepgoingjkg/PatchCT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09067">Evaluate Fine-tuning Strategies for Fetal Head Ultrasound Image Segmentation with U-Net. (arXiv:2307.09067v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Wang_F/0/1/0/all/0/1">Fangyijie Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Silvestre_G/0/1/0/all/0/1">Gu&#xe9;nol&#xe9; Silvestre</a>, <a href="http://arxiv.org/find/eess/1/au:+Curran_K/0/1/0/all/0/1">Kathleen M. Curran</a></p>
<p>Fetal head segmentation is a crucial step in measuring the fetal head
circumference (HC) during gestation, an important biometric in obstetrics for
monitoring fetal growth. However, manual biometry generation is time-consuming
and results in inconsistent accuracy. To address this issue, convolutional
neural network (CNN) models have been utilized to improve the efficiency of
medical biometry. But training a CNN network from scratch is a challenging
task, we proposed a Transfer Learning (TL) method. Our approach involves
fine-tuning (FT) a U-Net network with a lightweight MobileNet as the encoder to
perform segmentation on a set of fetal head ultrasound (US) images with limited
effort. This method addresses the challenges associated with training a CNN
network from scratch. It suggests that our proposed FT strategy yields
segmentation performance that is comparable when trained with a reduced number
of parameters by 85.8%. And our proposed FT strategy outperforms other
strategies with smaller trainable parameter sizes below 4.4 million. Thus, we
contend that it can serve as a dependable FT approach for reducing the size of
models in medical image analysis. Our key findings highlight the importance of
the balance between model performance and size in developing Artificial
Intelligence (AI) applications by TL methods. Code is available at
https://github.<a href="/abs/com/1320494">com/1320494</a>2/FT_Methods_for_Fetal_Head_Segmentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09070">PixelHuman: Animatable Neural Radiance Fields from Few Images. (arXiv:2307.09070v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shim_G/0/1/0/all/0/1">Gyumin Shim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jaeseong Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Hyung_J/0/1/0/all/0/1">Junha Hyung</a>, <a href="http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1">Jaegul Choo</a></p>
<p>In this paper, we propose PixelHuman, a novel human rendering model that
generates animatable human scenes from a few images of a person with unseen
identity, views, and poses. Previous work have demonstrated reasonable
performance in novel view and pose synthesis, but they rely on a large number
of images to train and are trained per scene from videos, which requires
significant amount of time to produce animatable scenes from unseen human
images. Our method differs from existing methods in that it can generalize to
any input image for animatable human synthesis. Given a random pose sequence,
our method synthesizes each target scene using a neural radiance field that is
conditioned on a canonical representation and pose-aware pixel-aligned
features, both of which can be obtained through deformation fields learned in a
data-driven manner. Our experiments show that our method achieves
state-of-the-art performance in multiview and novel pose synthesis from
few-shot images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09099">A Survey on Multi-Objective Neural Architecture Search. (arXiv:2307.09099v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shariatzadeh_S/0/1/0/all/0/1">Seyed Mahdi Shariatzadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Fathy_M/0/1/0/all/0/1">Mahmood Fathy</a>, <a href="http://arxiv.org/find/cs/1/au:+Berangi_R/0/1/0/all/0/1">Reza Berangi</a>, <a href="http://arxiv.org/find/cs/1/au:+Shahverdy_M/0/1/0/all/0/1">Mohammad Shahverdy</a></p>
<p>Recently, the expert-crafted neural architectures is increasing overtaken by
the utilization of neural architecture search (NAS) and automatic generation
(and tuning) of network structures which has a close relation to the
Hyperparameter Optimization and Auto Machine Learning (AutoML). After the
earlier NAS attempts to optimize only the prediction accuracy, Multi-Objective
Neural architecture Search (MONAS) has been attracting attentions which
considers more goals such as computational complexity, power consumption, and
size of the network for optimization, reaching a trade-off between the accuracy
and other features like the computational cost. In this paper, we present an
overview of principal and state-of-the-art works in the field of MONAS.
Starting from a well-categorized taxonomy and formulation for the NAS, we
address and correct some miscategorizations in previous surveys of the NAS
field. We also provide a list of all known objectives used and add a number of
new ones and elaborate their specifications. We have provides analyses about
the most important objectives and shown that the stochastic properties of some
the them should be differed from deterministic ones in the multi-objective
optimization procedure of NAS. We finalize this paper with a number of future
directions and topics in the field of MONAS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09104">Division Gets Better: Learning Brightness-Aware and Detail-Sensitive Representations for Low-Light Image Enhancement. (arXiv:2307.09104v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Huake Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1">Xiaoyang Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1">Xingsong Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Junhui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Dun_Y/0/1/0/all/0/1">Yujie Dun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kaibing Zhang</a></p>
<p>Low-light image enhancement strives to improve the contrast, adjust the
visibility, and restore the distortion in color and texture. Existing methods
usually pay more attention to improving the visibility and contrast via
increasing the lightness of low-light images, while disregarding the
significance of color and texture restoration for high-quality images. Against
above issue, we propose a novel luminance and chrominance dual branch network,
termed LCDBNet, for low-light image enhancement, which divides low-light image
enhancement into two sub-tasks, e.g., luminance adjustment and chrominance
restoration. Specifically, LCDBNet is composed of two branches, namely
luminance adjustment network (LAN) and chrominance restoration network (CRN).
LAN takes responsibility for learning brightness-aware features leveraging
long-range dependency and local attention correlation. While CRN concentrates
on learning detail-sensitive features via multi-level wavelet decomposition.
Finally, a fusion network is designed to blend their learned features to
produce visually impressive images. Extensive experiments conducted on seven
benchmark datasets validate the effectiveness of our proposed LCDBNet, and the
results manifest that LCDBNet achieves superior performance in terms of
multiple reference/non-reference quality evaluators compared to other
state-of-the-art competitors. Our code and pretrained model will be available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09109">Mining of Single-Class by Active Learning for Semantic Segmentation. (arXiv:2307.09109v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lambert_H/0/1/0/all/0/1">Hugues Lambert</a>, <a href="http://arxiv.org/find/cs/1/au:+Slade_E/0/1/0/all/0/1">Emma Slade</a></p>
<p>Several Active Learning (AL) policies require retraining a target model
several times in order to identify the most informative samples and rarely
offer the option to focus on the acquisition of samples from underrepresented
classes. Here the Mining of Single-Class by Active Learning (MiSiCAL) paradigm
is introduced where an AL policy is constructed through deep reinforcement
learning and exploits quantity-accuracy correlations to build datasets on which
high-performance models can be trained with regards to specific classes.
MiSiCAL is especially helpful in the case of very large batch sizes since it
does not require repeated model training sessions as is common in other AL
methods. This is thanks to its ability to exploit fixed representations of the
candidate data points. We find that MiSiCAL is able to outperform a random
policy on 150 out of 171 COCO10k classes, while the strongest baseline only
outperforms random on 101 classes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09112">NU-MCC: Multiview Compressive Coding with Neighborhood Decoder and Repulsive UDF. (arXiv:2307.09112v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lionar_S/0/1/0/all/0/1">Stefan Lionar</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiangyu Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1">Min Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1">Gim Hee Lee</a></p>
<p>Remarkable progress has been made in 3D reconstruction from single-view RGB-D
inputs. MCC is the current state-of-the-art method in this field, which
achieves unprecedented success by combining vision Transformers with
large-scale training. However, we identified two key limitations of MCC: 1) The
Transformer decoder is inefficient in handling large number of query points; 2)
The 3D representation struggles to recover high-fidelity details. In this
paper, we propose a new approach called NU-MCC that addresses these
limitations. NU-MCC includes two key innovations: a Neighborhood decoder and a
Repulsive Unsigned Distance Function (Repulsive UDF). First, our Neighborhood
decoder introduces center points as an efficient proxy of input visual
features, allowing each query point to only attend to a small neighborhood.
This design not only results in much faster inference speed but also enables
the exploitation of finer-scale visual features for improved recovery of 3D
textures. Second, our Repulsive UDF is a novel alternative to the occupancy
field used in MCC, significantly improving the quality of 3D object
reconstruction. Compared to standard UDFs that suffer from holes in results,
our proposed Repulsive UDF can achieve more complete surface reconstruction.
Experimental results demonstrate that NU-MCC is able to learn a strong 3D
representation, significantly advancing the state of the art in single-view 3D
reconstruction. Particularly, it outperforms MCC by 9.7% in terms of the
F1-score on the CO3D-v2 dataset with more than 5x faster running speed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09120">Light-Weight Vision Transformer with Parallel Local and Global Self-Attention. (arXiv:2307.09120v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ebert_N/0/1/0/all/0/1">Nikolas Ebert</a>, <a href="http://arxiv.org/find/cs/1/au:+Reichardt_L/0/1/0/all/0/1">Laurenz Reichardt</a>, <a href="http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1">Didier Stricker</a>, <a href="http://arxiv.org/find/cs/1/au:+Wasenmuller_O/0/1/0/all/0/1">Oliver Wasenm&#xfc;ller</a></p>
<p>While transformer architectures have dominated computer vision in recent
years, these models cannot easily be deployed on hardware with limited
resources for autonomous driving tasks that require real-time-performance.
Their computational complexity and memory requirements limits their use,
especially for applications with high-resolution inputs. In our work, we
redesign the powerful state-of-the-art Vision Transformer PLG-ViT to a much
more compact and efficient architecture that is suitable for such tasks. We
identify computationally expensive blocks in the original PLG-ViT architecture
and propose several redesigns aimed at reducing the number of parameters and
floating-point operations. As a result of our redesign, we are able to reduce
PLG-ViT in size by a factor of 5, with a moderate drop in performance. We
propose two variants, optimized for the best trade-off between parameter count
to runtime as well as parameter count to accuracy. With only 5 million
parameters, we achieve 79.5$\%$ top-1 accuracy on the ImageNet-1K
classification benchmark. Our networks demonstrate great performance on general
vision benchmarks like COCO instance segmentation. In addition, we conduct a
series of experiments, demonstrating the potential of our approach in solving
various tasks specifically tailored to the challenges of autonomous driving and
transportation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09136">DropMix: Reducing Class Dependency in Mixed Sample Data Augmentation. (arXiv:2307.09136v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Haeil Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hansang Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Junmo Kim</a></p>
<p>Mixed sample data augmentation (MSDA) is a widely used technique that has
been found to improve performance in a variety of tasks. However, in this
paper, we show that the effects of MSDA are class-dependent, with some classes
seeing an improvement in performance while others experience a decline. To
reduce class dependency, we propose the DropMix method, which excludes a
specific percentage of data from the MSDA computation. By training on a
combination of MSDA and non-MSDA data, the proposed method not only improves
the performance of classes that were previously degraded by MSDA, but also
increases overall average accuracy, as shown in experiments on two datasets
(CIFAR-100 and ImageNet) using three MSDA methods (Mixup, CutMix and
PuzzleMix).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09143">MVA2023 Small Object Detection Challenge for Spotting Birds: Dataset, Methods, and Results. (arXiv:2307.09143v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kondo_Y/0/1/0/all/0/1">Yuki Kondo</a>, <a href="http://arxiv.org/find/cs/1/au:+Ukita_N/0/1/0/all/0/1">Norimichi Ukita</a>, <a href="http://arxiv.org/find/cs/1/au:+Yamaguchi_T/0/1/0/all/0/1">Takayuki Yamaguchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_H/0/1/0/all/0/1">Hao-Yu Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_M/0/1/0/all/0/1">Mu-Yi Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1">Chia-Chi Hsu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_E/0/1/0/all/0/1">En-Ming Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yu-Chen Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1">Yu-Cheng Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chien-Yao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1">Chun-Yi Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Huo_D/0/1/0/all/0/1">Da Huo</a>, <a href="http://arxiv.org/find/cs/1/au:+Kastner_M/0/1/0/all/0/1">Marc A. Kastner</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tingwei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kawanishi_Y/0/1/0/all/0/1">Yasutomo Kawanishi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hirayama_T/0/1/0/all/0/1">Takatsugu Hirayama</a>, <a href="http://arxiv.org/find/cs/1/au:+Komamizu_T/0/1/0/all/0/1">Takahiro Komamizu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ide_I/0/1/0/all/0/1">Ichiro Ide</a>, <a href="http://arxiv.org/find/cs/1/au:+Shinya_Y/0/1/0/all/0/1">Yosuke Shinya</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xinyao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_G/0/1/0/all/0/1">Guang Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yasui_S/0/1/0/all/0/1">Syusuke Yasui</a></p>
<p>Small Object Detection (SOD) is an important machine vision topic because (i)
a variety of real-world applications require object detection for distant
objects and (ii) SOD is a challenging task due to the noisy, blurred, and
less-informative image appearances of small objects. This paper proposes a new
SOD dataset consisting of 39,070 images including 137,121 bird instances, which
is called the Small Object Detection for Spotting Birds (SOD4SB) dataset. The
detail of the challenge with the SOD4SB dataset is introduced in this paper. In
total, 223 participants joined this challenge. This paper briefly introduces
the award-winning methods. The dataset, the baseline code, and the website for
evaluation on the public testset are publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09146">PRO-Face S: Privacy-preserving Reversible Obfuscation of Face Images via Secure Flow. (arXiv:2307.09146v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1">Lin Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1">Kai Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pu_X/0/1/0/all/0/1">Xiao Pu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Leng_J/0/1/0/all/0/1">Jiaxu Leng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1">Tao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1">Nannan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xinbo Gao</a></p>
<p>This paper proposes a novel paradigm for facial privacy protection that
unifies multiple characteristics including anonymity, diversity, reversibility
and security within a single lightweight framework. We name it PRO-Face S,
short for Privacy-preserving Reversible Obfuscation of Face images via Secure
flow-based model. In the framework, an Invertible Neural Network (INN) is
utilized to process the input image along with its pre-obfuscated form, and
generate the privacy protected image that visually approximates to the
pre-obfuscated one, thus ensuring privacy. The pre-obfuscation applied can be
in diversified form with different strengths and styles specified by users.
Along protection, a secret key is injected into the network such that the
original image can only be recovered from the protection image via the same
model given the correct key provided. Two modes of image recovery are devised
to deal with malicious recovery attempts in different scenarios. Finally,
extensive experiments conducted on three public image datasets demonstrate the
superiority of the proposed framework over multiple state-of-the-art
approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09153">OPHAvatars: One-shot Photo-realistic Head Avatars. (arXiv:2307.09153v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shaoxu Li</a></p>
<p>We propose a method for synthesizing photo-realistic digital avatars from
only one portrait as the reference. Given a portrait, our method synthesizes a
coarse talking head video using driving keypoints features. And with the coarse
video, our method synthesizes a coarse talking head avatar with a deforming
neural radiance field. With rendered images of the coarse avatar, our method
updates the low-quality images with a blind face restoration model. With
updated images, we retrain the avatar for higher quality. After several
iterations, our method can synthesize a photo-realistic animatable 3D neural
head avatar. The motivation of our method is deformable neural radiance field
can eliminate the unnatural distortion caused by the image2video method. Our
method outperforms state-of-the-art methods in quantitative and qualitative
studies on various subjects.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09155">MLF-DET: Multi-Level Fusion for Cross-Modal 3D Object Detection. (arXiv:2307.09155v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zewei Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yanqing Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1">Sanping Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shitao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1">Nanning Zheng</a></p>
<p>In this paper, we propose a novel and effective Multi-Level Fusion network,
named as MLF-DET, for high-performance cross-modal 3D object DETection, which
integrates both the feature-level fusion and decision-level fusion to fully
utilize the information in the image. For the feature-level fusion, we present
the Multi-scale Voxel Image fusion (MVI) module, which densely aligns
multi-scale voxel features with image features. For the decision-level fusion,
we propose the lightweight Feature-cued Confidence Rectification (FCR) module
which further exploits image semantics to rectify the confidence of detection
candidates. Besides, we design an effective data augmentation strategy termed
Occlusion-aware GT Sampling (OGS) to reserve more sampled objects in the
training scenes, so as to reduce overfitting. Extensive experiments on the
KITTI dataset demonstrate the effectiveness of our method. Notably, on the
extremely competitive KITTI car 3D object detection benchmark, our method
reaches 82.89% moderate AP and achieves state-of-the-art performance without
bells and whistles.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09158">Class-relation Knowledge Distillation for Novel Class Discovery. (arXiv:2307.09158v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gu_P/0/1/0/all/0/1">Peiyan Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chuyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">Ruijie Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xuming He</a></p>
<p>We tackle the problem of novel class discovery, which aims to learn novel
classes without supervision based on labeled data from known classes. A key
challenge lies in transferring the knowledge in the known-class data to the
learning of novel classes. Previous methods mainly focus on building a shared
representation space for knowledge transfer and often ignore modeling class
relations. To address this, we introduce a class relation representation for
the novel classes based on the predicted class distribution of a model trained
on known classes. Empirically, we find that such class relation becomes less
informative during typical discovery training. To prevent such information
loss, we propose a novel knowledge distillation framework, which utilizes our
class-relation representation to regularize the learning of novel classes. In
addition, to enable a flexible knowledge distillation scheme for each data
point in novel classes, we develop a learnable weighting function for the
regularization, which adaptively promotes knowledge transfer based on the
semantic similarity between the novel and known classes. To validate the
effectiveness and generalization of our method, we conduct extensive
experiments on multiple benchmarks, including CIFAR100, Stanford Cars, CUB, and
FGVC-Aircraft datasets. Our results demonstrate that the proposed method
outperforms the previous state-of-the-art methods by a significant margin on
almost all benchmarks. Code is available at
\href{https://github.com/kleinzcy/Cr-KD-NCD}{here}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09160">Constraining Depth Map Geometry for Multi-View Stereo: A Dual-Depth Approach with Saddle-shaped Depth Cells. (arXiv:2307.09160v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1">Xinyi Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Weiyue Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tianqi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zihao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1">Zhiguo Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xin Li</a></p>
<p>Learning-based multi-view stereo (MVS) methods deal with predicting accurate
depth maps to achieve an accurate and complete 3D representation. Despite the
excellent performance, existing methods ignore the fact that a suitable depth
geometry is also critical in MVS. In this paper, we demonstrate that different
depth geometries have significant performance gaps, even using the same depth
prediction error. Therefore, we introduce an ideal depth geometry composed of
Saddle-Shaped Cells, whose predicted depth map oscillates upward and downward
around the ground-truth surface, rather than maintaining a continuous and
smooth depth plane. To achieve it, we develop a coarse-to-fine framework called
Dual-MVSNet (DMVSNet), which can produce an oscillating depth plane.
Technically, we predict two depth values for each pixel (Dual-Depth), and
propose a novel loss function and a checkerboard-shaped selecting strategy to
constrain the predicted depth geometry. Compared to existing methods,DMVSNet
achieves a high rank on the DTU benchmark and obtains the top performance on
challenging scenes of Tanks and Temples, demonstrating its strong performance
and generalization ability. Our method also points to a new research direction
for considering depth geometry in MVS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09161">CG-fusion CAM: Online segmentation of laser-induced damage on large-aperture optics. (arXiv:2307.09161v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1">Yueyue Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yingyan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1">Hangcheng Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1">Fengdong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_F/0/1/0/all/0/1">Fa Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1">Zhitao Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1">Qihua Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1">Guodong Liu</a></p>
<p>Online segmentation of laser-induced damage on large-aperture optics in
high-power laser facilities is challenged by complicated damage morphology,
uneven illumination and stray light interference. Fully supervised semantic
segmentation algorithms have achieved state-of-the-art performance, but rely on
plenty of pixel-level labels, which are time-consuming and labor-consuming to
produce. LayerCAM, an advanced weakly supervised semantic segmentation
algorithm, can generate pixel-accurate results using only image-level labels,
but its scattered and partially under-activated class activation regions
degrade segmentation performance. In this paper, we propose a weakly supervised
semantic segmentation method with Continuous Gradient CAM and its nonlinear
multi-scale fusion (CG-fusion CAM). The method redesigns the way of
back-propagating gradients and non-linearly activates the multi-scale fused
heatmaps to generate more fine-grained class activation maps with appropriate
activation degree for different sizes of damage sites. Experiments on our
dataset show that the proposed method can achieve segmentation performance
comparable to that of fully supervised algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09165">Towards Trustworthy Dataset Distillation. (arXiv:2307.09165v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1">Shijie Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1">Fei Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1">Zhen Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xu-Yao Zhang</a></p>
<p>Efficiency and trustworthiness are two eternal pursuits when applying deep
learning in real-world applications. With regard to efficiency, dataset
distillation (DD) endeavors to reduce training costs by distilling the large
dataset into a tiny synthetic dataset. However, existing methods merely
concentrate on in-distribution (InD) classification in a closed-world setting,
disregarding out-of-distribution (OOD) samples. On the other hand, OOD
detection aims to enhance models' trustworthiness, which is always
inefficiently achieved in full-data settings. For the first time, we
simultaneously consider both issues and propose a novel paradigm called
Trustworthy Dataset Distillation (TrustDD). By distilling both InD samples and
outliers, the condensed datasets are capable to train models competent in both
InD classification and OOD detection. To alleviate the requirement of real
outlier data and make OOD detection more practical, we further propose to
corrupt InD samples to generate pseudo-outliers and introduce Pseudo-Outlier
Exposure (POE). Comprehensive experiments on various settings demonstrate the
effectiveness of TrustDD, and the proposed POE surpasses state-of-the-art
method Outlier Exposure (OE). Compared with the preceding DD, TrustDD is more
trustworthy and applicable to real open-world scenarios. Our code will be
publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09172">Jean-Luc Picard at Touch\&#x27;e 2023: Comparing Image Generation, Stance Detection and Feature Matching for Image Retrieval for Arguments. (arXiv:2307.09172v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Moebius_M/0/1/0/all/0/1">Max Moebius</a>, <a href="http://arxiv.org/find/cs/1/au:+Enderling_M/0/1/0/all/0/1">Maximilian Enderling</a>, <a href="http://arxiv.org/find/cs/1/au:+Bachinger_S/0/1/0/all/0/1">Sarah T. Bachinger</a></p>
<p>Participating in the shared task "Image Retrieval for arguments", we used
different pipelines for image retrieval containing Image Generation, Stance
Detection, Preselection and Feature Matching. We submitted four different runs
with different pipeline layout and compare them to given baseline. Our
pipelines perform similarly to the baseline.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09183">Pixel-wise Graph Attention Networks for Person Re-identification. (arXiv:2307.09183v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_Q/0/1/0/all/0/1">Qing Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1">Jian Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yi Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1">Mingzhe Lu</a></p>
<p>Graph convolutional networks (GCN) is widely used to handle irregular data
since it updates node features by using the structure information of graph.
With the help of iterated GCN, high-order information can be obtained to
further enhance the representation of nodes. However, how to apply GCN to
structured data (such as pictures) has not been deeply studied. In this paper,
we explore the application of graph attention networks (GAT) in image feature
extraction. First of all, we propose a novel graph generation algorithm to
convert images into graphs through matrix transformation. It is one magnitude
faster than the algorithm based on K Nearest Neighbors (KNN). Then, GAT is used
on the generated graph to update the node features. Thus, a more robust
representation is obtained. These two steps are combined into a module called
pixel-wise graph attention module (PGA). Since the graph obtained by our graph
generation algorithm can still be transformed into a picture after processing,
PGA can be well combined with CNN. Based on these two modules, we consulted the
ResNet and design a pixel-wise graph attention network (PGANet). The PGANet is
applied to the task of person re-identification in the datasets Market1501,
DukeMTMC-reID and Occluded-DukeMTMC (outperforms state-of-the-art by 0.8\%,
1.1\% and 11\% respectively, in mAP scores). Experiment results show that it
achieves the state-of-the-art performance.
\href{https://github.com/wenyu1009/PGANet}{The code is available here}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09184">You&#x27;ve Got Two Teachers: Co-evolutionary Image and Report Distillation for Semi-supervised Anatomical Abnormality Detection in Chest X-ray. (arXiv:2307.09184v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jinghan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1">Dong Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zhe Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1">Donghuan Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Liansheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1">Yefeng Zheng</a></p>
<p>Chest X-ray (CXR) anatomical abnormality detection aims at localizing and
characterising cardiopulmonary radiological findings in the radiographs, which
can expedite clinical workflow and reduce observational oversights. Most
existing methods attempted this task in either fully supervised settings which
demanded costly mass per-abnormality annotations, or weakly supervised settings
which still lagged badly behind fully supervised methods in performance. In
this work, we propose a co-evolutionary image and report distillation (CEIRD)
framework, which approaches semi-supervised abnormality detection in CXR by
grounding the visual detection results with text-classified abnormalities from
paired radiology reports, and vice versa. Concretely, based on the classical
teacher-student pseudo label distillation (TSD) paradigm, we additionally
introduce an auxiliary report classification model, whose prediction is used
for report-guided pseudo detection label refinement (RPDLR) in the primary
vision detection task. Inversely, we also use the prediction of the vision
detection model for abnormality-guided pseudo classification label refinement
(APCLR) in the auxiliary report classification task, and propose a co-evolution
strategy where the vision and report models mutually promote each other with
RPDLR and APCLR performed alternatively. To this end, we effectively
incorporate the weak supervision by reports into the semi-supervised TSD
pipeline. Besides the cross-modal pseudo label refinement, we further propose
an intra-image-modal self-adaptive non-maximum suppression, where the pseudo
detection labels generated by the teacher vision model are dynamically
rectified by high-confidence predictions by the student. Experimental results
on the public MIMIC-CXR benchmark demonstrate CEIRD's superior performance to
several up-to-date weakly and semi-supervised methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09218">A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning. (arXiv:2307.09218v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhenyi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1">Enneng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1">Li Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Heng Huang</a></p>
<p>Forgetting refers to the loss or deterioration of previously acquired
information or knowledge. While the existing surveys on forgetting have
primarily focused on continual learning, forgetting is a prevalent phenomenon
observed in various other research domains within deep learning. Forgetting
manifests in research fields such as generative models due to generator shifts,
and federated learning due to heterogeneous data distributions across clients.
Addressing forgetting encompasses several challenges, including balancing the
retention of old task knowledge with fast learning of new tasks, managing task
interference with conflicting goals, and preventing privacy leakage, etc.
Moreover, most existing surveys on continual learning implicitly assume that
forgetting is always harmful. In contrast, our survey argues that forgetting is
a double-edged sword and can be beneficial and desirable in certain cases, such
as privacy-preserving scenarios. By exploring forgetting in a broader context,
we aim to present a more nuanced understanding of this phenomenon and highlight
its potential advantages. Through this comprehensive survey, we aspire to
uncover potential solutions by drawing upon ideas and approaches from various
fields that have dealt with forgetting. By examining forgetting beyond its
conventional boundaries, in future work, we hope to encourage the development
of novel strategies for mitigating, harnessing, or even embracing forgetting in
real applications. A comprehensive list of papers about forgetting in various
research fields is available at
\url{https://github.com/EnnengYang/Awesome-Forgetting-in-Deep-Learning}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09220">A Survey on Open-Vocabulary Detection and Segmentation: Past, Present, and Future. (arXiv:2307.09220v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1">Chaoyang Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Long Chen</a></p>
<p>As the most fundamental tasks of computer vision, object detection and
segmentation have made tremendous progress in the deep learning era. Due to the
expensive manual labeling, the annotated categories in existing datasets are
often small-scale and pre-defined, i.e., state-of-the-art detectors and
segmentors fail to generalize beyond the closed-vocabulary. To resolve this
limitation, the last few years have witnessed increasing attention toward
Open-Vocabulary Detection (OVD) and Segmentation (OVS). In this survey, we
provide a comprehensive review on the past and recent development of OVD and
OVS. To this end, we develop a taxonomy according to the type of task and
methodology. We find that the permission and usage of weak supervision signals
can well discriminate different methodologies, including: visual-semantic space
mapping, novel visual feature synthesis, region-aware training,
pseudo-labeling, knowledge distillation-based, and transfer learning-based. The
proposed taxonomy is universal across different tasks, covering object
detection, semantic/instance/panoptic segmentation, 3D scene and video
understanding. In each category, its main principles, key challenges,
development routes, strengths, and weaknesses are thoroughly discussed. In
addition, we benchmark each task along with the vital components of each
method. Finally, several promising directions are provided to stimulate future
research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09233">Augmenting CLIP with Improved Visio-Linguistic Reasoning. (arXiv:2307.09233v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1">Samyadeep Basu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanjabi_M/0/1/0/all/0/1">Maziar Sanjabi</a>, <a href="http://arxiv.org/find/cs/1/au:+Massiceti_D/0/1/0/all/0/1">Daniela Massiceti</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1">Shell Xu Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1">Soheil Feizi</a></p>
<p>Image-text contrastive models such as CLIP are useful for a variety of
downstream applications including zero-shot classification, image-text
retrieval and transfer learning. However, these contrastively trained
vision-language models often fail on compositional visio-linguistic tasks such
as Winoground with performance equivalent to random chance. In our paper, we
address this issue and propose a sample-efficient light-weight method called
SDS-CLIP to improve the compositional visio-linguistic reasoning capabilities
of CLIP. The core idea of our method is to use differentiable image
parameterizations to fine-tune CLIP with a distillation objective from large
text-to-image generative models such as Stable-Diffusion which are relatively
good at visio-linguistic reasoning tasks. On the challenging Winoground
compositional reasoning benchmark, our method improves the absolute
visio-linguistic performance of different CLIP models by up to 7%, while on the
ARO dataset, our method improves the visio-linguistic performance by upto 3%.
As a byproduct of inducing visio-linguistic reasoning into CLIP, we also find
that the zero-shot performance improves marginally on a variety of downstream
datasets. Our method reinforces that carefully designed distillation objectives
from generative models can be leveraged to extend existing contrastive
image-text models with improved visio-linguistic reasoning capabilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09238">Fusing Hand and Body Skeletons for Human Action Recognition in Assembly. (arXiv:2307.09238v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aganian_D/0/1/0/all/0/1">Dustin Aganian</a>, <a href="http://arxiv.org/find/cs/1/au:+Kohler_M/0/1/0/all/0/1">Mona K&#xf6;hler</a>, <a href="http://arxiv.org/find/cs/1/au:+Stephan_B/0/1/0/all/0/1">Benedict Stephan</a>, <a href="http://arxiv.org/find/cs/1/au:+Eisenbach_M/0/1/0/all/0/1">Markus Eisenbach</a>, <a href="http://arxiv.org/find/cs/1/au:+Gross_H/0/1/0/all/0/1">Horst-Michael Gross</a></p>
<p>As collaborative robots (cobots) continue to gain popularity in industrial
manufacturing, effective human-robot collaboration becomes crucial. Cobots
should be able to recognize human actions to assist with assembly tasks and act
autonomously. To achieve this, skeleton-based approaches are often used due to
their ability to generalize across various people and environments. Although
body skeleton approaches are widely used for action recognition, they may not
be accurate enough for assembly actions where the worker's fingers and hands
play a significant role. To address this limitation, we propose a method in
which less detailed body skeletons are combined with highly detailed hand
skeletons. We investigate CNNs and transformers, the latter of which are
particularly adept at extracting and combining important information from both
skeleton types using attention. This paper demonstrates the effectiveness of
our proposed approach in enhancing action recognition in assembly scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09239">Generation of High Spatial Resolution Terrestrial Surface from Low Spatial Resolution Elevation Contour Maps via Hierarchical Computation of Median Elevation Regions. (arXiv:2307.09239v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Barman_G/0/1/0/all/0/1">Geetika Barman</a>, <a href="http://arxiv.org/find/cs/1/au:+Sagar_B/0/1/0/all/0/1">B.S. Daya Sagar</a></p>
<p>We proposed a simple yet effective morphological approach to convert a sparse
Digital Elevation Model (DEM) to a dense Digital Elevation Model. The
conversion is similar to that of the generation of high-resolution DEM from its
low-resolution DEM. The approach involves the generation of median contours to
achieve the purpose. It is a sequential step of the I) decomposition of the
existing sparse Contour map into the maximum possible Threshold Elevation
Region (TERs). II) Computing all possible non-negative and non-weighted Median
Elevation Region (MER) hierarchically between the successive TER decomposed
from a sparse contour map. III) Computing the gradient of all TER, and MER
computed from previous steps would yield the predicted intermediate elevation
contour at a higher spatial resolution. We presented this approach initially
with some self-made synthetic data to show how the contour prediction works and
then experimented with the available contour map of Washington, NH to justify
its usefulness. This approach considers the geometric information of existing
contours and interpolates the elevation contour at a new spatial region of a
topographic surface until no elevation contours are necessary to generate. This
novel approach is also very low-cost and robust as it uses elevation contours.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09259">Adaptive Topological Feature via Persistent Homology: Filtration Learning for Point Clouds. (arXiv:2307.09259v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nishikawa_N/0/1/0/all/0/1">Naoki Nishikawa</a>, <a href="http://arxiv.org/find/cs/1/au:+Ike_Y/0/1/0/all/0/1">Yuichi Ike</a>, <a href="http://arxiv.org/find/cs/1/au:+Yamanishi_K/0/1/0/all/0/1">Kenji Yamanishi</a></p>
<p>Machine learning for point clouds has been attracting much attention, with
many applications in various fields, such as shape recognition and material
science. To enhance the accuracy of such machine learning methods, it is known
to be effective to incorporate global topological features, which are typically
extracted by persistent homology. In the calculation of persistent homology for
a point cloud, we need to choose a filtration for the point clouds, an
increasing sequence of spaces. Because the performance of machine learning
methods combined with persistent homology is highly affected by the choice of a
filtration, we need to tune it depending on data and tasks. In this paper, we
propose a framework that learns a filtration adaptively with the use of neural
networks. In order to make the resulting persistent homology
isometry-invariant, we develop a neural network architecture with such
invariance. Additionally, we theoretically show a finite-dimensional
approximation result that justifies our architecture. Experimental results
demonstrated the efficacy of our framework in several classification tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09262">Neuromorphic spintronics simulated using an unconventional data-driven Thiele equation approach. (arXiv:2307.09262v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Moureaux_A/0/1/0/all/0/1">Anatole Moureaux</a>, <a href="http://arxiv.org/find/cs/1/au:+Wergifosse_S/0/1/0/all/0/1">Simon de Wergifosse</a>, <a href="http://arxiv.org/find/cs/1/au:+Chopin_C/0/1/0/all/0/1">Chlo&#xe9; Chopin</a>, <a href="http://arxiv.org/find/cs/1/au:+Araujo_F/0/1/0/all/0/1">Flavio Abreu Araujo</a></p>
<p>In this study, we developed a quantitative description of the dynamics of
spin-torque vortex nano-oscillators (STVOs) through an unconventional model
based on the combination of the Thiele equation approach (TEA) and data from
micromagnetic simulations (MMS). Solving the STVO dynamics with our analytical
model allows to accelerate the simulations by 9 orders of magnitude compared to
MMS while reaching the same level of accuracy. Here, we showcase our model by
simulating a STVO-based neural network for solving a classification task. We
assess its performance with respect to the input signal current intensity and
the level of noise that might affect such a system. Our approach is promising
for accelerating the design of STVO-based neuromorphic computing devices while
decreasing drastically its computational cost.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09264">Knowledge Distillation for Object Detection: from generic to remote sensing datasets. (arXiv:2307.09264v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1">Ho&#xe0;ng-&#xc2;n L&#xea;</a>, <a href="http://arxiv.org/find/cs/1/au:+Pham_M/0/1/0/all/0/1">Minh-Tan Pham</a></p>
<p>Knowledge distillation, a well-known model compression technique, is an
active research area in both computer vision and remote sensing communities. In
this paper, we evaluate in a remote sensing context various off-the-shelf
object detection knowledge distillation methods which have been originally
developed on generic computer vision datasets such as Pascal VOC. In
particular, methods covering both logit mimicking and feature imitation
approaches are applied for vehicle detection using the well-known benchmarks
such as xView and VEDAI datasets. Extensive experiments are performed to
compare the relative performance and interrelationships of the methods.
Experimental results show high variations and confirm the importance of result
aggregation and cross validation on remote sensing datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09267">Distilling Coarse-to-Fine Semantic Matching Knowledge for Weakly Supervised 3D Visual Grounding. (arXiv:2307.09267v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zehan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Haifeng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Linjun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1">Xize Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yichen Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_A/0/1/0/all/0/1">Aoxiong Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zhou Zhao</a></p>
<p>3D visual grounding involves finding a target object in a 3D scene that
corresponds to a given sentence query. Although many approaches have been
proposed and achieved impressive performance, they all require dense
object-sentence pair annotations in 3D point clouds, which are both
time-consuming and expensive. To address the problem that fine-grained
annotated data is difficult to obtain, we propose to leverage weakly supervised
annotations to learn the 3D visual grounding model, i.e., only coarse
scene-sentence correspondences are used to learn object-sentence links. To
accomplish this, we design a novel semantic matching model that analyzes the
semantic similarity between object proposals and sentences in a coarse-to-fine
manner. Specifically, we first extract object proposals and coarsely select the
top-K candidates based on feature and class similarity matrices. Next, we
reconstruct the masked keywords of the sentence using each candidate one by
one, and the reconstructed accuracy finely reflects the semantic similarity of
each candidate to the query. Additionally, we distill the coarse-to-fine
semantic matching knowledge into a typical two-stage 3D visual grounding model,
which reduces inference costs and improves performance by taking full advantage
of the well-studied structure of the existing architectures. We conduct
extensive experiments on ScanRefer, Nr3D, and Sr3D, which demonstrate the
effectiveness of our proposed method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09279">Regression-free Blind Image Quality Assessment. (arXiv:2307.09279v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaoqi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1">Jian Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1">Hao Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1">Weisi Lin</a></p>
<p>Regression-based blind image quality assessment (IQA) models are susceptible
to biased training samples, leading to a biased estimation of model parameters.
To mitigate this issue, we propose a regression-free framework for image
quality evaluation, which is founded upon retrieving similar instances by
incorporating semantic and distortion features. The motivation behind this
approach is rooted in the observation that the human visual system (HVS) has
analogous visual responses to semantically similar image contents degraded by
the same distortion. The proposed framework comprises two classification-based
modules: semantic-based classification (SC) module and distortion-based
classification (DC) module. Given a test image and an IQA database, the SC
module retrieves multiple pristine images based on semantic similarity. The DC
module then retrieves instances based on distortion similarity from the
distorted images that correspond to each retrieved pristine image. Finally, the
predicted quality score is derived by aggregating the subjective quality scores
of multiple retrieved instances. Experimental results on four benchmark
databases validate that the proposed model can remarkably outperform the
state-of-the-art regression-based models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09283">RepViT: Revisiting Mobile CNN From ViT Perspective. (arXiv:2307.09283v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1">Ao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hui Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zijia Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Pu_H/0/1/0/all/0/1">Hengjun Pu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1">Guiguang Ding</a></p>
<p>Recently, lightweight Vision Transformers (ViTs) demonstrate superior
performance and lower latency compared with lightweight Convolutional Neural
Networks (CNNs) on resource-constrained mobile devices. This improvement is
usually attributed to the multi-head self-attention module, which enables the
model to learn global representations. However, the architectural disparities
between lightweight ViTs and lightweight CNNs have not been adequately
examined. In this study, we revisit the efficient design of lightweight CNNs
and emphasize their potential for mobile devices. We incrementally enhance the
mobile-friendliness of a standard lightweight CNN, specifically MobileNetV3, by
integrating the efficient architectural choices of lightweight ViTs. This ends
up with a new family of pure lightweight CNNs, namely RepViT. Extensive
experiments show that RepViT outperforms existing state-of-the-art lightweight
ViTs and exhibits favorable latency in various vision tasks. On ImageNet,
RepViT achieves over 80\% top-1 accuracy with nearly 1ms latency on an iPhone
12, which is the first time for a lightweight model, to the best of our
knowledge. Our largest model, RepViT-M3, obtains 81.4\% accuracy with only
1.3ms latency. The code and trained models are available at
\url{https://github.com/jameslahm/RepViT}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09302">Conformal prediction under ambiguous ground truth. (arXiv:2307.09302v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stutz_D/0/1/0/all/0/1">David Stutz</a>, <a href="http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1">Abhijit Guha Roy</a>, <a href="http://arxiv.org/find/cs/1/au:+Matejovicova_T/0/1/0/all/0/1">Tatiana Matejovicova</a>, <a href="http://arxiv.org/find/cs/1/au:+Strachan_P/0/1/0/all/0/1">Patricia Strachan</a>, <a href="http://arxiv.org/find/cs/1/au:+Cemgil_A/0/1/0/all/0/1">Ali Taylan Cemgil</a>, <a href="http://arxiv.org/find/cs/1/au:+Doucet_A/0/1/0/all/0/1">Arnaud Doucet</a></p>
<p>In safety-critical classification tasks, conformal prediction allows to
perform rigorous uncertainty quantification by providing confidence sets
including the true class with a user-specified probability. This generally
assumes the availability of a held-out calibration set with access to ground
truth labels. Unfortunately, in many domains, such labels are difficult to
obtain and usually approximated by aggregating expert opinions. In fact, this
holds true for almost all datasets, including well-known ones such as CIFAR and
ImageNet. Applying conformal prediction using such labels underestimates
uncertainty. Indeed, when expert opinions are not resolvable, there is inherent
ambiguity present in the labels. That is, we do not have ``crisp'', definitive
ground truth labels and this uncertainty should be taken into account during
calibration. In this paper, we develop a conformal prediction framework for
such ambiguous ground truth settings which relies on an approximation of the
underlying posterior distribution of labels given inputs. We demonstrate our
methodology on synthetic and real datasets, including a case study of skin
condition classification in dermatology.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09306">EigenTrajectory: Low-Rank Descriptors for Multi-Modal Trajectory Forecasting. (arXiv:2307.09306v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bae_I/0/1/0/all/0/1">Inhwan Bae</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1">Jean Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeon_H/0/1/0/all/0/1">Hae-Gon Jeon</a></p>
<p>Capturing high-dimensional social interactions and feasible futures is
essential for predicting trajectories. To address this complex nature, several
attempts have been devoted to reducing the dimensionality of the output
variables via parametric curve fitting such as the B\'ezier curve and B-spline
function. However, these functions, which originate in computer graphics
fields, are not suitable to account for socially acceptable human dynamics. In
this paper, we present EigenTrajectory ($\mathbb{ET}$), a trajectory prediction
approach that uses a novel trajectory descriptor to form a compact space, known
here as $\mathbb{ET}$ space, in place of Euclidean space, for representing
pedestrian movements. We first reduce the complexity of the trajectory
descriptor via a low-rank approximation. We transform the pedestrians' history
paths into our $\mathbb{ET}$ space represented by spatio-temporal principle
components, and feed them into off-the-shelf trajectory forecasting models. The
inputs and outputs of the models as well as social interactions are all
gathered and aggregated in the corresponding $\mathbb{ET}$ space. Lastly, we
propose a trajectory anchor-based refinement method to cover all possible
futures in the proposed $\mathbb{ET}$ space. Extensive experiments demonstrate
that our EigenTrajectory predictor can significantly improve both the
prediction accuracy and reliability of existing trajectory forecasting models
on public benchmarks, indicating that the proposed descriptor is suited to
represent pedestrian behaviors. Code is publicly available at
https://github.com/inhwanbae/EigenTrajectory .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09316">MarS3D: A Plug-and-Play Motion-Aware Model for Semantic Segmentation on Multi-Scan 3D Point Clouds. (arXiv:2307.09316v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiahui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1">Chirui Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jianhui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xiaoyang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1">Lan Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1">Xiaojuan Qi</a></p>
<p>3D semantic segmentation on multi-scan large-scale point clouds plays an
important role in autonomous systems. Unlike the single-scan-based semantic
segmentation task, this task requires distinguishing the motion states of
points in addition to their semantic categories. However, methods designed for
single-scan-based segmentation tasks perform poorly on the multi-scan task due
to the lacking of an effective way to integrate temporal information. We
propose MarS3D, a plug-and-play motion-aware module for semantic segmentation
on multi-scan 3D point clouds. This module can be flexibly combined with
single-scan models to allow them to have multi-scan perception abilities. The
model encompasses two key designs: the Cross-Frame Feature Embedding module for
enriching representation learning and the Motion-Aware Feature Learning module
for enhancing motion awareness. Extensive experiments show that MarS3D can
improve the performance of the baseline model by a large margin. The code is
available at https://github.com/CVMI-Lab/MarS3D.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09323">Efficient Region-Aware Neural Radiance Fields for High-Fidelity Talking Portrait Synthesis. (arXiv:2307.09323v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiahe Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiawei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1">Xiao Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jun Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_L/0/1/0/all/0/1">Lin Gu</a></p>
<p>This paper presents ER-NeRF, a novel conditional Neural Radiance Fields
(NeRF) based architecture for talking portrait synthesis that can concurrently
achieve fast convergence, real-time rendering, and state-of-the-art performance
with small model size. Our idea is to explicitly exploit the unequal
contribution of spatial regions to guide talking portrait modeling.
Specifically, to improve the accuracy of dynamic head reconstruction, a compact
and expressive NeRF-based Tri-Plane Hash Representation is introduced by
pruning empty spatial regions with three planar hash encoders. For speech
audio, we propose a Region Attention Module to generate region-aware condition
feature via an attention mechanism. Different from existing methods that
utilize an MLP-based encoder to learn the cross-modal relation implicitly, the
attention mechanism builds an explicit connection between audio features and
spatial regions to capture the priors of local motions. Moreover, a direct and
fast Adaptive Pose Encoding is introduced to optimize the head-torso separation
problem by mapping the complex transformation of the head pose into spatial
coordinates. Extensive experiments demonstrate that our method renders better
high-fidelity and audio-lips synchronized talking portrait videos, with
realistic details and high efficiency compared to previous methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09329">Towards a performance analysis on pre-trained Visual Question Answering models for autonomous driving. (arXiv:2307.09329v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rekanar_K/0/1/0/all/0/1">Kaavya Rekanar</a>, <a href="http://arxiv.org/find/cs/1/au:+Eising_C/0/1/0/all/0/1">Ciar&#xe1;n Eising</a>, <a href="http://arxiv.org/find/cs/1/au:+Sistu_G/0/1/0/all/0/1">Ganesh Sistu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hayes_M/0/1/0/all/0/1">Martin Hayes</a></p>
<p>This short paper presents a preliminary analysis of three popular Visual
Question Answering (VQA) models, namely ViLBERT, ViLT, and LXMERT, in the
context of answering questions relating to driving scenarios. The performance
of these models is evaluated by comparing the similarity of responses to
reference answers provided by computer vision experts. Model selection is
predicated on the analysis of transformer utilization in multimodal
architectures. The results indicate that models incorporating cross-modal
attention and late fusion techniques exhibit promising potential for generating
improved answers within a driving perspective. This initial analysis serves as
a launchpad for a forthcoming comprehensive comparative study involving nine
VQA models and sets the scene for further investigations into the effectiveness
of VQA model queries in self-driving scenarios. Supplementary material is
available at
https://github.com/KaavyaRekanar/Towards-a-performance-analysis-on-pre-trained-VQA-models-for-autonomous-driving.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09330">Visual Validation versus Visual Estimation: A Study on the Average Value in Scatterplots. (arXiv:2307.09330v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Braun_D/0/1/0/all/0/1">Daniel Braun</a>, <a href="http://arxiv.org/find/cs/1/au:+Suh_A/0/1/0/all/0/1">Ashley Suh</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_R/0/1/0/all/0/1">Remco Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gleicher_M/0/1/0/all/0/1">Michael Gleicher</a>, <a href="http://arxiv.org/find/cs/1/au:+Landesberger_T/0/1/0/all/0/1">Tatiana von Landesberger</a></p>
<p>We investigate the ability of individuals to visually validate statistical
models in terms of their fit to the data. While visual model estimation has
been studied extensively, visual model validation remains under-investigated.
It is unknown how well people are able to visually validate models, and how
their performance compares to visual and computational estimation. As a
starting point, we conducted a study across two populations (crowdsourced and
volunteers). Participants had to both visually estimate (i.e, draw) and
visually validate (i.e., accept or reject) the frequently studied model of
averages. Across both populations, the level of accuracy of the models that
were considered valid was lower than the accuracy of the estimated models. We
find that participants' validation and estimation were unbiased. Moreover,
their natural critical point between accepting and rejecting a given mean value
is close to the boundary of its 95% confidence interval, indicating that the
visually perceived confidence interval corresponds to a common statistical
standard. Our work contributes to the understanding of visual model validation
and opens new research opportunities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09351">SphereNet: Learning a Noise-Robust and General Descriptor for Point Cloud Registration. (arXiv:2307.09351v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1">Guiyu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Zhentao Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1">Hongbin Ma</a></p>
<p>Point cloud registration is to estimate a transformation to align point
clouds collected in different perspectives. In learning-based point cloud
registration, a robust descriptor is vital for high-accuracy registration.
However, most methods are susceptible to noise and have poor generalization
ability on unseen datasets. Motivated by this, we introduce SphereNet to learn
a noise-robust and unseen-general descriptor for point cloud registration. In
our method, first, the spheroid generator builds a geometric domain based on
spherical voxelization to encode initial features. Then, the spherical
interpolation of the sphere is introduced to realize robustness against noise.
Finally, a new spherical convolutional neural network with spherical integrity
padding completes the extraction of descriptors, which reduces the loss of
features and fully captures the geometric features. To evaluate our methods, a
new benchmark 3DMatch-noise with strong noise is introduced. Extensive
experiments are carried out on both indoor and outdoor datasets. Under
high-intensity noise, SphereNet increases the feature matching recall by more
than 25 percentage points on 3DMatch-noise. In addition, it sets a new
state-of-the-art performance for the 3DMatch and 3DLoMatch benchmarks with
93.5\% and 75.6\% registration recall and also has the best generalization
ability on unseen datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09356">OnlineRefer: A Simple Online Baseline for Referring Video Object Segmentation. (arXiv:2307.09356v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1">Dongming Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tiancai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiangyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1">Jianbing Shen</a></p>
<p>Referring video object segmentation (RVOS) aims at segmenting an object in a
video following human instruction. Current state-of-the-art methods fall into
an offline pattern, in which each clip independently interacts with text
embedding for cross-modal understanding. They usually present that the offline
pattern is necessary for RVOS, yet model limited temporal association within
each clip. In this work, we break up the previous offline belief and propose a
simple yet effective online model using explicit query propagation, named
OnlineRefer. Specifically, our approach leverages target cues that gather
semantic information and position prior to improve the accuracy and ease of
referring predictions for the current frame. Furthermore, we generalize our
online model into a semi-online framework to be compatible with video-based
backbones. To show the effectiveness of our method, we evaluate it on four
benchmarks, \ie, Refer-Youtube-VOS, Refer-DAVIS17, A2D-Sentences, and
JHMDB-Sentences. Without bells and whistles, our OnlineRefer with a Swin-L
backbone achieves 63.5 J&amp;F and 64.8 J&amp;F on Refer-Youtube-VOS and Refer-DAVIS17,
outperforming all other offline methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09361">MOCA: Self-supervised Representation Learning by Predicting Masked Online Codebook Assignments. (arXiv:2307.09361v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gidaris_S/0/1/0/all/0/1">Spyros Gidaris</a>, <a href="http://arxiv.org/find/cs/1/au:+Bursuc_A/0/1/0/all/0/1">Andrei Bursuc</a>, <a href="http://arxiv.org/find/cs/1/au:+Simeoni_O/0/1/0/all/0/1">Oriane Simeoni</a>, <a href="http://arxiv.org/find/cs/1/au:+Vobecky_A/0/1/0/all/0/1">Antonin Vobecky</a>, <a href="http://arxiv.org/find/cs/1/au:+Komodakis_N/0/1/0/all/0/1">Nikos Komodakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1">Matthieu Cord</a>, <a href="http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1">Patrick P&#xe9;rez</a></p>
<p>Self-supervised learning can be used for mitigating the greedy needs of
Vision Transformer networks for very large fully-annotated datasets. Different
classes of self-supervised learning offer representations with either good
contextual reasoning properties, e.g., using masked image modeling strategies,
or invariance to image perturbations, e.g., with contrastive methods. In this
work, we propose a single-stage and standalone method, MOCA, which unifies both
desired properties using novel mask-and-predict objectives defined with
high-level features (instead of pixel-level details). Moreover, we show how to
effectively employ both learning paradigms in a synergistic and
computation-efficient way. Doing so, we achieve new state-of-the-art results on
low-shot settings and strong experimental results in various evaluation
protocols with a training that is at least 3 times faster than prior methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09362">Disentangle then Parse:Night-time Semantic Segmentation with Illumination Disentanglement. (arXiv:2307.09362v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1">Zhixiang Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_T/0/1/0/all/0/1">Tao Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huaian Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ling_P/0/1/0/all/0/1">Pengyang Ling</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1">Yi Jin</a></p>
<p>Most prior semantic segmentation methods have been developed for day-time
scenes, while typically underperforming in night-time scenes due to
insufficient and complicated lighting conditions. In this work, we tackle this
challenge by proposing a novel night-time semantic segmentation paradigm, i.e.,
disentangle then parse (DTP). DTP explicitly disentangles night-time images
into light-invariant reflectance and light-specific illumination components and
then recognizes semantics based on their adaptive fusion. Concretely, the
proposed DTP comprises two key components: 1) Instead of processing
lighting-entangled features as in prior works, our Semantic-Oriented
Disentanglement (SOD) framework enables the extraction of reflectance component
without being impeded by lighting, allowing the network to consistently
recognize the semantics under cover of varying and complicated lighting
conditions. 2) Based on the observation that the illumination component can
serve as a cue for some semantically confused regions, we further introduce an
Illumination-Aware Parser (IAParser) to explicitly learn the correlation
between semantics and lighting, and aggregate the illumination features to
yield more precise predictions. Extensive experiments on the night-time
segmentation task with various settings demonstrate that DTP significantly
outperforms state-of-the-art methods. Furthermore, with negligible additional
parameters, DTP can be directly used to benefit existing day-time methods for
night-time segmentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09365">An Evaluation of Zero-Cost Proxies -- from Neural Architecture Performance to Model Robustness. (arXiv:2307.09365v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lukasik_J/0/1/0/all/0/1">Jovita Lukasik</a>, <a href="http://arxiv.org/find/cs/1/au:+Moeller_M/0/1/0/all/0/1">Michael Moeller</a>, <a href="http://arxiv.org/find/cs/1/au:+Keuper_M/0/1/0/all/0/1">Margret Keuper</a></p>
<p>Zero-cost proxies are nowadays frequently studied and used to search for
neural architectures. They show an impressive ability to predict the
performance of architectures by making use of their untrained weights. These
techniques allow for immense search speed-ups. So far the joint search for
well-performing and robust architectures has received much less attention in
the field of NAS. Therefore, the main focus of zero-cost proxies is the clean
accuracy of architectures, whereas the model robustness should play an evenly
important part. In this paper, we analyze the ability of common zero-cost
proxies to serve as performance predictors for robustness in the popular
NAS-Bench-201 search space. We are interested in the single prediction task for
robustness and the joint multi-objective of clean and robust accuracy. We
further analyze the feature importance of the proxies and show that predicting
the robustness makes the prediction task from existing zero-cost proxies more
challenging. As a result, the joint consideration of several proxies becomes
necessary to predict a model's robustness while the clean accuracy can be
regressed from a single such feature.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09367">LEST: Large-scale LiDAR Semantic Segmentation with Transformer. (arXiv:2307.09367v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1">Chuanyu Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1">Nuo Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1">Sikun Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Han Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaohan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1">Shengguang Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Pu Li</a></p>
<p>Large-scale LiDAR-based point cloud semantic segmentation is a critical task
in autonomous driving perception. Almost all of the previous state-of-the-art
LiDAR semantic segmentation methods are variants of sparse 3D convolution.
Although the Transformer architecture is becoming popular in the field of
natural language processing and 2D computer vision, its application to
large-scale point cloud semantic segmentation is still limited. In this paper,
we propose a LiDAR sEmantic Segmentation architecture with pure Transformer,
LEST. LEST comprises two novel components: a Space Filling Curve (SFC) Grouping
strategy and a Distance-based Cosine Linear Transformer, DISCO. On the public
nuScenes semantic segmentation validation set and SemanticKITTI test set, our
model outperforms all the other state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09368">Plug the Leaks: Advancing Audio-driven Talking Face Generation by Preventing Unintended Information Flow. (arXiv:2307.09368v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yaman_D/0/1/0/all/0/1">Dogucan Yaman</a>, <a href="http://arxiv.org/find/cs/1/au:+Eyiokur_F/0/1/0/all/0/1">Fevziye Irem Eyiokur</a>, <a href="http://arxiv.org/find/cs/1/au:+Barmann_L/0/1/0/all/0/1">Leonard B&#xe4;rmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Ekenel_H/0/1/0/all/0/1">Hazim Kemal Ekenel</a>, <a href="http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1">Alexander Waibel</a></p>
<p>Audio-driven talking face generation is the task of creating a
lip-synchronized, realistic face video from given audio and reference frames.
This involves two major challenges: overall visual quality of generated images
on the one hand, and audio-visual synchronization of the mouth part on the
other hand. In this paper, we start by identifying several problematic aspects
of synchronization methods in recent audio-driven talking face generation
approaches. Specifically, this involves unintended flow of lip and pose
information from the reference to the generated image, as well as instabilities
during model training. Subsequently, we propose various techniques for
obviating these issues: First, a silent-lip reference image generator prevents
leaking of lips from the reference to the generated image. Second, an adaptive
triplet loss handles the pose leaking problem. Finally, we propose a stabilized
formulation of synchronization loss, circumventing aforementioned training
instabilities while additionally further alleviating the lip leaking issue.
Combining the individual improvements, we present state-of-the art performance
on LRS2 and LRW in both synchronization and visual quality. We further validate
our design in various ablation experiments, confirming the individual
contributions as well as their complementary effects.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09402">Study of Vision Transformers for Covid-19 Detection from Chest X-rays. (arXiv:2307.09402v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Angara_S/0/1/0/all/0/1">Sandeep Angara</a>, <a href="http://arxiv.org/find/eess/1/au:+Thirunagaru_S/0/1/0/all/0/1">Sharath Thirunagaru</a></p>
<p>The COVID-19 pandemic has led to a global health crisis, highlighting the
need for rapid and accurate virus detection. This research paper examines
transfer learning with vision transformers for COVID-19 detection, known for
its excellent performance in image recognition tasks. We leverage the
capability of Vision Transformers to capture global context and learn complex
patterns from chest X-ray images. In this work, we explored the recent
state-of-art transformer models to detect Covid-19 using CXR images such as
vision transformer (ViT), Swin-transformer, Max vision transformer (MViT), and
Pyramid Vision transformer (PVT). Through the utilization of transfer learning
with IMAGENET weights, the models achieved an impressive accuracy range of
98.75% to 99.5%. Our experiments demonstrate that Vision Transformers achieve
state-of-the-art performance in COVID-19 detection, outperforming traditional
methods and even Convolutional Neural Networks (CNNs). The results highlight
the potential of Vision Transformers as a powerful tool for COVID-19 detection,
with implications for improving the efficiency and accuracy of screening and
diagnosis in clinical settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09416">Let&#x27;s ViCE! Mimicking Human Cognitive Behavior in Image Generation Evaluation. (arXiv:2307.09416v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Betti_F/0/1/0/all/0/1">Federico Betti</a>, <a href="http://arxiv.org/find/cs/1/au:+Staiano_J/0/1/0/all/0/1">Jacopo Staiano</a>, <a href="http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1">Lorenzo Baraldi</a>, <a href="http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1">Lorenzo Baraldi</a>, <a href="http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1">Rita Cucchiara</a>, <a href="http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1">Nicu Sebe</a></p>
<p>Research in Image Generation has recently made significant progress,
particularly boosted by the introduction of Vision-Language models which are
able to produce high-quality visual content based on textual inputs. Despite
ongoing advancements in terms of generation quality and realism, no methodical
frameworks have been defined yet to quantitatively measure the quality of the
generated content and the adherence with the prompted requests: so far, only
human-based evaluations have been adopted for quality satisfaction and for
comparing different generative methods. We introduce a novel automated method
for Visual Concept Evaluation (ViCE), i.e. to assess consistency between a
generated/edited image and the corresponding prompt/instructions, with a
process inspired by the human cognitive behaviour. ViCE combines the strengths
of Large Language Models (LLMs) and Visual Question Answering (VQA) into a
unified pipeline, aiming to replicate the human cognitive process in quality
assessment. This method outlines visual concepts, formulates image-specific
verification questions, utilizes the Q&amp;A system to investigate the image, and
scores the combined outcome. Although this brave new hypothesis of mimicking
humans in the image evaluation process is in its preliminary assessment stage,
results are promising and open the door to a new form of automatic evaluation
which could have significant impact as the image generation or the image target
editing tasks become more and more sophisticated.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09420">Measuring Student Behavioral Engagement using Histogram of Actions. (arXiv:2307.09420v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abdelkawy_A/0/1/0/all/0/1">Ahmed Abdelkawy</a>, <a href="http://arxiv.org/find/cs/1/au:+Alkabbany_I/0/1/0/all/0/1">Islam Alkabbany</a>, <a href="http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1">Asem Ali</a>, <a href="http://arxiv.org/find/cs/1/au:+Farag_A/0/1/0/all/0/1">Aly Farag</a></p>
<p>In this paper, we propose a novel technique for measuring behavioral
engagement through students' actions recognition. The proposed approach
recognizes student actions then predicts the student behavioral engagement
level. For student action recognition, we use human skeletons to model student
postures and upper body movements. To learn the dynamics of student upper body,
a 3D-CNN model is used. The trained 3D-CNN model is used to recognize actions
within every 2minute video segment then these actions are used to build a
histogram of actions which encodes the student actions and their frequencies.
This histogram is utilized as an input to SVM classifier to classify whether
the student is engaged or disengaged. To evaluate the proposed framework, we
build a dataset consisting of 1414 2-minute video segments annotated with 13
actions and 112 video segments annotated with two engagement levels.
Experimental results indicate that student actions can be recognized with top 1
accuracy 83.63% and the proposed framework can capture the average engagement
of the class.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09437">Unsupervised Conditional Slot Attention for Object Centric Learning. (arXiv:2307.09437v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kori_A/0/1/0/all/0/1">Avinash Kori</a>, <a href="http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1">Francesco Locatello</a>, <a href="http://arxiv.org/find/cs/1/au:+Toni_F/0/1/0/all/0/1">Francesca Toni</a>, <a href="http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1">Ben Glocker</a></p>
<p>Extracting object-level representations for downstream reasoning tasks is an
emerging area in AI. Learning object-centric representations in an unsupervised
setting presents multiple challenges, a key one being binding an arbitrary
number of object instances to a specialized object slot. Recent object-centric
representation methods like Slot Attention utilize iterative attention to learn
composable representations with dynamic inference level binding but fail to
achieve specialized slot level binding. To address this, in this paper we
propose Unsupervised Conditional Slot Attention using a novel Probabilistic
Slot Dictionary (PSD). We define PSD with (i) abstract object-level property
vectors as key and (ii) parametric Gaussian distribution as its corresponding
value. We demonstrate the benefits of the learnt specific object-level
conditioning distributions in multiple downstream tasks, namely object
discovery, compositional scene generation, and compositional visual reasoning.
We show that our method provides scene composition capabilities and a
significant boost in a few shot adaptability tasks of compositional visual
reasoning, while performing similarly or better than slot attention in object
discovery tasks
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09456">A comparative analysis of SR-GAN models. (arXiv:2307.09456v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nikroo_F/0/1/0/all/0/1">Fatemeh Rezapoor Nikroo</a>, <a href="http://arxiv.org/find/cs/1/au:+Deshmukh_A/0/1/0/all/0/1">Ajinkya Deshmukh</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1">Anantha Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Tam_A/0/1/0/all/0/1">Adrian Tam</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_K/0/1/0/all/0/1">Kaarthik Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Noris_C/0/1/0/all/0/1">Cleo Noris</a></p>
<p>In this study, we evaluate the performance of multiple state-of-the-art SR
GAN (Super Resolution Generative Adversarial Network) models, ESRGAN,
Real-ESRGAN and EDSR, on a benchmark dataset of real-world images which undergo
degradation using a pipeline. Our results show that some models seem to
significantly increase the resolution of the input images while preserving
their visual quality, this is assessed using Tesseract OCR engine. We observe
that EDSR-BASE model from huggingface outperforms the remaining candidate
models in terms of both quantitative metrics and subjective visual quality
assessments with least compute overhead. Specifically, EDSR generates images
with higher peak signal-to-noise ratio (PSNR) and structural similarity index
(SSIM) values and are seen to return high quality OCR results with Tesseract
OCR engine. These findings suggest that EDSR is a robust and effective approach
for single-image super-resolution and may be particularly well-suited for
applications where high-quality visual fidelity is critical and optimized
compute.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09465">Occlusion Aware Student Emotion Recognition based on Facial Action Unit Detection. (arXiv:2307.09465v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wally_S/0/1/0/all/0/1">Shrouk Wally</a>, <a href="http://arxiv.org/find/cs/1/au:+Elsayed_A/0/1/0/all/0/1">Ahmed Elsayed</a>, <a href="http://arxiv.org/find/cs/1/au:+Alkabbany_I/0/1/0/all/0/1">Islam Alkabbany</a>, <a href="http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1">Asem Ali</a>, <a href="http://arxiv.org/find/cs/1/au:+Farag_A/0/1/0/all/0/1">Aly Farag</a></p>
<p>Given that approximately half of science, technology, engineering, and
mathematics (STEM) undergraduate students in U.S. colleges and universities
leave by the end of the first year [15], it is crucial to improve the quality
of classroom environments. This study focuses on monitoring students' emotions
in the classroom as an indicator of their engagement and proposes an approach
to address this issue. The impact of different facial parts on the performance
of an emotional recognition model is evaluated through experimentation. To test
the proposed model under partial occlusion, an artificially occluded dataset is
introduced. The novelty of this work lies in the proposal of an occlusion-aware
architecture for facial action units (AUs) extraction, which employs attention
mechanism and adaptive feature learning. The AUs can be used later to classify
facial expressions in classroom settings.
</p>
<p>This research paper's findings provide valuable insights into handling
occlusion in analyzing facial images for emotional engagement analysis. The
proposed experiments demonstrate the significance of considering occlusion and
enhancing the reliability of facial analysis models in classroom environments.
These findings can also be extended to other settings where occlusions are
prevalent.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09472">GroupLane: End-to-End 3D Lane Detection with Channel-wise Grouping. (arXiv:2307.09472v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhuoling Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1">Chunrui Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1">Zheng Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jinrong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_E/0/1/0/all/0/1">En Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haoqian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Hengshuang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiangyu Zhang</a></p>
<p>Efficiency is quite important for 3D lane detection due to practical
deployment demand. In this work, we propose a simple, fast, and end-to-end
detector that still maintains high detection precision. Specifically, we devise
a set of fully convolutional heads based on row-wise classification. In
contrast to previous counterparts, ours supports recognizing both vertical and
horizontal lanes. Besides, our method is the first one to perform row-wise
classification in bird-eye-view. In the heads, we split feature into multiple
groups and every group of feature corresponds to a lane instance. During
training, the predictions are associated with lane labels using the proposed
single-win one-to-one matching to compute loss, and no post-processing
operation is demanded for inference. In this way, our proposed fully
convolutional detector, GroupLane, realizes end-to-end detection like DETR.
Evaluated on 3 real world 3D lane benchmarks, OpenLane, Once-3DLanes, and
OpenLane-Huawei, GroupLane adopting ConvNext-Base as the backbone outperforms
the published state-of-the-art PersFormer by 13.6% F1 score in the OpenLane
validation set. Besides, GroupLane with ResNet18 still surpasses PersFormer by
4.9% F1 score, while the inference speed is nearly 7x faster and the FLOPs is
only 13.3% of it.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09474">ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning. (arXiv:2307.09474v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1">Liang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_E/0/1/0/all/0/1">En Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1">Zheng Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jinrong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1">Haoran Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Hongyu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jianjian Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1">Yuang Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1">Runpei Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1">Chunrui Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiangyu Zhang</a></p>
<p>Human-AI interactivity is a critical aspect that reflects the usability of
multimodal large language models (MLLMs). However, existing end-to-end MLLMs
only allow users to interact with them through language instructions, leading
to the limitation of the interactive accuracy and efficiency. In this study, we
present precise referring instructions that utilize diverse reference
representations such as points and boxes as referring prompts to refer to the
special region. This enables MLLMs to focus on the region of interest and
achieve finer-grained interaction. Based on precise referring instruction, we
propose ChatSpot, a unified end-to-end multimodal large language model that
supports diverse forms of interactivity including mouse clicks, drag-and-drop,
and drawing boxes, which provides a more flexible and seamless interactive
experience. We also construct a multi-grained vision-language
instruction-following dataset based on existing datasets and GPT-4 generating.
Furthermore, we design a series of evaluation tasks to assess the effectiveness
of region recognition and interaction. Experimental results showcase ChatSpot's
promising performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09480">FACTS: Facial Animation Creation using the Transfer of Styles. (arXiv:2307.09480v1 [cs.GR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Saunders_J/0/1/0/all/0/1">Jack Saunders</a>, <a href="http://arxiv.org/find/cs/1/au:+Caulkin_S/0/1/0/all/0/1">Steven Caulkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1">Vinay Namboodiri</a></p>
<p>The ability to accurately capture and express emotions is a critical aspect
of creating believable characters in video games and other forms of
entertainment. Traditionally, this animation has been achieved with artistic
effort or performance capture, both requiring costs in time and labor. More
recently, audio-driven models have seen success, however, these often lack
expressiveness in areas not correlated to the audio signal. In this paper, we
present a novel approach to facial animation by taking existing animations and
allowing for the modification of style characteristics. Specifically, we
explore the use of a StarGAN to enable the conversion of 3D facial animations
into different emotions and person-specific styles. We are able to maintain the
lip-sync of the animations with this method thanks to the use of a novel
viseme-preserving loss.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09481">AnyDoor: Zero-shot Object-level Image Customization. (arXiv:2307.09481v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1">Lianghua Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yujun Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1">Deli Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Hengshuang Zhao</a></p>
<p>This work presents AnyDoor, a diffusion-based image generator with the power
to teleport target objects to new scenes at user-specified locations in a
harmonious way. Instead of tuning parameters for each object, our model is
trained only once and effortlessly generalizes to diverse object-scene
combinations at the inference stage. Such a challenging zero-shot setting
requires an adequate characterization of a certain object. To this end, we
complement the commonly used identity feature with detail features, which are
carefully designed to maintain texture details yet allow versatile local
variations (e.g., lighting, orientation, posture, etc.), supporting the object
in favorably blending with different surroundings. We further propose to borrow
knowledge from video datasets, where we can observe various forms (i.e., along
the time axis) of a single object, leading to stronger model generalizability
and robustness. Extensive experiments demonstrate the superiority of our
approach over existing alternatives as well as its great potential in
real-world applications, such as virtual try-on and object moving. Project page
is https://damo-vilab.github.io/AnyDoor-Page/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2011.04408">SeasonDepth: Cross-Season Monocular Depth Prediction Dataset and Benchmark under Multiple Environments. (arXiv:2011.04408v7 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1">Hanjiang Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1">Baoquan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1">Zhijian Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shiqi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jiacheng Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zuxin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1">Wenhao Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1">Ding Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hesheng Wang</a></p>
<p>Different environments pose a great challenge to the outdoor robust visual
perception for long-term autonomous driving, and the generalization of
learning-based algorithms on different environments is still an open problem.
Although monocular depth prediction has been well studied recently, few works
focus on the robustness of learning-based depth prediction across different
environments, e.g. changing illumination and seasons, owing to the lack of such
a multi-environment real-world dataset and benchmark. To this end, the first
cross-season monocular depth prediction dataset and benchmark, SeasonDepth, is
introduced to benchmark the depth estimation performance under different
environments. We investigate several state-of-the-art representative
open-source supervised and self-supervised depth prediction methods using
newly-formulated metrics. Through extensive experimental evaluation on the
proposed dataset and cross-dataset evaluation with current autonomous driving
datasets, the performance and robustness against the influence of multiple
environments are analyzed qualitatively and quantitatively. We show that
long-term monocular depth prediction is still challenging and believe our work
can boost further research on the long-term robustness and generalization for
outdoor visual perception. The dataset is available on
https://seasondepth.github.io, and the benchmark toolkit is available on
https://github.com/ SeasonDepth/SeasonDepth.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.02159">Robustness Analysis of Video-Language Models Against Visual and Language Perturbations. (arXiv:2207.02159v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schiappa_M/0/1/0/all/0/1">Madeline C. Schiappa</a>, <a href="http://arxiv.org/find/cs/1/au:+Vyas_S/0/1/0/all/0/1">Shruti Vyas</a>, <a href="http://arxiv.org/find/cs/1/au:+Palangi_H/0/1/0/all/0/1">Hamid Palangi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rawat_Y/0/1/0/all/0/1">Yogesh S. Rawat</a>, <a href="http://arxiv.org/find/cs/1/au:+Vineet_V/0/1/0/all/0/1">Vibhav Vineet</a></p>
<p>Joint visual and language modeling on large-scale datasets has recently shown
good progress in multi-modal tasks when compared to single modal learning.
However, robustness of these approaches against real-world perturbations has
not been studied. In this work, we perform the first extensive robustness study
of video-language models against various real-world perturbations. We focus on
text-to-video retrieval and propose two large-scale benchmark datasets,
MSRVTT-P and YouCook2-P, which utilize 90 different visual and 35 different
text perturbations. The study reveals some interesting initial findings from
the studied models: 1) models are generally more susceptible when only video is
perturbed as opposed to when only text is perturbed, 2) models that are
pre-trained are more robust than those trained from scratch, 3) models attend
more to scene and objects rather than motion and action. We hope this study
will serve as a benchmark and guide future research in robust video-language
learning. The benchmark introduced in this study along with the code and
datasets is available at https://bit.ly/3CNOly4.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.03824">Boosting Zero-shot Learning via Contrastive Optimization of Attribute Representations. (arXiv:2207.03824v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1">Yu Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1">Miaojing Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1">Fangyun Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1">Guoqi Li</a></p>
<p>Zero-shot learning (ZSL) aims to recognize classes that do not have samples
in the training set. One representative solution is to directly learn an
embedding function associating visual features with corresponding class
semantics for recognizing new classes. Many methods extend upon this solution,
and recent ones are especially keen on extracting rich features from images,
e.g. attribute features. These attribute features are normally extracted within
each individual image; however, the common traits for features across images
yet belonging to the same attribute are not emphasized. In this paper, we
propose a new framework to boost ZSL by explicitly learning attribute
prototypes beyond images and contrastively optimizing them with attribute-level
features within images. Besides the novel architecture, two elements are
highlighted for attribute representations: a new prototype generation module is
designed to generate attribute prototypes from attribute semantics; a hard
example-based contrastive optimization scheme is introduced to reinforce
attribute-level features in the embedding space. We explore two alternative
backbones, CNN-based and transformer-based, to build our framework and conduct
experiments on three standard benchmarks, CUB, SUN, AwA2. Results on these
benchmarks demonstrate that our method improves the state of the art by a
considerable margin. Our codes will be available at
https://github.com/dyabel/CoAR-ZSL.git
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.15304">Hiding Visual Information via Obfuscating Adversarial Perturbations. (arXiv:2209.15304v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1">Zhigang Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1">Dawei Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1">Decheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1">Nannan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xinbo Gao</a></p>
<p>Growing leakage and misuse of visual information raise security and privacy
concerns, which promotes the development of information protection. Existing
adversarial perturbations-based methods mainly focus on the de-identification
against deep learning models. However, the inherent visual information of the
data has not been well protected. In this work, inspired by the Type-I
adversarial attack, we propose an adversarial visual information hiding method
to protect the visual privacy of data. Specifically, the method generates
obfuscating adversarial perturbations to obscure the visual information of the
data. Meanwhile, it maintains the hidden objectives to be correctly predicted
by models. In addition, our method does not modify the parameters of the
applied model, which makes it flexible for different scenarios. Experimental
results on the recognition and classification tasks demonstrate that the
proposed method can effectively hide visual information and hardly affect the
performances of models. The code is available in the supplementary material.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.13108">Heat Demand Forecasting with Multi-Resolutional Representation of Heterogeneous Temporal Ensemble. (arXiv:2210.13108v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ramachandran_A/0/1/0/all/0/1">Adithya Ramachandran</a>, <a href="http://arxiv.org/find/cs/1/au:+Chatterjee_S/0/1/0/all/0/1">Satyaki Chatterjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Bayer_S/0/1/0/all/0/1">Siming Bayer</a>, <a href="http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1">Andreas Maier</a>, <a href="http://arxiv.org/find/cs/1/au:+Flensmark_T/0/1/0/all/0/1">Thorkil Flensmark</a></p>
<p>One of the primal challenges faced by utility companies is ensuring efficient
supply with minimal greenhouse gas emissions. The advent of smart meters and
smart grids provide an unprecedented advantage in realizing an optimised supply
of thermal energies through proactive techniques such as load forecasting. In
this paper, we propose a forecasting framework for heat demand based on neural
networks where the time series are encoded as scalograms equipped with the
capacity of embedding exogenous variables such as weather, and
holiday/non-holiday. Subsequently, CNNs are utilized to predict the heat load
multi-step ahead. Finally, the proposed framework is compared with other
state-of-the-art methods, such as SARIMAX and LSTM. The quantitative results
from retrospective experiments show that the proposed framework consistently
outperforms the state-of-the-art baseline method with real-world data acquired
from Denmark. A minimal mean error of 7.54% for MAPE and 417kW for RMSE is
achieved with the proposed framework in comparison to all other methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.03507">Ensuring Visual Commonsense Morality for Text-to-Image Generation. (arXiv:2212.03507v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1">Seongbeom Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1">Suhong Moon</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jinkyu Kim</a></p>
<p>Text-to-image generation methods produce high-resolution and high-quality
images, but these methods should not produce immoral images that may contain
inappropriate content from the perspective of commonsense morality. In this
paper, we aim to automatically judge the immorality of synthesized images and
manipulate these images into morally acceptable alternatives. To this end, we
build a model that has three main primitives: (1) recognition of the visual
commonsense immorality in a given image, (2) localization or highlighting of
immoral visual (and textual) attributes that contribute to the immorality of
the image, and (3) manipulation of an immoral image to create a
morally-qualifying alternative. We conduct experiments and human studies using
the state-of-the-art Stable Diffusion text-to-image generation model,
demonstrating the effectiveness of our ethical image manipulation approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.14581">HopFIR: Hop-wise GraphFormer with Intragroup Joint Refinement for 3D Human Pose Estimation. (arXiv:2302.14581v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhai_K/0/1/0/all/0/1">Kai Zhai</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_Q/0/1/0/all/0/1">Qiang Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Ouyang_B/0/1/0/all/0/1">Bo Ouyang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">ShanLin Yang</a></p>
<p>2D-to-3D human pose lifting is fundamental for 3D human pose estimation
(HPE). Graph Convolutional Network (GCN) has been proven inherently suitable to
model the human skeletal topology. However, current GCN-based 3D HPE methods
update the node features by aggregating their neighbors' information without
considering the interaction of joints in different motion patterns. Although
some studies import limb information to learn the movement patterns, the latent
synergies among joints, such as maintaining balance in the motion are seldom
investigated. We propose a hop-wise GraphFormer with intragroup joint
refinement (HopFIR) to tackle the 3D HPE problem. The HopFIR mainly consists of
a novel Hop-wise GraphFormer(HGF) module and an Intragroup Joint
Refinement(IJR) module which leverages the prior limb information for
peripheral joints refinement. The HGF module groups the joints by $k$-hop
neighbors and utilizes a hop-wise transformer-like attention mechanism among
these groups to discover latent joint synergy. Extensive experimental results
show that HopFIR outperforms the SOTA methods with a large margin (on the
Human3.6M dataset, the mean per joint position error (MPJPE) is 32.67mm).
Furthermore, it is also demonstrated that previous SOTA GCN-based methods can
benefit from the proposed hop-wise attention mechanism efficiently with
significant performance promotion, such as SemGCN and MGCN are improved by 8.9%
and 4.5%, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.02401">Open-Vocabulary Affordance Detection in 3D Point Clouds. (arXiv:2303.02401v4 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Toan Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Vu_M/0/1/0/all/0/1">Minh Nhat Vu</a>, <a href="http://arxiv.org/find/cs/1/au:+Vuong_A/0/1/0/all/0/1">An Vuong</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1">Dzung Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Vo_T/0/1/0/all/0/1">Thieu Vo</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1">Ngan Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1">Anh Nguyen</a></p>
<p>Affordance detection is a challenging problem with a wide variety of robotic
applications. Traditional affordance detection methods are limited to a
predefined set of affordance labels, hence potentially restricting the
adaptability of intelligent robots in complex and dynamic environments. In this
paper, we present the Open-Vocabulary Affordance Detection (OpenAD) method,
which is capable of detecting an unbounded number of affordances in 3D point
clouds. By simultaneously learning the affordance text and the point feature,
OpenAD successfully exploits the semantic relationships between affordances.
Therefore, our proposed method enables zero-shot detection and can be able to
detect previously unseen affordances without a single annotation example.
Intensive experimental results show that OpenAD works effectively on a wide
range of affordance detection setups and outperforms other baselines by a large
margin. Additionally, we demonstrate the practicality of the proposed OpenAD in
real-world robotic applications with a fast inference speed (~100ms). Our
project is available at https://openad2023.github.io.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.02885">Improving Transformer-based Image Matching by Cascaded Capturing Spatially Informative Keypoints. (arXiv:2303.02885v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1">Chenjie Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1">Yanwei Fu</a></p>
<p>Learning robust local image feature matching is a fundamental low-level
vision task, which has been widely explored in the past few years. Recently,
detector-free local feature matchers based on transformers have shown promising
results, which largely outperform pure Convolutional Neural Network (CNN) based
ones. But correlations produced by transformer-based methods are spatially
limited to the center of source views' coarse patches, because of the costly
attention learning. In this work, we rethink this issue and find that such
matching formulation degrades pose estimation, especially for low-resolution
images. So we propose a transformer-based cascade matching model -- Cascade
feature Matching TRansformer (CasMTR), to efficiently learn dense feature
correlations, which allows us to choose more reliable matching pairs for the
relative pose estimation. Instead of re-training a new detector, we use a
simple yet effective Non-Maximum Suppression (NMS) post-process to filter
keypoints through the confidence map, and largely improve the matching
precision. CasMTR achieves state-of-the-art performance in indoor and outdoor
pose estimation as well as visual localization. Moreover, thorough ablations
show the efficacy of the proposed components and techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.05118">SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model. (arXiv:2303.05118v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Gengwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Liyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_G/0/1/0/all/0/1">Guoliang Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Ling Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1">Yunchao Wei</a></p>
<p>The goal of continual learning is to improve the performance of recognition
models in learning sequentially arrived data. Although most existing works are
established on the premise of learning from scratch, growing efforts have been
devoted to incorporating the benefits of pre-training. However, how to
adaptively exploit the pre-trained knowledge for each incremental task while
maintaining its generalizability remains an open question. In this work, we
present an extensive analysis for continual learning on a pre-trained model
(CLPM), and attribute the key challenge to a progressive overfitting problem.
Observing that selectively reducing the learning rate can almost resolve this
issue in the representation layer, we propose a simple but extremely effective
approach named Slow Learner with Classifier Alignment (SLCA), which further
improves the classification layer by modeling the class-wise distributions and
aligning the classification layers in a post-hoc fashion. Across a variety of
scenarios, our proposal provides substantial improvements for CLPM (e.g., up to
49.76%, 50.05%, 44.69% and 40.16% on Split CIFAR-100, Split ImageNet-R, Split
CUB-200 and Split Cars-196, respectively), and thus outperforms
state-of-the-art approaches by a large margin. Based on such a strong baseline,
critical factors and promising directions are analyzed in-depth to facilitate
subsequent research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.15932">Unify, Align and Refine: Multi-Level Semantic Alignment for Radiology Report Generation. (arXiv:2303.15932v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yaowei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1">Bang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1">Xuxin Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Zhihong Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hongxiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1">Yuexian Zou</a></p>
<p>Automatic radiology report generation has attracted enormous research
interest due to its practical value in reducing the workload of radiologists.
However, simultaneously establishing global correspondences between the image
(e.g., Chest X-ray) and its related report and local alignments between image
patches and keywords remains challenging. To this end, we propose an Unify,
Align and then Refine (UAR) approach to learn multi-level cross-modal
alignments and introduce three novel modules: Latent Space Unifier (LSU),
Cross-modal Representation Aligner (CRA) and Text-to-Image Refiner (TIR).
Specifically, LSU unifies multimodal data into discrete tokens, making it
flexible to learn common knowledge among modalities with a shared network. The
modality-agnostic CRA learns discriminative features via a set of orthonormal
basis and a dual-gate mechanism first and then globally aligns visual and
textual representations under a triplet contrastive loss. TIR boosts
token-level local alignment via calibrating text-to-image attention with a
learnable mask. Additionally, we design a two-stage training procedure to make
UAR gradually grasp cross-modal alignments at different levels, which imitates
radiologists' workflow: writing sentence by sentence first and then checking
word by word. Extensive experiments and analyses on IU-Xray and MIMIC-CXR
benchmark datasets demonstrate the superiority of our UAR against varied
state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.01580">Untargeted Near-collision Attacks in Biometric Recognition. (arXiv:2304.01580v3 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Durbet_A/0/1/0/all/0/1">Axel Durbet</a>, <a href="http://arxiv.org/find/cs/1/au:+Grollemund_P/0/1/0/all/0/1">Paul-Marie Grollemund</a>, <a href="http://arxiv.org/find/cs/1/au:+Thiry_Atighehchi_K/0/1/0/all/0/1">Kevin Thiry-Atighehchi</a></p>
<p>A biometric recognition system can operate in two distinct modes,
identification or verification. In the first mode, the system recognizes an
individual by searching the enrolled templates of all the users for a match. In
the second mode, the system validates a user's identity claim by comparing the
fresh provided template with the enrolled template. The biometric
transformation schemes usually produce binary templates that are better handled
by cryptographic schemes, and the comparison is based on a distance that leaks
information about the similarities between two biometric templates. Both the
experimentally determined false match rate and false non-match rate through
recognition threshold adjustment define the recognition accuracy, and hence the
security of the system. To the best of our knowledge, few works provide a
formal treatment of the security under minimum leakage of information, i.e.,
the binary outcome of a comparison with a threshold. In this paper, we rely on
probabilistic modelling to quantify the security strength of binary templates.
We investigate the influence of template size, database size and threshold on
the probability of having a near-collision. We highlight several untargeted
attacks on biometric systems considering naive and adaptive adversaries.
Interestingly, these attacks can be launched both online and offline and, both
in the identification mode and in the verification mode. We discuss the choice
of parameters through the generic presented attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.02689">ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast. (arXiv:2304.02689v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1">Chenyu You</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1">Weicheng Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Min_Y/0/1/0/all/0/1">Yifei Min</a>, <a href="http://arxiv.org/find/cs/1/au:+Staib_L/0/1/0/all/0/1">Lawrence Staib</a>, <a href="http://arxiv.org/find/cs/1/au:+Sekhon_J/0/1/0/all/0/1">Jasjeet S. Sekhon</a>, <a href="http://arxiv.org/find/cs/1/au:+Duncan_J/0/1/0/all/0/1">James S. Duncan</a></p>
<p>Medical data often exhibits long-tail distributions with heavy class
imbalance, which naturally leads to difficulty in classifying the minority
classes (i.e., boundary regions or rare objects). Recent work has significantly
improved semi-supervised medical image segmentation in long-tailed scenarios by
equipping them with unsupervised contrastive criteria. However, it remains
unclear how well they will perform in the labeled portion of data where class
distribution is also highly imbalanced. In this work, we present ACTION++, an
improved contrastive learning framework with adaptive anatomical contrast for
semi-supervised medical segmentation. Specifically, we propose an adaptive
supervised contrastive loss, where we first compute the optimal locations of
class centers uniformly distributed on the embedding space (i.e., off-line),
and then perform online contrastive matching training by encouraging different
class features to adaptively match these distinct and uniformly distributed
class centers. Moreover, we argue that blindly adopting a constant temperature
$\tau$ in the contrastive loss on long-tailed medical data is not optimal, and
propose to use a dynamic $\tau$ via a simple cosine schedule to yield better
separation between majority and minority classes. Empirically, we evaluate
ACTION++ on ACDC and LA benchmarks and show that it achieves state-of-the-art
across two semi-supervised settings. Theoretically, we analyze the performance
of adaptive anatomical contrast and confirm its superiority in label
efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.03209">Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts. (arXiv:2304.03209v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1">Chenyu You</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1">Weicheng Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Min_Y/0/1/0/all/0/1">Yifei Min</a>, <a href="http://arxiv.org/find/cs/1/au:+Staib_L/0/1/0/all/0/1">Lawrence Staib</a>, <a href="http://arxiv.org/find/cs/1/au:+Duncan_J/0/1/0/all/0/1">James S. Duncan</a></p>
<p>Integrating high-level semantically correlated contents and low-level
anatomical features is of central importance in medical image segmentation.
Towards this end, recent deep learning-based medical segmentation methods have
shown great promise in better modeling such information. However, convolution
operators for medical segmentation typically operate on regular grids, which
inherently blur the high-frequency regions, i.e., boundary regions. In this
work, we propose MORSE, a generic implicit neural rendering framework designed
at an anatomical level to assist learning in medical image segmentation. Our
method is motivated by the fact that implicit neural representation has been
shown to be more effective in fitting complex signals and solving computer
graphics problems than discrete grid-based representation. The core of our
approach is to formulate medical image segmentation as a rendering problem in
an end-to-end manner. Specifically, we continuously align the coarse
segmentation prediction with the ambiguous coordinate-based point
representations and aggregate these features to adaptively refine the boundary
region. To parallelly optimize multi-scale pixel-level features, we leverage
the idea from Mixture-of-Expert (MoE) to design and train our MORSE with a
stochastic gating mechanism. Our experiments demonstrate that MORSE can work
well with different medical segmentation backbones, consistently achieving
competitive performance improvements in both 2D and 3D supervised medical
segmentation methods. We also theoretically analyze the superiority of MORSE.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.08101">LLA-FLOW: A Lightweight Local Aggregation on Cost Volume for Optical Flow Estimation. (arXiv:2304.08101v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jiawei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Zongqing Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1">Qingmin Liao</a></p>
<p>Lack of texture often causes ambiguity in matching, and handling this issue
is an important challenge in optical flow estimation. Some methods insert
stacked transformer modules that allow the network to use global information of
cost volume for estimation. But the global information aggregation often incurs
serious memory and time costs during training and inference, which hinders
model deployment. We draw inspiration from the traditional local region
constraint and design the local similarity aggregation (LSA) and the shifted
local similarity aggregation (SLSA). The aggregation for cost volume is
implemented with lightweight modules that act on the feature maps. Experiments
on the final pass of Sintel show the lower cost required for our approach while
maintaining competitive performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.10769">Deep Multiview Clustering by Contrasting Cluster Assignments. (arXiv:2304.10769v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jie Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1">Hua Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Woo_W/0/1/0/all/0/1">Wai Lok Woo</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1">Xi Peng</a></p>
<p>Multiview clustering (MVC) aims to reveal the underlying structure of
multiview data by categorizing data samples into clusters. Deep learning-based
methods exhibit strong feature learning capabilities on large-scale datasets.
For most existing deep MVC methods, exploring the invariant representations of
multiple views is still an intractable problem. In this paper, we propose a
cross-view contrastive learning (CVCL) method that learns view-invariant
representations and produces clustering results by contrasting the cluster
assignments among multiple views. Specifically, we first employ deep
autoencoders to extract view-dependent features in the pretraining stage. Then,
a cluster-level CVCL strategy is presented to explore consistent semantic label
information among the multiple views in the fine-tuning stage. Thus, the
proposed CVCL method is able to produce more discriminative cluster assignments
by virtue of this learning strategy. Moreover, we provide a theoretical
analysis of soft cluster assignment alignment. Extensive experimental results
obtained on several datasets demonstrate that the proposed CVCL method
outperforms several state-of-the-art approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.11862">Universal Domain Adaptation via Compressive Attention Matching. (arXiv:2304.11862v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1">Didi Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yincuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1">Junkun Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zexi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1">Kun Kuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1">Chao Wu</a></p>
<p>Universal domain adaptation (UniDA) aims to transfer knowledge from the
source domain to the target domain without any prior knowledge about the label
set. The challenge lies in how to determine whether the target samples belong
to common categories. The mainstream methods make judgments based on the sample
features, which overemphasizes global information while ignoring the most
crucial local objects in the image, resulting in limited accuracy. To address
this issue, we propose a Universal Attention Matching (UniAM) framework by
exploiting the self-attention mechanism in vision transformer to capture the
crucial object information. The proposed framework introduces a novel
Compressive Attention Matching (CAM) approach to explore the core information
by compressively representing attentions. Furthermore, CAM incorporates a
residual-based measurement to determine the sample commonness. By utilizing the
measurement, UniAM achieves domain-wise and category-wise Common Feature
Alignment (CFA) and Target Class Separation (TCS). Notably, UniAM is the first
method utilizing the attention in vision transformer directly to perform
classification tasks. Extensive experiments show that UniAM outperforms the
current state-of-the-art methods on various benchmark datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.12306">Segment Anything in Medical Images. (arXiv:2304.12306v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ma_J/0/1/0/all/0/1">Jun Ma</a>, <a href="http://arxiv.org/find/eess/1/au:+He_Y/0/1/0/all/0/1">Yuting He</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_F/0/1/0/all/0/1">Feifei Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Han_L/0/1/0/all/0/1">Lin Han</a>, <a href="http://arxiv.org/find/eess/1/au:+You_C/0/1/0/all/0/1">Chenyu You</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_B/0/1/0/all/0/1">Bo Wang</a></p>
<p>Medical image segmentation is a critical component in clinical practice,
facilitating accurate diagnosis, treatment planning, and disease monitoring.
However, current methods predominantly rely on customized models, which exhibit
limited generality across diverse tasks. In this study, we present MedSAM, the
inaugural foundation model designed for universal medical image segmentation.
Harnessing the power of a meticulously curated dataset comprising over one
million images, MedSAM not only outperforms existing state-of-the-art
segmentation foundation models, but also exhibits comparable or even superior
performance to specialist models. Moreover, MedSAM enables the precise
extraction of essential biomarkers for tumor burden quantification. By
delivering accurate and efficient segmentation across a wide spectrum of tasks,
MedSAM holds significant potential to expedite the evolution of diagnostic
tools and the personalization of treatment plans.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.04422">Performance Gaps of Artificial Intelligence Models Screening Mammography -- Towards Fair and Interpretable Models. (arXiv:2305.04422v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1">Linglin Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Brown_Mulry_B/0/1/0/all/0/1">Beatrice Brown-Mulry</a>, <a href="http://arxiv.org/find/eess/1/au:+Nalla_V/0/1/0/all/0/1">Vineela Nalla</a>, <a href="http://arxiv.org/find/eess/1/au:+Hwang_I/0/1/0/all/0/1">InChan Hwang</a>, <a href="http://arxiv.org/find/eess/1/au:+Gichoya_J/0/1/0/all/0/1">Judy Wawira Gichoya</a>, <a href="http://arxiv.org/find/eess/1/au:+Gastounioti_A/0/1/0/all/0/1">Aimilia Gastounioti</a>, <a href="http://arxiv.org/find/eess/1/au:+Banerjee_I/0/1/0/all/0/1">Imon Banerjee</a>, <a href="http://arxiv.org/find/eess/1/au:+Seyyed_Kalantari_L/0/1/0/all/0/1">Laleh Seyyed-Kalantari</a>, <a href="http://arxiv.org/find/eess/1/au:+Woo_M/0/1/0/all/0/1">MinJae Woo</a>, <a href="http://arxiv.org/find/eess/1/au:+Trivedi_H/0/1/0/all/0/1">Hari Trivedi</a></p>
<p>Even though deep learning models for abnormality classification can perform
well in screening mammography, the demographic and imaging characteristics
associated with increased risk of failure for abnormality classification in
screening mammograms remain unclear. This retrospective study used data from
the Emory BrEast Imaging Dataset (EMBED) including mammograms from 115,931
patients imaged at Emory University Healthcare between 2013 to 2020. Clinical
and imaging data includes Breast Imaging Reporting and Data System (BI-RADS)
assessment, region of interest coordinates for abnormalities, imaging features,
pathologic outcomes, and patient demographics. Deep learning models including
InceptionV3, VGG16, ResNet50V2, and ResNet152V2 were developed to distinguish
between patches of abnormal tissue and randomly selected patches of normal
tissue from the screening mammograms. The distributions of the training,
validation and test sets are 29,144 (55.6%) patches of 10,678 (54.2%) patients,
9,910 (18.9%) patches of 3,609 (18.3%) patients, and 13,390 (25.5%) patches of
5,404 (27.5%) patients. We assessed model performance overall and within
subgroups defined by age, race, pathologic outcome, and imaging characteristics
to evaluate reasons for misclassifications. On the test set, a ResNet152V2
model trained to classify normal versus abnormal tissue patches achieved an
accuracy of 92.6% (95%CI=92.0-93.2%), and area under the receiver operative
characteristics curve 0.975 (95%CI=0.972-0.978). Imaging characteristics
associated with higher misclassifications of images include higher tissue
densities (risk ratio [RR]=1.649; p=.010, BI-RADS density C and RR=2.026;
p=.003, BI-RADS density D), and presence of architectural distortion (RR=1.026;
p&lt;.001). Small but statistically significant differences in performance were
observed by age, race, pathologic outcome, and other imaging features (p&lt;.001).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.07848">Meta-Polyp: a baseline for efficient Polyp segmentation. (arXiv:2305.07848v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Trinh_Q/0/1/0/all/0/1">Quoc-Huy Trinh</a></p>
<p>In recent years, polyp segmentation has gained significant importance, and
many methods have been developed using CNN, Vision Transformer, and Transformer
techniques to achieve competitive results. However, these methods often face
difficulties when dealing with out-of-distribution datasets, missing
boundaries, and small polyps. In 2022, Meta-Former was introduced as a new
baseline for vision, which not only improved the performance of multi-task
computer vision but also addressed the limitations of the Vision Transformer
and CNN family backbones. To further enhance segmentation, we propose a fusion
of Meta-Former with UNet, along with the introduction of a Multi-scale
Upsampling block with a level-up combination in the decoder stage to enhance
the texture, also we propose the Convformer block base on the idea of the
Meta-former to enhance the crucial information of the local feature. These
blocks enable the combination of global information, such as the overall shape
of the polyp, with local information and boundary information, which is crucial
for the decision of the medical segmentation. Our proposed approach achieved
competitive performance and obtained the top result in the State of the Art on
the CVC-300 dataset, Kvasir, and CVC-ColonDB dataset. Apart from Kvasir-SEG,
others are out-of-distribution datasets. The implementation can be found at:
https://github.com/huyquoctrinh/MetaPolyp-CBMS2023.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.08546">Towards Visual Saliency Explanations of Face Verification. (arXiv:2305.08546v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yuhang Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zewei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ebrahimi_T/0/1/0/all/0/1">Touradj Ebrahimi</a></p>
<p>In the past years, deep convolutional neural networks have been pushing the
frontier of face recognition (FR) techniques in both verification and
identification scenarios. Despite the high accuracy, they are often criticized
for lacking explainability. There has been an increasing demand for
understanding the decision-making process of deep face recognition systems.
Recent studies have investigated the usage of visual saliency maps as an
explanation, but they often lack a discussion and analysis in the context of
face recognition. This paper concentrates on explainable face verification
tasks and conceives a new explanation framework. First, a definition of the
saliency-based explanation method is provided, which focuses on the decisions
made by the deep FR model. Then, a new model-agnostic explanation method named
CorrRISE is proposed to produce saliency maps, which reveal both the similar
and dissimilar regions of any given pair of face images. Besides, two
evaluation metrics are designed to measure the performance of general visual
saliency explanation methods in face verification. Consequently, substantial
visual and quantitative results have shown that the proposed CorrRISE method
demonstrates promising results in comparison with other state-of-the-art
explainable face verification approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.09211">CB-HVTNet: A channel-boosted hybrid vision transformer network for lymphocyte assessment in histopathological images. (arXiv:2305.09211v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ali_M/0/1/0/all/0/1">Momina Liaqat Ali</a>, <a href="http://arxiv.org/find/eess/1/au:+Rauf_Z/0/1/0/all/0/1">Zunaira Rauf</a>, <a href="http://arxiv.org/find/eess/1/au:+Khan_A/0/1/0/all/0/1">Asifullah Khan</a>, <a href="http://arxiv.org/find/eess/1/au:+Sohail_A/0/1/0/all/0/1">Anabia Sohail</a>, <a href="http://arxiv.org/find/eess/1/au:+Ullah_R/0/1/0/all/0/1">Rafi Ullah</a>, <a href="http://arxiv.org/find/eess/1/au:+Gwak_J/0/1/0/all/0/1">Jeonghwan Gwak</a></p>
<p>Transformers, due to their ability to learn long range dependencies, have
overcome the shortcomings of convolutional neural networks (CNNs) for global
perspective learning. Therefore, they have gained the focus of researchers for
several vision related tasks including medical diagnosis. However, their
multi-head attention module only captures global level feature representations,
which is insufficient for medical images. To address this issue, we propose a
Channel Boosted Hybrid Vision Transformer (CB HVT) that uses transfer learning
to generate boosted channels and employs both transformers and CNNs to analyse
lymphocytes in histopathological images. The proposed CB HVT comprises five
modules, including a channel generation module, channel exploitation module,
channel merging module, region-aware module, and a detection and segmentation
head, which work together to effectively identify lymphocytes. The channel
generation module uses the idea of channel boosting through transfer learning
to extract diverse channels from different auxiliary learners. In the CB HVT,
these boosted channels are first concatenated and ranked using an attention
mechanism in the channel exploitation module. A fusion block is then utilized
in the channel merging module for a gradual and systematic merging of the
diverse boosted channels to improve the network's learning representations. The
CB HVT also employs a proposal network in its region aware module and a head to
effectively identify objects, even in overlapping regions and with artifacts.
We evaluated the proposed CB HVT on two publicly available datasets for
lymphocyte assessment in histopathological images. The results show that CB HVT
outperformed other state of the art detection models, and has good
generalization ability, demonstrating its value as a tool for pathologists.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13399">Efficient Large-Scale Visual Representation Learning And Evaluation. (arXiv:2305.13399v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dolev_E/0/1/0/all/0/1">Eden Dolev</a>, <a href="http://arxiv.org/find/cs/1/au:+Awad_A/0/1/0/all/0/1">Alaa Awad</a>, <a href="http://arxiv.org/find/cs/1/au:+Roberts_D/0/1/0/all/0/1">Denisa Roberts</a>, <a href="http://arxiv.org/find/cs/1/au:+Ebrahimzadeh_Z/0/1/0/all/0/1">Zahra Ebrahimzadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Mejran_M/0/1/0/all/0/1">Marcin Mejran</a>, <a href="http://arxiv.org/find/cs/1/au:+Malpani_V/0/1/0/all/0/1">Vaibhav Malpani</a>, <a href="http://arxiv.org/find/cs/1/au:+Yavuz_M/0/1/0/all/0/1">Mahir Yavuz</a></p>
<p>In this article, we present our approach to single-modality visual
representation learning. Understanding visual representations of items is vital
for fashion recommendations in e-commerce. We detail and contrast techniques
used to finetune large-scale visual representation learning models in an
efficient manner under low-resource settings, including several pretrained
backbone architectures, both in the convolutional neural network as well as the
vision transformer family. We describe the challenges for e-commerce
applications at-scale and highlight the efforts to more efficiently train,
evaluate, and serve visual representations. We present ablation studies
evaluating the representation offline performance for several downstream tasks,
including visually similar ad recommendations on mobile devices. To this end,
we present a novel multilingual text-to-image generative offline evaluation
method for visually similar recommendation systems. Finally, we include online
results from deployed machine learning systems in production at Etsy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16656">Clustering Method for Time-Series Images Using Quantum-Inspired Computing Technology. (arXiv:2305.16656v3 [eess.SP] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Inoue_T/0/1/0/all/0/1">Tomoki Inoue</a>, <a href="http://arxiv.org/find/eess/1/au:+Kubota_K/0/1/0/all/0/1">Koyo Kubota</a>, <a href="http://arxiv.org/find/eess/1/au:+Ikami_T/0/1/0/all/0/1">Tsubasa Ikami</a>, <a href="http://arxiv.org/find/eess/1/au:+Egami_Y/0/1/0/all/0/1">Yasuhiro Egami</a>, <a href="http://arxiv.org/find/eess/1/au:+Nagai_H/0/1/0/all/0/1">Hiroki Nagai</a>, <a href="http://arxiv.org/find/eess/1/au:+Kashikawa_T/0/1/0/all/0/1">Takahiro Kashikawa</a>, <a href="http://arxiv.org/find/eess/1/au:+Kimura_K/0/1/0/all/0/1">Koichi Kimura</a>, <a href="http://arxiv.org/find/eess/1/au:+Matsuda_Y/0/1/0/all/0/1">Yu Matsuda</a></p>
<p>Time-series clustering serves as a powerful data mining technique for
time-series data in the absence of prior knowledge about clusters. A large
amount of time-series data with large size has been acquired and used in
various research fields. Hence, clustering method with low computational cost
is required. Given that a quantum-inspired computing technology, such as a
simulated annealing machine, surpasses conventional computers in terms of fast
and accurately solving combinatorial optimization problems, it holds promise
for accomplishing clustering tasks that are challenging to achieve using
existing methods. This study proposes a novel time-series clustering method
that leverages an annealing machine. The proposed method facilitates an even
classification of time-series data into clusters close to each other while
maintaining robustness against outliers. Moreover, its applicability extends to
time-series images. We compared the proposed method with a standard existing
method for clustering an online distributed dataset. In the existing method,
the distances between each data are calculated based on the Euclidean distance
metric, and the clustering is performed using the k-means++ method. We found
that both methods yielded comparable results. Furthermore, the proposed method
was applied to a flow measurement image dataset containing noticeable noise
with a signal-to-noise ratio of approximately 1. Despite a small signal
variation of approximately 2%, the proposed method effectively classified the
data without any overlap among the clusters. In contrast, the clustering
results by the standard existing method and the conditional image sampling
(CIS) method, a specialized technique for flow measurement data, displayed
overlapping clusters. Consequently, the proposed method provides better results
than the other two methods, demonstrating its potential as a superior
clustering method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.05439">Contrastive Representation Disentanglement for Clustering. (arXiv:2306.05439v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1">Fei Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Krovi_V/0/1/0/all/0/1">Venkat Krovi</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1">Feng Luo</a></p>
<p>Clustering continues to be a significant and challenging task. Recent studies
have demonstrated impressive results by applying clustering to feature
representations acquired through self-supervised learning, particularly on
small datasets. However, when dealing with datasets containing a large number
of clusters, such as ImageNet, current methods struggle to achieve satisfactory
clustering performance. In this paper, we introduce a novel method called
Contrastive representation Disentanglement for Clustering (CDC) that leverages
contrastive learning to directly disentangle the feature representation for
clustering. In CDC, we decompose the representation into two distinct
components: one component encodes categorical information under an
equipartition constraint, and the other component captures instance-specific
factors. To train our model, we propose a contrastive loss that effectively
utilizes both components of the representation. We conduct a theoretical
analysis of the proposed loss and highlight how it assigns different weights to
negative samples during the process of disentangling the feature
representation. Further analysis of the gradients reveals that larger weights
emphasize a stronger focus on hard negative samples. As a result, the proposed
loss exhibits strong expressiveness, enabling efficient disentanglement of
categorical information. Through experimental evaluation on various benchmark
datasets, our method demonstrates either state-of-the-art or highly competitive
clustering performance. Notably, on the complete ImageNet dataset, we achieve
an accuracy of 53.4%, surpassing existing methods by a substantial margin of
+10.2%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.06849">Mitigating Transformer Overconfidence via Lipschitz Regularization. (arXiv:2306.06849v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1">Wenqian Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yunsheng Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1">Xu Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1">Kun Tang</a></p>
<p>Though Transformers have achieved promising results in many computer vision
tasks, they tend to be over-confident in predictions, as the standard Dot
Product Self-Attention (DPSA) can barely preserve distance for the unbounded
input domain. In this work, we fill this gap by proposing a novel Lipschitz
Regularized Transformer (LRFormer). Specifically, we present a new similarity
function with the distance within Banach Space to ensure the Lipschitzness and
also regularize the term by a contractive Lipschitz Bound. The proposed method
is analyzed with a theoretical guarantee, providing a rigorous basis for its
effectiveness and reliability. Extensive experiments conducted on standard
vision benchmarks demonstrate that our method outperforms the state-of-the-art
single forward pass approaches in prediction, calibration, and uncertainty
estimation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.07615">UOD: Universal One-shot Detection of Anatomical Landmarks. (arXiv:2306.07615v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1">Heqin Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Quan_Q/0/1/0/all/0/1">Quan Quan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1">Qingsong Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zaiyi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1">S. Kevin Zhou</a></p>
<p>One-shot medical landmark detection gains much attention and achieves great
success for its label-efficient training process. However, existing one-shot
learning methods are highly specialized in a single domain and suffer domain
preference heavily in the situation of multi-domain unlabeled data. Moreover,
one-shot learning is not robust that it faces performance drop when annotating
a sub-optimal image. To tackle these issues, we resort to developing a
domain-adaptive one-shot landmark detection framework for handling multi-domain
medical images, named Universal One-shot Detection (UOD). UOD consists of two
stages and two corresponding universal models which are designed as
combinations of domain-specific modules and domain-shared modules. In the first
stage, a domain-adaptive convolution model is self-supervised learned to
generate pseudo landmark labels. In the second stage, we design a
domain-adaptive transformer to eliminate domain preference and build the global
context for multi-domain data. Even though only one annotated sample from each
domain is available for training, the domain-shared modules help UOD aggregate
all one-shot samples to detect more robust and accurate landmarks. We
investigated both qualitatively and quantitatively the proposed UOD on three
widely-used public X-ray datasets in different anatomical domains (i.e., head,
hand, chest) and obtained state-of-the-art performances in each domain. The
code is available at
https://github.com/heqin-zhu/UOD_universal_oneshot_detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.13074">Iterative Scale-Up ExpansionIoU and Deep Features Association for Multi-Object Tracking in Sports. (arXiv:2306.13074v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Hsiang-Wei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Cheng-Yen Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jiacheng Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1">Jenq-Neng Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1">Chung-I Huang</a></p>
<p>Multi-object tracking algorithms have made significant advancements due to
the recent developments in object detection. However, most existing methods
primarily focus on tracking pedestrians or vehicles, which exhibit relatively
simple and regular motion patterns. Consequently, there is a scarcity of
algorithms that address the tracking of targets with irregular or non-linear
motion, such as multi-athlete tracking. Furthermore, popular tracking
algorithms often rely on the Kalman filter for object motion modeling, which
fails to track objects when their motion contradicts the linear motion
assumption of the Kalman filter. Due to this reason, we proposed a novel online
and robust multi-object tracking approach, named Iterative Scale-Up
ExpansionIoU and Deep Features for multi-object tracking. Unlike conventional
methods, we abandon the use of the Kalman filter and propose utilizing the
iterative scale-up expansion IoU. This approach achieves superior tracking
performance without requiring additional training data or adopting a more
robust detector, all while maintaining a lower computational cost compared to
other appearance-based methods. Our proposed method demonstrates remarkable
effectiveness in tracking irregular motion objects, achieving a score of 76.9%
in HOTA. It outperforms all state-of-the-art tracking algorithms on the
SportsMOT dataset, covering various kinds of sport scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.15548">Geometric Ultrasound Localization Microscopy. (arXiv:2306.15548v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hahne_C/0/1/0/all/0/1">Christopher Hahne</a>, <a href="http://arxiv.org/find/cs/1/au:+Sznitman_R/0/1/0/all/0/1">Raphael Sznitman</a></p>
<p>Contrast-Enhanced Ultra-Sound (CEUS) has become a viable method for
non-invasive, dynamic visualization in medical diagnostics, yet Ultrasound
Localization Microscopy (ULM) has enabled a revolutionary breakthrough by
offering ten times higher resolution. To date, Delay-And-Sum (DAS) beamformers
are used to render ULM frames, ultimately determining the image resolution
capability. To take full advantage of ULM, this study questions whether
beamforming is the most effective processing step for ULM, suggesting an
alternative approach that relies solely on Time-Difference-of-Arrival (TDoA)
information. To this end, a novel geometric framework for micro bubble
localization via ellipse intersections is proposed to overcome existing
beamforming limitations. We present a benchmark comparison based on a public
dataset for which our geometric ULM outperforms existing baseline methods in
terms of accuracy and robustness while only utilizing a portion of the
available transducer data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01740">Synchronous Image-Label Diffusion Probability Model with Application to Stroke Lesion Segmentation on Non-contrast CT. (arXiv:2307.01740v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jianhai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_T/0/1/0/all/0/1">Tonghua Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+MacDonald_E/0/1/0/all/0/1">Ethan MacDonald</a>, <a href="http://arxiv.org/find/cs/1/au:+Menon_B/0/1/0/all/0/1">Bijoy Menon</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganesh_A/0/1/0/all/0/1">Aravind Ganesh</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1">Qiu Wu</a></p>
<p>Stroke lesion volume is a key radiologic measurement for assessing the
prognosis of Acute Ischemic Stroke (AIS) patients, which is challenging to be
automatically measured on Non-Contrast CT (NCCT) scans. Recent diffusion
probabilistic models have shown potentials of being used for image
segmentation. In this paper, a novel Synchronous image-label Diffusion
Probability Model (SDPM) is proposed for stroke lesion segmentation on NCCT
using Markov diffusion process. The proposed SDPM is fully based on a Latent
Variable Model (LVM), offering a complete probabilistic elaboration. An
additional net-stream, parallel with a noise prediction stream, is introduced
to obtain initial noisy label estimates for efficiently inferring the final
labels. By optimizing the specified variational boundaries, the trained model
can infer multiple label estimates for reference given the input images with
noises. The proposed model was assessed on three stroke lesion datasets
including one public and two private datasets. Compared to several U-net and
transformer-based segmentation methods, our proposed SDPM model is able to
achieve state-of-the-art performance. The code is publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.02347">Detecting Images Generated by Deep Diffusion Models using their Local Intrinsic Dimensionality. (arXiv:2307.02347v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lorenz_P/0/1/0/all/0/1">Peter Lorenz</a>, <a href="http://arxiv.org/find/cs/1/au:+Durall_R/0/1/0/all/0/1">Ricard Durall</a>, <a href="http://arxiv.org/find/cs/1/au:+Keuper_J/0/1/0/all/0/1">Janis Keuper</a></p>
<p>Diffusion models recently have been successfully applied for the visual
synthesis of strikingly realistic appearing images. This raises strong concerns
about their potential for malicious purposes. In this paper, we propose using
the lightweight multi Local Intrinsic Dimensionality (multiLID), which has been
originally developed in context of the detection of adversarial examples, for
the automatic detection of synthetic images and the identification of the
according generator networks. In contrast to many existing detection
approaches, which often only work for GAN-generated images, the proposed method
provides close to perfect detection results in many realistic use cases.
Extensive experiments on known and newly created datasets demonstrate that the
proposed multiLID approach exhibits superiority in diffusion detection and
model identification. Since the empirical evaluations of recent publications on
the detection of generated images are often mainly focused on the
"LSUN-Bedroom" dataset, we further establish a comprehensive benchmark for the
detection of diffusion-generated images, including samples from several
diffusion models with different image sizes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07250">Mitigating Adversarial Vulnerability through Causal Parameter Estimation by Adversarial Double Machine Learning. (arXiv:2307.07250v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1">Byung-Kwan Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Junho Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Ro_Y/0/1/0/all/0/1">Yong Man Ro</a></p>
<p>Adversarial examples derived from deliberately crafted perturbations on
visual inputs can easily harm decision process of deep neural networks. To
prevent potential threats, various adversarial training-based defense methods
have grown rapidly and become a de facto standard approach for robustness.
Despite recent competitive achievements, we observe that adversarial
vulnerability varies across targets and certain vulnerabilities remain
prevalent. Intriguingly, such peculiar phenomenon cannot be relieved even with
deeper architectures and advanced defense methods. To address this issue, in
this paper, we introduce a causal approach called Adversarial Double Machine
Learning (ADML), which allows us to quantify the degree of adversarial
vulnerability for network predictions and capture the effect of treatments on
outcome of interests. ADML can directly estimate causal parameter of
adversarial perturbations per se and mitigate negative effects that can
potentially damage robustness, bridging a causal perspective into the
adversarial vulnerability. Through extensive experiments on various CNN and
Transformer architectures, we corroborate that ADML improves adversarial
robustness with large margins and relieve the empirical observation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07483">Multimodal Distillation for Egocentric Action Recognition. (arXiv:2307.07483v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Radevski_G/0/1/0/all/0/1">Gorjan Radevski</a>, <a href="http://arxiv.org/find/cs/1/au:+Grujicic_D/0/1/0/all/0/1">Dusan Grujicic</a>, <a href="http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1">Marie-Francine Moens</a>, <a href="http://arxiv.org/find/cs/1/au:+Blaschko_M/0/1/0/all/0/1">Matthew Blaschko</a>, <a href="http://arxiv.org/find/cs/1/au:+Tuytelaars_T/0/1/0/all/0/1">Tinne Tuytelaars</a></p>
<p>The focal point of egocentric video understanding is modelling hand-object
interactions. Standard models, e.g. CNNs or Vision Transformers, which receive
RGB frames as input perform well. However, their performance improves further
by employing additional input modalities that provide complementary cues, such
as object detections, optical flow, audio, etc. The added complexity of the
modality-specific modules, on the other hand, makes these models impractical
for deployment. The goal of this work is to retain the performance of such a
multimodal approach, while using only the RGB frames as input at inference
time. We demonstrate that for egocentric action recognition on the
Epic-Kitchens and the Something-Something datasets, students which are taught
by multimodal teachers tend to be more accurate and better calibrated than
architecturally equivalent models trained on ground truth labels in a unimodal
or multimodal fashion. We further adopt a principled multimodal knowledge
distillation framework, allowing us to deal with issues which occur when
applying multimodal knowledge distillation in a naive manner. Lastly, we
demonstrate the achieved reduction in computational complexity, and show that
our approach maintains higher performance with the reduction of the number of
input views. We release our code at
https://github.com/gorjanradevski/multimodal-distillation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07754">Bidirectionally Deformable Motion Modulation For Video-based Human Pose Transfer. (arXiv:2307.07754v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1">Wing-Yin Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Po_L/0/1/0/all/0/1">Lai-Man Po</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheung_R/0/1/0/all/0/1">Ray C.C. Cheung</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yuzhi Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1">Yu Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Kun Li</a></p>
<p>Video-based human pose transfer is a video-to-video generation task that
animates a plain source human image based on a series of target human poses.
Considering the difficulties in transferring highly structural patterns on the
garments and discontinuous poses, existing methods often generate
unsatisfactory results such as distorted textures and flickering artifacts. To
address these issues, we propose a novel Deformable Motion Modulation (DMM)
that utilizes geometric kernel offset with adaptive weight modulation to
simultaneously perform feature alignment and style transfer. Different from
normal style modulation used in style transfer, the proposed modulation
mechanism adaptively reconstructs smoothed frames from style codes according to
the object shape through an irregular receptive field of view. To enhance the
spatio-temporal consistency, we leverage bidirectional propagation to extract
the hidden motion information from a warped image sequence generated by noisy
poses. The proposed feature propagation significantly enhances the motion
prediction ability by forward and backward propagation. Both quantitative and
qualitative experimental results demonstrate superiority over the
state-of-the-arts in terms of image fidelity and visual continuity. The source
code is publicly available at github.com/rocketappslab/bdmm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07813">Ultra-Fast and Ultra-Low-Power In-Sensor Edge Vision for Gaze Estimation. (arXiv:2307.07813v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bonazzi_P/0/1/0/all/0/1">Pietro Bonazzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruegg_T/0/1/0/all/0/1">Thomas Ruegg</a>, <a href="http://arxiv.org/find/cs/1/au:+Bian_S/0/1/0/all/0/1">Sizhen Bian</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yawei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Magno_M/0/1/0/all/0/1">Michele Magno</a></p>
<p>Intelligent edge vision tasks encounter the critical challenge of ensuring
power and latency efficiency due to the typically heavy computational load they
impose on edge platforms.This work leverages one of the first "AI in sensor"
vision platforms, IMX500 by Sony, to achieve ultra-fast and ultra-low-power
end-to-end edge vision applications. We evaluate the IMX500 and compare it to
other edge platforms, such as the Google Coral Dev Micro and Sony Spresense, by
exploring gaze estimation as a case study. We propose TinyTracker, a highly
efficient, fully quantized model for 2D gaze estimation designed to maximize
the performance of the edge vision systems considered in this study.
TinyTracker achieves a 41x size reduction (600Kb) compared to iTracker [1]
without significant loss in gaze estimation accuracy (maximum of 0.16 cm when
fully quantized). TinyTracker's deployment on the Sony IMX500 vision sensor
results in end-to-end latency of around 19ms. The camera takes around 17.9ms to
read, process and transmit the pixels to the accelerator. The inference time of
the network is 0.86ms with an additional 0.24 ms for retrieving the results
from the sensor. The overall energy consumption of the end-to-end system is 4.9
mJ, including 0.06 mJ for inference. The end-to-end study shows that IMX500 is
1.7x faster than CoralMicro (19ms vs 34.4ms) and 7x more power efficient (4.9mJ
VS 34.2mJ)
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07916">On the Robustness of Split Learning against Adversarial Attacks. (arXiv:2307.07916v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1">Mingyuan Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Cen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chengyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1">Wenmeng Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jun Huang</a></p>
<p>Split learning enables collaborative deep learning model training while
preserving data privacy and model security by avoiding direct sharing of raw
data and model details (i.e., sever and clients only hold partial sub-networks
and exchange intermediate computations). However, existing research has mainly
focused on examining its reliability for privacy protection, with little
investigation into model security. Specifically, by exploring full models,
attackers can launch adversarial attacks, and split learning can mitigate this
severe threat by only disclosing part of models to untrusted servers.This paper
aims to evaluate the robustness of split learning against adversarial attacks,
particularly in the most challenging setting where untrusted servers only have
access to the intermediate layers of the model.Existing adversarial attacks
mostly focus on the centralized setting instead of the collaborative setting,
thus, to better evaluate the robustness of split learning, we develop a
tailored attack called SPADV, which comprises two stages: 1) shadow model
training that addresses the issue of lacking part of the model and 2) local
adversarial attack that produces adversarial examples to evaluate.The first
stage only requires a few unlabeled non-IID data, and, in the second stage,
SPADV perturbs the intermediate output of natural samples to craft the
adversarial ones. The overall cost of the proposed attack process is relatively
low, yet the empirical attack effectiveness is significantly high,
demonstrating the surprising vulnerability of split learning to adversarial
attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08397">CLIP-Guided StyleGAN Inversion for Text-Driven Real Image Editing. (arXiv:2307.08397v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Baykal_A/0/1/0/all/0/1">Ahmet Canberk Baykal</a>, <a href="http://arxiv.org/find/cs/1/au:+Anees_A/0/1/0/all/0/1">Abdul Basit Anees</a>, <a href="http://arxiv.org/find/cs/1/au:+Ceylan_D/0/1/0/all/0/1">Duygu Ceylan</a>, <a href="http://arxiv.org/find/cs/1/au:+Erdem_E/0/1/0/all/0/1">Erkut Erdem</a>, <a href="http://arxiv.org/find/cs/1/au:+Erdem_A/0/1/0/all/0/1">Aykut Erdem</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuret_D/0/1/0/all/0/1">Deniz Yuret</a></p>
<p>Researchers have recently begun exploring the use of StyleGAN-based models
for real image editing. One particularly interesting application is using
natural language descriptions to guide the editing process. Existing approaches
for editing images using language either resort to instance-level latent code
optimization or map predefined text prompts to some editing directions in the
latent space. However, these approaches have inherent limitations. The former
is not very efficient, while the latter often struggles to effectively handle
multi-attribute changes. To address these weaknesses, we present CLIPInverter,
a new text-driven image editing approach that is able to efficiently and
reliably perform multi-attribute changes. The core of our method is the use of
novel, lightweight text-conditioned adapter layers integrated into pretrained
GAN-inversion networks. We demonstrate that by conditioning the initial
inversion step on the CLIP embedding of the target description, we are able to
obtain more successful edit directions. Additionally, we use a CLIP-guided
refinement step to make corrections in the resulting residual latent codes,
which further improves the alignment with the text prompt. Our method
outperforms competing approaches in terms of manipulation accuracy and
photo-realism on various domains including human faces, cats, and birds, as
shown by our qualitative and quantitative results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08466">Generalizable Classification of UHF Partial Discharge Signals in Gas-Insulated HVDC Systems Using Neural Networks. (arXiv:2307.08466v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Seitz_S/0/1/0/all/0/1">Steffen Seitz</a>, <a href="http://arxiv.org/find/cs/1/au:+Gotz_T/0/1/0/all/0/1">Thomas G&#xf6;tz</a>, <a href="http://arxiv.org/find/cs/1/au:+Lindenberg_C/0/1/0/all/0/1">Christopher Lindenberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Tetzlaff_R/0/1/0/all/0/1">Ronald Tetzlaff</a>, <a href="http://arxiv.org/find/cs/1/au:+Schlegel_S/0/1/0/all/0/1">Stephan Schlegel</a></p>
<p>Undetected partial discharges (PDs) are a safety critical issue in high
voltage (HV) gas insulated systems (GIS). While the diagnosis of PDs under AC
voltage is well-established, the analysis of PDs under DC voltage remains an
active research field. A key focus of these investigations is the
classification of different PD sources to enable subsequent sophisticated
analysis.
</p>
<p>In this paper, we propose and analyze a neural network-based approach for
classifying PD signals caused by metallic protrusions and conductive particles
on the insulator of HVDC GIS, without relying on pulse sequence analysis
features. In contrast to previous approaches, our proposed model can
discriminate the studied PD signals obtained at negative and positive
potentials, while also generalizing to unseen operating voltage multiples.
Additionally, we compare the performance of time- and frequency-domain input
signals and explore the impact of different normalization schemes to mitigate
the influence of free-space path loss between the sensor and defect location.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08535">Multi-class point cloud completion networks for 3D cardiac anatomy reconstruction from cine magnetic resonance images. (arXiv:2307.08535v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Beetz_M/0/1/0/all/0/1">Marcel Beetz</a>, <a href="http://arxiv.org/find/eess/1/au:+Banerjee_A/0/1/0/all/0/1">Abhirup Banerjee</a>, <a href="http://arxiv.org/find/eess/1/au:+Ossenberg_Engels_J/0/1/0/all/0/1">Julius Ossenberg-Engels</a>, <a href="http://arxiv.org/find/eess/1/au:+Grau_V/0/1/0/all/0/1">Vicente Grau</a></p>
<p>Cine magnetic resonance imaging (MRI) is the current gold standard for the
assessment of cardiac anatomy and function. However, it typically only acquires
a set of two-dimensional (2D) slices of the underlying three-dimensional (3D)
anatomy of the heart, thus limiting the understanding and analysis of both
healthy and pathological cardiac morphology and physiology. In this paper, we
propose a novel fully automatic surface reconstruction pipeline capable of
reconstructing multi-class 3D cardiac anatomy meshes from raw cine MRI
acquisitions. Its key component is a multi-class point cloud completion network
(PCCN) capable of correcting both the sparsity and misalignment issues of the
3D reconstruction task in a unified model. We first evaluate the PCCN on a
large synthetic dataset of biventricular anatomies and observe Chamfer
distances between reconstructed and gold standard anatomies below or similar to
the underlying image resolution for multiple levels of slice misalignment.
Furthermore, we find a reduction in reconstruction error compared to a
benchmark 3D U-Net by 32% and 24% in terms of Hausdorff distance and mean
surface distance, respectively. We then apply the PCCN as part of our automated
reconstruction pipeline to 1000 subjects from the UK Biobank study in a
cross-domain transfer setting and demonstrate its ability to reconstruct
accurate and topologically plausible biventricular heart meshes with clinical
metrics comparable to the previous literature. Finally, we investigate the
robustness of our proposed approach and observe its capacity to successfully
handle multiple common outlier conditions.
</p>
</p>
</div>

    </div>
    </body>
    