<!DOCTYPE html>
<html>
<head>
<title>2024-01-05-cs-lg</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2401.01342">Securing the Digital World: Protecting smart infrastructures and digital industries with Artificial Intelligence (AI)-enabled malware and intrusion detection. (arXiv:2401.01342v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schmitt_M/0/1/0/all/0/1">Marc Schmitt</a></p>
<p>The last decades have been characterized by unprecedented technological
advances, many of them powered by modern technologies such as Artificial
Intelligence (AI) and Machine Learning (ML). The world has become more
digitally connected than ever, but we face major challenges. One of the most
significant is cybercrime, which has emerged as a global threat to governments,
businesses, and civil societies. The pervasiveness of digital technologies
combined with a constantly shifting technological foundation has created a
complex and powerful playground for cybercriminals, which triggered a surge in
demand for intelligent threat detection systems based on machine and deep
learning. This paper investigates AI-based cyber threat detection to protect
our modern digital ecosystems. The primary focus is on evaluating ML-based
classifiers and ensembles for anomaly-based malware detection and network
intrusion detection and how to integrate those models in the context of network
security, mobile security, and IoT security. The discussion highlights the
challenges when deploying and integrating AI-enabled cybersecurity solutions
into existing enterprise systems and IT infrastructures, including options to
overcome those challenges. Finally, the paper provides future research
directions to further increase the security and resilience of our modern
digital industries, infrastructures, and ecosystems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01343">IoTGeM: Generalizable Models for Behaviour-Based IoT Attack Detection. (arXiv:2401.01343v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kostas_K/0/1/0/all/0/1">Kahraman Kostas</a>, <a href="http://arxiv.org/find/cs/1/au:+Just_M/0/1/0/all/0/1">Mike Just</a>, <a href="http://arxiv.org/find/cs/1/au:+Lones_M/0/1/0/all/0/1">Michael A. Lones</a></p>
<p>Previous research on behaviour-based attack detection on networks of IoT
devices has resulted in machine learning models whose ability to adapt to
unseen data is limited, and often not demonstrated. In this paper we present an
approach for modelling IoT network attacks that focuses on generalizability,
yet also leads to better detection and performance. First, we present an
improved rolling window approach for feature extraction, and introduce a
multi-step feature selection process that reduces overfitting. Second, we build
and test models using isolated train and test datasets, thereby avoiding common
data leaks that have limited the generalizability of previous models. Third, we
rigorously evaluate our methodology using a diverse portfolio of machine
learning models, evaluation metrics and datasets. Finally, we build confidence
in the models by using explainable AI techniques, allowing us to identify the
features that underlie accurate detection of attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01361">Optimizing Convolutional Neural Network Architecture. (arXiv:2401.01361v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Balderas_L/0/1/0/all/0/1">Luis Balderas</a>, <a href="http://arxiv.org/find/cs/1/au:+Lastra_M/0/1/0/all/0/1">Miguel Lastra</a>, <a href="http://arxiv.org/find/cs/1/au:+Benitez_J/0/1/0/all/0/1">Jos&#xe9; M. Ben&#xed;tez</a></p>
<p>Convolutional Neural Networks (CNN) are widely used to face challenging tasks
like speech recognition, natural language processing or computer vision. As CNN
architectures get larger and more complex, their computational requirements
increase, incurring significant energetic costs and challenging their
deployment on resource-restricted devices. In this paper, we propose Optimizing
Convolutional Neural Network Architecture (OCNNA), a novel CNN optimization and
construction method based on pruning and knowledge distillation designed to
establish the importance of convolutional layers. The proposal has been
evaluated though a thorough empirical study including the best known datasets
(CIFAR-10, CIFAR-100 and Imagenet) and CNN architectures (VGG-16, ResNet-50,
DenseNet-40 and MobileNet), setting Accuracy Drop and Remaining Parameters
Ratio as objective metrics to compare the performance of OCNNA against the
other state-of-art approaches. Our method has been compared with more than 20
convolutional neural network simplification algorithms obtaining outstanding
results. As a result, OCNNA is a competitive CNN constructing method which
could ease the deployment of neural networks into IoT or resource-limited
devices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01364">Multi-Modal Cognitive Maps based on Neural Networks trained on Successor Representations. (arXiv:2401.01364v1 [q-bio.NC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Stoewer_P/0/1/0/all/0/1">Paul Stoewer</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Schilling_A/0/1/0/all/0/1">Achim Schilling</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Maier_A/0/1/0/all/0/1">Andreas Maier</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Krauss_P/0/1/0/all/0/1">Patrick Krauss</a></p>
<p>Cognitive maps are a proposed concept on how the brain efficiently organizes
memories and retrieves context out of them. The entorhinal-hippocampal complex
is heavily involved in episodic and relational memory processing, as well as
spatial navigation and is thought to built cognitive maps via place and grid
cells. To make use of the promising properties of cognitive maps, we set up a
multi-modal neural network using successor representations which is able to
model place cell dynamics and cognitive map representations. Here, we use
multi-modal inputs consisting of images and word embeddings. The network learns
the similarities between novel inputs and the training database and therefore
the representation of the cognitive map successfully. Subsequently, the
prediction of the network can be used to infer from one modality to another
with over $90\%$ accuracy. The proposed method could therefore be a building
block to improve current AI systems for better understanding of the environment
and the different modalities in which objects appear. The association of
specific modalities with certain encounters can therefore lead to context
awareness in novel situations when similar encounters with less information
occur and additional information can be inferred from the learned cognitive
map. Cognitive maps, as represented by the entorhinal-hippocampal complex in
the brain, organize and retrieve context from memories, suggesting that large
language models (LLMs) like ChatGPT could harness similar architectures to
function as a high-level processing center, akin to how the hippocampus
operates within the cortex hierarchy. Finally, by utilizing multi-modal inputs,
LLMs can potentially bridge the gap between different forms of data (like
images and words), paving the way for context-awareness and grounding of
abstract concepts through learned associations, addressing the grounding
problem in AI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01369">RL-MPCA: A Reinforcement Learning Based Multi-Phase Computation Allocation Approach for Recommender Systems. (arXiv:2401.01369v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jiahong Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1">Shunhui Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1">Guoliang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1">Bo Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1">Qianlong Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1">Lebin Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xingxing Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dong Wang</a></p>
<p>Recommender systems aim to recommend the most suitable items to users from a
large number of candidates. Their computation cost grows as the number of user
requests and the complexity of services (or models) increases. Under the
limitation of computation resources (CRs), how to make a trade-off between
computation cost and business revenue becomes an essential question. The
existing studies focus on dynamically allocating CRs in queue truncation
scenarios (i.e., allocating the size of candidates), and formulate the CR
allocation problem as an optimization problem with constraints. Some of them
focus on single-phase CR allocation, and others focus on multi-phase CR
allocation but introduce some assumptions about queue truncation scenarios.
However, these assumptions do not hold in other scenarios, such as retrieval
channel selection and prediction model selection. Moreover, existing studies
ignore the state transition process of requests between different phases,
limiting the effectiveness of their approaches.
</p>
<p>This paper proposes a Reinforcement Learning (RL) based Multi-Phase
Computation Allocation approach (RL-MPCA), which aims to maximize the total
business revenue under the limitation of CRs. RL-MPCA formulates the CR
allocation problem as a Weakly Coupled MDP problem and solves it with an
RL-based approach. Specifically, RL-MPCA designs a novel deep Q-network to
adapt to various CR allocation scenarios, and calibrates the Q-value by
introducing multiple adaptive Lagrange multipliers (adaptive-$\lambda$) to
avoid violating the global CR constraints. Finally, experiments on the offline
simulation environment and online real-world recommender system validate the
effectiveness of our approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01373">Boosting Defect Detection in Manufacturing using Tensor Convolutional Neural Networks. (arXiv:2401.01373v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Martin_Ramiro_P/0/1/0/all/0/1">Pablo Martin-Ramiro</a>, <a href="http://arxiv.org/find/cs/1/au:+Maza_U/0/1/0/all/0/1">Unai Sainz de la Maza</a>, <a href="http://arxiv.org/find/cs/1/au:+Orus_R/0/1/0/all/0/1">Roman Orus</a>, <a href="http://arxiv.org/find/cs/1/au:+Mugel_S/0/1/0/all/0/1">Samuel Mugel</a></p>
<p>Defect detection is one of the most important yet challenging tasks in the
quality control stage in the manufacturing sector. In this work, we introduce a
Tensor Convolutional Neural Network (T-CNN) and examine its performance on a
real defect detection application in one of the components of the ultrasonic
sensors produced at Robert Bosch's manufacturing plants. Our quantum-inspired
T-CNN operates on a reduced model parameter space to substantially improve the
training speed and performance of an equivalent CNN model without sacrificing
accuracy. More specifically, we demonstrate how T-CNNs are able to reach the
same performance as classical CNNs as measured by quality metrics, with up to
fifteen times fewer parameters and 4% to 19% faster training times. Our results
demonstrate that the T-CNN greatly outperforms the results of traditional human
visual inspection, providing value in a current real application in
manufacturing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01375">Mapping Walnut water Stress with High Resolution Multispectral UAV Imagery and Machine Learning. (arXiv:2401.01375v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kaitlyn Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1">Yufang Jin</a></p>
<p>Effective monitoring of walnut water status and stress level across the whole
orchard is an essential step towards precision irrigation management of
walnuts, a significant crop in California. This study presents a machine
learning approach using Random Forest (RF) models to map stem water potential
(SWP) by integrating high-resolution multispectral remote sensing imagery from
Unmanned Aerial Vehicle (UAV) flights with weather data. From 2017 to 2018,
five flights of an UAV equipped with a seven-band multispectral camera were
conducted over a commercial walnut orchard, paired with concurrent ground
measurements of sampled walnut plants. The RF regression model, utilizing
vegetation indices derived from orthomosaiced UAV imagery and weather data,
effectively estimated ground-measured SWPs, achieving an $R^2$ of 0.63 and a
mean absolute error (MAE) of 0.80 bars. The integration of weather data was
particularly crucial for consolidating data across various flight dates.
Significant variables for SWP estimation included wind speed and vegetation
indices such as NDVI, NDRE, and PSRI.A reduced RF model excluding red-edge
indices of NDRE and PSRI, demonstrated slightly reduced accuracy ($R^2$ =
0.54). Additionally, the RF classification model predicted water stress levels
in walnut trees with 85% accuracy, surpassing the 80% accuracy of the reduced
classification model. The results affirm the efficacy of UAV-based
multispectral imaging combined with machine learning, incorporating thermal
data, NDVI, red-edge indices, and weather data, in walnut water stress
estimation and assessment. This methodology offers a scalable, cost-effective
tool for data-driven precision irrigation management at an individual plant
level in walnut orchards.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01383">Predicting Infant Brain Connectivity with Federated Multi-Trajectory GNNs using Scarce Data. (arXiv:2401.01383v1 [q-bio.NC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Pistos_M/0/1/0/all/0/1">Michalis Pistos</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Rekik_I/0/1/0/all/0/1">Islem Rekik</a></p>
<p>The understanding of the convoluted evolution of infant brain networks during
the first postnatal year is pivotal for identifying the dynamics of early brain
connectivity development. Existing deep learning solutions suffer from three
major limitations. First, they cannot generalize to multi-trajectory prediction
tasks, where each graph trajectory corresponds to a particular imaging modality
or connectivity type (e.g., T1-w MRI). Second, existing models require
extensive training datasets to achieve satisfactory performance which are often
challenging to obtain. Third, they do not efficiently utilize incomplete time
series data. To address these limitations, we introduce FedGmTE-Net++, a
federated graph-based multi-trajectory evolution network. Using the power of
federation, we aggregate local learnings among diverse hospitals with limited
datasets. As a result, we enhance the performance of each hospital's local
generative model, while preserving data privacy. The three key innovations of
FedGmTE-Net++ are: (i) presenting the first federated learning framework
specifically designed for brain multi-trajectory evolution prediction in a
data-scarce environment, (ii) incorporating an auxiliary regularizer in the
local objective function to exploit all the longitudinal brain connectivity
within the evolution trajectory and maximize data utilization, (iii)
introducing a two-step imputation process, comprising a preliminary KNN-based
precompletion followed by an imputation refinement step that employs regressors
to improve similarity scores and refine imputations. Our comprehensive
experimental results showed the outperformance of FedGmTE-Net++ in brain
multi-trajectory prediction from a single baseline graph in comparison with
benchmark methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01384">Strong Transitivity Relations and Graph Neural Networks. (arXiv:2401.01384v1 [cs.SI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mohamadi_Y/0/1/0/all/0/1">Yassin Mohamadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Chehreghani_M/0/1/0/all/0/1">Mostafa Haghir Chehreghani</a></p>
<p>Local neighborhoods play a crucial role in embedding generation in
graph-based learning. It is commonly believed that nodes ought to have
embeddings that resemble those of their neighbors. In this research, we try to
carefully expand the concept of similarity from nearby neighborhoods to the
entire graph. We provide an extension of similarity that is based on
transitivity relations, which enables Graph Neural Networks (GNNs) to capture
both global similarities and local similarities over the whole graph. We
introduce Transitivity Graph Neural Network (TransGNN), which more than local
node similarities, takes into account global similarities by distinguishing
strong transitivity relations from weak ones and exploiting them. We evaluate
our model over several real-world datasets and showed that it considerably
improves the performance of several well-known GNN models, for tasks such as
node classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01386">Tissue Artifact Segmentation and Severity Analysis for Automated Diagnosis Using Whole Slide Images. (arXiv:2401.01386v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Himel_G/0/1/0/all/0/1">Galib Muhammad Shahriar Himel</a></p>
<p>Traditionally, pathological analysis and diagnosis are performed by manually
eyeballing glass slide specimens under a microscope by an expert. The whole
slide image is the digital specimen produced from the glass slide. Whole slide
image enabled specimens to be observed on a computer screen and led to
computational pathology where computer vision and artificial intelligence are
utilized for automated analysis and diagnosis. With the current computational
advancement, the entire whole slide image can be analyzed autonomously without
human supervision. However, the analysis could fail or lead to wrong diagnosis
if the whole slide image is affected by tissue artifacts such as tissue fold or
air bubbles depending on the severity. Existing artifact detection methods rely
on experts for severity assessment to eliminate artifact affected regions from
the analysis. This process is time consuming, exhausting and undermines the
goal of automated analysis or removal of artifacts without evaluating their
severity, which could result in the loss of diagnostically important data.
Therefore, it is necessary to detect artifacts and then assess their severity
automatically. In this paper, we propose a system that incorporates severity
evaluation with artifact detection utilizing convolutional neural networks. The
proposed system uses DoubleUNet to segment artifacts and an ensemble network of
six fine tuned convolutional neural network models to determine severity. This
method outperformed current state of the art in accuracy by 9 percent for
artifact segmentation and achieved a strong correlation of 97 percent with the
evaluation of pathologists for severity assessment. The robustness of the
system was demonstrated using our proposed heterogeneous dataset and practical
usability was ensured by integrating it with an automated analysis system.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01388">Directional Antenna Systems for Long-Range Through-Wall Human Activity Recognition. (arXiv:2401.01388v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Strohmayer_J/0/1/0/all/0/1">Julian Strohmayer</a>, <a href="http://arxiv.org/find/cs/1/au:+Kampel_M/0/1/0/all/0/1">Martin Kampel</a></p>
<p>WiFi Channel State Information (CSI)-based human activity recognition (HAR)
enables contactless, long-range sensing in spatially constrained environments
while preserving visual privacy. However, despite the presence of numerous
WiFi-enabled devices around us, few expose CSI to users, resulting in a lack of
sensing hardware options. Variants of the Espressif ESP32 have emerged as
potential low-cost and easy-to-deploy solutions for WiFi CSI-based HAR. In this
work, four ESP32-S3-based 2.4GHz directional antenna systems are evaluated for
their ability to facilitate long-range through-wall HAR. Two promising systems
are proposed, one of which combines the ESP32-S3 with a directional biquad
antenna. This combination represents, to the best of our knowledge, the first
demonstration of such a system in WiFi-based HAR. The second system relies on
the built-in printed inverted-F antenna (PIFA) of the ESP32-S3 and achieves
directionality through a plane reflector. In a comprehensive evaluation of
line-of-sight (LOS) and non-line-of-sight (NLOS) HAR performance, both systems
are deployed in an office environment spanning a distance of 18 meters across
five rooms. In this experimental setup, the Wallhack1.8k dataset, comprising
1806 CSI amplitude spectrograms of human activities, is collected and made
publicly available. Based on Wallhack1.8k, we train activity recognition models
using the EfficientNetV2 architecture to assess system performance in LOS and
NLOS scenarios. For the core NLOS activity recognition problem, the biquad
antenna and PIFA-based systems achieve accuracies of 92.0$\pm$3.5 and
86.8$\pm$4.7, respectively, demonstrating the feasibility of long-range
through-wall HAR with the proposed systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01391">On Optimal Sampling for Learning SDF Using MLPs Equipped with Positional Encoding. (arXiv:2401.01391v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1">Guying Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Lei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Congyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1">Junhui Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1">Xiaogang Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Komura_T/0/1/0/all/0/1">Taku Komura</a>, <a href="http://arxiv.org/find/cs/1/au:+Keyser_J/0/1/0/all/0/1">John Keyser</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenping Wang</a></p>
<p>Neural implicit fields, such as the neural signed distance field (SDF) of a
shape, have emerged as a powerful representation for many applications, e.g.,
encoding a 3D shape and performing collision detection. Typically, implicit
fields are encoded by Multi-layer Perceptrons (MLP) with positional encoding
(PE) to capture high-frequency geometric details. However, a notable side
effect of such PE-equipped MLPs is the noisy artifacts present in the learned
implicit fields. While increasing the sampling rate could in general mitigate
these artifacts, in this paper we aim to explain this adverse phenomenon
through the lens of Fourier analysis. We devise a tool to determine the
appropriate sampling rate for learning an accurate neural implicit field
without undesirable side effects. Specifically, we propose a simple yet
effective method to estimate the intrinsic frequency of a given network with
randomized weights based on the Fourier analysis of the network's responses. It
is observed that a PE-equipped MLP has an intrinsic frequency much higher than
the highest frequency component in the PE layer. Sampling against this
intrinsic frequency following the Nyquist-Sannon sampling theorem allows us to
determine an appropriate training sampling rate. We empirically show in the
setting of SDF fitting that this recommended sampling rate is sufficient to
secure accurate fitting results, while further increasing the sampling rate
would not further noticeably reduce the fitting error. Training PE-equipped
MLPs simply with our sampling strategy leads to performances superior to the
existing methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01393">Backtracking New Q-Newton&#x27;s method, Newton&#x27;s flow, Voronoi&#x27;s diagram and Stochastic root finding. (arXiv:2401.01393v1 [math.OC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Fornaess_J/0/1/0/all/0/1">John Erik Fornaess</a>, <a href="http://arxiv.org/find/math/1/au:+Hu_M/0/1/0/all/0/1">Mi Hu</a>, <a href="http://arxiv.org/find/math/1/au:+Truong_T/0/1/0/all/0/1">Tuyen Trung Truong</a>, <a href="http://arxiv.org/find/math/1/au:+Watanabe_T/0/1/0/all/0/1">Takayuki Watanabe</a></p>
<p>A new variant of Newton's method - named Backtracking New Q-Newton's method
(BNQN) - which has strong theoretical guarantee, is easy to implement, and has
good experimental performance, was recently introduced by the third author.
</p>
<p>Experiments performed previously showed some remarkable properties of the
basins of attractions for finding roots of polynomials and meromorphic
functions, with BNQN. In general, they look more smooth than that of Newton's
method.
</p>
<p>In this paper, we continue to experimentally explore in depth this remarkable
phenomenon, and connect BNQN to Newton's flow and Voronoi's diagram. This link
poses a couple of challenging puzzles to be explained. Experiments also
indicate that BNQN is more robust against random perturbations than Newton's
method and Random Relaxed Newton's method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01395">Deep autoregressive modeling for land use land cover. (arXiv:2401.01395v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Krapu_C/0/1/0/all/0/1">Christopher Krapu</a>, <a href="http://arxiv.org/find/cs/1/au:+Borsuk_M/0/1/0/all/0/1">Mark Borsuk</a>, <a href="http://arxiv.org/find/cs/1/au:+Calder_R/0/1/0/all/0/1">Ryan Calder</a></p>
<p>Land use / land cover (LULC) modeling is a challenging task due to long-range
dependencies between geographic features and distinct spatial patterns related
to topography, ecology, and human development. We identify a close connection
between modeling of spatial patterns of land use and the task of image
inpainting from computer vision and conduct a study of a modified PixelCNN
architecture with approximately 19 million parameters for modeling LULC. In
comparison with a benchmark spatial statistical model, we find that the former
is capable of capturing much richer spatial correlation patterns such as roads
and water bodies but does not produce a calibrated predictive distribution,
suggesting the need for additional tuning. We find evidence of predictive
underdispersion with regard to important ecologically-relevant land use
statistics such as patch count and adjacency which can be ameliorated to some
extent by manipulating sampling variability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01398">Accelerating Black-Box Molecular Property Optimization by Adaptively Learning Sparse Subspaces. (arXiv:2401.01398v1 [q-bio.BM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Sorourifar_F/0/1/0/all/0/1">Farshud Sorourifar</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Banker_T/0/1/0/all/0/1">Thomas Banker</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Paulson_J/0/1/0/all/0/1">Joel A. Paulson</a></p>
<p>Molecular property optimization (MPO) problems are inherently challenging
since they are formulated over discrete, unstructured spaces and the labeling
process involves expensive simulations or experiments, which fundamentally
limits the amount of available data. Bayesian optimization (BO) is a powerful
and popular framework for efficient optimization of noisy, black-box objective
functions (e.g., measured property values), thus is a potentially attractive
framework for MPO. To apply BO to MPO problems, one must select a structured
molecular representation that enables construction of a probabilistic surrogate
model. Many molecular representations have been developed, however, they are
all high-dimensional, which introduces important challenges in the BO process
-- mainly because the curse of dimensionality makes it difficult to define and
perform inference over a suitable class of surrogate models. This challenge has
been recently addressed by learning a lower-dimensional encoding of a SMILE or
graph representation of a molecule in an unsupervised manner and then
performing BO in the encoded space. In this work, we show that such methods
have a tendency to "get stuck," which we hypothesize occurs since the mapping
from the encoded space to property values is not necessarily well-modeled by a
Gaussian process. We argue for an alternative approach that combines numerical
molecular descriptors with a sparse axis-aligned Gaussian process model, which
is capable of rapidly identifying sparse subspaces that are most relevant to
modeling the unknown property function. We demonstrate that our proposed method
substantially outperforms existing MPO methods on a variety of benchmark and
real-world problems. Specifically, we show that our method can routinely find
near-optimal molecules out of a set of more than $&gt;100$k alternatives within
100 or fewer expensive queries.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01404">Scalable network reconstruction in subquadratic time. (arXiv:2401.01404v1 [cs.DS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peixoto_T/0/1/0/all/0/1">Tiago P. Peixoto</a></p>
<p>Network reconstruction consists in determining the unobserved pairwise
couplings between $N$ nodes given only observational data on the resulting
behavior that is conditioned on those couplings -- typically a time-series or
independent samples from a graphical model. A major obstacle to the scalability
of algorithms proposed for this problem is a seemingly unavoidable quadratic
complexity of $O(N^2)$, corresponding to the requirement of each possible
pairwise coupling being contemplated at least once, despite the fact that most
networks of interest are sparse, with a number of non-zero couplings that is
only $O(N)$. Here we present a general algorithm applicable to a broad range of
reconstruction problems that achieves its result in subquadratic time, with a
data-dependent complexity loosely upper bounded by $O(N^{3/2}\log N)$, but with
a more typical log-linear complexity of $O(N\log^2N)$. Our algorithm relies on
a stochastic second neighbor search that produces the best edge candidates with
high probability, thus bypassing an exhaustive quadratic search. In practice,
our algorithm achieves a performance that is many orders of magnitude faster
than the quadratic baseline, allows for easy parallelization, and thus enables
the reconstruction of networks with hundreds of thousands and even millions of
nodes and edges.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01414">VALD-MD: Visual Attribution via Latent Diffusion for Medical Diagnostics. (arXiv:2401.01414v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Siddiqui_A/0/1/0/all/0/1">Ammar A. Siddiqui</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Tirunagari_S/0/1/0/all/0/1">Santosh Tirunagari</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Zia_T/0/1/0/all/0/1">Tehseen Zia</a> (2), <a href="http://arxiv.org/find/eess/1/au:+Windridge_D/0/1/0/all/0/1">David Windridge</a> (1) ((1) Middlesex University, London, UK, (2) COMSATS University, Islamabad, Pakistan)</p>
<p>Visual attribution in medical imaging seeks to make evident the
diagnostically-relevant components of a medical image, in contrast to the more
common detection of diseased tissue deployed in standard machine vision
pipelines (which are less straightforwardly interpretable/explainable to
clinicians). We here present a novel generative visual attribution technique,
one that leverages latent diffusion models in combination with domain-specific
large language models, in order to generate normal counterparts of abnormal
images. The discrepancy between the two hence gives rise to a mapping
indicating the diagnostically-relevant image components. To achieve this, we
deploy image priors in conjunction with appropriate conditioning mechanisms in
order to control the image generative process, including natural language text
prompts acquired from medical science and applied radiology. We perform
experiments and quantitatively evaluate our results on the COVID-19 Radiography
Database containing labelled chest X-rays with differing pathologies via the
Frechet Inception Distance (FID), Structural Similarity (SSIM) and Multi Scale
Structural Similarity Metric (MS-SSIM) metrics obtained between real and
generated images. The resulting system also exhibits a range of latent
capabilities including zero-shot localized disease induction, which are
evaluated with real examples from the cheXpert dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01425">SwapTransformer: highway overtaking tactical planner model via imitation learning on OSHA dataset. (arXiv:2401.01425v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shamsoshoara_A/0/1/0/all/0/1">Alireza Shamsoshoara</a>, <a href="http://arxiv.org/find/cs/1/au:+Salih_S/0/1/0/all/0/1">Safin B Salih</a>, <a href="http://arxiv.org/find/cs/1/au:+Aghazadeh_P/0/1/0/all/0/1">Pedram Aghazadeh</a></p>
<p>This paper investigates the high-level decision-making problem in highway
scenarios regarding lane changing and over-taking other slower vehicles. In
particular, this paper aims to improve the Travel Assist feature for automatic
overtaking and lane changes on highways. About 9 million samples including lane
images and other dynamic objects are collected in simulation. This data;
Overtaking on Simulated HighwAys (OSHA) dataset is released to tackle this
challenge. To solve this problem, an architecture called SwapTransformer is
designed and implemented as an imitation learning approach on the OSHA dataset.
Moreover, auxiliary tasks such as future points and car distance network
predictions are proposed to aid the model in better understanding the
surrounding environment. The performance of the proposed solution is compared
with a multi-layer perceptron (MLP) and multi-head self-attention networks as
baselines in a simulation environment. We also demonstrate the performance of
the model with and without auxiliary tasks. All models are evaluated based on
different metrics such as time to finish each lap, number of overtakes, and
speed difference with speed limit. The evaluation shows that the
SwapTransformer model outperforms other models in different traffic densities
in the inference phase.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01426">Modular Learning of Deep Causal Generative Models for High-dimensional Causal Inference. (arXiv:2401.01426v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1">Md Musfiqur Rahman</a>, <a href="http://arxiv.org/find/cs/1/au:+Kocaoglu_M/0/1/0/all/0/1">Murat Kocaoglu</a></p>
<p>Pearl's causal hierarchy establishes a clear separation between
observational, interventional, and counterfactual questions. Researchers
proposed sound and complete algorithms to compute identifiable causal queries
at a given level of the hierarchy using the causal structure and data from the
lower levels of the hierarchy. However, most of these algorithms assume that we
can accurately estimate the probability distribution of the data, which is an
impractical assumption for high-dimensional variables such as images. On the
other hand, modern generative deep learning architectures can be trained to
learn how to accurately sample from such high-dimensional distributions.
Especially with the recent rise of foundation models for images, it is
desirable to leverage pre-trained models to answer causal queries with such
high-dimensional data. To address this, we propose a sequential training
algorithm that, given the causal structure and a pre-trained conditional
generative model, can train a deep causal generative model, which utilizes the
pre-trained model and can provably sample from identifiable interventional and
counterfactual distributions. Our algorithm, called Modular-DCM, uses
adversarial training to learn the network weights, and to the best of our
knowledge, is the first algorithm that can make use of pre-trained models and
provably sample from any identifiable causal query in the presence of latent
confounders with high-dimensional data. We demonstrate the utility of our
algorithm using semi-synthetic and real-world datasets containing images as
variables in the causal structure.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01442">Hierarchical Over-the-Air Federated Learning with Awareness of Interference and Data Heterogeneity. (arXiv:2401.01442v1 [cs.IT])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Azimi_Abarghouyi_S/0/1/0/all/0/1">Seyed Mohammad Azimi-Abarghouyi</a>, <a href="http://arxiv.org/find/cs/1/au:+Fodor_V/0/1/0/all/0/1">Viktoria Fodor</a></p>
<p>When implementing hierarchical federated learning over wireless networks,
scalability assurance and the ability to handle both interference and device
data heterogeneity are crucial. This work introduces a learning method designed
to address these challenges, along with a scalable transmission scheme that
efficiently uses a single wireless resource through over-the-air computation.
To provide resistance against data heterogeneity, we employ gradient
aggregations. Meanwhile, the impact of interference is minimized through
optimized receiver normalizing factors. For this, we model a multi-cluster
wireless network using stochastic geometry, and characterize the mean squared
error of the aggregation estimations as a function of the network parameters.
We show that despite the interference and the data heterogeneity, the proposed
scheme achieves high learning accuracy and can significantly outperform the
conventional hierarchical algorithm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01448">ProbMCL: Simple Probabilistic Contrastive Learning for Multi-label Visual Classification. (arXiv:2401.01448v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sajedi_A/0/1/0/all/0/1">Ahmad Sajedi</a>, <a href="http://arxiv.org/find/cs/1/au:+Khaki_S/0/1/0/all/0/1">Samir Khaki</a>, <a href="http://arxiv.org/find/cs/1/au:+Lawryshyn_Y/0/1/0/all/0/1">Yuri A. Lawryshyn</a>, <a href="http://arxiv.org/find/cs/1/au:+Plataniotis_K/0/1/0/all/0/1">Konstantinos N. Plataniotis</a></p>
<p>Multi-label image classification presents a challenging task in many domains,
including computer vision and medical imaging. Recent advancements have
introduced graph-based and transformer-based methods to improve performance and
capture label dependencies. However, these methods often include complex
modules that entail heavy computation and lack interpretability. In this paper,
we propose Probabilistic Multi-label Contrastive Learning (ProbMCL), a novel
framework to address these challenges in multi-label image classification
tasks. Our simple yet effective approach employs supervised contrastive
learning, in which samples that share enough labels with an anchor image based
on a decision threshold are introduced as a positive set. This structure
captures label dependencies by pulling positive pair embeddings together and
pushing away negative samples that fall below the threshold. We enhance
representation learning by incorporating a mixture density network into
contrastive learning and generating Gaussian mixture distributions to explore
the epistemic uncertainty of the feature encoder. We validate the effectiveness
of our framework through experimentation with datasets from the computer vision
and medical imaging domains. Our method outperforms the existing
state-of-the-art methods while achieving a low computational footprint on both
datasets. Visualization analyses also demonstrate that ProbMCL-learned
classifiers maintain a meaningful semantic topology.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01458">Concurrent Self-testing of Neural Networks Using Uncertainty Fingerprint. (arXiv:2401.01458v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1">Soyed Tuhin Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+tahoori_M/0/1/0/all/0/1">Mehdi B. tahoori</a></p>
<p>Neural networks (NNs) are increasingly used in always-on safety-critical
applications deployed on hardware accelerators (NN-HAs) employing various
memory technologies. Reliable continuous operation of NN is essential for
safety-critical applications. During online operation, NNs are susceptible to
single and multiple permanent and soft errors due to factors such as radiation,
aging, and thermal effects. Explicit NN-HA testing methods cannot detect
transient faults during inference, are unsuitable for always-on applications,
and require extensive test vector generation and storage. Therefore, in this
paper, we propose the \emph{uncertainty fingerprint} approach representing the
online fault status of NN. Furthermore, we propose a dual head NN topology
specifically designed to produce uncertainty fingerprints and the primary
prediction of the NN in \emph{a single shot}. During the online operation, by
matching the uncertainty fingerprint, we can concurrently self-test NNs with up
to $100\%$ coverage with a low false positive rate while maintaining a similar
performance of the primary task. Compared to existing works, memory overhead is
reduced by up to $243.7$ MB, multiply and accumulate (MAC) operation is reduced
by up to $10000\times$, and false-positive rates are reduced by up to $89\%$.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01460">Point Cloud Classification via Deep Set Linearized Optimal Transport. (arXiv:2401.01460v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mahan_S/0/1/0/all/0/1">Scott Mahan</a>, <a href="http://arxiv.org/find/cs/1/au:+Moosmuller_C/0/1/0/all/0/1">Caroline Moosm&#xfc;ller</a>, <a href="http://arxiv.org/find/cs/1/au:+Cloninger_A/0/1/0/all/0/1">Alexander Cloninger</a></p>
<p>We introduce Deep Set Linearized Optimal Transport, an algorithm designed for
the efficient simultaneous embedding of point clouds into an $L^2-$space. This
embedding preserves specific low-dimensional structures within the Wasserstein
space while constructing a classifier to distinguish between various classes of
point clouds. Our approach is motivated by the observation that $L^2-$distances
between optimal transport maps for distinct point clouds, originating from a
shared fixed reference distribution, provide an approximation of the
Wasserstein-2 distance between these point clouds, under certain assumptions.
To learn approximations of these transport maps, we employ input convex neural
networks (ICNNs) and establish that, under specific conditions, Euclidean
distances between samples from these ICNNs closely mirror Wasserstein-2
distances between the true distributions. Additionally, we train a
discriminator network that attaches weights these samples and creates a
permutation invariant classifier to differentiate between different classes of
point clouds. We showcase the advantages of our algorithm over the standard
deep set approach through experiments on a flow cytometry dataset with a
limited number of labeled point clouds.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01470">Token Propagation Controller for Efficient Vision Transformer. (arXiv:2401.01470v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1">Wentao Zhu</a></p>
<p>Vision transformers (ViTs) have achieved promising results on a variety of
Computer Vision tasks, however their quadratic complexity in the number of
input tokens has limited their application specially in resource-constrained
settings. Previous approaches that employ gradual token reduction to address
this challenge assume that token redundancy in one layer implies redundancy in
all the following layers. We empirically demonstrate that this assumption is
often not correct, i.e., tokens that are redundant in one layer can be useful
in later layers. We employ this key insight to propose a novel token
propagation controller (TPC) that incorporates two different
token-distributions, i.e., pause probability and restart probability to control
the reduction and reuse of tokens respectively, which results in more efficient
token utilization. To improve the estimates of token distributions, we propose
a smoothing mechanism that acts as a regularizer and helps remove noisy
outliers. Furthermore, to improve the training-stability of our proposed TPC,
we introduce a model stabilizer that is able to implicitly encode local image
structures and minimize accuracy fluctuations during model training. We present
extensive experimental results on the ImageNet-1K dataset using DeiT, LV-ViT
and Swin models to demonstrate the effectiveness of our proposed method. For
example, compared to baseline models, our proposed method improves the
inference speed of the DeiT-S by 250% while increasing the classification
accuracy by 1.0%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01472">A First Look at Information Highlighting in Stack Overflow Answers. (arXiv:2401.01472v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1">Shahla Shaan Ahmed</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shaowei Wang</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yuan Tian</a> (2), <a href="http://arxiv.org/find/cs/1/au:+Tse-Hsun/0/1/0/all/0/1">Tse-Hsun</a> (Peter) <a href="http://arxiv.org/find/cs/1/au:+Chen/0/1/0/all/0/1">Chen</a> (3), <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haoxiang Zhang</a> (4) ((1) Department of Computer Science, University of Manitoba, Canada, (2) School of Computing, Queen&#x27;s University, Canada, (3) Department of Computer Science and Software Engineering, Concordia University, Canada, (4) Huawei, Canada)</p>
<p>Context: Navigating the knowledge of Stack Overflow (SO) remains challenging.
To make the posts vivid to users, SO allows users to write and edit posts with
Markdown or HTML so that users can leverage various formatting styles (e.g.,
bold, italic, and code) to highlight the important information. Nonetheless,
there have been limited studies on the highlighted information. Objective: We
carried out the first large-scale exploratory study on the information
highlighted in SO answers in our recent study. To extend our previous study, we
develop approaches to automatically recommend highlighted content with
formatting styles using neural network architectures initially designed for the
Named Entity Recognition task. Method: In this paper, we studied 31,169,429
answers of Stack Overflow. For training recommendation models, we choose CNN
and BERT models for each type of formatting (i.e., Bold, Italic, Code, and
Heading) using the information highlighting dataset we collected from SO
answers. Results: Our models based on CNN architecture achieve precision
ranging from 0.71 to 0.82. The trained model for automatic code content
highlighting achieves a recall of 0.73 and an F1 score of 0.71, outperforming
the trained models for other formatting styles. The BERT models have even lower
recalls and F1 scores than the CNN models. Our analysis of failure cases
indicates that the majority of the failure cases are missing identification
(i.e., the model misses the content that is supposed to be highlighted) due to
the models tend to learn the frequently highlighted words while struggling to
learn less frequent words. Conclusion: Our findings suggest that it is possible
to develop recommendation models for highlighting information for answers with
different formatting styles on Stack Overflow.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01479">Kernel-U-Net: Hierarchical and Symmetrical Framework for Multivariate Time Series Forecasting. (arXiv:2401.01479v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1">Jiang You</a>, <a href="http://arxiv.org/find/cs/1/au:+Natowicz_R/0/1/0/all/0/1">Re&#x144;e Natowicz</a>, <a href="http://arxiv.org/find/cs/1/au:+Cela_A/0/1/0/all/0/1">Arben Cela</a>, <a href="http://arxiv.org/find/cs/1/au:+Ouanounou_J/0/1/0/all/0/1">Jacob Ouanounou</a>, <a href="http://arxiv.org/find/cs/1/au:+Siarry_P/0/1/0/all/0/1">Patrick Siarry</a></p>
<p>Time series forecasting task predicts future trends based on historical
information. Recent U-Net-based methods have demonstrated superior performance
in predicting real-world datasets. However, the performance of these models is
lower than patch-based models or linear models. In this work, we propose a
symmetric and hierarchical framework, Kernel-U-Net, which cuts the input
sequence into slices at each layer of the network and then computes them using
kernels. Furthermore, it generalizes the concept of convolutional kernels in
classic U-Net to accept custom kernels that follow the same design pattern.
Compared to the existing linear or transformer-based solution, our model
contains 3 advantages: 1) A small number of parameters: the parameters size is
$O(log(L)^2)$ where $L$ is the look-back window size, 2) Flexibility: its
kernels can be customized and fitted to the datasets, 3) Computation
efficiency: the computation complexity of transformer modules is reduced to
$O(log(L)^2)$ if they are placed close to the latent vector. Kernel-U-Net
accuracy was greater than or equal to the state-of-the-art model on six (out of
seven) real-world datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01482">Incorporating Geo-Diverse Knowledge into Prompting for Increased Geographical Robustness in Object Recognition. (arXiv:2401.01482v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Buettner_K/0/1/0/all/0/1">Kyle Buettner</a>, <a href="http://arxiv.org/find/cs/1/au:+Malakouti_S/0/1/0/all/0/1">Sina Malakouti</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Lorraine Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Kovashka_A/0/1/0/all/0/1">Adriana Kovashka</a></p>
<p>Existing object recognition models have been shown to lack robustness in
diverse geographical scenarios due to significant domain shifts in design and
context. Class representations need to be adapted to more accurately reflect an
object concept under these shifts. In the absence of training data from target
geographies, we hypothesize that geography-specific descriptive knowledge of
object categories can be leveraged to enhance robustness. For this purpose, we
explore the feasibility of probing a large-language model for
geography-specific object knowledge, and we investigate integrating knowledge
in zero-shot and learnable soft prompting with the CLIP vision-language model.
In particular, we propose a geography knowledge regularization method to ensure
that soft prompts trained on a source set of geographies generalize to an
unseen target set of geographies. Our gains on DollarStreet when generalizing
from a model trained only on data from Europe are as large as +2.8 on countries
from Africa, and +4.6 on the hardest classes. We further show competitive
performance vs. few-shot target training, and provide insights into how
descriptive knowledge captures geographical differences.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01484">Uncertainty Regularized Evidential Regression. (arXiv:2401.01484v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ye_K/0/1/0/all/0/1">Kai Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tiejin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1">Hua Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhan_L/0/1/0/all/0/1">Liang Zhan</a></p>
<p>The Evidential Regression Network (ERN) represents a novel approach that
integrates deep learning with Dempster-Shafer's theory to predict a target and
quantify the associated uncertainty. Guided by the underlying theory, specific
activation functions must be employed to enforce non-negative values, which is
a constraint that compromises model performance by limiting its ability to
learn from all samples. This paper provides a theoretical analysis of this
limitation and introduces an improvement to overcome it. Initially, we define
the region where the models can't effectively learn from the samples. Following
this, we thoroughly analyze the ERN and investigate this constraint. Leveraging
the insights from our analysis, we address the limitation by introducing a
novel regularization term that empowers the ERN to learn from the whole
training set. Our extensive experiments substantiate our theoretical findings
and demonstrate the effectiveness of the proposed solution.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01487">Natural Language Processing and Multimodal Stock Price Prediction. (arXiv:2401.01487v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Taylor_K/0/1/0/all/0/1">Kevin Taylor</a>, <a href="http://arxiv.org/find/cs/1/au:+Ng_J/0/1/0/all/0/1">Jerry Ng</a></p>
<p>In the realm of financial decision-making, predicting stock prices is
pivotal. Artificial intelligence techniques such as long short-term memory
networks (LSTMs), support-vector machines (SVMs), and natural language
processing (NLP) models are commonly employed to predict said prices. This
paper utilizes stock percentage change as training data, in contrast to the
traditional use of raw currency values, with a focus on analyzing publicly
released news articles. The choice of percentage change aims to provide models
with context regarding the significance of price fluctuations and overall price
change impact on a given stock. The study employs specialized BERT natural
language processing models to predict stock price trends, with a particular
emphasis on various data modalities. The results showcase the capabilities of
such strategies with a small natural language processing model to accurately
predict overall stock trends, and highlight the effectiveness of certain data
features and sector-specific data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01493">Free Lunch for Federated Remote Sensing Target Fine-Grained Classification: A Parameter-Efficient Framework. (arXiv:2401.01493v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shengchao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shu_T/0/1/0/all/0/1">Ting Shu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Huan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiahao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1">Sufen Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Lina Yang</a></p>
<p>Remote Sensing Target Fine-grained Classification (TFGC) is of great
significance in both military and civilian fields. Due to location differences,
growth in data size, and centralized server storage constraints, these data are
usually stored under different databases across regions/countries. However,
privacy laws and national security concerns constrain researchers from
accessing these sensitive remote sensing images for further analysis.
Additionally, low-resource remote sensing devices encounter challenges in terms
of communication overhead and efficiency when dealing with the ever-increasing
data and model scales. To solve the above challenges, this paper proposes a
novel Privacy-Reserving TFGC Framework based on Federated Learning, dubbed
PRFL. The proposed framework allows each client to learn global and local
knowledge to enhance the local representation of private data in environments
with extreme statistical heterogeneity (non. Independent and Identically
Distributed, IID). Thus, it provides highly customized models to clients with
differentiated data distributions. Moreover, the framework minimizes
communication overhead and improves efficiency while ensuring satisfactory
performance, thereby enhancing robustness and practical applicability under
resource-scarce conditions. We demonstrate the effectiveness of the proposed
PRFL on the classical TFGC task by leveraging four public datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01498">Utilizing Neural Transducers for Two-Stage Text-to-Speech via Semantic Token Prediction. (arXiv:2401.01498v1 [eess.AS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Kim_M/0/1/0/all/0/1">Minchan Kim</a>, <a href="http://arxiv.org/find/eess/1/au:+Jeong_M/0/1/0/all/0/1">Myeonghun Jeong</a>, <a href="http://arxiv.org/find/eess/1/au:+Choi_B/0/1/0/all/0/1">Byoung Jin Choi</a>, <a href="http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1">Semin Kim</a>, <a href="http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1">Joun Yeop Lee</a>, <a href="http://arxiv.org/find/eess/1/au:+Kim_N/0/1/0/all/0/1">Nam Soo Kim</a></p>
<p>We propose a novel text-to-speech (TTS) framework centered around a neural
transducer. Our approach divides the whole TTS pipeline into semantic-level
sequence-to-sequence (seq2seq) modeling and fine-grained acoustic modeling
stages, utilizing discrete semantic tokens obtained from wav2vec2.0 embeddings.
For a robust and efficient alignment modeling, we employ a neural transducer
named token transducer for the semantic token prediction, benefiting from its
hard monotonic alignment constraints. Subsequently, a non-autoregressive (NAR)
speech generator efficiently synthesizes waveforms from these semantic tokens.
Additionally, a reference speech controls temporal dynamics and acoustic
conditions at each stage. This decoupled framework reduces the training
complexity of TTS while allowing each stage to focus on semantic and acoustic
modeling. Our experimental results on zero-shot adaptive TTS demonstrate that
our model surpasses the baseline in terms of speech quality and speaker
similarity, both objectively and subjectively. We also delve into the inference
speed and prosody control capabilities of our approach, highlighting the
potential of neural transducers in TTS frameworks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01502">Pontryagin Neural Operator for Solving Parametric General-Sum Differential Games. (arXiv:2401.01502v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghimire_M/0/1/0/all/0/1">Mukesh Ghimire</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zhe Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenlong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1">Yi Ren</a></p>
<p>The values of two-player general-sum differential games are viscosity
solutions to Hamilton-Jacobi-Isaacs (HJI) equations. Value and policy
approximations for such games suffer from the curse of dimensionality (CoD).
Alleviating CoD through physics-informed neural networks (PINN) encounters
convergence issues when value discontinuity is present due to state
constraints. On top of these challenges, it is often necessary to learn
generalizable values and policies across a parametric space of games, e.g., for
game parameter inference when information is incomplete. To address these
challenges, we propose in this paper a Pontryagin-mode neural operator that
outperforms existing state-of-the-art (SOTA) on safety performance across games
with parametric state constraints. Our key contribution is the introduction of
a costate loss defined on the discrepancy between forward and backward costate
rollouts, which are computationally cheap. We show that the discontinuity of
costate dynamics (in the presence of state constraints) effectively enables the
learning of discontinuous values, without requiring manually supervised data as
suggested by the current SOTA. More importantly, we show that the close
relationship between costates and policies makes the former critical in
learning feedback control policies with generalizable safety performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01506">AIRI: Predicting Retention Indices and their Uncertainties using Artificial Intelligence. (arXiv:2401.01506v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Geer_L/0/1/0/all/0/1">Lewis Y. Geer</a>, <a href="http://arxiv.org/find/cs/1/au:+Stein_S/0/1/0/all/0/1">Stephen E. Stein</a>, <a href="http://arxiv.org/find/cs/1/au:+Mallard_W/0/1/0/all/0/1">William Gary Mallard</a>, <a href="http://arxiv.org/find/cs/1/au:+Slotta_D/0/1/0/all/0/1">Douglas J. Slotta</a></p>
<p>The Kov\'ats Retention index (RI) is a quantity measured using gas
chromatography and commonly used in the identification of chemical structures.
Creating libraries of observed RI values is a laborious task, so we explore the
use of a deep neural network for predicting RI values from structure for
standard semipolar columns. This network generated predictions with a mean
absolute error of 15.1 and, in a quantification of the tail of the error
distribution, a 95th percentile absolute error of 46.5. Because of the
Artificial Intelligence Retention Indices (AIRI) network's accuracy, it was
used to predict RI values for the NIST EI-MS spectral libraries. These RI
values are used to improve chemical identification methods and the quality of
the library. Estimating uncertainty is an important practical need when using
prediction models. To quantify the uncertainty of our network for each
individual prediction, we used the outputs of an ensemble of 8 networks to
calculate a predicted standard deviation for each RI value prediction. This
predicted standard deviation was corrected to follow the error between observed
and predicted RI values. The Z scores using these predicted standard deviations
had a standard deviation of 1.52 and a 95th percentile absolute Z score
corresponding to a mean RI value of 42.6.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01519">Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review. (arXiv:2401.01519v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ke_L/0/1/0/all/0/1">Luoma Ke</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Tong_S/0/1/0/all/0/1">Song Tong</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1">Peng Chen</a> (2), <a href="http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1">Kaiping Peng</a> (1) ((1) Department of Psychology, Tsinghua University, (2) School of Social Science, Tsinghua University)</p>
<p>This paper explores the frontiers of large language models (LLMs) in
psychology applications. Psychology has undergone several theoretical changes,
and the current use of Artificial Intelligence (AI) and Machine Learning,
particularly LLMs, promises to open up new research directions. We provide a
detailed exploration of how LLMs like ChatGPT are transforming psychological
research. It discusses the impact of LLMs across various branches of
psychology, including cognitive and behavioral, clinical and counseling,
educational and developmental, and social and cultural psychology, highlighting
their potential to simulate aspects of human cognition and behavior. The paper
delves into the capabilities of these models to emulate human-like text
generation, offering innovative tools for literature review, hypothesis
generation, experimental design, experimental subjects, data analysis, academic
writing, and peer review in psychology. While LLMs are essential in advancing
research methodologies in psychology, the paper also cautions about their
technical and ethical challenges. There are issues like data privacy, the
ethical implications of using LLMs in psychological research, and the need for
a deeper understanding of these models' limitations. Researchers should
responsibly use LLMs in psychological studies, adhering to ethical standards
and considering the potential consequences of deploying these technologies in
sensitive areas. Overall, the article provides a comprehensive overview of the
current state of LLMs in psychology, exploring potential benefits and
challenges. It serves as a call to action for researchers to leverage LLLs'
advantages responsibly while addressing associated risks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01520">S$^{2}$-DMs:Skip-Step Diffusion Models. (arXiv:2401.01520v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yixuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shuangyin Li</a></p>
<p>Diffusion models have emerged as powerful generative tools, rivaling GANs in
sample quality and mirroring the likelihood scores of autoregressive models. A
subset of these models, exemplified by DDIMs, exhibit an inherent asymmetry:
they are trained over $T$ steps but only sample from a subset of $T$ during
generation. This selective sampling approach, though optimized for speed,
inadvertently misses out on vital information from the unsampled steps, leading
to potential compromises in sample quality. To address this issue, we present
the S$^{2}$-DMs, which is a new training method by using an innovative
$L_{skip}$, meticulously designed to reintegrate the information omitted during
the selective sampling phase. The benefits of this approach are manifold: it
notably enhances sample quality, is exceptionally simple to implement, requires
minimal code modifications, and is flexible enough to be compatible with
various sampling algorithms. On the CIFAR10 dataset, models trained using our
algorithm showed an improvement of 3.27% to 14.06% over models trained with
traditional methods across various sampling algorithms (DDIMs, PNDMs, DEIS) and
different numbers of sampling steps (10, 20, ..., 1000). On the CELEBA dataset,
the improvement ranged from 8.97% to 27.08%. Access to the code and additional
resources is provided in the github.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01528">Improved Bandits in Many-to-one Matching Markets with Incentive Compatibility. (arXiv:2401.01528v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kong_F/0/1/0/all/0/1">Fang Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shuai Li</a></p>
<p>Two-sided matching markets have been widely studied in the literature due to
their rich applications. Since participants are usually uncertain about their
preferences, online algorithms have recently been adopted to learn them through
iterative interactions. \citet{wang2022bandit} initiate the study of this
problem in a many-to-one setting with \textit{responsiveness}. However, their
results are far from optimal and lack guarantees of incentive compatibility. An
extension of \citet{kong2023player} to this more general setting achieves a
near-optimal bound for player-optimal regret. Nevertheless, due to the
substantial requirement for collaboration, a single player's deviation could
lead to a huge increase in its own cumulative rewards and an $O(T)$ regret for
others. In this paper, we aim to enhance the regret bound in many-to-one
markets while ensuring incentive compatibility. We first propose the adaptively
explore-then-deferred-acceptance (AETDA) algorithm for responsiveness setting
and derive an $O(N\min\left\{N,K\right\}C\log T/\Delta^2)$ upper bound for
player-optimal stable regret while demonstrating its guarantee of incentive
compatibility, where $N$ represents the number of players, $K$ is the number of
arms, $T$ denotes the time horizon, $C$ is arms' total capacities and $\Delta$
signifies the minimum preference gap among players. This result is a
significant improvement over \citet{wang2022bandit}. And to the best of our
knowledge, it constitutes the first player-optimal guarantee in matching
markets that offers such robust assurances. We also consider broader
\textit{substitutable} preferences, one of the most general conditions to
ensure the existence of a stable matching and cover responsiveness. We devise
an online DA (ODA) algorithm and establish an $O(NK\log T/\Delta^2)$
player-pessimal stable regret bound for this setting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01531">Will 6G be Semantic Communications? Opportunities and Challenges from Task Oriented and Secure Communications to Integrated Sensing. (arXiv:2401.01531v1 [cs.NI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sagduyu_Y/0/1/0/all/0/1">Yalin E. Sagduyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Erpek_T/0/1/0/all/0/1">Tugba Erpek</a>, <a href="http://arxiv.org/find/cs/1/au:+Yener_A/0/1/0/all/0/1">Aylin Yener</a>, <a href="http://arxiv.org/find/cs/1/au:+Ulukus_S/0/1/0/all/0/1">Sennur Ulukus</a></p>
<p>This paper explores opportunities and challenges of task (goal)-oriented and
semantic communications for next-generation (NextG) communication networks
through the integration of multi-task learning. This approach employs deep
neural networks representing a dedicated encoder at the transmitter and
multiple task-specific decoders at the receiver, collectively trained to handle
diverse tasks including semantic information preservation, source input
reconstruction, and integrated sensing and communications. To extend the
applicability from point-to-point links to multi-receiver settings, we envision
the deployment of decoders at various receivers, where decentralized learning
addresses the challenges of communication load and privacy concerns, leveraging
federated learning techniques that distribute model updates across
decentralized nodes. However, the efficacy of this approach is contingent on
the robustness of the employed deep learning models. We scrutinize potential
vulnerabilities stemming from adversarial attacks during both training and
testing phases. These attacks aim to manipulate both the inputs at the encoder
at the transmitter and the signals received over the air on the receiver side,
highlighting the importance of fortifying semantic communications against
potential multi-domain exploits. Overall, the joint and robust design of
task-oriented communications, semantic communications, and integrated sensing
and communications in a multi-task learning framework emerges as the key
enabler for context-aware, resource-efficient, and secure communications
ultimately needed in NextG network systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01537">The Art of Deception: Robust Backdoor Attack using Dynamic Stacking of Triggers. (arXiv:2401.01537v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mengara_O/0/1/0/all/0/1">Orson Mengara</a></p>
<p>The area of Machine Learning as a Service (MLaaS) is experiencing increased
implementation due to recent advancements in the AI (Artificial Intelligence)
industry. However, this spike has prompted concerns regarding AI defense
mechanisms, specifically regarding potential covert attacks from third-party
providers that cannot be entirely trusted. Recent research has uncovered that
auditory backdoors may use certain modifications as their initiating mechanism.
DynamicTrigger is introduced as a methodology for carrying out dynamic backdoor
attacks that use cleverly designed tweaks to ensure that corrupted samples are
indistinguishable from clean. By utilizing fluctuating signal sampling rates
and masking speaker identities through dynamic sound triggers (such as the
clapping of hands), it is possible to deceive speech recognition systems (ASR).
Our empirical testing demonstrates that DynamicTrigger is both potent and
stealthy, achieving impressive success rates during covert attacks while
maintaining exceptional accuracy with non-poisoned datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01549">Towards Modeling Uncertainties of Self-explaining Neural Networks via Conformal Prediction. (arXiv:2401.01549v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qian_W/0/1/0/all/0/1">Wei Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1">Chenxu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yangyi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1">Fenglong Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huai_M/0/1/0/all/0/1">Mengdi Huai</a></p>
<p>Despite the recent progress in deep neural networks (DNNs), it remains
challenging to explain the predictions made by DNNs. Existing explanation
methods for DNNs mainly focus on post-hoc explanations where another
explanatory model is employed to provide explanations. The fact that post-hoc
methods can fail to reveal the actual original reasoning process of DNNs raises
the need to build DNNs with built-in interpretability. Motivated by this, many
self-explaining neural networks have been proposed to generate not only
accurate predictions but also clear and intuitive insights into why a
particular decision was made. However, existing self-explaining networks are
limited in providing distribution-free uncertainty quantification for the two
simultaneously generated prediction outcomes (i.e., a sample's final prediction
and its corresponding explanations for interpreting that prediction).
Importantly, they also fail to establish a connection between the confidence
values assigned to the generated explanations in the interpretation layer and
those allocated to the final predictions in the ultimate prediction layer. To
tackle the aforementioned challenges, in this paper, we design a novel
uncertainty modeling framework for self-explaining networks, which not only
demonstrates strong distribution-free uncertainty modeling performance for the
generated explanations in the interpretation layer but also excels in producing
efficient and effective prediction sets for the final predictions based on the
informative high-level basis explanations. We perform the theoretical analysis
for the proposed framework. Extensive experimental evaluation demonstrates the
effectiveness of the proposed uncertainty framework.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01579">An Invariant Information Geometric Method for High-Dimensional Online Optimization. (arXiv:2401.01579v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhengfei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1">Yunyue Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Sui_Y/0/1/0/all/0/1">Yanan Sui</a></p>
<p>Sample efficiency is crucial in optimization, particularly in black-box
scenarios characterized by expensive evaluations and zeroth-order feedback.
When computing resources are plentiful, Bayesian optimization is often favored
over evolution strategies. In this paper, we introduce a full invariance
oriented evolution strategies algorithm, derived from its corresponding
framework, that effectively rivals the leading Bayesian optimization method in
tasks with dimensions at the upper limit of Bayesian capability. Specifically,
we first build the framework InvIGO that fully incorporates historical
information while retaining the full invariant and computational complexity. We
then exemplify InvIGO on multi-dimensional Gaussian, which gives an invariant
and scalable optimizer SynCMA . The theoretical behavior and advantages of our
algorithm over other Gaussian-based evolution strategies are further analyzed.
Finally, We benchmark SynCMA against leading algorithms in Bayesian
optimization and evolution strategies on various high dimension tasks, in
cluding Mujoco locomotion tasks, rover planning task and synthetic functions.
In all scenarios, SynCMA demonstrates great competence, if not dominance, over
other algorithms in sample efficiency, showing the underdeveloped potential of
property oriented evolution strategies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01599">Generalization Error Curves for Analytic Spectral Algorithms under Power-law Decay. (arXiv:2401.01599v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yicheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gan_W/0/1/0/all/0/1">Weiye Gan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1">Zuoqiang Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1">Qian Lin</a></p>
<p>The generalization error curve of certain kernel regression method aims at
determining the exact order of generalization error with various source
condition, noise level and choice of the regularization parameter rather than
the minimax rate. In this work, under mild assumptions, we rigorously provide a
full characterization of the generalization error curves of the kernel gradient
descent method (and a large class of analytic spectral algorithms) in kernel
regression. Consequently, we could sharpen the near inconsistency of kernel
interpolation and clarify the saturation effects of kernel regression
algorithms with higher qualification, etc. Thanks to the neural tangent kernel
theory, these results greatly improve our understanding of the generalization
behavior of training the wide neural networks. A novel technical contribution,
the analytic functional argument, might be of independent interest.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01600">PLLaMa: An Open-source Large Language Model for Plant Science. (arXiv:2401.01600v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xianjun Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Junfeng Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1">Wenxin Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Alexandersson_E/0/1/0/all/0/1">Erik Alexandersson</a></p>
<p>Large Language Models (LLMs) have exhibited remarkable capabilities in
understanding and interacting with natural language across various sectors.
However, their effectiveness is limited in specialized areas requiring high
accuracy, such as plant science, due to a lack of specific expertise in these
fields. This paper introduces PLLaMa, an open-source language model that
evolved from LLaMa-2. It's enhanced with a comprehensive database, comprising
more than 1.5 million scholarly articles in plant science. This development
significantly enriches PLLaMa with extensive knowledge and proficiency in plant
and agricultural sciences. Our initial tests, involving specific datasets
related to plants and agriculture, show that PLLaMa substantially improves its
understanding of plant science-related topics. Moreover, we have formed an
international panel of professionals, including plant scientists, agricultural
engineers, and plant breeders. This team plays a crucial role in verifying the
accuracy of PLLaMa's responses to various academic inquiries, ensuring its
effective and reliable application in the field. To support further research
and development, we have made the model's checkpoints and source codes
accessible to the scientific community. These resources are available for
download at \url{https://github.com/Xianjun-Yang/PLLaMa}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01625">SCALA: Sparsification-based Contrastive Learning for Anomaly Detection on Attributed Networks. (arXiv:2401.01625v1 [cs.SI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_E/0/1/0/all/0/1">Enbo He</a>, <a href="http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1">Yitong Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yue Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_G/0/1/0/all/0/1">Guisheng Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1">Lina Yao</a></p>
<p>Anomaly detection on attributed networks aims to find the nodes whose
behaviors are significantly different from other majority nodes. Generally,
network data contains information about relationships between entities, and the
anomaly is usually embodied in these relationships. Therefore, how to
comprehensively model complex interaction patterns in networks is still a major
focus. It can be observed that anomalies in networks violate the homophily
assumption. However, most existing studies only considered this phenomenon
obliquely rather than explicitly. Besides, the node representation of normal
entities can be perturbed easily by the noise relationships introduced by
anomalous nodes. To address the above issues, we present a novel contrastive
learning framework for anomaly detection on attributed networks,
\textbf{SCALA}, aiming to improve the embedding quality of the network and
provide a new measurement of qualifying the anomaly score for each node by
introducing sparsification into the conventional method. Extensive experiments
are conducted on five benchmark real-world datasets and the results show that
SCALA consistently outperforms all baseline methods significantly.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01626">On the Expressive Power of Graph Neural Networks. (arXiv:2401.01626v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nalwade_A/0/1/0/all/0/1">Ashwin Nalwade</a>, <a href="http://arxiv.org/find/cs/1/au:+Marshall_K/0/1/0/all/0/1">Kelly Marshall</a>, <a href="http://arxiv.org/find/cs/1/au:+Eladi_A/0/1/0/all/0/1">Axel Eladi</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_U/0/1/0/all/0/1">Umang Sharma</a></p>
<p>The study of Graph Neural Networks has received considerable interest in the
past few years. By extending deep learning to graph-structured data, GNNs can
solve a diverse set of tasks in fields including social science, chemistry, and
medicine. The development of GNN architectures has largely been focused on
improving empirical performance on tasks like node or graph classification.
However, a line of recent work has instead sought to find GNN architectures
that have desirable theoretical properties - by studying their expressive power
and designing architectures that maximize this expressiveness.
</p>
<p>While there is no consensus on the best way to define the expressiveness of a
GNN, it can be viewed from several well-motivated perspectives. Perhaps the
most natural approach is to study the universal approximation properties of
GNNs, much in the way that this has been studied extensively for MLPs. Another
direction focuses on the extent to which GNNs can distinguish between different
graph structures, relating this to the graph isomorphism test. Besides, a GNN's
ability to compute graph properties such as graph moments has been suggested as
another form of expressiveness. All of these different definitions are
complementary and have yielded different recommendations for GNN architecture
choices. In this paper, we would like to give an overview of the notion of
"expressive power" of GNNs and provide some valuable insights regarding the
design choices of GNNs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01629">Synthetic Data in AI: Challenges, Applications, and Ethical Implications. (arXiv:2401.01629v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hao_S/0/1/0/all/0/1">Shuang Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1">Wenfeng Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1">Tao Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yiping Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Haonan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_C/0/1/0/all/0/1">Chunlin Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhangjun Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1">He Tang</a></p>
<p>In the rapidly evolving field of artificial intelligence, the creation and
utilization of synthetic datasets have become increasingly significant. This
report delves into the multifaceted aspects of synthetic data, particularly
emphasizing the challenges and potential biases these datasets may harbor. It
explores the methodologies behind synthetic data generation, spanning
traditional statistical models to advanced deep learning techniques, and
examines their applications across diverse domains. The report also critically
addresses the ethical considerations and legal implications associated with
synthetic datasets, highlighting the urgent need for mechanisms to ensure
fairness, mitigate biases, and uphold ethical standards in AI development.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01640">Evaluating Fairness in Self-supervised and Supervised Models for Sequential Data. (arXiv:2401.01640v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yfantidou_S/0/1/0/all/0/1">Sofia Yfantidou</a>, <a href="http://arxiv.org/find/cs/1/au:+Spathis_D/0/1/0/all/0/1">Dimitris Spathis</a>, <a href="http://arxiv.org/find/cs/1/au:+Constantinides_M/0/1/0/all/0/1">Marios Constantinides</a>, <a href="http://arxiv.org/find/cs/1/au:+Vakali_A/0/1/0/all/0/1">Athena Vakali</a>, <a href="http://arxiv.org/find/cs/1/au:+Quercia_D/0/1/0/all/0/1">Daniele Quercia</a>, <a href="http://arxiv.org/find/cs/1/au:+Kawsar_F/0/1/0/all/0/1">Fahim Kawsar</a></p>
<p>Self-supervised learning (SSL) has become the de facto training paradigm of
large models where pre-training is followed by supervised fine-tuning using
domain-specific data and labels. Hypothesizing that SSL models would learn more
generic, hence less biased, representations, this study explores the impact of
pre-training and fine-tuning strategies on fairness (i.e., performing equally
on different demographic breakdowns). Motivated by human-centric applications
on real-world timeseries data, we interpret inductive biases on the model,
layer, and metric levels by systematically comparing SSL models to their
supervised counterparts. Our findings demonstrate that SSL has the capacity to
achieve performance on par with supervised methods while significantly
enhancing fairness--exhibiting up to a 27% increase in fairness with a mere 1%
loss in performance through self-supervision. Ultimately, this work underscores
SSL's potential in human-centric computing, particularly high-stakes,
data-scarce application domains like healthcare.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01641">Towards a Foundation Purchasing Model: Pretrained Generative Autoregression on Transaction Sequences. (arXiv:2401.01641v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Skalski_P/0/1/0/all/0/1">Piotr Skalski</a>, <a href="http://arxiv.org/find/cs/1/au:+Sutton_D/0/1/0/all/0/1">David Sutton</a>, <a href="http://arxiv.org/find/cs/1/au:+Burrell_S/0/1/0/all/0/1">Stuart Burrell</a>, <a href="http://arxiv.org/find/cs/1/au:+Perez_I/0/1/0/all/0/1">Iker Perez</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1">Jason Wong</a></p>
<p>Machine learning models underpin many modern financial systems for use cases
such as fraud detection and churn prediction. Most are based on supervised
learning with hand-engineered features, which relies heavily on the
availability of labelled data. Large self-supervised generative models have
shown tremendous success in natural language processing and computer vision,
yet so far they haven't been adapted to multivariate time series of financial
transactions. In this paper, we present a generative pretraining method that
can be used to obtain contextualised embeddings of financial transactions.
Benchmarks on public datasets demonstrate that it outperforms state-of-the-art
self-supervised methods on a range of downstream tasks. We additionally perform
large-scale pretraining of an embedding model using a corpus of data from 180
issuing banks containing 5.1 billion transactions and apply it to the card
fraud detection problem on hold-out datasets. The embedding model significantly
improves value detection rate at high precision thresholds and transfers well
to out-of-domain distributions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01654">LESEN: Label-Efficient deep learning for Multi-parametric MRI-based Visual Pathway Segmentation. (arXiv:2401.01654v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Diakite_A/0/1/0/all/0/1">Alou Diakite</a> (1 and 2), <a href="http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1">Cheng Li</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Xie_L/0/1/0/all/0/1">Lei Xie</a> (3), <a href="http://arxiv.org/find/eess/1/au:+Feng_Y/0/1/0/all/0/1">Yuanjing Feng</a> (3), <a href="http://arxiv.org/find/eess/1/au:+Han_H/0/1/0/all/0/1">Hua Han</a> (1 and 2), <a href="http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1">Shanshan Wang</a> (1 and 4) ( (1) Paul C. Lauterbur Research Center for Biomedical Imaging, Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China, (2) University of Chinese Academy of Sciences, Beijing, China, (3) Zhejiang University of Technology, Hangzhou, China, (4) Peng Cheng Laboratory, Shenzhen, China)</p>
<p>Recent research has shown the potential of deep learning in multi-parametric
MRI-based visual pathway (VP) segmentation. However, obtaining labeled data for
training is laborious and time-consuming. Therefore, it is crucial to develop
effective algorithms in situations with limited labeled samples. In this work,
we propose a label-efficient deep learning method with self-ensembling (LESEN).
LESEN incorporates supervised and unsupervised losses, enabling the student and
teacher models to mutually learn from each other, forming a self-ensembling
mean teacher framework. Additionally, we introduce a reliable unlabeled sample
selection (RUSS) mechanism to further enhance LESEN's effectiveness. Our
experiments on the human connectome project (HCP) dataset demonstrate the
superior performance of our method when compared to state-of-the-art
techniques, advancing multimodal VP segmentation for comprehensive analysis in
clinical and research settings. The implementation code will be available at:
https://github.com/aldiak/Semi-Supervised-Multimodal-Visual-Pathway-
Delineation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01690">Zero-shot Active Learning Using Self Supervised Learning. (arXiv:2401.01690v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1">Abhishek Sinha</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Shreya Singh</a></p>
<p>Deep learning algorithms are often said to be data hungry. The performance of
such algorithms generally improve as more and more annotated data is fed into
the model. While collecting unlabelled data is easier (as they can be scraped
easily from the internet), annotating them is a tedious and expensive task.
Given a fixed budget available for data annotation, Active Learning helps
selecting the best subset of data for annotation, such that the deep learning
model when trained over that subset will have maximum generalization
performance under this budget. In this work, we aim to propose a new Active
Learning approach which is model agnostic as well as one doesn't require an
iterative process. We aim to leverage self-supervised learnt features for the
task of Active Learning. The benefit of self-supervised learning, is that one
can get useful feature representation of the input data, without having any
annotation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01710">EPA: Neural Collapse Inspired Robust Out-of-Distribution Detector. (arXiv:2401.01710v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiawei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yufan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1">Cheng Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1">Lei Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1">Yuantao Gu</a></p>
<p>Out-of-distribution (OOD) detection plays a crucial role in ensuring the
security of neural networks. Existing works have leveraged the fact that
In-distribution (ID) samples form a subspace in the feature space, achieving
state-of-the-art (SOTA) performance. However, the comprehensive characteristics
of the ID subspace still leave under-explored. Recently, the discovery of
Neural Collapse ($\mathcal{NC}$) sheds light on novel properties of the ID
subspace. Leveraging insight from $\mathcal{NC}$, we observe that the Principal
Angle between the features and the ID feature subspace forms a superior
representation for measuring the likelihood of OOD. Building upon this
observation, we propose a novel $\mathcal{NC}$-inspired OOD scoring function,
named Entropy-enhanced Principal Angle (EPA), which integrates both the global
characteristic of the ID subspace and its inner property. We experimentally
compare EPA with various SOTA approaches, validating its superior performance
and robustness across different network architectures and OOD datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01728">Ravnest: Decentralized Asynchronous Training on Heterogeneous Devices. (arXiv:2401.01728v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Menon_A/0/1/0/all/0/1">Anirudh Rajiv Menon</a>, <a href="http://arxiv.org/find/cs/1/au:+Menon_U/0/1/0/all/0/1">Unnikrishnan Menon</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahirwar_K/0/1/0/all/0/1">Kailash Ahirwar</a></p>
<p>Modern deep learning models, growing larger and more complex, have
demonstrated exceptional generalization and accuracy due to training on huge
datasets. This trend is expected to continue. However, the increasing size of
these models poses challenges in training, as traditional centralized methods
are limited by memory constraints at such scales. This paper proposes an
asynchronous decentralized training paradigm for large modern deep learning
models that harnesses the compute power of regular heterogeneous PCs with
limited resources connected across the internet to achieve favourable
performance metrics. Ravnest facilitates decentralized training by efficiently
organizing compute nodes into clusters with similar data transfer rates and
compute capabilities, without necessitating that each node hosts the entire
model. These clusters engage in $\textit{Zero-Bubble Asynchronous Model
Parallel}$ training, and a $\textit{Parallel Multi-Ring All-Reduce}$ method is
employed to effectively execute global parameter averaging across all clusters.
We have framed our asynchronous SGD loss function as a block structured
optimization problem with delayed updates and derived an optimal convergence
rate of $O\left(\frac{1}{\sqrt{K}}\right)$. We further discuss linear speedup
with respect to the number of participating clusters and the bound on the
staleness parameter.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01732">Task and Explanation Network. (arXiv:2401.01732v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sipper_M/0/1/0/all/0/1">Moshe Sipper</a></p>
<p>Explainability in deep networks has gained increased importance in recent
years. We argue herein that an AI must be tasked not just with a task but also
with an explanation of why said task was accomplished as such. We present a
basic framework -- Task and Explanation Network (TENet) -- which fully
integrates task completion and its explanation. We believe that the field of AI
as a whole should insist -- quite emphatically -- on explainability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01733">Investigating the Suitability of Concept Drift Detection for Detecting Leakages in Water Distribution Networks. (arXiv:2401.01733v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vaquet_V/0/1/0/all/0/1">Valerie Vaquet</a>, <a href="http://arxiv.org/find/cs/1/au:+Hinder_F/0/1/0/all/0/1">Fabian Hinder</a>, <a href="http://arxiv.org/find/cs/1/au:+Hammer_B/0/1/0/all/0/1">Barbara Hammer</a></p>
<p>Leakages are a major risk in water distribution networks as they cause water
loss and increase contamination risks. Leakage detection is a difficult task
due to the complex dynamics of water distribution networks. In particular,
small leakages are hard to detect. From a machine-learning perspective,
leakages can be modeled as concept drift. Thus, a wide variety of drift
detection schemes seems to be a suitable choice for detecting leakages. In this
work, we explore the potential of model-loss-based and distribution-based drift
detection methods to tackle leakage detection. We additionally discuss the
issue of temporal dependencies in the data and propose a way to cope with it
when applying distribution-based detection. We evaluate different methods
systematically for leakages of different sizes and detection times.
Additionally, we propose a first drift-detection-based technique for localizing
leakages.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01764">Understanding the Detrimental Class-level Effects of Data Augmentation. (arXiv:2401.01764v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kirichenko_P/0/1/0/all/0/1">Polina Kirichenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Ibrahim_M/0/1/0/all/0/1">Mark Ibrahim</a>, <a href="http://arxiv.org/find/cs/1/au:+Balestriero_R/0/1/0/all/0/1">Randall Balestriero</a>, <a href="http://arxiv.org/find/cs/1/au:+Bouchacourt_D/0/1/0/all/0/1">Diane Bouchacourt</a>, <a href="http://arxiv.org/find/cs/1/au:+Vedantam_R/0/1/0/all/0/1">Ramakrishna Vedantam</a>, <a href="http://arxiv.org/find/cs/1/au:+Firooz_H/0/1/0/all/0/1">Hamed Firooz</a>, <a href="http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1">Andrew Gordon Wilson</a></p>
<p>Data augmentation (DA) encodes invariance and provides implicit
regularization critical to a model's performance in image classification tasks.
However, while DA improves average accuracy, recent studies have shown that its
impact can be highly class dependent: achieving optimal average accuracy comes
at the cost of significantly hurting individual class accuracy by as much as
20% on ImageNet. There has been little progress in resolving class-level
accuracy drops due to a limited understanding of these effects. In this work,
we present a framework for understanding how DA interacts with class-level
learning dynamics. Using higher-quality multi-label annotations on ImageNet, we
systematically categorize the affected classes and find that the majority are
inherently ambiguous, co-occur, or involve fine-grained distinctions, while DA
controls the model's bias towards one of the closely related classes. While
many of the previously reported performance drops are explained by multi-label
annotations, our analysis of class confusions reveals other sources of accuracy
degradation. We show that simple class-conditional augmentation strategies
informed by our framework improve performance on the negatively affected
classes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01783">Approximating Numerical Flux by Fourier Neural Operators for the Hyperbolic Conservation Laws. (arXiv:2401.01783v1 [math.NA])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Kim_T/0/1/0/all/0/1">Taeyoung Kim</a>, <a href="http://arxiv.org/find/math/1/au:+Sang_M/0/1/0/all/0/1">Myungjoo Sang</a></p>
<p>Classical numerical schemes exist for solving PDEs numerically, and recently,
neural network-based methods have been developed. However, methodologies using
neural networks, such as PINN and neural operators, lack robustness and
generalization power. To compensate for such drawbacks, there are many types of
research combining classical numerical schemes and machine learning methods by
replacing a small portion of the numerical schemes with neural networks. In
this work, we focus on hyperbolic conservation laws and replace numerical
fluxes in the numerical schemes by neural operator. For this, we construct
losses that are motivated by numerical schemes for conservation laws and
approximate numerical flux by FNO. Through experiments, we show that our
methodology has advantages of both numerical schemes and FNO by comparing with
original methods. For instance, we demonstrate our method gains robustness,
resolution invariance property, and feasibility of a data-driven method. Our
method especially has the ability to predict continuously in time and
generalization power on the out-of-distribution samples, which are challenges
to be tackled for existing neural operator methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01788">Applications of machine learning and IoT for Outdoor Air Pollution Monitoring and Prediction: A Systematic Literature Review. (arXiv:2401.01788v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gryech_I/0/1/0/all/0/1">Ihsane Gryech</a>, <a href="http://arxiv.org/find/cs/1/au:+Assad_C/0/1/0/all/0/1">Chaimae Assad</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghogho_M/0/1/0/all/0/1">Mounir Ghogho</a>, <a href="http://arxiv.org/find/cs/1/au:+Kobbane_A/0/1/0/all/0/1">Abdellatif Kobbane</a></p>
<p>According to the World Health Organization (WHO), air pollution kills seven
million people every year. Outdoor air pollution is a major environmental
health problem affecting low, middle, and high-income countries. In the past
few years, the research community has explored IoT-enabled machine learning
applications for outdoor air pollution prediction. The general objective of
this paper is to systematically review applications of machine learning and
Internet of Things (IoT) for outdoor air pollution prediction and the
combination of monitoring sensors and input features used. Two research
questions were formulated for this review. 1086 publications were collected in
the initial PRISMA stage. After the screening and eligibility phases, 37 papers
were selected for inclusion. A cost-based analysis was conducted on the
findings to highlight high-cost monitoring, low-cost IoT and hybrid enabled
prediction. Three methods of prediction were identified: time series,
feature-based and spatio-temporal. This review's findings identify major
limitations in applications found in the literature, namely lack of coverage,
lack of diversity of data and lack of inclusion of context-specific features.
This review proposes directions for future research and underlines practical
implications in healthcare, urban planning, global synergy and smart cities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01789">Deep learning the Hurst parameter of linear fractional processes and assessing its reliability. (arXiv:2401.01789v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Boros_D/0/1/0/all/0/1">D&#xe1;niel Boros</a>, <a href="http://arxiv.org/find/stat/1/au:+Csanady_B/0/1/0/all/0/1">B&#xe1;lint Csan&#xe1;dy</a>, <a href="http://arxiv.org/find/stat/1/au:+Ivkovic_I/0/1/0/all/0/1">Iv&#xe1;n Ivkovic</a>, <a href="http://arxiv.org/find/stat/1/au:+Nagy_L/0/1/0/all/0/1">L&#xf3;r&#xe1;nt Nagy</a>, <a href="http://arxiv.org/find/stat/1/au:+Lukacs_A/0/1/0/all/0/1">Andr&#xe1;s Luk&#xe1;cs</a>, <a href="http://arxiv.org/find/stat/1/au:+Markus_L/0/1/0/all/0/1">L&#xe1;szl&#xf3; M&#xe1;rkus</a></p>
<p>This research explores the reliability of deep learning, specifically Long
Short-Term Memory (LSTM) networks, for estimating the Hurst parameter in
fractional stochastic processes. The study focuses on three types of processes:
fractional Brownian motion (fBm), fractional Ornstein-Uhlenbeck (fOU) process,
and linear fractional stable motions (lfsm). The work involves a fast
generation of extensive datasets for fBm and fOU to train the LSTM network on a
large volume of data in a feasible time. The study analyses the accuracy of the
LSTM network's Hurst parameter estimation regarding various performance
measures like RMSE, MAE, MRE, and quantiles of the absolute and relative
errors. It finds that LSTM outperforms the traditional statistical methods in
the case of fBm and fOU processes; however, it has limited accuracy on lfsm
processes. The research also delves into the implications of training length
and valuation sequence length on the LSTM's performance. The methodology is
applied by estimating the Hurst parameter in Li-ion battery degradation data
and obtaining confidence bounds for the estimation. The study concludes that
while deep learning methods show promise in parameter estimation of fractional
processes, their effectiveness is contingent on the process type and the
quality of training data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01792">CoMoSVC: Consistency Model-based Singing Voice Conversion. (arXiv:2401.01792v1 [eess.AS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Lu_Y/0/1/0/all/0/1">Yiwen Lu</a>, <a href="http://arxiv.org/find/eess/1/au:+Ye_Z/0/1/0/all/0/1">Zhen Ye</a>, <a href="http://arxiv.org/find/eess/1/au:+Xue_W/0/1/0/all/0/1">Wei Xue</a>, <a href="http://arxiv.org/find/eess/1/au:+Tan_X/0/1/0/all/0/1">Xu Tan</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1">Qifeng Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Guo_Y/0/1/0/all/0/1">Yike Guo</a></p>
<p>The diffusion-based Singing Voice Conversion (SVC) methods have achieved
remarkable performances, producing natural audios with high similarity to the
target timbre. However, the iterative sampling process results in slow
inference speed, and acceleration thus becomes crucial. In this paper, we
propose CoMoSVC, a consistency model-based SVC method, which aims to achieve
both high-quality generation and high-speed sampling. A diffusion-based teacher
model is first specially designed for SVC, and a student model is further
distilled under self-consistency properties to achieve one-step sampling.
Experiments on a single NVIDIA GTX4090 GPU reveal that although CoMoSVC has a
significantly faster inference speed than the state-of-the-art (SOTA)
diffusion-based SVC system, it still achieves comparable or superior conversion
performance based on both subjective and objective metrics. Audio samples and
codes are available at https://comosvc.github.io/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01801">A quatum inspired neural network for geometric modeling. (arXiv:2401.01801v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1">Weitao Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shengchao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1">Hongyu Guo</a></p>
<p>By conceiving physical systems as 3D many-body point clouds, geometric graph
neural networks (GNNs), such as SE(3)/E(3) equivalent GNNs, have showcased
promising performance. In particular, their effective message-passing mechanics
make them adept at modeling molecules and crystalline materials. However,
current geometric GNNs only offer a mean-field approximation of the many-body
system, encapsulated within two-body message passing, thus falling short in
capturing intricate relationships within these geometric graphs. To address
this limitation, tensor networks, widely employed by computational physics to
handle manybody systems using high-order tensors, have been introduced.
Nevertheless, integrating these tensorized networks into the message-passing
framework of GNNs faces scalability and symmetry conservation (e.g.,
permutation and rotation) challenges. In response, we introduce an innovative
equivariant Matrix Product State (MPS)-based message-passing strategy, through
achieving an efficient implementation of the tensor contraction operation. Our
method effectively models complex many-body relationships, suppressing
mean-field approximations, and captures symmetries within geometric graphs.
Importantly, it seamlessly replaces the standard message-passing and
layer-aggregation modules intrinsic to geometric GNNs. We empirically validate
the superior accuracy of our approach on benchmark tasks, including predicting
classical Newton systems and quantum tensor Hamiltonian matrices. To our
knowledge, our approach represents the inaugural utilization of parameterized
geometric tensor networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01813">Signal Processing in the Retina: Interpretable Graph Classifier to Predict Ganglion Cell Responses. (arXiv:2401.01813v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Parhizkar_Y/0/1/0/all/0/1">Yasaman Parhizkar</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheung_G/0/1/0/all/0/1">Gene Cheung</a>, <a href="http://arxiv.org/find/cs/1/au:+Eckford_A/0/1/0/all/0/1">Andrew W. Eckford</a></p>
<p>It is a popular hypothesis in neuroscience that ganglion cells in the retina
are activated by selectively detecting visual features in an observed scene.
While ganglion cell firings can be predicted via data-trained deep neural nets,
the networks remain indecipherable, thus providing little understanding of the
cells' underlying operations. To extract knowledge from the cell firings, in
this paper we learn an interpretable graph-based classifier from data to
predict the firings of ganglion cells in response to visual stimuli.
Specifically, we learn a positive semi-definite (PSD) metric matrix $\mathbf{M}
\succeq 0$ that defines Mahalanobis distances between graph nodes (visual
events) endowed with pre-computed feature vectors; the computed inter-node
distances lead to edge weights and a combinatorial graph that is amenable to
binary classification. Mathematically, we define the objective of metric matrix
$\mathbf{M}$ optimization using a graph adaptation of large margin nearest
neighbor (LMNN), which is rewritten as a semi-definite programming (SDP)
problem. We solve it efficiently via a fast approximation called Gershgorin
disc perfect alignment (GDPA) linearization. The learned metric matrix
$\mathbf{M}$ provides interpretability: important features are identified along
$\mathbf{M}$'s diagonal, and their mutual relationships are inferred from
off-diagonal terms. Our fast metric learning framework can be applied to other
biological systems with pre-chosen features that require interpretation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01830">Iterative Mask Filling: An Effective Text Augmentation Method Using Masked Language Modeling. (arXiv:2401.01830v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kesgin_H/0/1/0/all/0/1">Himmet Toprak Kesgin</a>, <a href="http://arxiv.org/find/cs/1/au:+Amasyali_M/0/1/0/all/0/1">Mehmet Fatih Amasyali</a></p>
<p>Data augmentation is an effective technique for improving the performance of
machine learning models. However, it has not been explored as extensively in
natural language processing (NLP) as it has in computer vision. In this paper,
we propose a novel text augmentation method that leverages the Fill-Mask
feature of the transformer-based BERT model. Our method involves iteratively
masking words in a sentence and replacing them with language model predictions.
We have tested our proposed method on various NLP tasks and found it to be
effective in many cases. Our results are presented along with a comparison to
existing augmentation methods. Experimental results show that our proposed
method significantly improves performance, especially on topic classification
datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01841">Act as You Learn: Adaptive Decision-Making in Non-Stationary Markov Decision Processes. (arXiv:2401.01841v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1">Baiting Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yunuo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dubey_A/0/1/0/all/0/1">Abhishek Dubey</a>, <a href="http://arxiv.org/find/cs/1/au:+Mukhopadhyay_A/0/1/0/all/0/1">Ayan Mukhopadhyay</a></p>
<p>A fundamental (and largely open) challenge in sequential decision-making is
dealing with non-stationary environments, where exogenous environmental
conditions change over time. Such problems are traditionally modeled as
non-stationary Markov decision processes (NSMDP). However, existing approaches
for decision-making in NSMDPs have two major shortcomings: first, they assume
that the updated environmental dynamics at the current time are known (although
future dynamics can change); and second, planning is largely pessimistic, i.e.,
the agent acts ``safely'' to account for the non-stationary evolution of the
environment. We argue that both these assumptions are invalid in practice --
updated environmental conditions are rarely known, and as the agent interacts
with the environment, it can learn about the updated dynamics and avoid being
pessimistic, at least in states whose dynamics it is confident about. We
present a heuristic search algorithm called \textit{Adaptive Monte Carlo Tree
Search (ADA-MCTS)} that addresses these challenges. We show that the agent can
learn the updated dynamics of the environment over time and then act as it
learns, i.e., if the agent is in a region of the state space about which it has
updated knowledge, it can avoid being pessimistic. To quantify ``updated
knowledge,'' we disintegrate the aleatoric and epistemic uncertainty in the
agent's updated belief and show how the agent can use these estimates for
decision-making. We compare the proposed approach with the multiple
state-of-the-art approaches in decision-making across multiple well-established
open-source problems and empirically show that our approach is faster and
highly adaptive without sacrificing safety.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01842">Wasserstein Nonnegative Tensor Factorization with Manifold Regularization. (arXiv:2401.01842v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jianyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1">Linruize Tang</a></p>
<p>Nonnegative tensor factorization (NTF) has become an important tool for
feature extraction and part-based representation with preserved intrinsic
structure information from nonnegative high-order data. However, the original
NTF methods utilize Euclidean or Kullback-Leibler divergence as the loss
function which treats each feature equally leading to the neglect of the
side-information of features. To utilize correlation information of features
and manifold information of samples, we introduce Wasserstein manifold
nonnegative tensor factorization (WMNTF), which minimizes the Wasserstein
distance between the distribution of input tensorial data and the distribution
of reconstruction. Although some researches about Wasserstein distance have
been proposed in nonnegative matrix factorization (NMF), they ignore the
spatial structure information of higher-order data. We use Wasserstein distance
(a.k.a Earth Mover's distance or Optimal Transport distance) as a metric and
add a graph regularizer to a latent factor. Experimental results demonstrate
the effectiveness of the proposed method compared with other NMF and NTF
methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01843">Investigating Semi-Supervised Learning Algorithms in Text Datasets. (arXiv:2401.01843v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kesgin_H/0/1/0/all/0/1">Himmet Toprak Kesgin</a>, <a href="http://arxiv.org/find/cs/1/au:+Amasyali_M/0/1/0/all/0/1">Mehmet Fatih Amasyali</a></p>
<p>Using large training datasets enhances the generalization capabilities of
neural networks. Semi-supervised learning (SSL) is useful when there are few
labeled data and a lot of unlabeled data. SSL methods that use data
augmentation are most successful for image datasets. In contrast, texts do not
have consistent augmentation methods as images. Consequently, methods that use
augmentation are not as effective in text data as they are in image data. In
this study, we compared SSL algorithms that do not require augmentation; these
are self-training, co-training, tri-training, and tri-training with
disagreement. In the experiments, we used 4 different text datasets for
different tasks. We examined the algorithms from a variety of perspectives by
asking experiment questions and suggested several improvements. Among the
algorithms, tri-training with disagreement showed the closest performance to
the Oracle; however, performance gap shows that new semi-supervised algorithms
or improvements in existing methods are needed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01846">DGDNN: Decoupled Graph Diffusion Neural Network for Stock Movement Prediction. (arXiv:2401.01846v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+You_Z/0/1/0/all/0/1">Zinuo You</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1">Zijian Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Bo_H/0/1/0/all/0/1">Hongbo Bo</a>, <a href="http://arxiv.org/find/cs/1/au:+Cartlidge_J/0/1/0/all/0/1">John Cartlidge</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Li Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1">Yan Ge</a></p>
<p>Forecasting future stock trends remains challenging for academia and industry
due to stochastic inter-stock dynamics and hierarchical intra-stock dynamics
influencing stock prices. In recent years, graph neural networks have achieved
remarkable performance in this problem by formulating multiple stocks as
graph-structured data. However, most of these approaches rely on artificially
defined factors to construct static stock graphs, which fail to capture the
intrinsic interdependencies between stocks that rapidly evolve. In addition,
these methods often ignore the hierarchical features of the stocks and lose
distinctive information within. In this work, we propose a novel graph learning
approach implemented without expert knowledge to address these issues. First,
our approach automatically constructs dynamic stock graphs by entropy-driven
edge generation from a signal processing perspective. Then, we further learn
task-optimal dependencies between stocks via a generalized graph diffusion
process on constructed stock graphs. Last, a decoupled representation learning
scheme is adopted to capture distinctive hierarchical intra-stock features.
Experimental results demonstrate substantial improvements over state-of-the-art
baselines on real-world datasets. Moreover, the ablation study and sensitivity
study further illustrate the effectiveness of the proposed method in modeling
the time-evolving inter-stock and intra-stock dynamics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01851">The Power of Training: How Different Neural Network Setups Influence the Energy Demand. (arXiv:2401.01851v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Geissler_D/0/1/0/all/0/1">Daniel Gei&#xdf;ler</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1">Bo Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Mengxi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Suh_S/0/1/0/all/0/1">Sungho Suh</a>, <a href="http://arxiv.org/find/cs/1/au:+Lukowicz_P/0/1/0/all/0/1">Paul Lukowicz</a></p>
<p>This work examines the effects of variations in machine learning training
regimes and learning paradigms on the corresponding energy consumption. While
increasing data availability and innovation in high-performance hardware fuels
the training of sophisticated models, it also supports the fading perception of
energy consumption and carbon emission. Therefore, the goal of this work is to
create awareness about the energy impact of general training parameters and
processes, from learning rate over batch size to knowledge transfer. Multiple
setups with different hyperparameter initializations are evaluated on two
different hardware configurations to obtain meaningful results. Experiments on
pretraining and multitask training are conducted on top of the baseline results
to determine their potential towards sustainable machine learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01854">Multilingual Instruction Tuning With Just a Pinch of Multilinguality. (arXiv:2401.01854v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shaham_U/0/1/0/all/0/1">Uri Shaham</a>, <a href="http://arxiv.org/find/cs/1/au:+Herzig_J/0/1/0/all/0/1">Jonathan Herzig</a>, <a href="http://arxiv.org/find/cs/1/au:+Aharoni_R/0/1/0/all/0/1">Roee Aharoni</a>, <a href="http://arxiv.org/find/cs/1/au:+Szpektor_I/0/1/0/all/0/1">Idan Szpektor</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsarfaty_R/0/1/0/all/0/1">Reut Tsarfaty</a>, <a href="http://arxiv.org/find/cs/1/au:+Eyal_M/0/1/0/all/0/1">Matan Eyal</a></p>
<p>As instruction-tuned large language models (LLMs) gain global adoption, their
ability to follow instructions in multiple languages becomes increasingly
crucial. One promising approach is cross-lingual transfer, where a model
acquires specific functionality on some language by finetuning on another
language. In this work, we investigate how multilinguality during instruction
tuning of a multilingual LLM affects instruction-following across languages. We
first show that many languages transfer some instruction-following capabilities
to other languages from even monolingual tuning. Furthermore, we find that only
40 multilingual examples in an English tuning set substantially improve
multilingual instruction-following, both in seen and unseen languages during
tuning. In general, we observe that models tuned on multilingual mixtures
exhibit comparable or superior performance in several languages compared to
monolingually tuned models, despite training on 10x fewer examples in those
languages. Finally, we find that increasing the number of languages in the
instruction tuning set from 1 to only 2, 3, or 4 increases cross-lingual
generalization. Our results suggest that building massively multilingual
instruction-tuned models can be done with only a very small set of multilingual
instruction-responses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01855">Transformer Neural Autoregressive Flows. (arXiv:2401.01855v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Patacchiola_M/0/1/0/all/0/1">Massimiliano Patacchiola</a>, <a href="http://arxiv.org/find/cs/1/au:+Shysheya_A/0/1/0/all/0/1">Aliaksandra Shysheya</a>, <a href="http://arxiv.org/find/cs/1/au:+Hofmann_K/0/1/0/all/0/1">Katja Hofmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Turner_R/0/1/0/all/0/1">Richard E. Turner</a></p>
<p>Density estimation, a central problem in machine learning, can be performed
using Normalizing Flows (NFs). NFs comprise a sequence of invertible
transformations, that turn a complex target distribution into a simple one, by
exploiting the change of variables theorem. Neural Autoregressive Flows (NAFs)
and Block Neural Autoregressive Flows (B-NAFs) are arguably the most perfomant
members of the NF family. However, they suffer scalability issues and training
instability due to the constraints imposed on the network structure. In this
paper, we propose a novel solution to these challenges by exploiting
transformers to define a new class of neural flows called Transformer Neural
Autoregressive Flows (T-NAFs). T-NAFs treat each dimension of a random variable
as a separate input token, using attention masking to enforce an autoregressive
constraint. We take an amortization-inspired approach where the transformer
outputs the parameters of an invertible transformation. The experimental
results demonstrate that T-NAFs consistently match or outperform NAFs and
B-NAFs across multiple datasets from the UCI benchmark. Remarkably, T-NAFs
achieve these results using an order of magnitude fewer parameters than
previous approaches, without composing multiple flows.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01857">Optimal cross-learning for contextual bandits with unknown context distributions. (arXiv:2401.01857v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1">Jon Schneider</a>, <a href="http://arxiv.org/find/cs/1/au:+Zimmert_J/0/1/0/all/0/1">Julian Zimmert</a></p>
<p>We consider the problem of designing contextual bandit algorithms in the
``cross-learning'' setting of Balseiro et al., where the learner observes the
loss for the action they play in all possible contexts, not just the context of
the current round. We specifically consider the setting where losses are chosen
adversarially and contexts are sampled i.i.d. from an unknown distribution. In
this setting, we resolve an open problem of Balseiro et al. by providing an
efficient algorithm with a nearly tight (up to logarithmic factors) regret
bound of $\widetilde{O}(\sqrt{TK})$, independent of the number of contexts. As
a consequence, we obtain the first nearly tight regret bounds for the problems
of learning to bid in first-price auctions (under unknown value distributions)
and sleeping bandits with a stochastic action set.
</p>
<p>At the core of our algorithm is a novel technique for coordinating the
execution of a learning algorithm over multiple epochs in such a way to remove
correlations between estimation of the unknown distribution and the actions
played by the algorithm. This technique may be of independent interest for
other learning problems involving estimation of an unknown context
distribution.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01862">A Vision Check-up for Language Models. (arXiv:2401.01862v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1">Pratyusha Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Shaham_T/0/1/0/all/0/1">Tamar Rott Shaham</a>, <a href="http://arxiv.org/find/cs/1/au:+Baradad_M/0/1/0/all/0/1">Manel Baradad</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1">Stephanie Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_Munoz_A/0/1/0/all/0/1">Adrian Rodriguez-Munoz</a>, <a href="http://arxiv.org/find/cs/1/au:+Duggal_S/0/1/0/all/0/1">Shivam Duggal</a>, <a href="http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1">Phillip Isola</a>, <a href="http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1">Antonio Torralba</a></p>
<p>What does learning to model relationships between strings teach large
language models (LLMs) about the visual world? We systematically evaluate LLMs'
abilities to generate and recognize an assortment of visual concepts of
increasing complexity and then demonstrate how a preliminary visual
representation learning system can be trained using models of text. As language
models lack the ability to consume or output visual information as pixels, we
use code to represent images in our study. Although LLM-generated images do not
look like natural images, results on image generation and the ability of models
to correct these generated images indicate that precise modeling of strings can
teach language models about numerous aspects of the visual world. Furthermore,
experiments on self-supervised visual representation learning, utilizing images
generated with text models, highlight the potential to train vision models
capable of making semantic assessments of natural images using just LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01867">Dataset Difficulty and the Role of Inductive Bias. (arXiv:2401.01867v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kwok_D/0/1/0/all/0/1">Devin Kwok</a>, <a href="http://arxiv.org/find/cs/1/au:+Anand_N/0/1/0/all/0/1">Nikhil Anand</a>, <a href="http://arxiv.org/find/cs/1/au:+Frankle_J/0/1/0/all/0/1">Jonathan Frankle</a>, <a href="http://arxiv.org/find/cs/1/au:+Dziugaite_G/0/1/0/all/0/1">Gintare Karolina Dziugaite</a>, <a href="http://arxiv.org/find/cs/1/au:+Rolnick_D/0/1/0/all/0/1">David Rolnick</a></p>
<p>Motivated by the goals of dataset pruning and defect identification, a
growing body of methods have been developed to score individual examples within
a dataset. These methods, which we call "example difficulty scores", are
typically used to rank or categorize examples, but the consistency of rankings
between different training runs, scoring methods, and model architectures is
generally unknown. To determine how example rankings vary due to these random
and controlled effects, we systematically compare different formulations of
scores over a range of runs and model architectures. We find that scores
largely share the following traits: they are noisy over individual runs of a
model, strongly correlated with a single notion of difficulty, and reveal
examples that range from being highly sensitive to insensitive to the inductive
biases of certain model architectures. Drawing from statistical genetics, we
develop a simple method for fingerprinting model architectures using a few
sensitive examples. These findings guide practitioners in maximizing the
consistency of their scores (e.g. by choosing appropriate scoring methods,
number of runs, and subsets of examples), and establishes comprehensive
baselines for evaluating scores in the future.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01869">On the hardness of learning under symmetries. (arXiv:2401.01869v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kiani_B/0/1/0/all/0/1">Bobak T. Kiani</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1">Thien Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Lawrence_H/0/1/0/all/0/1">Hannah Lawrence</a>, <a href="http://arxiv.org/find/cs/1/au:+Jegelka_S/0/1/0/all/0/1">Stefanie Jegelka</a>, <a href="http://arxiv.org/find/cs/1/au:+Weber_M/0/1/0/all/0/1">Melanie Weber</a></p>
<p>We study the problem of learning equivariant neural networks via gradient
descent. The incorporation of known symmetries ("equivariance") into neural
nets has empirically improved the performance of learning pipelines, in domains
ranging from biology to computer vision. However, a rich yet separate line of
learning theoretic research has demonstrated that actually learning shallow,
fully-connected (i.e. non-symmetric) networks has exponential complexity in the
correlational statistical query (CSQ) model, a framework encompassing gradient
descent. In this work, we ask: are known problem symmetries sufficient to
alleviate the fundamental hardness of learning neural nets with gradient
descent? We answer this question in the negative. In particular, we give lower
bounds for shallow graph neural networks, convolutional networks, invariant
polynomials, and frame-averaged networks for permutation subgroups, which all
scale either superpolynomially or exponentially in the relevant input
dimension. Therefore, in spite of the significant inductive bias imparted via
symmetry, actually learning the complete classes of functions represented by
equivariant neural networks via gradient descent remains hard.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01874">Graph Neural Networks for Surfactant Multi-Property Prediction. (arXiv:2401.01874v1 [physics.chem-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Brozos_C/0/1/0/all/0/1">Christoforos Brozos</a>, <a href="http://arxiv.org/find/physics/1/au:+Rittig_J/0/1/0/all/0/1">Jan G. Rittig</a>, <a href="http://arxiv.org/find/physics/1/au:+Bhattacharya_S/0/1/0/all/0/1">Sandip Bhattacharya</a>, <a href="http://arxiv.org/find/physics/1/au:+Akanny_E/0/1/0/all/0/1">Elie Akanny</a>, <a href="http://arxiv.org/find/physics/1/au:+Kohlmann_C/0/1/0/all/0/1">Christina Kohlmann</a>, <a href="http://arxiv.org/find/physics/1/au:+Mitsos_A/0/1/0/all/0/1">Alexander Mitsos</a></p>
<p>Surfactants are of high importance in different industrial sectors such as
cosmetics, detergents, oil recovery and drug delivery systems. Therefore, many
quantitative structure-property relationship (QSPR) models have been developed
for surfactants. Each predictive model typically focuses on one surfactant
class, mostly nonionics. Graph Neural Networks (GNNs) have exhibited a great
predictive performance for property prediction of ionic liquids, polymers and
drugs in general. Specifically for surfactants, GNNs can successfully predict
critical micelle concentration (CMC), a key surfactant property associated with
micellization. A key factor in the predictive ability of QSPR and GNN models is
the data available for training. Based on extensive literature search, we
create the largest available CMC database with 429 molecules and the first
large data collection for surface excess concentration ($\Gamma$$_{m}$),
another surfactant property associated with foaming, with 164 molecules. Then,
we develop GNN models to predict the CMC and $\Gamma$$_{m}$ and we explore
different learning approaches, i.e., single- and multi-task learning, as well
as different training strategies, namely ensemble and transfer learning. We
find that a multi-task GNN with ensemble learning trained on all $\Gamma$$_{m}$
and CMC data performs best. Finally, we test the ability of our CMC model to
generalize on industrial grade pure component surfactants. The GNN yields
highly accurate predictions for CMC, showing great potential for future
industrial applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01879">Theoretical guarantees on the best-of-n alignment policy. (arXiv:2401.01879v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Beirami_A/0/1/0/all/0/1">Ahmad Beirami</a>, <a href="http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1">Alekh Agarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1">Jonathan Berant</a>, <a href="http://arxiv.org/find/cs/1/au:+DAmour_A/0/1/0/all/0/1">Alexander D&#x27;Amour</a>, <a href="http://arxiv.org/find/cs/1/au:+Eisenstein_J/0/1/0/all/0/1">Jacob Eisenstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Nagpal_C/0/1/0/all/0/1">Chirag Nagpal</a>, <a href="http://arxiv.org/find/cs/1/au:+Suresh_A/0/1/0/all/0/1">Ananda Theertha Suresh</a></p>
<p>A simple and effective method for the alignment of generative models is the
best-of-$n$ policy, where $n$ samples are drawn from a base policy, and ranked
based on a reward function, and the highest ranking one is selected. A commonly
used analytical expression in the literature claims that the KL divergence
between the best-of-$n$ policy and the base policy is equal to $\log (n) -
(n-1)/n.$ We disprove the validity of this claim, and show that it is an upper
bound on the actual KL divergence. We also explore the tightness of this upper
bound in different regimes. Finally, we propose a new estimator for the KL
divergence and empirically show that it provides a tight approximation through
a few examples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01883">Mining Temporal Attack Patterns from Cyberthreat Intelligence Reports. (arXiv:2401.01883v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1">Md Rayhanur Rahman</a>, <a href="http://arxiv.org/find/cs/1/au:+Wroblewski_B/0/1/0/all/0/1">Brandon Wroblewski</a>, <a href="http://arxiv.org/find/cs/1/au:+Matthews_Q/0/1/0/all/0/1">Quinn Matthews</a>, <a href="http://arxiv.org/find/cs/1/au:+Morgan_B/0/1/0/all/0/1">Brantley Morgan</a>, <a href="http://arxiv.org/find/cs/1/au:+Menzies_T/0/1/0/all/0/1">Tim Menzies</a>, <a href="http://arxiv.org/find/cs/1/au:+Williams_L/0/1/0/all/0/1">Laurie Williams</a></p>
<p>Defending from cyberattacks requires practitioners to operate on high-level
adversary behavior. Cyberthreat intelligence (CTI) reports on past cyberattack
incidents describe the chain of malicious actions with respect to time. To
avoid repeating cyberattack incidents, practitioners must proactively identify
and defend against recurring chain of actions - which we refer to as temporal
attack patterns. Automatically mining the patterns among actions provides
structured and actionable information on the adversary behavior of past
cyberattacks. The goal of this paper is to aid security practitioners in
prioritizing and proactive defense against cyberattacks by mining temporal
attack patterns from cyberthreat intelligence reports. To this end, we propose
ChronoCTI, an automated pipeline for mining temporal attack patterns from
cyberthreat intelligence (CTI) reports of past cyberattacks. To construct
ChronoCTI, we build the ground truth dataset of temporal attack patterns and
apply state-of-the-art large language models, natural language processing, and
machine learning techniques. We apply ChronoCTI on a set of 713 CTI reports,
where we identify 124 temporal attack patterns - which we categorize into nine
pattern categories. We identify that the most prevalent pattern category is to
trick victim users into executing malicious code to initiate the attack,
followed by bypassing the anti-malware system in the victim network. Based on
the observed patterns, we advocate organizations to train users about
cybersecurity best practices, introduce immutable operating systems with
limited functionalities, and enforce multi-user authentications. Moreover, we
advocate practitioners to leverage the automated mining capability of ChronoCTI
and design countermeasures against the recurring attack patterns.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2108.02497">How to avoid machine learning pitfalls: a guide for academic researchers. (arXiv:2108.02497v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lones_M/0/1/0/all/0/1">Michael A. Lones</a></p>
<p>This document outlines some of the common mistakes that occur when using
machine learning, and what can be done to avoid them. Whilst it should be
accessible to anyone with a basic understanding of machine learning techniques,
it was originally written for research students, and focuses on issues that are
of particular concern within academic research, such as the need to do rigorous
comparisons and reach valid conclusions. It covers five stages of the machine
learning process: what to do before model building, how to reliably build
models, how to robustly evaluate models, how to compare models fairly, and how
to report results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2108.08454">Improving Human Sequential Decision-Making with Reinforcement Learning. (arXiv:2108.08454v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bastani_H/0/1/0/all/0/1">Hamsa Bastani</a>, <a href="http://arxiv.org/find/cs/1/au:+Bastani_O/0/1/0/all/0/1">Osbert Bastani</a>, <a href="http://arxiv.org/find/cs/1/au:+Sinchaisri_W/0/1/0/all/0/1">Wichinpong Park Sinchaisri</a></p>
<p>Workers spend a significant amount of time learning how to make good
decisions. Evaluating the efficacy of a given decision, however, can be
complicated -- e.g., decision outcomes are often long-term and relate to the
original decision in complex ways. Surprisingly, even though learning good
decision-making strategies is difficult, they can often be expressed in simple
and concise forms. Focusing on sequential decision-making, we design a novel
machine learning algorithm that is capable of extracting "best practices" from
trace data and conveying its insights to humans in the form of interpretable
"tips". Our algorithm selects the tip that best bridges the gap between the
actions taken by human workers and those taken by the optimal policy in a way
that accounts for which actions are consequential for achieving higher
performance. We evaluate our approach through a series of randomized controlled
experiments where participants manage a virtual kitchen. Our experiments show
that the tips generated by our algorithm can significantly improve human
performance relative to intuitive baselines. In addition, we discuss a number
of empirical insights that can help inform the design of algorithms intended
for human-AI interfaces. For instance, we find evidence that participants do
not simply blindly follow our tips; instead, they combine them with their own
experience to discover additional strategies for improving performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.00147">DIRA: Dynamic Domain Incremental Regularised Adaptation. (arXiv:2205.00147v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ghobrial_A/0/1/0/all/0/1">Abanoub Ghobrial</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1">Xuan Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Hond_D/0/1/0/all/0/1">Darryl Hond</a>, <a href="http://arxiv.org/find/cs/1/au:+Asgari_H/0/1/0/all/0/1">Hamid Asgari</a>, <a href="http://arxiv.org/find/cs/1/au:+Eder_K/0/1/0/all/0/1">Kerstin Eder</a></p>
<p>Autonomous systems (AS) often use Deep Neural Network (DNN) classifiers to
allow them to operate in complex, high-dimensional, non-linear, and dynamically
changing environments. Due to the complexity of these environments, DNN
classifiers may output misclassifications during operation when they face
domains not identified during development. Removing a system from operation for
retraining becomes impractical as the number of such AS increases. To increase
AS reliability and overcome this limitation, DNN classifiers need to have the
ability to adapt during operation when faced with different operational domains
using a few samples (e.g. 2 to 100 samples). However, retraining DNNs on a few
samples is known to cause catastrophic forgetting and poor generalisation. In
this paper, we introduce Dynamic Incremental Regularised Adaptation (DIRA), an
approach for dynamic operational domain adaption of DNNs using regularisation
techniques. We show that DIRA improves on the problem of forgetting and
achieves strong gains in performance when retraining using a few samples from
the target domain. Our approach shows improvements on different image
classification benchmarks aimed at evaluating robustness to distribution shifts
(e.g.CIFAR-10C/100C, ImageNet-C), and produces state-of-the-art performance in
comparison with other methods from the literature.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.15580">A Computation and Communication Efficient Method for Distributed Nonconvex Problems in the Partial Participation Setting. (arXiv:2205.15580v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tyurin_A/0/1/0/all/0/1">Alexander Tyurin</a>, <a href="http://arxiv.org/find/cs/1/au:+Richtarik_P/0/1/0/all/0/1">Peter Richt&#xe1;rik</a></p>
<p>We present a new method that includes three key components of distributed
optimization and federated learning: variance reduction of stochastic
gradients, partial participation, and compressed communication. We prove that
the new method has optimal oracle complexity and state-of-the-art communication
complexity in the partial participation setting. Regardless of the
communication compression feature, our method successfully combines variance
reduction and partial participation: we get the optimal oracle complexity,
never need the participation of all nodes, and do not require the bounded
gradients (dissimilarity) assumption.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.04688">A New Frontier of AI: On-Device AI Training and Personalization. (arXiv:2206.04688v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1">Ji Joong Moon</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hyeonseok Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_J/0/1/0/all/0/1">Jiho Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1">Donghak Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1">Seungbaek Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Seo_H/0/1/0/all/0/1">Hyungjun Seo</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeong_D/0/1/0/all/0/1">Donghyeon Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Kong_S/0/1/0/all/0/1">Sungsik Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Ham_M/0/1/0/all/0/1">MyungJoo Ham</a></p>
<p>Modern consumer electronic devices have started executing deep learning-based
intelligence services on devices, not cloud servers, to keep personal data on
devices and to reduce network and cloud costs. We find such a trend as the
opportunity to personalize intelligence services by updating neural networks
with user data without exposing the data out of devices: on-device training.
However, the limited resources of devices incurs significant difficulties. We
propose a light-weight on-device training framework, NNTrainer, which provides
highly memory-efficient neural network training techniques and proactive
swapping based on fine-grained execution order analysis for neural networks.
Moreover, its optimizations do not sacrifice accuracy and are transparent to
training algorithms; thus, prior algorithmic studies may be implemented on top
of NNTrainer. The evaluations show that NNTrainer can reduce memory consumption
down to 1/20 (saving 95%!) and effectively personalizes intelligence services
on devices. NNTrainer is cross-platform and practical open-source software,
which is being deployed to millions of mobile devices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.14650">SYNTA: A novel approach for deep learning-based image analysis in muscle histopathology using photo-realistic synthetic data. (arXiv:2207.14650v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Mill_L/0/1/0/all/0/1">Leonid Mill</a>, <a href="http://arxiv.org/find/eess/1/au:+Aust_O/0/1/0/all/0/1">Oliver Aust</a>, <a href="http://arxiv.org/find/eess/1/au:+Ackermann_J/0/1/0/all/0/1">Jochen A. Ackermann</a>, <a href="http://arxiv.org/find/eess/1/au:+Burger_P/0/1/0/all/0/1">Philipp Burger</a>, <a href="http://arxiv.org/find/eess/1/au:+Pascual_M/0/1/0/all/0/1">Monica Pascual</a>, <a href="http://arxiv.org/find/eess/1/au:+Palumbo_Zerr_K/0/1/0/all/0/1">Katrin Palumbo-Zerr</a>, <a href="http://arxiv.org/find/eess/1/au:+Kronke_G/0/1/0/all/0/1">Gerhard Kr&#xf6;nke</a>, <a href="http://arxiv.org/find/eess/1/au:+Uderhardt_S/0/1/0/all/0/1">Stefan Uderhardt</a>, <a href="http://arxiv.org/find/eess/1/au:+Schett_G/0/1/0/all/0/1">Georg Schett</a>, <a href="http://arxiv.org/find/eess/1/au:+Clemen_C/0/1/0/all/0/1">Christoph S. Clemen</a>, <a href="http://arxiv.org/find/eess/1/au:+Schroder_R/0/1/0/all/0/1">Rolf Schr&#xf6;der</a>, <a href="http://arxiv.org/find/eess/1/au:+Holtzhausen_C/0/1/0/all/0/1">Christian Holtzhausen</a>, <a href="http://arxiv.org/find/eess/1/au:+Jabari_S/0/1/0/all/0/1">Samir Jabari</a>, <a href="http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1">Andreas Maier</a>, <a href="http://arxiv.org/find/eess/1/au:+Gruneboom_A/0/1/0/all/0/1">Anika Gr&#xfc;neboom</a></p>
<p>Artificial intelligence (AI), machine learning, and deep learning (DL)
methods are becoming increasingly important in the field of biomedical image
analysis. However, to exploit the full potential of such methods, a
representative number of experimentally acquired images containing a
significant number of manually annotated objects is needed as training data.
Here we introduce SYNTA (synthetic data) as a novel approach for the generation
of synthetic, photo-realistic, and highly complex biomedical images as training
data for DL systems. We show the versatility of our approach in the context of
muscle fiber and connective tissue analysis in histological sections. We
demonstrate that it is possible to perform robust and expert-level segmentation
tasks on previously unseen real-world data, without the need for manual
annotations using synthetic training data alone. Being a fully parametric
technique, our approach poses an interpretable and controllable alternative to
Generative Adversarial Networks (GANs) and has the potential to significantly
accelerate quantitative image analysis in a variety of biomedical applications
in microscopy and beyond.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.10962">Prediction of good reaction coordinates and future evolution of MD trajectories using Regularized Sparse Autoencoders: A novel deep learning approach. (arXiv:2208.10962v2 [physics.chem-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Gupta_A/0/1/0/all/0/1">Abhijit Gupta</a></p>
<p>Identifying reaction coordinates(RCs) is an active area of research, given
the crucial role RCs play in determining the progress of a chemical reaction.
The choice of the reaction coordinate is often based on heuristic knowledge.
However, an essential criterion for the choice is that the coordinate should
capture both the reactant and product states unequivocally. Also, the
coordinate should be the slowest one so that all the other degrees of freedom
can easily equilibrate along the reaction coordinate. Also, the coordinate
should be the slowest one so that all the other degrees of freedom can easily
equilibrate along the reaction coordinate. We used a regularised sparse
autoencoder, an energy-based model, to discover a crucial set of reaction
coordinates. Along with discovering reaction coordinates, our model also
predicts the evolution of a molecular dynamics(MD) trajectory. We showcased
that including sparsity enforcing regularisation helps in choosing a small but
important set of reaction coordinates. We used two model systems to demonstrate
our approach: alanine dipeptide system and proflavine and DNA system, which
exhibited intercalation of proflavine into DNA minor groove in an aqueous
environment. We model MD trajectory as a multivariate time series, and our
latent variable model performs the task of multi-step time series prediction.
This idea is inspired by the popular sparse coding approach - to represent each
input sample as a linear combination of few elements taken from a set of
representative patterns.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.12511">Lower Difficulty and Better Robustness: A Bregman Divergence Perspective for Adversarial Training. (arXiv:2208.12511v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zihui Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1">Haichang Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1">Bingqian Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1">Xiaoyan Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shudong Zhang</a></p>
<p>In this paper, we investigate on improving the adversarial robustness
obtained in adversarial training (AT) via reducing the difficulty of
optimization. To better study this problem, we build a novel Bregman divergence
perspective for AT, in which AT can be viewed as the sliding process of the
training data points on the negative entropy curve. Based on this perspective,
we analyze the learning objectives of two typical AT methods, i.e., PGD-AT and
TRADES, and we find that the optimization process of TRADES is easier than
PGD-AT for that TRADES separates PGD-AT. In addition, we discuss the function
of entropy in TRADES, and we find that models with high entropy can be better
robustness learners. Inspired by the above findings, we propose two methods,
i.e., FAIT and MER, which can both not only reduce the difficulty of
optimization under the 10-step PGD adversaries, but also provide better
robustness. Our work suggests that reducing the difficulty of optimization
under the 10-step PGD adversaries is a promising approach for enhancing the
adversarial robustness in AT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.12061">Validation of Composite Systems by Discrepancy Propagation. (arXiv:2210.12061v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Reeb_D/0/1/0/all/0/1">David Reeb</a>, <a href="http://arxiv.org/find/cs/1/au:+Patel_K/0/1/0/all/0/1">Kanil Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Barsim_K/0/1/0/all/0/1">Karim Barsim</a>, <a href="http://arxiv.org/find/cs/1/au:+Schiegg_M/0/1/0/all/0/1">Martin Schiegg</a>, <a href="http://arxiv.org/find/cs/1/au:+Gerwinn_S/0/1/0/all/0/1">Sebastian Gerwinn</a></p>
<p>Assessing the validity of a real-world system with respect to given quality
criteria is a common yet costly task in industrial applications due to the vast
number of required real-world tests. Validating such systems by means of
simulation offers a promising and less expensive alternative, but requires an
assessment of the simulation accuracy and therefore end-to-end measurements.
Additionally, covariate shifts between simulations and actual usage can cause
difficulties for estimating the reliability of such systems. In this work, we
present a validation method that propagates bounds on distributional
discrepancy measures through a composite system, thereby allowing us to derive
an upper bound on the failure probability of the real system from potentially
inaccurate simulations. Each propagation step entails an optimization problem,
where -- for measures such as maximum mean discrepancy (MMD) -- we develop
tight convex relaxations based on semidefinite programs. We demonstrate that
our propagation method yields valid and useful bounds for composite systems
exhibiting a variety of realistic effects. In particular, we show that the
proposed method can successfully account for data shifts within the
experimental design as well as model inaccuracies within the simulation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.12282">Bridging the Gap Between Target Networks and Functional Regularization. (arXiv:2210.12282v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Piche_A/0/1/0/all/0/1">Alexandre Piche</a>, <a href="http://arxiv.org/find/cs/1/au:+Thomas_V/0/1/0/all/0/1">Valentin Thomas</a>, <a href="http://arxiv.org/find/cs/1/au:+Marino_J/0/1/0/all/0/1">Joseph Marino</a>, <a href="http://arxiv.org/find/cs/1/au:+Pardinas_R/0/1/0/all/0/1">Rafael Pardinas</a>, <a href="http://arxiv.org/find/cs/1/au:+Marconi_G/0/1/0/all/0/1">Gian Maria Marconi</a>, <a href="http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1">Christopher Pal</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1">Mohammad Emtiyaz Khan</a></p>
<p>Bootstrapping is behind much of the successes of Deep Reinforcement Learning.
However, learning the value function via bootstrapping often leads to unstable
training due to fast-changing target values. Target Networks are employed to
stabilize training by using an additional set of lagging parameters to estimate
the target values. Despite the popularity of Target Networks, their effect on
the optimization is still misunderstood. In this work, we show that they act as
an implicit regularizer. This regularizer has disadvantages such as being
inflexible and non convex. To overcome these issues, we propose an explicit
Functional Regularization that is a convex regularizer in function space and
can easily be tuned. We analyze the convergence of our method theoretically and
empirically demonstrate that replacing Target Networks with the more
theoretically grounded Functional Regularization approach leads to better
sample efficiency and performance improvements.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.13702">DeXtreme: Transfer of Agile In-hand Manipulation from Simulation to Reality. (arXiv:2210.13702v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Handa_A/0/1/0/all/0/1">Ankur Handa</a>, <a href="http://arxiv.org/find/cs/1/au:+Allshire_A/0/1/0/all/0/1">Arthur Allshire</a>, <a href="http://arxiv.org/find/cs/1/au:+Makoviychuk_V/0/1/0/all/0/1">Viktor Makoviychuk</a>, <a href="http://arxiv.org/find/cs/1/au:+Petrenko_A/0/1/0/all/0/1">Aleksei Petrenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1">Ritvik Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jingzhou Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Makoviichuk_D/0/1/0/all/0/1">Denys Makoviichuk</a>, <a href="http://arxiv.org/find/cs/1/au:+Wyk_K/0/1/0/all/0/1">Karl Van Wyk</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhurkevich_A/0/1/0/all/0/1">Alexander Zhurkevich</a>, <a href="http://arxiv.org/find/cs/1/au:+Sundaralingam_B/0/1/0/all/0/1">Balakumar Sundaralingam</a>, <a href="http://arxiv.org/find/cs/1/au:+Narang_Y/0/1/0/all/0/1">Yashraj Narang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lafleche_J/0/1/0/all/0/1">Jean-Francois Lafleche</a>, <a href="http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1">Dieter Fox</a>, <a href="http://arxiv.org/find/cs/1/au:+State_G/0/1/0/all/0/1">Gavriel State</a></p>
<p>Recent work has demonstrated the ability of deep reinforcement learning (RL)
algorithms to learn complex robotic behaviours in simulation, including in the
domain of multi-fingered manipulation. However, such models can be challenging
to transfer to the real world due to the gap between simulation and reality. In
this paper, we present our techniques to train a) a policy that can perform
robust dexterous manipulation on an anthropomorphic robot hand and b) a robust
pose estimator suitable for providing reliable real-time information on the
state of the object being manipulated. Our policies are trained to adapt to a
wide range of conditions in simulation. Consequently, our vision-based policies
significantly outperform the best vision policies in the literature on the same
reorientation task and are competitive with policies that are given privileged
state information via motion capture systems. Our work reaffirms the
possibilities of sim-to-real transfer for dexterous manipulation in diverse
kinds of hardware and simulator setups, and in our case, with the Allegro Hand
and Isaac Gym GPU-based simulation. Furthermore, it opens up possibilities for
researchers to achieve such results with commonly-available, affordable robot
hands and cameras. Videos of the resulting policy and supplementary
information, including experiments and demos, can be found at
https://dextreme.org/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.00086">Disentangled (Un)Controllable Features. (arXiv:2211.00086v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kooi_J/0/1/0/all/0/1">Jacob E. Kooi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoogendoorn_M/0/1/0/all/0/1">Mark Hoogendoorn</a>, <a href="http://arxiv.org/find/cs/1/au:+Francois_Lavet_V/0/1/0/all/0/1">Vincent Fran&#xe7;ois-Lavet</a></p>
<p>In the context of MDPs with high-dimensional states, downstream tasks are
predominantly applied on a compressed, low-dimensional representation of the
original input space. A variety of learning objectives have therefore been used
to attain useful representations. However, these representations usually lack
interpretability of the different features. We present a novel approach that is
able to disentangle latent features into a controllable and an uncontrollable
partition. We illustrate that the resulting partitioned representations are
easily interpretable on three types of environments and show that, in a
distribution of procedurally generated maze environments, it is feasible to
interpretably employ a planning algorithm in the isolated controllable latent
partition.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.03932">Low Variance Off-policy Evaluation with State-based Importance Sampling. (arXiv:2212.03932v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bossens_D/0/1/0/all/0/1">David M. Bossens</a>, <a href="http://arxiv.org/find/cs/1/au:+Thomas_P/0/1/0/all/0/1">Philip S. Thomas</a></p>
<p>In off-policy reinforcement learning, a behaviour policy performs exploratory
interactions with the environment to obtain state-action-reward samples which
are then used to learn a target policy that optimises the expected return. This
leads to a problem of off-policy evaluation, where one needs to evaluate the
target policy from samples collected by the often unrelated behaviour policy.
Importance sampling is a traditional statistical technique that is often
applied to off-policy evaluation. While importance sampling estimators are
unbiased, their variance increases exponentially with the horizon of the
decision process due to computing the importance weight as a product of action
probability ratios, yielding estimates with low accuracy for domains involving
long-term planning. This paper proposes state-based importance sampling, which
drops the action probability ratios of sub-trajectories with ``negligible
states'' -- roughly speaking, those for which the chosen actions have no impact
on the return estimate -- from the computation of the importance weight.
Theoretical results show this reduces the ordinary importance sampling variance
from $O(\exp(H))$ to $O(\exp(X))$ where $X &lt; H$ is the largest subtrajectory
with non-negligible states. To identify negligible states, two search
algorithms are proposed, one based on covariance testing and one based on
state-action values. We formulate state-based variants of ordinary importance
sampling, weighted importance sampling, per-decision importance sampling,
incremental importance sampling, doubly robust off-policy evaluation, and
stationary density ratio estimation. Experiments in four distinct domains show
that state-based methods consistently yield reduced variance and improved
accuracy compared to their traditional counterparts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.05987">Selective classification using a robust meta-learning approach. (arXiv:2212.05987v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jain_N/0/1/0/all/0/1">Nishant Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Shanmugam_K/0/1/0/all/0/1">Karthikeyan Shanmugam</a>, <a href="http://arxiv.org/find/cs/1/au:+Shenoy_P/0/1/0/all/0/1">Pradeep Shenoy</a></p>
<p>Predictive uncertainty-a model's self awareness regarding its accuracy on an
input-is key for both building robust models via training interventions and for
test-time applications such as selective classification. We propose a novel
instance-conditioned reweighting approach that captures predictive uncertainty
using an auxiliary network and unifies these train- and test-time applications.
The auxiliary network is trained using a meta-objective in a bilevel
optimization framework. A key contribution of our proposal is the
meta-objective of minimizing the dropout variance, an approximation of Bayesian
Predictive uncertainty. We show in controlled experiments that we effectively
capture the diverse specific notions of uncertainty through this
meta-objective, while previous approaches only capture certain aspects. These
results translate to significant gains in real-world settings-selective
classification, label noise, domain adaptation, calibration-and across
datasets-Imagenet, Cifar100, diabetic retinopathy, Camelyon, WILDs,
Imagenet-C,-A,-R, Clothing1M, etc. For Diabetic Retinopathy, we see upto
3.4%/3.3% accuracy and AUC gains over SOTA in selective classification. We also
improve upon large-scale pretrained models such as PLEX.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.08123">Bayesian posterior approximation with stochastic ensembles. (arXiv:2212.08123v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Balabanov_O/0/1/0/all/0/1">Oleksandr Balabanov</a>, <a href="http://arxiv.org/find/cs/1/au:+Mehlig_B/0/1/0/all/0/1">Bernhard Mehlig</a>, <a href="http://arxiv.org/find/cs/1/au:+Linander_H/0/1/0/all/0/1">Hampus Linander</a></p>
<p>We introduce ensembles of stochastic neural networks to approximate the
Bayesian posterior, combining stochastic methods such as dropout with deep
ensembles. The stochastic ensembles are formulated as families of distributions
and trained to approximate the Bayesian posterior with variational inference.
We implement stochastic ensembles based on Monte Carlo dropout, DropConnect and
a novel non-parametric version of dropout and evaluate them on a toy problem
and CIFAR image classification. For both tasks, we test the quality of the
posteriors directly against Hamiltonian Monte Carlo simulations. Our results
show that stochastic ensembles provide more accurate posterior estimates than
other popular baselines for Bayesian inference.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.03421">A unified recipe for deriving (time-uniform) PAC-Bayes bounds. (arXiv:2302.03421v5 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Chugg_B/0/1/0/all/0/1">Ben Chugg</a>, <a href="http://arxiv.org/find/stat/1/au:+Wang_H/0/1/0/all/0/1">Hongjian Wang</a>, <a href="http://arxiv.org/find/stat/1/au:+Ramdas_A/0/1/0/all/0/1">Aaditya Ramdas</a></p>
<p>We present a unified framework for deriving PAC-Bayesian generalization
bounds. Unlike most previous literature on this topic, our bounds are
anytime-valid (i.e., time-uniform), meaning that they hold at all stopping
times, not only for a fixed sample size. Our approach combines four tools in
the following order: (a) nonnegative supermartingales or reverse
submartingales, (b) the method of mixtures, (c) the Donsker-Varadhan formula
(or other convex duality principles), and (d) Ville's inequality. Our main
result is a PAC-Bayes theorem which holds for a wide class of discrete
stochastic processes. We show how this result implies time-uniform versions of
well-known classical PAC-Bayes bounds, such as those of Seeger, McAllester,
Maurer, and Catoni, in addition to many recent bounds. We also present several
novel bounds. Our framework also enables us to relax traditional assumptions;
in particular, we consider nonstationary loss functions and non-i.i.d. data. In
sum, we unify the derivation of past bounds and ease the search for future
bounds: one may simply check if our supermartingale or submartingale conditions
are met and, if so, be guaranteed a (time-uniform) PAC-Bayes bound.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.00011">Adversarial Representation Learning for Robust Privacy Preservation in Audio. (arXiv:2305.00011v2 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gharib_S/0/1/0/all/0/1">Shayan Gharib</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1">Minh Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Luong_D/0/1/0/all/0/1">Diep Luong</a>, <a href="http://arxiv.org/find/cs/1/au:+Drossos_K/0/1/0/all/0/1">Konstantinos Drossos</a>, <a href="http://arxiv.org/find/cs/1/au:+Virtanen_T/0/1/0/all/0/1">Tuomas Virtanen</a></p>
<p>Sound event detection systems are widely used in various applications such as
surveillance and environmental monitoring where data is automatically
collected, processed, and sent to a cloud for sound recognition. However, this
process may inadvertently reveal sensitive information about users or their
surroundings, hence raising privacy concerns. In this study, we propose a novel
adversarial training method for learning representations of audio recordings
that effectively prevents the detection of speech activity from the latent
features of the recordings. The proposed method trains a model to generate
invariant latent representations of speech-containing audio recordings that
cannot be distinguished from non-speech recordings by a speech classifier. The
novelty of our work is in the optimization algorithm, where the speech
classifier's weights are regularly replaced with the weights of classifiers
trained in a supervised manner. This increases the discrimination power of the
speech classifier constantly during the adversarial training, motivating the
model to generate latent representations in which speech is not
distinguishable, even using new speech classifiers trained outside the
adversarial training loop. The proposed method is evaluated against a baseline
approach with no privacy measures and a prior adversarial training method,
demonstrating a significant reduction in privacy violations compared to the
baseline approach. Additionally, we show that the prior adversarial method is
practically ineffective for this purpose.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.11348">In the Name of Fairness: Assessing the Bias in Clinical Record De-identification. (arXiv:2305.11348v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1">Yuxin Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1">Shulammite Lim</a>, <a href="http://arxiv.org/find/cs/1/au:+Pollard_T/0/1/0/all/0/1">Tom Joseph Pollard</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghassemi_M/0/1/0/all/0/1">Marzyeh Ghassemi</a></p>
<p>Data sharing is crucial for open science and reproducible research, but the
legal sharing of clinical data requires the removal of protected health
information from electronic health records. This process, known as
de-identification, is often achieved through the use of machine learning
algorithms by many commercial and open-source systems. While these systems have
shown compelling results on average, the variation in their performance across
different demographic groups has not been thoroughly examined. In this work, we
investigate the bias of de-identification systems on names in clinical notes
via a large-scale empirical analysis. To achieve this, we create 16 name sets
that vary along four demographic dimensions: gender, race, name popularity, and
the decade of popularity. We insert these names into 100 manually curated
clinical templates and evaluate the performance of nine public and private
de-identification methods. Our findings reveal that there are statistically
significant performance gaps along a majority of the demographic dimensions in
most methods. We further illustrate that de-identification quality is affected
by polysemy in names, gender context, and clinical note characteristics. To
mitigate the identified gaps, we propose a simple and method-agnostic solution
by fine-tuning de-identification methods with clinical context and diverse
names. Overall, it is imperative to address the bias in existing methods
immediately so that downstream stakeholders can build high-quality systems to
serve all demographic parties fairly.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.17033">The Brain Tumor Segmentation (BraTS) Challenge 2023: Focus on Pediatrics (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs). (arXiv:2305.17033v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Kazerooni_A/0/1/0/all/0/1">Anahita Fathi Kazerooni</a>, <a href="http://arxiv.org/find/eess/1/au:+Khalili_N/0/1/0/all/0/1">Nastaran Khalili</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1">Xinyang Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Haldar_D/0/1/0/all/0/1">Debanjan Haldar</a>, <a href="http://arxiv.org/find/eess/1/au:+Jiang_Z/0/1/0/all/0/1">Zhifan Jiang</a>, <a href="http://arxiv.org/find/eess/1/au:+Anwar_S/0/1/0/all/0/1">Syed Muhammed Anwar</a>, <a href="http://arxiv.org/find/eess/1/au:+Albrecht_J/0/1/0/all/0/1">Jake Albrecht</a>, <a href="http://arxiv.org/find/eess/1/au:+Adewole_M/0/1/0/all/0/1">Maruf Adewole</a>, <a href="http://arxiv.org/find/eess/1/au:+Anazodo_U/0/1/0/all/0/1">Udunna Anazodo</a>, <a href="http://arxiv.org/find/eess/1/au:+Anderson_H/0/1/0/all/0/1">Hannah Anderson</a>, <a href="http://arxiv.org/find/eess/1/au:+Bagheri_S/0/1/0/all/0/1">Sina Bagheri</a>, <a href="http://arxiv.org/find/eess/1/au:+Baid_U/0/1/0/all/0/1">Ujjwal Baid</a>, <a href="http://arxiv.org/find/eess/1/au:+Bergquist_T/0/1/0/all/0/1">Timothy Bergquist</a>, <a href="http://arxiv.org/find/eess/1/au:+Borja_A/0/1/0/all/0/1">Austin J. Borja</a>, <a href="http://arxiv.org/find/eess/1/au:+Calabrese_E/0/1/0/all/0/1">Evan Calabrese</a>, <a href="http://arxiv.org/find/eess/1/au:+Chung_V/0/1/0/all/0/1">Verena Chung</a>, <a href="http://arxiv.org/find/eess/1/au:+Conte_G/0/1/0/all/0/1">Gian-Marco Conte</a>, <a href="http://arxiv.org/find/eess/1/au:+Dako_F/0/1/0/all/0/1">Farouk Dako</a>, <a href="http://arxiv.org/find/eess/1/au:+Eddy_J/0/1/0/all/0/1">James Eddy</a>, <a href="http://arxiv.org/find/eess/1/au:+Ezhov_I/0/1/0/all/0/1">Ivan Ezhov</a>, <a href="http://arxiv.org/find/eess/1/au:+Familiar_A/0/1/0/all/0/1">Ariana Familiar</a>, <a href="http://arxiv.org/find/eess/1/au:+Farahani_K/0/1/0/all/0/1">Keyvan Farahani</a>, <a href="http://arxiv.org/find/eess/1/au:+Haldar_S/0/1/0/all/0/1">Shuvanjan Haldar</a>, <a href="http://arxiv.org/find/eess/1/au:+Iglesias_J/0/1/0/all/0/1">Juan Eugenio Iglesias</a>, <a href="http://arxiv.org/find/eess/1/au:+Janas_A/0/1/0/all/0/1">Anastasia Janas</a>, <a href="http://arxiv.org/find/eess/1/au:+Johansen_E/0/1/0/all/0/1">Elaine Johansen</a>, <a href="http://arxiv.org/find/eess/1/au:+Jones_B/0/1/0/all/0/1">Blaise V Jones</a>, <a href="http://arxiv.org/find/eess/1/au:+Kofler_F/0/1/0/all/0/1">Florian Kofler</a>, <a href="http://arxiv.org/find/eess/1/au:+LaBella_D/0/1/0/all/0/1">Dominic LaBella</a>, <a href="http://arxiv.org/find/eess/1/au:+Lai_H/0/1/0/all/0/1">Hollie Anne Lai</a>, <a href="http://arxiv.org/find/eess/1/au:+Leemput_K/0/1/0/all/0/1">Koen Van Leemput</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1">Hongwei Bran Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Maleki_N/0/1/0/all/0/1">Nazanin Maleki</a>, <a href="http://arxiv.org/find/eess/1/au:+McAllister_A/0/1/0/all/0/1">Aaron S McAllister</a>, <a href="http://arxiv.org/find/eess/1/au:+Meier_Z/0/1/0/all/0/1">Zeke Meier</a>, <a href="http://arxiv.org/find/eess/1/au:+Menze_B/0/1/0/all/0/1">Bjoern Menze</a>, <a href="http://arxiv.org/find/eess/1/au:+Moawad_A/0/1/0/all/0/1">Ahmed W Moawad</a>, <a href="http://arxiv.org/find/eess/1/au:+Nandolia_K/0/1/0/all/0/1">Khanak K Nandolia</a>, <a href="http://arxiv.org/find/eess/1/au:+Pavaine_J/0/1/0/all/0/1">Julija Pavaine</a>, <a href="http://arxiv.org/find/eess/1/au:+Piraud_M/0/1/0/all/0/1">Marie Piraud</a>, <a href="http://arxiv.org/find/eess/1/au:+Poussaint_T/0/1/0/all/0/1">Tina Poussaint</a>, <a href="http://arxiv.org/find/eess/1/au:+Prabhu_S/0/1/0/all/0/1">Sanjay P Prabhu</a>, <a href="http://arxiv.org/find/eess/1/au:+Reitman_Z/0/1/0/all/0/1">Zachary Reitman</a>, <a href="http://arxiv.org/find/eess/1/au:+Rodriguez_A/0/1/0/all/0/1">Andres Rodriguez</a>, <a href="http://arxiv.org/find/eess/1/au:+Rudie_J/0/1/0/all/0/1">Jeffrey D Rudie</a>, <a href="http://arxiv.org/find/eess/1/au:+Shaikh_I/0/1/0/all/0/1">Ibraheem Salman Shaikh</a>, <a href="http://arxiv.org/find/eess/1/au:+Shah_L/0/1/0/all/0/1">Lubdha M. Shah</a>, <a href="http://arxiv.org/find/eess/1/au:+Sheth_N/0/1/0/all/0/1">Nakul Sheth</a>, <a href="http://arxiv.org/find/eess/1/au:+Shinohara_R/0/1/0/all/0/1">Russel Taki Shinohara</a>, et al. (23 additional authors not shown)</p>
<p>Pediatric tumors of the central nervous system are the most common cause of
cancer-related death in children. The five-year survival rate for high-grade
gliomas in children is less than 20\%. Due to their rarity, the diagnosis of
these entities is often delayed, their treatment is mainly based on historic
treatment concepts, and clinical trials require multi-institutional
collaborations. The MICCAI Brain Tumor Segmentation (BraTS) Challenge is a
landmark community benchmark event with a successful history of 12 years of
resource creation for the segmentation and analysis of adult glioma. Here we
present the CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023 challenge, which
represents the first BraTS challenge focused on pediatric brain tumors with
data acquired across multiple international consortia dedicated to pediatric
neuro-oncology and clinical trials. The BraTS-PEDs 2023 challenge focuses on
benchmarking the development of volumentric segmentation algorithms for
pediatric brain glioma through standardized quantitative performance evaluation
metrics utilized across the BraTS 2023 cluster of challenges. Models gaining
knowledge from the BraTS-PEDs multi-parametric structural MRI (mpMRI) training
data will be evaluated on separate validation and unseen test mpMRI dataof
high-grade pediatric glioma. The CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023
challenge brings together clinicians and AI/imaging scientists to lead to
faster development of automated segmentation techniques that could benefit
clinical trials, and ultimately the care of children with brain tumors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19555">Large Language Models Are Not Strong Abstract Reasoners. (arXiv:2305.19555v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gendron_G/0/1/0/all/0/1">Ga&#xeb;l Gendron</a>, <a href="http://arxiv.org/find/cs/1/au:+Bao_Q/0/1/0/all/0/1">Qiming Bao</a>, <a href="http://arxiv.org/find/cs/1/au:+Witbrock_M/0/1/0/all/0/1">Michael Witbrock</a>, <a href="http://arxiv.org/find/cs/1/au:+Dobbie_G/0/1/0/all/0/1">Gillian Dobbie</a></p>
<p>Large Language Models have shown tremendous performance on a large variety of
natural language processing tasks, ranging from text comprehension to common
sense reasoning. However, the mechanisms responsible for this success remain
opaque, and it is unclear whether LLMs can achieve human-like cognitive
capabilities or whether these models are still fundamentally circumscribed.
Abstract reasoning is a fundamental task for cognition, consisting of finding
and applying a general pattern from few data. Evaluating deep neural
architectures on this task could give insight into their potential limitations
regarding reasoning and their broad generalisation abilities, yet this is
currently an under-explored area. In this paper, we introduce a new benchmark
for evaluating language models beyond memorization on abstract reasoning tasks.
We perform extensive evaluations of state-of-the-art LLMs, showing that they
currently achieve very limited performance in contrast with other natural
language tasks, even when applying techniques that have been shown to improve
performance on other NLP tasks. We argue that guiding LLM generation to follow
causal paths could help improve the generalisation and reasoning abilities of
LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.00732">Sharper Bounds for $\ell_p$ Sensitivity Sampling. (arXiv:2306.00732v2 [cs.DS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1">David P. Woodruff</a>, <a href="http://arxiv.org/find/cs/1/au:+Yasuda_T/0/1/0/all/0/1">Taisuke Yasuda</a></p>
<p>In large scale machine learning, random sampling is a popular way to
approximate datasets by a small representative subset of examples. In
particular, sensitivity sampling is an intensely studied technique which
provides provable guarantees on the quality of approximation, while reducing
the number of examples to the product of the VC dimension $d$ and the total
sensitivity $\mathfrak S$ in remarkably general settings. However, guarantees
going beyond this general bound of $\mathfrak S d$ are known in perhaps only
one setting, for $\ell_2$ subspace embeddings, despite intense study of
sensitivity sampling in prior work. In this work, we show the first bounds for
sensitivity sampling for $\ell_p$ subspace embeddings for $p &gt; 2$ that improve
over the general $\mathfrak S d$ bound, achieving a bound of roughly $\mathfrak
S^{2-2/p}$ for $2&lt;p&lt;\infty$. Furthermore, our techniques yield further new
results in the study of sampling algorithms, showing that the root leverage
score sampling algorithm achieves a bound of roughly $d$ for $1\leq p&lt;2$, and
that a combination of leverage score and sensitivity sampling achieves an
improved bound of roughly $d^{2/p}\mathfrak S^{2-4/p}$ for $2&lt;p&lt;\infty$. Our
sensitivity sampling results yield the best known sample complexity for a wide
class of structured matrices that have small $\ell_p$ sensitivity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.07618">Hyperbolic Graph Diffusion Model. (arXiv:2306.07618v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1">Lingfeng Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1">Xuan Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ouyang_M/0/1/0/all/0/1">Mingjie Ouyang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1">Xiangxiang Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jian Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1">Daxin Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Mingsong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1">Xian Wei</a></p>
<p>Diffusion generative models (DMs) have achieved promising results in image
and graph generation. However, real-world graphs, such as social networks,
molecular graphs, and traffic graphs, generally share non-Euclidean topologies
and hidden hierarchies. For example, the degree distributions of graphs are
mostly power-law distributions. The current latent diffusion model embeds the
hierarchical data in a Euclidean space, which leads to distortions and
interferes with modeling the distribution. Instead, hyperbolic space has been
found to be more suitable for capturing complex hierarchical structures due to
its exponential growth property. In order to simultaneously utilize the data
generation capabilities of diffusion models and the ability of hyperbolic
embeddings to extract latent hierarchical distributions, we propose a novel
graph generation method called, Hyperbolic Graph Diffusion Model (HGDM), which
consists of an auto-encoder to encode nodes into successive hyperbolic
embeddings, and a DM that operates in the hyperbolic latent space. HGDM
captures the crucial graph structure distributions by constructing a hyperbolic
potential node space that incorporates edge information. Extensive experiments
show that HGDM achieves better performance in generic graph and molecule
generation benchmarks, with a $48\%$ improvement in the quality of graph
generation with highly hierarchical structures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.07774">The Rank-Reduced Kalman Filter: Approximate Dynamical-Low-Rank Filtering In High Dimensions. (arXiv:2306.07774v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Schmidt_J/0/1/0/all/0/1">Jonathan Schmidt</a>, <a href="http://arxiv.org/find/stat/1/au:+Hennig_P/0/1/0/all/0/1">Philipp Hennig</a>, <a href="http://arxiv.org/find/stat/1/au:+Nick_J/0/1/0/all/0/1">J&#xf6;rg Nick</a>, <a href="http://arxiv.org/find/stat/1/au:+Tronarp_F/0/1/0/all/0/1">Filip Tronarp</a></p>
<p>Inference and simulation in the context of high-dimensional dynamical systems
remain computationally challenging problems. Some form of dimensionality
reduction is required to make the problem tractable in general. In this paper,
we propose a novel approximate Gaussian filtering and smoothing method which
propagates low-rank approximations of the covariance matrices. This is
accomplished by projecting the Lyapunov equations associated with the
prediction step to a manifold of low-rank matrices, which are then solved by a
recently developed, numerically stable, dynamical low-rank integrator.
Meanwhile, the update steps are made tractable by noting that the covariance
update only transforms the column space of the covariance matrix, which is
low-rank by construction. The algorithm differentiates itself from existing
ensemble-based approaches in that the low-rank approximations of the covariance
matrices are deterministic, rather than stochastic. Crucially, this enables the
method to reproduce the exact Kalman filter as the low-rank dimension
approaches the true dimensionality of the problem. Our method reduces
computational complexity from cubic (for the Kalman filter) to \emph{quadratic}
in the state-space size in the worst-case, and can achieve \emph{linear}
complexity if the state-space model satisfies certain criteria. Through a set
of experiments in classical data-assimilation and spatio-temporal regression,
we show that the proposed method consistently outperforms the ensemble-based
methods in terms of error in the mean and covariance with respect to the exact
Kalman filter. This comes at no additional cost in terms of asymptotic
computational complexity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.00859">CardiGraphormer: Unveiling the Power of Self-Supervised Learning in Revolutionizing Drug Discovery. (arXiv:2307.00859v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1">Abhijit Gupta</a></p>
<p>In the expansive realm of drug discovery, with approximately 15,000 known
drugs and only around 4,200 approved, the combinatorial nature of the chemical
space presents a formidable challenge. While Artificial Intelligence (AI) has
emerged as a powerful ally, traditional AI frameworks face significant hurdles.
This manuscript introduces CardiGraphormer, a groundbreaking approach that
synergizes self-supervised learning (SSL), Graph Neural Networks (GNNs), and
Cardinality Preserving Attention to revolutionize drug discovery.
CardiGraphormer, a novel combination of Graphormer and Cardinality Preserving
Attention, leverages SSL to learn potent molecular representations and employs
GNNs to extract molecular fingerprints, enhancing predictive performance and
interpretability while reducing computation time. It excels in handling complex
data like molecular structures and performs tasks associated with nodes, pairs
of nodes, subgraphs, or entire graph structures. CardiGraphormer's potential
applications in drug discovery and drug interactions are vast, from identifying
new drug targets to predicting drug-to-drug interactions and enabling novel
drug discovery. This innovative approach provides an AI-enhanced methodology in
drug development, utilizing SSL combined with GNNs to overcome existing
limitations and pave the way for a richer exploration of the vast combinatorial
chemical space in drug discovery.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04049">Parallel Algorithms Align with Neural Execution. (arXiv:2307.04049v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Engelmayer_V/0/1/0/all/0/1">Valerie Engelmayer</a>, <a href="http://arxiv.org/find/cs/1/au:+Georgiev_D/0/1/0/all/0/1">Dobrik Georgiev</a>, <a href="http://arxiv.org/find/cs/1/au:+Velickovic_P/0/1/0/all/0/1">Petar Veli&#x10d;kovi&#x107;</a></p>
<p>Neural algorithmic reasoners are parallel processors. Teaching them
sequential algorithms contradicts this nature, rendering a significant share of
their computations redundant. Parallel algorithms however may exploit their
full computational power, therefore requiring fewer layers to be executed. This
drastically reduces training times, as we observe when comparing parallel
implementations of searching, sorting and finding strongly connected components
to their sequential counterparts on the CLRS framework. Additionally, parallel
versions achieve (often strongly) superior predictive performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05134">TIAM -- A Metric for Evaluating Alignment in Text-to-Image Generation. (arXiv:2307.05134v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Grimal_P/0/1/0/all/0/1">Paul Grimal</a>, <a href="http://arxiv.org/find/cs/1/au:+Borgne_H/0/1/0/all/0/1">Herv&#xe9; Le Borgne</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferret_O/0/1/0/all/0/1">Olivier Ferret</a>, <a href="http://arxiv.org/find/cs/1/au:+Tourille_J/0/1/0/all/0/1">Julien Tourille</a></p>
<p>The progress in the generation of synthetic images has made it crucial to
assess their quality. While several metrics have been proposed to assess the
rendering of images, it is crucial for Text-to-Image (T2I) models, which
generate images based on a prompt, to consider additional aspects such as to
which extent the generated image matches the important content of the prompt.
Moreover, although the generated images usually result from a random starting
point, the influence of this one is generally not considered. In this article,
we propose a new metric based on prompt templates to study the alignment
between the content specified in the prompt and the corresponding generated
images. It allows us to better characterize the alignment in terms of the type
of the specified objects, their number, and their color. We conducted a study
on several recent T2I models about various aspects. An additional interesting
result we obtained with our approach is that image quality can vary drastically
depending on the noise used as a seed for the images. We also quantify the
influence of the number of concepts in the prompt, their order as well as their
(color) attributes. Finally, our method allows us to identify some seeds that
produce better images than others, opening novel directions of research on this
understudied topic.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05520">Do DL models and training environments have an impact on energy consumption?. (arXiv:2307.05520v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rey_S/0/1/0/all/0/1">Santiago del Rey</a>, <a href="http://arxiv.org/find/cs/1/au:+Martinez_Fernandez_S/0/1/0/all/0/1">Silverio Mart&#xed;nez-Fern&#xe1;ndez</a>, <a href="http://arxiv.org/find/cs/1/au:+Cruz_L/0/1/0/all/0/1">Lu&#xed;s Cruz</a>, <a href="http://arxiv.org/find/cs/1/au:+Franch_X/0/1/0/all/0/1">Xavier Franch</a></p>
<p>Current research in the computer vision field mainly focuses on improving
Deep Learning (DL) correctness and inference time performance. However, there
is still little work on the huge carbon footprint that has training DL models.
This study aims to analyze the impact of the model architecture and training
environment when training greener computer vision models. We divide this goal
into two research questions. First, we analyze the effects of model
architecture on achieving greener models while keeping correctness at optimal
levels. Second, we study the influence of the training environment on producing
greener models. To investigate these relationships, we collect multiple metrics
related to energy efficiency and model correctness during the models' training.
Then, we outline the trade-offs between the measured energy efficiency and the
models' correctness regarding model architecture, and their relationship with
the training environment. We conduct this research in the context of a computer
vision system for image classification. In conclusion, we show that selecting
the proper model architecture and training environment can reduce energy
consumption dramatically (up to 81.38%) at the cost of negligible decreases in
correctness. Also, we find evidence that GPUs should scale with the models'
computational complexity for better energy efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06263">On the hierarchical Bayesian modelling of frequency response functions. (arXiv:2307.06263v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dardeno_T/0/1/0/all/0/1">T.A. Dardeno</a>, <a href="http://arxiv.org/find/cs/1/au:+Worden_K/0/1/0/all/0/1">K. Worden</a>, <a href="http://arxiv.org/find/cs/1/au:+Dervilis_N/0/1/0/all/0/1">N. Dervilis</a>, <a href="http://arxiv.org/find/cs/1/au:+Mills_R/0/1/0/all/0/1">R.S. Mills</a>, <a href="http://arxiv.org/find/cs/1/au:+Bull_L/0/1/0/all/0/1">L.A. Bull</a></p>
<p>For situations that may benefit from information sharing among datasets,
e.g., population-based SHM of similar structures, the hierarchical Bayesian
approach provides a useful modelling structure. Hierarchical Bayesian models
learn statistical distributions at the population (or parent) and the domain
levels simultaneously, to bolster statistical strength among the parameters. As
a result, variance is reduced among the parameter estimates, particularly when
data are limited. In this paper, a combined probabilistic FRF model is
developed for a small population of nominally-identical helicopter blades,
using a hierarchical Bayesian structure, to support information transfer in the
context of sparse data. The modelling approach is also demonstrated in a
traditional SHM context, for a single helicopter blade exposed to varying
temperatures, to show how the inclusion of physics-based knowledge can improve
generalisation beyond the training data, in the context of scarce data. These
models address critical challenges in SHM, by accommodating benign variations
that present as differences in the underlying dynamics, while also considering
(and utilising), the similarities among the domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10253">Efficient selective attention LSTM for well log curve synthesis. (arXiv:2307.10253v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yuankai Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Huanyu Li</a></p>
<p>Non-core drilling has gradually become the primary exploration method in
geological exploration engineering, and well logging curves have increasingly
gained importance as the main carriers of geological information. However,
factors such as geological environment, logging equipment, borehole quality,
and unexpected events can all impact the quality of well logging curves.
Previous methods of re-logging or manual corrections have been associated with
high costs and low efficiency. This paper proposes a machine learning method
that utilizes existing data to predict missing data, and its effectiveness and
feasibility have been validated through field experiments. The proposed method
builds on the traditional Long Short-Term Memory (LSTM) neural network by
incorporating a self-attention mechanism to analyze the sequential dependencies
of the data. It selects the dominant computational results in the LSTM,
reducing the computational complexity from O(n^2) to O(nlogn) and improving
model efficiency. Experimental results demonstrate that the proposed method
achieves higher accuracy compared to traditional curve synthesis methods based
on Fully Connected Neural Networks (FCNN) and vanilla LSTM. This accurate,
efficient, and cost-effective prediction method holds a practical value in
engineering applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14823">Fading memory as inductive bias in residual recurrent networks. (arXiv:2307.14823v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dubinin_I/0/1/0/all/0/1">Igor Dubinin</a>, <a href="http://arxiv.org/find/cs/1/au:+Effenberger_F/0/1/0/all/0/1">Felix Effenberger</a></p>
<p>Residual connections have been proposed as an architecture-based inductive
bias to mitigate the problem of exploding and vanishing gradients and increased
task performance in both feed-forward and recurrent networks (RNNs) when
trained with the backpropagation algorithm. Yet, little is known about how
residual connections in RNNs influence their dynamics and fading memory
properties. Here, we introduce weakly coupled residual recurrent networks
(WCRNNs) in which residual connections result in well-defined Lyapunov
exponents and allow for studying properties of fading memory. We investigate
how the residual connections of WCRNNs influence their performance, network
dynamics, and memory properties on a set of benchmark tasks. We show that
several distinct forms of residual connections yield effective inductive biases
that result in increased network expressivity. In particular, those are
residual connections that (i) result in network dynamics at the proximity of
the edge of chaos, (ii) allow networks to capitalize on characteristic spectral
properties of the data, and (iii) result in heterogeneous memory properties. In
addition, we demonstrate how our results can be extended to non-linear
residuals and introduce a weakly coupled residual initialization scheme that
can be used for Elman RNNs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.00583">Semisupervised Anomaly Detection using Support Vector Regression with Quantum Kernel. (arXiv:2308.00583v2 [quant-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Tscharke_K/0/1/0/all/0/1">Kilian Tscharke</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Issel_S/0/1/0/all/0/1">Sebastian Issel</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Debus_P/0/1/0/all/0/1">Pascal Debus</a></p>
<p>Anomaly detection (AD) involves identifying observations or events that
deviate in some way from the rest of the data. Machine learning techniques have
shown success in automating this process by detecting hidden patterns and
deviations in large-scale data. The potential of quantum computing for machine
learning has been widely recognized, leading to extensive research efforts to
develop suitable quantum machine learning (QML) algorithms. In particular, the
search for QML algorithms for near-term NISQ devices is in full swing. However,
NISQ devices pose additional challenges due to their limited qubit coherence
times, low number of qubits, and high error rates. Kernel methods based on
quantum kernel estimation have emerged as a promising approach to QML on NISQ
devices, offering theoretical guarantees, versatility, and compatibility with
NISQ constraints. Especially support vector machines (SVM) utilizing quantum
kernel estimation have shown success in various supervised learning tasks.
However, in the context of AD, semisupervised learning is of great relevance,
and yet there is limited research published in this area. This paper introduces
an approach to semisupervised AD based on the reconstruction loss of a support
vector regression (SVR) with quantum kernel. This novel model is an alternative
to the variational quantum and quantum kernel one-class classifiers, and is
compared to a quantum autoencoder as quantum baseline and a SVR with
radial-basis-function (RBF) kernel as well as a classical autoencoder as
classical baselines. The models are benchmarked extensively on 10 real-world AD
data sets and one toy data set, and it is shown that our SVR model with quantum
kernel performs better than the SVR with RBF kernel as well as all other
models, achieving highest mean AUC over all data sets. In addition, our QSVR
outperforms the quantum autoencoder on 9 out of 11 data sets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.08469">LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series Forecasters. (arXiv:2308.08469v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1">Ching Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wei-Yao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1">Wen-Chih Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tien-Fu Chen</a></p>
<p>Multivariate time-series forecasting is vital in various domains, e.g.,
economic planning and weather prediction. Deep train-from-scratch models have
exhibited effective performance yet require large amounts of data, which limits
real-world applicability. Recently, researchers have explored pre-trained Large
Language Models (LLMs) for limited non-linguistic datasets. However,
incorporating LLMs with time-series data presents challenges of limited
adaptation due to different compositions between time-series and linguistic
data, and the inability to process multi-scale temporal information. To tackle
these challenges, we propose LLM4TS, a framework for time-series forecasting
with pre-trained LLMs. LLM4TS consists of a two-stage fine-tuning strategy: the
time-series alignment stage to align LLMs with the nuances of time-series data,
and the forecasting fine-tuning stage, which is specifically designed for
time-series forecasting tasks. Furthermore, our framework features a novel
two-level aggregation method that integrates multi-scale temporal data within
pre-trained LLMs, enhancing their ability to interpret time-specific
information. In experiments across 7 time-series forecasting datasets, LLM4TS
is superior to existing state-of-the-art methods, including those trained from
scratch, in full-shot scenarios, and also achieves an average improvement of
6.84% in MSE in few-shot scenarios. In addition, evaluations compared with
different self-supervised learning approaches highlight LLM4TS's effectiveness
with representation learning in forecasting scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.02084">Unsupervised Out-of-Distribution Detection by Restoring Lossy Inputs with Variational Autoencoder. (arXiv:2309.02084v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1">Zezhen Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bin Liu</a></p>
<p>Deep generative models have been demonstrated as problematic in the
unsupervised out-of-distribution (OOD) detection task, where they tend to
assign higher likelihoods to OOD samples. Previous studies on this issue are
usually not applicable to the Variational Autoencoder (VAE). As a popular
subclass of generative models, the VAE can be effective with a relatively
smaller model size and be more stable and faster in training and inference,
which can be more advantageous in real-world applications. In this paper, We
propose a novel VAE-based score called Error Reduction (ER) for OOD detection,
which is based on a VAE that takes a lossy version of the training set as
inputs and the original set as targets. Experiments are carried out on various
datasets to show the effectiveness of our method, we also present the effect of
design choices with ablation experiments. Our code is available at:
https://github.com/ZJLAB-AMMI/VAE4OOD.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.15216">Diabetic Retinopathy Using Gaussian Filter. (arXiv:2309.15216v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Muddaluru_R/0/1/0/all/0/1">Roshan Vasu Muddaluru</a>, <a href="http://arxiv.org/find/cs/1/au:+Thoguluva_S/0/1/0/all/0/1">Sharvaani Ravikumar Thoguluva</a>, <a href="http://arxiv.org/find/cs/1/au:+Prabha_S/0/1/0/all/0/1">Shruti Prabha</a>, <a href="http://arxiv.org/find/cs/1/au:+Reddy_T/0/1/0/all/0/1">Tanuja Konda Reddy</a>, <a href="http://arxiv.org/find/cs/1/au:+P_D/0/1/0/all/0/1">Dr. Suja P</a></p>
<p>The retina is an essential component of the visual system, and maintaining
eyesight depends on the timely and correct detection of disorders. This
research specifically addresses the early-stage detection and severity
classification of diabetic retinopathy (DR), a serious public health hazard. We
compare the results of different deep learning models such as InceptionV3,
DenseNet121 and other CNN based models by using different image filters, such
as Gaussian, grayscale and Gabor. These models could detect subtle pathological
alterations and use that information to estimate the risk of retinal illnesses.
The objective is to improve the diagnostic processes for diabetic retinopathy,
the primary cause of diabetes-related blindness, by utilizing deep learning
models. A comparative analysis between Greyscale, Gaussian and Gabor filters
has been provided after applying these filters on the retinal images. The
Gaussian filter resulted to be the most promising filter giving the best
accuracies for all the models. The best performing model was InceptionV3 which
gave an accuracy of 96% on Gaussian images, therefore Gaussian filter emerged
as our most promising filter.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.17207">Memory Gym: Towards Endless Tasks to Benchmark Memory Capabilities of Agents. (arXiv:2309.17207v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pleines_M/0/1/0/all/0/1">Marco Pleines</a>, <a href="http://arxiv.org/find/cs/1/au:+Pallasch_M/0/1/0/all/0/1">Matthias Pallasch</a>, <a href="http://arxiv.org/find/cs/1/au:+Zimmer_F/0/1/0/all/0/1">Frank Zimmer</a>, <a href="http://arxiv.org/find/cs/1/au:+Preuss_M/0/1/0/all/0/1">Mike Preuss</a></p>
<p>Memory Gym presents a suite of 2D partially observable environments, namely
Mortar Mayhem, Mystery Path, and Searing Spotlights, designed to benchmark
memory capabilities in decision-making agents. These environments, originally
with finite tasks, are expanded into innovative, endless formats, mirroring the
escalating challenges of cumulative memory games such as ``I packed my bag''.
This progression in task design shifts the focus from merely assessing sample
efficiency to also probing the levels of memory effectiveness in dynamic,
prolonged scenarios. To address the gap in available memory-based Deep
Reinforcement Learning baselines, we introduce an implementation that
integrates Transformer-XL (TrXL) with Proximal Policy Optimization. This
approach utilizes TrXL as a form of episodic memory, employing a sliding window
technique. Our comparative study between the Gated Recurrent Unit (GRU) and
TrXL reveals varied performances across different settings. TrXL, on the finite
environments, demonstrates superior sample efficiency in Mystery Path and
outperforms in Mortar Mayhem. However, GRU is more efficient on Searing
Spotlights. Most notably, in all endless tasks, GRU makes a remarkable
resurgence, consistently outperforming TrXL by significant margins. Website and
Source Code: https://github.com/MarcoMeter/endless-memory-gym/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00077">Optimizing with Low Budgets: a Comparison on the Black-box Optimization Benchmarking Suite and OpenAI Gym. (arXiv:2310.00077v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Raponi_E/0/1/0/all/0/1">Elena Raponi</a>, <a href="http://arxiv.org/find/cs/1/au:+Carraz_N/0/1/0/all/0/1">Nathanael Rakotonirina Carraz</a>, <a href="http://arxiv.org/find/cs/1/au:+Rapin_J/0/1/0/all/0/1">J&#xe9;r&#xe9;my Rapin</a>, <a href="http://arxiv.org/find/cs/1/au:+Doerr_C/0/1/0/all/0/1">Carola Doerr</a>, <a href="http://arxiv.org/find/cs/1/au:+Teytaud_O/0/1/0/all/0/1">Olivier Teytaud</a></p>
<p>The growing ubiquity of machine learning (ML) has led it to enter various
areas of computer science, including black-box optimization (BBO). Recent
research is particularly concerned with Bayesian optimization (BO). BO-based
algorithms are popular in the ML community, as they are used for hyperparameter
optimization and more generally for algorithm configuration. However, their
efficiency decreases as the dimensionality of the problem and the budget of
evaluations increase. Meanwhile, derivative-free optimization methods have
evolved independently in the optimization community. Therefore, we urge to
understand whether cross-fertilization is possible between the two communities,
ML and BBO, i.e., whether algorithms that are heavily used in ML also work well
in BBO and vice versa. Comparative experiments often involve rather small
benchmarks and show visible problems in the experimental setup, such as poor
initialization of baselines, overfitting due to problem-specific setting of
hyperparameters, and low statistical significance.
</p>
<p>With this paper, we update and extend a comparative study presented by Hutter
et al. in 2013. We compare BBO tools for ML with more classical heuristics,
first on the well-known BBOB benchmark suite from the COCO environment and then
on Direct Policy Search for OpenAI Gym, a reinforcement learning benchmark. Our
results confirm that BO-based optimizers perform well on both benchmarks when
budgets are limited, albeit with a higher computational cost, while they are
often outperformed by algorithms from other families when the evaluation budget
becomes larger. We also show that some algorithms from the BBO community
perform surprisingly well on ML tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00488">On Memorization and Privacy Risks of Sharpness Aware Minimization. (arXiv:2310.00488v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Young In Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1">Pratiksha Agrawal</a>, <a href="http://arxiv.org/find/cs/1/au:+Royset_J/0/1/0/all/0/1">Johannes O. Royset</a>, <a href="http://arxiv.org/find/cs/1/au:+Khanna_R/0/1/0/all/0/1">Rajiv Khanna</a></p>
<p>In many recent works, there is an increased focus on designing algorithms
that seek flatter optima for neural network loss optimization as there is
empirical evidence that it leads to better generalization performance in many
datasets. In this work, we dissect these performance gains through the lens of
data memorization in overparameterized models. We define a new metric that
helps us identify which data points specifically do algorithms seeking flatter
optima do better when compared to vanilla SGD. We find that the generalization
gains achieved by Sharpness Aware Minimization (SAM) are particularly
pronounced for atypical data points, which necessitate memorization. This
insight helps us unearth higher privacy risks associated with SAM, which we
verify through exhaustive empirical evaluations. Finally, we propose mitigation
strategies to achieve a more desirable accuracy vs privacy tradeoff.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04171">Dynamic Relation-Attentive Graph Neural Networks for Fraud Detection. (arXiv:2310.04171v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">Heehyeon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1">Jinhyeok Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Whang_J/0/1/0/all/0/1">Joyce Jiyoung Whang</a></p>
<p>Fraud detection aims to discover fraudsters deceiving other users by, for
example, leaving fake reviews or making abnormal transactions. Graph-based
fraud detection methods consider this task as a classification problem with two
classes: frauds or normal. We address this problem using Graph Neural Networks
(GNNs) by proposing a dynamic relation-attentive aggregation mechanism. Based
on the observation that many real-world graphs include different types of
relations, we propose to learn a node representation per relation and aggregate
the node representations using a learnable attention function that assigns a
different attention coefficient to each relation. Furthermore, we combine the
node representations from different layers to consider both the local and
global structures of a target node, which is beneficial to improving the
performance of fraud detection on graphs with heterophily. By employing dynamic
graph attention in all the aggregation processes, our method adaptively
computes the attention coefficients for each node. Experimental results show
that our method, DRAG, outperforms state-of-the-art fraud detection methods on
real-world benchmark datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04444">What&#x27;s the Magic Word? A Control Theory of LLM Prompting. (arXiv:2310.04444v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bhargava_A/0/1/0/all/0/1">Aman Bhargava</a>, <a href="http://arxiv.org/find/cs/1/au:+Witkowski_C/0/1/0/all/0/1">Cameron Witkowski</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1">Manav Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Thomson_M/0/1/0/all/0/1">Matt Thomson</a></p>
<p>Prompt engineering is crucial for deploying LLMs but is poorly understood
mathematically. We formalize LLM systems as a class of discrete stochastic
dynamical systems to explore prompt engineering through the lens of control
theory. We investigate the reachable set of output token sequences $R_y(\mathbf
x_0)$ for which there exists a control input sequence $\mathbf u$ for each
$\mathbf y \in R_y(\mathbf x_0)$ that steers the LLM to output $\mathbf y$ from
initial state sequence $\mathbf x_0$. We offer analytic analysis on the
limitations on the controllability of self-attention in terms of reachable set,
where we prove an upper bound on the reachable set of outputs $R_y(\mathbf
x_0)$ as a function of the singular values of the parameter matrices. We
present complementary empirical analysis on the controllability of a panel of
LLMs, including Falcon-7b, Llama-7b, and Falcon-40b. Our results demonstrate a
lower bound on the reachable set of outputs $R_y(\mathbf x_0)$ w.r.t. initial
state sequences $\mathbf x_0$ sampled from the Wikitext dataset. We find that
the correct next Wikitext token following sequence $\mathbf x_0$ is reachable
over 97% of the time with prompts of $k\leq 10$ tokens. We also establish that
the top 75 most likely next tokens, as estimated by the LLM itself, are
reachable at least 85% of the time with prompts of $k\leq 10$ tokens.
Intriguingly, short prompt sequences can dramatically alter the likelihood of
specific outputs, even making the least likely tokens become the most likely
ones. This control-centric analysis of LLMs demonstrates the significant and
poorly understood role of input sequences in steering output probabilities,
offering a foundational perspective for enhancing language model system
capabilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.06452">Understanding the Effects of RLHF on LLM Generalisation and Diversity. (arXiv:2310.06452v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kirk_R/0/1/0/all/0/1">Robert Kirk</a>, <a href="http://arxiv.org/find/cs/1/au:+Mediratta_I/0/1/0/all/0/1">Ishita Mediratta</a>, <a href="http://arxiv.org/find/cs/1/au:+Nalmpantis_C/0/1/0/all/0/1">Christoforos Nalmpantis</a>, <a href="http://arxiv.org/find/cs/1/au:+Luketina_J/0/1/0/all/0/1">Jelena Luketina</a>, <a href="http://arxiv.org/find/cs/1/au:+Hambro_E/0/1/0/all/0/1">Eric Hambro</a>, <a href="http://arxiv.org/find/cs/1/au:+Grefenstette_E/0/1/0/all/0/1">Edward Grefenstette</a>, <a href="http://arxiv.org/find/cs/1/au:+Raileanu_R/0/1/0/all/0/1">Roberta Raileanu</a></p>
<p>Large language models (LLMs) fine-tuned with reinforcement learning from
human feedback (RLHF) have been used in some of the most widely deployed AI
models to date, such as OpenAI's ChatGPT or Anthropic's Claude. % , or Meta's
LLaMA-2. While there has been significant work developing these methods, our
understanding of the benefits and downsides of each stage in RLHF is still
limited. To fill this gap, we present an extensive analysis of how each stage
of the process (i.e.~supervised fine-tuning (SFT), reward modelling, and RLHF)
affects two key properties: out-of-distribution (OOD) generalisation and output
diversity. OOD generalisation is crucial given the wide range of real-world
scenarios in which these models are being used, while output diversity refers
to the model's ability to generate varied outputs and is important for a
variety of use cases. We perform our analysis across two base models on both
summarisation and instruction following tasks, the latter being highly relevant
for current LLM use cases. We find that RLHF generalises better than SFT to new
inputs, particularly as the distribution shift between train and test becomes
larger. However, RLHF significantly reduces output diversity compared to SFT
across a variety of measures, implying a tradeoff in current LLM fine-tuning
methods between generalisation and diversity. Our results provide guidance on
which fine-tuning method should be used depending on the application, and show
that more research is needed to improve the tradeoff between generalisation and
diversity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19274">Prediction of Effective Elastic Moduli of Rocks using Graph Neural Networks. (arXiv:2310.19274v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1">Jaehong Chung</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmad_R/0/1/0/all/0/1">Rasool Ahmad</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1">WaiChing Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1">Wei Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Mukerji_T/0/1/0/all/0/1">Tapan Mukerji</a></p>
<p>This study presents a Graph Neural Networks (GNNs)-based approach for
predicting the effective elastic moduli of rocks from their digital CT-scan
images. We use the Mapper algorithm to transform 3D digital rock images into
graph datasets, encapsulating essential geometrical information. These graphs,
after training, prove effective in predicting elastic moduli. Our GNN model
shows robust predictive capabilities across various graph sizes derived from
various subcube dimensions. Not only does it perform well on the test dataset,
but it also maintains high prediction accuracy for unseen rocks and unexplored
subcube sizes. Comparative analysis with Convolutional Neural Networks (CNNs)
reveals the superior performance of GNNs in predicting unseen rock properties.
Moreover, the graph representation of microstructures significantly reduces GPU
memory requirements (compared to the grid representation for CNNs), enabling
greater flexibility in the batch size selection. This work demonstrates the
potential of GNN models in enhancing the prediction accuracy of rock properties
and boosting the efficiency of digital rock analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19923">Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents. (arXiv:2310.19923v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gunther_M/0/1/0/all/0/1">Michael G&#xfc;nther</a>, <a href="http://arxiv.org/find/cs/1/au:+Ong_J/0/1/0/all/0/1">Jackmin Ong</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohr_I/0/1/0/all/0/1">Isabelle Mohr</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdessalem_A/0/1/0/all/0/1">Alaeddine Abdessalem</a>, <a href="http://arxiv.org/find/cs/1/au:+Abel_T/0/1/0/all/0/1">Tanguy Abel</a>, <a href="http://arxiv.org/find/cs/1/au:+Akram_M/0/1/0/all/0/1">Mohammad Kalim Akram</a>, <a href="http://arxiv.org/find/cs/1/au:+Guzman_S/0/1/0/all/0/1">Susana Guzman</a>, <a href="http://arxiv.org/find/cs/1/au:+Mastrapas_G/0/1/0/all/0/1">Georgios Mastrapas</a>, <a href="http://arxiv.org/find/cs/1/au:+Sturua_S/0/1/0/all/0/1">Saba Sturua</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Werk_M/0/1/0/all/0/1">Maximilian Werk</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1">Nan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1">Han Xiao</a></p>
<p>Text embedding models have emerged as powerful tools for transforming
sentences into fixed-sized feature vectors that encapsulate semantic
information. While these models are essential for tasks like information
retrieval, semantic clustering, and text re-ranking, most existing open-source
models, especially those built on architectures like BERT, struggle to
represent lengthy documents and often resort to truncation. One common approach
to mitigate this challenge involves splitting documents into smaller paragraphs
for embedding. However, this strategy results in a much larger set of vectors,
consequently leading to increased memory consumption and computationally
intensive vector searches with elevated latency.
</p>
<p>To address these challenges, we introduce Jina Embeddings 2, an open-source
text embedding model capable of accommodating up to 8192 tokens. This model is
designed to transcend the conventional 512-token limit and adeptly process long
documents. Jina Embeddings 2 not only achieves state-of-the-art performance on
a range of embedding-related tasks in the MTEB benchmark but also matches the
performance of OpenAI's proprietary ada-002 model. Additionally, our
experiments indicate that an extended context can enhance performance in tasks
such as NarrativeQA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12564">Summary of the DISPLACE Challenge 2023 - DIarization of SPeaker and LAnguage in Conversational Environments. (arXiv:2311.12564v3 [eess.AS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Baghel_S/0/1/0/all/0/1">Shikha Baghel</a>, <a href="http://arxiv.org/find/eess/1/au:+Ramoji_S/0/1/0/all/0/1">Shreyas Ramoji</a>, <a href="http://arxiv.org/find/eess/1/au:+Jain_S/0/1/0/all/0/1">Somil Jain</a>, <a href="http://arxiv.org/find/eess/1/au:+Chowdhuri_P/0/1/0/all/0/1">Pratik Roy Chowdhuri</a>, <a href="http://arxiv.org/find/eess/1/au:+Singh_P/0/1/0/all/0/1">Prachi Singh</a>, <a href="http://arxiv.org/find/eess/1/au:+Vijayasenan_D/0/1/0/all/0/1">Deepu Vijayasenan</a>, <a href="http://arxiv.org/find/eess/1/au:+Ganapathy_S/0/1/0/all/0/1">Sriram Ganapathy</a></p>
<p>In multi-lingual societies, where multiple languages are spoken in a small
geographic vicinity, informal conversations often involve mix of languages.
Existing speech technologies may be inefficient in extracting information from
such conversations, where the speech data is rich in diversity with multiple
languages and speakers. The DISPLACE (DIarization of SPeaker and LAnguage in
Conversational Environments) challenge constitutes an open-call for evaluating
and bench-marking the speaker and language diarization technologies on this
challenging condition. The challenge entailed two tracks: Track-1 focused on
speaker diarization (SD) in multilingual situations while, Track-2 addressed
the language diarization (LD) in a multi-speaker scenario. Both the tracks were
evaluated using the same underlying audio data. To facilitate this evaluation,
a real-world dataset featuring multilingual, multi-speaker conversational
far-field speech was recorded and distributed. Furthermore, a baseline system
was made available for both SD and LD task which mimicked the state-of-art in
these tasks. The challenge garnered a total of $42$ world-wide registrations
and received a total of $19$ combined submissions for Track-1 and Track-2. This
paper describes the challenge, details of the datasets, tasks, and the baseline
system. Additionally, the paper provides a concise overview of the submitted
systems in both tracks, with an emphasis given to the top performing systems.
The paper also presents insights and future perspectives for SD and LD tasks,
focusing on the key challenges that the systems need to overcome before
wide-spread commercial deployment on such conversations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05840">Topological Data Analysis for Neural Network Analysis: A Comprehensive Survey. (arXiv:2312.05840v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ballester_R/0/1/0/all/0/1">Rub&#xe9;n Ballester</a>, <a href="http://arxiv.org/find/cs/1/au:+Casacuberta_C/0/1/0/all/0/1">Carles Casacuberta</a>, <a href="http://arxiv.org/find/cs/1/au:+Escalera_S/0/1/0/all/0/1">Sergio Escalera</a></p>
<p>This survey provides a comprehensive exploration of applications of
Topological Data Analysis (TDA) within neural network analysis. Using TDA tools
such as persistent homology and Mapper, we delve into the intricate structures
and behaviors of neural networks and their datasets. We discuss different
strategies to obtain topological information from data and neural networks by
means of TDA. Additionally, we review how topological information can be
leveraged to analyze properties of neural networks, such as their
generalization capacity or expressivity. We explore practical implications of
deep learning, specifically focusing on areas like adversarial detection and
model selection. Our survey organizes the examined works into four broad
domains: 1. Characterization of neural network architectures; 2. Analysis of
decision regions and boundaries; 3. Study of internal representations,
activations, and parameters; 4. Exploration of training dynamics and loss
functions. Within each category, we discuss several articles, offering
background information to aid in understanding the various methodologies. We
conclude with a synthesis of key insights gained from our study, accompanied by
a discussion of challenges and potential advancements in the field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13307">Not All Steps are Equal: Efficient Generation with Progressive Diffusion Models. (arXiv:2312.13307v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wenhao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1">Xiu Su</a>, <a href="http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1">Shan You</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1">Tao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Fei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1">Chen Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Chang Xu</a></p>
<p>Diffusion models have demonstrated remarkable efficacy in various generative
tasks with the predictive prowess of denoising model. Currently, these models
employ a uniform denoising approach across all timesteps. However, the inherent
variations in noisy latents at each timestep lead to conflicts during training,
constraining the potential of diffusion models. To address this challenge, we
propose a novel two-stage training strategy termed Step-Adaptive Training. In
the initial stage, a base denoising model is trained to encompass all
timesteps. Subsequently, we partition the timesteps into distinct groups,
fine-tuning the model within each group to achieve specialized denoising
capabilities. Recognizing that the difficulties of predicting noise at
different timesteps vary, we introduce a diverse model size requirement. We
dynamically adjust the model size for each timestep by estimating task
difficulty based on its signal-to-noise ratio before fine-tuning. This
adjustment is facilitated by a proxy-based structural importance assessment
mechanism, enabling precise and efficient pruning of the base denoising model.
Our experiments validate the effectiveness of the proposed training strategy,
demonstrating an improvement in the FID score on CIFAR10 by over 0.3 while
utilizing only 80\% of the computational resources. This innovative approach
not only enhances model performance but also significantly reduces
computational costs, opening new avenues for the development and application of
diffusion models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13763">Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and Composed Diffusion Models. (arXiv:2312.13763v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1">Huan Ling</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Seung Wook Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1">Antonio Torralba</a>, <a href="http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1">Sanja Fidler</a>, <a href="http://arxiv.org/find/cs/1/au:+Kreis_K/0/1/0/all/0/1">Karsten Kreis</a></p>
<p>Text-guided diffusion models have revolutionized image and video generation
and have also been successfully used for optimization-based 3D object
synthesis. Here, we instead focus on the underexplored text-to-4D setting and
synthesize dynamic, animated 3D objects using score distillation methods with
an additional temporal dimension. Compared to previous work, we pursue a novel
compositional generation-based approach, and combine text-to-image,
text-to-video, and 3D-aware multiview diffusion models to provide feedback
during 4D object optimization, thereby simultaneously enforcing temporal
consistency, high-quality visual appearance and realistic geometry. Our method,
called Align Your Gaussians (AYG), leverages dynamic 3D Gaussian Splatting with
deformation fields as 4D representation. Crucial to AYG is a novel method to
regularize the distribution of the moving 3D Gaussians and thereby stabilize
the optimization and induce motion. We also propose a motion amplification
mechanism as well as a new autoregressive synthesis scheme to generate and
combine multiple 4D sequences for longer generation. These techniques allow us
to synthesize vivid dynamic scenes, outperform previous work qualitatively and
quantitatively and achieve state-of-the-art text-to-4D performance. Due to the
Gaussian 4D representation, different 4D animations can be seamlessly combined,
as we demonstrate. AYG opens up promising avenues for animation, simulation and
digital content creation as well as synthetic data generation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13863">Manipulating Trajectory Prediction with Backdoors. (arXiv:2312.13863v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Messaoud_K/0/1/0/all/0/1">Kaouther Messaoud</a>, <a href="http://arxiv.org/find/cs/1/au:+Grosse_K/0/1/0/all/0/1">Kathrin Grosse</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Mickael Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1">Matthieu Cord</a>, <a href="http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1">Patrick P&#xe9;rez</a>, <a href="http://arxiv.org/find/cs/1/au:+Alahi_A/0/1/0/all/0/1">Alexandre Alahi</a></p>
<p>Autonomous vehicles ought to predict the surrounding agents' trajectories to
allow safe maneuvers in uncertain and complex traffic situations. As companies
increasingly apply trajectory prediction in the real world, security becomes a
relevant concern. In this paper, we focus on backdoors - a security threat
acknowledged in other fields but so far overlooked for trajectory prediction.
To this end, we describe and investigate four triggers that could affect
trajectory prediction. We then show that these triggers (for example, a braking
vehicle), when correlated with a desired output (for example, a curve) during
training, cause the desired output of a state-of-the-art trajectory prediction
model. In other words, the model has good benign performance but is vulnerable
to backdoors. This is the case even if the trigger maneuver is performed by a
non-casual agent behind the target vehicle. As a side-effect, our analysis
reveals interesting limitations within trajectory prediction models. Finally,
we evaluate a range of defenses against backdoors. While some, like simple
offroad checks, do not enable detection for all triggers, clustering is a
promising candidate to support manual inspection to find backdoors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14705">SCUNet++: Swin-UNet and CNN Bottleneck Hybrid Architecture with Multi-Fusion Dense Skip Connection for Pulmonary Embolism CT Image Segmentation. (arXiv:2312.14705v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1">Yifei Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Zou_B/0/1/0/all/0/1">Binfeng Zou</a>, <a href="http://arxiv.org/find/eess/1/au:+Guo_Z/0/1/0/all/0/1">Zhaoxin Guo</a>, <a href="http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1">Yiyu Huang</a>, <a href="http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1">Yifan Huang</a>, <a href="http://arxiv.org/find/eess/1/au:+Qin_F/0/1/0/all/0/1">Feiwei Qin</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1">Qinhai Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1">Changmiao Wang</a></p>
<p>Pulmonary embolism (PE) is a prevalent lung disease that can lead to right
ventricular hypertrophy and failure in severe cases, ranking second in severity
only to myocardial infarction and sudden death. Pulmonary artery CT angiography
(CTPA) is a widely used diagnostic method for PE. However, PE detection
presents challenges in clinical practice due to limitations in imaging
technology. CTPA can produce noises similar to PE, making confirmation of its
presence time-consuming and prone to overdiagnosis. Nevertheless, the
traditional segmentation method of PE can not fully consider the hierarchical
structure of features, local and global spatial features of PE CT images. In
this paper, we propose an automatic PE segmentation method called SCUNet++
(Swin Conv UNet++). This method incorporates multiple fusion dense skip
connections between the encoder and decoder, utilizing the Swin Transformer as
the encoder. And fuses features of different scales in the decoder subnetwork
to compensate for spatial information loss caused by the inevitable
downsampling in Swin-UNet or other state-of-the-art methods, effectively
solving the above problem. We provide a theoretical analysis of this method in
detail and validate it on publicly available PE CT image datasets FUMPE and
CAD-PE. The experimental results indicate that our proposed method achieved a
Dice similarity coefficient (DSC) of 83.47% and a Hausdorff distance 95th
percentile (HD95) of 3.83 on the FUMPE dataset, as well as a DSC of 83.42% and
an HD95 of 5.10 on the CAD-PE dataset. These findings demonstrate that our
method exhibits strong performance in PE segmentation tasks, potentially
enhancing the accuracy of automatic segmentation of PE and providing a powerful
diagnostic tool for clinical physicians. Our source code and new FUMPE dataset
are available at https://github.com/JustlfC03/SCUNet-plusplus.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15097">Recourse under Model Multiplicity via Argumentative Ensembling (Technical Report). (arXiv:2312.15097v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1">Junqi Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Rago_A/0/1/0/all/0/1">Antonio Rago</a>, <a href="http://arxiv.org/find/cs/1/au:+Leofante_F/0/1/0/all/0/1">Francesco Leofante</a>, <a href="http://arxiv.org/find/cs/1/au:+Toni_F/0/1/0/all/0/1">Francesca Toni</a></p>
<p>Model Multiplicity (MM) arises when multiple, equally performing machine
learning models can be trained to solve the same prediction task. Recent
studies show that models obtained under MM may produce inconsistent predictions
for the same input. When this occurs, it becomes challenging to provide
counterfactual explanations (CEs), a common means for offering recourse
recommendations to individuals negatively affected by models' predictions. In
this paper, we formalise this problem, which we name recourse-aware ensembling,
and identify several desirable properties which methods for solving it should
satisfy. We show that existing ensembling methods, naturally extended in
different ways to provide CEs, fail to satisfy these properties. We then
introduce argumentative ensembling, deploying computational argumentation to
guarantee robustness of CEs to MM, while also accommodating customisable user
preferences. We show theoretically and experimentally that argumentative
ensembling satisfies properties which the existing methods lack, and that the
trade-offs are minimal wrt accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15927">M3D: Dataset Condensation by Minimizing Maximum Mean Discrepancy. (arXiv:2312.15927v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hansong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shikun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Pengju Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1">Dan Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1">Shiming Ge</a></p>
<p>Training state-of-the-art (SOTA) deep models often requires extensive data,
resulting in substantial training and storage costs. To address these
challenges, dataset condensation has been developed to learn a small synthetic
set that preserves essential information from the original large-scale dataset.
Nowadays, optimization-oriented methods have been the primary method in the
field of dataset condensation for achieving SOTA results. However, the bi-level
optimization process hinders the practical application of such methods to
realistic and larger datasets. To enhance condensation efficiency, previous
works proposed Distribution-Matching (DM) as an alternative, which
significantly reduces the condensation cost. Nonetheless, current DM-based
methods have yielded less comparable results to optimization-oriented methods
due to their focus on aligning only the first moment of the distributions. In
this paper, we present a novel DM-based method named M3D for dataset
condensation by Minimizing the Maximum Mean Discrepancy between feature
representations of the synthetic and real images. By embedding their
distributions in a reproducing kernel Hilbert space, we align all orders of
moments of the distributions of real and synthetic images, resulting in a more
generalized condensed set. Notably, our method even surpasses the SOTA
optimization-oriented method IDC on the high-resolution ImageNet dataset.
Extensive analysis is conducted to verify the effectiveness of the proposed
method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16243">Are All Unseen Data Out-of-Distribution?. (arXiv:2312.16243v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Songming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1">Yuxiao Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qizhou Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chi_H/0/1/0/all/0/1">Haoang Chi</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Weikai Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1">Bo Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jinyan Li</a></p>
<p>Distributions of unseen data have been all treated as out-of-distribution
(OOD), making their generalization a significant challenge. Much evidence
suggests that the size increase of training data can monotonically decrease
generalization errors in test data. However, this is not true from other
observations and analysis. In particular, when the training data have multiple
source domains and the test data contain distribution drifts, then not all
generalization errors on the test data decrease monotonically with the
increasing size of training data. Such a non-decreasing phenomenon is formally
investigated under a linear setting with empirical verification across varying
visual benchmarks. Motivated by these results, we redefine the OOD data as a
type of data outside the convex hull of the training domains and prove a new
generalization bound based on this new definition. It implies that the
effectiveness of a well-trained model can be guaranteed for the unseen data
that is within the convex hull of the training domains. But, for some data
beyond the convex hull, a non-decreasing error trend can happen. Therefore, we
investigate the performance of popular strategies such as data augmentation and
pre-training to overcome this issue. Moreover, we propose a novel reinforcement
learning selection algorithm in the source domains only that can deliver
superior performance over the baseline methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.00110">Diffusion Model with Perceptual Loss. (arXiv:2401.00110v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1">Shanchuan Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xiao Yang</a></p>
<p>Diffusion models trained with mean squared error loss tend to generate
unrealistic samples. Current state-of-the-art models rely on classifier-free
guidance to improve sample quality, yet its surprising effectiveness is not
fully understood. In this paper, We show that the effectiveness of
classifier-free guidance partly originates from it being a form of implicit
perceptual guidance. As a result, we can directly incorporate perceptual loss
in diffusion training to improve sample quality. Since the score matching
objective used in diffusion training strongly resembles the denoising
autoencoder objective used in unsupervised training of perceptual networks, the
diffusion model itself is a perceptual network and can be used to generate
meaningful perceptual loss. We propose a novel self-perceptual objective that
results in diffusion models capable of generating more realistic samples. For
conditional generation, our method only improves sample quality without
entanglement with the conditional input and therefore does not sacrifice sample
diversity. Our method can also improve sample quality for unconditional
generation, which was not possible with classifier-free guidance before.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.00490">Kernel Density Estimation for Multiclass Quantification. (arXiv:2401.00490v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Moreo_A/0/1/0/all/0/1">Alejandro Moreo</a>, <a href="http://arxiv.org/find/cs/1/au:+Gonzalez_P/0/1/0/all/0/1">Pablo Gonz&#xe1;lez</a>, <a href="http://arxiv.org/find/cs/1/au:+Coz_J/0/1/0/all/0/1">Juan Jos&#xe9; del Coz</a></p>
<p>Several disciplines, like the social sciences, epidemiology, sentiment
analysis, or market research, are interested in knowing the distribution of the
classes in a population rather than the individual labels of the members
thereof. Quantification is the supervised machine learning task concerned with
obtaining accurate predictors of class prevalence, and to do so particularly in
the presence of label shift. The distribution-matching (DM) approaches
represent one of the most important families among the quantification methods
that have been proposed in the literature so far. Current DM approaches model
the involved populations by means of histograms of posterior probabilities. In
this paper, we argue that their application to the multiclass setting is
suboptimal since the histograms become class-specific, thus missing the
opportunity to model inter-class information that may exist in the data. We
propose a new representation mechanism based on multivariate densities that we
model via kernel density estimation (KDE). The experiments we have carried out
show our method, dubbed KDEy, yields superior quantification performance with
respect to previous DM approaches. We also investigate the KDE-based
representation within the maximum likelihood framework and show KDEy often
shows superior performance with respect to the expectation-maximization method
for quantification, arguably the strongest contender in the quantification
arena to date.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.00744">Harmonizing Covariance and Expressiveness for Deep Hamiltonian Regression in Crystalline Material Research: a Hybrid Cascaded Regression Framework. (arXiv:2401.00744v3 [physics.comp-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Yin_S/0/1/0/all/0/1">Shi Yin</a>, <a href="http://arxiv.org/find/physics/1/au:+Zhu_X/0/1/0/all/0/1">Xudong Zhu</a>, <a href="http://arxiv.org/find/physics/1/au:+Gao_T/0/1/0/all/0/1">Tianyu Gao</a>, <a href="http://arxiv.org/find/physics/1/au:+Zhang_H/0/1/0/all/0/1">Haochong Zhang</a>, <a href="http://arxiv.org/find/physics/1/au:+Wu_F/0/1/0/all/0/1">Feng Wu</a>, <a href="http://arxiv.org/find/physics/1/au:+He_L/0/1/0/all/0/1">Lixin He</a></p>
<p>Deep learning for Hamiltonian regression of quantum systems in material
research necessitates satisfying the covariance laws, among which achieving
SO(3)-equivariance without sacrificing the expressiveness of networks remains
an elusive challenge due to the restriction to non-linear mappings on
guaranteeing theoretical equivariance. To alleviate the
covariance-expressiveness dilemma, we propose a hybrid framework with two
cascaded regression stages. The first stage, with a theoretically-guaranteed
covariant neural network modeling symmetry properties of 3D atom systems,
yields theoretically covariant features and baseline Hamiltonian predictions,
assisting the second stage in learning covariance. Meanwhile, the second stage,
powered by a non-linear 3D graph Transformer network we propose for structural
modeling of 3D atomic systems, refines the first stage's output as a
fine-grained prediction of Hamiltonians with better expressiveness capability.
The combination of a theoretically covariant yet inevitably less expressive
model with a highly expressive non-linear network enables precise,
generalizable predictions while maintaining robust covariance under coordinate
transformations. Our method achieves state-of-the-art performance in
Hamiltonian prediction for electronic structure calculations, confirmed through
experiments on five crystalline material databases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01262">Fairness Certification for Natural Language Processing and Large Language Models. (arXiv:2401.01262v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Freiberger_V/0/1/0/all/0/1">Vincent Freiberger</a>, <a href="http://arxiv.org/find/cs/1/au:+Buchmann_E/0/1/0/all/0/1">Erik Buchmann</a></p>
<p>Natural Language Processing (NLP) plays an important role in our daily lives,
particularly due to the enormous progress of Large Language Models (LLM).
However, NLP has many fairness-critical use cases, e.g., as an expert system in
recruitment or as an LLM-based tutor in education. Since NLP is based on human
language, potentially harmful biases can diffuse into NLP systems and produce
unfair results, discriminate against minorities or generate legal issues.
Hence, it is important to develop a fairness certification for NLP approaches.
We follow a qualitative research approach towards a fairness certification for
NLP. In particular, we have reviewed a large body of literature on algorithmic
fairness, and we have conducted semi-structured expert interviews with a wide
range of experts from that area. We have systematically devised six fairness
criteria for NLP, which can be further refined into 18 sub-categories. Our
criteria offer a foundation for operationalizing and testing processes to
certify fairness, both from the perspective of the auditor and the audited
organization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.14942">On the Optimality of Misspecified Spectral Algorithms. (arXiv:2303.14942v2 [math.ST] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Zhang_H/0/1/0/all/0/1">Haobo Zhang</a>, <a href="http://arxiv.org/find/math/1/au:+Li_Y/0/1/0/all/0/1">Yicheng Li</a>, <a href="http://arxiv.org/find/math/1/au:+Lin_Q/0/1/0/all/0/1">Qian Lin</a></p>
<p>In the misspecified spectral algorithms problem, researchers usually assume
the underground true function $f_{\rho}^{*} \in [\mathcal{H}]^{s}$, a
less-smooth interpolation space of a reproducing kernel Hilbert space (RKHS)
$\mathcal{H}$ for some $s\in (0,1)$. The existing minimax optimal results
require $\|f_{\rho}^{*}\|_{L^{\infty}}&lt;\infty$ which implicitly requires $s &gt;
\alpha_{0}$ where $\alpha_{0}\in (0,1)$ is the embedding index, a constant
depending on $\mathcal{H}$. Whether the spectral algorithms are optimal for all
$s\in (0,1)$ is an outstanding problem lasting for years. In this paper, we
show that spectral algorithms are minimax optimal for any
$\alpha_{0}-\frac{1}{\beta} &lt; s &lt; 1$, where $\beta$ is the eigenvalue decay
rate of $\mathcal{H}$. We also give several classes of RKHSs whose embedding
index satisfies $ \alpha_0 = \frac{1}{\beta} $. Thus, the spectral algorithms
are minimax optimal for all $s\in (0,1)$ on these RKHSs.
</p>
</p>
</div>

    </div>
    </body>
    