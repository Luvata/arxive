<!DOCTYPE html>
<html>
<head>
<title>2023-12-06-cs-ai</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2312.00793">Variants of Tagged Sentential Decision Diagrams. (arXiv:2312.00793v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhong_D/0/1/0/all/0/1">Deyuan Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Mingwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_Q/0/1/0/all/0/1">Quanlong Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1">Liangda Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lai_Z/0/1/0/all/0/1">Zhaorong Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1">Yong Lai</a></p>
<p>A recently proposed canonical form of Boolean functions, namely tagged
sentential decision diagrams (TSDDs), exploits both the standard and
zero-suppressed trimming rules. The standard ones minimize the size of
sentential decision diagrams (SDDs) while the zero-suppressed trimming rules
have the same objective as the standard ones but for zero-suppressed sentential
decision diagrams (ZSDDs). The original TSDDs, which we call zero-suppressed
TSDDs (ZTSDDs), firstly fully utilize the zero-suppressed trimming rules, and
then the standard ones. In this paper, we present a variant of TSDDs which we
call standard TSDDs (STSDDs) by reversing the order of trimming rules. We then
prove the canonicity of STSDDs and present the algorithms for binary operations
on TSDDs. In addition, we offer two kinds of implementations of STSDDs and
ZTSDDs and acquire three variations of the original TSDDs. Experimental
evaluations demonstrate that the four versions of TSDDs have the size advantage
over SDDs and ZSDDs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00794">Informative Priors Improve the Reliability of Multimodal Clinical Data Classification. (arXiv:2312.00794v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lopez_L/0/1/0/all/0/1">L. Julian Lechuga Lopez</a>, <a href="http://arxiv.org/find/cs/1/au:+Rudner_T/0/1/0/all/0/1">Tim G. J. Rudner</a>, <a href="http://arxiv.org/find/cs/1/au:+Shamout_F/0/1/0/all/0/1">Farah E. Shamout</a></p>
<p>Machine learning-aided clinical decision support has the potential to
significantly improve patient care. However, existing efforts in this domain
for principled quantification of uncertainty have largely been limited to
applications of ad-hoc solutions that do not consistently improve reliability.
In this work, we consider stochastic neural networks and design a tailor-made
multimodal data-driven (M2D2) prior distribution over network parameters. We
use simple and scalable Gaussian mean-field variational inference to train a
Bayesian neural network using the M2D2 prior. We train and evaluate the
proposed approach using clinical time-series data in MIMIC-IV and corresponding
chest X-ray images in MIMIC-CXR for the classification of acute care
conditions. Our empirical results show that the proposed method produces a more
reliable predictive model compared to deterministic and Bayesian neural network
baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00795">Talent-Interview: Web-Client Cheating Detection for Online Exams. (arXiv:2312.00795v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ege_M/0/1/0/all/0/1">Mert Ege</a>, <a href="http://arxiv.org/find/cs/1/au:+Ceyhan_M/0/1/0/all/0/1">Mustafa Ceyhan</a></p>
<p>Online exams are more attractive after the Covid-19 pandemic. Furthermore,
during recruitment, online exams are used. However, there are more cheating
possibilities for online exams. Assigning a proctor for each exam increases
cost. At this point, automatic proctor systems detect possible cheating status.
This article proposes an end-to-end system and submodules to get better results
for online proctoring. Object detection, face recognition, human voice
detection, and segmentation are used in our system. Furthermore, our proposed
model works on the PCs of users, meaning a client-based system. So, server cost
is eliminated. As far as we know, it is the first time the client-based online
proctoring system has been used for recruitment. Online exams are more
attractive after the Covid-19 pandemic. Furthermore, during recruitment, online
exams are used. However, there are more cheating possibilities for online
exams. Assigning a proctor for each exam increases cost. At this point,
automatic proctor systems detect possible cheating status. This article
proposes an end-to-end system and submodules to get better results for online
proctoring. Object detection, face recognition, human voice detection, and
segmentation are used in our system. Furthermore, our proposed model works on
the PCs of users, meaning a client-based system. So, server cost is eliminated.
As far as we know, it is the first time the client-based online proctoring
system has been used for recruitment. Furthermore, this cheating system works
at https://www.talent-interview.com/tr/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00798">A Turing Test: Are AI Chatbots Behaviorally Similar to Humans?. (arXiv:2312.00798v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mei_Q/0/1/0/all/0/1">Qiaozhu Mei</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1">Yutong Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1">Walter Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Jackson_M/0/1/0/all/0/1">Matthew O. Jackson</a></p>
<p>We administer a Turing Test to AI Chatbots. We examine how Chatbots behave in
a suite of classic behavioral games that are designed to elicit characteristics
such as trust, fairness, risk-aversion, cooperation, \textit{etc.}; as well as
a traditional Big-5 psychological survey that measures personality traits.
ChatGPT-4 passes the Turing Test in that it consistently exhibits human-like
behavioral and personality traits based on a comparison to the behavior of
hundreds of thousands of humans from more than 50 countries. Chatbots also
modify their behavior based on previous experience and contexts ``as if'' they
were learning from the interactions, and change their behavior in response to
different framings of the same strategic situation. Their behaviors are often
distinct from average and modal human behaviors, in which case they tend to
behave on the more altruistic and cooperative end of the distribution. We
estimate that they act as if they are maximizing an average of their own and
partner's payoff.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00802">Continuous Authentication Using Mouse Clickstream Data Analysis. (arXiv:2312.00802v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Almalki_S/0/1/0/all/0/1">Sultan Almalki</a>, <a href="http://arxiv.org/find/eess/1/au:+Chatterjee_P/0/1/0/all/0/1">Prosenjit Chatterjee</a>, <a href="http://arxiv.org/find/eess/1/au:+Roy_K/0/1/0/all/0/1">Kaushik Roy</a></p>
<p>Biometrics is used to authenticate an individual based on physiological or
behavioral traits. Mouse dynamics is an example of a behavioral biometric that
can be used to perform continuous authentication as protection against security
breaches. Recent research on mouse dynamics has shown promising results in
identifying users; however, it has not yet reached an acceptable level of
accuracy. In this paper, an empirical evaluation of different classification
techniques is conducted on a mouse dynamics dataset, the Balabit Mouse
Challenge dataset. User identification is carried out using three mouse
actions: mouse move, point and click, and drag and drop. Verification and
authentication methods are conducted using three machine-learning classifiers:
the Decision Tree classifier, the K-Nearest Neighbors classifier, and the
Random Forest classifier. The results show that the three classifiers can
distinguish between a genuine user and an impostor with a relatively high
degree of accuracy. In the verification mode, all the classifiers achieve a
perfect accuracy of 100%. In authentication mode, all three classifiers
achieved the highest accuracy (ACC) and Area Under Curve (AUC) from scenario B
using the point and click action data: (Decision Tree ACC:87.6%, AUC:90.3%),
(K-Nearest Neighbors ACC:99.3%, AUC:99.9%), and (Random Forest ACC:89.9%,
AUC:92.5%).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00805">Gender inference: can chatGPT outperform common commercial tools?. (arXiv:2312.00805v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alexopoulos_M/0/1/0/all/0/1">Michelle Alexopoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyons_K/0/1/0/all/0/1">Kelly Lyons</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahetaji_K/0/1/0/all/0/1">Kaushar Mahetaji</a>, <a href="http://arxiv.org/find/cs/1/au:+Barnes_M/0/1/0/all/0/1">Marcus Emmanuel Barnes</a>, <a href="http://arxiv.org/find/cs/1/au:+Gutwillinger_R/0/1/0/all/0/1">Rogan Gutwillinger</a></p>
<p>An increasing number of studies use gender information to understand
phenomena such as gender bias, inequity in access and participation, or the
impact of the Covid pandemic response. Unfortunately, most datasets do not
include self-reported gender information, making it necessary for researchers
to infer gender from other information, such as names or names and country
information. An important limitation of these tools is that they fail to
appropriately capture the fact that gender exists on a non-binary scale,
however, it remains important to evaluate and compare how well these tools
perform in a variety of contexts. In this paper, we compare the performance of
a generative Artificial Intelligence (AI) tool ChatGPT with three commercially
available list-based and machine learning-based gender inference tools (Namsor,
Gender-API, and genderize.io) on a unique dataset. Specifically, we use a large
Olympic athlete dataset and report how variations in the input (e.g., first
name and first and last name, with and without country information) impact the
accuracy of their predictions. We report results for the full set, as well as
for the subsets: medal versus non-medal winners, athletes from the largest
English-speaking countries, and athletes from East Asia. On these sets, we find
that Namsor is the best traditional commercially available tool. However,
ChatGPT performs at least as well as Namsor and often outperforms it,
especially for the female sample when country and/or last name information is
available. All tools perform better on medalists versus non-medalists and on
names from English-speaking countries. Although not designed for this purpose,
ChatGPT may be a cost-effective tool for gender prediction. In the future, it
might even be possible for ChatGPT or other large scale language models to
better identify self-reported gender rather than report gender on a binary
scale.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00808">Transforming organic chemistry research paradigms: moving from manual efforts to the intersection of automation and artificial intelligence. (arXiv:2312.00808v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chengchun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuntian Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Mo_F/0/1/0/all/0/1">Fanyang Mo</a></p>
<p>Organic chemistry is undergoing a major paradigm shift, moving from a
labor-intensive approach to a new era dominated by automation and artificial
intelligence (AI). This transformative shift is being driven by technological
advances, the ever-increasing demand for greater research efficiency and
accuracy, and the burgeoning growth of interdisciplinary research. AI models,
supported by computational power and algorithms, are drastically reshaping
synthetic planning and introducing groundbreaking ways to tackle complex
molecular synthesis. In addition, autonomous robotic systems are rapidly
accelerating the pace of discovery by performing tedious tasks with
unprecedented speed and precision. This article examines the multiple
opportunities and challenges presented by this paradigm shift and explores its
far-reaching implications. It provides valuable insights into the future
trajectory of organic chemistry research, which is increasingly defined by the
synergistic interaction of automation and AI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00812">Empowering Autonomous Driving with Large Language Models: A Safety Perspective. (arXiv:2312.00812v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yixuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiao_R/0/1/0/all/0/1">Ruochen Jiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lang_C/0/1/0/all/0/1">Chengtian Lang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhan_S/0/1/0/all/0/1">Sinong Simon Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1">Chao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhaoran Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhuoran Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1">Qi Zhu</a></p>
<p>Autonomous Driving (AD) faces crucial hurdles for commercial launch, notably
in the form of diminished public trust and safety concerns from long-tail
unforeseen driving scenarios. This predicament is due to the limitation of deep
neural networks in AD software, which struggle with interpretability and
exhibit poor generalization capabilities in out-of-distribution and uncertain
scenarios. To this end, this paper advocates for the integration of Large
Language Models (LLMs) into the AD system, leveraging their robust common-sense
knowledge, reasoning abilities, and human-interaction capabilities. The
proposed approach deploys the LLM as an intelligent decision-maker in planning,
incorporating safety verifiers for contextual safety learning to enhance
overall AD performance and safety. We present results from two case studies
that affirm the efficacy of our approach. We further discuss the potential
integration of LLM for other AD software components including perception,
prediction, and simulation. Despite the observed challenges in the case
studies, the integration of LLMs is promising and beneficial for reinforcing
both safety and performance in AD.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00817">TimelyGPT: Recurrent Convolutional Transformer for Long Time-series Representation. (arXiv:2312.00817v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1">Ziyang Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1">Qincheng Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yue Li</a></p>
<p>Pre-trained models (PTMs) have gained prominence in Natural Language
Processing and Computer Vision domains. When it comes to time-series PTMs,
their development has been limited. Previous research on time-series
transformers has mainly been devoted to small-scale tasks, yet these models
have not consistently outperformed traditional models. Additionally, the
performance of these transformers on large-scale data remains unexplored. These
findings raise doubts about Transformer's capabilities to scale up and capture
temporal dependencies. In this study, we re-examine time-series transformers
and identify the shortcomings of prior studies. Drawing from these insights, we
then introduce a pioneering architecture called Timely Generative Pre-trained
Transformer (\model). This architecture integrates recurrent attention and
temporal convolution modules to effectively capture global-local temporal
dependencies in long sequences. The relative position embedding with time decay
can effectively deal with trend and periodic patterns from time-series. Our
experiments show that \model~excels in modeling continuously monitored
biosignal as well as irregularly-sampled time-series data commonly observed in
longitudinal electronic health records. This breakthrough suggests a priority
shift in time-series deep learning research, moving from small-scale modeling
from scratch to large-scale pre-training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00818">The perpetual motion machine of AI-generated data and the distraction of ChatGPT-as-scientist. (arXiv:2312.00818v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Listgarten_J/0/1/0/all/0/1">Jennifer Listgarten</a></p>
<p>Since ChatGPT works so well, are we on the cusp of solving science with AI?
Is not AlphaFold2 suggestive that the potential of LLMs in biology and the
sciences more broadly is limitless? Can we use AI itself to bridge the lack of
data in the sciences in order to then train an AI? Herein we present a
discussion of these topics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00819">Large Language Models for Travel Behavior Prediction. (arXiv:2312.00819v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mo_B/0/1/0/all/0/1">Baichuan Mo</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hanyong Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_D/0/1/0/all/0/1">Dingyi Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1">Ruoyun Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1">Xiaotong Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jinhua Zhao</a></p>
<p>Travel behavior prediction is a fundamental task in transportation demand
management. The conventional methods for travel behavior prediction rely on
numerical data to construct mathematical models and calibrate model parameters
to represent human preferences. Recent advancement in large language models
(LLMs) has shown great reasoning abilities to solve complex problems. In this
study, we propose to use LLMs to predict travel behavior with prompt
engineering without data-based parameter learning. Specifically, we carefully
design our prompts that include 1) task description, 2) travel characteristics,
3) individual attributes, and 4) guides of thinking with domain knowledge, and
ask the LLMs to predict an individual's travel behavior and explain the
results. We select the travel mode choice task as a case study. Results show
that, though no training samples are provided, LLM-based predictions have
competitive accuracy and F1-score as canonical supervised learning methods such
as multinomial logit, random forest, and neural networks. LLMs can also output
reasons that support their prediction. However, though in most of the cases,
the output explanations are reasonable, we still observe cases that violate
logic or with hallucinations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00820">Non-Cross Diffusion for Semantic Consistency. (arXiv:2312.00820v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Ziyang Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1">Ruiyuan Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1">Qiang Xu</a></p>
<p>In diffusion models, deviations from a straight generative flow are a common
issue, resulting in semantic inconsistencies and suboptimal generations. To
address this challenge, we introduce `Non-Cross Diffusion', an innovative
approach in generative modeling for learning ordinary differential equation
(ODE) models. Our methodology strategically incorporates an ascending dimension
of input to effectively connect points sampled from two distributions with
uncrossed paths. This design is pivotal in ensuring enhanced semantic
consistency throughout the inference process, which is especially critical for
applications reliant on consistent generative flows, including various
distillation methods and deterministic sampling, which are fundamental in image
editing and interpolation tasks. Our empirical results demonstrate the
effectiveness of Non-Cross Diffusion, showing a substantial reduction in
semantic inconsistencies at different inference steps and a notable enhancement
in the overall performance of diffusion models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00823">Adaptive Multi-Modality Prompt Learning. (arXiv:2312.00823v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zongqian Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yujing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhan_M/0/1/0/all/0/1">Mengmeng Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1">Jialie Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1">Ping Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiaofeng Zhu</a></p>
<p>Although current prompt learning methods have successfully been designed to
effectively reuse the large pre-trained models without fine-tuning their large
number of parameters, they still have limitations to be addressed, i.e.,
without considering the adverse impact of meaningless patches in every image
and without simultaneously considering in-sample generalization and
out-of-sample generalization. In this paper, we propose an adaptive
multi-modality prompt learning to address the above issues. To do this, we
employ previous text prompt learning and propose a new image prompt learning.
The image prompt learning achieves in-sample and out-of-sample generalization,
by first masking meaningless patches and then padding them with the learnable
parameters and the information from texts. Moreover, each of the prompts
provides auxiliary information to each other, further strengthening these two
kinds of generalization. Experimental results on real datasets demonstrate that
our method outperforms SOTA methods, in terms of different downstream tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00825">Probing and Mitigating Intersectional Social Biases in Vision-Language Models with Counterfactual Examples. (arXiv:2312.00825v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Howard_P/0/1/0/all/0/1">Phillip Howard</a>, <a href="http://arxiv.org/find/cs/1/au:+Madasu_A/0/1/0/all/0/1">Avinash Madasu</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1">Tiep Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Moreno_G/0/1/0/all/0/1">Gustavo Lujan Moreno</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhiwandiwalla_A/0/1/0/all/0/1">Anahita Bhiwandiwalla</a>, <a href="http://arxiv.org/find/cs/1/au:+Lal_V/0/1/0/all/0/1">Vasudev Lal</a></p>
<p>While vision-language models (VLMs) have achieved remarkable performance
improvements recently, there is growing evidence that these models also posses
harmful biases with respect to social attributes such as gender and race. Prior
studies have primarily focused on probing such bias attributes individually
while ignoring biases associated with intersections between social attributes.
This could be due to the difficulty of collecting an exhaustive set of
image-text pairs for various combinations of social attributes. To address this
challenge, we employ text-to-image diffusion models to produce counterfactual
examples for probing intserctional social biases at scale. Our approach
utilizes Stable Diffusion with cross attention control to produce sets of
counterfactual image-text pairs that are highly similar in their depiction of a
subject (e.g., a given occupation) while differing only in their depiction of
intersectional social attributes (e.g., race &amp; gender). Through our
over-generate-then-filter methodology, we produce SocialCounterfactuals, a
high-quality dataset containing over 171k image-text pairs for probing
intersectional biases related to gender, race, and physical characteristics. We
conduct extensive experiments to demonstrate the usefulness of our generated
dataset for probing and mitigating intersectional social biases in
state-of-the-art VLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00839">PipeOptim: Ensuring Effective 1F1B Schedule with Optimizer-Dependent Weight Prediction. (arXiv:2312.00839v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guan_L/0/1/0/all/0/1">Lei Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dongsheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1">Jiye Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenjian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1">Xicheng Lu</a></p>
<p>Asynchronous pipeline model parallelism with a "1F1B" (one forward, one
backward) schedule generates little bubble overhead and always provides quite a
high throughput. However, the "1F1B" schedule inevitably leads to weight
inconsistency and weight staleness issues due to the cross-training of
different mini-batches across GPUs. To simultaneously address these two
problems, in this paper, we propose an optimizer-dependent weight prediction
strategy (a.k.a PipeOptim) for asynchronous pipeline training. The key insight
of our proposal is that we employ a weight prediction strategy in the forward
pass to ensure that each mini-batch uses consistent and staleness-free weights
to compute the forward pass. To be concrete, we first construct the weight
prediction scheme based on the update rule of the used optimizer when training
the deep neural network models. Then throughout the "1F1B" pipelined training,
each mini-batch is mandated to execute weight prediction ahead of the forward
pass, subsequently employing the predicted weights to perform the forward pass.
As a result, PipeOptim 1) inherits the advantage of the "1F1B" schedule and
generates pretty high throughput, and 2) can ensure effective parameter
learning regardless of the type of the used optimizer. To verify the
effectiveness of our proposal, we conducted extensive experimental evaluations
using eight different deep-learning models spanning three machine-learning
tasks including image classification, sentiment analysis, and machine
translation. The experiment results demonstrate that PipeOptim outperforms the
popular pipelined approaches including GPipe, PipeDream, PipeDream-2BW, and
SpecTrain. The code of PipeOptim will be accessible at
https://github.com/guanleics/PipeOptim.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00843">Exploring the Robustness of Decentralized Training for Large Language Models. (arXiv:2312.00843v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1">Lin Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_C/0/1/0/all/0/1">Chenxi Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_W/0/1/0/all/0/1">Wangcheng Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1">Binhang Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yanan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1">Pan Zhou</a></p>
<p>Decentralized training of large language models has emerged as an effective
way to democratize this technology. However, the potential threats associated
with this approach have not been carefully discussed, which would hinder the
development of decentralized training infrastructures. This paper aims to
initiate discussion towards this end by exploring the robustness of
decentralized training from three main perspectives. First, we demonstrate the
vulnerabilities inherent in decentralized training frameworks in terms of
hardware, data, and models. Second, we highlight the fundamental difference
between decentralized foundation model training and vanilla federated learning,
where the security techniques employed in federated learning cannot be applied
directly. Third, we discuss the essential components required for a robust and
efficient decentralized training framework and present a case study by modeling
a concrete threat model. Our objective in this vision paper is to emphasize the
importance of addressing security concerns in the context of decentralized
training for large language models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00844">Sparse Beats Dense: Rethinking Supervision in Radar-Camera Depth Completion. (arXiv:2312.00844v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Huadong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Jing_M/0/1/0/all/0/1">Minhao Jing</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1">Jiajun Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1">Haoqiang Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1">Renhe Ji</a></p>
<p>It is widely believed that the dense supervision is better than the sparse
supervision in the field of depth completion, but the underlying reasons for
this are rarely discussed. In this paper, we find that the challenge of using
sparse supervision for training Radar-Camera depth prediction models is the
Projection Transformation Collapse (PTC). The PTC implies that sparse
supervision leads the model to learn unexpected collapsed projection
transformations between Image/Radar/LiDAR spaces. Building on this insight, we
propose a novel ``Disruption-Compensation" framework to handle the PTC, thereby
relighting the use of sparse supervision in depth completion tasks. The
disruption part deliberately discards position correspondences among
Image/Radar/LiDAR, while the compensation part leverages 3D spatial and 2D
semantic information to compensate for the discarded beneficial position
correspondence. Extensive experimental results demonstrate that our framework
(sparse supervision) outperforms the state-of-the-art (dense supervision) with
11.6$\%$ improvement in mean absolute error and $1.6 \times$ speedup. The code
is available at ...
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00845">VMC: Video Motion Customization using Temporal Attention Adaption for Text-to-Video Diffusion Models. (arXiv:2312.00845v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jeong_H/0/1/0/all/0/1">Hyeonho Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_G/0/1/0/all/0/1">Geon Yeong Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1">Jong Chul Ye</a></p>
<p>Text-to-video diffusion models have advanced video generation significantly.
However, customizing these models to generate videos with tailored motions
presents a substantial challenge. In specific, they encounter hurdles in (a)
accurately reproducing motion from a target video, and (b) creating diverse
visual variations. For example, straightforward extensions of static image
customization methods to video often lead to intricate entanglements of
appearance and motion data. To tackle this, here we present the Video Motion
Customization (VMC) framework, a novel one-shot tuning approach crafted to
adapt temporal attention layers within video diffusion models. Our approach
introduces a novel motion distillation objective using residual vectors between
consecutive frames as a motion reference. The diffusion process then preserves
low-frequency motion trajectories while mitigating high-frequency
motion-unrelated noise in image space. We validate our method against
state-of-the-art video generative models across diverse real-world motions and
contexts. Our codes, data and the project demo can be found at
https://video-motion-customization.github.io
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00854">A Probabilistic Neural Twin for Treatment Planning in Peripheral Pulmonary Artery Stenosis. (arXiv:2312.00854v1 [physics.med-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Lee_J/0/1/0/all/0/1">John D. Lee</a>, <a href="http://arxiv.org/find/physics/1/au:+Richter_J/0/1/0/all/0/1">Jakob Richter</a>, <a href="http://arxiv.org/find/physics/1/au:+Pfaller_M/0/1/0/all/0/1">Martin R. Pfaller</a>, <a href="http://arxiv.org/find/physics/1/au:+Szafron_J/0/1/0/all/0/1">Jason M. Szafron</a>, <a href="http://arxiv.org/find/physics/1/au:+Menon_K/0/1/0/all/0/1">Karthik Menon</a>, <a href="http://arxiv.org/find/physics/1/au:+Zanoni_A/0/1/0/all/0/1">Andrea Zanoni</a>, <a href="http://arxiv.org/find/physics/1/au:+Ma_M/0/1/0/all/0/1">Michael R. Ma</a>, <a href="http://arxiv.org/find/physics/1/au:+Feinstein_J/0/1/0/all/0/1">Jeffrey A. Feinstein</a>, <a href="http://arxiv.org/find/physics/1/au:+Kreutzer_J/0/1/0/all/0/1">Jacqueline Kreutzer</a>, <a href="http://arxiv.org/find/physics/1/au:+Marsden_A/0/1/0/all/0/1">Alison L. Marsden</a>, <a href="http://arxiv.org/find/physics/1/au:+Schiavazzi_D/0/1/0/all/0/1">Daniele E. Schiavazzi</a></p>
<p>The substantial computational cost of high-fidelity models in numerical
hemodynamics has, so far, relegated their use mainly to offline treatment
planning. New breakthroughs in data-driven architectures and optimization
techniques for fast surrogate modeling provide an exciting opportunity to
overcome these limitations, enabling the use of such technology for
time-critical decisions. We discuss an application to the repair of multiple
stenosis in peripheral pulmonary artery disease through either transcatheter
pulmonary artery rehabilitation or surgery, where it is of interest to achieve
desired pressures and flows at specific locations in the pulmonary artery tree,
while minimizing the risk for the patient. Since different degrees of success
can be achieved in practice during treatment, we formulate the problem in
probability, and solve it through a sample-based approach. We propose a new
offline-online pipeline for probabilsitic real-time treatment planning which
combines offline assimilation of boundary conditions, model reduction, and
training dataset generation with online estimation of marginal probabilities,
possibly conditioned on the degree of augmentation observed in already repaired
lesions. Moreover, we propose a new approach for the parametrization of
arbitrarily shaped vascular repairs through iterative corrections of a
zero-dimensional approximant. We demonstrate this pipeline for a diseased model
of the pulmonary artery tree available through the Vascular Model Repository.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00855">Refine, Discriminate and Align: Stealing Encoders via Sample-Wise Prototypes and Multi-Relational Extraction. (arXiv:2312.00855v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Shuchi Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1">Chuan Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1">Kang Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiaogang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1">Ming Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1">Yuwen Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1">Tao Xiang</a></p>
<p>This paper introduces RDA, a pioneering approach designed to address two
primary deficiencies prevalent in previous endeavors aiming at stealing
pre-trained encoders: (1) suboptimal performances attributed to biased
optimization objectives, and (2) elevated query costs stemming from the
end-to-end paradigm that necessitates querying the target encoder every epoch.
Specifically, we initially Refine the representations of the target encoder for
each training sample, thereby establishing a less biased optimization objective
before the steal-training phase. This is accomplished via a sample-wise
prototype, which consolidates the target encoder's representations for a given
sample's various perspectives. Demanding exponentially fewer queries compared
to the end-to-end approach, prototypes can be instantiated to guide subsequent
query-free training. For more potent efficacy, we develop a multi-relational
extraction loss that trains the surrogate encoder to Discriminate mismatched
embedding-prototype pairs while Aligning those matched ones in terms of both
amplitude and angle. In this way, the trained surrogate encoder achieves
state-of-the-art results across the board in various downstream datasets with
limited queries. Moreover, RDA is shown to be robust to multiple widely-used
defenses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00857">Latent Space Explorer: Visual Analytics for Multimodal Latent Space Exploration. (arXiv:2312.00857v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kwon_B/0/1/0/all/0/1">Bum Chul Kwon</a>, <a href="http://arxiv.org/find/cs/1/au:+Friedman_S/0/1/0/all/0/1">Samuel Friedman</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Kai Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lubitz_S/0/1/0/all/0/1">Steven A Lubitz</a>, <a href="http://arxiv.org/find/cs/1/au:+Philippakis_A/0/1/0/all/0/1">Anthony Philippakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Batra_P/0/1/0/all/0/1">Puneet Batra</a>, <a href="http://arxiv.org/find/cs/1/au:+Ellinor_P/0/1/0/all/0/1">Patrick T Ellinor</a>, <a href="http://arxiv.org/find/cs/1/au:+Ng_K/0/1/0/all/0/1">Kenney Ng</a></p>
<p>Machine learning models built on training data with multiple modalities can
reveal new insights that are not accessible through unimodal datasets. For
example, cardiac magnetic resonance images (MRIs) and electrocardiograms (ECGs)
are both known to capture useful information about subjects' cardiovascular
health status. A multimodal machine learning model trained from large datasets
can potentially predict the onset of heart-related diseases and provide novel
medical insights about the cardiovascular system. Despite the potential
benefits, it is difficult for medical experts to explore multimodal
representation models without visual aids and to test the predictive
performance of the models on various subpopulations. To address the challenges,
we developed a visual analytics system called Latent Space Explorer. Latent
Space Explorer provides interactive visualizations that enable users to explore
the multimodal representation of subjects, define subgroups of interest,
interactively decode data with different modalities with the selected subjects,
and inspect the accuracy of the embedding in downstream prediction tasks. A
user study was conducted with medical experts and their feedback provided
useful insights into how Latent Space Explorer can help their analysis and
possible new direction for further development in the medical domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00858">DeepCache: Accelerating Diffusion Models for Free. (arXiv:2312.00858v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xinyin Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_G/0/1/0/all/0/1">Gongfan Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinchao Wang</a></p>
<p>Diffusion models have recently gained unprecedented attention in the field of
image synthesis due to their remarkable generative capabilities.
Notwithstanding their prowess, these models often incur substantial
computational costs, primarily attributed to the sequential denoising process
and cumbersome model size. Traditional methods for compressing diffusion models
typically involve extensive retraining, presenting cost and feasibility
challenges. In this paper, we introduce DeepCache, a novel training-free
paradigm that accelerates diffusion models from the perspective of model
architecture. DeepCache capitalizes on the inherent temporal redundancy
observed in the sequential denoising steps of diffusion models, which caches
and retrieves features across adjacent denoising stages, thereby curtailing
redundant computations. Utilizing the property of the U-Net, we reuse the
high-level features while updating the low-level features in a very cheap way.
This innovative strategy, in turn, enables a speedup factor of 2.3$\times$ for
Stable Diffusion v1.5 with only a 0.05 decline in CLIP Score, and 4.1$\times$
for LDM-4-G with a slight decrease of 0.22 in FID on ImageNet. Our experiments
also demonstrate DeepCache's superiority over existing pruning and distillation
methods that necessitate retraining and its compatibility with current sampling
techniques. Furthermore, we find that under the same throughput, DeepCache
effectively achieves comparable or even marginally improved results with DDIM
or PLMS. The code is available at https://github.com/horseee/DeepCache
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00870">3DiFACE: Diffusion-based Speech-driven 3D Facial Animation and Editing. (arXiv:2312.00870v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Thambiraja_B/0/1/0/all/0/1">Balamurugan Thambiraja</a>, <a href="http://arxiv.org/find/cs/1/au:+Aliakbarian_S/0/1/0/all/0/1">Sadegh Aliakbarian</a>, <a href="http://arxiv.org/find/cs/1/au:+Cosker_D/0/1/0/all/0/1">Darren Cosker</a>, <a href="http://arxiv.org/find/cs/1/au:+Thies_J/0/1/0/all/0/1">Justus Thies</a></p>
<p>We present 3DiFACE, a novel method for personalized speech-driven 3D facial
animation and editing. While existing methods deterministically predict facial
animations from speech, they overlook the inherent one-to-many relationship
between speech and facial expressions, i.e., there are multiple reasonable
facial expression animations matching an audio input. It is especially
important in content creation to be able to modify generated motion or to
specify keyframes. To enable stochasticity as well as motion editing, we
propose a lightweight audio-conditioned diffusion model for 3D facial motion.
This diffusion model can be trained on a small 3D motion dataset, maintaining
expressive lip motion output. In addition, it can be finetuned for specific
subjects, requiring only a short video of the person. Through quantitative and
qualitative evaluations, we show that our method outperforms existing
state-of-the-art techniques and yields speech-driven animations with greater
fidelity and diversity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00878">Grounding Everything: Emerging Localization Properties in Vision-Language Transformers. (arXiv:2312.00878v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bousselham_W/0/1/0/all/0/1">Walid Bousselham</a>, <a href="http://arxiv.org/find/cs/1/au:+Petersen_F/0/1/0/all/0/1">Felix Petersen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferrari_V/0/1/0/all/0/1">Vittorio Ferrari</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuehne_H/0/1/0/all/0/1">Hilde Kuehne</a></p>
<p>Vision-language foundation models have shown remarkable performance in
various zero-shot settings such as image retrieval, classification, or
captioning. But so far, those models seem to fall behind when it comes to
zero-shot localization of referential expressions and objects in images. As a
result, they need to be fine-tuned for this task. In this paper, we show that
pretrained vision-language (VL) models allow for zero-shot open-vocabulary
object localization without any fine-tuning. To leverage those capabilities, we
propose a Grounding Everything Module (GEM) that generalizes the idea of
value-value attention introduced by CLIPSurgery to a self-self attention path.
We show that the concept of self-self attention corresponds to clustering, thus
enforcing groups of tokens arising from the same object to be similar while
preserving the alignment with the language space. To further guide the group
formation, we propose a set of regularizations that allows the model to finally
generalize across datasets and backbones. We evaluate the proposed GEM
framework on various benchmark tasks and datasets for semantic segmentation. It
shows that GEM not only outperforms other training-free open-vocabulary
localization methods, but also achieves state-of-the-art results on the
recently proposed OpenImagesV7 large-scale segmentation benchmark.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00886">Nash Learning from Human Feedback. (arXiv:2312.00886v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Munos_R/0/1/0/all/0/1">R&#xe9;mi Munos</a>, <a href="http://arxiv.org/find/stat/1/au:+Valko_M/0/1/0/all/0/1">Michal Valko</a>, <a href="http://arxiv.org/find/stat/1/au:+Calandriello_D/0/1/0/all/0/1">Daniele Calandriello</a>, <a href="http://arxiv.org/find/stat/1/au:+Azar_M/0/1/0/all/0/1">Mohammad Gheshlaghi Azar</a>, <a href="http://arxiv.org/find/stat/1/au:+Rowland_M/0/1/0/all/0/1">Mark Rowland</a>, <a href="http://arxiv.org/find/stat/1/au:+Guo_D/0/1/0/all/0/1">Daniel Guo</a>, <a href="http://arxiv.org/find/stat/1/au:+Tang_Y/0/1/0/all/0/1">Yunhao Tang</a>, <a href="http://arxiv.org/find/stat/1/au:+Geist_M/0/1/0/all/0/1">Matthieu Geist</a>, <a href="http://arxiv.org/find/stat/1/au:+Mesnard_T/0/1/0/all/0/1">Thomas M&#xe9;snard</a>, <a href="http://arxiv.org/find/stat/1/au:+Michi_A/0/1/0/all/0/1">Andrea Michi</a>, <a href="http://arxiv.org/find/stat/1/au:+Selvi_M/0/1/0/all/0/1">Marco Selvi</a>, <a href="http://arxiv.org/find/stat/1/au:+Girgin_S/0/1/0/all/0/1">Sertan Girgin</a>, <a href="http://arxiv.org/find/stat/1/au:+Momchev_N/0/1/0/all/0/1">Nikola Momchev</a>, <a href="http://arxiv.org/find/stat/1/au:+Bachem_O/0/1/0/all/0/1">Olivier Bachem</a>, <a href="http://arxiv.org/find/stat/1/au:+Mankowitz_D/0/1/0/all/0/1">Daniel J. Mankowitz</a>, <a href="http://arxiv.org/find/stat/1/au:+Precup_D/0/1/0/all/0/1">Doina Precup</a>, <a href="http://arxiv.org/find/stat/1/au:+Piot_B/0/1/0/all/0/1">Bilal Piot</a></p>
<p>Reinforcement learning from human feedback (RLHF) has emerged as the main
paradigm for aligning large language models (LLMs) with human preferences.
Typically, RLHF involves the initial step of learning a reward model from human
feedback, often expressed as preferences between pairs of text generations
produced by a pre-trained LLM. Subsequently, the LLM's policy is fine-tuned by
optimizing it to maximize the reward model through a reinforcement learning
algorithm. However, an inherent limitation of current reward models is their
inability to fully represent the richness of human preferences and their
dependency on the sampling distribution.
</p>
<p>In this study, we introduce an alternative pipeline for the fine-tuning of
LLMs using pairwise human feedback. Our approach entails the initial learning
of a preference model, which is conditioned on two inputs given a prompt,
followed by the pursuit of a policy that consistently generates responses
preferred over those generated by any competing policy, thus defining the Nash
equilibrium of this preference model. We term this approach Nash learning from
human feedback (NLHF).
</p>
<p>In the context of a tabular policy representation, we present a novel
algorithmic solution, Nash-MD, founded on the principles of mirror descent.
This algorithm produces a sequence of policies, with the last iteration
converging to the regularized Nash equilibrium. Additionally, we explore
parametric representations of policies and introduce gradient descent
algorithms for deep-learning architectures. To demonstrate the effectiveness of
our approach, we present experimental results involving the fine-tuning of a
LLM for a text summarization task. We believe NLHF offers a compelling avenue
for preference learning and policy optimization with the potential of advancing
the field of aligning LLMs with human preferences.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00909">LLM-TAKE: Theme Aware Keyword Extraction Using Large Language Models. (arXiv:2312.00909v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maragheh_R/0/1/0/all/0/1">Reza Yousefi Maragheh</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1">Chenhao Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Irugu_C/0/1/0/all/0/1">Charan Chand Irugu</a>, <a href="http://arxiv.org/find/cs/1/au:+Parikh_P/0/1/0/all/0/1">Parth Parikh</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1">Jason Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jianpeng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sukumar_S/0/1/0/all/0/1">Saranyan Sukumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Patel_M/0/1/0/all/0/1">Malay Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Korpeoglu_E/0/1/0/all/0/1">Evren Korpeoglu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1">Sushant Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Achan_K/0/1/0/all/0/1">Kannan Achan</a></p>
<p>Keyword extraction is one of the core tasks in natural language processing.
Classic extraction models are notorious for having a short attention span which
make it hard for them to conclude relational connections among the words and
sentences that are far from each other. This, in turn, makes their usage
prohibitive for generating keywords that are inferred from the context of the
whole text. In this paper, we explore using Large Language Models (LLMs) in
generating keywords for items that are inferred from the items textual
metadata. Our modeling framework includes several stages to fine grain the
results by avoiding outputting keywords that are non informative or sensitive
and reduce hallucinations common in LLM. We call our LLM-based framework
Theme-Aware Keyword Extraction (LLM TAKE). We propose two variations of
framework for generating extractive and abstractive themes for products in an E
commerce setting. We perform an extensive set of experiments on three real data
sets and show that our modeling framework can enhance accuracy based and
diversity based metrics when compared with benchmark models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00910">Effectiveness of probabilistic contact tracing in epidemic containment: the role of super-spreaders and transmission paths reconstruction. (arXiv:2312.00910v1 [q-bio.PE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Muntoni_A/0/1/0/all/0/1">A.P. Muntoni</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Mazza_F/0/1/0/all/0/1">F. Mazza</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Braunstein_A/0/1/0/all/0/1">A. Braunstein</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Catania_G/0/1/0/all/0/1">G. Catania</a>, <a href="http://arxiv.org/find/q-bio/1/au:+DallAsta_L/0/1/0/all/0/1">L. Dall&#x27;Asta</a></p>
<p>The recent COVID-19 pandemic underscores the significance of early-stage
non-pharmacological intervention strategies. The widespread use of masks and
the systematic implementation of contact tracing strategies provide a
potentially equally effective and socially less impactful alternative to more
conventional approaches, such as large-scale mobility restrictions. However,
manual contact tracing faces strong limitations in accessing the network of
contacts, and the scalability of currently implemented protocols for
smartphone-based digital contact tracing becomes impractical during the rapid
expansion phases of the outbreaks, due to the surge in exposure notifications
and associated tests. A substantial improvement in digital contact tracing can
be obtained through the integration of probabilistic techniques for risk
assessment that can more effectively guide the allocation of new diagnostic
tests. In this study, we first quantitatively analyze the diagnostic and social
costs associated with these containment measures based on contact tracing,
employing three state-of-the-art models of SARS-CoV-2 spreading. Our results
suggest that probabilistic techniques allow for more effective mitigation at a
lower cost. Secondly, our findings reveal a remarkable efficacy of
probabilistic contact-tracing techniques in capturing backward propagations and
super-spreading events, relevant features of the diffusion of many pathogens,
including SARS-CoV-2.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00960">The Cost of Compression: Investigating the Impact of Compression on Parametric Knowledge in Language Models. (arXiv:2312.00960v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Namburi_S/0/1/0/all/0/1">Satya Sai Srinath Namburi</a>, <a href="http://arxiv.org/find/cs/1/au:+Sreedhar_M/0/1/0/all/0/1">Makesh Sreedhar</a>, <a href="http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1">Srinath Srinivasan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sala_F/0/1/0/all/0/1">Frederic Sala</a></p>
<p>Compressing large language models (LLMs), often consisting of billions of
parameters, provides faster inference, smaller memory footprints, and enables
local deployment. Two standard compression techniques are pruning and
quantization, with the former eliminating redundant connections in model layers
and the latter representing model parameters with fewer bits. The key tradeoff
is between the degree of compression and the impact on the quality of the
compressed model. Existing research on LLM compression primarily focuses on
performance in terms of general metrics like perplexity or downstream task
accuracy. More fine-grained metrics, such as those measuring parametric
knowledge, remain significantly underexplored. To help bridge this gap, we
present a comprehensive analysis across multiple model families (ENCODER,
ENCODER-DECODER, and DECODER) using the LAMA and LM-HARNESS benchmarks in order
to systematically quantify the effect of commonly employed compression
techniques on model performance. A particular focus is on tradeoffs involving
parametric knowledge, with the goal of providing practitioners with practical
insights to help make informed decisions on compression. We release our
codebase1 to enable further research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00966">Spectral Temporal Contrastive Learning. (arXiv:2312.00966v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Morin_S/0/1/0/all/0/1">Sacha Morin</a>, <a href="http://arxiv.org/find/cs/1/au:+Nath_S/0/1/0/all/0/1">Somjit Nath</a>, <a href="http://arxiv.org/find/cs/1/au:+Kahou_S/0/1/0/all/0/1">Samira Ebrahimi Kahou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wolf_G/0/1/0/all/0/1">Guy Wolf</a></p>
<p>Learning useful data representations without requiring labels is a
cornerstone of modern deep learning. Self-supervised learning methods,
particularly contrastive learning (CL), have proven successful by leveraging
data augmentations to define positive pairs. This success has prompted a number
of theoretical studies to better understand CL and investigate theoretical
bounds for downstream linear probing tasks. This work is concerned with the
temporal contrastive learning (TCL) setting where the sequential structure of
the data is used instead to define positive pairs, which is more commonly used
in RL and robotics contexts. In this paper, we adapt recent work on Spectral CL
to formulate Spectral Temporal Contrastive Learning (STCL). We discuss a
population loss based on a state graph derived from a time-homogeneous
reversible Markov chain with uniform stationary distribution. The STCL loss
enables to connect the linear probing performance to the spectral properties of
the graph, and can be estimated by considering previously observed data
sequences as an ensemble of MCMC chains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01001">Learning county from pixels: Corn yield prediction with attention-weighted multiple instance learning. (arXiv:2312.01001v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaoyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yuchi Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1">Qunying Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhengwei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhou Zhang</a></p>
<p>Remote sensing technology has become a promising tool in yield prediction.
Most prior work employs satellite imagery for county-level corn yield
prediction by spatially aggregating all pixels within a county into a single
value, potentially overlooking the detailed information and valuable insights
offered by more granular data. To this end, this research examines each county
at the pixel level and applies multiple instance learning to leverage detailed
information within a county. In addition, our method addresses the "mixed
pixel" issue caused by the inconsistent resolution between feature datasets and
crop mask, which may introduce noise into the model and therefore hinder
accurate yield prediction. Specifically, the attention mechanism is employed to
automatically assign weights to different pixels, which can mitigate the
influence of mixed pixels. The experimental results show that the developed
model outperforms four other machine learning models over the past five years
in the U.S. corn belt and demonstrates its best performance in 2022, achieving
a coefficient of determination (R2) value of 0.84 and a root mean square error
(RMSE) of 0.83. This paper demonstrates the advantages of our approach from
both spatial and temporal perspectives. Furthermore, through an in-depth study
of the relationship between mixed pixels and attention, it is verified that our
approach can capture critical feature information while filtering out noise
from mixed pixels.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01007">A Hypergraph-Based Approach to Recommend Online Resources in a Library. (arXiv:2312.01007v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Roy_D/0/1/0/all/0/1">Debashish Roy</a>, <a href="http://arxiv.org/find/cs/1/au:+Chowdhury_R/0/1/0/all/0/1">Rajarshi Roy Chowdhury</a></p>
<p>When users in a digital library read or browse online resources, it generates
an immense amount of data. If the underlying system can recommend items, such
as books and journals, to the users, it will help them to find the related
items. This research analyzes a digital library's usage data to recommend items
to its users, and it uses different clustering algorithms to design the
recommender system. We have used content-based clustering, including
hierarchical, expectation maximization (EM), K-mean, FarthestFirst, and
density-based clustering algorithms, and user access pattern-based clustering,
which uses a hypergraph-based approach to generate the clusters. This research
shows that the recommender system designed using the hypergraph algorithm
generates the most accurate recommendation model compared to those designed
using the content-based clustering approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01017">Unveiling the Power of Audio-Visual Early Fusion Transformers with Dense Interactions through Masked Modeling. (arXiv:2312.01017v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1">Shentong Mo</a>, <a href="http://arxiv.org/find/cs/1/au:+Morgado_P/0/1/0/all/0/1">Pedro Morgado</a></p>
<p>Humans possess a remarkable ability to integrate auditory and visual
information, enabling a deeper understanding of the surrounding environment.
This early fusion of audio and visual cues, demonstrated through cognitive
psychology and neuroscience research, offers promising potential for developing
multimodal perception models. However, training early fusion architectures
poses significant challenges, as the increased model expressivity requires
robust learning frameworks to harness their enhanced capabilities. In this
paper, we address this challenge by leveraging the masked reconstruction
framework, previously successful in unimodal settings, to train audio-visual
encoders with early fusion. Additionally, we propose an attention-based fusion
module that captures interactions between local audio and visual
representations, enhancing the model's ability to capture fine-grained
interactions. While effective, this procedure can become computationally
intractable, as the number of local representations increases. Thus, to address
the computational complexity, we propose an alternative procedure that
factorizes the local representations before representing audio-visual
interactions. Extensive evaluations on a variety of datasets demonstrate the
superiority of our approach in audio-event classification, visual sound
localization, sound separation, and audio-visual segmentation. These
contributions enable the efficient training of deeply integrated audio-visual
models and significantly advance the usefulness of early fusion architectures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01024">Hybrid Quantum Neural Network in High-dimensional Data Classification. (arXiv:2312.01024v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hao-Yuan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1">Yen-Jui Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1">Shih-Wei Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1">Ching-Ray Chang</a></p>
<p>The research explores the potential of quantum deep learning models to
address challenging machine learning problems that classical deep learning
models find difficult to tackle. We introduce a novel model architecture that
combines classical convolutional layers with a quantum neural network, aiming
to surpass state-of-the-art accuracy while maintaining a compact model size.
The experiment is to classify high-dimensional audio data from the Bird-CLEF
2021 dataset. Our evaluation focuses on key metrics, including training
duration, model accuracy, and total model size. This research demonstrates the
promising potential of quantum machine learning in enhancing machine learning
tasks and solving practical machine learning challenges available today.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01032">Harnessing the Power of Prompt-based Techniques for Generating School-Level Questions using Large Language Models. (arXiv:2312.01032v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maity_S/0/1/0/all/0/1">Subhankar Maity</a>, <a href="http://arxiv.org/find/cs/1/au:+Deroy_A/0/1/0/all/0/1">Aniket Deroy</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1">Sudeshna Sarkar</a></p>
<p>Designing high-quality educational questions is a challenging and
time-consuming task. In this work, we propose a novel approach that utilizes
prompt-based techniques to generate descriptive and reasoning-based questions.
However, current question-answering (QA) datasets are inadequate for conducting
our experiments on prompt-based question generation (QG) in an educational
setting. Therefore, we curate a new QG dataset called EduProbe for school-level
subjects, by leveraging the rich content of NCERT textbooks. We carefully
annotate this dataset as quadruples of 1) Context: a segment upon which the
question is formed; 2) Long Prompt: a long textual cue for the question (i.e.,
a longer sequence of words or phrases, covering the main theme of the context);
3) Short Prompt: a short textual cue for the question (i.e., a condensed
representation of the key information or focus of the context); 4) Question: a
deep question that aligns with the context and is coherent with the prompts. We
investigate several prompt-based QG methods by fine-tuning pre-trained
transformer-based large language models (LLMs), namely PEGASUS, T5, MBART, and
BART. Moreover, we explore the performance of two general-purpose pre-trained
LLMs such as Text-Davinci-003 and GPT-3.5-Turbo without any further training.
By performing automatic evaluation, we show that T5 (with long prompt)
outperforms all other models, but still falls short of the human baseline.
Under human evaluation criteria, TextDavinci-003 usually shows better results
than other models under various prompt settings. Even in the case of human
evaluation criteria, QG models mostly fall short of the human baseline. Our
code and dataset are available at: https://github.com/my625/PromptQG
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01037">Eliciting Latent Knowledge from Quirky Language Models. (arXiv:2312.01037v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mallen_A/0/1/0/all/0/1">Alex Mallen</a>, <a href="http://arxiv.org/find/cs/1/au:+Belrose_N/0/1/0/all/0/1">Nora Belrose</a></p>
<p>Eliciting Latent Knowledge (ELK) aims to find patterns in a neural network's
activations which robustly track the true state of the world, even when the
network's overt output is false or misleading. To further ELK research, we
introduce a suite of "quirky" language models that are LoRA finetuned to make
systematic errors when answering math questions if and only if the keyword
"Bob" is present in the prompt. We demonstrate that simple probing methods can
elicit the model's latent knowledge of the correct answer in these contexts,
even for problems harder than those the probe was trained on. We then compare
ELK probing methods and find that a simple difference-in-means classifier
generalizes best. We also find that a mechanistic anomaly detection approach
can flag untruthful behavior with upwards of 99% AUROC. Our results show
promise for eliciting superhuman knowledge from capable models, and we aim to
facilitate future research that expands on our findings, employing more diverse
and challenging datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01045">PROFL: A Privacy-Preserving Federated Learning Method with Stringent Defense Against Poisoning Attacks. (arXiv:2312.01045v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1">Yisheng Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Li-Ping Wang</a></p>
<p>Federated Learning (FL) faces two major issues: privacy leakage and poisoning
attacks, which may seriously undermine the reliability and security of the
system. Overcoming them simultaneously poses a great challenge. This is because
privacy protection policies prohibit access to users' local gradients to avoid
privacy leakage, while Byzantine-robust methods necessitate access to these
gradients to defend against poisoning attacks. To address these problems, we
propose a novel privacy-preserving Byzantine-robust FL framework PROFL. PROFL
is based on the two-trapdoor additional homomorphic encryption algorithm and
blinding techniques to ensure the data privacy of the entire FL process. During
the defense process, PROFL first utilize secure Multi-Krum algorithm to remove
malicious gradients at the user level. Then, according to the Pauta criterion,
we innovatively propose a statistic-based privacy-preserving defense algorithm
to eliminate outlier interference at the feature level and resist impersonation
poisoning attacks with stronger concealment. Detailed theoretical analysis
proves the security and efficiency of the proposed method. We conducted
extensive experiments on two benchmark datasets, and PROFL improved accuracy by
39% to 75% across different attack settings compared to similar
privacy-preserving robust methods, demonstrating its significant advantage in
robustness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2010.08158">An Accurate and Fully-Automated Ensemble Model for Weekly Time Series Forecasting. (arXiv:2010.08158v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Godahewa_R/0/1/0/all/0/1">Rakshitha Godahewa</a>, <a href="http://arxiv.org/find/cs/1/au:+Bergmeir_C/0/1/0/all/0/1">Christoph Bergmeir</a>, <a href="http://arxiv.org/find/cs/1/au:+Webb_G/0/1/0/all/0/1">Geoffrey I. Webb</a>, <a href="http://arxiv.org/find/cs/1/au:+Montero_Manso_P/0/1/0/all/0/1">Pablo Montero-Manso</a></p>
<p>Many businesses and industries require accurate forecasts for weekly time
series nowadays. However, the forecasting literature does not currently provide
easy-to-use, automatic, reproducible and accurate approaches dedicated to this
task. We propose a forecasting method in this domain to fill this gap,
leveraging state-of-the-art forecasting techniques, such as forecast
combination, meta-learning, and global modelling. We consider different
meta-learning architectures, algorithms, and base model pools. Based on all
considered model variants, we propose to use a stacking approach with lasso
regression which optimally combines the forecasts of four base models: a global
Recurrent Neural Network model (RNN), Theta, Trigonometric Box-Cox ARMA Trend
Seasonal (TBATS) and Dynamic Harmonic Regression ARIMA (DHR-ARIMA), as it shows
the overall best performance across seven experimental weekly datasets on four
evaluation metrics. Our proposed method also consistently outperforms a set of
benchmarks and state-of-the-art weekly forecasting models by a considerable
margin with statistical significance. Our method can produce the most accurate
forecasts, in terms of mean sMAPE, for the M4 weekly dataset among all
benchmarks and all original competition participants.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2102.01936">A Bayesian Federated Learning Framework with Online Laplace Approximation. (arXiv:2102.01936v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Liangxi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1">Xi Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1">Feng Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1">Guo-Jun Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Heng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1">Ling Shao</a></p>
<p>Federated learning (FL) allows multiple clients to collaboratively learn a
globally shared model through cycles of model aggregation and local model
training, without the need to share data. Most existing FL methods train local
models separately on different clients, and then simply average their
parameters to obtain a centralized model on the server side. However, these
approaches generally suffer from large aggregation errors and severe local
forgetting, which are particularly bad in heterogeneous data settings. To
tackle these issues, in this paper, we propose a novel FL framework that uses
online Laplace approximation to approximate posteriors on both the client and
server side. On the server side, a multivariate Gaussian product mechanism is
employed to construct and maximize a global posterior, largely reducing the
aggregation errors induced by large discrepancies between local models. On the
client side, a prior loss that uses the global posterior probabilistic
parameters delivered from the server is designed to guide the local training.
Binding such learning constraints from other clients enables our method to
mitigate local forgetting. Finally, we achieve state-of-the-art results on
several benchmarks, clearly demonstrating the advantages of the proposed
method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.10890">Revisiting Game Representations: The~Hidden Costs of Efficiency in~Sequential Decision-making Algorithms. (arXiv:2112.10890v2 [cs.GT] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kovarik_V/0/1/0/all/0/1">Vojt&#x11b;ch Kova&#x159;&#xed;k</a>, <a href="http://arxiv.org/find/cs/1/au:+Milec_D/0/1/0/all/0/1">David Milec</a>, <a href="http://arxiv.org/find/cs/1/au:+Sustr_M/0/1/0/all/0/1">Michal &#x160;ustr</a>, <a href="http://arxiv.org/find/cs/1/au:+Seitz_D/0/1/0/all/0/1">Dominik Seitz</a>, <a href="http://arxiv.org/find/cs/1/au:+Lisy_V/0/1/0/all/0/1">Viliam Lis&#xfd;</a></p>
<p>Recent advancements in algorithms for sequential decision-making under
imperfect information have shown remarkable success in large games such as
limit- and no-limit poker. These algorithms traditionally formalize the games
using the extensive-form game formalism, which, as we show, while theoretically
sound, is memory-inefficient and computationally intensive in practice. To
mitigate these challenges, a popular workaround involves using a specialized
representation based on player specific information-state trees. However, as we
show, this alternative significantly narrows the set of games that can be
represented efficiently.
</p>
<p>In this study, we identify the set of large games on which modern algorithms
have been benchmarked as being naturally represented by Sequential Bayesian
Games. We elucidate the critical differences between extensive-form game and
sequential Bayesian game representations, both theoretically and empirically.
We further argue that the impressive experimental results often cited in the
literature may be skewed, as they frequently stem from testing these algorithms
only on this restricted class of games. By understanding these nuances, we aim
to guide future research in developing more universally applicable and
efficient algorithms for sequential decision-making under imperfect
information.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.08063">Information Extraction in Low-Resource Scenarios: Survey and Perspective. (arXiv:2202.08063v5 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1">Shumin Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yubo Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Ningyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yixin Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1">Bryan Hooi</a></p>
<p>Information Extraction (IE) seeks to derive structured information from
unstructured texts, often facing challenges in low-resource scenarios due to
data scarcity and unseen classes. This paper presents a review of neural
approaches to low-resource IE from \emph{traditional} and \emph{LLM-based}
perspectives, systematically categorizing them into a fine-grained taxonomy.
Then we conduct empirical study on LLM-based methods compared with previous
state-of-the-art models, and discover that (1) well-tuned LMs are still
predominant; (2) tuning open-resource LLMs and ICL with GPT family is promising
in general; (3) the optimal LLM-based technical solution for low-resource IE
can be task-dependent. In addition, we discuss low-resource IE with LLMs,
highlight promising applications, and outline potential research directions.
This survey aims to foster understanding of this field, inspire new ideas, and
encourage widespread applications in both academia and industry.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.09590">Data-Driven Causal Effect Estimation Based on Graphical Causal Modelling: A Survey. (arXiv:2208.09590v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1">Debo Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiuyong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Lin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jixue Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1">Thuc Duy Le</a></p>
<p>In many fields of scientific research and real-world applications, unbiased
estimation of causal effects from non-experimental data is crucial for
understanding the mechanism underlying the data and for decision-making on
effective responses or interventions. A great deal of research has been
conducted to address this challenging problem from different angles. For
estimating causal effect in observational data, assumptions such as Markov
condition, faithfulness and causal sufficiency are always made. Under the
assumptions, full knowledge such as, a set of covariates or an underlying
causal graph, is typically required. A practical challenge is that in many
applications, no such full knowledge or only some partial knowledge is
available. In recent years, research has emerged to use search strategies based
on graphical causal modelling to discover useful knowledge from data for causal
effect estimation, with some mild assumptions, and has shown promise in
tackling the practical challenge. In this survey, we review these data-driven
methods on causal effect estimation for a single treatment with a single
outcome of interest and focus on the challenges faced by data-driven causal
effect estimation. We concisely summarise the basic concepts and theories that
are essential for data-driven causal effect estimation using graphical causal
modelling but are scattered around the literature. We identify and discuss the
challenges faced by data-driven causal effect estimation and characterise the
existing methods by their assumptions and the approaches to tackling the
challenges. We analyse the strengths and limitations of the different types of
methods and present an empirical evaluation to support the discussions. We hope
this review will motivate more researchers to design better data-driven methods
based on graphical causal modelling for the challenging problem of causal
effect estimation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.01432">From Monte Carlo to neural networks approximations of boundary value problems. (arXiv:2209.01432v2 [math.PR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Beznea_L/0/1/0/all/0/1">Lucian Beznea</a>, <a href="http://arxiv.org/find/math/1/au:+Cimpean_I/0/1/0/all/0/1">Iulian Cimpean</a>, <a href="http://arxiv.org/find/math/1/au:+Lupascu_Stamate_O/0/1/0/all/0/1">Oana Lupascu-Stamate</a>, <a href="http://arxiv.org/find/math/1/au:+Popescu_I/0/1/0/all/0/1">Ionel Popescu</a>, <a href="http://arxiv.org/find/math/1/au:+Zarnescu_A/0/1/0/all/0/1">Arghir Zarnescu</a></p>
<p>In this paper we study probabilistic and neural network approximations for
solutions to Poisson equation subject to H\" older data in general bounded
domains of $\mathbb{R}^d$. We aim at two fundamental goals.
</p>
<p>The first, and the most important, we show that the solution to Poisson
equation can be numerically approximated in the sup-norm by Monte Carlo
methods, { and that this can be done highly efficiently if we use a modified
version} of the walk on spheres algorithm { as an acceleration method. This
provides estimates which are efficient with respect to the prescribed
approximation error and with polynomial complexity in the dimension and the
reciprocal of the error.} {A crucial feature is that} the overall number of
samples does not not depend on the point at which the approximation is
performed.
</p>
<p>As a second goal, we show that the obtained Monte Carlo solver renders { in a
constructive way} ReLU deep neural network (DNN) solutions to Poisson problem,
whose sizes depend at most polynomialy in the dimension $d$ and in the desired
error. In fact we show that the random DNN provides with high probability a
small approximation error and low polynomial complexity in the dimension.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.15146">Ensemble Machine Learning Model Trained on a New Synthesized Dataset Generalizes Well for Stress Prediction Using Wearable Devices. (arXiv:2209.15146v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vos_G/0/1/0/all/0/1">Gideon Vos</a>, <a href="http://arxiv.org/find/cs/1/au:+Trinh_K/0/1/0/all/0/1">Kelly Trinh</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarnyai_Z/0/1/0/all/0/1">Zoltan Sarnyai</a>, <a href="http://arxiv.org/find/cs/1/au:+Azghadi_M/0/1/0/all/0/1">Mostafa Rahimi Azghadi</a></p>
<p>Introduction. We investigate the generalization ability of models built on
datasets containing a small number of subjects, recorded in single study
protocols. Next, we propose and evaluate methods combining these datasets into
a single, large dataset. Finally, we propose and evaluate the use of ensemble
techniques by combining gradient boosting with an artificial neural network to
measure predictive power on new, unseen data.
</p>
<p>Methods. Sensor biomarker data from six public datasets were utilized in this
study. To test model generalization, we developed a gradient boosting model
trained on one dataset (SWELL), and tested its predictive power on two datasets
previously used in other studies (WESAD, NEURO). Next, we merged four small
datasets, i.e. (SWELL, NEURO, WESAD, UBFC-Phys), to provide a combined total of
99 subjects,. In addition, we utilized random sampling combined with another
dataset (EXAM) to build a larger training dataset consisting of 200 synthesized
subjects,. Finally, we developed an ensemble model that combines our gradient
boosting model with an artificial neural network, and tested it on two
additional, unseen publicly available stress datasets (WESAD and Toadstool).
</p>
<p>Results. Our method delivers a robust stress measurement system capable of
achieving 85% predictive accuracy on new, unseen validation data, achieving a
25% performance improvement over single models trained on small datasets.
</p>
<p>Conclusion. Models trained on small, single study protocol datasets do not
generalize well for use on new, unseen data and lack statistical power.
Ma-chine learning models trained on a dataset containing a larger number of
varied study subjects capture physiological variance better, resulting in more
robust stress detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.03728">Representing Data as Atoms: Unifying Intra- and Inter-Sample Relationship to Discretize Data Representation. (arXiv:2210.03728v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tuan_Y/0/1/0/all/0/1">Yi-Lin Tuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chiu_Z/0/1/0/all/0/1">Zih-Yun Chiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">William Yang Wang</a></p>
<p>The quality of data representation is paramount for the performance of a
model. Recent research has focused on enhancing representation learning by
incorporating more information about the intra-sample structures of individual
data points, such as local and global attention. Additionally, researchers have
explored methods to model the inter-sample relationships, including manifold,
contrastive, and discrete representation learning. In this study, we introduce
a new training loss, which considers both intra-sample structure and
inter-sample relationships, leveraging the concept of {\it atoms} to represent
data points. This new approach, {\it Atom Modeling}, offers a fresh perspective
to discretize data representations within a continuous space. Through
experiments, we demonstrate that Atom Modeling enhances the performance of
existing models in tasks involving classification and generation, across
diverse domains including vision and language. These findings underscore the
potential of Atom Modeling to enhance data representation and improve model
learning, suggesting a promising direction for future research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.07675">Learning image representations for anomaly detection: application to discovery of histological alterations in drug development. (arXiv:2210.07675v6 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zingman_I/0/1/0/all/0/1">Igor Zingman</a>, <a href="http://arxiv.org/find/cs/1/au:+Stierstorfer_B/0/1/0/all/0/1">Birgit Stierstorfer</a>, <a href="http://arxiv.org/find/cs/1/au:+Lempp_C/0/1/0/all/0/1">Charlotte Lempp</a>, <a href="http://arxiv.org/find/cs/1/au:+Heinemann_F/0/1/0/all/0/1">Fabian Heinemann</a></p>
<p>We present a system for anomaly detection in histopathological images. In
histology, normal samples are usually abundant, whereas anomalous
(pathological) cases are scarce or not available. Under such settings,
one-class classifiers trained on healthy data can detect out-of-distribution
anomalous samples. Such approaches combined with pre-trained Convolutional
Neural Network (CNN) representations of images were previously employed for
anomaly detection (AD). However, pre-trained off-the-shelf CNN representations
may not be sensitive to abnormal conditions in tissues, while natural
variations of healthy tissue may result in distant representations. To adapt
representations to relevant details in healthy tissue we propose training a CNN
on an auxiliary task that discriminates healthy tissue of different species,
organs, and staining reagents. Almost no additional labeling workload is
required, since healthy samples come automatically with aforementioned labels.
During training we enforce compact image representations with a center-loss
term, which further improves representations for AD. The proposed system
outperforms established AD methods on a published dataset of liver anomalies.
Moreover, it provided comparable results to conventional methods specifically
tailored for quantification of liver anomalies. We show that our approach can
be used for toxicity assessment of candidate drugs at early development stages
and thereby may reduce expensive late-stage drug attrition.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.13452">MetaFormer Baselines for Vision. (arXiv:2210.13452v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1">Weihao Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1">Chenyang Si</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1">Pan Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1">Mi Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yichen Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1">Jiashi Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1">Shuicheng Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinchao Wang</a></p>
<p>MetaFormer, the abstracted architecture of Transformer, has been found to
play a significant role in achieving competitive performance. In this paper, we
further explore the capacity of MetaFormer, again, without focusing on token
mixer design: we introduce several baseline models under MetaFormer using the
most basic or common mixers, and summarize our observations as follows: (1)
MetaFormer ensures solid lower bound of performance. By merely adopting
identity mapping as the token mixer, the MetaFormer model, termed
IdentityFormer, achieves &gt;80% accuracy on ImageNet-1K. (2) MetaFormer works
well with arbitrary token mixers. When specifying the token mixer as even a
random matrix to mix tokens, the resulting model RandFormer yields an accuracy
of &gt;81%, outperforming IdentityFormer. Rest assured of MetaFormer's results
when new token mixers are adopted. (3) MetaFormer effortlessly offers
state-of-the-art results. With just conventional token mixers dated back five
years ago, the models instantiated from MetaFormer already beat state of the
art. (a) ConvFormer outperforms ConvNeXt. Taking the common depthwise separable
convolutions as the token mixer, the model termed ConvFormer, which can be
regarded as pure CNNs, outperforms the strong CNN model ConvNeXt. (b) CAFormer
sets new record on ImageNet-1K. By simply applying depthwise separable
convolutions as token mixer in the bottom stages and vanilla self-attention in
the top stages, the resulting model CAFormer sets a new record on ImageNet-1K:
it achieves an accuracy of 85.5% at 224x224 resolution, under normal supervised
training without external data or distillation. In our expedition to probe
MetaFormer, we also find that a new activation, StarReLU, reduces 71% FLOPs of
activation compared with GELU yet achieves better performance. We expect
StarReLU to find great potential in MetaFormer-like models alongside other
neural networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.02658">Dealing with Drift of Adaptation Spaces in Learning-based Self-Adaptive Systems using Lifelong Self-Adaptation. (arXiv:2211.02658v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gheibi_O/0/1/0/all/0/1">Omid Gheibi</a>, <a href="http://arxiv.org/find/cs/1/au:+Weyns_D/0/1/0/all/0/1">Danny Weyns</a></p>
<p>Recently, machine learning (ML) has become a popular approach to support
self-adaptation. ML has been used to deal with several problems in
self-adaptation, such as maintaining an up-to-date runtime model under
uncertainty and scalable decision-making. Yet, exploiting ML comes with
inherent challenges. In this paper, we focus on a particularly important
challenge for learning-based self-adaptive systems: drift in adaptation spaces.
With adaptation space we refer to the set of adaptation options a self-adaptive
system can select from at a given time to adapt based on the estimated quality
properties of the adaptation options. Drift of adaptation spaces originates
from uncertainties, affecting the quality properties of the adaptation options.
Such drift may imply that eventually no adaptation option can satisfy the
initial set of the adaptation goals, deteriorating the quality of the system,
or adaptation options may emerge that allow enhancing the adaptation goals. In
ML, such shift corresponds to novel class appearance, a type of concept drift
in target data that common ML techniques have problems dealing with. To tackle
this problem, we present a novel approach to self-adaptation that enhances
learning-based self-adaptive systems with a lifelong ML layer. We refer to this
approach as lifelong self-adaptation. The lifelong ML layer tracks the system
and its environment, associates this knowledge with the current tasks,
identifies new tasks based on differences, and updates the learning models of
the self-adaptive system accordingly. A human stakeholder may be involved to
support the learning process and adjust the learning and goal models. We
present a general architecture for lifelong self-adaptation and apply it to the
case of drift of adaptation spaces that affects the decision-making in
self-adaptation. We validate the approach for a series of scenarios using the
DeltaIoT exemplar.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.03181">Funnel-based Reward Shaping for Signal Temporal Logic Tasks in Reinforcement Learning. (arXiv:2212.03181v3 [eess.SY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Saxena_N/0/1/0/all/0/1">Naman Saxena</a>, <a href="http://arxiv.org/find/eess/1/au:+Sandeep_G/0/1/0/all/0/1">Gorantla Sandeep</a>, <a href="http://arxiv.org/find/eess/1/au:+Jagtap_P/0/1/0/all/0/1">Pushpak Jagtap</a></p>
<p>Signal Temporal Logic (STL) is a powerful framework for describing the
complex temporal and logical behaviour of the dynamical system. Numerous
studies have attempted to employ reinforcement learning to learn a controller
that enforces STL specifications; however, they have been unable to effectively
tackle the challenges of ensuring robust satisfaction in continuous state space
and maintaining tractability. In this paper, leveraging the concept of funnel
functions, we propose a tractable reinforcement learning algorithm to learn a
time-dependent policy for robust satisfaction of STL specification in
continuous state space. We demonstrate the utility of our approach on several
STL tasks using different environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.04611">A Text-guided Protein Design Framework. (arXiv:2302.04611v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shengchao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yanjing Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhuoxinran Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gitter_A/0/1/0/all/0/1">Anthony Gitter</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yutao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Jiarui Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zhao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_W/0/1/0/all/0/1">Weili Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramanathan_A/0/1/0/all/0/1">Arvind Ramanathan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Chaowei Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jian Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1">Hongyu Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1">Anima Anandkumar</a></p>
<p>Current AI-assisted protein design mainly utilizes protein sequential and
structural information. Meanwhile, there exists tremendous knowledge curated by
humans in the text format describing proteins' high-level functionalities. Yet,
whether the incorporation of such text data can help protein design tasks has
not been explored. To bridge this gap, we propose ProteinDT, a multi-modal
framework that leverages textual descriptions for protein design. ProteinDT
consists of three subsequent steps: ProteinCLAP which aligns the representation
of two modalities, a facilitator that generates the protein representation from
the text modality, and a decoder that creates the protein sequences from the
representation. To train ProteinDT, we construct a large dataset,
SwissProtCLAP, with 441K text and protein pairs. We quantitatively verify the
effectiveness of ProteinDT on three challenging tasks: (1) over 90\% accuracy
for text-guided protein generation; (2) best hit ratio on 10 zero-shot
text-guided protein editing tasks; (3) superior performance on four out of six
protein property prediction benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.06884">Conservative State Value Estimation for Offline Reinforcement Learning. (arXiv:2302.06884v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Liting Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1">Jie Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1">Zhengdao Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1">Qingwei Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajmohan_S/0/1/0/all/0/1">Saravan Rajmohan</a>, <a href="http://arxiv.org/find/cs/1/au:+Moscibroda_T/0/1/0/all/0/1">Thomas Moscibroda</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dongmei Zhang</a></p>
<p>Offline reinforcement learning faces a significant challenge of value
over-estimation due to the distributional drift between the dataset and the
current learned policy, leading to learning failure in practice. The common
approach is to incorporate a penalty term to reward or value estimation in the
Bellman iterations. Meanwhile, to avoid extrapolation on out-of-distribution
(OOD) states and actions, existing methods focus on conservative Q-function
estimation. In this paper, we propose Conservative State Value Estimation
(CSVE), a new approach that learns conservative V-function via directly
imposing penalty on OOD states. Compared to prior work, CSVE allows more
effective state value estimation with conservative guarantees and further
better policy optimization. Further, we apply CSVE and develop a practical
actor-critic algorithm in which the critic does the conservative value
estimation by additionally sampling and penalizing the states \emph{around} the
dataset, and the actor applies advantage weighted updates extended with state
exploration to improve the policy. We evaluate in classic continual control
tasks of D4RL, showing that our method performs better than the conservative
Q-function learning methods and is strongly competitive among recent SOTA
methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.07026">Visual-Policy Learning through Multi-Camera View to Single-Camera View Knowledge Distillation for Robot Manipulation Tasks. (arXiv:2303.07026v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Acar_C/0/1/0/all/0/1">Cihan Acar</a>, <a href="http://arxiv.org/find/cs/1/au:+Binici_K/0/1/0/all/0/1">Kuluhan Binici</a>, <a href="http://arxiv.org/find/cs/1/au:+Tekirdag_A/0/1/0/all/0/1">Alp Tekirda&#x11f;</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yan Wu</a></p>
<p>The use of multi-camera views simultaneously has been shown to improve the
generalization capabilities and performance of visual policies. However, the
hardware cost and design constraints in real-world scenarios can potentially
make it challenging to use multiple cameras. In this study, we present a novel
approach to enhance the generalization performance of vision-based
Reinforcement Learning (RL) algorithms for robotic manipulation tasks. Our
proposed method involves utilizing a technique known as knowledge distillation,
in which a pre-trained ``teacher'' policy trained with multiple camera
viewpoints guides a ``student'' policy in learning from a single camera
viewpoint. To enhance the student policy's robustness against camera location
perturbations, it is trained using data augmentation and extreme viewpoint
changes. As a result, the student policy learns robust visual features that
allow it to locate the object of interest accurately and consistently,
regardless of the camera viewpoint. The efficacy and efficiency of the proposed
method were evaluated both in simulation and real-world environments. The
results demonstrate that the single-view visual student policy can successfully
learn to grasp and lift a challenging object, which was not possible with a
single-view policy alone. Furthermore, the student policy demonstrates
zero-shot transfer capability, where it can successfully grasp and lift objects
in real-world scenarios for unseen visual configurations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.12484">Label-Efficient Deep Learning in Medical Image Analysis: Challenges and Future Directions. (arXiv:2303.12484v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1">Cheng Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Zhengrui Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yi Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1">Luyang Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hao Chen</a></p>
<p>Deep learning has seen rapid growth in recent years and achieved
state-of-the-art performance in a wide range of applications. However, training
models typically requires expensive and time-consuming collection of large
quantities of labeled data. This is particularly true within the scope of
medical imaging analysis (MIA), where data are limited and labels are expensive
to be acquired. Thus, label-efficient deep learning methods are developed to
make comprehensive use of the labeled data as well as the abundance of
unlabeled and weak-labeled data. In this survey, we extensively investigated
over 300 recent papers to provide a comprehensive overview of recent progress
on label-efficient learning strategies in MIA. We first present the background
of label-efficient learning and categorize the approaches into different
schemes. Next, we examine the current state-of-the-art methods in detail
through each scheme. Specifically, we provide an in-depth investigation,
covering not only canonical semi-supervised, self-supervised, and
multi-instance learning schemes, but also recently emerged active and
annotation-efficient learning strategies. Moreover, as a comprehensive
contribution to the field, this survey not only elucidates the commonalities
and unique features of the surveyed methods but also presents a detailed
analysis of the current challenges in the field and suggests potential avenues
for future research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.13397">DiffMesh: A Motion-aware Diffusion-like Framework for Human Mesh Recovery from Videos. (arXiv:2303.13397v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1">Ce Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xianpeng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Mengyuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1">Tianfu Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1">Guo-Jun Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chen Chen</a></p>
<p>Human mesh recovery (HMR) provides rich human body information for various
real-world applications. While image-based HMR methods have achieved impressive
results, they often struggle to recover humans in dynamic scenarios, leading to
temporal inconsistencies and non-smooth 3D motion predictions due to the
absence of human motion. In contrast, video-based approaches leverage temporal
information to mitigate this issue. In this paper, we present DiffMesh, an
innovative motion-aware Diffusion-like framework for video-based HMR. DiffMesh
establishes a bridge between diffusion models and human motion, efficiently
generating accurate and smooth output mesh sequences by incorporating human
motion within the forward process and reverse process in the diffusion model.
Extensive experiments are conducted on the widely used datasets (Human3.6M
\cite{h36m_pami} and 3DPW \cite{pw3d2018}), which demonstrate the effectiveness
and efficiency of our DiffMesh. Visual comparisons in real-world scenarios
further highlight DiffMesh's suitability for practical applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.16563">Skill Reinforcement Learning and Planning for Open-World Long-Horizon Tasks. (arXiv:2303.16563v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1">Haoqi Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hongcheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_F/0/1/0/all/0/1">Feiyang Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1">Penglin Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1">Hao Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Zongqing Lu</a></p>
<p>We study building multi-task agents in open-world environments. Without human
demonstrations, learning to accomplish long-horizon tasks in a large open-world
environment with reinforcement learning (RL) is extremely inefficient. To
tackle this challenge, we convert the multi-task learning problem into learning
basic skills and planning over the skills. Using the popular open-world game
Minecraft as the testbed, we propose three types of fine-grained basic skills,
and use RL with intrinsic rewards to acquire skills. A novel Finding-skill that
performs exploration to find diverse items provides better initialization for
other skills, improving the sample efficiency for skill learning. In skill
planning, we leverage the prior knowledge in Large Language Models to find the
relationships between skills and build a skill graph. When the agent is solving
a task, our skill search algorithm walks on the skill graph and generates the
proper skill plans for the agent. In experiments, our method accomplishes 40
diverse Minecraft tasks, where many tasks require sequentially executing for
more than 10 skills. Our method outperforms baselines by a large margin and is
the most sample-efficient demonstration-free RL method to solve Minecraft Tech
Tree tasks. The project's website and code can be found at
https://sites.google.com/view/plan4mc.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.17580">HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face. (arXiv:2303.17580v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yongliang Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1">Kaitao Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1">Xu Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dongsheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1">Weiming Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1">Yueting Zhuang</a></p>
<p>Solving complicated AI tasks with different domains and modalities is a key
step toward artificial general intelligence. While there are numerous AI models
available for various domains and modalities, they cannot handle complicated AI
tasks autonomously. Considering large language models (LLMs) have exhibited
exceptional abilities in language understanding, generation, interaction, and
reasoning, we advocate that LLMs could act as a controller to manage existing
AI models to solve complicated AI tasks, with language serving as a generic
interface to empower this. Based on this philosophy, we present HuggingGPT, an
LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI
models in machine learning communities (e.g., Hugging Face) to solve AI tasks.
Specifically, we use ChatGPT to conduct task planning when receiving a user
request, select models according to their function descriptions available in
Hugging Face, execute each subtask with the selected AI model, and summarize
the response according to the execution results. By leveraging the strong
language capability of ChatGPT and abundant AI models in Hugging Face,
HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different
modalities and domains and achieve impressive results in language, vision,
speech, and other challenging tasks, which paves a new way towards the
realization of artificial general intelligence.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.01196">Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data. (arXiv:2304.01196v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Canwen Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1">Daya Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1">Nan Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1">Julian McAuley</a></p>
<p>Chat models, such as ChatGPT, have shown impressive capabilities and have
been rapidly adopted across numerous domains. However, these models are only
accessible through a restricted API, creating barriers for new research and
progress in the field. We propose a pipeline that can automatically generate a
high-quality multi-turn chat corpus by leveraging ChatGPT to engage in a
conversation with itself. Subsequently, we employ parameter-efficient tuning to
enhance LLaMA, an open-source large language model. The resulting model, named
Baize, demonstrates good performance in multi-turn dialogues with guardrails
that minimize potential risks. Furthermore, we propose a new technique called
Self-Distill with Feedback, to further improve the performance of the Baize
models with feedback from ChatGPT. The Baize models and data are released for
research purposes only at https://github.com/project-baize/baize-chatbot. An
online demo is also available at
https://huggingface.co/spaces/project-baize/chat-with-baize.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.04150">RoboPianist: Dexterous Piano Playing with Deep Reinforcement Learning. (arXiv:2304.04150v3 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zakka_K/0/1/0/all/0/1">Kevin Zakka</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1">Philipp Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Smith_L/0/1/0/all/0/1">Laura Smith</a>, <a href="http://arxiv.org/find/cs/1/au:+Gileadi_N/0/1/0/all/0/1">Nimrod Gileadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Howell_T/0/1/0/all/0/1">Taylor Howell</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1">Xue Bin Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Sumeet Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Tassa_Y/0/1/0/all/0/1">Yuval Tassa</a>, <a href="http://arxiv.org/find/cs/1/au:+Florence_P/0/1/0/all/0/1">Pete Florence</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1">Andy Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1">Pieter Abbeel</a></p>
<p>Replicating human-like dexterity in robot hands represents one of the largest
open problems in robotics. Reinforcement learning is a promising approach that
has achieved impressive progress in the last few years; however, the class of
problems it has typically addressed corresponds to a rather narrow definition
of dexterity as compared to human capabilities. To address this gap, we
investigate piano-playing, a skill that challenges even the human limits of
dexterity, as a means to test high-dimensional control, and which requires high
spatial and temporal precision, and complex finger coordination and planning.
We introduce RoboPianist, a system that enables simulated anthropomorphic hands
to learn an extensive repertoire of 150 piano pieces where traditional
model-based optimization struggles. We additionally introduce an open-sourced
environment, benchmark of tasks, interpretable evaluation metrics, and open
challenges for future study. Our website featuring videos, code, and datasets
is available at https://kzakka.com/robopianist/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.10159">Deep-Q Learning with Hybrid Quantum Neural Network on Solving Maze Problems. (arXiv:2304.10159v3 [quant-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Chen_H/0/1/0/all/0/1">Hao-Yuan Chen</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Chang_Y/0/1/0/all/0/1">Yen-Jui Chang</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Liao_S/0/1/0/all/0/1">Shih-Wei Liao</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Chang_C/0/1/0/all/0/1">Ching-Ray Chang</a></p>
<p>Quantum computing holds great potential for advancing the limitations of
machine learning algorithms to handle higher dimensions of data and reduce
overall training parameters in deep learning (DL) models. This study uses a
trainable variational quantum circuit (VQC) on a gate-based quantum computing
model to investigate the potential for quantum benefit in a model-free
reinforcement learning problem. Through a comprehensive investigation and
evaluation of the current model and capabilities of quantum computers, we
designed and trained a novel hybrid quantum neural network based on the latest
Qiskit and PyTorch framework. We compared its performance with a full-classical
CNN with and without an incorporated VQC. Our research provides insights into
the potential of deep quantum learning to solve a maze problem and,
potentially, other reinforcement learning problems. We conclude that
reinforcement learning problems can be practical with reasonable training
epochs. Moreover, a comparative study of full-classical and hybrid quantum
neural networks is discussed to understand these two approaches' performance,
advantages, and disadvantages to deep-Q learning problems, especially on
larger-scale maze problems larger than 4x4.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.10513">Why Does ChatGPT Fall Short in Providing Truthful Answers?. (arXiv:2304.10513v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1">Shen Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jie Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1">Kevin Chen-Chuan Chang</a></p>
<p>Recent advancements in large language models, such as ChatGPT, have
demonstrated significant potential to impact various aspects of human life.
However, ChatGPT still faces challenges in providing reliable and accurate
answers to user questions. To better understand the model's particular
weaknesses in providing truthful answers, we embark an in-depth exploration of
open-domain question answering. Specifically, we undertake a detailed
examination of ChatGPT's failures, categorized into: comprehension, factuality,
specificity, and inference. We further pinpoint factuality as the most
contributing failure and identify two critical abilities associated with
factuality: knowledge memorization and knowledge recall. Through experiments
focusing on factuality, we propose several potential enhancement strategies.
Our findings suggest that augmenting the model with granular external knowledge
and cues for knowledge recall can enhance the model's factuality in answering
questions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.03047">Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision. (arXiv:2305.03047v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1">Zhiqing Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yikang Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1">Qinhong Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongxin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhenfang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cox_D/0/1/0/all/0/1">David Cox</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yiming Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1">Chuang Gan</a></p>
<p>Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised
fine-tuning (SFT) with human annotations and reinforcement learning from human
feedback (RLHF) to align the output of large language models (LLMs) with human
intentions, ensuring they are helpful, ethical, and reliable. However, this
dependence can significantly constrain the true potential of AI-assistant
agents due to the high cost of obtaining human supervision and the related
issues on quality, reliability, diversity, self-consistency, and undesirable
biases. To address these challenges, we propose a novel approach called
SELF-ALIGN, which combines principle-driven reasoning and the generative power
of LLMs for the self-alignment of AI agents with minimal human supervision. Our
approach encompasses four stages: first, we use an LLM to generate synthetic
prompts, and a topic-guided method to augment the prompt diversity; second, we
use a small set of human-written principles for AI models to follow, and guide
the LLM through in-context learning from demonstrations (of principles
application) to produce helpful, ethical, and reliable responses to user's
queries; third, we fine-tune the original LLM with the high-quality
self-aligned responses so that the resulting model can generate desirable
responses for each query directly without the principle set and the
demonstrations anymore; and finally, we offer a refinement step to address the
issues of overly-brief or indirect responses. Applying SELF-ALIGN to the
LLaMA-65b base language model, we develop an AI assistant named Dromedary. With
fewer than 300 lines of human annotations (including &lt; 200 seed prompts, 16
generic principles, and 5 exemplars for in-context learning). Dromedary
significantly surpasses the performance of several state-of-the-art AI systems,
including Text-Davinci-003 and Alpaca, on benchmark datasets with various
settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.04782">HistAlign: Improving Context Dependency in Language Generation by Aligning with History. (arXiv:2305.04782v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wan_D/0/1/0/all/0/1">David Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shiyue Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1">Mohit Bansal</a></p>
<p>Language models (LMs) can generate hallucinations and incoherent outputs,
which highlights their weak context dependency. Cache-LMs, which augment LMs
with a memory of recent history, can increase context dependency and have shown
remarkable performance in diverse language generation tasks. However, we find
that even with training, the performance gain stemming from the cache component
of current cache-LMs is suboptimal due to the misalignment between the current
hidden states and those stored in the memory. In this work, we present
HistAlign, a new training approach to ensure good cache alignment such that the
model receives useful signals from the history. We first prove our concept on a
simple and synthetic task where the memory is essential for correct
predictions, and we show that the cache component of HistAlign is better
aligned and improves overall performance. Next, we evaluate HistAlign on
diverse downstream language generation tasks, including prompt continuation,
abstractive summarization, and data-to-text. We demonstrate that HistAlign
improves text coherence and faithfulness in open-ended and conditional
generation settings respectively. HistAlign is also generalizable across
different model families, showcasing its strength in improving context
dependency of LMs in diverse scenarios. Our code is publicly available at
https://github.com/meetdavidwan/histalign
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.10601">Tree of Thoughts: Deliberate Problem Solving with Large Language Models. (arXiv:2305.10601v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1">Shunyu Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1">Dian Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jeffrey Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Shafran_I/0/1/0/all/0/1">Izhak Shafran</a>, <a href="http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1">Thomas L. Griffiths</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yuan Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1">Karthik Narasimhan</a></p>
<p>Language models are increasingly being deployed for general problem solving
across a wide range of tasks, but are still confined to token-level,
left-to-right decision-making processes during inference. This means they can
fall short in tasks that require exploration, strategic lookahead, or where
initial decisions play a pivotal role. To surmount these challenges, we
introduce a new framework for language model inference, Tree of Thoughts (ToT),
which generalizes over the popular Chain of Thought approach to prompting
language models, and enables exploration over coherent units of text (thoughts)
that serve as intermediate steps toward problem solving. ToT allows LMs to
perform deliberate decision making by considering multiple different reasoning
paths and self-evaluating choices to decide the next course of action, as well
as looking ahead or backtracking when necessary to make global choices. Our
experiments show that ToT significantly enhances language models'
problem-solving abilities on three novel tasks requiring non-trivial planning
or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in
Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of
tasks, our method achieved a success rate of 74%. Code repo with all prompts:
https://github.com/princeton-nlp/tree-of-thought-llm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13256">TaskWeb: Selecting Better Source Tasks for Multi-task NLP. (arXiv:2305.13256v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Joongwon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Asai_A/0/1/0/all/0/1">Akari Asai</a>, <a href="http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1">Gabriel Ilharco</a>, <a href="http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1">Hannaneh Hajishirzi</a></p>
<p>Recent work in NLP has shown promising results in training models on large
amounts of tasks to achieve better generalization. However, it is not
well-understood how tasks are related, and how helpful training tasks can be
chosen for a new task. In this work, we investigate whether knowing task
relationships via pairwise task transfer improves choosing one or more source
tasks that help to learn a new target task. We provide TaskWeb, a large-scale
benchmark of pairwise task transfers for 22 NLP tasks using three different
model types, sizes, and adaptation methods, spanning about 25,000 experiments.
Then, we design a new method TaskShop based on our analysis of TaskWeb.
TaskShop uses TaskWeb to estimate the benefit of using a source task for
learning a new target task, and to choose a subset of helpful training tasks
for multi-task training. Our method improves overall rankings and top-k
precision of source tasks by 10% and 38%, respectively. We also use TaskShop to
build much smaller multi-task training sets that improve zero-shot performances
across 11 different target tasks by at least 4.3%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14735">Centering the Margins: Outlier-Based Identification of Harmed Populations in Toxicity Detection. (arXiv:2305.14735v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Raman_V/0/1/0/all/0/1">Vyoma Raman</a>, <a href="http://arxiv.org/find/cs/1/au:+Fleisig_E/0/1/0/all/0/1">Eve Fleisig</a>, <a href="http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1">Dan Klein</a></p>
<p>The impact of AI models on marginalized communities has traditionally been
measured by identifying performance differences between specified demographic
subgroups. Though this approach aims to center vulnerable groups, it risks
obscuring patterns of harm faced by intersectional subgroups or shared across
multiple groups. To address this, we draw on theories of marginalization from
disability studies and related disciplines, which state that people farther
from the norm face greater adversity, to consider the "margins" in the domain
of toxicity detection. We operationalize the "margins" of a dataset by
employing outlier detection to identify text about people with demographic
attributes distant from the "norm". We find that model performance is
consistently worse for demographic outliers, with mean squared error (MSE)
between outliers and non-outliers up to 70.4% worse across toxicity types. It
is also worse for text outliers, with a MSE up to 68.4% higher for outliers
than non-outliers. We also find text and demographic outliers to be
particularly susceptible to errors in the classification of severe toxicity and
identity attacks. Compared to analysis of disparities using traditional
demographic breakdowns, we find that our outlier analysis frequently surfaces
greater harms faced by a larger, more intersectional group, which suggests that
outlier analysis is particularly beneficial for identifying harms against those
groups.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15712">Knowledge Diffusion for Distillation. (arXiv:2305.15712v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1">Tao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1">Mingkai Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1">Shan You</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Fei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1">Chen Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Chang Xu</a></p>
<p>The representation gap between teacher and student is an emerging topic in
knowledge distillation (KD). To reduce the gap and improve the performance,
current methods often resort to complicated training schemes, loss functions,
and feature alignments, which are task-specific and feature-specific. In this
paper, we state that the essence of these methods is to discard the noisy
information and distill the valuable information in the feature, and propose a
novel KD method dubbed DiffKD, to explicitly denoise and match features using
diffusion models. Our approach is based on the observation that student
features typically contain more noises than teacher features due to the smaller
capacity of student model. To address this, we propose to denoise student
features using a diffusion model trained by teacher features. This allows us to
perform better distillation between the refined clean feature and teacher
feature. Additionally, we introduce a light-weight diffusion model with a
linear autoencoder to reduce the computation cost and an adaptive noise
matching module to improve the denoising performance. Extensive experiments
demonstrate that DiffKD is effective across various types of features and
achieves state-of-the-art performance consistently on image classification,
object detection, and semantic segmentation tasks. Code is available at
https://github.com/hunto/DiffKD.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19125">Graph Generation with $K^2$-trees. (arXiv:2305.19125v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1">Yunhui Jang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1">Dongwoo Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1">Sungsoo Ahn</a></p>
<p>Generating graphs from a target distribution is a significant challenge
across many domains, including drug discovery and social network analysis. In
this work, we introduce a novel graph generation method leveraging $K^2$-tree
representation, originally designed for lossless graph compression. The
$K^2$-tree representation {encompasses inherent hierarchy while enabling
compact graph generation}. In addition, we make contributions by (1) presenting
a sequential $K^2$-treerepresentation that incorporates pruning, flattening,
and tokenization processes and (2) introducing a Transformer-based architecture
designed to generate the sequence by incorporating a specialized tree
positional encoding scheme. Finally, we extensively evaluate our algorithm on
four general and two molecular graph datasets to confirm its superiority for
graph generation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.01242">Responsible Task Automation: Empowering Large Language Models as Responsible Task Automators. (arXiv:2306.01242v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhizheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaoyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1">Wenxuan Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yan Lu</a></p>
<p>The recent success of Large Language Models (LLMs) signifies an impressive
stride towards artificial general intelligence. They have shown a promising
prospect in automatically completing tasks upon user instructions, functioning
as brain-like coordinators. The associated risks will be revealed as we
delegate an increasing number of tasks to machines for automated completion. A
big question emerges: how can we make machines behave responsibly when helping
humans automate tasks as personal copilots? In this paper, we explore this
question in depth from the perspectives of feasibility, completeness and
security. In specific, we present Responsible Task Automation (ResponsibleTA)
as a fundamental framework to facilitate responsible collaboration between
LLM-based coordinators and executors for task automation with three empowered
capabilities: 1) predicting the feasibility of the commands for executors; 2)
verifying the completeness of executors; 3) enhancing the security (e.g., the
protection of users' privacy). We further propose and compare two paradigms for
implementing the first two capabilities. One is to leverage the generic
knowledge of LLMs themselves via prompt engineering while the other is to adopt
domain-specific learnable models. Moreover, we introduce a local memory
mechanism for achieving the third capability. We evaluate our proposed
ResponsibleTA on UI task automation and hope it could bring more attentions to
ensuring LLMs more responsible in diverse scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.04633">Contrastive Lift: 3D Object Instance Segmentation by Slow-Fast Contrastive Fusion. (arXiv:2306.04633v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bhalgat_Y/0/1/0/all/0/1">Yash Bhalgat</a>, <a href="http://arxiv.org/find/cs/1/au:+Laina_I/0/1/0/all/0/1">Iro Laina</a>, <a href="http://arxiv.org/find/cs/1/au:+Henriques_J/0/1/0/all/0/1">Jo&#xe3;o F. Henriques</a>, <a href="http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1">Andrew Zisserman</a>, <a href="http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1">Andrea Vedaldi</a></p>
<p>Instance segmentation in 3D is a challenging task due to the lack of
large-scale annotated datasets. In this paper, we show that this task can be
addressed effectively by leveraging instead 2D pre-trained models for instance
segmentation. We propose a novel approach to lift 2D segments to 3D and fuse
them by means of a neural field representation, which encourages multi-view
consistency across frames. The core of our approach is a slow-fast clustering
objective function, which is scalable and well-suited for scenes with a large
number of objects. Unlike previous approaches, our method does not require an
upper bound on the number of objects or object tracking across frames. To
demonstrate the scalability of the slow-fast clustering, we create a new
semi-realistic dataset called the Messy Rooms dataset, which features scenes
with up to 500 objects per scene. Our approach outperforms the state-of-the-art
on challenging scenes from the ScanNet, Hypersim, and Replica datasets, as well
as on our newly created Messy Rooms dataset, demonstrating the effectiveness
and scalability of our slow-fast clustering method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.09267">Are ChatGPT and Other Similar Systems the Modern Lernaean Hydras of AI?. (arXiv:2306.09267v2 [cs.CY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ioannidis_D/0/1/0/all/0/1">Dimitrios Ioannidis</a>, <a href="http://arxiv.org/find/cs/1/au:+Kepner_J/0/1/0/all/0/1">Jeremy Kepner</a>, <a href="http://arxiv.org/find/cs/1/au:+Bowne_A/0/1/0/all/0/1">Andrew Bowne</a>, <a href="http://arxiv.org/find/cs/1/au:+Bryant_H/0/1/0/all/0/1">Harriet S. Bryant</a></p>
<p>The rise of Generative Artificial Intelligence systems (''AI systems'') has
created unprecedented social engagement. AI code generation systems provide
responses (output) to questions or requests by accessing the vast library of
open-source code created by developers over the past few decades. However, they
do so by allegedly stealing the open-source code stored in virtual libraries,
known as repositories. This Article focuses on how this happens and whether
there is a solution that protects innovation and avoids years of litigation. We
also touch upon the array of issues raised by the relationship between AI and
copyright. Looking ahead, we propose the following: (a) immediate changes to
the licenses for open-source code created by developers that will limit access
and/or use of any open-source code to humans only; (b) we suggest revisions to
the Massachusetts Institute of Technology (''MIT'') license so that AI systems
are required to procure appropriate licenses from open-source code developers,
which we believe will harmonize standards and build social consensus for the
benefit of all of humanity, rather than promote profit-driven centers of
innovation; (c) we call for urgent legislative action to protect the future of
AI systems while also promoting innovation; and (d) we propose a shift in the
burden of proof to AI systems in obfuscation cases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.11300">RS5M: A Large Scale Vision-Language Dataset for Remote Sensing Vision-Language Foundation Model. (arXiv:2306.11300v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zilun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1">Tiancheng Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yulong Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1">Jianwei Yin</a></p>
<p>Pre-trained Vision-Language Models (VLMs) utilizing extensive image-text
paired data have demonstrated unprecedented image-text association
capabilities, achieving remarkable results across various downstream tasks. A
critical challenge is how to make use of existing large-scale pre-trained VLMs,
which are trained on common objects, to perform the domain-specific transfer
for accomplishing domain-related downstream tasks. A critical challenge is how
to make use of existing large-scale pre-trained VLMs, which are trained on
common objects, to perform the domain-specific transfer for accomplishing
domain-related downstream tasks. In this paper, we propose a new framework that
includes the Domain pre-trained Vision-Language Model (DVLM), bridging the gap
between the General Vision-Language Model (GVLM) and domain-specific downstream
tasks. Moreover, we present an image-text paired dataset in the field of remote
sensing (RS), RS5M, which has 5 million RS images with English descriptions.
The dataset is obtained from filtering publicly available image-text paired
datasets and captioning label-only RS datasets with pre-trained VLM. These
constitute the first large-scale RS image-text paired dataset. Additionally, we
fine-tuned the CLIP model and tried several Parameter-Efficient Fine-Tuning
methods on RS5M to implement the DVLM. Experimental results show that our
proposed dataset is highly effective for various tasks, and our model GeoRSCLIP
improves upon the baseline or previous state-of-the-art model by $3\%\sim20\%$
in Zero-shot Classification (ZSC), $3\%\sim6\%$ in Remote Sensing Cross-Modal
Text-Image Retrieval (RSCTIR) and $4\%\sim5\%$ in Semantic Localization (SeLo)
tasks. Dataset and models have been released in:
\url{https://github.com/om-ai-lab/RS5M}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.12059">EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations. (arXiv:2306.12059v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1">Yi-Lun Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wood_B/0/1/0/all/0/1">Brandon Wood</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1">Abhishek Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Smidt_T/0/1/0/all/0/1">Tess Smidt</a></p>
<p>Equivariant Transformers such as Equiformer have demonstrated the efficacy of
applying Transformers to the domain of 3D atomistic systems. However, they are
limited to small degrees of equivariant representations due to their
computational complexity. In this paper, we investigate whether these
architectures can scale well to higher degrees. Starting from Equiformer, we
first replace $SO(3)$ convolutions with eSCN convolutions to efficiently
incorporate higher-degree tensors. Then, to better leverage the power of higher
degrees, we propose three architectural improvements -- attention
re-normalization, separable $S^2$ activation and separable layer normalization.
Putting this all together, we propose EquiformerV2, which outperforms previous
state-of-the-art methods on large-scale OC20 dataset by up to $9\%$ on forces,
$4\%$ on energies, offers better speed-accuracy trade-offs, and $2\times$
reduction in DFT calculations needed for computing adsorption energies.
Additionally, EquiformerV2 trained on only OC22 dataset outperforms GemNet-OC
trained on both OC20 and OC22 datasets, achieving much better data efficiency.
Finally, we compare EquiformerV2 with Equiformer on QM9 and OC20 S2EF-2M
datasets to better understand the performance gain brought by higher degrees.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.17100">RL4CO: a Unified Reinforcement Learning for Combinatorial Optimization Library. (arXiv:2306.17100v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Berto_F/0/1/0/all/0/1">Federico Berto</a>, <a href="http://arxiv.org/find/cs/1/au:+Hua_C/0/1/0/all/0/1">Chuanbo Hua</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Junyoung Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1">Minsu Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">Hyeonah Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Son_J/0/1/0/all/0/1">Jiwoo Son</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">Haeyeon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Joungho Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Jinkyoo Park</a></p>
<p>Deep reinforcement learning offers notable benefits in addressing
combinatorial problems over traditional solvers, reducing the reliance on
domain-specific knowledge and expert solutions, and improving computational
efficiency. Despite the recent surge in interest in neural combinatorial
optimization, practitioners often do not have access to a standardized code
base. Moreover, different algorithms are frequently based on fragmentized
implementations that hinder reproducibility and fair comparison. To address
these challenges, we introduce RL4CO, a unified Reinforcement Learning (RL) for
Combinatorial Optimization (CO) library. We employ state-of-the-art software
and best practices in implementation, such as modularity and configuration
management, to be flexible, easily modifiable, and extensible by researchers.
Thanks to our unified codebase, we benchmark baseline RL solvers with different
evaluation schemes on zero-shot performance, generalization, and adaptability
on diverse tasks. Notably, we find that some recent methods may fall behind
their predecessors depending on the evaluation settings. We hope RL4CO will
encourage the exploration of novel solutions to complex real-world tasks,
allowing the community to compare with existing methods through a unified
framework that decouples the science from software engineering. We open-source
our library at https://github.com/ai4co/rl4co.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01708">Distributional Model Equivalence for Risk-Sensitive Reinforcement Learning. (arXiv:2307.01708v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kastner_T/0/1/0/all/0/1">Tyler Kastner</a>, <a href="http://arxiv.org/find/cs/1/au:+Erdogdu_M/0/1/0/all/0/1">Murat A. Erdogdu</a>, <a href="http://arxiv.org/find/cs/1/au:+Farahmand_A/0/1/0/all/0/1">Amir-massoud Farahmand</a></p>
<p>We consider the problem of learning models for risk-sensitive reinforcement
learning. We theoretically demonstrate that proper value equivalence, a method
of learning models which can be used to plan optimally in the risk-neutral
setting, is not sufficient to plan optimally in the risk-sensitive setting. We
leverage distributional reinforcement learning to introduce two new notions of
model equivalence, one which is general and can be used to plan for any risk
measure, but is intractable; and a practical variation which allows one to
choose which risk measures they may plan optimally for. We demonstrate how our
framework can be used to augment any model-free risk-sensitive algorithm, and
provide both tabular and large-scale experiments to demonstrate its ability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08304">Efficient Computation of Counterfactual Bounds. (arXiv:2307.08304v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zaffalon_M/0/1/0/all/0/1">Marco Zaffalon</a>, <a href="http://arxiv.org/find/cs/1/au:+Antonucci_A/0/1/0/all/0/1">Alessandro Antonucci</a>, <a href="http://arxiv.org/find/cs/1/au:+Cabanas_R/0/1/0/all/0/1">Rafael Caba&#xf1;as</a>, <a href="http://arxiv.org/find/cs/1/au:+Huber_D/0/1/0/all/0/1">David Huber</a>, <a href="http://arxiv.org/find/cs/1/au:+Azzimonti_D/0/1/0/all/0/1">Dario Azzimonti</a></p>
<p>We assume to be given structural equations over discrete variables inducing a
directed acyclic graph, namely, a structural causal model, together with data
about its internal nodes. The question we want to answer is how we can compute
bounds for partially identifiable counterfactual queries from such an input. We
start by giving a map from structural casual models to credal networks. This
allows us to compute exact counterfactual bounds via algorithms for credal nets
on a subclass of structural causal models. Exact computation is going to be
inefficient in general given that, as we show, causal inference is NP-hard even
on polytrees. We target then approximate bounds via a causal EM scheme. We
evaluate their accuracy by providing credible intervals on the quality of the
approximation; we show through a synthetic benchmark that the EM scheme
delivers accurate results in a fair number of runs. In the course of the
discussion, we also point out what seems to be a neglected limitation to the
trending idea that counterfactual bounds can be computed without knowledge of
the structural equations. We also present a real case study on palliative care
to show how our algorithms can readily be used for practical purposes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10936">PASTA: Pretrained Action-State Transformer Agents. (arXiv:2307.10936v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Boige_R/0/1/0/all/0/1">Raphael Boige</a>, <a href="http://arxiv.org/find/cs/1/au:+Flet_Berliac_Y/0/1/0/all/0/1">Yannis Flet-Berliac</a>, <a href="http://arxiv.org/find/cs/1/au:+Flajolet_A/0/1/0/all/0/1">Arthur Flajolet</a>, <a href="http://arxiv.org/find/cs/1/au:+Richard_G/0/1/0/all/0/1">Guillaume Richard</a>, <a href="http://arxiv.org/find/cs/1/au:+Pierrot_T/0/1/0/all/0/1">Thomas Pierrot</a></p>
<p>Self-supervised learning has brought about a revolutionary paradigm shift in
various computing domains, including NLP, vision, and biology. Recent
approaches involve pre-training transformer models on vast amounts of unlabeled
data, serving as a starting point for efficiently solving downstream tasks. In
reinforcement learning, researchers have recently adapted these approaches,
developing models pre-trained on expert trajectories. This advancement enables
the models to tackle a broad spectrum of tasks, ranging from robotics to
recommendation systems. However, existing methods mostly rely on intricate
pre-training objectives tailored to specific downstream applications. This
paper conducts a comprehensive investigation of models, referred to as
pre-trained action-state transformer agents (PASTA). Our study covers a unified
methodology and covers an extensive set of general downstream tasks including
behavioral cloning, offline RL, sensor failure robustness, and dynamics change
adaptation. Our objective is to systematically compare various design choices
and offer valuable insights that will aid practitioners in developing robust
models. Key highlights of our study include tokenization at the component level
for actions and states, the use of fundamental pre-training objectives such as
next token prediction or masked language modeling, simultaneous training of
models across multiple domains, and the application of various fine-tuning
strategies. In this study, the developed models contain fewer than 7 million
parameters allowing a broad community to use these models and reproduce our
experiments. We hope that this study will encourage further research into the
use of transformers with first principle design choices to represent RL
trajectories and contribute to robust policy learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04371">Cumulative Reasoning with Large Language Models. (arXiv:2308.04371v5 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yifan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jingqin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1">Yang Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1">Andrew Chi-Chih Yao</a></p>
<p>While language models are powerful and versatile, they often fail to address
highly complex problems. This is because solving complex problems requires
deliberate thinking, which has been only minimally guided during training. In
this paper, we propose a new method called Cumulative Reasoning (CR), which
employs language models in a cumulative and iterative manner to emulate human
thought processes. By decomposing tasks into smaller components, CR streamlines
the problem-solving process, rendering it both more manageable and effective.
For logical inference tasks, CR consistently outperforms existing methods with
an improvement up to 9.3%, and achieves an accuracy of 98.04% on the curated
FOLIO wiki dataset. In the context of the Game of 24, CR achieves an accuracy
of 98%, which signifies a substantial enhancement of 24% over the previous
state-of-the-art method. Finally, on the MATH dataset, we establish new
state-of-the-art results with 58.0% overall accuracy, surpassing the previous
best approach by a margin of 4.2%, and achieving 43% relative improvement on
the hardest level 5 problems (22.4% to 32.1%). Additionally, we expand the
concept of Cumulative Reasoning to incorporate a Python code environment,
deliberately omitting external aids such as retrieval and web browsing and
focusing solely on the LLM's intrinsic reasoning capabilities within a Python
code environment. Our experiments in this setting yielded impressive results,
with an overall accuracy of 72.2% on the MATH dataset, significantly
outperforming the PAL method with 38.8% relative improvement. Code is available
at https://github.com/iiis-ai/cumulative-reasoning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04774">E$^3$-UAV: An Edge-based Energy-Efficient Object Detection System for Unmanned Aerial Vehicles. (arXiv:2308.04774v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Suo_J/0/1/0/all/0/1">Jiashun Suo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xingzhou Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1">Weisong Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1">Wei Zhou</a></p>
<p>Motivated by the advances in deep learning techniques, the application of
Unmanned Aerial Vehicle (UAV)-based object detection has proliferated across a
range of fields, including vehicle counting, fire detection, and city
monitoring. While most existing research studies only a subset of the
challenges inherent to UAV-based object detection, there are few studies that
balance various aspects to design a practical system for energy consumption
reduction. In response, we present the E$^3$-UAV, an edge-based
energy-efficient object detection system for UAVs. The system is designed to
dynamically support various UAV devices, edge devices, and detection
algorithms, with the aim of minimizing energy consumption by deciding the most
energy-efficient flight parameters (including flight altitude, flight speed,
detection algorithm, and sampling rate) required to fulfill the detection
requirements of the task. We first present an effective evaluation metric for
actual tasks and construct a transparent energy consumption model based on
hundreds of actual flight data to formalize the relationship between energy
consumption and flight parameters. Then we present a lightweight
energy-efficient priority decision algorithm based on a large quantity of
actual flight data to assist the system in deciding flight parameters. Finally,
we evaluate the performance of the system, and our experimental results
demonstrate that it can significantly decrease energy consumption in real-world
scenarios. Additionally, we provide four insights that can assist researchers
and engineers in their efforts to study UAV-based object detection further.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.08693">AI planning in the imagination: High-level planning on learned abstract search spaces. (arXiv:2308.08693v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Martin_C/0/1/0/all/0/1">Carlos Martin</a>, <a href="http://arxiv.org/find/cs/1/au:+Sandholm_T/0/1/0/all/0/1">Tuomas Sandholm</a></p>
<p>Search and planning algorithms have been a cornerstone of artificial
intelligence since the field's inception. Giving reinforcement learning agents
the ability to plan during execution time has resulted in significant
performance improvements in various domains. However, in real-world
environments, the model with respect to which the agent plans has been
constrained to be grounded in the real environment itself, as opposed to a more
abstract model which allows for planning over compound actions and behaviors.
We propose a new method, called PiZero, that gives an agent the ability to plan
in an abstract search space that the agent learns during training, which is
completely decoupled from the real environment. Unlike prior approaches, this
enables the agent to perform high-level planning at arbitrary timescales and
reason in terms of compound or temporally-extended actions, which can be useful
in environments where large numbers of base-level micro-actions are needed to
perform relevant macro-actions. In addition, our method is more general than
comparable prior methods because it seamlessly handles settings with continuous
action spaces, combinatorial action spaces, and partial observability. We
evaluate our method on multiple domains, including the traveling salesman
problem, Sokoban, 2048, the facility location problem, and Pacman.
Experimentally, it outperforms comparable prior methods without assuming access
to an environment simulator at execution time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.15452">When Do Program-of-Thoughts Work for Reasoning?. (arXiv:2308.15452v5 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1">Zhen Bi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Ningyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yinuo Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1">Shumin Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1">Guozhou Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huajun Chen</a></p>
<p>In the realm of embodied artificial intelligence, the reasoning capabilities
of Large Language Models (LLMs) play a pivotal role. Although there are
effective methods like program-of-thought prompting for LLMs which uses
programming language to tackle complex reasoning tasks, the specific impact of
code data on the improvement of reasoning capabilities remains under-explored.
To address this gap, we propose complexity-impacted reasoning score (CIRS),
which combines structural and logical attributes, to measure the correlation
between code and reasoning abilities. Specifically, we use the abstract syntax
tree to encode the structural information and calculate logical complexity by
considering the difficulty and the cyclomatic complexity. Through an empirical
analysis, we find not all code data of complexity can be learned or understood
by LLMs. Optimal level of complexity is critical to the improvement of
reasoning abilities by program-aided prompting. Then we design an
auto-synthesizing and stratifying algorithm, and apply it to instruction
generation for mathematical reasoning and code data filtering for code
generation tasks. Extensive results demonstrates the effectiveness of our
proposed approach. Code will be integrated into the EasyInstruct framework at
https://github.com/zjunlp/EasyInstruct.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16458">BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge. (arXiv:2308.16458v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1">Xiangru Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_B/0/1/0/all/0/1">Bill Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1">Rick Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiakang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xinyun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gerstein_M/0/1/0/all/0/1">Mark Gerstein</a></p>
<p>Pre-trained large language models have significantly improved code
generation. As these models scale up, there is an increasing need for the
output to handle more intricate tasks and to be appropriately specialized to
particular domains. Here, we target bioinformatics due to the amount of
specialized domain knowledge, algorithms, and data operations this discipline
requires. We present BioCoder, a benchmark developed to evaluate large language
models (LLMs) in generating bioinformatics-specific code. BioCoder spans a
broad spectrum of the field and covers cross-file dependencies, class
declarations, and global variables. It incorporates 1026 Python functions and
1243 Java methods extracted from GitHub, along with 253 examples from the
Rosalind Project, all pertaining to bioinformatics. Using topic modeling we
show that overall coverage of the included code is representative of the full
spectrum of bioinformatics calculations. BioCoder incorporates a fuzz-testing
framework for evaluation. We have applied it to evaluate many models including
InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT5+,
GPT-3.5, and GPT-4. Furthermore, we finetuned StarCoder, demonstrating how our
dataset can effectively enhance the performance of LLMs on our benchmark (by
&gt;15% in terms of Pass@K in certain prompt configurations and always &gt;3%). The
results highlight two key aspects of successful models: (1) Successful models
accommodate a long prompt (&gt; ~2600 tokens) with full context, for functional
dependencies. (2) They contain specific domain knowledge of bioinformatics,
beyond just general coding knowledge. This is evident from the performance gain
of GPT-3.5/4 compared to the smaller models on the benchmark (50% vs up to
~25%). Our dataset, benchmark, Docker images, and scripts required for testing
are all available at https://github.com/gersteinlab/biocoder.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.03886">FIND: A Function Description Benchmark for Evaluating Interpretability Methods. (arXiv:2309.03886v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schwettmann_S/0/1/0/all/0/1">Sarah Schwettmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Shaham_T/0/1/0/all/0/1">Tamar Rott Shaham</a>, <a href="http://arxiv.org/find/cs/1/au:+Materzynska_J/0/1/0/all/0/1">Joanna Materzynska</a>, <a href="http://arxiv.org/find/cs/1/au:+Chowdhury_N/0/1/0/all/0/1">Neil Chowdhury</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shuang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1">Jacob Andreas</a>, <a href="http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1">David Bau</a>, <a href="http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1">Antonio Torralba</a></p>
<p>Labeling neural network submodules with human-legible descriptions is useful
for many downstream tasks: such descriptions can surface failures, guide
interventions, and perhaps even explain important model behaviors. To date,
most mechanistic descriptions of trained networks have involved small models,
narrowly delimited phenomena, and large amounts of human labor. Labeling all
human-interpretable sub-computations in models of increasing size and
complexity will almost certainly require tools that can generate and validate
descriptions automatically. Recently, techniques that use learned models
in-the-loop for labeling have begun to gain traction, but methods for
evaluating their efficacy are limited and ad-hoc. How should we validate and
compare open-ended labeling tools? This paper introduces FIND (Function
INterpretation and Description), a benchmark suite for evaluating the building
blocks of automated interpretability methods. FIND contains functions that
resemble components of trained neural networks, and accompanying descriptions
of the kind we seek to generate. The functions span textual and numeric
domains, and involve a range of real-world complexities. We evaluate methods
that use pretrained language models (LMs) to produce descriptions of function
behavior in natural language and code. Additionally, we introduce a new
interactive method in which an Automated Interpretability Agent (AIA) generates
function descriptions. We find that an AIA, built from an LM with black-box
access to functions, can infer function structure, acting as a scientist by
forming hypotheses, proposing experiments, and updating descriptions in light
of new data. However, AIA descriptions tend to capture global function behavior
and miss local details. These results suggest that FIND will be useful for
evaluating more sophisticated interpretability methods before they are applied
to real-world models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.06550">Synthetic Text Generation using Hypergraph Representations. (arXiv:2309.06550v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Raman_N/0/1/0/all/0/1">Natraj Raman</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1">Sameena Shah</a></p>
<p>Generating synthetic variants of a document is often posed as text-to-text
transformation. We propose an alternate LLM based method that first decomposes
a document into semantic frames and then generates text using this interim
sparse format. The frames are modeled using a hypergraph, which allows
perturbing the frame contents in a principled manner. Specifically, new
hyperedges are mined through topological analysis and complex polyadic
relationships including hierarchy and temporal dynamics are accommodated. We
show that our solution generates documents that are diverse, coherent and vary
in style, sentiment, format, composition and facts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07064">A Comprehensive Analysis of the Role of Artificial Intelligence and Machine Learning in Modern Digital Forensics and Incident Response. (arXiv:2309.07064v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dunsin_D/0/1/0/all/0/1">Dipo Dunsin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghanem_M/0/1/0/all/0/1">Mohamed C. Ghanem</a>, <a href="http://arxiv.org/find/cs/1/au:+Ouazzane_K/0/1/0/all/0/1">Karim Ouazzane</a>, <a href="http://arxiv.org/find/cs/1/au:+Vassilev_V/0/1/0/all/0/1">Vassil Vassilev</a></p>
<p>In the dynamic landscape of digital forensics, the integration of Artificial
Intelligence (AI) and Machine Learning (ML) stands as a transformative
technology, poised to amplify the efficiency and precision of digital forensics
investigations. However, the use of ML and AI in digital forensics is still in
its nascent stages. As a result, this paper gives a thorough and in-depth
analysis that goes beyond a simple survey and review. The goal is to look
closely at how AI and ML techniques are used in digital forensics and incident
response. This research explores cutting-edge research initiatives that cross
domains such as data collection and recovery, the intricate reconstruction of
cybercrime timelines, robust big data analysis, pattern recognition,
safeguarding the chain of custody, and orchestrating responsive strategies to
hacking incidents. This endeavour digs far beneath the surface to unearth the
intricate ways AI-driven methodologies are shaping these crucial facets of
digital forensics practice. While the promise of AI in digital forensics is
evident, the challenges arising from increasing database sizes and evolving
criminal tactics necessitate ongoing collaborative research and refinement
within the digital forensics profession. This study examines the contributions,
limitations, and gaps in the existing research, shedding light on the potential
and limitations of AI and ML techniques. By exploring these different research
areas, we highlight the critical need for strategic planning, continual
research, and development to unlock AI's full potential in digital forensics
and incident response. Ultimately, this paper underscores the significance of
AI and ML integration in digital forensics, offering insights into their
benefits, drawbacks, and broader implications for tackling modern cyber
threats.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.09126">How much can ChatGPT really help Computational Biologists in Programming?. (arXiv:2309.09126v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rahman_C/0/1/0/all/0/1">Chowdhury Rafeed Rahman</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_L/0/1/0/all/0/1">Limsoon Wong</a></p>
<p>ChatGPT, a recently developed product by openAI, is successfully leaving its
mark as a multi-purpose natural language based chatbot. In this paper, we are
more interested in analyzing its potential in the field of computational
biology. A major share of work done by computational biologists these days
involve coding up bioinformatics algorithms, analyzing data, creating
pipelining scripts and even machine learning modeling and feature extraction.
This paper focuses on the potential influence (both positive and negative) of
ChatGPT in the mentioned aspects with illustrative examples from different
perspectives. Compared to other fields of computer science, computational
biology has - (1) less coding resources, (2) more sensitivity and bias issues
(deals with medical data) and (3) more necessity of coding assistance (people
from diverse background come to this field). Keeping such issues in mind, we
cover use cases such as code writing, reviewing, debugging, converting,
refactoring and pipelining using ChatGPT from the perspective of computational
biologists in this paper.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.13409">Time-Series Forecasting: Unleashing Long-Term Dependencies with Fractionally Differenced Data. (arXiv:2309.13409v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maitra_S/0/1/0/all/0/1">Sarit Maitra</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishra_V/0/1/0/all/0/1">Vivek Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Dwivedi_S/0/1/0/all/0/1">Srashti Dwivedi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kundu_S/0/1/0/all/0/1">Sukanya Kundu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kundu_G/0/1/0/all/0/1">Goutam Kumar Kundu</a></p>
<p>This study introduces a novel forecasting strategy that leverages the power
of fractional differencing (FD) to capture both short- and long-term
dependencies in time series data. Unlike traditional integer differencing
methods, FD preserves memory in series while stabilizing it for modeling
purposes. By applying FD to financial data from the SPY index and incorporating
sentiment analysis from news reports, this empirical analysis explores the
effectiveness of FD in conjunction with binary classification of target
variables. Supervised classification algorithms were employed to validate the
performance of FD series. The results demonstrate the superiority of FD over
integer differencing, as confirmed by Receiver Operating Characteristic/Area
Under the Curve (ROCAUC) and Mathews Correlation Coefficient (MCC) evaluations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.13550">I-AI: A Controllable &amp; Interpretable AI System for Decoding Radiologists&#x27; Intense Focus for Accurate CXR Diagnoses. (arXiv:2309.13550v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1">Trong Thang Pham</a>, <a href="http://arxiv.org/find/cs/1/au:+Brecheisen_J/0/1/0/all/0/1">Jacob Brecheisen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1">Anh Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1">Hien Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1">Ngan Le</a></p>
<p>In the field of chest X-ray (CXR) diagnosis, existing works often focus
solely on determining where a radiologist looks, typically through tasks such
as detection, segmentation, or classification. However, these approaches are
often designed as black-box models, lacking interpretability. In this paper, we
introduce Interpretable Artificial Intelligence (I-AI) a novel and unified
controllable interpretable pipeline for decoding the intense focus of
radiologists in CXR diagnosis. Our I-AI addresses three key questions: where a
radiologist looks, how long they focus on specific areas, and what findings
they diagnose. By capturing the intensity of the radiologist's gaze, we provide
a unified solution that offers insights into the cognitive process underlying
radiological interpretation. Unlike current methods that rely on black-box
machine learning models, which can be prone to extracting erroneous information
from the entire input image during the diagnosis process, we tackle this issue
by effectively masking out irrelevant information. Our proposed I-AI leverages
a vision-language model, allowing for precise control over the interpretation
process while ensuring the exclusion of irrelevant features. To train our I-AI
model, we utilize an eye gaze dataset to extract anatomical gaze information
and generate ground truth heatmaps. Through extensive experimentation, we
demonstrate the efficacy of our method. We showcase that the attention
heatmaps, designed to mimic radiologists' focus, encode sufficient and relevant
information, enabling accurate classification tasks using only a portion of
CXR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.13925">Towards Surveillance Video-and-Language Understanding: New Dataset, Baselines, and Challenges. (arXiv:2309.13925v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_T/0/1/0/all/0/1">Tongtong Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xuange Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1">Kun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1">Jian Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiao_Z/0/1/0/all/0/1">Zhenzhen Jiao</a></p>
<p>Surveillance videos are an essential component of daily life with various
critical applications, particularly in public security. However, current
surveillance video tasks mainly focus on classifying and localizing anomalous
events. Existing methods are limited to detecting and classifying the
predefined events with unsatisfactory semantic understanding, although they
have obtained considerable performance. To address this issue, we propose a new
research direction of surveillance video-and-language understanding, and
construct the first multimodal surveillance video dataset. We manually annotate
the real-world surveillance dataset UCF-Crime with fine-grained event content
and timing. Our newly annotated dataset, UCA (UCF-Crime Annotation), contains
23,542 sentences, with an average length of 20 words, and its annotated videos
are as long as 110.7 hours. Furthermore, we benchmark SOTA models for four
multimodal tasks on this newly created dataset, which serve as new baselines
for surveillance video-and-language understanding. Through our experiments, we
find that mainstream models used in previously publicly available datasets
perform poorly on surveillance video, which demonstrates the new challenges in
surveillance video-and-language understanding. To validate the effectiveness of
our UCA, we conducted experiments on multimodal anomaly detection. The results
demonstrate that our multimodal surveillance learning can improve the
performance of conventional anomaly detection tasks. All the experiments
highlight the necessity of constructing this dataset to advance surveillance
AI. The link to our dataset is provided at:
https://xuange923.github.io/Surveillance-Video-Understanding.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00052">AI ensemble for signal detection of higher order gravitational wave modes of quasi-circular, spinning, non-precessing binary black hole mergers. (arXiv:2310.00052v2 [astro-ph.IM] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/astro-ph/1/au:+Tian_M/0/1/0/all/0/1">Minyang Tian</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Huerta_E/0/1/0/all/0/1">E. A. Huerta</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Zheng_H/0/1/0/all/0/1">Huihuo Zheng</a></p>
<p>We introduce spatiotemporal-graph models that concurrently process data from
the twin advanced LIGO detectors and the advanced Virgo detector. We trained
these AI classifiers with 2.4 million IMRPhenomXPHM waveforms that describe
quasi-circular, spinning, non-precessing binary black hole mergers with
component masses $m_{\{1,2\}}\in[3M_\odot, 50 M_\odot]$, and individual spins
$s^z_{\{1,2\}}\in[-0.9, 0.9]$; and which include the $(\ell, |m|) = \{(2, 2),
(2, 1), (3, 3), (3, 2), (4, 4)\}$ modes, and mode mixing effects in the $\ell =
3, |m| = 2$ harmonics. We trained these AI classifiers within 22 hours using
distributed training over 96 NVIDIA V100 GPUs in the Summit supercomputer. We
then used transfer learning to create AI predictors that estimate the total
mass of potential binary black holes identified by all AI classifiers in the
ensemble. We used this ensemble, 3 classifiers for signal detection and 2 total
mass predictors, to process a year-long test set in which we injected 300,000
signals. This year-long test set was processed within 5.19 minutes using 1024
NVIDIA A100 GPUs in the Polaris supercomputer (for AI inference) and 128 CPU
nodes in the ThetaKNL supercomputer (for post-processing of noise triggers),
housed at the Argonne Leadership Computing Facility. These studies indicate
that our AI ensemble provides state-of-the-art signal detection accuracy, and
reports 2 misclassifications for every year of searched data. This is the first
AI ensemble designed to search for and find higher order gravitational wave
mode signals.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02066">De Novo Drug Design with Joint Transformers. (arXiv:2310.02066v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Izdebski_A/0/1/0/all/0/1">Adam Izdebski</a>, <a href="http://arxiv.org/find/cs/1/au:+Weglarz_Tomczak_E/0/1/0/all/0/1">Ewelina Weglarz-Tomczak</a>, <a href="http://arxiv.org/find/cs/1/au:+Szczurek_E/0/1/0/all/0/1">Ewa Szczurek</a>, <a href="http://arxiv.org/find/cs/1/au:+Tomczak_J/0/1/0/all/0/1">Jakub M. Tomczak</a></p>
<p>De novo drug design requires simultaneously generating novel molecules
outside of training data and predicting their target properties, making it a
hard task for generative models. To address this, we propose Joint Transformer
that combines a Transformer decoder, Transformer encoder, and a predictor in a
joint generative model with shared weights. We formulate a probabilistic
black-box optimization algorithm that employs Joint Transformer to generate
novel molecules with improved target properties and outperforms other
SMILES-based optimization methods in de novo drug design.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02261">Adaptive Online Non-stochastic Control. (arXiv:2310.02261v2 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Mhaisen_N/0/1/0/all/0/1">Naram Mhaisen</a>, <a href="http://arxiv.org/find/math/1/au:+Iosifidis_G/0/1/0/all/0/1">George Iosifidis</a></p>
<p>We tackle the problem of Non-stochastic Control (NSC) with the aim of
obtaining algorithms whose policy regret is proportional to the difficulty of
the controlled environment. Namely, we tailor the Follow The Regularized Leader
(FTRL) framework to dynamical systems by using regularizers that are
proportional to the actual witnessed costs. The main challenge arises from
using the proposed adaptive regularizers in the presence of a state, or
equivalently, a memory, which couples the effect of the online decisions and
requires new tools for bounding the regret. Via new analysis techniques for NSC
and FTRL integration, we obtain novel disturbance action controllers (DAC) with
sub-linear data adaptive policy regret bounds that shrink when the trajectory
of costs has small gradients, while staying sub-linear even in the worst case.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03234">Non-Smooth Weakly-Convex Finite-sum Coupled Compositional Optimization. (arXiv:2310.03234v2 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Hu_Q/0/1/0/all/0/1">Quanqi Hu</a>, <a href="http://arxiv.org/find/math/1/au:+Zhu_D/0/1/0/all/0/1">Dixian Zhu</a>, <a href="http://arxiv.org/find/math/1/au:+Yang_T/0/1/0/all/0/1">Tianbao Yang</a></p>
<p>This paper investigates new families of compositional optimization problems,
called $\underline{\bf n}$on-$\underline{\bf s}$mooth $\underline{\bf
w}$eakly-$\underline{\bf c}$onvex $\underline{\bf f}$inite-sum $\underline{\bf
c}$oupled $\underline{\bf c}$ompositional $\underline{\bf o}$ptimization (NSWC
FCCO). There has been a growing interest in FCCO due to its wide-ranging
applications in machine learning and AI, as well as its ability to address the
shortcomings of stochastic algorithms based on empirical risk minimization.
However, current research on FCCO presumes that both the inner and outer
functions are smooth, limiting their potential to tackle a more diverse set of
problems. Our research expands on this area by examining non-smooth
weakly-convex FCCO, where the outer function is weakly convex and
non-decreasing, and the inner function is weakly-convex. We analyze a
single-loop algorithm and establish its complexity for finding an
$\epsilon$-stationary point of the Moreau envelop of the objective function.
Additionally, we also extend the algorithm to solving novel non-smooth
weakly-convex tri-level finite-sum coupled compositional optimization problems,
which feature a nested arrangement of three functions. Lastly, we explore the
applications of our algorithms in deep learning for two-way partial AUC
maximization and multi-instance two-way partial AUC maximization, using
empirical studies to showcase the effectiveness of the proposed algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07944">AutoRepo: A general framework for multi-modal LLM-based automated construction reporting. (arXiv:2310.07944v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pu_H/0/1/0/all/0/1">Hongxu Pu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xincong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jing Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1">Runhao Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Heng Li</a></p>
<p>Ensuring the safety, quality, and timely completion of construction projects
is paramount, with construction inspections serving as a vital instrument
towards these goals. Nevertheless, the predominantly manual approach of
present-day inspections frequently results in inefficiencies and inadequate
information management. Such methods often fall short of providing holistic,
exhaustive assessments, consequently engendering regulatory oversights and
potential safety hazards. To address this issue, this paper presents a novel
framework named AutoRepo for automated generation of construction inspection
reports. The unmanned vehicles efficiently perform construction inspections and
collect scene information, while the multimodal large language models (LLMs)
are leveraged to automatically generate the inspection reports. The framework
was applied and tested on a real-world construction site, demonstrating its
potential to expedite the inspection process, significantly reduce resource
allocation, and produce high-quality, regulatory standard-compliant inspection
reports. This research thus underscores the immense potential of multimodal
large language models in revolutionizing construction inspection practices,
signaling a significant leap forward towards a more efficient and safer
construction management paradigm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.09219">&quot;Kelly is a Warm Person, Joseph is a Role Model&quot;: Gender Biases in LLM-Generated Reference Letters. (arXiv:2310.09219v5 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1">Yixin Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Pu_G/0/1/0/all/0/1">George Pu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jiao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Garimella_A/0/1/0/all/0/1">Aparna Garimella</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1">Kai-Wei Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1">Nanyun Peng</a></p>
<p>Large Language Models (LLMs) have recently emerged as an effective tool to
assist individuals in writing various types of content, including professional
documents such as recommendation letters. Though bringing convenience, this
application also introduces unprecedented fairness concerns. Model-generated
reference letters might be directly used by users in professional scenarios. If
underlying biases exist in these model-constructed letters, using them without
scrutinization could lead to direct societal harms, such as sabotaging
application success rates for female applicants. In light of this pressing
issue, it is imminent and necessary to comprehensively study fairness issues
and associated harms in this real-world use case. In this paper, we critically
examine gender biases in LLM-generated reference letters. Drawing inspiration
from social science findings, we design evaluation methods to manifest biases
through 2 dimensions: (1) biases in language style and (2) biases in lexical
content. We further investigate the extent of bias propagation by analyzing the
hallucination bias of models, a term that we define to be bias exacerbation in
model-hallucinated contents. Through benchmarking evaluation on 2 popular LLMs-
ChatGPT and Alpaca, we reveal significant gender biases in LLM-generated
recommendation letters. Our findings not only warn against using LLMs for this
application without scrutinization, but also illuminate the importance of
thoroughly studying hidden biases and harms in LLM-generated professional
documents.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.11211">Understanding Fairness Surrogate Functions in Algorithmic Fairness. (arXiv:2310.11211v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1">Wei Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhanke Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhicong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1">Bo Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yong Liu</a></p>
<p>It has been observed that machine learning algorithms exhibit biased
predictions against certain population groups. To mitigate such bias while
achieving comparable accuracy, a promising approach is to introduce surrogate
functions of the concerned fairness definition and solve a constrained
optimization problem. However, it is intriguing in previous work that such
fairness surrogate functions may yield unfair results and high instability. In
this work, in order to deeply understand them, taking a widely used fairness
definition--demographic parity as an example, we show that there is a
surrogate-fairness gap between the fairness definition and the fairness
surrogate function. Also, the theoretical analysis and experimental results
about the gap motivate us that the fairness and stability will be affected by
the points far from the decision boundary, which is the large margin points
issue investigated in this paper. To address it, we propose the general sigmoid
surrogate to simultaneously reduce both the surrogate-fairness gap and the
variance, and offer a rigorous fairness and stability upper bound.
Interestingly, the theory also provides insights into two important issues that
deal with the large margin points as well as obtaining a more balanced dataset
are beneficial to fairness and stability. Furthermore, we elaborate a novel and
general algorithm called Balanced Surrogate, which iteratively reduces the gap
to mitigate unfairness. Finally, we provide empirical evidence showing that our
methods consistently improve fairness and stability while maintaining accuracy
comparable to the baselines in three real-world datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.11730">Federated Heterogeneous Graph Neural Network for Privacy-preserving Recommendation. (arXiv:2310.11730v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1">Bo Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yang Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haoyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1">Wenchuan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1">Junping Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1">Chuan Shi</a></p>
<p>Heterogeneous information network (HIN), which contains rich semantics
depicted by meta-paths, has become a powerful tool to alleviate data sparsity
in recommender systems. Existing HIN-based recommendations hold the data
centralized storage assumption and conduct centralized model training. However,
the real-world data is often stored in a distributed manner for privacy
concerns, resulting in the failure of centralized HIN-based recommendations. In
this paper, we suggest the HIN is partitioned into private HINs stored in the
client side and shared HINs in the server. Following this setting, we propose a
federated heterogeneous graph neural network (FedHGNN) based framework, which
can collaboratively train a recommendation model on distributed HINs without
leaking user privacy. Specifically, we first formalize the privacy definition
in the light of differential privacy for HIN-based federated recommendation,
which aims to protect user-item interactions of private HIN as well as user's
high-order patterns from shared HINs. To recover the broken meta-path based
semantics caused by distributed data storage and satisfy the proposed privacy,
we elaborately design a semantic-preserving user interactions publishing
method, which locally perturbs user's high-order patterns as well as related
user-item interactions for publishing. After that, we propose a HGNN model for
recommendation, which conducts node- and semantic-level aggregations to capture
recovered semantics. Extensive experiments on three datasets demonstrate our
model outperforms existing methods by a large margin (up to 34% in HR@10 and
42% in NDCG@10) under an acceptable privacy budget.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.14714">BatteryML:An Open-source platform for Machine Learning on Battery Degradation. (arXiv:2310.14714v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Han Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gui_X/0/1/0/all/0/1">Xiaofan Gui</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1">Shun Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Ziheng Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuqi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1">Jiang Bian</a></p>
<p>Battery degradation remains a pivotal concern in the energy storage domain,
with machine learning emerging as a potent tool to drive forward insights and
solutions. However, this intersection of electrochemical science and machine
learning poses complex challenges. Machine learning experts often grapple with
the intricacies of battery science, while battery researchers face hurdles in
adapting intricate models tailored to specific datasets. Beyond this, a
cohesive standard for battery degradation modeling, inclusive of data formats
and evaluative benchmarks, is conspicuously absent. Recognizing these
impediments, we present BatteryML - a one-step, all-encompass, and open-source
platform designed to unify data preprocessing, feature extraction, and the
implementation of both traditional and state-of-the-art models. This
streamlined approach promises to enhance the practicality and efficiency of
research applications. BatteryML seeks to fill this void, fostering an
environment where experts from diverse specializations can collaboratively
contribute, thus elevating the collective understanding and advancement of
battery research.The code for our project is publicly available on GitHub at
https://github.com/microsoft/BatteryML.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.15020">Invariance is Key to Generalization: Examining the Role of Representation in Sim-to-Real Transfer for Visual Navigation. (arXiv:2310.15020v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ai_B/0/1/0/all/0/1">Bo Ai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhanxin Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hsu_D/0/1/0/all/0/1">David Hsu</a></p>
<p>The data-driven approach to robot control has been gathering pace rapidly,
yet generalization to unseen task domains remains a critical challenge. We
argue that the key to generalization is representations that are (i) rich
enough to capture all task-relevant information and (ii) invariant to
superfluous variability between the training and the test domains. We
experimentally study such a representation -- containing both depth and
semantic information -- for visual navigation and show that it enables a
control policy trained entirely in simulated indoor scenes to generalize to
diverse real-world environments, both indoors and outdoors. Further, we show
that our representation reduces the A-distance between the training and test
domains, improving the generalization error bound as a result. Our proposed
approach is scalable: the learned policy improves continuously, as the
foundation models that it exploits absorb more diverse data during
pre-training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17378">Optimization dependent generalization bound for ReLU networks based on sensitivity in the tangent bundle. (arXiv:2310.17378v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Racz_D/0/1/0/all/0/1">D&#xe1;niel R&#xe1;cz</a>, <a href="http://arxiv.org/find/cs/1/au:+Petreczky_M/0/1/0/all/0/1">Mih&#xe1;ly Petreczky</a>, <a href="http://arxiv.org/find/cs/1/au:+Csertan_A/0/1/0/all/0/1">Andr&#xe1;s Csert&#xe1;n</a>, <a href="http://arxiv.org/find/cs/1/au:+Daroczy_B/0/1/0/all/0/1">B&#xe1;lint Dar&#xf3;czy</a></p>
<p>Recent advances in deep learning have given us some very promising results on
the generalization ability of deep neural networks, however literature still
lacks a comprehensive theory explaining why heavily over-parametrized models
are able to generalize well while fitting the training data. In this paper we
propose a PAC type bound on the generalization error of feedforward ReLU
networks via estimating the Rademacher complexity of the set of networks
available from an initial parameter vector via gradient descent. The key idea
is to bound the sensitivity of the network's gradient to perturbation of the
input data along the optimization trajectory. The obtained bound does not
explicitly depend on the depth of the network. Our results are experimentally
verified on the MNIST and CIFAR-10 datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17769">Social Contract AI: Aligning AI Assistants with Implicit Group Norms. (arXiv:2310.17769v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Franken_J/0/1/0/all/0/1">Jan-Philipp Fr&#xe4;nken</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwok_S/0/1/0/all/0/1">Sam Kwok</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_P/0/1/0/all/0/1">Peixuan Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Gandhi_K/0/1/0/all/0/1">Kanishk Gandhi</a>, <a href="http://arxiv.org/find/cs/1/au:+Arumugam_D/0/1/0/all/0/1">Dilip Arumugam</a>, <a href="http://arxiv.org/find/cs/1/au:+Moore_J/0/1/0/all/0/1">Jared Moore</a>, <a href="http://arxiv.org/find/cs/1/au:+Tamkin_A/0/1/0/all/0/1">Alex Tamkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Gerstenberg_T/0/1/0/all/0/1">Tobias Gerstenberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1">Noah D. Goodman</a></p>
<p>We explore the idea of aligning an AI assistant by inverting a model of
users' (unknown) preferences from observed interactions. To validate our
proposal, we run proof-of-concept simulations in the economic ultimatum game,
formalizing user preferences as policies that guide the actions of simulated
players. We find that the AI assistant accurately aligns its behavior to match
standard policies from the economic literature (e.g., selfish, altruistic).
However, the assistant's learned policies lack robustness and exhibit limited
generalization in an out-of-distribution setting when confronted with a
currency (e.g., grams of medicine) that was not included in the assistant's
training distribution. Additionally, we find that when there is inconsistency
in the relationship between language use and an unknown policy (e.g., an
altruistic policy combined with rude language), the assistant's learning of the
policy is slowed. Overall, our preliminary results suggest that developing
simulation frameworks in which AI assistants need to infer preferences from
diverse users can provide a valuable approach for studying practical alignment
questions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.18144">Improving Intrinsic Exploration by Creating Stationary Objectives. (arXiv:2310.18144v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Castanyer_R/0/1/0/all/0/1">Roger Creus Castanyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Romoff_J/0/1/0/all/0/1">Joshua Romoff</a>, <a href="http://arxiv.org/find/cs/1/au:+Berseth_G/0/1/0/all/0/1">Glen Berseth</a></p>
<p>Exploration bonuses in reinforcement learning guide long-horizon exploration
by defining custom intrinsic objectives. Several exploration objectives like
count-based bonuses, pseudo-counts, and state-entropy maximization are
non-stationary and hence are difficult to optimize for the agent. While this
issue is generally known, it is usually omitted and solutions remain
under-explored. The key contribution of our work lies in transforming the
original non-stationary rewards into stationary rewards through an augmented
state representation. For this purpose, we introduce the Stationary Objectives
For Exploration (SOFE) framework. SOFE requires identifying sufficient
statistics for different exploration bonuses and finding an efficient encoding
of these statistics to use as input to a deep network. SOFE is based on
proposing state augmentations that expand the state space but hold the promise
of simplifying the optimization of the agent's objective. We show that SOFE
improves the performance of several exploration objectives, including
count-based bonuses, pseudo-counts, and state-entropy maximization. Moreover,
SOFE outperforms prior methods that attempt to stabilize the optimization of
intrinsic objectives. We demonstrate the efficacy of SOFE in hard-exploration
problems, including sparse-reward tasks, pixel-based observations, 3D
navigation, and procedurally generated environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.18534">Multi Time Scale World Models. (arXiv:2310.18534v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shaj_V/0/1/0/all/0/1">Vaisakh Shaj</a>, <a href="http://arxiv.org/find/cs/1/au:+Zadeh_S/0/1/0/all/0/1">Saleh Gholam Zadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Demir_O/0/1/0/all/0/1">Ozan Demir</a>, <a href="http://arxiv.org/find/cs/1/au:+Douat_L/0/1/0/all/0/1">Luiz Ricardo Douat</a>, <a href="http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1">Gerhard Neumann</a></p>
<p>Intelligent agents use internal world models to reason and make predictions
about different courses of their actions at many scales. Devising learning
paradigms and architectures that allow machines to learn world models that
operate at multiple levels of temporal abstractions while dealing with complex
uncertainty predictions is a major technical hurdle. In this work, we propose a
probabilistic formalism to learn multi-time scale world models which we call
the Multi Time Scale State Space (MTS3) model. Our model uses a computationally
efficient inference scheme on multiple time scales for highly accurate
long-horizon predictions and uncertainty estimates over several seconds into
the future. Our experiments, which focus on action conditional long horizon
future predictions, show that MTS3 outperforms recent methods on several system
identification benchmarks including complex simulated and real-world dynamical
systems. Code is available at this repository: https://github.com/ALRhub/MTS3.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.18940">Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game. (arXiv:2310.18940v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zelai Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1">Chao Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_F/0/1/0/all/0/1">Fei Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yi Wu</a></p>
<p>Agents built with large language models (LLMs) have recently achieved great
advancements. However, most of the efforts focus on single-agent or cooperative
settings, leaving more general multi-agent environments underexplored. We
propose a new framework powered by reinforcement learning (RL) to develop
strategic language agents, i.e., LLM-based agents with strategic thinking
ability, for a popular language game, Werewolf. Werewolf is a social deduction
game with hidden roles that involves both cooperation and competition and
emphasizes deceptive communication and diverse gameplay. Our agent tackles this
game by first using LLMs to reason about potential deceptions and generate a
set of strategically diverse actions. Then an RL policy, which selects an
action from the candidates, is learned by population-based training to enhance
the agents' decision-making ability. By combining LLMs with the RL policy, our
agent produces a variety of emergent strategies, achieves the highest win rate
against other LLM-based agents, and stays robust against adversarial human
players in the Werewolf game.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19112">Efficient IoT Inference via Context-Awareness. (arXiv:2310.19112v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rastikerdar_M/0/1/0/all/0/1">Mohammad Mehdi Rastikerdar</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1">Shiwei Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_H/0/1/0/all/0/1">Hui Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganesan_D/0/1/0/all/0/1">Deepak Ganesan</a></p>
<p>While existing strategies to execute deep learning-based classification on
low-power platforms assume the models are trained on all classes of interest,
this paper posits that adopting context-awareness i.e. narrowing down a
classification task to the current deployment context consisting of only recent
inference queries can substantially enhance performance in resource-constrained
environments. We propose a new paradigm, CACTUS, for scalable and efficient
context-aware classification where a micro-classifier recognizes a small set of
classes relevant to the current context and, when context change happens (e.g.,
a new class comes into the scene), rapidly switches to another suitable
micro-classifier. CACTUS features several innovations, including optimizing the
training cost of context-aware classifiers, enabling on-the-fly context-aware
switching between classifiers, and balancing context switching costs and
performance gains via simple yet effective switching policies. We show that
CACTUS achieves significant benefits in accuracy, latency, and compute budget
across a range of datasets and IoT platforms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19308">Free from Bellman Completeness: Trajectory Stitching via Model-based Return-conditioned Supervised Learning. (arXiv:2310.19308v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhaoyi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1">Chuning Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1">Runlong Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_Q/0/1/0/all/0/1">Qiwen Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1">Abhishek Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1">Simon Shaolei Du</a></p>
<p>Off-policy dynamic programming (DP) techniques such as $Q$-learning have
proven to be important in sequential decision-making problems. In the presence
of function approximation, however, these techniques often diverge due to the
absence of Bellman completeness in the function classes considered, a crucial
condition for the success of DP-based methods. In this paper, we show how
off-policy learning techniques based on return-conditioned supervised learning
(RCSL) are able to circumvent these challenges of Bellman completeness,
converging under significantly more relaxed assumptions inherited from
supervised learning. We prove there exists a natural environment in which if
one uses two-layer multilayer perceptron as the function approximator, the
layer width needs to grow linearly with the state space size to satisfy Bellman
completeness while a constant layer width is enough for RCSL. These findings
take a step towards explaining the superior empirical performance of RCSL
methods compared to DP-based methods in environments with near-optimal
datasets. Furthermore, in order to learn from sub-optimal datasets, we propose
a simple framework called MBRCSL, granting RCSL methods the ability of dynamic
programming to stitch together segments from distinct trajectories. MBRCSL
leverages learned dynamics models and forward sampling to accomplish trajectory
stitching while avoiding the need for Bellman completeness that plagues all
dynamic programming algorithms. We propose both theoretical analysis and
experimental evaluation to back these claims, outperforming state-of-the-art
model-free and model-based offline RL algorithms across several simulated
robotics problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08239">Learning Physics-Inspired Regularization for Medical Image Registration with Hypernetworks. (arXiv:2311.08239v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Reithmeir_A/0/1/0/all/0/1">Anna Reithmeir</a>, <a href="http://arxiv.org/find/eess/1/au:+Schnabel_J/0/1/0/all/0/1">Julia A. Schnabel</a>, <a href="http://arxiv.org/find/eess/1/au:+Zimmer_V/0/1/0/all/0/1">Veronika A. Zimmer</a></p>
<p>Medical image registration aims at identifying the spatial deformation
between images of the same anatomical region and is fundamental to image-based
diagnostics and therapy. To date, the majority of the deep learning-based
registration methods employ regularizers that enforce global spatial
smoothness, e.g., the diffusion regularizer. However, such regularizers are not
tailored to the data and might not be capable of reflecting the complex
underlying deformation. In contrast, physics-inspired regularizers promote
physically plausible deformations. One such regularizer is the linear elastic
regularizer which models the deformation of elastic material. These
regularizers are driven by parameters that define the material's physical
properties. For biological tissue, a wide range of estimations of such
parameters can be found in the literature and it remains an open challenge to
identify suitable parameter values for successful registration. To overcome
this problem and to incorporate physical properties into learning-based
registration, we propose to use a hypernetwork that learns the effect of the
physical parameters of a physics-inspired regularizer on the resulting spatial
deformation field. In particular, we adapt the HyperMorph framework to learn
the effect of the two elasticity parameters of the linear elastic regularizer.
Our approach enables the efficient discovery of suitable, data-specific
physical parameters at test time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10538">Testing Language Model Agents Safely in the Wild. (arXiv:2311.10538v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Naihin_S/0/1/0/all/0/1">Silen Naihin</a>, <a href="http://arxiv.org/find/cs/1/au:+Atkinson_D/0/1/0/all/0/1">David Atkinson</a>, <a href="http://arxiv.org/find/cs/1/au:+Green_M/0/1/0/all/0/1">Marc Green</a>, <a href="http://arxiv.org/find/cs/1/au:+Hamadi_M/0/1/0/all/0/1">Merwane Hamadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Swift_C/0/1/0/all/0/1">Craig Swift</a>, <a href="http://arxiv.org/find/cs/1/au:+Schonholtz_D/0/1/0/all/0/1">Douglas Schonholtz</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalai_A/0/1/0/all/0/1">Adam Tauman Kalai</a>, <a href="http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1">David Bau</a></p>
<p>A prerequisite for safe autonomy-in-the-wild is safe testing-in-the-wild. Yet
real-world autonomous tests face several unique safety challenges, both due to
the possibility of causing harm during a test, as well as the risk of
encountering new unsafe agent behavior through interactions with real-world and
potentially malicious actors. We propose a framework for conducting safe
autonomous agent tests on the open internet: agent actions are audited by a
context-sensitive monitor that enforces a stringent safety boundary to stop an
unsafe test, with suspect behavior ranked and logged to be examined by humans.
We design a basic safety monitor (AgentMonitor) that is flexible enough to
monitor existing LLM agents, and, using an adversarial simulated agent, we
measure its ability to identify and stop unsafe situations. Then we apply the
AgentMonitor on a battery of real-world tests of AutoGPT, and we identify
several limitations and challenges that will face the creation of safe
in-the-wild tests as autonomous agents grow more capable.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.11824">Neural Graph Collaborative Filtering Using Variational Inference. (arXiv:2311.11824v2 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dehkordi_N/0/1/0/all/0/1">Narges Sadat Fazeli Dehkordi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zare_H/0/1/0/all/0/1">Hadi Zare</a>, <a href="http://arxiv.org/find/cs/1/au:+Moradi_P/0/1/0/all/0/1">Parham Moradi</a>, <a href="http://arxiv.org/find/cs/1/au:+Jalili_M/0/1/0/all/0/1">Mahdi Jalili</a></p>
<p>The customization of recommended content to users holds significant
importance in enhancing user experiences across a wide spectrum of applications
such as e-commerce, music, and shopping. Graph-based methods have achieved
considerable performance by capturing user-item interactions. However, these
methods tend to utilize randomly constructed embeddings in the dataset used for
training the recommender, which lacks any user preferences. Here, we propose
the concept of variational embeddings as a means of pre-training the
recommender system to improve the feature propagation through the layers of
graph convolutional networks (GCNs). The graph variational embedding
collaborative filtering (GVECF) is introduced as a novel framework to
incorporate representations learned through a variational graph auto-encoder
which are embedded into a GCN-based collaborative filtering. This approach
effectively transforms latent high-order user-item interactions into more
trainable vectors, ultimately resulting in better performance in terms of
recall and normalized discounted cumulative gain(NDCG) metrics. The experiments
conducted on benchmark datasets demonstrate that our proposed method achieves
up to 13.78% improvement in the recall over the test data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12589">Improving Source-Free Target Adaptation with Vision Transformers Leveraging Domain Representation Images. (arXiv:2311.12589v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sawhney_G/0/1/0/all/0/1">Gauransh Sawhney</a>, <a href="http://arxiv.org/find/cs/1/au:+Dave_D/0/1/0/all/0/1">Daksh Dave</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1">Adeel Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jiechao Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Saleem_K/0/1/0/all/0/1">Khalid Saleem</a></p>
<p>Unsupervised Domain Adaptation (UDA) methods facilitate knowledge transfer
from a labeled source domain to an unlabeled target domain, navigating the
obstacle of domain shift. While Convolutional Neural Networks (CNNs) are a
staple in UDA, the rise of Vision Transformers (ViTs) provides new avenues for
domain generalization. This paper presents an innovative method to bolster ViT
performance in source-free target adaptation, beginning with an evaluation of
how key, query, and value elements affect ViT outcomes. Experiments indicate
that altering the key component has negligible effects on Transformer
performance. Leveraging this discovery, we introduce Domain Representation
Images (DRIs), feeding embeddings through the key element. DRIs act as
domain-specific markers, effortlessly merging with the training regimen. To
assess our method, we perform target adaptation tests on the Cross Instance DRI
source-only (SO) control. We measure the efficacy of target adaptation with and
without DRIs, against existing benchmarks like SHOT-B* and adaptations via
CDTrans. Findings demonstrate that excluding DRIs offers limited gains over
SHOT-B*, while their inclusion in the key segment boosts average precision
promoting superior domain generalization. This research underscores the vital
role of DRIs in enhancing ViT efficiency in UDA scenarios, setting a precedent
for further domain adaptation explorations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12713">Alpha Zero for Physics: Application of Symbolic Regression with Alpha Zero to find the analytical methods in physics. (arXiv:2311.12713v2 [physics.comp-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Michishita_Y/0/1/0/all/0/1">Yoshihiro Michishita</a></p>
<p>Machine learning with neural networks is now becoming a more and more
powerful tool for various tasks, such as natural language processing, image
recognition, winning the game, and even for the issues of physics. Although
there are many studies on the application of machine learning to numerical
calculation and assistance of experiments, the methods of applying machine
learning to find the analytical method are poorly studied. In this paper, we
propose the frameworks of developing analytical methods in physics by using the
symbolic regression with the Alpha Zero algorithm, that is Alpha Zero for
physics (AZfP). As a demonstration, we show that AZfP can derive the
high-frequency expansion in the Floquet systems. AZfP may have the possibility
of developing a new theoretical framework in physics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12919">SPOT! Revisiting Video-Language Models for Event Understanding. (arXiv:2311.12919v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Gengyuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1">Jinhe Bi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jindong Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yanyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1">Volker Tresp</a></p>
<p>Understanding videos is an important research topic for multimodal learning.
Leveraging large-scale datasets of web-crawled video-text pairs as weak
supervision has become a pre-training paradigm for learning joint
representations and showcased remarkable potential in video understanding
tasks. However, videos can be multi-event and multi-grained, while these
video-text pairs usually contain only broad-level video captions. This raises a
question: with such weak supervision, can video representation in
video-language models gain the ability to distinguish even factual
discrepancies in textual description and understand fine-grained events? To
address this, we introduce SPOT Prober, to benchmark existing video-language
models's capacities of distinguishing event-level discrepancies as an indicator
of models' event understanding ability. Our approach involves extracting events
as tuples (&lt;Subject, Predicate, Object, Attribute, Timestamps&gt;) from videos and
generating false event tuples by manipulating tuple components systematically.
We reevaluate the existing video-language models with these positive and
negative captions and find they fail to distinguish most of the manipulated
events. Based on our findings, we propose to plug in these manipulated event
captions as hard negative samples and find them effective in enhancing models
for event understanding.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13743">FinMem: A Performance-Enhanced LLM Trading Agent with Layered Memory and Character Design. (arXiv:2311.13743v2 [q-fin.CP] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+Yu_Y/0/1/0/all/0/1">Yangyang Yu</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Li_H/0/1/0/all/0/1">Haohang Li</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Chen_Z/0/1/0/all/0/1">Zhi Chen</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Jiang_Y/0/1/0/all/0/1">Yuechen Jiang</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Li_Y/0/1/0/all/0/1">Yang Li</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Zhang_D/0/1/0/all/0/1">Denghui Zhang</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Liu_R/0/1/0/all/0/1">Rong Liu</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Suchow_J/0/1/0/all/0/1">Jordan W. Suchow</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Khashanah_K/0/1/0/all/0/1">Khaldoun Khashanah</a></p>
<p>Recent advancements in Large Language Models (LLMs) have exhibited notable
efficacy in question-answering (QA) tasks across diverse domains. Their prowess
in integrating extensive web knowledge has fueled interest in developing
LLM-based autonomous agents. While LLMs are efficient in decoding human
instructions and deriving solutions by holistically processing historical
inputs, transitioning to purpose-driven agents requires a supplementary
rational architecture to process multi-source information, establish reasoning
chains, and prioritize critical tasks. Addressing this, we introduce
\textsc{FinMem}, a novel LLM-based agent framework devised for financial
decision-making. It encompasses three core modules: Profiling, to customize the
agent's characteristics; Memory, with layered message processing, to aid the
agent in assimilating hierarchical financial data; and Decision-making, to
convert insights gained from memories into investment decisions. Notably,
\textsc{FinMem}'s memory module aligns closely with the cognitive structure of
human traders, offering robust interpretability and real-time tuning. Its
adjustable cognitive span allows for the retention of critical information
beyond human perceptual limits, thereby enhancing trading outcomes. This
framework enables the agent to self-evolve its professional knowledge, react
agilely to new investment cues, and continuously refine trading decisions in
the volatile financial environment. We first compare \textsc{FinMem} with
various algorithmic agents on a scalable real-world financial dataset,
underscoring its leading trading performance in stocks. We then fine-tuned the
agent's perceptual span and character setting to achieve a significantly
enhanced trading performance. Collectively, \textsc{FinMem} presents a
cutting-edge LLM agent framework for automated trading, boosting cumulative
investment returns.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14648">Calibrated Language Models Must Hallucinate. (arXiv:2311.14648v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kalai_A/0/1/0/all/0/1">Adam Tauman Kalai</a>, <a href="http://arxiv.org/find/cs/1/au:+Vempala_S/0/1/0/all/0/1">Santosh S. Vempala</a></p>
<p>Recent language models generate false but plausible-sounding text with
surprising frequency. Such "hallucinations" are an obstacle to the usability of
language-based AI systems and can harm people who rely upon their outputs. This
work shows shows that there is an inherent statistical lower-bound on the rate
that pretrained language models hallucinate certain types of facts, having
nothing to do with the transformer LM architecture or data quality. For
"arbitrary" facts whose veracity cannot be determined from the training data,
we show that hallucinations must occur at a certain rate for language models
that satisfy a statistical calibration condition appropriate for generative
language models. Specifically, if the maximum probability of any fact is
bounded, we show that the probability of generating a hallucination is close to
the fraction of facts that occur exactly once in the training data (a
"Good-Turing" estimate), even assuming ideal training data without errors.
</p>
<p>One conclusion is that models pretrained to be sufficiently good predictors
(i.e., calibrated) may require post-training to mitigate hallucinations on the
type of arbitrary facts that tend to appear once in the training set. However,
our analysis also suggests that there is no statistical reason that pretraining
will lead to hallucination on facts that tend to appear more than once in the
training data (like references to publications such as articles and books,
whose hallucinations have been particularly notable and problematic) or on
systematic facts (like arithmetic calculations). Therefore, different
architectures and learning algorithms may mitigate these latter types of
hallucinations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15112">Everybody Needs a Little HELP: Explaining Graphs via Hierarchical Concepts. (arXiv:2311.15112v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jurss_J/0/1/0/all/0/1">Jonas J&#xfc;r&#xdf;</a>, <a href="http://arxiv.org/find/cs/1/au:+Magister_L/0/1/0/all/0/1">Lucie Charlotte Magister</a>, <a href="http://arxiv.org/find/cs/1/au:+Barbiero_P/0/1/0/all/0/1">Pietro Barbiero</a>, <a href="http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1">Pietro Li&#xf2;</a>, <a href="http://arxiv.org/find/cs/1/au:+Simidjievski_N/0/1/0/all/0/1">Nikola Simidjievski</a></p>
<p>Graph neural networks (GNNs) have led to major breakthroughs in a variety of
domains such as drug discovery, social network analysis, and travel time
estimation. However, they lack interpretability which hinders human trust and
thereby deployment to settings with high-stakes decisions. A line of
interpretable methods approach this by discovering a small set of relevant
concepts as subgraphs in the last GNN layer that together explain the
prediction. This can yield oversimplified explanations, failing to explain the
interaction between GNN layers. To address this oversight, we provide HELP
(Hierarchical Explainable Latent Pooling), a novel, inherently interpretable
graph pooling approach that reveals how concepts from different GNN layers
compose to new ones in later steps. HELP is more than 1-WL expressive and is
the first non-spectral, end-to-end-learnable, hierarchical graph pooling method
that can learn to pool a variable number of arbitrary connected components. We
empirically demonstrate that it performs on-par with standard GCNs and popular
pooling methods in terms of accuracy while yielding explanations that are
aligned with expert knowledge in the domains of chemistry and social networks.
In addition to a qualitative analysis, we employ concept completeness scores as
well as concept conformity, a novel metric to measure the noise in discovered
concepts, quantitatively verifying that the discovered concepts are
significantly easier to fully understand than those from previous work. Our
work represents a first step towards an understanding of graph neural networks
that goes beyond a set of concepts from the final layer and instead explains
the complex interplay of concepts on different levels.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15209">See and Think: Embodied Agent in Virtual Environment. (arXiv:2311.15209v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zhonghan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chai_W/0/1/0/all/0/1">Wenhao Chai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Boyi_L/0/1/0/all/0/1">Li Boyi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hao_S/0/1/0/all/0/1">Shengyu Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1">Shidong Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1">Tian Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1">Jenq-Neng Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Gaoang Wang</a></p>
<p>Large language models (LLMs) have achieved impressive progress on several
open-world tasks. Recently, using LLMs to build embodied agents has been a
hotspot. In this paper, we propose STEVE, a comprehensive and visionary
embodied agent in the Minecraft virtual environment. STEVE consists of three
key components: vision perception, language instruction, and code action.
Vision perception involves the interpretation of visual information in the
environment, which is then integrated into the LLMs component with agent state
and task instruction. Language instruction is responsible for iterative
reasoning and decomposing complex tasks into manageable guidelines. Code action
generates executable skill actions based on retrieval in skill database,
enabling the agent to interact effectively within the Minecraft environment. We
also collect STEVE-21K dataset, which includes 600$+$ vision-environment pairs,
20K knowledge question-answering pairs, and 200$+$ skill-code pairs. We conduct
continuous block search, knowledge question and answering, and tech tree
mastery to evaluate the performance. Extensive experiments show that STEVE
achieves at most $1.5 \times$ faster unlocking key tech trees and $2.5 \times$
quicker in block search tasks compared to previous state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15565">Evaluating the Efficacy of Hybrid Deep Learning Models in Distinguishing AI-Generated Text. (arXiv:2311.15565v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oketunji_F/0/1/0/all/0/1">Finbarrs Oketunji</a></p>
<p>My research investigates the use of cutting-edge hybrid deep learning models
to accurately differentiate between AI-generated text and human writing. I
applied a robust methodology, utilising a carefully selected dataset comprising
AI and human texts from various sources, each tagged with instructions.
Advanced natural language processing techniques facilitated the analysis of
textual features. Combining sophisticated neural networks, the custom model
enabled it to detect nuanced differences between AI and human content.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15786">YUAN 2.0: A Large Language Model with Localized Filtering-based Attention. (arXiv:2311.15786v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Shaohua Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xudong Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shenling Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1">Jiangang Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lingjun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1">Bing Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1">Tong Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Rongguo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiahua Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chao Wang</a></p>
<p>In this work, we develop and release Yuan 2.0, a series of large language
models with parameters ranging from 2.1 billion to 102.6 billion. The Localized
Filtering-based Attention (LFA) is introduced to incorporate prior knowledge of
local dependencies of natural language into Attention. A data filtering and
generating system is presented to build pre-training and fine-tuning dataset in
high quality. A distributed training method with non-uniform pipeline parallel,
data parallel, and optimizer parallel is proposed, which greatly reduces the
bandwidth requirements of intra-node communication, and achieves good
performance in large-scale distributed training. Yuan 2.0 models display
impressive ability in code generation, math problem-solving, and chatting
compared with existing models. The latest version of YUAN 2.0, including model
weights and source code, is accessible at Github.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16450">Typhoon Intensity Prediction with Vision Transformer. (arXiv:2311.16450v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huanxin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1">Pengshuai Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Huichou Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1">Qingyao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1">Ruirui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiatian Zhu</a></p>
<p>Predicting typhoon intensity accurately across space and time is crucial for
issuing timely disaster warnings and facilitating emergency response. This has
vast potential for minimizing life losses and property damages as well as
reducing economic and environmental impacts. Leveraging satellite imagery for
scenario analysis is effective but also introduces additional challenges due to
the complex relations among clouds and the highly dynamic context. Existing
deep learning methods in this domain rely on convolutional neural networks
(CNNs), which suffer from limited per-layer receptive fields. This limitation
hinders their ability to capture long-range dependencies and global contextual
knowledge during inference. In response, we introduce a novel approach, namely
"Typhoon Intensity Transformer" (Tint), which leverages self-attention
mechanisms with global receptive fields per layer. Tint adopts a
sequence-to-sequence feature representation learning perspective. It begins by
cutting a given satellite image into a sequence of patches and recursively
employs self-attention operations to extract both local and global contextual
relations between all patch pairs simultaneously, thereby enhancing per-patch
feature representation learning. Extensive experiments on a publicly available
typhoon benchmark validate the efficacy of Tint in comparison with both
state-of-the-art deep learning and conventional meteorological methods. Our
code is available at https://github.com/chen-huanxin/Tint.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17431">Grounding Foundation Models through Federated Transfer Learning: A General Framework. (arXiv:2311.17431v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1">Yan Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_T/0/1/0/all/0/1">Tao Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_H/0/1/0/all/0/1">Hanlin Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1">Lixin Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1">Qiang Yang</a></p>
<p>Foundation Models (FMs) such as GPT-4 encoded with vast knowledge and
powerful emergent abilities have achieved remarkable success in various natural
language processing and computer vision tasks. Grounding FMs by adapting them
to domain-specific tasks or augmenting them with domain-specific knowledge
enables us to exploit the full potential of FMs. However, grounding FMs faces
several challenges, stemming primarily from constrained computing resources,
data privacy, model heterogeneity, and model ownership. Federated Transfer
Learning (FTL), the combination of federated learning and transfer learning,
provides promising solutions to address these challenges. In recent years, the
need for grounding FMs leveraging FTL, coined FTL-FM, has arisen strongly in
both academia and industry. Motivated by the strong growth in FTL-FM research
and the potential impact of FTL-FM on industrial applications, we propose an
FTL-FM framework that formulates problems of grounding FMs in the federated
learning setting, construct a detailed taxonomy based on the FTL-FM framework
to categorize state-of-the-art FTL-FM works, and comprehensively overview
FTL-FM works based on the proposed taxonomy. We also establish correspondences
between FTL-FM and conventional phases of adapting FM so that FM practitioners
can align their research works with FTL-FM. In addition, we overview advanced
efficiency-improving and privacy-preserving techniques because efficiency and
privacy are critical concerns in FTL-FM. Last, we discuss opportunities and
future research directions of FTL-FM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18206">SCOPE-RL: A Python Library for Offline Reinforcement Learning and Off-Policy Evaluation. (arXiv:2311.18206v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kiyohara_H/0/1/0/all/0/1">Haruka Kiyohara</a>, <a href="http://arxiv.org/find/cs/1/au:+Kishimoto_R/0/1/0/all/0/1">Ren Kishimoto</a>, <a href="http://arxiv.org/find/cs/1/au:+Kawakami_K/0/1/0/all/0/1">Kosuke Kawakami</a>, <a href="http://arxiv.org/find/cs/1/au:+Kobayashi_K/0/1/0/all/0/1">Ken Kobayashi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nakata_K/0/1/0/all/0/1">Kazuhide Nakata</a>, <a href="http://arxiv.org/find/cs/1/au:+Saito_Y/0/1/0/all/0/1">Yuta Saito</a></p>
<p>This paper introduces SCOPE-RL, a comprehensive open-source Python software
designed for offline reinforcement learning (offline RL), off-policy evaluation
(OPE), and selection (OPS). Unlike most existing libraries that focus solely on
either policy learning or evaluation, SCOPE-RL seamlessly integrates these two
key aspects, facilitating flexible and complete implementations of both offline
RL and OPE processes. SCOPE-RL put particular emphasis on its OPE modules,
offering a range of OPE estimators and robust evaluation-of-OPE protocols. This
approach enables more in-depth and reliable OPE compared to other packages. For
instance, SCOPE-RL enhances OPE by estimating the entire reward distribution
under a policy rather than its mere point-wise expected value. Additionally,
SCOPE-RL provides a more thorough evaluation-of-OPE by presenting the
risk-return tradeoff in OPE results, extending beyond mere accuracy evaluations
in existing OPE literature. SCOPE-RL is designed with user accessibility in
mind. Its user-friendly APIs, comprehensive documentation, and a variety of
easy-to-follow examples assist researchers and practitioners in efficiently
implementing and experimenting with various offline RL methods and OPE
estimators, tailored to their specific problem contexts. The documentation of
SCOPE-RL is available at https://scope-rl.readthedocs.io/en/latest/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18207">Towards Assessing and Benchmarking Risk-Return Tradeoff of Off-Policy Evaluation. (arXiv:2311.18207v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kiyohara_H/0/1/0/all/0/1">Haruka Kiyohara</a>, <a href="http://arxiv.org/find/cs/1/au:+Kishimoto_R/0/1/0/all/0/1">Ren Kishimoto</a>, <a href="http://arxiv.org/find/cs/1/au:+Kawakami_K/0/1/0/all/0/1">Kosuke Kawakami</a>, <a href="http://arxiv.org/find/cs/1/au:+Kobayashi_K/0/1/0/all/0/1">Ken Kobayashi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nakata_K/0/1/0/all/0/1">Kazuhide Nakata</a>, <a href="http://arxiv.org/find/cs/1/au:+Saito_Y/0/1/0/all/0/1">Yuta Saito</a></p>
<p>Off-Policy Evaluation (OPE) aims to assess the effectiveness of
counterfactual policies using only offline logged data and is often used to
identify the top-k promising policies for deployment in online A/B tests.
Existing evaluation metrics for OPE estimators primarily focus on the
"accuracy" of OPE or that of downstream policy selection, neglecting
risk-return tradeoff in the subsequent online policy deployment. To address
this issue, we draw inspiration from portfolio evaluation in finance and
develop a new metric, called SharpeRatio@k, which measures the risk-return
tradeoff of policy portfolios formed by an OPE estimator under varying online
evaluation budgets (k). We validate our metric in two example scenarios,
demonstrating its ability to effectively distinguish between low-risk and
high-risk estimators and to accurately identify the most efficient estimator.
This efficient estimator is characterized by its capability to form the most
advantageous policy portfolios, maximizing returns while minimizing risks
during online deployment, a nuance that existing metrics typically overlook. To
facilitate a quick, accurate, and consistent evaluation of OPE via
SharpeRatio@k, we have also integrated this metric into an open-source
software, SCOPE-RL. Employing SharpeRatio@k and SCOPE-RL, we conduct
comprehensive benchmarking experiments on various estimators and RL tasks,
focusing on their risk-return tradeoff. These experiments offer several
interesting directions and suggestions for future OPE research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18743">AlignBench: Benchmarking Chinese Alignment of Large Language Models. (arXiv:2311.18743v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_X/0/1/0/all/0/1">Xuanyu Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shengyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yue Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1">Zhuoer Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1">Bosi Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1">Jiale Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ke_P/0/1/0/all/0/1">Pei Ke</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yifan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tam_W/0/1/0/all/0/1">Weng Lam Tam</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaohan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Lichao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hongning Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1">Minlie Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1">Yuxiao Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jie Tang</a></p>
<p>Alignment has become a critical step for instruction-tuned Large Language
Models (LLMs) to become helpful assistants. However, effective evaluation of
alignment for emerging Chinese LLMs is still significantly lacking, calling for
real-scenario grounded, open-ended, challenging and automatic evaluations
tailored for alignment. To fill in this gap, we introduce AlignBench, a
comprehensive multi-dimensional benchmark for evaluating LLMs' alignment in
Chinese. Equipped with a human-in-the-loop data curation pipeline, our
benchmark employs a rule-calibrated multi-dimensional LLM-as-Judge with
Chain-of-Thought to generate explanations and final ratings as evaluations,
ensuring high reliability and interpretability. Furthermore, we report
AlignBench evaluated by CritiqueLLM, a dedicated Chinese evaluator LLM that
recovers 95% of GPT-4's evaluation ability. We will provide public APIs for
evaluating AlignBench with CritiqueLLM to facilitate the evaluation of LLMs'
Chinese alignment. All evaluation codes, data, and LLM generations are
available at \url{https://github.com/THUDM/AlignBench}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00102">FedEmb: A Vertical and Hybrid Federated Learning Algorithm using Network And Feature Embedding Aggregation. (arXiv:2312.00102v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1">Fanfei Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lele Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuxin Wang</a></p>
<p>Federated learning (FL) is an emerging paradigm for decentralized training of
machine learning models on distributed clients, without revealing the data to
the central server. The learning scheme may be horizontal, vertical or hybrid
(both vertical and horizontal). Most existing research work with deep neural
network (DNN) modelling is focused on horizontal data distributions, while
vertical and hybrid schemes are much less studied. In this paper, we propose a
generalized algorithm FedEmb, for modelling vertical and hybrid DNN-based
learning. The idea of our algorithm is characterised by higher inference
accuracy, stronger privacy-preserving properties, and lower client-server
communication bandwidth demands as compared with existing work. The
experimental results show that FedEmb is an effective method to tackle both
split feature &amp; subject space decentralized problems, shows 0.3% to 4.2%
inference accuracy improvement with limited privacy revealing for datasets
stored in local clients, and reduces 88.9 % time complexity over vertical
baseline method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00656">Simple Transferability Estimation for Regression Tasks. (arXiv:2312.00656v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1">Cuong N. Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_P/0/1/0/all/0/1">Phong Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Ho_L/0/1/0/all/0/1">Lam Si Tung Ho</a>, <a href="http://arxiv.org/find/cs/1/au:+Dinh_V/0/1/0/all/0/1">Vu Dinh</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_A/0/1/0/all/0/1">Anh T. Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Hassner_T/0/1/0/all/0/1">Tal Hassner</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1">Cuong V. Nguyen</a></p>
<p>We consider transferability estimation, the problem of estimating how well
deep learning models transfer from a source to a target task. We focus on
regression tasks, which received little previous attention, and propose two
simple and computationally efficient approaches that estimate transferability
based on the negative regularized mean squared error of a linear regression
model. We prove novel theoretical results connecting our approaches to the
actual transferability of the optimal target models obtained from the transfer
learning process. Despite their simplicity, our approaches significantly
outperform existing state-of-the-art regression transferability estimators in
both accuracy and efficiency. On two large-scale keypoint regression
benchmarks, our approaches yield 12% to 36% better results on average while
being at least 27% faster than previous state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00761">Deep Unlearning: Fast and Efficient Training-free Approach to Controlled Forgetting. (arXiv:2312.00761v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kodge_S/0/1/0/all/0/1">Sangamesh Kodge</a>, <a href="http://arxiv.org/find/cs/1/au:+Saha_G/0/1/0/all/0/1">Gobinda Saha</a>, <a href="http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1">Kaushik Roy</a></p>
<p>Machine unlearning has emerged as a prominent and challenging area of
interest, driven in large part by the rising regulatory demands for industries
to delete user data upon request and the heightened awareness of privacy.
Existing approaches either retrain models from scratch or use several
finetuning steps for every deletion request, often constrained by computational
resource limitations and restricted access to the original training data. In
this work, we introduce a novel class unlearning algorithm designed to
strategically eliminate an entire class or a group of classes from the learned
model. To that end, our algorithm first estimates the Retain Space and the
Forget Space, representing the feature or activation spaces for samples from
classes to be retained and unlearned, respectively. To obtain these spaces, we
propose a novel singular value decomposition-based technique that requires
layer wise collection of network activations from a few forward passes through
the network. We then compute the shared information between these spaces and
remove it from the forget space to isolate class-discriminatory feature space
for unlearning. Finally, we project the model weights in the orthogonal
direction of the class-discriminatory space to obtain the unlearned model. We
demonstrate our algorithm's efficacy on ImageNet using a Vision Transformer
with only $\sim$1.5% drop in retain accuracy compared to the original model
while maintaining under 1% accuracy on the unlearned class samples. Further,
our algorithm consistently performs well when subject to Membership Inference
Attacks showing 7.8% improvement on average across a variety of image
classification datasets and network architectures, as compared to other
baselines while being $\sim$6x more computationally efficient.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/1812.05227">Deep Learning Framework for Wireless Systems: Applications to Optical Wireless Communications. (arXiv:1812.05227v1 [cs.IT] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hoon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Sang Hyun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Quek_T/0/1/0/all/0/1">Tony Q. S. Quek</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1">Inkyu Lee</a></p>
<p>Optical wireless communication (OWC) is a promising technology for future
wireless communications owing to its potentials for cost-effective network
deployment and high data rate. There are several implementation issues in the
OWC which have not been encountered in radio frequency wireless communications.
First, practical OWC transmitters need an illumination control on color,
intensity, and luminance, etc., which poses complicated modulation design
challenges. Furthermore, signal-dependent properties of optical channels raise
non-trivial challenges both in modulation and demodulation of the optical
signals. To tackle such difficulties, deep learning (DL) technologies can be
applied for optical wireless transceiver design. This article addresses recent
efforts on DL-based OWC system designs. A DL framework for emerging image
sensor communication is proposed and its feasibility is verified by simulation.
Finally, technical challenges and implementation issues for the DL-based
optical wireless technology are discussed.
</p>
</p>
</div>

    </div>
    </body>
    