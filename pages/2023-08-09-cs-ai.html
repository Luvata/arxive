<!DOCTYPE html>
<html>
<head>
<title>2023-08-09-cs-ai</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2308.03764">Deployment of Leader-Follower Automated Vehicle Systems for Smart Work Zone Applications with a Queuing-based Traffic Assignment Approach. (arXiv:2308.03764v1 [eess.SY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Tang_Q/0/1/0/all/0/1">Qing Tang</a>, <a href="http://arxiv.org/find/eess/1/au:+Hu_X/0/1/0/all/0/1">Xianbiao Hu</a></p>
<p>The emerging technology of the Autonomous Truck Mounted Attenuator (ATMA), a
leader-follower style vehicle system, utilizes connected and automated vehicle
capabilities to enhance safety during transportation infrastructure maintenance
in work zones. However, the speed difference between ATMA vehicles and general
vehicles creates a moving bottleneck that reduces capacity and increases queue
length, resulting in additional delays. The different routes taken by ATMA
cause diverse patterns of time-varying capacity drops, which may affect the
user equilibrium traffic assignment and lead to different system costs. This
manuscript focuses on optimizing the routing for ATMA vehicles in a network to
minimize the system cost associated with the slow-moving operation.
</p>
<p>To achieve this, a queuing-based traffic assignment approach is proposed to
identify the system cost caused by the ATMA system. A queuing-based
time-dependent (QBTD) travel time function, considering capacity drop, is
introduced and applied in the static user equilibrium traffic assignment
problem, with a result of adding dynamic characteristics. Subsequently, we
formulate the queuing-based traffic assignment problem and solve it using a
modified path-based algorithm. The methodology is validated using a small-size
and a large-size network and compared with two benchmark models to analyze the
benefit of capacity drop modeling and QBTD travel time function. Furthermore,
the approach is applied to quantify the impact of different routes on the
traffic system and identify an optimal route for ATMA vehicles performing
maintenance work. Finally, sensitivity analysis is conducted to explore how the
impact changes with variations in traffic demand and capacity reduction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03766">AMaizeD: An End to End Pipeline for Automatic Maize Disease Detection. (arXiv:2308.03766v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mall_A/0/1/0/all/0/1">Anish Mall</a>, <a href="http://arxiv.org/find/cs/1/au:+Kabra_S/0/1/0/all/0/1">Sanchit Kabra</a>, <a href="http://arxiv.org/find/cs/1/au:+Lhila_A/0/1/0/all/0/1">Ankur Lhila</a>, <a href="http://arxiv.org/find/cs/1/au:+Ajmera_P/0/1/0/all/0/1">Pawan Ajmera</a></p>
<p>This research paper presents AMaizeD: An End to End Pipeline for Automatic
Maize Disease Detection, an automated framework for early detection of diseases
in maize crops using multispectral imagery obtained from drones. A custom
hand-collected dataset focusing specifically on maize crops was meticulously
gathered by expert researchers and agronomists. The dataset encompasses a
diverse range of maize varieties, cultivation practices, and environmental
conditions, capturing various stages of maize growth and disease progression.
By leveraging multispectral imagery, the framework benefits from improved
spectral resolution and increased sensitivity to subtle changes in plant
health. The proposed framework employs a combination of convolutional neural
networks (CNNs) as feature extractors and segmentation techniques to identify
both the maize plants and their associated diseases. Experimental results
demonstrate the effectiveness of the framework in detecting a range of maize
diseases, including powdery mildew, anthracnose, and leaf blight. The framework
achieves state-of-the-art performance on the custom hand-collected dataset and
contributes to the field of automated disease detection in agriculture,
offering a practical solution for early identification of diseases in maize
crops advanced machine learning techniques and deep learning architectures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03767">Enhancing image captioning with depth information using a Transformer-based framework. (arXiv:2308.03767v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1">Aya Mahmoud Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Yousef_M/0/1/0/all/0/1">Mohamed Yousef</a>, <a href="http://arxiv.org/find/cs/1/au:+Hussain_K/0/1/0/all/0/1">Khaled F. Hussain</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahdy_Y/0/1/0/all/0/1">Yousef Bassyouni Mahdy</a></p>
<p>Captioning images is a challenging scene-understanding task that connects
computer vision and natural language processing. While image captioning models
have been successful in producing excellent descriptions, the field has
primarily focused on generating a single sentence for 2D images. This paper
investigates whether integrating depth information with RGB images can enhance
the captioning task and generate better descriptions. For this purpose, we
propose a Transformer-based encoder-decoder framework for generating a
multi-sentence description of a 3D scene. The RGB image and its corresponding
depth map are provided as inputs to our framework, which combines them to
produce a better understanding of the input scene. Depth maps could be ground
truth or estimated, which makes our framework widely applicable to any RGB
captioning dataset. We explored different fusion approaches to fuse RGB and
depth images. The experiments are performed on the NYU-v2 dataset and the
Stanford image paragraph captioning dataset. During our work with the NYU-v2
dataset, we found inconsistent labeling that prevents the benefit of using
depth information to enhance the captioning task. The results were even worse
than using RGB images only. As a result, we propose a cleaned version of the
NYU-v2 dataset that is more consistent and informative. Our results on both
datasets demonstrate that the proposed framework effectively benefits from
depth information, whether it is ground truth or estimated, and generates
better captions. Code, pre-trained models, and the cleaned version of the
NYU-v2 dataset will be made publically available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03769">Towards Integrated Traffic Control with Operating Decentralized Autonomous Organization. (arXiv:2308.03769v1 [eess.SY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Yao_S/0/1/0/all/0/1">Shengyue Yao</a>, <a href="http://arxiv.org/find/eess/1/au:+Yu_J/0/1/0/all/0/1">Jingru Yu</a>, <a href="http://arxiv.org/find/eess/1/au:+Yu_Y/0/1/0/all/0/1">Yi Yu</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1">Jia Xu</a>, <a href="http://arxiv.org/find/eess/1/au:+Dai_X/0/1/0/all/0/1">Xingyuan Dai</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1">Honghai Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_F/0/1/0/all/0/1">Fei-Yue Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Lin_Y/0/1/0/all/0/1">Yilun Lin</a></p>
<p>With a growing complexity of the intelligent traffic system (ITS), an
integrated control of ITS that is capable of considering plentiful
heterogeneous intelligent agents is desired. However, existing control methods
based on the centralized or the decentralized scheme have not presented their
competencies in considering the optimality and the scalability simultaneously.
To address this issue, we propose an integrated control method based on the
framework of Decentralized Autonomous Organization (DAO). The proposed method
achieves a global consensus on energy consumption efficiency (ECE), meanwhile
to optimize the local objectives of all involved intelligent agents, through a
consensus and incentive mechanism. Furthermore, an operation algorithm is
proposed regarding the issue of structural rigidity in DAO. Specifically, the
proposed operation approach identifies critical agents to execute the smart
contract in DAO, which ultimately extends the capability of DAO-based control.
In addition, a numerical experiment is designed to examine the performance of
the proposed method. The experiment results indicate that the controlled agents
can achieve a consensus faster on the global objective with improved local
objectives by the proposed method, compare to existing decentralized control
methods. In general, the proposed method shows a great potential in developing
an integrated control system in the ITS
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03770">Visual Saliency Detection in Advanced Driver Assistance Systems. (arXiv:2308.03770v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rundo_F/0/1/0/all/0/1">Francesco Rundo</a>, <a href="http://arxiv.org/find/cs/1/au:+Rundo_M/0/1/0/all/0/1">Michael Sebastian Rundo</a>, <a href="http://arxiv.org/find/cs/1/au:+Spampinato_C/0/1/0/all/0/1">Concetto Spampinato</a></p>
<p>Visual Saliency refers to the innate human mechanism of focusing on and
extracting important features from the observed environment. Recently, there
has been a notable surge of interest in the field of automotive research
regarding the estimation of visual saliency. While operating a vehicle, drivers
naturally direct their attention towards specific objects, employing
brain-driven saliency mechanisms that prioritize certain elements over others.
In this investigation, we present an intelligent system that combines a
drowsiness detection system for drivers with a scene comprehension pipeline
based on saliency. To achieve this, we have implemented a specialized 3D deep
network for semantic segmentation, which has been pretrained and tailored for
processing the frames captured by an automotive-grade external camera. The
proposed pipeline was hosted on an embedded platform utilizing the STA1295
core, featuring ARM A7 dual-cores, and embeds an hardware accelerator.
Additionally, we employ an innovative biosensor embedded on the car steering
wheel to monitor the driver drowsiness, gathering the PhotoPlethysmoGraphy
(PPG) signal of the driver. A dedicated 1D temporal deep convolutional network
has been devised to classify the collected PPG time-series, enabling us to
assess the driver level of attentiveness. Ultimately, we compare the determined
attention level of the driver with the corresponding saliency-based scene
classification to evaluate the overall safety level. The efficacy of the
proposed pipeline has been validated through extensive experimental results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03772">Improved Neural Radiance Fields Using Pseudo-depth and Fusion. (arXiv:2308.03772v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jingliang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1">Qiang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1">Chaohui Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Zhengda Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1">Jun Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhibin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Fan Wang</a></p>
<p>Since the advent of Neural Radiance Fields, novel view synthesis has received
tremendous attention. The existing approach for the generalization of radiance
field reconstruction primarily constructs an encoding volume from nearby source
images as additional inputs. However, these approaches cannot efficiently
encode the geometric information of real scenes with various scale
objects/structures. In this work, we propose constructing multi-scale encoding
volumes and providing multi-scale geometry information to NeRF models. To make
the constructed volumes as close as possible to the surfaces of objects in the
scene and the rendered depth more accurate, we propose to perform depth
prediction and radiance field reconstruction simultaneously. The predicted
depth map will be used to supervise the rendered depth, narrow the depth range,
and guide points sampling. Finally, the geometric information contained in
point volume features may be inaccurate due to occlusion, lighting, etc. To
this end, we propose enhancing the point volume feature from depth-guided
neighbor feature fusion. Experiments demonstrate the superior performance of
our method in both novel view synthesis and dense geometry modeling without
per-scene optimization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03773">Goodness-of-Fit of Attributed Probabilistic Graph Generative Models. (arXiv:2308.03773v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Robles_Granda_P/0/1/0/all/0/1">Pablo Robles-Granda</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsai_K/0/1/0/all/0/1">Katherine Tsai</a>, <a href="http://arxiv.org/find/cs/1/au:+Koyejo_O/0/1/0/all/0/1">Oluwasanmi Koyejo</a></p>
<p>Probabilistic generative models of graphs are important tools that enable
representation and sampling. Many recent works have created probabilistic
models of graphs that are capable of representing not only entity interactions
but also their attributes. However, given a generative model of random
attributed graph(s), the general conditions that establish goodness of fit are
not clear a-priori. In this paper, we define goodness of fit in terms of the
mean square contingency coefficient for random binary networks. For this
statistic, we outline a procedure for assessing the quality of the structure of
a learned attributed graph by ensuring that the discrepancy of the mean square
contingency coefficient (constant, or random) is minimal with high probability.
We apply these criteria to verify the representation capability of a
probabilistic generative model for various popular types of graph models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03782">Bio+Clinical BERT, BERT Base, and CNN Performance Comparison for Predicting Drug-Review Satisfaction. (arXiv:2308.03782v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ling_Y/0/1/0/all/0/1">Yue Ling</a></p>
<p>The objective of this study is to develop natural language processing (NLP)
models that can analyze patients' drug reviews and accurately classify their
satisfaction levels as positive, neutral, or negative. Such models would reduce
the workload of healthcare professionals and provide greater insight into
patients' quality of life, which is a critical indicator of treatment
effectiveness. To achieve this, we implemented and evaluated several
classification models, including a BERT base model, Bio+Clinical BERT, and a
simpler CNN. Results indicate that the medical domain-specific Bio+Clinical
BERT model significantly outperformed the general domain base BERT model,
achieving macro f1 and recall score improvement of 11%, as shown in Table 2.
Future research could explore how to capitalize on the specific strengths of
each model. Bio+Clinical BERT excels in overall performance, particularly with
medical jargon, while the simpler CNN demonstrates the ability to identify
crucial words and accurately classify sentiment in texts with conflicting
sentiments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03789">Semantic Channel Equalizer: Modelling Language Mismatch in Multi-User Semantic Communications. (arXiv:2308.03789v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sana_M/0/1/0/all/0/1">Mohamed Sana</a>, <a href="http://arxiv.org/find/cs/1/au:+Strinati_E/0/1/0/all/0/1">Emilio Calvanese Strinati</a></p>
<p>We consider a multi-user semantic communications system in which agents
(transmitters and receivers) interact through the exchange of semantic messages
to convey meanings. In this context, languages are instrumental in structuring
the construction and consolidation of knowledge, influencing conceptual
representation and semantic extraction and interpretation. Yet, the crucial
role of languages in semantic communications is often overlooked. When this is
not the case, agent languages are assumed compatible and unambiguously
interoperable, ignoring practical limitations that may arise due to language
mismatching. This is the focus of this work. When agents use distinct
languages, message interpretation is prone to semantic noise resulting from
critical distortion introduced by semantic channels. To address this problem,
this paper proposes a new semantic channel equalizer to counteract and limit
the critical ambiguity in message interpretation. Our proposed solution models
the mismatch of languages with measurable transformations over semantic
representation spaces. We achieve this using optimal transport theory, where we
model such transformations as transportation maps. Then, to recover at the
receiver the meaning intended by the teacher we operate semantic equalization
to compensate for the transformation introduced by the semantic channel, either
before transmission and/or after the reception of semantic messages. We
implement the proposed approach as an operation over a codebook of
transformations specifically designed for successful communication. Numerical
results show that the proposed semantic channel equalizer outperforms
traditional approaches in terms of operational complexity and transmission
accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03800">Textual Data Mining for Financial Fraud Detection: A Deep Learning Approach. (arXiv:2308.03800v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qiuru Li</a></p>
<p>In this report, I present a deep learning approach to conduct a natural
language processing (hereafter NLP) binary classification task for analyzing
financial-fraud texts. First, I searched for regulatory announcements and
enforcement bulletins from HKEX news to define fraudulent companies and to
extract their MD&amp;A reports before I organized the sentences from the reports
with labels and reporting time. My methodology involved different kinds of
neural network models, including Multilayer Perceptrons with Embedding layers,
vanilla Recurrent Neural Network (RNN), Long-Short Term Memory (LSTM), and
Gated Recurrent Unit (GRU) for the text classification task. By utilizing this
diverse set of models, I aim to perform a comprehensive comparison of their
accuracy in detecting financial fraud. My results bring significant
implications for financial fraud detection as this work contributes to the
growing body of research at the intersection of deep learning, NLP, and
finance, providing valuable insights for industry practitioners, regulators,
and researchers in the pursuit of more robust and effective fraud detection
methodologies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03805">Weakly Supervised Multi-Task Representation Learning for Human Activity Analysis Using Wearables. (arXiv:2308.03805v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sheng_T/0/1/0/all/0/1">Taoran Sheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Huber_M/0/1/0/all/0/1">Manfred Huber</a></p>
<p>Sensor data streams from wearable devices and smart environments are widely
studied in areas like human activity recognition (HAR), person identification,
or health monitoring. However, most of the previous works in activity and
sensor stream analysis have been focusing on one aspect of the data, e.g. only
recognizing the type of the activity or only identifying the person who
performed the activity. We instead propose an approach that uses a weakly
supervised multi-output siamese network that learns to map the data into
multiple representation spaces, where each representation space focuses on one
aspect of the data. The representation vectors of the data samples are
positioned in the space such that the data with the same semantic meaning in
that aspect are closely located to each other. Therefore, as demonstrated with
a set of experiments, the trained model can provide metrics for clustering data
based on multiple aspects, allowing it to address multiple tasks simultaneously
and even to outperform single task supervised methods in many situations. In
addition, further experiments are presented that in more detail analyze the
effect of the architecture and of using multiple tasks within this framework,
that investigate the scalability of the model to include additional tasks, and
that demonstrate the ability of the framework to combine data for which only
partial relationship information with respect to the target tasks is available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03813">High-Resolution Cranial Defect Reconstruction by Iterative, Low-Resolution, Point Cloud Completion Transformers. (arXiv:2308.03813v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Wodzinski_M/0/1/0/all/0/1">Marek Wodzinski</a>, <a href="http://arxiv.org/find/eess/1/au:+Daniol_M/0/1/0/all/0/1">Mateusz Daniol</a>, <a href="http://arxiv.org/find/eess/1/au:+Hemmerling_D/0/1/0/all/0/1">Daria Hemmerling</a>, <a href="http://arxiv.org/find/eess/1/au:+Socha_M/0/1/0/all/0/1">Miroslaw Socha</a></p>
<p>Each year thousands of people suffer from various types of cranial injuries
and require personalized implants whose manual design is expensive and
time-consuming. Therefore, an automatic, dedicated system to increase the
availability of personalized cranial reconstruction is highly desirable. The
problem of the automatic cranial defect reconstruction can be formulated as the
shape completion task and solved using dedicated deep networks. Currently, the
most common approach is to use the volumetric representation and apply deep
networks dedicated to image segmentation. However, this approach has several
limitations and does not scale well into high-resolution volumes, nor takes
into account the data sparsity. In our work, we reformulate the problem into a
point cloud completion task. We propose an iterative, transformer-based method
to reconstruct the cranial defect at any resolution while also being fast and
resource-efficient during training and inference. We compare the proposed
methods to the state-of-the-art volumetric approaches and show superior
performance in terms of GPU memory consumption while maintaining high-quality
of the reconstructed defects.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03826">Recurrent Multi-scale Transformer for High-Resolution Salient Object Detection. (arXiv:2308.03826v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1">Xinhao Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Pingping Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1">Huchuan Lu</a></p>
<p>Salient Object Detection (SOD) aims to identify and segment the most
conspicuous objects in an image or video. As an important pre-processing step,
it has many potential applications in multimedia and vision tasks. With the
advance of imaging devices, SOD with high-resolution images is of great demand,
recently. However, traditional SOD methods are largely limited to
low-resolution images, making them difficult to adapt to the development of
High-Resolution SOD (HRSOD). Although some HRSOD methods emerge, there are no
large enough datasets for training and evaluating. Besides, current HRSOD
methods generally produce incomplete object regions and irregular object
boundaries. To address above issues, in this work, we first propose a new
HRS10K dataset, which contains 10,500 high-quality annotated images at 2K-8K
resolution. As far as we know, it is the largest dataset for the HRSOD task,
which will significantly help future works in training and evaluating models.
Furthermore, to improve the HRSOD performance, we propose a novel Recurrent
Multi-scale Transformer (RMFormer), which recurrently utilizes shared
Transformers and multi-scale refinement architectures. Thus, high-resolution
saliency maps can be generated with the guidance of lower-resolution
predictions. Extensive experiments on both high-resolution and low-resolution
benchmarks show the effectiveness and superiority of the proposed framework.
The source code and dataset are released at:
https://github.com/DrowsyMon/RMFormer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03854">Revisiting Prompt Engineering via Declarative Crowdsourcing. (arXiv:2308.03854v1 [cs.DB])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Parameswaran_A/0/1/0/all/0/1">Aditya G. Parameswaran</a>, <a href="http://arxiv.org/find/cs/1/au:+Shankar_S/0/1/0/all/0/1">Shreya Shankar</a>, <a href="http://arxiv.org/find/cs/1/au:+Asawa_P/0/1/0/all/0/1">Parth Asawa</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_N/0/1/0/all/0/1">Naman Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yujie Wang</a></p>
<p>Large language models (LLMs) are incredibly powerful at comprehending and
generating data in the form of text, but are brittle and error-prone. There has
been an advent of toolkits and recipes centered around so-called prompt
engineering-the process of asking an LLM to do something via a series of
prompts. However, for LLM-powered data processing workflows, in particular,
optimizing for quality, while keeping cost bounded, is a tedious, manual
process. We put forth a vision for declarative prompt engineering. We view LLMs
like crowd workers and leverage ideas from the declarative crowdsourcing
literature-including leveraging multiple prompting strategies, ensuring
internal consistency, and exploring hybrid-LLM-non-LLM approaches-to make
prompt engineering a more principled process. Preliminary case studies on
sorting, entity resolution, and imputation demonstrate the promise of our
approach
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03855">Mobile Supply: The Last Piece of Jigsaw of Recommender System. (arXiv:2308.03855v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1">Zhenhao Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_B/0/1/0/all/0/1">Biao Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1">Hao Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jie Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1">Jia Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_N/0/1/0/all/0/1">Ning Hu</a></p>
<p>Recommendation system is a fundamental functionality of online platforms.
With the development of computing power of mobile phones, some researchers have
deployed recommendation algorithms on users' devices to solve the problems of
data transmission delay and pagination mechanism. However, the existing
edge-side mobile rankings cannot completely solve the problem of pagination
mechanism. The mobile rankings can only sort the items on the current page, so
it will not work if it is called once or twice. Besides, after the user has
viewed the items of interest to the user on the current page, the user refresh
to get a new page of items. This will make the mobile ranking model do a lot of
useless work and affect the user's immersive experience. In order to solve the
pagination mechanism problem, we propose a completely new module in the
pipeline of recommender named Mobile Supply. The pipeline of recommender system
is extended to "retrival-&gt;pre-ranking-&gt;ranking-&gt;re-ranking-&gt;Mobile
Supply-&gt;mobile ranking". Specifically, we introduce the concept of list value
and use point-wise method to approximate list-wise estimation. We also design a
new mobile ranking named device-aware mobile ranking considering the difference
of mobile devices tailored to the new pipeline. Extensive offline and online
experiments show the superiority of our proposed method and prove that Mobile
Supply can further improve the performance of edge-side recommender system and
user experience. Mobile Supply has been deployed on the homepage page of a
large-scale online food platform and has yielded considerable profits in our
business.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03866">Trusting Language Models in Education. (arXiv:2308.03866v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Neto_J/0/1/0/all/0/1">Jogi Suda Neto</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1">Li Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Raya_T/0/1/0/all/0/1">Thejaswi Raya</a>, <a href="http://arxiv.org/find/cs/1/au:+Shahbazi_R/0/1/0/all/0/1">Reza Shahbazi</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1">Nick Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Venkatesh_A/0/1/0/all/0/1">Adhitya Venkatesh</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1">Miral Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Khosla_N/0/1/0/all/0/1">Neeru Khosla</a>, <a href="http://arxiv.org/find/cs/1/au:+Guido_R/0/1/0/all/0/1">Rodrigo Capobianco Guido</a></p>
<p>Language Models are being widely used in Education. Even though modern deep
learning models achieve very good performance on question-answering tasks,
sometimes they make errors. To avoid misleading students by showing wrong
answers, it is important to calibrate the confidence - that is, the prediction
probability - of these models. In our work, we propose to use an XGBoost on top
of BERT to output the corrected probabilities, using features based on the
attention mechanism. Our hypothesis is that the level of uncertainty contained
in the flow of attention is related to the quality of the model's response
itself.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03880">Guarding the Guardians: Automated Analysis of Online Child Sexual Abuse. (arXiv:2308.03880v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Puentes_J/0/1/0/all/0/1">Juanita Puentes</a>, <a href="http://arxiv.org/find/cs/1/au:+Castillo_A/0/1/0/all/0/1">Angela Castillo</a>, <a href="http://arxiv.org/find/cs/1/au:+Osejo_W/0/1/0/all/0/1">Wilmar Osejo</a>, <a href="http://arxiv.org/find/cs/1/au:+Calderon_Y/0/1/0/all/0/1">Yuly Calder&#xf3;n</a>, <a href="http://arxiv.org/find/cs/1/au:+Quintero_V/0/1/0/all/0/1">Viviana Quintero</a>, <a href="http://arxiv.org/find/cs/1/au:+Saldarriaga_L/0/1/0/all/0/1">Lina Saldarriaga</a>, <a href="http://arxiv.org/find/cs/1/au:+Agudelo_D/0/1/0/all/0/1">Diana Agudelo</a>, <a href="http://arxiv.org/find/cs/1/au:+Arbelaez_P/0/1/0/all/0/1">Pablo Arbel&#xe1;ez</a></p>
<p>Online violence against children has increased globally recently, demanding
urgent attention. Competent authorities manually analyze abuse complaints to
comprehend crime dynamics and identify patterns. However, the manual analysis
of these complaints presents a challenge because it exposes analysts to harmful
content during the review process. Given these challenges, we present a novel
solution, an automated tool designed to analyze children's sexual abuse reports
comprehensively. By automating the analysis process, our tool significantly
reduces the risk of exposure to harmful content by categorizing the reports on
three dimensions: Subject, Degree of Criminality, and Damage. Furthermore,
leveraging our multidisciplinary team's expertise, we introduce a novel
approach to annotate the collected data, enabling a more in-depth analysis of
the reports. This approach improves the comprehension of fundamental patterns
and trends, enabling law enforcement agencies and policymakers to create
focused strategies in the fight against children's violence.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03882">Exploiting Generalization in Offline Reinforcement Learning via Unseen State Augmentations. (arXiv:2308.03882v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Modhe_N/0/1/0/all/0/1">Nirbhay Modhe</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1">Qiaozi Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalyan_A/0/1/0/all/0/1">Ashwin Kalyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1">Dhruv Batra</a>, <a href="http://arxiv.org/find/cs/1/au:+Thattai_G/0/1/0/all/0/1">Govind Thattai</a>, <a href="http://arxiv.org/find/cs/1/au:+Sukhatme_G/0/1/0/all/0/1">Gaurav Sukhatme</a></p>
<p>Offline reinforcement learning (RL) methods strike a balance between
exploration and exploitation by conservative value estimation -- penalizing
values of unseen states and actions. Model-free methods penalize values at all
unseen actions, while model-based methods are able to further exploit unseen
states via model rollouts. However, such methods are handicapped in their
ability to find unseen states far away from the available offline data due to
two factors -- (a) very short rollout horizons in models due to cascading model
errors, and (b) model rollouts originating solely from states observed in
offline data. We relax the second assumption and present a novel unseen state
augmentation strategy to allow exploitation of unseen states where the learned
model and value estimates generalize. Our strategy finds unseen states by
value-informed perturbations of seen states followed by filtering out states
with epistemic uncertainty estimates too high (high error) or too low (too
similar to seen data). We observe improved performance in several offline RL
tasks and find that our augmentation strategy consistently leads to overall
lower average dataset Q-value estimates i.e. more conservative Q-value
estimates than a baseline.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03901">FLIPS: Federated Learning using Intelligent Participant Selection. (arXiv:2308.03901v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bhope_R/0/1/0/all/0/1">Rahul Atul Bhope</a>, <a href="http://arxiv.org/find/cs/1/au:+Jayaram_K/0/1/0/all/0/1">K. R. Jayaram</a>, <a href="http://arxiv.org/find/cs/1/au:+Venkatasubramanian_N/0/1/0/all/0/1">Nalini Venkatasubramanian</a>, <a href="http://arxiv.org/find/cs/1/au:+Verma_A/0/1/0/all/0/1">Ashish Verma</a>, <a href="http://arxiv.org/find/cs/1/au:+Thomas_G/0/1/0/all/0/1">Gegi Thomas</a></p>
<p>This paper presents the design and implementation of FLIPS, a middleware
system to manage data and participant heterogeneity in federated learning (FL)
training workloads. In particular, we examine the benefits of label
distribution clustering on participant selection in federated learning. FLIPS
clusters parties involved in an FL training job based on the label distribution
of their data apriori, and during FL training, ensures that each cluster is
equitably represented in the participants selected. FLIPS can support the most
common FL algorithms, including FedAvg, FedProx, FedDyn, FedOpt and FedYogi. To
manage platform heterogeneity and dynamic resource availability, FLIPS
incorporates a straggler management mechanism to handle changing capacities in
distributed, smart community applications. Privacy of label distributions,
clustering and participant selection is ensured through a trusted execution
environment (TEE). Our comprehensive empirical evaluation compares FLIPS with
random participant selection, as well as two other "smart" selection mechanisms
- Oort and gradient clustering using two real-world datasets, two different
non-IID distributions and three common FL algorithms (FedYogi, FedProx and
FedAvg). We demonstrate that FLIPS significantly improves convergence,
achieving higher accuracy by 17 - 20 % with 20 - 60 % lower communication
costs, and these benefits endure in the presence of straggler participants.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03905">Intelligent Assistant Language Understanding On Device. (arXiv:2308.03905v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aas_C/0/1/0/all/0/1">Cecilia Aas</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdelsalam_H/0/1/0/all/0/1">Hisham Abdelsalam</a>, <a href="http://arxiv.org/find/cs/1/au:+Belousova_I/0/1/0/all/0/1">Irina Belousova</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhargava_S/0/1/0/all/0/1">Shruti Bhargava</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1">Jianpeng Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Daland_R/0/1/0/all/0/1">Robert Daland</a>, <a href="http://arxiv.org/find/cs/1/au:+Driesen_J/0/1/0/all/0/1">Joris Driesen</a>, <a href="http://arxiv.org/find/cs/1/au:+Flego_F/0/1/0/all/0/1">Federico Flego</a>, <a href="http://arxiv.org/find/cs/1/au:+Guigue_T/0/1/0/all/0/1">Tristan Guigue</a>, <a href="http://arxiv.org/find/cs/1/au:+Johannsen_A/0/1/0/all/0/1">Anders Johannsen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lal_P/0/1/0/all/0/1">Partha Lal</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Jiarui Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Moniz_J/0/1/0/all/0/1">Joel Ruben Antony Moniz</a>, <a href="http://arxiv.org/find/cs/1/au:+Perkins_N/0/1/0/all/0/1">Nathan Perkins</a>, <a href="http://arxiv.org/find/cs/1/au:+Piraviperumal_D/0/1/0/all/0/1">Dhivya Piraviperumal</a>, <a href="http://arxiv.org/find/cs/1/au:+Pulman_S/0/1/0/all/0/1">Stephen Pulman</a>, <a href="http://arxiv.org/find/cs/1/au:+Seaghdha_D/0/1/0/all/0/1">Diarmuid &#xd3; S&#xe9;aghdha</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1">David Q. Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Torr_J/0/1/0/all/0/1">John Torr</a>, <a href="http://arxiv.org/find/cs/1/au:+Vecchio_M/0/1/0/all/0/1">Marco Del Vecchio</a>, <a href="http://arxiv.org/find/cs/1/au:+Wacker_J/0/1/0/all/0/1">Jay Wacker</a>, <a href="http://arxiv.org/find/cs/1/au:+Williams_J/0/1/0/all/0/1">Jason D. Williams</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Hong Yu</a></p>
<p>It has recently become feasible to run personal digital assistants on phones
and other personal devices. In this paper we describe a design for a natural
language understanding system that runs on device. In comparison to a
server-based assistant, this system is more private, more reliable, faster,
more expressive, and more accurate. We describe what led to key choices about
architecture and technologies. For example, some approaches in the dialog
systems literature are difficult to maintain over time in a deployment setting.
We hope that sharing learnings from our practical experiences may help inform
future work in the research community.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03908">ViLP: Knowledge Exploration using Vision, Language, and Pose Embeddings for Video Action Recognition. (arXiv:2308.03908v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1">Soumyabrata Chaudhuri</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1">Saumik Bhattacharya</a></p>
<p>Video Action Recognition (VAR) is a challenging task due to its inherent
complexities. Though different approaches have been explored in the literature,
designing a unified framework to recognize a large number of human actions is
still a challenging problem. Recently, Multi-Modal Learning (MML) has
demonstrated promising results in this domain. In literature, 2D skeleton or
pose modality has often been used for this task, either independently or in
conjunction with the visual information (RGB modality) present in videos.
However, the combination of pose, visual information, and text attributes has
not been explored yet, though text and pose attributes independently have been
proven to be effective in numerous computer vision tasks. In this paper, we
present the first pose augmented Vision-language model (VLM) for VAR. Notably,
our scheme achieves an accuracy of 92.81% and 73.02% on two popular human video
action recognition benchmark datasets, UCF-101 and HMDB-51, respectively, even
without any video data pre-training, and an accuracy of 96.11% and 75.75% after
kinetics pre-training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03929">Establishing Trust in ChatGPT BioMedical Generated Text: An Ontology-Based Knowledge Graph to Validate Disease-Symptom Links. (arXiv:2308.03929v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hamed_A/0/1/0/all/0/1">Ahmed Abdeen Hamed</a>, <a href="http://arxiv.org/find/cs/1/au:+Crimi_A/0/1/0/all/0/1">Alessandro Crimi</a>, <a href="http://arxiv.org/find/cs/1/au:+Misiak_M/0/1/0/all/0/1">Magdalena M. Misiak</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1">Byung Suk Lee</a></p>
<p>Methods: Through an innovative approach, we construct ontology-based
knowledge graphs from authentic medical literature and AI-generated content.
Our goal is to distinguish factual information from unverified data. We
compiled two datasets: one from biomedical literature using a "human disease
and symptoms" query, and another generated by ChatGPT, simulating articles.
With these datasets (PubMed and ChatGPT), we curated 10 sets of 250 abstracts
each, selected randomly with a specific seed. Our method focuses on utilizing
disease ontology (DOID) and symptom ontology (SYMP) to build knowledge graphs,
robust mathematical models that facilitate unbiased comparisons. By employing
our fact-checking algorithms and network centrality metrics, we conducted GPT
disease-symptoms link analysis to quantify the accuracy of factual knowledge
amid noise, hypotheses, and significant findings.
</p>
<p>Results: The findings obtained from the comparison of diverse ChatGPT
knowledge graphs with their PubMed counterparts revealed some interesting
observations. While PubMed knowledge graphs exhibit a wealth of disease-symptom
terms, it is surprising to observe that some ChatGPT graphs surpass them in the
number of connections. Furthermore, some GPT graphs are demonstrating supremacy
of the centrality scores, especially for the overlapping nodes. This striking
contrast indicates the untapped potential of knowledge that can be derived from
AI-generated content, awaiting verification. Out of all the graphs, the factual
link ratio between any two graphs reached its peak at 60%.
</p>
<p>Conclusions: An intriguing insight from our findings was the striking number
of links among terms in the knowledge graph generated from ChatGPT datasets,
surpassing some of those in its PubMed counterpart. This early discovery has
prompted further investigation using universal network metrics to unveil the
new knowledge the links may hold.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03936">ALFA -- Leveraging All Levels of Feature Abstraction for Enhancing the Generalization of Histopathology Image Classification Across Unseen Hospitals. (arXiv:2308.03936v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sikaroudi_M/0/1/0/all/0/1">Milad Sikaroudi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rahnamayan_S/0/1/0/all/0/1">Shahryar Rahnamayan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tizhoosh_H/0/1/0/all/0/1">H.R. Tizhoosh</a></p>
<p>We propose an exhaustive methodology that leverages all levels of feature
abstraction, targeting an enhancement in the generalizability of image
classification to unobserved hospitals. Our approach incorporates
augmentation-based self-supervision with common distribution shifts in
histopathology scenarios serving as the pretext task. This enables us to derive
invariant features from training images without relying on training labels,
thereby covering different abstraction levels. Moving onto the subsequent
abstraction level, we employ a domain alignment module to facilitate further
extraction of invariant features across varying training hospitals. To
represent the highly specific features of participating hospitals, an encoder
is trained to classify hospital labels, independent of their diagnostic labels.
The features from each of these encoders are subsequently disentangled to
minimize redundancy and segregate the features. This representation, which
spans a broad spectrum of semantic information, enables the development of a
model demonstrating increased robustness to unseen images from disparate
distributions. Experimental results from the PACS dataset (a domain
generalization benchmark), a synthetic dataset created by applying
histopathology-specific jitters to the MHIST dataset (defining different
domains with varied distribution shifts), and a Renal Cell Carcinoma dataset
derived from four image repositories from TCGA, collectively indicate that our
proposed model is adept at managing varying levels of image granularity. Thus,
it shows improved generalizability when faced with new, out-of-distribution
hospital images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03968">CheXFusion: Effective Fusion of Multi-View Features using Transformers for Long-Tailed Chest X-Ray Classification. (arXiv:2308.03968v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1">Dongkyun Kim</a></p>
<p>Medical image classification poses unique challenges due to the long-tailed
distribution of diseases, the co-occurrence of diagnostic findings, and the
multiple views available for each study or patient. This paper introduces our
solution to the ICCV CVAMD 2023 Shared Task on CXR-LT: Multi-Label Long-Tailed
Classification on Chest X-Rays. Our approach introduces CheXFusion, a
transformer-based fusion module incorporating multi-view images. The fusion
module, guided by self-attention and cross-attention mechanisms, efficiently
aggregates multi-view features while considering label co-occurrence.
Furthermore, we explore data balancing and self-training methods to optimize
the model's performance. Our solution achieves state-of-the-art results with
0.372 mAP in the MIMIC-CXR test set, securing 1st place in the competition. Our
success in the task underscores the significance of considering multi-view
settings, class imbalance, and label co-occurrence in medical image
classification. Public code is available at
https://github.com/dongkyuk/CXR-LT-public-solution
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03983">SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool. (arXiv:2308.03983v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ng_Y/0/1/0/all/0/1">Youyang Ng</a>, <a href="http://arxiv.org/find/cs/1/au:+Miyashita_D/0/1/0/all/0/1">Daisuke Miyashita</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoshi_Y/0/1/0/all/0/1">Yasuto Hoshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Morioka_Y/0/1/0/all/0/1">Yasuhiro Morioka</a>, <a href="http://arxiv.org/find/cs/1/au:+Torii_O/0/1/0/all/0/1">Osamu Torii</a>, <a href="http://arxiv.org/find/cs/1/au:+Kodama_T/0/1/0/all/0/1">Tomoya Kodama</a>, <a href="http://arxiv.org/find/cs/1/au:+Deguchi_J/0/1/0/all/0/1">Jun Deguchi</a></p>
<p>Large Language Model (LLM) based Generative AI systems have seen significant
progress in recent years. Integrating a knowledge retrieval architecture allows
for seamless integration of private data into publicly available Generative AI
systems using pre-trained LLM without requiring additional model fine-tuning.
Moreover, Retrieval-Centric Generation (RCG) approach, a promising future
research direction that explicitly separates roles of LLMs and retrievers in
context interpretation and knowledge memorization, potentially leads to more
efficient implementation. SimplyRetrieve is an open-source tool with the goal
of providing a localized, lightweight, and user-friendly interface to these
sophisticated advancements to the machine learning community. SimplyRetrieve
features a GUI and API based RCG platform, assisted by a Private Knowledge Base
Constructor and a Retrieval Tuning Module. By leveraging these capabilities,
users can explore the potential of RCG for improving generative AI performance
while maintaining privacy standards. The tool is available at
https://github.com/RCGAI/SimplyRetrieve with an MIT license.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03990">NEOLAF, an LLM-powered neural-symbolic cognitive architecture. (arXiv:2308.03990v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tong_R/0/1/0/all/0/1">Richard Jiarui Tong</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1">Cassie Chen Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1">Timothy Xueqian Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1">Guodong Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_R/0/1/0/all/0/1">Ray Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Feiyue Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xiangen Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmucker_R/0/1/0/all/0/1">Robin Schmucker</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1">Jinsheng Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Quevedo_J/0/1/0/all/0/1">Julian Quevedo</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yu Lu</a></p>
<p>This paper presents the Never Ending Open Learning Adaptive Framework
(NEOLAF), an integrated neural-symbolic cognitive architecture that models and
constructs intelligent agents. The NEOLAF framework is a superior approach to
constructing intelligent agents than both the pure connectionist and pure
symbolic approaches due to its explainability, incremental learning,
efficiency, collaborative and distributed learning, human-in-the-loop
enablement, and self-improvement. The paper further presents a compelling
experiment where a NEOLAF agent, built as a problem-solving agent, is fed with
complex math problems from the open-source MATH dataset. The results
demonstrate NEOLAF's superior learning capability and its potential to
revolutionize the field of cognitive architectures and self-improving adaptive
instructional systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03992">AI Chatbots as Multi-Role Pedagogical Agents: Transforming Engagement in CS Education. (arXiv:2308.03992v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1">Cassie Chen Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1">Zijian Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Jionghao Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Hopfgartner_F/0/1/0/all/0/1">Frank Hopfgartner</a></p>
<p>This study investigates the use of Artificial Intelligence (AI)-powered,
multi-role chatbots as a means to enhance learning experiences and foster
engagement in computer science education. Leveraging a design-based research
approach, we develop, implement, and evaluate a novel learning environment
enriched with four distinct chatbot roles: Instructor Bot, Peer Bot, Career
Advising Bot, and Emotional Supporter Bot. These roles, designed around the
tenets of Self-Determination Theory, cater to the three innate psychological
needs of learners - competence, autonomy, and relatedness. Additionally, the
system embraces an inquiry-based learning paradigm, encouraging students to ask
questions, seek solutions, and explore their curiosities.
</p>
<p>We test this system in a higher education context over a period of one month
with 200 participating students, comparing outcomes with conditions involving a
human tutor and a single chatbot. Our research utilizes a mixed-methods
approach, encompassing quantitative measures such as chat log sequence
analysis, and qualitative methods including surveys and focus group interviews.
By integrating cutting-edge Natural Language Processing techniques such as
topic modelling and sentiment analysis, we offer an in-depth understanding of
the system's impact on learner engagement, motivation, and inquiry-based
learning.
</p>
<p>This study, through its rigorous design and innovative approach, provides
significant insights into the potential of AI-empowered, multi-role chatbots in
reshaping the landscape of computer science education and fostering an
engaging, supportive, and motivating learning environment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03995">Cooperative Multi-Type Multi-Agent Deep Reinforcement Learning for Resource Management in Space-Air-Ground Integrated Networks. (arXiv:2308.03995v1 [cs.MA])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hengxi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1">Huaze Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1">Wenbo Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiao-Ping Zhang</a></p>
<p>The Space-Air-Ground Integrated Network (SAGIN), integrating heterogeneous
devices including low earth orbit (LEO) satellites, unmanned aerial vehicles
(UAVs), and ground users (GUs), holds significant promise for advancing smart
city applications. However, resource management of the SAGIN is a challenge
requiring urgent study in that inappropriate resource management will cause
poor data transmission, and hence affect the services in smart cities. In this
paper, we develop a comprehensive SAGIN system that encompasses five distinct
communication links and propose an efficient cooperative multi-type multi-agent
deep reinforcement learning (CMT-MARL) method to address the resource
management issue. The experimental results highlight the efficacy of the
proposed CMT-MARL, as evidenced by key performance indicators such as the
overall transmission rate and transmission success rate. These results
underscore the potential value and feasibility of future implementation of the
SAGIN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03999">Understanding CNN Hidden Neuron Activations using Structured Background Knowledge and Deductive Reasoning. (arXiv:2308.03999v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dalal_A/0/1/0/all/0/1">Abhilekha Dalal</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarker_M/0/1/0/all/0/1">Md Kamruzzaman Sarker</a>, <a href="http://arxiv.org/find/cs/1/au:+Barua_A/0/1/0/all/0/1">Adrita Barua</a>, <a href="http://arxiv.org/find/cs/1/au:+Vasserman_E/0/1/0/all/0/1">Eugene Vasserman</a>, <a href="http://arxiv.org/find/cs/1/au:+Hitzler_P/0/1/0/all/0/1">Pascal Hitzler</a></p>
<p>A major challenge in Explainable AI is in correctly interpreting activations
of hidden neurons: accurate interpretations would provide insights into the
question of what a deep learning system has internally detected as relevant on
the input, de-mystifying the otherwise black-box character of deep learning
systems. The state of the art indicates that hidden node activations can, in
some cases, be interpretable in a way that makes sense to humans, but
systematic automated methods that would be able to hypothesize and verify
interpretations of hidden neuron activations are underexplored. In this paper,
we provide such a method and demonstrate that it provides meaningful
interpretations. Our approach is based on using large-scale background
knowledge approximately 2 million classes curated from the Wikipedia concept
hierarchy together with a symbolic reasoning approach called Concept Induction
based on description logics, originally developed for applications in the
Semantic Web field. Our results show that we can automatically attach
meaningful labels from the background knowledge to individual neurons in the
dense layer of a Convolutional Neural Network through a hypothesis and
verification process
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04017">Multi-Granularity Attention Model for Group Recommendation. (arXiv:2308.04017v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1">Jianye Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1">Jiayan Pei</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1">Shaochuan Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1">Taotao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1">Hengxu He</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1">Jia Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_N/0/1/0/all/0/1">Ning Hu</a></p>
<p>Group recommendation provides personalized recommendations to a group of
users based on their shared interests, preferences, and characteristics.
Current studies have explored different methods for integrating individual
preferences and making collective decisions that benefit the group as a whole.
However, most of them heavily rely on users with rich behavior and ignore
latent preferences of users with relatively sparse behavior, leading to
insufficient learning of individual interests. To address this challenge, we
present the Multi-Granularity Attention Model (MGAM), a novel approach that
utilizes multiple levels of granularity (i.e., subsets, groups, and supersets)
to uncover group members' latent preferences and mitigate recommendation noise.
Specially, we propose a Subset Preference Extraction module that enhances the
representation of users' latent subset-level preferences by incorporating their
previous interactions with items and utilizing a hierarchical mechanism.
Additionally, our method introduces a Group Preference Extraction module and a
Superset Preference Extraction module, which explore users' latent preferences
on two levels: the group-level, which maintains users' original preferences,
and the superset-level, which includes group-group exterior information. By
incorporating the subset-level embedding, group-level embedding, and
superset-level embedding, our proposed method effectively reduces group
recommendation noise across multiple granularities and comprehensively learns
individual interests. Extensive offline and online experiments have
demonstrated the superiority of our method in terms of performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04018">Improving Performance of Semi-Supervised Learning by Adversarial Attacks. (arXiv:2308.04018v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1">Dongyoon Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1">Kunwoong Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Yongdai Kim</a></p>
<p>Semi-supervised learning (SSL) algorithm is a setup built upon a realistic
assumption that access to a large amount of labeled data is tough. In this
study, we present a generalized framework, named SCAR, standing for Selecting
Clean samples with Adversarial Robustness, for improving the performance of
recent SSL algorithms. By adversarially attacking pre-trained models with
semi-supervision, our framework shows substantial advances in classifying
images. We introduce how adversarial attacks successfully select high-confident
unlabeled data to be labeled with current predictions. On CIFAR10, three recent
SSL algorithms with SCAR result in significantly improved image classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04024">Scope Loss for Imbalanced Classification and RL Exploration. (arXiv:2308.04024v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Burhani_H/0/1/0/all/0/1">Hasham Burhani</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1">Xiao Qi Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Jaegerman_J/0/1/0/all/0/1">Jonathan Jaegerman</a>, <a href="http://arxiv.org/find/cs/1/au:+Balicki_D/0/1/0/all/0/1">Daniel Balicki</a></p>
<p>We demonstrate equivalence between the reinforcement learning problem and the
supervised classification problem. We consequently equate the exploration
exploitation trade-off in reinforcement learning to the dataset imbalance
problem in supervised classification, and find similarities in how they are
addressed. From our analysis of the aforementioned problems we derive a novel
loss function for reinforcement learning and supervised classification. Scope
Loss, our new loss function, adjusts gradients to prevent performance losses
from over-exploitation and dataset imbalances, without the need for any tuning.
We test Scope Loss against SOTA loss functions over a basket of benchmark
reinforcement learning tasks and a skewed classification dataset, and show that
Scope Loss outperforms other loss functions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04025">MSAC: Multiple Speech Attribute Control Method for Speech Emotion Recognition. (arXiv:2308.04025v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1">Yu Pan</a></p>
<p>Despite significant progress, speech emotion recognition (SER) remains
challenging due to inherent complexity and ambiguity of the emotion attribute,
particularly in wild world. Whereas current studies primarily focus on
recognition and generalization capabilities, this work pioneers an exploration
into the reliability of SER methods and investigates how to model the speech
emotion from the aspect of data distribution across various speech attributes.
Specifically, we first build a novel CNN-based SER model which adopts additive
margin softmax loss to expand the distance between features of different
classes, thereby enhancing their discrimination. Second, a novel multiple
speech attribute control method MSAC is proposed to explicitly control speech
attributes, enabling the model to be less affected by emotion-agnostic
attributes and capture more fine-grained emotion-related features. Third, we
make a first attempt to test and analyze the reliability of the proposed SER
workflow using the out-of-distribution detection method. Extensive experiments
on both single and cross-corpus SER scenarios show that our proposed unified
SER workflow consistently outperforms the baseline in terms of recognition,
generalization, and reliability performance. Besides, in single-corpus SER, the
proposed SER workflow achieves superior recognition results with a WAR of
72.97\% and a UAR of 71.76\% on the IEMOCAP corpus.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04026">AgentSims: An Open-Source Sandbox for Large Language Model Evaluation. (arXiv:2308.04026v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Jiaju Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Haoran Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1">Aochi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yiting Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ping_H/0/1/0/all/0/1">Huqiuyue Ping</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Qin Chen</a></p>
<p>With ChatGPT-like large language models (LLM) prevailing in the community,
how to evaluate the ability of LLMs is an open question. Existing evaluation
methods suffer from following shortcomings: (1) constrained evaluation
abilities, (2) vulnerable benchmarks, (3) unobjective metrics. We suggest that
task-based evaluation, where LLM agents complete tasks in a simulated
environment, is a one-for-all solution to solve above problems. We present
AgentSims, an easy-to-use infrastructure for researchers from all disciplines
to test the specific capacities they are interested in. Researchers can build
their evaluation tasks by adding agents and buildings on an interactive GUI or
deploy and test new support mechanisms, i.e. memory, planning and tool-use
systems, by a few lines of codes. Our demo is available at
https://agentsims.com .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04028">Top K Relevant Passage Retrieval for Biomedical Question Answering. (arXiv:2308.04028v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1">Shashank Gupta</a></p>
<p>Question answering is a task that answers factoid questions using a large
collection of documents. It aims to provide precise answers in response to the
user's questions in natural language. Question answering relies on efficient
passage retrieval to select candidate contexts, where traditional sparse vector
space models, such as TF-IDF or BM25, are the de facto method. On the web,
there is no single article that could provide all the possible answers
available on the internet to the question of the problem asked by the user. The
existing Dense Passage Retrieval model has been trained on Wikipedia dump from
Dec. 20, 2018, as the source documents for answering questions. Question
answering (QA) has made big strides with several open-domain and machine
comprehension systems built using large-scale annotated datasets. However, in
the clinical domain, this problem remains relatively unexplored. According to
multiple surveys, Biomedical Questions cannot be answered correctly from
Wikipedia Articles. In this work, we work on the existing DPR framework for the
biomedical domain and retrieve answers from the Pubmed articles which is a
reliable source to answer medical questions. When evaluated on a BioASQ QA
dataset, our fine-tuned dense retriever results in a 0.81 F1 score.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04030">Gentopia: A Collaborative Platform for Tool-Augmented LLMs. (arXiv:2308.04030v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1">Binfeng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xukun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1">Hua Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1">Zeyu Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuhan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yue_M/0/1/0/all/0/1">Murong Yue</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1">Zhiyuan Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuchen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1">Ziyu Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1">Dongkuan Xu</a></p>
<p>Augmented Language Models (ALMs) empower large language models with the
ability to use tools, transforming them into intelligent agents for real-world
interactions. However, most existing frameworks for ALMs, to varying degrees,
are deficient in the following critical features: flexible customization,
collaborative democratization, and holistic evaluation. We present gentopia, an
ALM framework enabling flexible customization of agents through simple
configurations, seamlessly integrating various language models, task formats,
prompting modules, and plugins into a unified paradigm. Furthermore, we
establish gentpool, a public platform enabling the registration and sharing of
user-customized agents. Agents registered in gentpool are composable such that
they can be assembled together for agent collaboration, advancing the
democratization of artificial intelligence. To ensure high-quality agents,
gentbench, an integral component of gentpool, is designed to thoroughly
evaluate user-customized agents across diverse aspects such as safety,
robustness, efficiency, etc. We release gentopia on Github and will
continuously move forward.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04032">Measure of Uncertainty in Human Emotions. (arXiv:2308.04032v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Naude_E/0/1/0/all/0/1">Etienne Naude</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Gann_H/0/1/0/all/0/1">Henry Gann</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Panda_B/0/1/0/all/0/1">Balaram Panda</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lance Zhang</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1">Raina Song</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yuwei Shen</a> (1) ((1) The University of Auckland)</p>
<p>Many research explore how well computers are able to examine emotions
displayed by humans and use that data to perform different tasks. However,
there have been very few research which evaluate the computers ability to
generate emotion classification information in an attempt to help the user make
decisions or perform tasks. This is a crucial area to explore as it is
paramount to the two way communication between humans and computers. This
research conducted an experiment to investigate the impact of different
uncertainty information displays of emotion classification on the human
decision making process. Results show that displaying more uncertainty
information can help users to be more confident when making decisions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04033">Adapting Foundation Models for Information Synthesis of Wireless Communication Specifications. (arXiv:2308.04033v1 [cs.NI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kotaru_M/0/1/0/all/0/1">Manikanta Kotaru</a></p>
<p>Existing approaches to understanding, developing and researching modern
wireless communication technologies involves time-intensive and arduous process
of sifting through numerous webpages and technical specification documents,
gathering the required information and synthesizing it. This paper presents
NextGen Communications Copilot, a conversational artificial intelligence tool
for information synthesis of wireless communication specifications. The system
builds on top of recent advancements in foundation models and consists of three
key additional components: a domain-specific database, a context extractor, and
a feedback mechanism. The system appends user queries with concise and
query-dependent contextual information extracted from a database of wireless
technical specifications and incorporates tools for expert feedback and data
contributions. On evaluation using a benchmark dataset of queries and reference
responses created by subject matter experts, the system demonstrated more
relevant and accurate answers with an average BLEU score and BERTScore
F1-measure of 0.37 and 0.79 respectively compared to the corresponding values
of 0.07 and 0.59 achieved by state-of-the-art tools like ChatGPT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04041">InfeRE: Step-by-Step Regex Generation via Chain of Inference. (arXiv:2308.04041v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shuai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1">Xiaodong Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuting Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1">Beijun Shen</a></p>
<p>Automatically generating regular expressions (abbrev. regexes) from natural
language description (NL2RE) has been an emerging research area. Prior studies
treat regex as a linear sequence of tokens and generate the final expressions
autoregressively in a single pass. They did not take into account the
step-by-step internal text-matching processes behind the final results. This
significantly hinders the efficacy and interpretability of regex generation by
neural language models. In this paper, we propose a new paradigm called InfeRE,
which decomposes the generation of regexes into chains of step-by-step
inference. To enhance the robustness, we introduce a self-consistency decoding
mechanism that ensembles multiple outputs sampled from different models. We
evaluate InfeRE on two publicly available datasets, NL-RX-Turk and KB13, and
compare the results with state-of-the-art approaches and the popular tree-based
generation approach TRANX. Experimental results show that InfeRE substantially
outperforms previous baselines, yielding 16.3% and 14.7% improvement in DFA@5
accuracy on two datasets, respectively. Particularly, InfeRE outperforms the
popular tree-based generation approach by 18.1% and 11.3% on both datasets,
respectively, in terms of DFA@5 accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04047">SODFormer: Streaming Object Detection with Transformer Using Events and Frames. (arXiv:2308.04047v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dianze Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jianing Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yonghong Tian</a></p>
<p>DAVIS camera, streaming two complementary sensing modalities of asynchronous
events and frames, has gradually been used to address major object detection
challenges (e.g., fast motion blur and low-light). However, how to effectively
leverage rich temporal cues and fuse two heterogeneous visual streams remains a
challenging endeavor. To address this challenge, we propose a novel streaming
object detector with Transformer, namely SODFormer, which first integrates
events and frames to continuously detect objects in an asynchronous manner.
Technically, we first build a large-scale multimodal neuromorphic object
detection dataset (i.e., PKU-DAVIS-SOD) over 1080.1k manual labels. Then, we
design a spatiotemporal Transformer architecture to detect objects via an
end-to-end sequence prediction problem, where the novel temporal Transformer
module leverages rich temporal cues from two visual streams to improve the
detection performance. Finally, an asynchronous attention-based fusion module
is proposed to integrate two heterogeneous sensing modalities and take
complementary advantages from each end, which can be queried at any time to
locate objects and break through the limited output frequency from synchronized
frame-based fusion strategies. The results show that the proposed SODFormer
outperforms four state-of-the-art methods and our eight baselines by a
significant margin. We also show that our unifying framework works well even in
cases where the conventional frame-based camera fails, e.g., high-speed motion
and low-light conditions. Our dataset and code can be available at
https://github.com/dianzl/SODFormer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04061">Enhancing Adversarial Robustness in Low-Label Regime via Adaptively Weighted Regularization and Knowledge Distillation. (arXiv:2308.04061v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1">Dongyoon Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kong_I/0/1/0/all/0/1">Insung Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Yongdai Kim</a></p>
<p>Adversarial robustness is a research area that has recently received a lot of
attention in the quest for trustworthy artificial intelligence. However, recent
works on adversarial robustness have focused on supervised learning where it is
assumed that labeled data is plentiful. In this paper, we investigate
semi-supervised adversarial training where labeled data is scarce. We derive
two upper bounds for the robust risk and propose a regularization term for
unlabeled data motivated by these two upper bounds. Then, we develop a
semi-supervised adversarial training algorithm that combines the proposed
regularization term with knowledge distillation using a semi-supervised teacher
(i.e., a teacher model trained using a semi-supervised learning algorithm). Our
experiments show that our proposed algorithm achieves state-of-the-art
performance with significant margins compared to existing algorithms. In
particular, compared to supervised learning algorithms, performance of our
proposed algorithm is not much worse even when the amount of labeled data is
very small. For example, our algorithm with only 8\% labeled data is comparable
to supervised adversarial training algorithms that use all labeled data, both
in terms of standard and robust accuracies on CIFAR-10.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04071">Path Signatures for Diversity in Probabilistic Trajectory Optimisation. (arXiv:2308.04071v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Barcelos_L/0/1/0/all/0/1">Lucas Barcelos</a>, <a href="http://arxiv.org/find/cs/1/au:+Lai_T/0/1/0/all/0/1">Tin Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+Oliveira_R/0/1/0/all/0/1">Rafael Oliveira</a>, <a href="http://arxiv.org/find/cs/1/au:+Borges_P/0/1/0/all/0/1">Paulo Borges</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramos_F/0/1/0/all/0/1">Fabio Ramos</a></p>
<p>Motion planning can be cast as a trajectory optimisation problem where a cost
is minimised as a function of the trajectory being generated. In complex
environments with several obstacles and complicated geometry, this optimisation
problem is usually difficult to solve and prone to local minima. However,
recent advancements in computing hardware allow for parallel trajectory
optimisation where multiple solutions are obtained simultaneously, each
initialised from a different starting point. Unfortunately, without a strategy
preventing two solutions to collapse on each other, naive parallel optimisation
can suffer from mode collapse diminishing the efficiency of the approach and
the likelihood of finding a global solution. In this paper we leverage on
recent advances in the theory of rough paths to devise an algorithm for
parallel trajectory optimisation that promotes diversity over the range of
solutions, therefore avoiding mode collapses and achieving better global
properties. Our approach builds on path signatures and Hilbert space
representations of trajectories, and connects parallel variational inference
for trajectory estimation with diversity promoting kernels. We empirically
demonstrate that this strategy achieves lower average costs than competing
alternatives on a range of problems, from 2D navigation to robotic manipulators
operating in cluttered environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04077">Federated Zeroth-Order Optimization using Trajectory-Informed Surrogate Gradients. (arXiv:2308.04077v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shu_Y/0/1/0/all/0/1">Yao Shu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1">Xiaoqiang Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1">Zhongxiang Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Low_B/0/1/0/all/0/1">Bryan Kian Hsiang Low</a></p>
<p>Federated optimization, an emerging paradigm which finds wide real-world
applications such as federated learning, enables multiple clients (e.g., edge
devices) to collaboratively optimize a global function. The clients do not
share their local datasets and typically only share their local gradients.
However, the gradient information is not available in many applications of
federated optimization, which hence gives rise to the paradigm of federated
zeroth-order optimization (ZOO). Existing federated ZOO algorithms suffer from
the limitations of query and communication inefficiency, which can be
attributed to (a) their reliance on a substantial number of function queries
for gradient estimation and (b) the significant disparity between their
realized local updates and the intended global updates. To this end, we (a)
introduce trajectory-informed gradient surrogates which is able to use the
history of function queries during optimization for accurate and
query-efficient gradient estimation, and (b) develop the technique of adaptive
gradient correction using these gradient surrogates to mitigate the
aforementioned disparity. Based on these, we propose the federated zeroth-order
optimization using trajectory-informed surrogate gradients (FZooS) algorithm
for query- and communication-efficient federated ZOO. Our FZooS achieves
theoretical improvements over the existing approaches, which is supported by
our real-world experiments such as federated black-box adversarial attack and
federated non-differentiable metric optimization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04083">Heterogeneous 360 Degree Videos in Metaverse: Differentiated Reinforcement Learning Approaches. (arXiv:2308.04083v1 [cs.NI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1">Wenhan Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jun Zhao</a></p>
<p>Advanced video technologies are driving the development of the futuristic
Metaverse, which aims to connect users from anywhere and anytime. As such, the
use cases for users will be much more diverse, leading to a mix of 360-degree
videos with two types: non-VR and VR 360-degree videos. This paper presents a
novel Quality of Service model for heterogeneous 360-degree videos with
different requirements for frame rates and cybersickness. We propose a
frame-slotted structure and conduct frame-wise optimization using self-designed
differentiated deep reinforcement learning algorithms. Specifically, we design
two structures, Separate Input Differentiated Output (SIDO) and Merged Input
Differentiated Output (MIDO), for this heterogeneous scenario. We also conduct
comprehensive experiments to demonstrate their effectiveness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04161">Current and Future Challenges in Knowledge Representation and Reasoning. (arXiv:2308.04161v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Delgrande_J/0/1/0/all/0/1">James P. Delgrande</a>, <a href="http://arxiv.org/find/cs/1/au:+Glimm_B/0/1/0/all/0/1">Birte Glimm</a>, <a href="http://arxiv.org/find/cs/1/au:+Meyer_T/0/1/0/all/0/1">Thomas Meyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Truszczynski_M/0/1/0/all/0/1">Miroslaw Truszczynski</a>, <a href="http://arxiv.org/find/cs/1/au:+Wolter_F/0/1/0/all/0/1">Frank Wolter</a></p>
<p>Knowledge Representation and Reasoning is a central, longstanding, and active
area of Artificial Intelligence. Over the years it has evolved significantly;
more recently it has been challenged and complemented by research in areas such
as machine learning and reasoning under uncertainty. In July 2022 a Dagstuhl
Perspectives workshop was held on Knowledge Representation and Reasoning. The
goal of the workshop was to describe the state of the art in the field,
including its relation with other areas, its shortcomings and strengths,
together with recommendations for future progress. We developed this manifesto
based on the presentations, panels, working groups, and discussions that took
place at the Dagstuhl Workshop. It is a declaration of our views on Knowledge
Representation: its origins, goals, milestones, and current foci; its relation
to other disciplines, especially to Artificial Intelligence; and on its
challenges, along with key priorities for the next decade.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04172">Predicting Drug-Drug Interactions Using Knowledge Graphs. (arXiv:2308.04172v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Farrugia_L/0/1/0/all/0/1">Lizzy Farrugia</a>, <a href="http://arxiv.org/find/cs/1/au:+Azzopardi_L/0/1/0/all/0/1">Lilian M. Azzopardi</a>, <a href="http://arxiv.org/find/cs/1/au:+Debattista_J/0/1/0/all/0/1">Jeremy Debattista</a>, <a href="http://arxiv.org/find/cs/1/au:+Abela_C/0/1/0/all/0/1">Charlie Abela</a></p>
<p>In the last decades, people have been consuming and combining more drugs than
before, increasing the number of Drug-Drug Interactions (DDIs). To predict
unknown DDIs, recently, studies started incorporating Knowledge Graphs (KGs)
since they are able to capture the relationships among entities providing
better drug representations than using a single drug property. In this paper,
we propose the medicX end-to-end framework that integrates several drug
features from public drug repositories into a KG and embeds the nodes in the
graph using various translation, factorisation and Neural Network (NN) based KG
Embedding (KGE) methods. Ultimately, we use a Machine Learning (ML) algorithm
that predicts unknown DDIs. Among the different translation and
factorisation-based KGE models, we found that the best performing combination
was the ComplEx embedding method with a Long Short-Term Memory (LSTM) network,
which obtained an F1-score of 95.19% on a dataset based on the DDIs found in
DrugBank version 5.1.8. This score is 5.61% better than the state-of-the-art
model DeepDDI. Additionally, we also developed a graph auto-encoder model that
uses a Graph Neural Network (GNN), which achieved an F1-score of 91.94%.
Consequently, GNNs have demonstrated a stronger ability to mine the underlying
semantics of the KG than the ComplEx model, and thus using higher dimension
embeddings within the GNN can lead to state-of-the-art performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04178">Assistive Chatbots for healthcare: a succinct review. (arXiv:2308.04178v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_B/0/1/0/all/0/1">Basabdatta Sen Bhattacharya</a>, <a href="http://arxiv.org/find/cs/1/au:+Pissurlenkar_V/0/1/0/all/0/1">Vibhav Sinai Pissurlenkar</a></p>
<p>Artificial Intelligence (AI) for supporting healthcare services has never
been more necessitated than by the recent global pandemic. Here, we review the
state-of-the-art in AI-enabled Chatbots in healthcare proposed during the last
10 years (2013-2023). The focus on AI-enabled technology is because of its
potential for enhancing the quality of human-machine interaction via Chatbots,
reducing dependence on human-human interaction and saving man-hours. Our review
indicates that there are a handful of (commercial) Chatbots that are being used
for patient support, while there are others (non-commercial) that are in the
clinical trial phases. However, there is a lack of trust on this technology
regarding patient safety and data protection, as well as a lack of wider
awareness on its benefits among the healthcare workers and professionals. Also,
patients have expressed dissatisfaction with Natural Language Processing (NLP)
skills of the Chatbots in comparison to humans. Notwithstanding the recent
introduction of ChatGPT that has raised the bar for the NLP technology, this
Chatbot cannot be trusted with patient safety and medical ethics without
thorough and rigorous checks to serve in the `narrow' domain of assistive
healthcare. Our review suggests that to enable deployment and integration of
AI-enabled Chatbots in public health services, the need of the hour is: to
build technology that is simple and safe to use; to build confidence on the
technology among: (a) the medical community by focussed training and
development; (b) the patients and wider community through outreach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04187">Adding Why to What? Analyses of an Everyday Explanation. (arXiv:2308.04187v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Terfloth_L/0/1/0/all/0/1">Lutz Terfloth</a>, <a href="http://arxiv.org/find/cs/1/au:+Schaffer_M/0/1/0/all/0/1">Michael Schaffer</a>, <a href="http://arxiv.org/find/cs/1/au:+Buhl_H/0/1/0/all/0/1">Heike M. Buhl</a>, <a href="http://arxiv.org/find/cs/1/au:+Schulte_C/0/1/0/all/0/1">Carsten Schulte</a></p>
<p>In XAI it is important to consider that, in contrast to explanations for
professional audiences, one cannot assume common expertise when explaining for
laypeople. But such explanations between humans vary greatly, making it
difficult to research commonalities across explanations. We used the dual
nature theory, a techno-philosophical approach, to cope with these challenges.
According to it, one can explain, for example, an XAI's decision by addressing
its dual nature: by focusing on the Architecture (e.g., the logic of its
algorithms) or the Relevance (e.g., the severity of a decision, the
implications of a recommendation). We investigated 20 game explanations using
the theory as an analytical framework. We elaborate how we used the theory to
quickly structure and compare explanations of technological artifacts. We
supplemented results from analyzing the explanation contents with results from
a video recall to explore how explainers justified their explanation. We found
that explainers were focusing on the physical aspects of the game first
(Architecture) and only later on aspects of the Relevance. Reasoning in the
video recalls indicated that EX regarded the focus on the Architecture as
important for structuring the explanation initially by explaining the basic
components before focusing on more complex, intangible aspects. Shifting
between addressing the two sides was justified by explanation goals, emerging
misunderstandings, and the knowledge needs of the explainee. We discovered
several commonalities that inspire future research questions which, if further
generalizable, provide first ideas for the construction of synthetic
explanations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04214">A Differential Datalog Interpreter. (arXiv:2308.04214v1 [cs.DB])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lima_B/0/1/0/all/0/1">Bruno Rucy Carneiro Alves de Lima</a>, <a href="http://arxiv.org/find/cs/1/au:+Kramer_M/0/1/0/all/0/1">Merlin Kramer</a>, <a href="http://arxiv.org/find/cs/1/au:+Apinis_K/0/1/0/all/0/1">Kalmer Apinis</a></p>
<p>The core reasoning task for datalog engines is materialization, the
evaluation of a datalog program over a database alongside its physical
incorporation into the database itself. The de-facto method of computing it, is
through the recursive application of inference rules. Due to it being a costly
operation, it is a must for datalog engines to provide incremental
materialization, that is, to adjust the computation to new data, instead of
restarting from scratch. One of the major caveats, is that deleting data is
notoriously more involved than adding, since one has to take into account all
possible data that has been entailed from what is being deleted. Differential
Dataflow is a computational model that provides efficient incremental
maintenance, notoriously with equal performance between additions and
deletions, and work distribution, of iterative dataflows. In this paper we
investigate the performance of materialization with three reference datalog
implementations, out of which one is built on top of a lightweight relational
engine, and the two others are differential-dataflow and non-differential
versions of the same rewrite algorithm, with the same optimizations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04215">Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance. (arXiv:2308.04215v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xuchao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1">Menglin Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Couturier_C/0/1/0/all/0/1">Camille Couturier</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1">Guoqing Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajmohan_S/0/1/0/all/0/1">Saravan Rajmohan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruhle_V/0/1/0/all/0/1">Victor Ruhle</a></p>
<p>Retrieval augmented models show promise in enhancing traditional language
models by improving their contextual understanding, integrating private data,
and reducing hallucination. However, the processing time required for retrieval
augmented large language models poses a challenge when applying them to tasks
that require real-time responses, such as composition assistance.
</p>
<p>To overcome this limitation, we propose the Hybrid Retrieval-Augmented
Generation (HybridRAG) framework that leverages a hybrid setting that combines
both client and cloud models. HybridRAG incorporates retrieval-augmented memory
generated asynchronously by a Large Language Model (LLM) in the cloud. By
integrating this retrieval augmented memory, the client model acquires the
capability to generate highly effective responses, benefiting from the LLM's
capabilities. Furthermore, through asynchronous memory integration, the client
model is capable of delivering real-time responses to user requests without the
need to wait for memory synchronization from the cloud. Our experiments on
Wikitext and Pile subsets show that HybridRAG achieves lower latency than a
cloud-based retrieval-augmented LLM, while outperforming client-only models in
utility.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04220">Semantic Interpretation and Validation of Graph Attention-based Explanations for GNN Models. (arXiv:2308.04220v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Panagiotaki_E/0/1/0/all/0/1">Efimia Panagiotaki</a>, <a href="http://arxiv.org/find/cs/1/au:+Martini_D/0/1/0/all/0/1">Daniele De Martini</a>, <a href="http://arxiv.org/find/cs/1/au:+Kunze_L/0/1/0/all/0/1">Lars Kunze</a></p>
<p>In this work, we propose a methodology for investigating the application of
semantic attention to enhance the explainability of Graph Neural Network
(GNN)-based models, introducing semantically-informed perturbations and
establishing a correlation between predicted feature-importance weights and
model accuracy. Graph Deep Learning (GDL) has emerged as a promising field for
tasks like scene interpretation, leveraging flexible graph structures to
concisely describe complex features and relationships. As traditional
explainability methods used in eXplainable AI (XAI) cannot be directly applied
to such structures, graph-specific approaches are introduced. Attention
mechanisms have demonstrated their efficacy in estimating the importance of
input features in deep learning models and thus have been previously employed
to provide feature-based explanations for GNN predictions. Building upon these
insights, we extend existing attention-based graph-explainability methods
investigating the use of attention weights as importance indicators of
semantically sorted feature sets. Through analysing the behaviour of predicted
attention-weights distribution in correlation with model accuracy, we gain
valuable insights into feature importance with respect to the behaviour of the
GNN model. We apply our methodology to a lidar pointcloud estimation model
successfully identifying key semantic classes that contribute to enhanced
performance effectively generating reliable post-hoc semantic explanations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04237">Federated Inference with Reliable Uncertainty Quantification over Wireless Channels via Conformal Prediction. (arXiv:2308.04237v1 [cs.IT])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1">Meiyi Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zecchin_M/0/1/0/all/0/1">Matteo Zecchin</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1">Sangwoo Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1">Caili Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1">Chunyan Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Simeone_O/0/1/0/all/0/1">Osvaldo Simeone</a></p>
<p>Consider a setting in which devices and a server share a pre-trained model.
The server wishes to make an inference on a new input given the model. Devices
have access to data, previously not used for training, and can communicate to
the server over a common wireless channel. If the devices have no access to the
new input, can communication from devices to the server enhance the quality of
the inference decision at the server? Recent work has introduced federated
conformal prediction (CP), which leverages devices-to-server communication to
improve the reliability of the server's decision. With federated CP, devices
communicate to the server information about the loss accrued by the shared
pre-trained model on the local data, and the server leverages this information
to calibrate a decision interval, or set, so that it is guaranteed to contain
the correct answer with a pre-defined target reliability level. Previous work
assumed noise-free communication, whereby devices can communicate a single real
number to the server. In this paper, we study for the first time federated CP
in a wireless setting. We introduce a novel protocol, termed wireless federated
conformal prediction (WFCP), which builds on type-based multiple access (TBMA)
and on a novel quantile correction strategy. WFCP is proved to provide formal
reliability guarantees in terms of coverage of the predicted set produced by
the server. Using numerical results, we demonstrate the significant advantages
of WFCP against digital implementations of existing federated CP schemes,
especially in regimes with limited communication resources and/or large number
of devices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04241">AutoPCF: Efficient Product Carbon Footprint Accounting with Large Language Models. (arXiv:2308.04241v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1">Zhu Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jinjie Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1">Biao Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1">Can Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1">Qingrun Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1">Lei Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1">Wenwen Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhu Liu</a></p>
<p>The product carbon footprint (PCF) is crucial for decarbonizing the supply
chain, as it measures the direct and indirect greenhouse gas emissions caused
by all activities during the product's life cycle. However, PCF accounting
often requires expert knowledge and significant time to construct life cycle
models. In this study, we test and compare the emergent ability of five large
language models (LLMs) in modeling the 'cradle-to-gate' life cycles of products
and generating the inventory data of inputs and outputs, revealing their
limitations as a generalized PCF knowledge database. By utilizing LLMs, we
propose an automatic AI-driven PCF accounting framework, called AutoPCF, which
also applies deep learning algorithms to automatically match calculation
parameters, and ultimately calculate the PCF. The results of estimating the
carbon footprint for three case products using the AutoPCF framework
demonstrate its potential in achieving automatic modeling and estimation of PCF
with a large reduction in modeling time from days to minutes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04248">Gloss Alignment Using Word Embeddings. (arXiv:2308.04248v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Walsh_H/0/1/0/all/0/1">Harry Walsh</a>, <a href="http://arxiv.org/find/cs/1/au:+Sincan_O/0/1/0/all/0/1">Ozge Mercanoglu Sincan</a>, <a href="http://arxiv.org/find/cs/1/au:+Saunders_B/0/1/0/all/0/1">Ben Saunders</a>, <a href="http://arxiv.org/find/cs/1/au:+Bowden_R/0/1/0/all/0/1">Richard Bowden</a></p>
<p>Capturing and annotating Sign language datasets is a time consuming and
costly process. Current datasets are orders of magnitude too small to
successfully train unconstrained \acf{slt} models. As a result, research has
turned to TV broadcast content as a source of large-scale training data,
consisting of both the sign language interpreter and the associated audio
subtitle. However, lack of sign language annotation limits the usability of
this data and has led to the development of automatic annotation techniques
such as sign spotting. These spottings are aligned to the video rather than the
subtitle, which often results in a misalignment between the subtitle and
spotted signs. In this paper we propose a method for aligning spottings with
their corresponding subtitles using large spoken language models. Using a
single modality means our method is computationally inexpensive and can be
utilized in conjunction with existing alignment techniques. We quantitatively
demonstrate the effectiveness of our method on the \acf{mdgs} and \acf{bobsl}
datasets, recovering up to a 33.22 BLEU-1 score in word alignment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04249">MindDiffuser: Controlled Image Reconstruction from Human Brain Activity with Semantic and Structural Diffusion. (arXiv:2308.04249v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yizhuo Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1">Changde Du</a>, <a href="http://arxiv.org/find/cs/1/au:+zhou_Q/0/1/0/all/0/1">Qiongyi zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dianpeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1">Huiguang He</a></p>
<p>Reconstructing visual stimuli from brain recordings has been a meaningful and
challenging task. Especially, the achievement of precise and controllable image
reconstruction bears great significance in propelling the progress and
utilization of brain-computer interfaces. Despite the advancements in complex
image reconstruction techniques, the challenge persists in achieving a cohesive
alignment of both semantic (concepts and objects) and structure (position,
orientation, and size) with the image stimuli. To address the aforementioned
issue, we propose a two-stage image reconstruction model called MindDiffuser.
In Stage 1, the VQ-VAE latent representations and the CLIP text embeddings
decoded from fMRI are put into Stable Diffusion, which yields a preliminary
image that contains semantic information. In Stage 2, we utilize the CLIP
visual feature decoded from fMRI as supervisory information, and continually
adjust the two feature vectors decoded in Stage 1 through backpropagation to
align the structural information. The results of both qualitative and
quantitative analyses demonstrate that our model has surpassed the current
state-of-the-art models on Natural Scenes Dataset (NSD). The subsequent
experimental findings corroborate the neurobiological plausibility of the
model, as evidenced by the interpretability of the multimodal feature employed,
which align with the corresponding brain responses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04265">FLIRT: Feedback Loop In-context Red Teaming. (arXiv:2308.04265v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mehrabi_N/0/1/0/all/0/1">Ninareh Mehrabi</a>, <a href="http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1">Palash Goyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Dupuy_C/0/1/0/all/0/1">Christophe Dupuy</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1">Qian Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1">Shalini Ghosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1">Richard Zemel</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1">Kai-Wei Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1">Aram Galstyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1">Rahul Gupta</a></p>
<p>Warning: this paper contains content that may be inappropriate or offensive.
</p>
<p>As generative models become available for public use in various applications,
testing and analyzing vulnerabilities of these models has become a priority.
Here we propose an automatic red teaming framework that evaluates a given model
and exposes its vulnerabilities against unsafe and inappropriate content
generation. Our framework uses in-context learning in a feedback loop to red
team models and trigger them into unsafe content generation. We propose
different in-context attack strategies to automatically learn effective and
diverse adversarial prompts for text-to-image models. Our experiments
demonstrate that compared to baseline approaches, our proposed strategy is
significantly more effective in exposing vulnerabilities in Stable Diffusion
(SD) model, even when the latter is enhanced with safety features. Furthermore,
we demonstrate that the proposed framework is effective for red teaming
text-to-text models, resulting in significantly higher toxic response
generation rate compared to previously reported numbers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04268">Teacher-Student Architecture for Knowledge Distillation: A Survey. (arXiv:2308.04268v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1">Chengming Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1">Dan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Haolun Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Ju Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xue Liu</a></p>
<p>Although Deep neural networks (DNNs) have shown a strong capacity to solve
large-scale problems in many areas, such DNNs are hard to be deployed in
real-world systems due to their voluminous parameters. To tackle this issue,
Teacher-Student architectures were proposed, where simple student networks with
a few parameters can achieve comparable performance to deep teacher networks
with many parameters. Recently, Teacher-Student architectures have been
effectively and widely embraced on various knowledge distillation (KD)
objectives, including knowledge compression, knowledge expansion, knowledge
adaptation, and knowledge enhancement. With the help of Teacher-Student
architectures, current studies are able to achieve multiple distillation
objectives through lightweight and generalized student networks. Different from
existing KD surveys that primarily focus on knowledge compression, this survey
first explores Teacher-Student architectures across multiple distillation
objectives. This survey presents an introduction to various knowledge
representations and their corresponding optimization objectives. Additionally,
we provide a systematic overview of Teacher-Student architectures with
representative learning algorithms and effective distillation schemes. This
survey also summarizes recent applications of Teacher-Student architectures
across multiple purposes, including classification, recognition, generation,
ranking, and regression. Lastly, potential research directions in KD are
investigated, focusing on architecture design, knowledge quality, and
theoretical studies of regression-based learning, respectively. Through this
comprehensive survey, industry practitioners and the academic community can
gain valuable insights and guidelines for effectively designing, learning, and
applying Teacher-Student architectures on various distillation objectives.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04269">Lossy and Lossless (L$^2$) Post-training Model Size Compression. (arXiv:2308.04269v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yumeng Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1">Shihao Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1">Xiuying Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1">Ruihao Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jianlei Yang</a></p>
<p>Deep neural networks have delivered remarkable performance and have been
widely used in various visual tasks. However, their huge size causes
significant inconvenience for transmission and storage. Many previous studies
have explored model size compression. However, these studies often approach
various lossy and lossless compression methods in isolation, leading to
challenges in achieving high compression ratios efficiently. This work proposes
a post-training model size compression method that combines lossy and lossless
compression in a unified way. We first propose a unified parametric weight
transformation, which ensures different lossy compression methods can be
performed jointly in a post-training manner. Then, a dedicated differentiable
counter is introduced to guide the optimization of lossy compression to arrive
at a more suitable point for later lossless compression. Additionally, our
method can easily control a desired global compression ratio and allocate
adaptive ratios for different layers. Finally, our method can achieve a stable
$10\times$ compression ratio without sacrificing accuracy and a $20\times$
compression ratio with minor accuracy loss in a short time. Our code is
available at https://github.com/ModelTC/L2_Compression .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04275">In-Context Alignment: Chat with Vanilla Language Models Before Fine-Tuning. (arXiv:2308.04275v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1">Xiaochuang Han</a></p>
<p>In this note, we explore inference-time alignment through in-context
learning. We consider a vanilla pretrained language model Llama-2 before any
fine-tuning and retrieve an average of 9 demonstration alignment examples when
the model is prompted to follow chat-style instructions. Compared to direct
prompting, the in-context alignment without changing model weights leads to a
7x increase in win-rate w.r.t. the text-davinci-003 model from OpenAI, making
the vanilla language model comparable to strong baselines with alignment
fine-tuning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04292">Engineering LaCAM$^\ast$: Towards Real-Time, Large-Scale, and Near-Optimal Multi-Agent Pathfinding. (arXiv:2308.04292v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Okumura_K/0/1/0/all/0/1">Keisuke Okumura</a></p>
<p>This paper addresses the challenges of real-time, large-scale, and
near-optimal multi-agent pathfinding (MAPF) through enhancements to the
recently proposed LaCAM* algorithm. LaCAM* is a scalable search-based algorithm
that guarantees the eventual finding of optimal solutions for cumulative
transition costs. While it has demonstrated remarkable planning success rates,
surpassing various state-of-the-art MAPF methods, its initial solution quality
is far from optimal, and its convergence speed to the optimum is slow. To
overcome these limitations, this paper introduces several improvement
techniques, partly drawing inspiration from other MAPF methods. We provide
empirical evidence that the fusion of these techniques significantly improves
the solution quality of LaCAM*, thus further pushing the boundaries of MAPF
algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04299">Actor-Critic with variable time discretization via sustained actions. (arXiv:2308.04299v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lyskawa_J/0/1/0/all/0/1">Jakub &#x141;yskawa</a>, <a href="http://arxiv.org/find/cs/1/au:+Wawrzynski_P/0/1/0/all/0/1">Pawe&#x142; Wawrzy&#x144;ski</a></p>
<p>Reinforcement learning (RL) methods work in discrete time. In order to apply
RL to inherently continuous problems like robotic control, a specific time
discretization needs to be defined. This is a choice between sparse time
control, which may be easier to train, and finer time control, which may allow
for better ultimate performance. In this work, we propose SusACER, an
off-policy RL algorithm that combines the advantages of different time
discretization settings. Initially, it operates with sparse time discretization
and gradually switches to a fine one. We analyze the effects of the changing
time discretization in robotic control environments: Ant, HalfCheetah, Hopper,
and Walker2D. In all cases our proposed algorithm outperforms state of the art.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04303">Vehicle Motion Forecasting using Prior Information and Semantic-assisted Occupancy Grid Maps. (arXiv:2308.04303v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Asghar_R/0/1/0/all/0/1">Rabbia Asghar</a>, <a href="http://arxiv.org/find/cs/1/au:+Diaz_Zapata_M/0/1/0/all/0/1">Manuel Diaz-Zapata</a>, <a href="http://arxiv.org/find/cs/1/au:+Rummelhard_L/0/1/0/all/0/1">Lukas Rummelhard</a>, <a href="http://arxiv.org/find/cs/1/au:+Spalanzani_A/0/1/0/all/0/1">Anne Spalanzani</a>, <a href="http://arxiv.org/find/cs/1/au:+Laugier_C/0/1/0/all/0/1">Christian Laugier</a></p>
<p>Motion prediction is a challenging task for autonomous vehicles due to
uncertainty in the sensor data, the non-deterministic nature of future, and
complex behavior of agents. In this paper, we tackle this problem by
representing the scene as dynamic occupancy grid maps (DOGMs), associating
semantic labels to the occupied cells and incorporating map information. We
propose a novel framework that combines deep-learning-based spatio-temporal and
probabilistic approaches to predict vehicle behaviors.Contrary to the
conventional OGM prediction methods, evaluation of our work is conducted
against the ground truth annotations. We experiment and validate our results on
real-world NuScenes dataset and show that our model shows superior ability to
predict both static and dynamic vehicles compared to OGM predictions.
Furthermore, we perform an ablation study and assess the role of semantic
labels and map in the architecture.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04312">Interpretable Goal-Based model for Vehicle Trajectory Prediction in Interactive Scenarios. (arXiv:2308.04312v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ghoul_A/0/1/0/all/0/1">Amina Ghoul</a>, <a href="http://arxiv.org/find/cs/1/au:+Yahiaoui_I/0/1/0/all/0/1">Itheri Yahiaoui</a>, <a href="http://arxiv.org/find/cs/1/au:+Verroust_Blondet_A/0/1/0/all/0/1">Anne Verroust-Blondet</a>, <a href="http://arxiv.org/find/cs/1/au:+Nashashibi_F/0/1/0/all/0/1">Fawzi Nashashibi</a></p>
<p>The abilities to understand the social interaction behaviors between a
vehicle and its surroundings while predicting its trajectory in an urban
environment are critical for road safety in autonomous driving. Social
interactions are hard to explain because of their uncertainty. In recent years,
neural network-based methods have been widely used for trajectory prediction
and have been shown to outperform hand-crafted methods. However, these methods
suffer from their lack of interpretability. In order to overcome this
limitation, we combine the interpretability of a discrete choice model with the
high accuracy of a neural network-based model for the task of vehicle
trajectory prediction in an interactive environment. We implement and evaluate
our model using the INTERACTION dataset and demonstrate the effectiveness of
our proposed architecture to explain its predictions without compromising the
accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04313">Apple Vision Pro for Healthcare: &quot;The Ultimate Display&quot;?. (arXiv:2308.04313v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Egger_J/0/1/0/all/0/1">Jan Egger</a>, <a href="http://arxiv.org/find/cs/1/au:+Gsaxner_C/0/1/0/all/0/1">Christina Gsaxner</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xiaojun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1">Jiang Bian</a>, <a href="http://arxiv.org/find/cs/1/au:+Kleesiek_J/0/1/0/all/0/1">Jens Kleesiek</a>, <a href="http://arxiv.org/find/cs/1/au:+Puladi_B/0/1/0/all/0/1">Behrus Puladi</a></p>
<p>At the Worldwide Developers Conference (WWDC) in June 2023, Apple introduced
the Vision Pro. The Vision Pro is a Mixed Reality (MR) headset, more
specifically it is a Virtual Reality (VR) device with an additional Video
See-Through (VST) capability. The VST capability turns the Vision Pro also into
an Augmented Reality (AR) device. The AR feature is enabled by streaming the
real world via cameras to the (VR) screens in front of the user's eyes. This is
of course not unique and similar to other devices, like the Varjo XR-3.
Nevertheless, the Vision Pro has some interesting features, like an inside-out
screen that can show the headset wearers' eyes to "outsiders" or a button on
the top, called "Digital Crown", that allows you to seamlessly blend digital
content with your physical space by turning it. In addition, it is untethered,
except for the cable to the battery, which makes the headset more agile,
compared to the Varjo XR-3. This could actually come closer to the "Ultimate
Display", which Ivan Sutherland had already sketched in 1965. Not available to
the public yet, like the Ultimate Display, we want to take a look into the
crystal ball in this perspective to see if it can overcome some clinical
challenges that - especially - AR still faces in the medical domain, but also
go beyond and discuss if the Vision Pro could support clinicians in essential
tasks to spend more time with their patients.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04314">Cooperative Multi-agent Bandits: Distributed Algorithms with Optimal Individual Regret and Constant Communication Costs. (arXiv:2308.04314v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Lin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xuchuang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hajiesmaili_M/0/1/0/all/0/1">Mohammad Hajiesmaili</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lijun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lui_J/0/1/0/all/0/1">John C.S. Lui</a>, <a href="http://arxiv.org/find/cs/1/au:+Towsley_D/0/1/0/all/0/1">Don Towsley</a></p>
<p>Recently, there has been extensive study of cooperative multi-agent
multi-armed bandits where a set of distributed agents cooperatively play the
same multi-armed bandit game. The goal is to develop bandit algorithms with the
optimal group and individual regrets and low communication between agents. The
prior work tackled this problem using two paradigms: leader-follower and fully
distributed algorithms. Prior algorithms in both paradigms achieve the optimal
group regret. The leader-follower algorithms achieve constant communication
costs but fail to achieve optimal individual regrets. The state-of-the-art
fully distributed algorithms achieve optimal individual regrets but fail to
achieve constant communication costs. This paper presents a simple yet
effective communication policy and integrates it into a learning algorithm for
cooperative bandits. Our algorithm achieves the best of both paradigms: optimal
individual regret and constant communication costs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04337">Pengembangan Model untuk Mendeteksi Kerusakan pada Terumbu Karang dengan Klasifikasi Citra. (arXiv:2308.04337v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Muhammad_F/0/1/0/all/0/1">Fadhil Muhammad</a>, <a href="http://arxiv.org/find/cs/1/au:+Elfandra_A/0/1/0/all/0/1">Alif Bintang Elfandra</a>, <a href="http://arxiv.org/find/cs/1/au:+Amin_I/0/1/0/all/0/1">Iqbal Pahlevi Amin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wicaksono_A/0/1/0/all/0/1">Alfan Farizki Wicaksono</a></p>
<p>The abundant biodiversity of coral reefs in Indonesian waters is a valuable
asset that needs to be preserved. Rapid climate change and uncontrolled human
activities have led to the degradation of coral reef ecosystems, including
coral bleaching, which is a critical indicator of coral health conditions.
Therefore, this research aims to develop an accurate classification model to
distinguish between healthy corals and corals experiencing bleaching. This
study utilizes a specialized dataset consisting of 923 images collected from
Flickr using the Flickr API. The dataset comprises two distinct classes:
healthy corals (438 images) and bleached corals (485 images). These images have
been resized to a maximum of 300 pixels in width or height, whichever is
larger, to maintain consistent sizes across the dataset.
</p>
<p>The method employed in this research involves the use of machine learning
models, particularly convolutional neural networks (CNN), to recognize and
differentiate visual patterns associated with healthy and bleached corals. In
this context, the dataset can be used to train and test various classification
models to achieve optimal results. By leveraging the ResNet model, it was found
that a from-scratch ResNet model can outperform pretrained models in terms of
precision and accuracy. The success in developing accurate classification
models will greatly benefit researchers and marine biologists in gaining a
better understanding of coral reef health. These models can also be employed to
monitor changes in the coral reef environment, thereby making a significant
contribution to conservation and ecosystem restoration efforts that have
far-reaching impacts on life.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04340">A Lightweight and Accurate Face Detection Algorithm Based on Retinaface. (arXiv:2308.04340v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Baozhu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Hewei Yu</a></p>
<p>In this paper, we propose a lightweight and accurate face detection algorithm
LAFD (Light and accurate face detection) based on Retinaface. Backbone network
in the algorithm is a modified MobileNetV3 network which adjusts the size of
the convolution kernel, the channel expansion multiplier of the inverted
residuals block and the use of the SE attention mechanism. Deformable
convolution network(DCN) is introduced in the context module and the algorithm
uses focal loss function instead of cross-entropy loss function as the
classification loss function of the model. The test results on the WIDERFACE
dataset indicate that the average accuracy of LAFD is 94.1%, 92.2% and 82.1%
for the "easy", "medium" and "hard" validation subsets respectively with an
improvement of 3.4%, 4.0% and 8.3% compared to Retinaface and 3.1%, 4.1% and
4.1% higher than the well-performing lightweight model, LFFD. If the input
image is pre-processed and scaled to 1560px in length or 1200px in width, the
model achieves an average accuracy of 86.2% on the 'hard' validation subset.
The model is lightweight, with a size of only 10.2MB.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04356">Learning Unbiased Image Segmentation: A Case Study with Plain Knee Radiographs. (arXiv:2308.04356v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Littlefield_N/0/1/0/all/0/1">Nickolas Littlefield</a>, <a href="http://arxiv.org/find/cs/1/au:+Plate_J/0/1/0/all/0/1">Johannes F. Plate</a>, <a href="http://arxiv.org/find/cs/1/au:+Weiss_K/0/1/0/all/0/1">Kurt R. Weiss</a>, <a href="http://arxiv.org/find/cs/1/au:+Lohse_I/0/1/0/all/0/1">Ines Lohse</a>, <a href="http://arxiv.org/find/cs/1/au:+Chhabra_A/0/1/0/all/0/1">Avani Chhabra</a>, <a href="http://arxiv.org/find/cs/1/au:+Siddiqui_I/0/1/0/all/0/1">Ismaeel A. Siddiqui</a>, <a href="http://arxiv.org/find/cs/1/au:+Menezes_Z/0/1/0/all/0/1">Zoe Menezes</a>, <a href="http://arxiv.org/find/cs/1/au:+Mastorakos_G/0/1/0/all/0/1">George Mastorakos</a>, <a href="http://arxiv.org/find/cs/1/au:+Thakar_S/0/1/0/all/0/1">Sakshi Mehul Thakar</a>, <a href="http://arxiv.org/find/cs/1/au:+Abedian_M/0/1/0/all/0/1">Mehrnaz Abedian</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1">Matthew F. Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Carlson_L/0/1/0/all/0/1">Luke A. Carlson</a>, <a href="http://arxiv.org/find/cs/1/au:+Moradi_H/0/1/0/all/0/1">Hamidreza Moradi</a>, <a href="http://arxiv.org/find/cs/1/au:+Amirian_S/0/1/0/all/0/1">Soheyla Amirian</a>, <a href="http://arxiv.org/find/cs/1/au:+Tafti_A/0/1/0/all/0/1">Ahmad P. Tafti</a></p>
<p>Automatic segmentation of knee bony anatomy is essential in orthopedics, and
it has been around for several years in both pre-operative and post-operative
settings. While deep learning algorithms have demonstrated exceptional
performance in medical image analysis, the assessment of fairness and potential
biases within these models remains limited. This study aims to revisit deep
learning-powered knee-bony anatomy segmentation using plain radiographs to
uncover visible gender and racial biases. The current contribution offers the
potential to advance our understanding of biases, and it provides practical
insights for researchers and practitioners in medical imaging. The proposed
mitigation strategies mitigate gender and racial biases, ensuring fair and
unbiased segmentation results. Furthermore, this work promotes equal access to
accurate diagnoses and treatment outcomes for diverse patient populations,
fostering equitable and inclusive healthcare provision.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04371">Cumulative Reasoning With Large Language Models. (arXiv:2308.04371v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yifan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jingqin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1">Yang Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1">Andrew Chi-Chih Yao</a></p>
<p>While language models are powerful and versatile, they often fail to address
highly complex problems. This is because solving complex problems requires
deliberate thinking, which has been only minimally guided during training. In
this paper, we propose a new method called Cumulative Reasoning (CR), which
employs language models in a cumulative and iterative manner to emulate human
thought processes. By decomposing tasks into smaller components, \ournameb
streamlines the problem-solving process, rendering it both more manageable and
effective. For logical inference tasks, CR consistently outperforms existing
methods with an improvement up to 9.3\%, and achieves the astonishing accuracy
of 98.04\% on the curated FOLIO wiki dataset. In the context of the Game of 24,
CR achieves an accuracy of 94\%, which signifies a substantial enhancement of
20\% over the previous state-of-the-art method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04372">Some Options for Instantiation of Bipolar Argument Graphs with Deductive Arguments. (arXiv:2308.04372v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hunter_A/0/1/0/all/0/1">Anthony Hunter</a></p>
<p>Argument graphs provide an abstract representation of an argumentative
situation. A bipolar argument graph is a directed graph where each node denotes
an argument, and each arc denotes the influence of one argument on another.
Here we assume that the influence is supporting, attacking, or ambiguous. In a
bipolar argument graph, each argument is atomic and so it has no internal
structure. Yet to better understand the nature of the individual arguments, and
how they interact, it is important to consider their internal structure. To
address this need, this paper presents a framework based on the use of logical
arguments to instantiate bipolar argument graphs, and a set of possible
constraints on instantiating arguments that take into account the internal
structure of the arguments, and the types of relationship between arguments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04375">Understanding the Effect of Counterfactual Explanations on Trust and Reliance on AI for Human-AI Collaborative Clinical Decision Making. (arXiv:2308.04375v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1">Min Hun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Chew_C/0/1/0/all/0/1">Chong Jun Chew</a></p>
<p>Artificial intelligence (AI) is increasingly being considered to assist human
decision-making in high-stake domains (e.g. health). However, researchers have
discussed an issue that humans can over-rely on wrong suggestions of the AI
model instead of achieving human AI complementary performance. In this work, we
utilized salient feature explanations along with what-if, counterfactual
explanations to make humans review AI suggestions more analytically to reduce
overreliance on AI and explored the effect of these explanations on trust and
reliance on AI during clinical decision-making. We conducted an experiment with
seven therapists and ten laypersons on the task of assessing post-stroke
survivors' quality of motion, and analyzed their performance, agreement level
on the task, and reliance on AI without and with two types of AI explanations.
Our results showed that the AI model with both salient features and
counterfactual explanations assisted therapists and laypersons to improve their
performance and agreement level on the task when `right' AI outputs are
presented. While both therapists and laypersons over-relied on `wrong' AI
outputs, counterfactual explanations assisted both therapists and laypersons to
reduce their over-reliance on `wrong' AI outputs by 21\% compared to salient
feature explanations. Specifically, laypersons had higher performance degrades
by 18.0 f1-score with salient feature explanations and 14.0 f1-score with
counterfactual explanations than therapists with performance degrades of 8.6
and 2.8 f1-scores respectively. Our work discusses the potential of
counterfactual explanations to better estimate the accuracy of an AI model and
reduce over-reliance on `wrong' AI outputs and implications for improving
human-AI collaborative decision-making.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04396">Event Abstraction for Enterprise Collaboration Systems to Support Social Process Mining. (arXiv:2308.04396v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Blatt_J/0/1/0/all/0/1">Jonas Blatt</a>, <a href="http://arxiv.org/find/cs/1/au:+Delfmann_P/0/1/0/all/0/1">Patrick Delfmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Schubert_P/0/1/0/all/0/1">Petra Schubert</a></p>
<p>One aim of Process Mining (PM) is the discovery of process models from event
logs of information systems. PM has been successfully applied to
process-oriented enterprise systems but is less suited for communication- and
document-oriented Enterprise Collaboration Systems (ECS). ECS event logs are
very fine-granular and PM applied to their logs results in spaghetti models. A
common solution for this is event abstraction, i.e., converting low-level logs
into more abstract high-level logs before running discovery algorithms. ECS
logs come with special characteristics that have so far not been fully
addressed by existing event abstraction approaches. We aim to close this gap
with a tailored ECS event abstraction (ECSEA) approach that trains a model by
comparing recorded actual user activities (high-level traces) with the
system-generated low-level traces (extracted from the ECS). The model allows us
to automatically convert future low-level traces into an abstracted high-level
log that can be used for PM. Our evaluation shows that the algorithm produces
accurate results. ECSEA is a preprocessing method that is essential for the
interpretation of collaborative work activity in ECS, which we call Social
Process Mining.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04399">Fine-Tuning Games: Bargaining and Adaptation for General-Purpose Models. (arXiv:2308.04399v1 [cs.GT])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Laufer_B/0/1/0/all/0/1">Benjamin Laufer</a>, <a href="http://arxiv.org/find/cs/1/au:+Kleinberg_J/0/1/0/all/0/1">Jon Kleinberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Heidari_H/0/1/0/all/0/1">Hoda Heidari</a></p>
<p>Major advances in Machine Learning (ML) and Artificial Intelligence (AI)
increasingly take the form of developing and releasing general-purpose models.
These models are designed to be adapted by other businesses and agencies to
perform a particular, domain-specific function. This process has become known
as adaptation or fine-tuning. This paper offers a model of the fine-tuning
process where a Generalist brings the technological product (here an ML model)
to a certain level of performance, and one or more Domain-specialist(s) adapts
it for use in a particular domain. Both entities are profit-seeking and incur
costs when they invest in the technology, and they must reach a bargaining
agreement on how to share the revenue for the technology to reach the market.
For a relatively general class of cost and revenue functions, we characterize
the conditions under which the fine-tuning game yields a profit-sharing
solution. We observe that any potential domain-specialization will either
contribute, free-ride, or abstain in their uptake of the technology, and we
provide conditions yielding these different strategies. We show how methods
based on bargaining solutions and sub-game perfect equilibria provide insights
into the strategic behavior of firms in these types of interactions, and we
find that profit-sharing can still arise even when one firm has significantly
higher costs than another. We also provide methods for identifying
Pareto-optimal bargaining arrangements for a general set of utility functions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04412">Probabilistic Invariant Learning with Randomized Linear Classifiers. (arXiv:2308.04412v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cotta_L/0/1/0/all/0/1">Leonardo Cotta</a>, <a href="http://arxiv.org/find/cs/1/au:+Yehuda_G/0/1/0/all/0/1">Gal Yehuda</a>, <a href="http://arxiv.org/find/cs/1/au:+Schuster_A/0/1/0/all/0/1">Assaf Schuster</a>, <a href="http://arxiv.org/find/cs/1/au:+Maddison_C/0/1/0/all/0/1">Chris J. Maddison</a></p>
<p>Designing models that are both expressive and preserve known invariances of
tasks is an increasingly hard problem. Existing solutions tradeoff invariance
for computational or memory resources. In this work, we show how to leverage
randomness and design models that are both expressive and invariant but use
less resources. Inspired by randomized algorithms, our key insight is that
accepting probabilistic notions of universal approximation and invariance can
reduce our resource requirements. More specifically, we propose a class of
binary classification models called Randomized Linear Classifiers (RLCs). We
give parameter and sample size conditions in which RLCs can, with high
probability, approximate any (smooth) function while preserving invariance to
compact group transformations. Leveraging this result, we design three RLCs
that are provably probabilistic invariant for classification tasks over sets,
graphs, and spherical data. We show how these models can achieve probabilistic
invariance and universality using less resources than (deterministic) neural
networks and their invariant counterparts. Finally, we empirically demonstrate
the benefits of this new class of models on invariant tasks where deterministic
invariant neural networks are known to struggle.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04419">Stock Market Price Prediction: A Hybrid LSTM and Sequential Self-Attention based Approach. (arXiv:2308.04419v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pardeshi_K/0/1/0/all/0/1">Karan Pardeshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gill_S/0/1/0/all/0/1">Sukhpal Singh Gill</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdelmoniem_A/0/1/0/all/0/1">Ahmed M. Abdelmoniem</a></p>
<p>One of the most enticing research areas is the stock market, and projecting
stock prices may help investors profit by making the best decisions at the
correct time. Deep learning strategies have emerged as a critical technique in
the field of the financial market. The stock market is impacted due to two
aspects, one is the geo-political, social and global events on the bases of
which the price trends could be affected. Meanwhile, the second aspect purely
focuses on historical price trends and seasonality, allowing us to forecast
stock prices. In this paper, our aim is to focus on the second aspect and build
a model that predicts future prices with minimal errors. In order to provide
better prediction results of stock price, we propose a new model named Long
Short-Term Memory (LSTM) with Sequential Self-Attention Mechanism (LSTM-SSAM).
Finally, we conduct extensive experiments on the three stock datasets: SBIN,
HDFCBANK, and BANKBARODA. The experimental results prove the effectiveness and
feasibility of the proposed model compared to existing models. The experimental
findings demonstrate that the root-mean-squared error (RMSE), and R-square (R2)
evaluation indicators are giving the best results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04430">SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore. (arXiv:2308.04430v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1">Sewon Min</a>, <a href="http://arxiv.org/find/cs/1/au:+Gururangan_S/0/1/0/all/0/1">Suchin Gururangan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wallace_E/0/1/0/all/0/1">Eric Wallace</a>, <a href="http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1">Hannaneh Hajishirzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1">Noah A. Smith</a>, <a href="http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1">Luke Zettlemoyer</a></p>
<p>The legality of training language models (LMs) on copyrighted or otherwise
restricted data is under intense debate. However, as we show, model performance
significantly degrades if trained only on low-risk text (e.g., out-of-copyright
books or government documents), due to its limited size and domain coverage. We
present SILO, a new language model that manages this risk-performance tradeoff
during inference. SILO is built by (1) training a parametric LM on Open License
Corpus (OLC), a new corpus we curate with 228B tokens of public domain and
permissively licensed text and (2) augmenting it with a more general and easily
modifiable nonparametric datastore (e.g., containing copyrighted books or news)
that is only queried during inference. The datastore allows use of high-risk
data without training on it, supports sentence-level data attribution, and
enables data producers to opt out from the model by removing content from the
store. These capabilities can foster compliance with data-use regulations such
as the fair use doctrine in the United States and the GDPR in the European
Union. Our experiments show that the parametric LM struggles on domains not
covered by OLC. However, access to the datastore greatly improves out of domain
performance, closing 90% of the performance gap with an LM trained on the Pile,
a more diverse corpus with mostly high-risk text. We also analyze which
nonparametric approach works best, where the remaining errors lie, and how
performance scales with datastore size. Our results suggest that it is possible
to build high quality language models while mitigating their legal risk.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.01186">ORC: Network Group-based Knowledge Distillation using Online Role Change. (arXiv:2206.01186v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1">Junyong Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1">Hyeon Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheung_S/0/1/0/all/0/1">Seokhwa Cheung</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_W/0/1/0/all/0/1">Wonjun Hwang</a></p>
<p>In knowledge distillation, since a single, omnipotent teacher network cannot
solve all problems, multiple teacher-based knowledge distillations have been
studied recently. However, sometimes their improvements are not as good as
expected because some immature teachers may transfer the false knowledge to the
student. In this paper, to overcome this limitation and take the efficacy of
the multiple networks, we divide the multiple networks into teacher and student
groups, respectively. That is, the student group is a set of immature networks
that require learning the teacher's knowledge, while the teacher group consists
of the selected networks that are capable of teaching successfully. We propose
our online role change strategy where the top-ranked networks in the student
group are able to promote to the teacher group at every iteration. After
training the teacher group using the error samples of the student group to
refine the teacher group's knowledge, we transfer the collaborative knowledge
from the teacher group to the student group successfully. We verify the
superiority of the proposed method on CIFAR-10, CIFAR-100, and ImageNet which
achieves high performance. We further show the generality of our method with
various backbone architectures such as ResNet, WRN, VGG, Mobilenet, and
Shufflenet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.11699">Semi-supervised Deep Multi-view Stereo. (arXiv:2207.11699v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hongbin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhipeng Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Weitao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1">Baigui Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1">Wenxiong Kang</a></p>
<p>Significant progress has been witnessed in learning-based Multi-view Stereo
(MVS) under supervised and unsupervised settings. To combine their respective
merits in accuracy and completeness, meantime reducing the demand for expensive
labeled data, this paper explores the problem of learning-based MVS in a
semi-supervised setting that only a tiny part of the MVS data is attached with
dense depth ground truth. However, due to huge variation of scenarios and
flexible settings in views, it may break the basic assumption in classic
semi-supervised learning, that unlabeled data and labeled data share the same
label space and data distribution, named as semi-supervised distribution-gap
ambiguity in the MVS problem. To handle these issues, we propose a novel
semi-supervised distribution-augmented MVS framework, namely SDA-MVS. For the
simple case that the basic assumption works in MVS data, consistency
regularization encourages the model predictions to be consistent between
original sample and randomly augmented sample. For further troublesome case
that the basic assumption is conflicted in MVS data, we propose a novel style
consistency loss to alleviate the negative effect caused by the distribution
gap. The visual style of unlabeled sample is transferred to labeled sample to
shrink the gap, and the model prediction of generated sample is further
supervised with the label in original labeled sample. The experimental results
in semi-supervised settings of multiple MVS datasets show the superior
performance of the proposed method. With the same settings in backbone network,
our proposed SDA-MVS outperforms its fully-supervised and unsupervised
baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.07774">Learning To Rank Diversely At Airbnb. (arXiv:2210.07774v3 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Haldar_M/0/1/0/all/0/1">Malay Haldar</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdool_M/0/1/0/all/0/1">Mustafa Abdool</a>, <a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1">Liwei He</a>, <a href="http://arxiv.org/find/cs/1/au:+Davis_D/0/1/0/all/0/1">Dillon Davis</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1">Huiji Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Katariya_S/0/1/0/all/0/1">Sanjeev Katariya</a></p>
<p>Airbnb is a two-sided marketplace, bringing together hosts who own listings
for rent, with prospective guests from around the globe. Applying neural
network-based learning to rank techniques has led to significant improvements
in matching guests with hosts. These improvements in ranking were driven by a
core strategy: order the listings by their estimated booking probabilities,
then iterate on techniques to make these booking probability estimates more and
more accurate. Embedded implicitly in this strategy was an assumption that the
booking probability of a listing could be determined independently of other
listings in search results. In this paper we discuss how this assumption,
pervasive throughout the commonly-used learning to rank frameworks, is false.
We provide a theoretical foundation correcting this assumption, followed by
efficient neural network architectures based on the theory. Explicitly
accounting for possible similarities between listings, and reducing them to
diversify the search results generated strong positive impact. We discuss these
metric wins as part of the online A/B tests of the theory. Our method provides
a practical way to diversify search results for large-scale production ranking
systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.01635">SPTS v2: Single-Point Scene Text Spotting. (arXiv:2301.01635v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuliang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiaxin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1">Dezhi Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1">Mingxin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jingqun Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1">Can Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1">Dahua Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1">Chunhua Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1">Xiang Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1">Lianwen Jin</a></p>
<p>End-to-end scene text spotting has made significant progress due to its
intrinsic synergy between text detection and recognition. Previous methods
commonly regard manual annotations such as horizontal rectangles, rotated
rectangles, quadrangles, and polygons as a prerequisite, which are much more
expensive than using single-point. Our new framework, SPTS v2, allows us to
train high-performing text-spotting models using a single-point annotation.
SPTS v2 reserves the advantage of the auto-regressive Transformer with an
Instance Assignment Decoder (IAD) through sequentially predicting the center
points of all text instances inside the same predicting sequence, while with a
Parallel Recognition Decoder (PRD) for text recognition in parallel. These two
decoders share the same parameters and are interactively connected with a
simple but effective information transmission process to pass the gradient and
information. Comprehensive experiments on various existing benchmark datasets
demonstrate the SPTS v2 can outperform previous state-of-the-art single-point
text spotters with fewer parameters while achieving 19$\times$ faster inference
speed. Within the context of our SPTS v2 framework, our experiments suggest a
potential preference for single-point representation in scene text spotting
when compared to other representations. Such an attempt provides a significant
opportunity for scene text spotting applications beyond the realms of existing
paradigms. Code is available at https://github.com/Yuliang-Liu/SPTSv2.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.00286">Treat Different Negatives Differently: Enriching Loss Functions with Domain and Range Constraints for Link Prediction. (arXiv:2303.00286v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hubert_N/0/1/0/all/0/1">Nicolas Hubert</a>, <a href="http://arxiv.org/find/cs/1/au:+Monnin_P/0/1/0/all/0/1">Pierre Monnin</a>, <a href="http://arxiv.org/find/cs/1/au:+Brun_A/0/1/0/all/0/1">Armelle Brun</a>, <a href="http://arxiv.org/find/cs/1/au:+Monticolo_D/0/1/0/all/0/1">Davy Monticolo</a></p>
<p>Knowledge graph embedding models (KGEMs) are used for various tasks related
to knowledge graphs (KGs), including link prediction. They are trained with
loss functions that are computed considering a batch of scored triples and
their corresponding labels. Traditional approaches consider the label of a
triple to be either true or false. However, recent works suggest that all
negative triples should not be valued equally. In line with this recent
assumption, we posit that negative triples that are semantically valid w.r.t.
domain and range constraints might be high-quality negative triples. As such,
loss functions should treat them differently from semantically invalid negative
ones. To this aim, we propose semantic-driven versions for the three main loss
functions for link prediction. In an extensive and controlled experimental
setting, we show that the proposed loss functions systematically provide
satisfying results on three public benchmark KGs underpinned with different
schemas, which demonstrates both the generality and superiority of our proposed
approach. In fact, the proposed loss functions do (1) lead to better MRR and
Hits@10 values, (2) drive KGEMs towards better semantic awareness as measured
by the Sem@K metric. This highlights that semantic information globally
improves KGEMs, and thus should be incorporated into loss functions. Domains
and ranges of relations being largely available in schema-defined KGs, this
makes our approach both beneficial and widely usable in practice.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.16459">GNNBuilder: An Automated Framework for Generic Graph Neural Network Accelerator Generation, Simulation, and Optimization. (arXiv:2303.16459v2 [cs.AR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abi_Karam_S/0/1/0/all/0/1">Stefan Abi-Karam</a>, <a href="http://arxiv.org/find/cs/1/au:+Hao_C/0/1/0/all/0/1">Cong Hao</a></p>
<p>There are plenty of graph neural network (GNN) accelerators being proposed.
However, they highly rely on users' hardware expertise and are usually
optimized for one specific GNN model, making them challenging for practical
use. Therefore, in this work, we propose GNNBuilder, the first automated,
generic, end-to-end GNN accelerator generation framework. It features four
advantages: (1) GNNBuilder can automatically generate GNN accelerators for a
wide range of GNN models arbitrarily defined by users; (2) GNNBuilder takes
standard PyTorch programming interface, introducing zero overhead for algorithm
developers; (3) GNNBuilder supports end-to-end code generation, simulation,
accelerator optimization, and hardware deployment, realizing a push-button
fashion for GNN accelerator design; (4) GNNBuilder is equipped with accurate
performance models of its generated accelerator, enabling fast and flexible
design space exploration (DSE). In the experiments, first, we show that our
accelerator performance model has errors within $36\%$ for latency prediction
and $18\%$ for BRAM count prediction. Second, we show that our generated
accelerators can outperform CPU by $6.33\times$ and GPU by $6.87\times$. This
framework is open-source, and the code is available at
https://github.com/sharc-lab/gnn-builder.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.08275">Implementing Responsible AI: Tensions and Trade-Offs Between Ethics Aspects. (arXiv:2304.08275v3 [cs.CY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sanderson_C/0/1/0/all/0/1">Conrad Sanderson</a>, <a href="http://arxiv.org/find/cs/1/au:+Douglas_D/0/1/0/all/0/1">David Douglas</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1">Qinghua Lu</a></p>
<p>Many sets of ethics principles for responsible AI have been proposed to allay
concerns about misuse and abuse of AI/ML systems. The underlying aspects of
such sets of principles include privacy, accuracy, fairness, robustness,
explainability, and transparency. However, there are potential tensions between
these aspects that pose difficulties for AI/ML developers seeking to follow
these principles. For example, increasing the accuracy of an AI/ML system may
reduce its explainability. As part of the ongoing effort to operationalise the
principles into practice, in this work we compile and discuss a catalogue of 10
notable tensions, trade-offs and other interactions between the underlying
aspects. We primarily focus on two-sided interactions, drawing on support
spread across a diverse literature. This catalogue can be helpful in raising
awareness of the possible interactions between aspects of ethics principles, as
well as facilitating well-supported judgements by the designers and developers
of AI/ML systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.02640">Towards Causal Representation Learning and Deconfounding from Indefinite Data. (arXiv:2305.02640v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xinyu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1">Qing Yang</a></p>
<p>We redefine causal data from two novel perspectives: the number of causal
skeletons and the dimension of causal variables, thereby proposing three data
paradigms. Among them, the indefinite data (like dialogues or video sources) is
characterized by multi-skeleton structures and multi-value variables. Multi
skeletons induce low sample utilization, and multi values induce incapability
of the distribution assumption, both leading to the fact that learning causal
representation from indefinite data is, as of yet, largely unexplored. We
design the causal strength variational model to settle down these two problems.
Specifically, we leverage the causal strength instead of independent noise as
the latent variable to construct evidence lower bound. By this design ethos,
The causal strengths of different skeletons are regarded as a distribution and
can be expressed as a single-valued causal graph matrix. Moreover, considering
the latent confounders, we disentangle the causal graph G into two relation
subgraphs O and C. O contains pure relations between observed variables, while
C represents the relations from latent variables to observed variables. We
implement the above designs as a dynamic variational inference model, tailored
to learn causal representation from indefinite data under latent confounding.
Finally, we conduct comprehensive experiments on synthetic and real-world data
to demonstrate the effectiveness of our method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13452">Measuring and Modeling Physical Intrinsic Motivation. (arXiv:2305.13452v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Martinez_J/0/1/0/all/0/1">Julio Martinez</a>, <a href="http://arxiv.org/find/cs/1/au:+Binder_F/0/1/0/all/0/1">Felix Binder</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haoliang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Haber_N/0/1/0/all/0/1">Nick Haber</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1">Judith Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yamins_D/0/1/0/all/0/1">Daniel L. K. Yamins</a></p>
<p>Humans are interactive agents driven to seek out situations with interesting
physical dynamics. Here we formalize the functional form of physical intrinsic
motivation. We first collect ratings of how interesting humans find a variety
of physics scenarios. We then model human interestingness responses by
implementing various hypotheses of intrinsic motivation including models that
rely on simple scene features to models that depend on forward physics
prediction. We find that the single best predictor of human responses is
adversarial reward, a model derived from physical prediction loss. We also find
that simple scene feature models do not generalize their prediction of human
responses across all scenarios. Finally, linearly combining the adversarial
model with the number of collisions in a scene leads to the greatest
improvement in predictivity of human responses, suggesting humans are driven
towards scenarios that result in high information gain and physical activity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.01941">AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap. (arXiv:2306.01941v2 [cs.HC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1">Q. Vera Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Vaughan_J/0/1/0/all/0/1">Jennifer Wortman Vaughan</a></p>
<p>The rise of powerful large language models (LLMs) brings about tremendous
opportunities for innovation but also looming risks for individuals and society
at large. We have reached a pivotal moment for ensuring that LLMs and
LLM-infused applications are developed and deployed responsibly. However, a
central pillar of responsible AI -- transparency -- is largely missing from the
current discourse around LLMs. It is paramount to pursue new approaches to
provide transparency for LLMs, and years of research at the intersection of AI
and human-computer interaction (HCI) highlight that we must do so with a
human-centered perspective: Transparency is fundamentally about supporting
appropriate human understanding, and this understanding is sought by different
stakeholders with different goals in different contexts. In this new era of
LLMs, we must develop and design approaches to transparency by considering the
needs of stakeholders in the emerging LLM ecosystem, the novel types of
LLM-infused applications being built, and the new usage patterns and challenges
around LLMs, all while building on lessons learned about how people process,
interact with, and make use of information. We reflect on the unique challenges
that arise in providing transparency for LLMs, along with lessons learned from
HCI and responsible AI research that has taken a human-centered perspective on
AI transparency. We then lay out four common approaches that the community has
taken to achieve transparency -- model reporting, publishing evaluation
results, providing explanations, and communicating uncertainty -- and call out
open questions around how these approaches may or may not be applied to LLMs.
We hope this provides a starting point for discussion and a useful roadmap for
future research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.02864">Leveraging Large Language Models for Topic Classification in the Domain of Public Affairs. (arXiv:2306.02864v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pena_A/0/1/0/all/0/1">Alejandro Pe&#xf1;a</a>, <a href="http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1">Aythami Morales</a>, <a href="http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1">Julian Fierrez</a>, <a href="http://arxiv.org/find/cs/1/au:+Serna_I/0/1/0/all/0/1">Ignacio Serna</a>, <a href="http://arxiv.org/find/cs/1/au:+Ortega_Garcia_J/0/1/0/all/0/1">Javier Ortega-Garcia</a>, <a href="http://arxiv.org/find/cs/1/au:+Puente_I/0/1/0/all/0/1">I&#xf1;igo Puente</a>, <a href="http://arxiv.org/find/cs/1/au:+Cordova_J/0/1/0/all/0/1">Jorge Cordova</a>, <a href="http://arxiv.org/find/cs/1/au:+Cordova_G/0/1/0/all/0/1">Gonzalo Cordova</a></p>
<p>The analysis of public affairs documents is crucial for citizens as it
promotes transparency, accountability, and informed decision-making. It allows
citizens to understand government policies, participate in public discourse,
and hold representatives accountable. This is crucial, and sometimes a matter
of life or death, for companies whose operation depend on certain regulations.
Large Language Models (LLMs) have the potential to greatly enhance the analysis
of public affairs documents by effectively processing and understanding the
complex language used in such documents. In this work, we analyze the
performance of LLMs in classifying public affairs documents. As a natural
multi-label task, the classification of these documents presents important
challenges. In this work, we use a regex-powered tool to collect a database of
public affairs documents with more than 33K samples and 22.5M tokens. Our
experiments assess the performance of 4 different Spanish LLMs to classify up
to 30 different topics in the data in different configurations. The results
shows that LLMs can be of great use to process domain-specific documents, such
as those in the domain of public affairs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.03082">InstructZero: Efficient Instruction Optimization for Black-Box Large Language Models. (arXiv:2306.03082v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lichang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiuhai Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1">Tom Goldstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Heng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1">Tianyi Zhou</a></p>
<p>Large language models~(LLMs) are instruction followers, but it can be
challenging to find the best instruction for different situations, especially
for black-box LLMs on which backpropagation is forbidden. Instead of directly
optimizing the discrete instruction, we optimize a low-dimensional soft prompt
applied to an open-source LLM to generate the instruction for the black-box
LLM. On each iteration of the proposed method, which we call InstructZero, a
soft prompt is converted into an instruction using the open-source LLM, which
is then submitted to the black-box LLM for zero-shot evaluation, and the
performance is sent to Bayesian optimization to produce new soft prompts
improving the zero-shot performance. We evaluate InstructZero on different
combinations of open-source LLMs and APIs including Vicuna and ChatGPT. Our
results show that InstructZero outperforms SOTA auto-instruction methods across
a variety of downstream tasks. Our code and data are publicly available at
https://github.com/Lichang-Chen/InstructZero.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.09841">Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation and Beyond. (arXiv:2306.09841v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1">Fangzhi Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1">Qika Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Jiawei Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1">Tianzhe Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1">Erik Cambria</a></p>
<p>Logical reasoning consistently plays a fundamental and significant role in
the domains of knowledge engineering and artificial intelligence. Recently,
Large Language Models (LLMs) have emerged as a noteworthy innovation in natural
language processing (NLP), exhibiting impressive achievements across various
classic NLP tasks. However, the question of whether LLMs can effectively
address the task of logical reasoning, which requires gradual cognitive
inference similar to human intelligence, remains unanswered. To this end, we
aim to bridge this gap and provide comprehensive evaluations in this paper.
Firstly, to offer systematic evaluations, we select fifteen typical logical
reasoning datasets and organize them into deductive, inductive, abductive and
mixed-form reasoning settings. Considering the comprehensiveness of
evaluations, we include three representative LLMs (i.e., text-davinci-003,
ChatGPT and BARD) and evaluate them on all selected datasets under zero-shot,
one-shot and three-shot settings. Secondly, different from previous evaluations
relying only on simple metrics (e.g., accuracy), we propose fine-level
evaluations from objective and subjective manners, covering both answers and
explanations. Additionally, to uncover the logical flaws of LLMs, problematic
cases will be attributed to five error types from two dimensions, i.e.,
evidence selection process and reasoning process. Thirdly, to avoid the
influences of knowledge bias and purely focus on benchmarking the logical
reasoning capability of LLMs, we propose a new dataset with neutral content. It
contains 3,000 samples and covers deductive, inductive and abductive settings.
Based on the in-depth evaluations, this paper finally forms a general
evaluation scheme of logical reasoning capability from six dimensions. It
reflects the pros and cons of LLMs and gives guiding directions for future
works.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.02227">MAE-DFER: Efficient Masked Autoencoder for Self-supervised Dynamic Facial Expression Recognition. (arXiv:2307.02227v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Licai Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Lian_Z/0/1/0/all/0/1">Zheng Lian</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_J/0/1/0/all/0/1">Jianhua Tao</a></p>
<p>Dynamic facial expression recognition (DFER) is essential to the development
of intelligent and empathetic machines. Prior efforts in this field mainly fall
into supervised learning paradigm, which is severely restricted by the limited
labeled data in existing datasets. Inspired by recent unprecedented success of
masked autoencoders (e.g., VideoMAE), this paper proposes MAE-DFER, a novel
self-supervised method which leverages large-scale self-supervised pre-training
on abundant unlabeled data to largely advance the development of DFER. Since
the vanilla Vision Transformer (ViT) employed in VideoMAE requires substantial
computation during fine-tuning, MAE-DFER develops an efficient local-global
interaction Transformer (LGI-Former) as the encoder. Moreover, in addition to
the standalone appearance content reconstruction in VideoMAE, MAE-DFER also
introduces explicit temporal facial motion modeling to encourage LGI-Former to
excavate both static appearance and dynamic motion information. Extensive
experiments on six datasets show that MAE-DFER consistently outperforms
state-of-the-art supervised methods by significant margins (e.g., +6.30\% UAR
on DFEW and +8.34\% UAR on MAFW), verifying that it can learn powerful dynamic
facial representations via large-scale self-supervised pre-training. Besides,
it has comparable or even better performance than VideoMAE, while largely
reducing the computational cost (about 38\% FLOPs). We believe MAE-DFER has
paved a new way for the advancement of DFER and can inspire more relevant
research in this field and even other related tasks. Codes and models are
publicly available at https://github.com/sunlicai/MAE-DFER.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.02947">A Neuromorphic Architecture for Reinforcement Learning from Real-Valued Observations. (arXiv:2307.02947v2 [cs.NE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chevtchenko_S/0/1/0/all/0/1">Sergio F. Chevtchenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Bethi_Y/0/1/0/all/0/1">Yeshwanth Bethi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1">Teresa B. Ludermir</a>, <a href="http://arxiv.org/find/cs/1/au:+Afshar_S/0/1/0/all/0/1">Saeed Afshar</a></p>
<p>Reinforcement Learning (RL) provides a powerful framework for decision-making
in complex environments. However, implementing RL in hardware-efficient and
bio-inspired ways remains a challenge. This paper presents a novel Spiking
Neural Network (SNN) architecture for solving RL problems with real-valued
observations. The proposed model incorporates multi-layered event-based
clustering, with the addition of Temporal Difference (TD)-error modulation and
eligibility traces, building upon prior work. An ablation study confirms the
significant impact of these components on the proposed model's performance. A
tabular actor-critic algorithm with eligibility traces and a state-of-the-art
Proximal Policy Optimization (PPO) algorithm are used as benchmarks. Our
network consistently outperforms the tabular approach and successfully
discovers stable control policies on classic RL environments: mountain car,
cart-pole, and acrobot. The proposed model offers an appealing trade-off in
terms of computational and hardware implementation requirements. The model does
not require an external memory buffer nor a global error gradient computation,
and synaptic updates occur online, driven by local learning rules and a
broadcasted TD-error signal. Thus, this work contributes to the development of
more hardware-efficient RL solutions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08496">Can We Trust Race Prediction?. (arXiv:2307.08496v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Cangyuan Li</a></p>
<p>In the absence of sensitive race and ethnicity data, researchers, regulators,
and firms alike turn to proxies. In this paper, I train a Bidirectional Long
Short-Term Memory (BiLSTM) model on a novel dataset of voter registration data
from all 50 US states and create an ensemble that achieves up to 36.8% higher
out of sample (OOS) F1 scores than the best performing machine learning models
in the literature. Additionally, I construct the most comprehensive database of
first and surname distributions in the US in order to improve the coverage and
accuracy of Bayesian Improved Surname Geocoding (BISG) and Bayesian Improved
Firstname Surname Geocoding (BIFSG). Finally, I provide the first high-quality
benchmark dataset in order to fairly compare existing models and aid future
model developers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09426">Balancing Privacy and Progress in Artificial Intelligence: Anonymization in Histopathology for Biomedical Research and Education. (arXiv:2307.09426v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kanwal_N/0/1/0/all/0/1">Neel Kanwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Janssen_E/0/1/0/all/0/1">Emiel A.M. Janssen</a>, <a href="http://arxiv.org/find/cs/1/au:+Engan_K/0/1/0/all/0/1">Kjersti Engan</a></p>
<p>The advancement of biomedical research heavily relies on access to large
amounts of medical data. In the case of histopathology, Whole Slide Images
(WSI) and clinicopathological information are valuable for developing
Artificial Intelligence (AI) algorithms for Digital Pathology (DP).
Transferring medical data "as open as possible" enhances the usability of the
data for secondary purposes but poses a risk to patient privacy. At the same
time, existing regulations push towards keeping medical data "as closed as
necessary" to avoid re-identification risks. Generally, these legal regulations
require the removal of sensitive data but do not consider the possibility of
data linkage attacks due to modern image-matching algorithms. In addition, the
lack of standardization in DP makes it harder to establish a single solution
for all formats of WSIs. These challenges raise problems for bio-informatics
researchers in balancing privacy and progress while developing AI algorithms.
This paper explores the legal regulations and terminologies for medical
data-sharing. We review existing approaches and highlight challenges from the
histopathological perspective. We also present a data-sharing guideline for
histological data to foster multidisciplinary research and education.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.11661">Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts. (arXiv:2307.11661v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maniparambil_M/0/1/0/all/0/1">Mayug Maniparambil</a>, <a href="http://arxiv.org/find/cs/1/au:+Vorster_C/0/1/0/all/0/1">Chris Vorster</a>, <a href="http://arxiv.org/find/cs/1/au:+Molloy_D/0/1/0/all/0/1">Derek Molloy</a>, <a href="http://arxiv.org/find/cs/1/au:+Murphy_N/0/1/0/all/0/1">Noel Murphy</a>, <a href="http://arxiv.org/find/cs/1/au:+McGuinness_K/0/1/0/all/0/1">Kevin McGuinness</a>, <a href="http://arxiv.org/find/cs/1/au:+OConnor_N/0/1/0/all/0/1">Noel E. O&#x27;Connor</a></p>
<p>Contrastive pretrained large Vision-Language Models (VLMs) like CLIP have
revolutionized visual representation learning by providing good performance on
downstream datasets. VLMs are 0-shot adapted to a downstream dataset by
designing prompts that are relevant to the dataset. Such prompt engineering
makes use of domain expertise and a validation dataset. Meanwhile, recent
developments in generative pretrained models like GPT-4 mean they can be used
as advanced internet search tools. They can also be manipulated to provide
visual information in any structure. In this work, we show that GPT-4 can be
used to generate text that is visually descriptive and how this can be used to
adapt CLIP to downstream tasks. We show considerable improvements in 0-shot
transfer accuracy on specialized fine-grained datasets like EuroSAT (~7%), DTD
(~7%), SUN397 (~4.6%), and CUB (~3.3%) when compared to CLIP's default prompt.
We also design a simple few-shot adapter that learns to choose the best
possible sentences to construct generalizable classifiers that outperform the
recently proposed CoCoOP by ~2% on average and by over 4% on 4 specialized
fine-grained datasets. The code, prompts, and auxiliary text dataset is
available at https://github.com/mayug/VDT-Adapter.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.12344">Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?. (arXiv:2307.12344v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1">Susu Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Koch_L/0/1/0/all/0/1">Lisa M. Koch</a>, <a href="http://arxiv.org/find/cs/1/au:+Baumgartner_C/0/1/0/all/0/1">Christian F. Baumgartner</a></p>
<p>While deep neural network models offer unmatched classification performance,
they are prone to learning spurious correlations in the data. Such dependencies
on confounding information can be difficult to detect using performance metrics
if the test data comes from the same distribution as the training data.
Interpretable ML methods such as post-hoc explanations or inherently
interpretable classifiers promise to identify faulty model reasoning. However,
there is mixed evidence whether many of these techniques are actually able to
do so. In this paper, we propose a rigorous evaluation strategy to assess an
explanation technique's ability to correctly identify spurious correlations.
Using this strategy, we evaluate five post-hoc explanation techniques and one
inherently interpretable method for their ability to detect three types of
artificially added confounders in a chest x-ray diagnosis task. We find that
the post-hoc technique SHAP, as well as the inherently interpretable Attri-Net
provide the best performance and can be used to reliably identify faulty model
behavior.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.12450">ProtoFL: Unsupervised Federated Learning via Prototypical Distillation. (arXiv:2307.12450v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">Hansol Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwak_Y/0/1/0/all/0/1">Youngjun Kwak</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_M/0/1/0/all/0/1">Minyoung Jung</a>, <a href="http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1">Jinho Shin</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Youngsung Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1">Changick Kim</a></p>
<p>Federated learning (FL) is a promising approach for enhancing data privacy
preservation, particularly for authentication systems. However, limited round
communications, scarce representation, and scalability pose significant
challenges to its deployment, hindering its full potential. In this paper, we
propose 'ProtoFL', Prototypical Representation Distillation based unsupervised
Federated Learning to enhance the representation power of a global model and
reduce round communication costs. Additionally, we introduce a local one-class
classifier based on normalizing flows to improve performance with limited data.
Our study represents the first investigation of using FL to improve one-class
classification performance. We conduct extensive experiments on five widely
used benchmarks, namely MNIST, CIFAR-10, CIFAR-100, ImageNet-30, and
Keystroke-Dynamics, to demonstrate the superior performance of our proposed
framework over previous methods in the literature.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.00352">MetaGPT: Meta Programming for Multi-Agent Collaborative Framework. (arXiv:2308.00352v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1">Sirui Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1">Xiawu Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jonathan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1">Yuheng Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jinlin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Ceyao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zili Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yau_S/0/1/0/all/0/1">Steven Ka Shing Yau</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zijuan Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1">Liyang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Ran_C/0/1/0/all/0/1">Chenyu Ran</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1">Lingfeng Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1">Chenglin Wu</a></p>
<p>Recently, remarkable progress has been made in automated task-solving through
the use of multi-agent driven by large language models (LLMs). However,
existing LLM-based multi-agent works primarily focus on solving simple dialogue
tasks, and complex tasks are rarely studied, mainly due to the LLM
hallucination problem. This type of hallucination becomes cascading when
naively chaining multiple intelligent agents, resulting in a failure to
effectively address complex problems. Therefore, we introduce MetaGPT, an
innovative framework that incorporates efficient human workflows as a meta
programming approach into LLM-based multi-agent collaboration. Specifically,
MetaGPT encodes Standardized Operating Procedures (SOPs) into prompts to
enhance structured coordination. Subsequently, it mandates modular outputs,
empowering agents with domain expertise comparable to human professionals, to
validate outputs and minimize compounded errors. In this way, MetaGPT leverages
the assembly line paradigm to assign diverse roles to various agents, thereby
establishing a framework that can effectively and cohesively deconstruct
complex multi-agent collaborative problems. Our experiments on collaborative
software engineering benchmarks demonstrate that MetaGPT generates more
coherent and correct solutions compared to existing chat-based multi-agent
systems. This highlights the potential of integrating human domain knowledge
into multi-agent systems, thereby creating new opportunities to tackle complex
real-world challenges. The GitHub repository of this project is publicly
available on:https://github.com/geekan/MetaGPT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01006">FusionAD: Multi-modality Fusion for Prediction and Planning Tasks of Autonomous Driving. (arXiv:2308.01006v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1">Tengju Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Jing_W/0/1/0/all/0/1">Wei Jing</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1">Chunyong Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Shikun Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1">Lingping Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1">Fangzhen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jingke Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1">Ke Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1">Wencong Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1">Weibo Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1">Hang Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Kun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Junbo Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1">Kaicheng Yu</a></p>
<p>Building a multi-modality multi-task neural network toward accurate and
robust performance is a de-facto standard in perception task of autonomous
driving. However, leveraging such data from multiple sensors to jointly
optimize the prediction and planning tasks remains largely unexplored. In this
paper, we present FusionAD, to the best of our knowledge, the first unified
framework that fuse the information from two most critical sensors, camera and
LiDAR, goes beyond perception task. Concretely, we first build a transformer
based multi-modality fusion network to effectively produce fusion based
features. In constrast to camera-based end-to-end method UniAD, we then
establish a fusion aided modality-aware prediction and status-aware planning
modules, dubbed FMSPnP that take advantages of multi-modality features. We
conduct extensive experiments on commonly used benchmark nuScenes dataset, our
FusionAD achieves state-of-the-art performance and surpassing baselines on
average 15% on perception tasks like detection and tracking, 10% on occupancy
prediction accuracy, reducing prediction error from 0.708 to 0.389 in ADE score
and reduces the collision rate from 0.31% to only 0.12%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01681">NBIAS: A Natural Language Processing Framework for Bias Identification in Text. (arXiv:2308.01681v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Raza_S/0/1/0/all/0/1">Shaina Raza</a>, <a href="http://arxiv.org/find/cs/1/au:+Garg_M/0/1/0/all/0/1">Muskan Garg</a>, <a href="http://arxiv.org/find/cs/1/au:+Reji_D/0/1/0/all/0/1">Deepak John Reji</a>, <a href="http://arxiv.org/find/cs/1/au:+Bashir_S/0/1/0/all/0/1">Syed Raza Bashir</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1">Chen Ding</a></p>
<p>Bias in textual data can lead to skewed interpretations and outcomes when the
data is used. These biases could perpetuate stereotypes, discrimination, or
other forms of unfair treatment. An algorithm trained on biased data ends up
making decisions that disproportionately impact a certain group of people.
Therefore, it is crucial to detect and remove these biases to ensure the fair
and ethical use of data. To this end, we develop a comprehensive and robust
framework \textsc{Nbias} that consists of a data layer, corpus contruction,
model development layer and an evaluation layer. The dataset is constructed by
collecting diverse data from various fields, including social media,
healthcare, and job hiring portals. As such, we applied a transformer-based
token classification model that is able to identify bias words/ phrases through
a unique named entity. In the assessment procedure, we incorporate a blend of
quantitative and qualitative evaluations to gauge the effectiveness of our
models. We achieve accuracy improvements ranging from 1% to 8% compared to
baselines. We are also able to generate a robust understanding of the model
functioning, capturing not only numerical data but also the quality and
intricacies of its performance. The proposed approach is applicable to a
variety of biases and contributes to the fair and ethical use of textual data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.02582">Adapt and Decompose: Efficient Generalization of Text-to-SQL via Domain Adapted Least-To-Most Prompting. (arXiv:2308.02582v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1">Aseem Arora</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhaisaheb_S/0/1/0/all/0/1">Shabbirhussain Bhaisaheb</a>, <a href="http://arxiv.org/find/cs/1/au:+Patwardhan_M/0/1/0/all/0/1">Manasi Patwardhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Vig_L/0/1/0/all/0/1">Lovekesh Vig</a>, <a href="http://arxiv.org/find/cs/1/au:+Shroff_G/0/1/0/all/0/1">Gautam Shroff</a></p>
<p>Cross-domain and cross-compositional generalization of Text-to-SQL semantic
parsing is a challenging task. Existing Large Language Model (LLM) based
solutions rely on inference-time retrieval of few-shot exemplars from the
training set to synthesize a run-time prompt for each Natural Language (NL)
test query. In contrast, we devise an algorithm which performs offline sampling
of a minimal set-of few-shots from the training data, with complete coverage of
SQL clauses, operators and functions, and maximal domain coverage within the
allowed token length. This allows for synthesis of a fixed Generic Prompt (GP),
with a diverse set-of exemplars common across NL test queries, avoiding
expensive test time exemplar retrieval. We further auto-adapt the GP to the
target database domain (DA-GP), to better handle cross-domain generalization;
followed by a decomposed Least-To-Most-Prompting (LTMP-DA-GP) to handle
cross-compositional generalization. The synthesis of LTMP-DA-GP is an offline
task, to be performed one-time per new database with minimal human
intervention. Our approach demonstrates superior performance on the KaggleDBQA
dataset, designed to evaluate generalizability for the Text-to-SQL task. We
further showcase consistent performance improvement of LTMP-DA-GP over GP,
across LLMs and databases of KaggleDBQA, highlighting the efficacy and model
agnostic benefits of our prompt based adapt and decompose approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.02632">Generation of Realistic Synthetic Raw Radar Data for Automated Driving Applications using Generative Adversarial Networks. (arXiv:2308.02632v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fidelis_E/0/1/0/all/0/1">Eduardo C. Fidelis</a>, <a href="http://arxiv.org/find/cs/1/au:+Reway_F/0/1/0/all/0/1">Fabio Reway</a>, <a href="http://arxiv.org/find/cs/1/au:+Ribeiro_H/0/1/0/all/0/1">Herick Y. S. Ribeiro</a>, <a href="http://arxiv.org/find/cs/1/au:+Campos_P/0/1/0/all/0/1">Pietro L. Campos</a>, <a href="http://arxiv.org/find/cs/1/au:+Huber_W/0/1/0/all/0/1">Werner Huber</a>, <a href="http://arxiv.org/find/cs/1/au:+Icking_C/0/1/0/all/0/1">Christian Icking</a>, <a href="http://arxiv.org/find/cs/1/au:+Faria_L/0/1/0/all/0/1">Lester A. Faria</a>, <a href="http://arxiv.org/find/cs/1/au:+Schon_T/0/1/0/all/0/1">Torsten Sch&#xf6;n</a></p>
<p>The main approaches for simulating FMCW radar are based on ray tracing, which
is usually computationally intensive and do not account for background noise.
This work proposes a faster method for FMCW radar simulation capable of
generating synthetic raw radar data using generative adversarial networks
(GAN). The code and pre-trained weights are open-source and available on
GitHub. This method generates 16 simultaneous chirps, which allows the
generated data to be used for the further development of algorithms for
processing radar data (filtering and clustering). This can increase the
potential for data augmentation, e.g., by generating data in non-existent or
safety-critical scenarios that are not reproducible in real life. In this work,
the GAN was trained with radar measurements of a motorcycle and used to
generate synthetic raw radar data of a motorcycle traveling in a straight line.
For generating this data, the distance of the motorcycle and Gaussian noise are
used as input to the neural network. The synthetic generated radar chirps were
evaluated using the Frechet Inception Distance (FID). Then, the Range-Azimuth
(RA) map is calculated twice: first, based on synthetic data using this GAN
and, second, based on real data. Based on these RA maps, an algorithm with
adaptive threshold and edge detection is used for object detection. The results
have shown that the data is realistic in terms of coherent radar reflections of
the motorcycle and background noise based on the comparison of chirps, the RA
maps and the object detection results. Thus, the proposed method in this work
has shown to minimize the simulation-to-reality gap for the generation of radar
data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03421">RecycleGPT: An Autoregressive Language Model with Recyclable Module. (arXiv:2308.03421v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yufan Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1">Qiaozhi He</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_X/0/1/0/all/0/1">Xiaomin Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhihua Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kunpeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Wenlai Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1">Guangwen Yang</a></p>
<p>Existing large language models have to run K times to generate a sequence of
K tokens. In this paper, we present RecycleGPT, a generative language model
with fast decoding speed by recycling pre-generated model states without
running the whole model in multiple steps. Our approach relies on the
observation that adjacent tokens in a sequence usually have strong correlations
and the next token in a sequence can be reasonably guessed or inferred based on
the preceding ones. Experiments and analysis demonstrate the effectiveness of
our approach in lowering inference latency, achieving up to 1.4x speedup while
preserving high performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03629">MedMine: Examining Pre-trained Language Models on Medication Mining. (arXiv:2308.03629v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alrdahi_H/0/1/0/all/0/1">Haifa Alrdahi</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1">Lifeng Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Suvalov_H/0/1/0/all/0/1">Hendrik &#x160;uvalov</a>, <a href="http://arxiv.org/find/cs/1/au:+Nenadic_G/0/1/0/all/0/1">Goran Nenadic</a></p>
<p>Automatic medication mining from clinical and biomedical text has become a
popular topic due to its real impact on healthcare applications and the recent
development of powerful language models (LMs). However, fully-automatic
extraction models still face obstacles to be overcome such that they can be
deployed directly into clinical practice for better impacts. Such obstacles
include their imbalanced performances on different entity types and clinical
events. In this work, we examine current state-of-the-art pre-trained language
models (PLMs) on such tasks, via fine-tuning including the monolingual model
Med7 and multilingual large language model (LLM) XLM-RoBERTa. We compare their
advantages and drawbacks using historical medication mining shared task data
sets from n2c2-2018 challenges. We report the findings we get from these
fine-tuning experiments such that they can facilitate future research on
addressing them, for instance, how to combine their outputs, merge such models,
or improve their overall accuracy by ensemble learning and data augmentation.
MedMine is part of the M3 Initiative \url{https://github.com/HECTA-UoM/M3}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.00085">Machine Learning and Computer Vision Techniques in Bee Monitoring Applications. (arXiv:2208.00085v1 [cs.CV] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bilik_S/0/1/0/all/0/1">Simon Bilik</a>, <a href="http://arxiv.org/find/cs/1/au:+Bostik_O/0/1/0/all/0/1">Ondrej Bostik</a>, <a href="http://arxiv.org/find/cs/1/au:+Kratochvila_L/0/1/0/all/0/1">Lukas Kratochvila</a>, <a href="http://arxiv.org/find/cs/1/au:+Ligocki_A/0/1/0/all/0/1">Adam Ligocki</a>, <a href="http://arxiv.org/find/cs/1/au:+Poncak_M/0/1/0/all/0/1">Matej Poncak</a>, <a href="http://arxiv.org/find/cs/1/au:+Zemcik_T/0/1/0/all/0/1">Tomas Zemcik</a>, <a href="http://arxiv.org/find/cs/1/au:+Richter_M/0/1/0/all/0/1">Milos Richter</a>, <a href="http://arxiv.org/find/cs/1/au:+Janakova_I/0/1/0/all/0/1">Ilona Janakova</a>, <a href="http://arxiv.org/find/cs/1/au:+Honec_P/0/1/0/all/0/1">Petr Honec</a>, <a href="http://arxiv.org/find/cs/1/au:+Horak_K/0/1/0/all/0/1">Karel Horak</a></p>
<p>Machine learning and computer vision are dynamically growing fields, which
have proven to be able to solve very complex tasks. They could also be used for
the monitoring of the honeybee colonies and for the inspection of their health
state, which could identify potentially dangerous states before the situation
is critical, or to better plan periodic bee colony inspections and therefore
save significant costs. In this paper, we present an overview of the
state-of-the-art computer vision and machine learning applications used for bee
monitoring. We also demonstrate the potential of those methods as an example of
an automated bee counter algorithm. The paper is aimed at veterinary and
apidology professionals and experts, who might not be familiar with machine
learning to introduce to them its possibilities, therefore each family of
applications is opened by a brief theoretical introduction and motivation
related to its base method. We hope that this paper will inspire other
scientists to use the machine learning techniques for other applications in bee
monitoring.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01941">Digital twin brain: a bridge between biological intelligence and artificial intelligence. (arXiv:2308.01941v1 [q-bio.NC] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Xiong_H/0/1/0/all/0/1">Hui Xiong</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Chu_C/0/1/0/all/0/1">Congying Chu</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Fan_L/0/1/0/all/0/1">Lingzhong Fan</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Song_M/0/1/0/all/0/1">Ming Song</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Zhang_J/0/1/0/all/0/1">Jiaqi Zhang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Ma_Y/0/1/0/all/0/1">Yawei Ma</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Zheng_R/0/1/0/all/0/1">Ruonan Zheng</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Zhang_J/0/1/0/all/0/1">Junyang Zhang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Yang_Z/0/1/0/all/0/1">Zhengyi Yang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Jiang_T/0/1/0/all/0/1">Tianzi Jiang</a></p>
<p>In recent years, advances in neuroscience and artificial intelligence have
paved the way for unprecedented opportunities for understanding the complexity
of the brain and its emulation by computational systems. Cutting-edge
advancements in neuroscience research have revealed the intricate relationship
between brain structure and function, while the success of artificial neural
networks highlights the importance of network architecture. Now is the time to
bring them together to better unravel how intelligence emerges from the brain's
multiscale repositories. In this review, we propose the Digital Twin Brain
(DTB) as a transformative platform that bridges the gap between biological and
artificial intelligence. It consists of three core elements: the brain
structure that is fundamental to the twinning process, bottom-layer models to
generate brain functions, and its wide spectrum of applications. Crucially,
brain atlases provide a vital constraint, preserving the brain's network
organization within the DTB. Furthermore, we highlight open questions that
invite joint efforts from interdisciplinary fields and emphasize the
far-reaching implications of the DTB. The DTB can offer unprecedented insights
into the emergence of intelligence and neurological disorders, which holds
tremendous promise for advancing our understanding of both biological and
artificial intelligence, and ultimately propelling the development of
artificial general intelligence and facilitating precision mental healthcare.
</p>
</p>
</div>

    </div>
    </body>
    