<!DOCTYPE html>
<html>
<head>
<title>2024-01-07-cs-lg</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2401.01895">A Robust Adversary Detection-Deactivation Method for Metaverse-oriented Collaborative Deep Learning. (arXiv:2401.01895v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Pengfei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhibo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Al_Sumaiti_A/0/1/0/all/0/1">Ameena S. Al-Sumaiti</a>, <a href="http://arxiv.org/find/cs/1/au:+Werghi_N/0/1/0/all/0/1">Naoufel Werghi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yeun_C/0/1/0/all/0/1">Chan Yeob Yeun</a></p>
<p>Metaverse is trending to create a digital circumstance that can transfer the
real world to an online platform supported by large quantities of real-time
interactions. Pre-trained Artificial Intelligence (AI) models are demonstrating
their increasing capability in aiding the metaverse to achieve an excellent
response with negligible delay, and nowadays, many large models are
collaboratively trained by various participants in a manner named collaborative
deep learning (CDL). However, several security weaknesses can threaten the
safety of the CDL training process, which might result in fatal attacks to
either the pre-trained large model or the local sensitive data sets possessed
by an individual entity. In CDL, malicious participants can hide within the
major innocent and silently uploads deceptive parameters to degenerate the
model performance, or they can abuse the downloaded parameters to construct a
Generative Adversarial Network (GAN) to acquire the private information of
others illegally. To compensate for these vulnerabilities, this paper proposes
an adversary detection-deactivation method, which can limit and isolate the
access of potential malicious participants, quarantine and disable the
GAN-attack or harmful backpropagation of received threatening gradients. A
detailed protection analysis has been conducted on a Multiview CDL case, and
results show that the protocol can effectively prevent harmful access by
heuristic manner analysis and can protect the existing model by swiftly
checking received gradients using only one low-cost branch with an embedded
firewall.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01896">Reputation-Based Federated Learning Defense to Mitigate Threats in EEG Signal Classification. (arXiv:2401.01896v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhibo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Pengfei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hammadi_A/0/1/0/all/0/1">Ahmed Y. Al Hammadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_F/0/1/0/all/0/1">Fusen Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Damiani_E/0/1/0/all/0/1">Ernesto Damiani</a>, <a href="http://arxiv.org/find/cs/1/au:+Yeun_C/0/1/0/all/0/1">Chan Yeob Yeun</a></p>
<p>This paper presents a reputation-based threat mitigation framework that
defends potential security threats in electroencephalogram (EEG) signal
classification during model aggregation of Federated Learning. While EEG signal
analysis has attracted attention because of the emergence of brain-computer
interface (BCI) technology, it is difficult to create efficient learning models
for EEG analysis because of the distributed nature of EEG data and related
privacy and security concerns. To address these challenges, the proposed
defending framework leverages the Federated Learning paradigm to preserve
privacy by collaborative model training with localized data from dispersed
sources and introduces a reputation-based mechanism to mitigate the influence
of data poisoning attacks and identify compromised participants. To assess the
efficiency of the proposed reputation-based federated learning defense
framework, data poisoning attacks based on the risk level of training data
derived by Explainable Artificial Intelligence (XAI) techniques are conducted
on both publicly available EEG signal datasets and the self-established EEG
signal dataset. Experimental results on the poisoned datasets show that the
proposed defense methodology performs well in EEG signal classification while
reducing the risks associated with security threats.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01905">Machine-learning-based particle identification with missing data. (arXiv:2401.01905v1 [physics.ins-det])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Kasak_M/0/1/0/all/0/1">Mi&#x142;osz Kasak</a>, <a href="http://arxiv.org/find/physics/1/au:+Deja_K/0/1/0/all/0/1">Kamil Deja</a>, <a href="http://arxiv.org/find/physics/1/au:+Karwowska_M/0/1/0/all/0/1">Maja Karwowska</a>, <a href="http://arxiv.org/find/physics/1/au:+Jakubowska_M/0/1/0/all/0/1">Monika Jakubowska</a>, <a href="http://arxiv.org/find/physics/1/au:+Graczykowski_L/0/1/0/all/0/1">&#x141;ukasz Graczykowski</a>, <a href="http://arxiv.org/find/physics/1/au:+Janik_M/0/1/0/all/0/1">Ma&#x142;gorzata Janik</a></p>
<p>In this work, we introduce a novel method for Particle Identification (PID)
within the scope of the ALICE experiment at the Large Hadron Collider at CERN.
Identifying products of ultrarelativisitc collisions delivered by the LHC is
one of the crucial objectives of ALICE. Typically employed PID methods rely on
hand-crafted selections, which compare experimental data to theoretical
simulations. To improve the performance of the baseline methods, novel
approaches use machine learning models that learn the proper assignment in a
classification task. However, because of the various detection techniques used
by different subdetectors, as well as the limited detector efficiency and
acceptance, produced particles do not always yield signals in all of the ALICE
components. This results in data with missing values. Machine learning
techniques cannot be trained with such examples, so a significant part of the
data is skipped during training. In this work, we propose the first method for
PID that can be trained with all of the available data examples, including
incomplete ones. Our approach improves the PID purity and efficiency of the
selected sample for all investigated particle species.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01911">Backdoor Attack on Unpaired Medical Image-Text Foundation Models: A Pilot Study on MedCLIP. (arXiv:2401.01911v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1">Ruinan Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1">Chun-Yin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1">Chenyu You</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaoxiao Li</a></p>
<p>In recent years, foundation models (FMs) have solidified their role as
cornerstone advancements in the deep learning domain. By extracting intricate
patterns from vast datasets, these models consistently achieve state-of-the-art
results across a spectrum of downstream tasks, all without necessitating
extensive computational resources. Notably, MedCLIP, a vision-language
contrastive learning-based medical FM, has been designed using unpaired
image-text training. While the medical domain has often adopted unpaired
training to amplify data, the exploration of potential security concerns linked
to this approach hasn't kept pace with its practical usage. Notably, the
augmentation capabilities inherent in unpaired training also indicate that
minor label discrepancies can result in significant model deviations. In this
study, we frame this label discrepancy as a backdoor attack problem. We further
analyze its impact on medical FMs throughout the FM supply chain. Our
evaluation primarily revolves around MedCLIP, emblematic of medical FM
employing the unpaired strategy. We begin with an exploration of
vulnerabilities in MedCLIP stemming from unpaired image-text matching, termed
BadMatch. BadMatch is achieved using a modest set of wrongly labeled data.
Subsequently, we disrupt MedCLIP's contrastive learning through
BadDist-assisted BadMatch by introducing a Bad-Distance between the embeddings
of clean and poisoned data. Additionally, combined with BadMatch and BadDist,
the attacking pipeline consistently fends off backdoor assaults across diverse
model designs, datasets, and triggers. Also, our findings reveal that current
defense strategies are insufficient in detecting these latent threats in
medical FMs' supply chains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01912">Shrinking Your TimeStep: Towards Low-Latency Neuromorphic Object Recognition with Spiking Neural Network. (arXiv:2401.01912v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1">Yongqi Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Zuo_L/0/1/0/all/0/1">Lin Zuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Jing_M/0/1/0/all/0/1">Mengmeng Jing</a>, <a href="http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1">Pei He</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1">Yongjun Xiao</a></p>
<p>Neuromorphic object recognition with spiking neural networks (SNNs) is the
cornerstone of low-power neuromorphic computing. However, existing SNNs suffer
from significant latency, utilizing 10 to 40 timesteps or more, to recognize
neuromorphic objects. At low latencies, the performance of existing SNNs is
drastically degraded. In this work, we propose the Shrinking SNN (SSNN) to
achieve low-latency neuromorphic object recognition without reducing
performance. Concretely, we alleviate the temporal redundancy in SNNs by
dividing SNNs into multiple stages with progressively shrinking timesteps,
which significantly reduces the inference latency. During timestep shrinkage,
the temporal transformer smoothly transforms the temporal scale and preserves
the information maximally. Moreover, we add multiple early classifiers to the
SNN during training to mitigate the mismatch between the surrogate gradient and
the true gradient, as well as the gradient vanishing/exploding, thus
eliminating the performance degradation at low latency. Extensive experiments
on neuromorphic datasets, CIFAR10-DVS, N-Caltech101, and DVS-Gesture have
revealed that SSNN is able to improve the baseline accuracy by 6.55% ~ 21.41%.
With only 5 average timesteps and without any data augmentation, SSNN is able
to achieve an accuracy of 73.63% on CIFAR10-DVS. This work presents a
heterogeneous temporal scale SNN and provides valuable insights into the
development of high-performance, low-latency SNNs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01916">AstroLLaMA-Chat: Scaling AstroLLaMA with Conversational and Diverse Datasets. (arXiv:2401.01916v1 [astro-ph.IM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/astro-ph/1/au:+Perkowski_E/0/1/0/all/0/1">Ernest Perkowski</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Pan_R/0/1/0/all/0/1">Rui Pan</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Nguyen_T/0/1/0/all/0/1">Tuan Dung Nguyen</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Ting_Y/0/1/0/all/0/1">Yuan-Sen Ting</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Kruk_S/0/1/0/all/0/1">Sandor Kruk</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Zhang_T/0/1/0/all/0/1">Tong Zhang</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+ONeill_C/0/1/0/all/0/1">Charlie O&#x27;Neill</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Jablonska_M/0/1/0/all/0/1">Maja Jablonska</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Smith_M/0/1/0/all/0/1">Michael J. Smith</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Schawinski_K/0/1/0/all/0/1">Kevin Schawinski</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Iyer_K/0/1/0/all/0/1">Kartheik Iyer</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+UniverseTBD_I/0/1/0/all/0/1">Ioana Ciuc&#x103; for UniverseTBD</a></p>
<p>We explore the potential of enhancing LLM performance in astronomy-focused
question-answering through targeted, continual pre-training. By employing a
compact 7B-parameter LLaMA-2 model and focusing exclusively on a curated set of
astronomy corpus -- comprising abstracts, introductions, and conclusions -- we
achieve notable improvements in specialized topic comprehension. While general
LLMs like GPT-4 outperform in broader question-answering scenarios due to
superior reasoning capabilities, our findings suggest that continual
pre-training with limited resources can still enhance model performance on
specialized topics. Additionally, we present an extension of AstroLLaMA: the
fine-tuning of the 7B LLaMA model on a domain-specific conversational dataset,
culminating in the release of the chat-enabled AstroLLaMA for community use.
Comprehensive quantitative benchmarking is currently in progress and will be
detailed in an upcoming full paper. The model, AstroLLaMA-Chat, is now
available at https://huggingface.co/universeTBD, providing the first
open-source conversational AI tool tailored for the astronomy community.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01922">Unsupervised Object-Centric Learning from Multiple Unspecified Viewpoints. (arXiv:2401.01922v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1">Jinyang Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tonglin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1">Zhimeng Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1">Xiangyang Xue</a></p>
<p>Visual scenes are extremely diverse, not only because there are infinite
possible combinations of objects and backgrounds but also because the
observations of the same scene may vary greatly with the change of viewpoints.
When observing a multi-object visual scene from multiple viewpoints, humans can
perceive the scene compositionally from each viewpoint while achieving the
so-called ``object constancy'' across different viewpoints, even though the
exact viewpoints are untold. This ability is essential for humans to identify
the same object while moving and to learn from vision efficiently. It is
intriguing to design models that have a similar ability. In this paper, we
consider a novel problem of learning compositional scene representations from
multiple unspecified (i.e., unknown and unrelated) viewpoints without using any
supervision and propose a deep generative model which separates latent
representations into a viewpoint-independent part and a viewpoint-dependent
part to solve this problem. During the inference, latent representations are
randomly initialized and iteratively updated by integrating the information in
different viewpoints with neural networks. Experiments on several specifically
designed synthetic datasets have shown that the proposed method can effectively
learn from multiple unspecified viewpoints.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01923">IoT in the Era of Generative AI: Vision and Challenges. (arXiv:2401.01923v1 [cs.DC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1">Zhongwei Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hekmati_A/0/1/0/all/0/1">Arvin Hekmati</a>, <a href="http://arxiv.org/find/cs/1/au:+Zong_M/0/1/0/all/0/1">Mingyu Zong</a>, <a href="http://arxiv.org/find/cs/1/au:+Alam_S/0/1/0/all/0/1">Samiul Alam</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Mi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishnamachari_B/0/1/0/all/0/1">Bhaskar Krishnamachari</a></p>
<p>Equipped with sensing, networking, and computing capabilities, Internet of
Things (IoT) such as smartphones, wearables, smart speakers, and household
robots have been seamlessly weaved into our daily lives. Recent advancements in
Generative AI exemplified by GPT, LLaMA, DALL-E, and Stable Difussion hold
immense promise to push IoT to the next level. In this article, we share our
vision and views on the benefits that Generative AI brings to IoT, and discuss
some of the most important applications of Generative AI in IoT-related
domains. Fully harnessing Generative AI in IoT is a complex challenge. We
identify some of the most critical challenges including high resource demands
of the Generative AI models, prompt engineering, on-device inference,
offloading, on-device fine-tuning, federated learning, security, as well as
development tools and benchmarks, and discuss current gaps as well as promising
opportunities on enabling Generative AI for IoT. We hope this article can
inspire new research on IoT in the era of Generative AI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01951">Can We Generate Realistic Hands Only Using Convolution?. (arXiv:2401.01951v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hosseini_M/0/1/0/all/0/1">Mehran Hosseini</a>, <a href="http://arxiv.org/find/cs/1/au:+Hosseini_P/0/1/0/all/0/1">Peyman Hosseini</a></p>
<p>The enduring inability of image generative models to recreate intricate
geometric features, such as those present in human hands and fingers has been
an ongoing problem in image generation for nearly a decade. While strides have
been made by increasing model sizes and diversifying training datasets, this
issue remains prevalent across all models, from denoising diffusion models to
Generative Adversarial Networks (GAN), pointing to a fundamental shortcoming in
the underlying architectures. In this paper, we demonstrate how this problem
can be mitigated by augmenting convolution layers geometric capabilities
through providing them with a single input channel incorporating the relative
$n$-dimensional Cartesian coordinate system. We show that this drastically
improves quality of hand and face images generated by GANs and Variational
AutoEncoders (VAE).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01974">Towards Truly Zero-shot Compositional Visual Reasoning with LLMs as Programmers. (arXiv:2401.01974v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stanic_A/0/1/0/all/0/1">Aleksandar Stani&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Caelles_S/0/1/0/all/0/1">Sergi Caelles</a>, <a href="http://arxiv.org/find/cs/1/au:+Tschannen_M/0/1/0/all/0/1">Michael Tschannen</a></p>
<p>Visual reasoning is dominated by end-to-end neural networks scaled to
billions of model parameters and training examples. However, even the largest
models struggle with compositional reasoning, generalization, fine-grained
spatial and temporal reasoning, and counting. Visual reasoning with large
language models (LLMs) as controllers can, in principle, address these
limitations by decomposing the task and solving subtasks by orchestrating a set
of (visual) tools. Recently, these models achieved great performance on tasks
such as compositional visual question answering, visual grounding, and video
temporal reasoning. Nevertheless, in their current form, these models heavily
rely on human engineering of in-context examples in the prompt, which are often
dataset- and task-specific and require significant labor by highly skilled
programmers. In this work, we present a framework that mitigates these issues
by introducing spatially and temporally abstract routines and by leveraging a
small number of labeled examples to automatically generate in-context examples,
thereby avoiding human-created in-context examples. On a number of visual
reasoning tasks, we show that our framework leads to consistent gains in
performance, makes LLMs as controllers setup more robust, and removes the need
for human engineering of in-context examples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01978">Tailor: Size Recommendations for High-End Fashion Marketplaces. (arXiv:2401.01978v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Candeias_A/0/1/0/all/0/1">Alexandre Candeias</a>, <a href="http://arxiv.org/find/cs/1/au:+Silva_I/0/1/0/all/0/1">Ivo Silva</a>, <a href="http://arxiv.org/find/cs/1/au:+Sousa_V/0/1/0/all/0/1">Vitor Sousa</a>, <a href="http://arxiv.org/find/cs/1/au:+Marcelino_J/0/1/0/all/0/1">Jos&#xe9; Marcelino</a></p>
<p>In the ever-changing and dynamic realm of high-end fashion marketplaces,
providing accurate and personalized size recommendations has become a critical
aspect. Meeting customer expectations in this regard is not only crucial for
ensuring their satisfaction but also plays a pivotal role in driving customer
retention, which is a key metric for the success of any fashion retailer. We
propose a novel sequence classification approach to address this problem,
integrating implicit (Add2Bag) and explicit (ReturnReason) user signals. Our
approach comprises two distinct models: one employs LSTMs to encode the user
signals, while the other leverages an Attention mechanism. Our best model
outperforms SFNet, improving accuracy by 45.7%. By using Add2Bag interactions
we increase the user coverage by 24.5% when compared with only using Orders.
Moreover, we evaluate the models' usability in real-time recommendation
scenarios by conducting experiments to measure their latency performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01981">Beyond Regrets: Geometric Metrics for Bayesian Optimization. (arXiv:2401.01981v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jungtaek Kim</a></p>
<p>Bayesian optimization is a principled optimization strategy for a black-box
objective function. It shows its effectiveness in a wide variety of real-world
applications such as scientific discovery and experimental design. In general,
the performance of Bayesian optimization is assessed by regret-based metrics
such as instantaneous, simple, and cumulative regrets. These metrics only rely
on function evaluations, so that they do not consider geometric relationships
between query points and global solutions, or query points themselves. Notably,
they cannot discriminate if multiple global solutions are successfully found.
Moreover, they do not evaluate Bayesian optimization's abilities to exploit and
explore a search space given. To tackle these issues, we propose four new
geometric metrics, i.e., precision, recall, average degree, and average
distance. These metrics allow us to compare Bayesian optimization algorithms
considering the geometry of both query points and global optima, or query
points. However, they are accompanied by an extra parameter, which needs to be
carefully determined. We therefore devise the parameter-free forms of the
respective metrics by integrating out the additional parameter. Finally, we
empirically validate that our proposed metrics can provide more convincing
interpretation and understanding of Bayesian optimization algorithms from
distinct perspectives, compared to the conventional metrics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01987">Representation Learning of Multivariate Time Series using Attention and Adversarial Training. (arXiv:2401.01987v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Scharwachter_L/0/1/0/all/0/1">Leon Scharw&#xe4;chter</a>, <a href="http://arxiv.org/find/cs/1/au:+Otte_S/0/1/0/all/0/1">Sebastian Otte</a></p>
<p>A critical factor in trustworthy machine learning is to develop robust
representations of the training data. Only under this guarantee methods are
legitimate to artificially generate data, for example, to counteract imbalanced
datasets or provide counterfactual explanations for blackbox decision-making
systems. In recent years, Generative Adversarial Networks (GANs) have shown
considerable results in forming stable representations and generating realistic
data. While many applications focus on generating image data, less effort has
been made in generating time series data, especially multivariate signals. In
this work, a Transformer-based autoencoder is proposed that is regularized
using an adversarial training scheme to generate artificial multivariate time
series signals. The representation is evaluated using t-SNE visualizations,
Dynamic Time Warping (DTW) and Entropy scores. Our results indicate that the
generated signals exhibit higher similarity to an exemplary dataset than using
a convolutional network approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01990">GPS-SSL: Guided Positive Sampling to Inject Prior Into Self-Supervised Learning. (arXiv:2401.01990v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feizi_A/0/1/0/all/0/1">Aarash Feizi</a>, <a href="http://arxiv.org/find/cs/1/au:+Balestriero_R/0/1/0/all/0/1">Randall Balestriero</a>, <a href="http://arxiv.org/find/cs/1/au:+Romero_Soriano_A/0/1/0/all/0/1">Adriana Romero-Soriano</a>, <a href="http://arxiv.org/find/cs/1/au:+Rabbany_R/0/1/0/all/0/1">Reihaneh Rabbany</a></p>
<p>We propose Guided Positive Sampling Self-Supervised Learning (GPS-SSL), a
general method to inject a priori knowledge into Self-Supervised Learning (SSL)
positive samples selection. Current SSL methods leverage Data-Augmentations
(DA) for generating positive samples and incorporate prior knowledge - an
incorrect, or too weak DA will drastically reduce the quality of the learned
representation. GPS-SSL proposes instead to design a metric space where
Euclidean distances become a meaningful proxy for semantic relationship. In
that space, it is now possible to generate positive samples from nearest
neighbor sampling. Any prior knowledge can now be embedded into that metric
space independently from the employed DA. From its simplicity, GPS-SSL is
applicable to any SSL method, e.g. SimCLR or BYOL. A key benefit of GPS-SSL is
in reducing the pressure in tailoring strong DAs. For example GPS-SSL reaches
85.58% on Cifar10 with weak DA while the baseline only reaches 37.51%. We
therefore move a step forward towards the goal of making SSL less reliant on
DA. We also show that even when using strong DAs, GPS-SSL outperforms the
baselines on under-studied domains. We evaluate GPS-SSL along with multiple
baseline SSL methods on numerous downstream datasets from different domains
when the models use strong or minimal data augmentations. We hope that GPS-SSL
will open new avenues in studying how to inject a priori knowledge into SSL in
a principled manner.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01996">Mean-Field Assisted Deep Boltzmann Learning with Probabilistic Computers. (arXiv:2401.01996v1 [cs.ET])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1">Shuvro Chowdhury</a>, <a href="http://arxiv.org/find/cs/1/au:+Niazi_S/0/1/0/all/0/1">Shaila Niazi</a>, <a href="http://arxiv.org/find/cs/1/au:+Camsari_K/0/1/0/all/0/1">Kerem Y. Camsari</a></p>
<p>Despite their appeal as physics-inspired, energy-based and generative nature,
general Boltzmann Machines (BM) are considered intractable to train. This
belief led to simplified models of BMs with restricted intralayer connections
or layer-by-layer training of deep BMs. Recent developments in domain-specific
hardware -- specifically probabilistic computers (p-computer) with
probabilistic bits (p-bit) -- may change established wisdom on the tractability
of deep BMs. In this paper, we show that deep and unrestricted BMs can be
trained using p-computers generating hundreds of billions of Markov Chain Monte
Carlo (MCMC) samples per second, on sparse networks developed originally for
use in D-Wave's annealers. To maximize the efficiency of learning the
p-computer, we introduce two families of Mean-Field Theory assisted learning
algorithms, or xMFTs (x = Naive and Hierarchical). The xMFTs are used to
estimate the averages and correlations during the positive phase of the
contrastive divergence (CD) algorithm and our custom-designed p-computer is
used to estimate the averages and correlations in the negative phase. A custom
Field-Programmable-Gate Array (FPGA) emulation of the p-computer architecture
takes up to 45 billion flips per second, allowing the implementation of CD-$n$
where $n$ can be of the order of millions, unlike RBMs where $n$ is typically 1
or 2. Experiments on the full MNIST dataset with the combined algorithm show
that the positive phase can be efficiently computed by xMFTs without much
degradation when the negative phase is computed by the p-computer. Our
algorithm can be used in other scalable Ising machines and its variants can be
used to train BMs, previously thought to be intractable.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02008">Two-Stage Surrogate Modeling for Data-Driven Design Optimization with Application to Composite Microstructure Generation. (arXiv:2401.02008v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pourkamali_Anaraki_F/0/1/0/all/0/1">Farhad Pourkamali-Anaraki</a>, <a href="http://arxiv.org/find/cs/1/au:+Husseini_J/0/1/0/all/0/1">Jamal F. Husseini</a>, <a href="http://arxiv.org/find/cs/1/au:+Pineda_E/0/1/0/all/0/1">Evan J. Pineda</a>, <a href="http://arxiv.org/find/cs/1/au:+Bednarcyk_B/0/1/0/all/0/1">Brett A. Bednarcyk</a>, <a href="http://arxiv.org/find/cs/1/au:+Stapleton_S/0/1/0/all/0/1">Scott E. Stapleton</a></p>
<p>This paper introduces a novel two-stage machine learning-based surrogate
modeling framework to address inverse problems in scientific and engineering
fields. In the first stage of the proposed framework, a machine learning model
termed the "learner" identifies a limited set of candidates within the input
design space whose predicted outputs closely align with desired outcomes.
Subsequently, in the second stage, a separate surrogate model, functioning as
an "evaluator," is employed to assess the reduced candidate space generated in
the first stage. This evaluation process eliminates inaccurate and uncertain
solutions, guided by a user-defined coverage level. The framework's distinctive
contribution is the integration of conformal inference, providing a versatile
and efficient approach that can be widely applicable. To demonstrate the
effectiveness of the proposed framework compared to conventional single-stage
inverse problems, we conduct several benchmark tests and investigate an
engineering application focused on the micromechanical modeling of
fiber-reinforced composites. The results affirm the superiority of our proposed
framework, as it consistently produces more reliable solutions. Therefore, the
introduced framework offers a unique perspective on fostering interactions
between machine learning-based surrogate models in real-world applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02011">Decentralized Multi-Task Online Convex Optimization Under Random Link Failures. (arXiv:2401.02011v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yan_W/0/1/0/all/0/1">Wenjing Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1">Xuanyu Cao</a></p>
<p>Decentralized optimization methods often entail information exchange between
neighbors. Transmission failures can happen due to network congestion,
hardware/software issues, communication outage, and other factors. In this
paper, we investigate the random link failure problem in decentralized
multi-task online convex optimization, where agents have individual decisions
that are coupled with each other via pairwise constraints. Although widely used
in constrained optimization, conventional saddle-point algorithms are not
directly applicable here because of random packet dropping. To address this
issue, we develop a robust decentralized saddle-point algorithm against random
link failures with heterogeneous probabilities by replacing the missing
decisions of neighbors with their latest received values. Then, by judiciously
bounding the accumulated deviation stemming from this replacement, we first
establish that our algorithm achieves $\mathcal{O}(\sqrt{T})$ regret and
$\mathcal{O}(T^\frac{3}{4})$ constraint violations for the full information
scenario, where the complete information on the local cost function is revealed
to each agent at the end of each time slot. These two bounds match, in order
sense, the performance bounds of algorithms with perfect communications.
Further, we extend our algorithm and analysis to the two-point bandit feedback
scenario, where only the values of the local cost function at two random points
are disclosed to each agent sequentially. Performance bounds of the same orders
as the full information case are derived. Finally, we corroborate the efficacy
of the proposed algorithms and the analytical results through numerical
simulations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02012">Fast &amp; Fair: Efficient Second-Order Robust Optimization for Fairness in Machine Learning. (arXiv:2401.02012v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Minch_A/0/1/0/all/0/1">Allen Minch</a>, <a href="http://arxiv.org/find/cs/1/au:+Vu_H/0/1/0/all/0/1">Hung Anh Vu</a>, <a href="http://arxiv.org/find/cs/1/au:+Warren_A/0/1/0/all/0/1">Anne Marie Warren</a></p>
<p>This project explores adversarial training techniques to develop fairer Deep
Neural Networks (DNNs) to mitigate the inherent bias they are known to exhibit.
DNNs are susceptible to inheriting bias with respect to sensitive attributes
such as race and gender, which can lead to life-altering outcomes (e.g.,
demographic bias in facial recognition software used to arrest a suspect). We
propose a robust optimization problem, which we demonstrate can improve
fairness in several datasets, both synthetic and real-world, using an affine
linear model. Leveraging second order information, we are able to find a
solution to our optimization problem more efficiently than a purely first order
method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02013">SwitchTab: Switched Autoencoders Are Effective Tabular Learners. (arXiv:2401.02013v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jing Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Suiyao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1">Qi Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sergazinov_R/0/1/0/all/0/1">Renat Sergazinov</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shengjie Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1">Chongchao Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1">Tianpei Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1">Hanqing Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_C/0/1/0/all/0/1">Cheng Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Cociorva_D/0/1/0/all/0/1">Daniel Cociorva</a>, <a href="http://arxiv.org/find/cs/1/au:+Brunzel_H/0/1/0/all/0/1">Hakan Brunzel</a></p>
<p>Self-supervised representation learning methods have achieved significant
success in computer vision and natural language processing, where data samples
exhibit explicit spatial or semantic dependencies. However, applying these
methods to tabular data is challenging due to the less pronounced dependencies
among data samples. In this paper, we address this limitation by introducing
SwitchTab, a novel self-supervised method specifically designed to capture
latent dependencies in tabular data. SwitchTab leverages an asymmetric
encoder-decoder framework to decouple mutual and salient features among data
pairs, resulting in more representative embeddings. These embeddings, in turn,
contribute to better decision boundaries and lead to improved results in
downstream tasks. To validate the effectiveness of SwitchTab, we conduct
extensive experiments across various domains involving tabular data. The
results showcase superior performance in end-to-end prediction tasks with
fine-tuning. Moreover, we demonstrate that pre-trained salient embeddings can
be utilized as plug-and-play features to enhance the performance of various
traditional classification methods (e.g., Logistic Regression, XGBoost, etc.).
Lastly, we highlight the capability of SwitchTab to create explainable
representations through visualization of decoupled mutual and salient features
in the latent space.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02019">From Function to Distribution Modeling: A PAC-Generative Approach to Offline Optimization. (arXiv:2401.02019v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qiang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1">Ruida Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yang Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tie Liu</a></p>
<p>This paper considers the problem of offline optimization, where the objective
function is unknown except for a collection of ``offline" data examples. While
recent years have seen a flurry of work on applying various machine learning
techniques to the offline optimization problem, the majority of these work
focused on learning a surrogate of the unknown objective function and then
applying existing optimization algorithms. While the idea of modeling the
unknown objective function is intuitive and appealing, from the learning point
of view it also makes it very difficult to tune the objective of the learner
according to the objective of optimization. Instead of learning and then
optimizing the unknown objective function, in this paper we take on a less
intuitive but more direct view that optimization can be thought of as a process
of sampling from a generative model. To learn an effective generative model
from the offline data examples, we consider the standard technique of
``re-weighting", and our main technical contribution is a probably
approximately correct (PAC) lower bound on the natural optimization objective,
which allows us to jointly learn a weight function and a score-based generative
model. The robustly competitive performance of the proposed approach is
demonstrated via empirical studies using the standard offline optimization
benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02020">Spikformer V2: Join the High Accuracy Club on ImageNet with an SNN Ticket. (arXiv:2401.02020v1 [cs.NE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhaokun Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Che_K/0/1/0/all/0/1">Kaiwei Che</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1">Wei Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_K/0/1/0/all/0/1">Keyu Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yuesheng Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1">Shuicheng Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yonghong Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1">Li Yuan</a></p>
<p>Spiking Neural Networks (SNNs), known for their biologically plausible
architecture, face the challenge of limited performance. The self-attention
mechanism, which is the cornerstone of the high-performance Transformer and
also a biologically inspired structure, is absent in existing SNNs. To this
end, we explore the potential of leveraging both self-attention capability and
biological properties of SNNs, and propose a novel Spiking Self-Attention (SSA)
and Spiking Transformer (Spikformer). The SSA mechanism eliminates the need for
softmax and captures the sparse visual feature employing spike-based Query,
Key, and Value. This sparse computation without multiplication makes SSA
efficient and energy-saving. Further, we develop a Spiking Convolutional Stem
(SCS) with supplementary convolutional layers to enhance the architecture of
Spikformer. The Spikformer enhanced with the SCS is referred to as Spikformer
V2. To train larger and deeper Spikformer V2, we introduce a pioneering
exploration of Self-Supervised Learning (SSL) within the SNN. Specifically, we
pre-train Spikformer V2 with masking and reconstruction style inspired by the
mainstream self-supervised Transformer, and then finetune the Spikformer V2 on
the image classification on ImageNet. Extensive experiments show that
Spikformer V2 outperforms other previous surrogate training and ANN2SNN
methods. An 8-layer Spikformer V2 achieves an accuracy of 80.38% using 4 time
steps, and after SSL, a 172M 16-layer Spikformer V2 reaches an accuracy of
81.10% with just 1 time step. To the best of our knowledge, this is the first
time that the SNN achieves 80+% accuracy on ImageNet. The code will be
available at Spikformer V2.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02058">Neural Collapse for Cross-entropy Class-Imbalanced Learning with Unconstrained ReLU Feature Model. (arXiv:2401.02058v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dang_H/0/1/0/all/0/1">Hien Dang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1">Tho Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Tan Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ho_N/0/1/0/all/0/1">Nhat Ho</a></p>
<p>The current paradigm of training deep neural networks for classification
tasks includes minimizing the empirical risk that pushes the training loss
value towards zero, even after the training error has been vanished. In this
terminal phase of training, it has been observed that the last-layer features
collapse to their class-means and these class-means converge to the vertices of
a simplex Equiangular Tight Frame (ETF). This phenomenon is termed as Neural
Collapse (NC). To theoretically understand this phenomenon, recent works employ
a simplified unconstrained feature model to prove that NC emerges at the global
solutions of the training problem. However, when the training dataset is
class-imbalanced, some NC properties will no longer be true. For example, the
class-means geometry will skew away from the simplex ETF when the loss
converges. In this paper, we generalize NC to imbalanced regime for
cross-entropy loss under the unconstrained ReLU feature model. We prove that,
while the within-class features collapse property still holds in this setting,
the class-means will converge to a structure consisting of orthogonal vectors
with different lengths. Furthermore, we find that the classifier weights are
aligned to the scaled and centered class-means with scaling factors depend on
the number of training samples of each class, which generalizes NC in the
class-balanced setting. We empirically prove our results through experiments on
practical architectures and dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02062">U-Trustworthy Models.Reliability, Competence, and Confidence in Decision-Making. (arXiv:2401.02062v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Vashistha_R/0/1/0/all/0/1">Ritwik Vashistha</a>, <a href="http://arxiv.org/find/stat/1/au:+Farahi_A/0/1/0/all/0/1">Arya Farahi</a></p>
<p>With growing concerns regarding bias and discrimination in predictive models,
the AI community has increasingly focused on assessing AI system
trustworthiness. Conventionally, trustworthy AI literature relies on the
probabilistic framework and calibration as prerequisites for trustworthiness.
In this work, we depart from this viewpoint by proposing a novel trust
framework inspired by the philosophy literature on trust. We present a precise
mathematical definition of trustworthiness, termed
$\mathcal{U}$-trustworthiness, specifically tailored for a subset of tasks
aimed at maximizing a utility function. We argue that a model's
$\mathcal{U}$-trustworthiness is contingent upon its ability to maximize Bayes
utility within this task subset. Our first set of results challenges the
probabilistic framework by demonstrating its potential to favor less
trustworthy models and introduce the risk of misleading trustworthiness
assessments. Within the context of $\mathcal{U}$-trustworthiness, we prove that
properly-ranked models are inherently $\mathcal{U}$-trustworthy. Furthermore,
we advocate for the adoption of the AUC metric as the preferred measure of
trustworthiness. By offering both theoretical guarantees and experimental
validation, AUC enables robust evaluation of trustworthiness, thereby enhancing
model selection and hyperparameter tuning to yield more trustworthy outcomes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02080">Energy based diffusion generator for efficient sampling of Boltzmann distributions. (arXiv:2401.02080v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1">Ling Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Hao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1">Tao Zhou</a></p>
<p>We introduce a novel sampler called the energy based diffusion generator for
generating samples from arbitrary target distributions. The sampling model
employs a structure similar to a variational autoencoder, utilizing a decoder
to transform latent variables from a simple distribution into random variables
approximating the target distribution, and we design an encoder based on the
diffusion model. Leveraging the powerful modeling capacity of the diffusion
model for complex distributions, we can obtain an accurate variational estimate
of the Kullback-Leibler divergence between the distributions of the generated
samples and the target. Moreover, we propose a decoder based on generalized
Hamiltonian dynamics to further enhance sampling performance. Through empirical
evaluation, we demonstrate the effectiveness of our method across various
complex distribution functions, showcasing its superiority compared to existing
methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02086">View-based Explanations for Graph Neural Networks. (arXiv:2401.02086v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tingyang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_D/0/1/0/all/0/1">Dazhuo Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yinghui Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1">Arijit Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ke_X/0/1/0/all/0/1">Xiangyu Ke</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yunjun Gao</a></p>
<p>Generating explanations for graph neural networks (GNNs) has been studied to
understand their behavior in analytical tasks such as graph classification.
Existing approaches aim to understand the overall results of GNNs rather than
providing explanations for specific class labels of interest, and may return
explanation structures that are hard to access, nor directly queryable.
</p>
<p>We propose GVEX, a novel paradigm that generates Graph Views for EXplanation.
(1) We design a two-tier explanation structure called explanation views. An
explanation view consists of a set of graph patterns and a set of induced
explanation subgraphs. Given a database G of multiple graphs and a specific
class label l assigned by a GNN-based classifier M, it concisely describes the
fraction of G that best explains why l is assigned by M. (2) We propose quality
measures and formulate an optimization problem to compute optimal explanation
views for GNN explanation. We show that the problem is $\Sigma^2_P$-hard. (3)
We present two algorithms. The first one follows an explain-and-summarize
strategy that first generates high-quality explanation subgraphs which best
explain GNNs in terms of feature influence maximization, and then performs a
summarization step to generate patterns. We show that this strategy provides an
approximation ratio of 1/2. Our second algorithm performs a single-pass to an
input node stream in batches to incrementally maintain explanation views,
having an anytime quality guarantee of 1/4 approximation. Using real-world
benchmark data, we experimentally demonstrate the effectiveness, efficiency,
and scalability of GVEX. Through case studies, we showcase the practical
applications of GVEX.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02088">Re-evaluating the Memory-balanced Pipeline Parallelism: BPipe. (arXiv:2401.02088v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1">Mincong Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1">Chi Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yineng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Peng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1">Lei Yu</a></p>
<p>Pipeline parallelism is an essential technique in the training of large-scale
Transformer models. However, it suffers from imbalanced memory consumption,
leading to insufficient memory utilization. The BPipe technique was proposed to
address this issue and has proven effective in the GPT-3 model. Nevertheless,
our experiments have not yielded similar benefits for LLaMA training.
Additionally, BPipe only yields negligible benefits for GPT-3 training when
applying flash attention. We analyze the underlying causes of the divergent
performance of BPipe on GPT-3 and LLaMA. Furthermore, we introduce a novel
method to estimate the performance of BPipe.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02106">Cadmium Zinc Telluride (CZT) photon counting detector Characterisation for soft tissue imaging. (arXiv:2401.02106v1 [physics.ins-det])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Hameed_K/0/1/0/all/0/1">Kamran Hameed</a>, <a href="http://arxiv.org/find/physics/1/au:+Zainon_R/0/1/0/all/0/1">Rafidah Zainon</a>, <a href="http://arxiv.org/find/physics/1/au:+Tamal_M/0/1/0/all/0/1">Mahbubunnabi Tamal</a></p>
<p>The use of photon counting detection technology has resulted in significant
X-ray imaging research interest in recent years. Computed Tomography (CT)
scanners can benefit from photon-counting detectors, which are new technology
with the potential to overcome key limitations of conventional CT detectors.
Researchers are still studying the effectiveness and sensitivity of
semiconductor detector materials in photon counting detectors for detecting
soft tissue contrasts. This study aimed to characterize the performance of the
Cadmium Zinc Telluride photon counting detector in identifying various tissues.
An optimal frame rate per second (FPS) of CZT detector was evaluated by setting
the X-ray tube voltage and current at 25 keV, 35 keV and 0.5 mA, 1.0 mA
respectively by keeping the optimum FPS fixed, the detector energy thresholds
were set in small steps from 15 keV to 35 keV and the Currents were set for
X-ray tubes in ranges of 0.1 mA to 1.0 mA to find the relationship between
voltage and current of the X-ray source and counts per second (CPS). The
samples i.e., fat, liver, muscles, paraffin wax, and contrast media were
stacked at six different thickness levels in a stair-step chamber made from
Plexi-glass. X-ray transmission at six different thicknesses of tissue samples
was also examined for five different energy (regions) thresholds (21 keV, 25
keV, 29 keV, 31 keV, and 45 keV) to determine the effect on count per second
(CPS). In this study, 12 frames per second is found to be the optimum frame
rate per second (FPS) based on the spectral response of an X-ray source and CPS
has a linear relationship with X-ray tube current as well. It was also noted
that A sample's thickness also affects its X-ray transmission at different
energy thresholds. A high sensitivity and linearity of the detectors make them
suitable for use in both preclinical and medical applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02117">Mobile ALOHA: Learning Bimanual Mobile Manipulation with Low-Cost Whole-Body Teleoperation. (arXiv:2401.02117v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1">Zipeng Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1">Tony Z. Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1">Chelsea Finn</a></p>
<p>Imitation learning from human demonstrations has shown impressive performance
in robotics. However, most results focus on table-top manipulation, lacking the
mobility and dexterity necessary for generally useful tasks. In this work, we
develop a system for imitating mobile manipulation tasks that are bimanual and
require whole-body control. We first present Mobile ALOHA, a low-cost and
whole-body teleoperation system for data collection. It augments the ALOHA
system with a mobile base, and a whole-body teleoperation interface. Using data
collected with Mobile ALOHA, we then perform supervised behavior cloning and
find that co-training with existing static ALOHA datasets boosts performance on
mobile manipulation tasks. With 50 demonstrations for each task, co-training
can increase success rates by up to 90%, allowing Mobile ALOHA to autonomously
complete complex mobile manipulation tasks such as sauteing and serving a piece
of shrimp, opening a two-door wall cabinet to store heavy cooking pots, calling
and entering an elevator, and lightly rinsing a used pan using a kitchen
faucet. Project website: https://mobile-aloha.github.io
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02124">ACP-ESM: A novel framework for classification of anticancer peptides using protein-oriented transformer approach. (arXiv:2401.02124v1 [q-bio.BM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Kilimci_Z/0/1/0/all/0/1">Zeynep Hilal Kilimci</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Yalcin_M/0/1/0/all/0/1">Mustafa Yalcin</a></p>
<p>Anticancer peptides (ACPs) are a class of molecules that have gained
significant attention in the field of cancer research and therapy. ACPs are
short chains of amino acids, the building blocks of proteins, and they possess
the ability to selectively target and kill cancer cells. One of the key
advantages of ACPs is their ability to selectively target cancer cells while
sparing healthy cells to a greater extent. This selectivity is often attributed
to differences in the surface properties of cancer cells compared to normal
cells. That is why ACPs are being investigated as potential candidates for
cancer therapy. ACPs may be used alone or in combination with other treatment
modalities like chemotherapy and radiation therapy. While ACPs hold promise as
a novel approach to cancer treatment, there are challenges to overcome,
including optimizing their stability, improving selectivity, and enhancing
their delivery to cancer cells, continuous increasing in number of peptide
sequences, developing a reliable and precise prediction model. In this work, we
propose an efficient transformer-based framework to identify anticancer
peptides for by performing accurate a reliable and precise prediction model.
For this purpose, four different transformer models, namely ESM, ProtBert,
BioBERT, and SciBERT are employed to detect anticancer peptides from amino acid
sequences. To demonstrate the contribution of the proposed framework, extensive
experiments are carried on widely-used datasets in the literature, two versions
of AntiCp2, cACP-DeepGram, ACP-740. Experiment results show the usage of
proposed model enhances classification accuracy when compared to the
state-of-the-art studies. The proposed framework, ESM, exhibits 96.45 of
accuracy for AntiCp2 dataset, 97.66 of accuracy for cACP-DeepGram dataset, and
88.51 of accuracy for ACP-740 dataset, thence determining new state-of-the-art.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02135">PosCUDA: Position based Convolution for Unlearnable Audio Datasets. (arXiv:2401.02135v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gokul_V/0/1/0/all/0/1">Vignesh Gokul</a>, <a href="http://arxiv.org/find/cs/1/au:+Dubnov_S/0/1/0/all/0/1">Shlomo Dubnov</a></p>
<p>Deep learning models require large amounts of clean data to acheive good
performance. To avoid the cost of expensive data acquisition, researchers use
the abundant data available on the internet. This raises significant privacy
concerns on the potential misuse of personal data for model training without
authorisation. Recent works such as CUDA propose solutions to this problem by
adding class-wise blurs to make datasets unlearnable, i.e a model can never use
the acquired dataset for learning. However these methods often reduce the
quality of the data making it useless for practical applications. We introduce
PosCUDA, a position based convolution for creating unlearnable audio datasets.
PosCUDA uses class-wise convolutions on small patches of audio. The location of
the patches are based on a private key for each class, hence the model learns
the relations between positional blurs and labels, while failing to generalize.
We empirically show that PosCUDA can achieve unlearnability while maintaining
the quality of the original audio datasets. Our proposed method is also robust
to different audio feature representations such as MFCC, raw audio and
different architectures such as transformers, convolutional networks etc.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02143">Graph Neural Networks for Tabular Data Learning: A Survey with Taxonomy and Directions. (arXiv:2401.02143v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Cheng-Te Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1">Yu-Che Tsai</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chih-Yao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1">Jay Chiehen Liao</a></p>
<p>In this survey, we dive into Tabular Data Learning (TDL) using Graph Neural
Networks (GNNs), a domain where deep learning-based approaches have
increasingly shown superior performance in both classification and regression
tasks compared to traditional methods. The survey highlights a critical gap in
deep neural TDL methods: the underrepresentation of latent correlations among
data instances and feature values. GNNs, with their innate capability to model
intricate relationships and interactions between diverse elements of tabular
data, have garnered significant interest and application across various TDL
domains. Our survey provides a systematic review of the methods involved in
designing and implementing GNNs for TDL (GNN4TDL). It encompasses a detailed
investigation into the foundational aspects and an overview of GNN-based TDL
methods, offering insights into their evolving landscape. We present a
comprehensive taxonomy focused on constructing graph structures and
representation learning within GNN-based TDL methods. In addition, the survey
examines various training plans, emphasizing the integration of auxiliary tasks
to enhance the effectiveness of instance representations. A critical part of
our discussion is dedicated to the practical application of GNNs across a
spectrum of GNN4TDL scenarios, demonstrating their versatility and impact.
Lastly, we discuss the limitations and propose future research directions,
aiming to spur advancements in GNN4TDL. This survey serves as a resource for
researchers and practitioners, offering a thorough understanding of GNNs' role
in revolutionizing TDL and pointing towards future innovations in this
promising area.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02154">Disentangle Estimation of Causal Effects from Cross-Silo Data. (arXiv:2401.02154v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuxuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haozhao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1">Zhiming He</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Wenchao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jialiang Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1">Fan Yang</a></p>
<p>Estimating causal effects among different events is of great importance to
critical fields such as drug development. Nevertheless, the data features
associated with events may be distributed across various silos and remain
private within respective parties, impeding direct information exchange between
them. This, in turn, can result in biased estimations of local causal effects,
which rely on the characteristics of only a subset of the covariates. To tackle
this challenge, we introduce an innovative disentangle architecture designed to
facilitate the seamless cross-silo transmission of model parameters, enriched
with causal mechanisms, through a combination of shared and private branches.
Besides, we introduce global constraints into the equation to effectively
mitigate bias within the various missing domains, thereby elevating the
accuracy of our causal effect estimation. Extensive experiments conducted on
new semi-synthetic datasets show that our method outperforms state-of-the-art
baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02183">FairGridSearch: A Framework to Compare Fairness-Enhancing Models. (arXiv:2401.02183v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1">Shih-Chi Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Ermakova_T/0/1/0/all/0/1">Tatiana Ermakova</a>, <a href="http://arxiv.org/find/cs/1/au:+Fabian_B/0/1/0/all/0/1">Benjamin Fabian</a></p>
<p>Machine learning models are increasingly used in critical decision-making
applications. However, these models are susceptible to replicating or even
amplifying bias present in real-world data. While there are various bias
mitigation methods and base estimators in the literature, selecting the optimal
model for a specific application remains challenging.
</p>
<p>This paper focuses on binary classification and proposes FairGridSearch, a
novel framework for comparing fairness-enhancing models. FairGridSearch enables
experimentation with different model parameter combinations and recommends the
best one. The study applies FairGridSearch to three popular datasets (Adult,
COMPAS, and German Credit) and analyzes the impacts of metric selection, base
estimator choice, and classification threshold on model fairness.
</p>
<p>The results highlight the significance of selecting appropriate accuracy and
fairness metrics for model evaluation. Additionally, different base estimators
and classification threshold values affect the effectiveness of bias mitigation
methods and fairness stability respectively, but the effects are not consistent
across all datasets. Based on these findings, future research on fairness in
machine learning should consider a broader range of factors when building fair
models, going beyond bias mitigation methods alone.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02192">Nodule detection and generation on chest X-rays: NODE21 Challenge. (arXiv:2401.02192v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Sogancioglu_E/0/1/0/all/0/1">Ecem Sogancioglu</a>, <a href="http://arxiv.org/find/eess/1/au:+Ginneken_B/0/1/0/all/0/1">Bram van Ginneken</a>, <a href="http://arxiv.org/find/eess/1/au:+Behrendt_F/0/1/0/all/0/1">Finn Behrendt</a>, <a href="http://arxiv.org/find/eess/1/au:+Bengs_M/0/1/0/all/0/1">Marcel Bengs</a>, <a href="http://arxiv.org/find/eess/1/au:+Schlaefer_A/0/1/0/all/0/1">Alexander Schlaefer</a>, <a href="http://arxiv.org/find/eess/1/au:+Radu_M/0/1/0/all/0/1">Miron Radu</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_D/0/1/0/all/0/1">Di Xu</a>, <a href="http://arxiv.org/find/eess/1/au:+Sheng_K/0/1/0/all/0/1">Ke Sheng</a>, <a href="http://arxiv.org/find/eess/1/au:+Scalzo_F/0/1/0/all/0/1">Fabien Scalzo</a>, <a href="http://arxiv.org/find/eess/1/au:+Marcus_E/0/1/0/all/0/1">Eric Marcus</a>, <a href="http://arxiv.org/find/eess/1/au:+Papa_S/0/1/0/all/0/1">Samuele Papa</a>, <a href="http://arxiv.org/find/eess/1/au:+Teuwen_J/0/1/0/all/0/1">Jonas Teuwen</a>, <a href="http://arxiv.org/find/eess/1/au:+Scholten_E/0/1/0/all/0/1">Ernst Th. Scholten</a>, <a href="http://arxiv.org/find/eess/1/au:+Schalekamp_S/0/1/0/all/0/1">Steven Schalekamp</a>, <a href="http://arxiv.org/find/eess/1/au:+Hendrix_N/0/1/0/all/0/1">Nils Hendrix</a>, <a href="http://arxiv.org/find/eess/1/au:+Jacobs_C/0/1/0/all/0/1">Colin Jacobs</a>, <a href="http://arxiv.org/find/eess/1/au:+Hendrix_W/0/1/0/all/0/1">Ward Hendrix</a>, <a href="http://arxiv.org/find/eess/1/au:+Sanchez_C/0/1/0/all/0/1">Clara I S&#xe1;nchez</a>, <a href="http://arxiv.org/find/eess/1/au:+Murphy_K/0/1/0/all/0/1">Keelin Murphy</a></p>
<p>Pulmonary nodules may be an early manifestation of lung cancer, the leading
cause of cancer-related deaths among both men and women. Numerous studies have
established that deep learning methods can yield high-performance levels in the
detection of lung nodules in chest X-rays. However, the lack of gold-standard
public datasets slows down the progression of the research and prevents
benchmarking of methods for this task. To address this, we organized a public
research challenge, NODE21, aimed at the detection and generation of lung
nodules in chest X-rays. While the detection track assesses state-of-the-art
nodule detection systems, the generation track determines the utility of nodule
generation algorithms to augment training data and hence improve the
performance of the detection systems. This paper summarizes the results of the
NODE21 challenge and performs extensive additional experiments to examine the
impact of the synthetically generated nodule training images on the detection
algorithm performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02199">LADRI: LeArning-based Dynamic Risk Indicator in Automated Driving System. (arXiv:2401.02199v1 [eess.SY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Patel_A/0/1/0/all/0/1">Anil Ranjitbhai Patel</a>, <a href="http://arxiv.org/find/eess/1/au:+Liggesmeyer_P/0/1/0/all/0/1">Peter Liggesmeyer</a></p>
<p>As the horizon of intelligent transportation expands with the evolution of
Automated Driving Systems (ADS), ensuring paramount safety becomes more
imperative than ever. Traditional risk assessment methodologies, primarily
crafted for human-driven vehicles, grapple to adequately adapt to the
multifaceted, evolving environments of ADS. This paper introduces a framework
for real-time Dynamic Risk Assessment (DRA) in ADS, harnessing the potency of
Artificial Neural Networks (ANNs).
</p>
<p>Our proposed solution transcends these limitations, drawing upon ANNs, a
cornerstone of deep learning, to meticulously analyze and categorize risk
dimensions using real-time On-board Sensor (OBS) data. This learning-centric
approach not only elevates the ADS's situational awareness but also enriches
its understanding of immediate operational contexts. By dissecting OBS data,
the system is empowered to pinpoint its current risk profile, thereby enhancing
safety prospects for onboard passengers and the broader traffic ecosystem.
</p>
<p>Through this framework, we chart a direction in risk assessment, bridging the
conventional voids and enhancing the proficiency of ADS. By utilizing ANNs, our
methodology offers a perspective, allowing ADS to adeptly navigate and react to
potential risk factors, ensuring safer and more informed autonomous journeys.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02203">Robust bilinear factor analysis based on the matrix-variate $t$ distribution. (arXiv:2401.02203v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Ma_X/0/1/0/all/0/1">Xuan Ma</a>, <a href="http://arxiv.org/find/stat/1/au:+Zhao_J/0/1/0/all/0/1">Jianhua Zhao</a>, <a href="http://arxiv.org/find/stat/1/au:+Shang_C/0/1/0/all/0/1">Changchun Shang</a>, <a href="http://arxiv.org/find/stat/1/au:+Jiang_F/0/1/0/all/0/1">Fen Jiang</a>, <a href="http://arxiv.org/find/stat/1/au:+Yu_P/0/1/0/all/0/1">Philip L.H. Yu</a></p>
<p>Factor Analysis based on multivariate $t$ distribution ($t$fa) is a useful
robust tool for extracting common factors on heavy-tailed or contaminated data.
However, $t$fa is only applicable to vector data. When $t$fa is applied to
matrix data, it is common to first vectorize the matrix observations. This
introduces two challenges for $t$fa: (i) the inherent matrix structure of the
data is broken, and (ii) robustness may be lost, as vectorized matrix data
typically results in a high data dimension, which could easily lead to the
breakdown of $t$fa. To address these issues, starting from the intrinsic matrix
structure of matrix data, a novel robust factor analysis model, namely bilinear
factor analysis built on the matrix-variate $t$ distribution ($t$bfa), is
proposed in this paper. The novelty is that it is capable to simultaneously
extract common factors for both row and column variables of interest on
heavy-tailed or contaminated matrix data. Two efficient algorithms for maximum
likelihood estimation of $t$bfa are developed. Closed-form expression for the
Fisher information matrix to calculate the accuracy of parameter estimates are
derived. Empirical studies are conducted to understand the proposed $t$bfa
model and compare with related competitors. The results demonstrate the
superiority and practicality of $t$bfa. Importantly, $t$bfa exhibits a
significantly higher breakdown point than $t$fa, making it more suitable for
matrix data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02225">Trajectory-Oriented Policy Optimization with Sparse Rewards. (arXiv:2401.02225v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guojian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1">Faguo Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiao Zhang</a></p>
<p>Deep reinforcement learning (DRL) remains challenging in tasks with sparse
rewards. These sparse rewards often only indicate whether the task is partially
or fully completed, meaning that many exploration actions must be performed
before the agent obtains useful feedback. Hence, most existing DRL algorithms
fail to learn feasible policies within a reasonable time frame. To overcome
this problem, we develop an approach that exploits offline demonstration
trajectories for faster and more efficient online RL in sparse reward settings.
Our key insight is that by regarding offline demonstration trajectories as
guidance, instead of imitating them, our method learns a policy whose
state-action visitation marginal distribution matches that of offline
demonstrations. Specifically, we introduce a novel trajectory distance based on
maximum mean discrepancy (MMD) and formulate policy optimization as a
distance-constrained optimization problem. Then, we show that this
distance-constrained optimization problem can be reduced into a policy-gradient
algorithm with shaped rewards learned from offline demonstrations. The proposed
algorithm is evaluated on extensive discrete and continuous control tasks with
sparse and deceptive rewards. The experimental results indicate that our
proposed algorithm is significantly better than the baseline methods regarding
diverse exploration and learning the optimal policy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02236">U-Mixer: An Unet-Mixer Architecture with Stationarity Correction for Time Series Forecasting. (arXiv:2401.02236v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xiang Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xuemei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1">Lexin Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1">Tianlong Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Caiming Zhang</a></p>
<p>Time series forecasting is a crucial task in various domains. Caused by
factors such as trends, seasonality, or irregular fluctuations, time series
often exhibits non-stationary. It obstructs stable feature propagation through
deep layers, disrupts feature distributions, and complicates learning data
distribution changes. As a result, many existing models struggle to capture the
underlying patterns, leading to degraded forecasting performance. In this
study, we tackle the challenge of non-stationarity in time series forecasting
with our proposed framework called U-Mixer. By combining Unet and Mixer,
U-Mixer effectively captures local temporal dependencies between different
patches and channels separately to avoid the influence of distribution
variations among channels, and merge low- and high-levels features to obtain
comprehensive data representations. The key contribution is a novel
stationarity correction method, explicitly restoring data distribution by
constraining the difference in stationarity between the data before and after
model processing to restore the non-stationarity information, while ensuring
the temporal dependencies are preserved. Through extensive experiments on
various real-world time series datasets, U-Mixer demonstrates its effectiveness
and robustness, and achieves 14.5\% and 7.7\% improvements over
state-of-the-art (SOTA) methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02244">Policy-regularized Offline Multi-objective Reinforcement Learning. (arXiv:2401.02244v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1">Qian Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1">Chao Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zongkai Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zifan Wu</a></p>
<p>In this paper, we aim to utilize only offline trajectory data to train a
policy for multi-objective RL. We extend the offline policy-regularized method,
a widely-adopted approach for single-objective offline RL problems, into the
multi-objective setting in order to achieve the above goal. However, such
methods face a new challenge in offline MORL settings, namely the
preference-inconsistent demonstration problem. We propose two solutions to this
problem: 1) filtering out preference-inconsistent demonstrations via
approximating behavior preferences, and 2) adopting regularization techniques
with high policy expressiveness. Moreover, we integrate the
preference-conditioned scalarized update method into policy-regularized offline
RL, in order to simultaneously learn a set of policies using a single policy
network, thus reducing the computational cost induced by the training of a
large number of individual policies for various preferences. Finally, we
introduce Regularization Weight Adaptation to dynamically determine appropriate
regularization weights for arbitrary target preferences during deployment.
Empirical results on various multi-objective datasets demonstrate the
capability of our approach in solving offline MORL problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02254">L3Cube-IndicNews: News-based Short Text and Long Document Classification Datasets in Indic Languages. (arXiv:2401.02254v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mirashi_A/0/1/0/all/0/1">Aishwarya Mirashi</a>, <a href="http://arxiv.org/find/cs/1/au:+Sonavane_S/0/1/0/all/0/1">Srushti Sonavane</a>, <a href="http://arxiv.org/find/cs/1/au:+Lingayat_P/0/1/0/all/0/1">Purva Lingayat</a>, <a href="http://arxiv.org/find/cs/1/au:+Padhiyar_T/0/1/0/all/0/1">Tejas Padhiyar</a>, <a href="http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1">Raviraj Joshi</a></p>
<p>In this work, we introduce L3Cube-IndicNews, a multilingual text
classification corpus aimed at curating a high-quality dataset for Indian
regional languages, with a specific focus on news headlines and articles. We
have centered our work on 10 prominent Indic languages, including Hindi,
Bengali, Marathi, Telugu, Tamil, Gujarati, Kannada, Odia, Malayalam, and
Punjabi. Each of these news datasets comprises 10 or more classes of news
articles. L3Cube-IndicNews offers 3 distinct datasets tailored to handle
different document lengths that are classified as: Short Headlines
Classification (SHC) dataset containing the news headline and news category,
Long Document Classification (LDC) dataset containing the whole news article
and the news category, and Long Paragraph Classification (LPC) containing
sub-articles of the news and the news category. We maintain consistent labeling
across all 3 datasets for in-depth length-based analysis. We evaluate each of
these Indic language datasets using 4 different models including monolingual
BERT, multilingual Indic Sentence BERT (IndicSBERT), and IndicBERT. This
research contributes significantly to expanding the pool of available text
classification datasets and also makes it possible to develop topic
classification models for Indian regional languages. This also serves as an
excellent resource for cross-lingual analysis owing to the high overlap of
labels among languages. The datasets and models are shared publicly at
https://github.com/l3cube-pune/indic-nlp
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02255">Balancing Continual Learning and Fine-tuning for Human Activity Recognition. (arXiv:2401.02255v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1">Chi Ian Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qendro_L/0/1/0/all/0/1">Lorena Qendro</a>, <a href="http://arxiv.org/find/cs/1/au:+Spathis_D/0/1/0/all/0/1">Dimitris Spathis</a>, <a href="http://arxiv.org/find/cs/1/au:+Kawsar_F/0/1/0/all/0/1">Fahim Kawsar</a>, <a href="http://arxiv.org/find/cs/1/au:+Mathur_A/0/1/0/all/0/1">Akhil Mathur</a>, <a href="http://arxiv.org/find/cs/1/au:+Mascolo_C/0/1/0/all/0/1">Cecilia Mascolo</a></p>
<p>Wearable-based Human Activity Recognition (HAR) is a key task in
human-centric machine learning due to its fundamental understanding of human
behaviours. Due to the dynamic nature of human behaviours, continual learning
promises HAR systems that are tailored to users' needs. However, because of the
difficulty in collecting labelled data with wearable sensors, existing
approaches that focus on supervised continual learning have limited
applicability, while unsupervised continual learning methods only handle
representation learning while delaying classifier training to a later stage.
This work explores the adoption and adaptation of CaSSLe, a continual
self-supervised learning model, and Kaizen, a semi-supervised continual
learning model that balances representation learning and down-stream
classification, for the task of wearable-based HAR. These schemes re-purpose
contrastive learning for knowledge retention and, Kaizen combines that with
self-training in a unified scheme that can leverage unlabelled and labelled
data for continual learning. In addition to comparing state-of-the-art
self-supervised continual learning schemes, we further investigated the
importance of different loss terms and explored the trade-off between knowledge
retention and learning from new tasks. In particular, our extensive evaluation
demonstrated that the use of a weighting factor that reflects the ratio between
learned and new classes achieves the best overall trade-off in continual
learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02258">Uncertainty-Aware Deep Attention Recurrent Neural Network for Heterogeneous Time Series Imputation. (arXiv:2401.02258v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qian_L/0/1/0/all/0/1">Linglong Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Ibrahim_Z/0/1/0/all/0/1">Zina Ibrahim</a>, <a href="http://arxiv.org/find/cs/1/au:+Dobson_R/0/1/0/all/0/1">Richard Dobson</a></p>
<p>Missingness is ubiquitous in multivariate time series and poses an obstacle
to reliable downstream analysis. Although recurrent network imputation achieved
the SOTA, existing models do not scale to deep architectures that can
potentially alleviate issues arising in complex data. Moreover, imputation
carries the risk of biased estimations of the ground truth. Yet, confidence in
the imputed values is always unmeasured or computed post hoc from model output.
We propose DEep Attention Recurrent Imputation (DEARI), which jointly estimates
missing values and their associated uncertainty in heterogeneous multivariate
time series. By jointly representing feature-wise correlations and temporal
dynamics, we adopt a self attention mechanism, along with an effective residual
component, to achieve a deep recurrent neural network with good imputation
performance and stable convergence. We also leverage self-supervised metric
learning to boost performance by optimizing sample similarity. Finally, we
transform DEARI into a Bayesian neural network through a novel Bayesian
marginalization strategy to produce stochastic DEARI, which outperforms its
deterministic equivalent. Experiments show that DEARI surpasses the SOTA in
diverse imputation tasks using real-world datasets, namely air quality control,
healthcare and traffic.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02277">Universal Approximation Theorem for Vector- and Hypercomplex-Valued Neural Networks. (arXiv:2401.02277v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Valle_M/0/1/0/all/0/1">Marcos Eduardo Valle</a>, <a href="http://arxiv.org/find/cs/1/au:+Vital_W/0/1/0/all/0/1">Wington L. Vital</a>, <a href="http://arxiv.org/find/cs/1/au:+Vieira_G/0/1/0/all/0/1">Guilherme Vieira</a></p>
<p>The universal approximation theorem states that a neural network with one
hidden layer can approximate continuous functions on compact sets with any
desired precision. This theorem supports using neural networks for various
applications, including regression and classification tasks. Furthermore, it is
valid for real-valued neural networks and some hypercomplex-valued neural
networks such as complex-, quaternion-, tessarine-, and Clifford-valued neural
networks. However, hypercomplex-valued neural networks are a type of
vector-valued neural network defined on an algebra with additional algebraic or
geometric properties. This paper extends the universal approximation theorem
for a wide range of vector-valued neural networks, including
hypercomplex-valued models as particular instances. Precisely, we introduce the
concept of non-degenerate algebra and state the universal approximation theorem
for neural networks defined on such algebras.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02278">Lightweight Fish Classification Model for Sustainable Marine Management: Indonesian Case. (arXiv:2401.02278v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kurniawan_F/0/1/0/all/0/1">Febrian Kurniawan</a>, <a href="http://arxiv.org/find/cs/1/au:+Satrya_G/0/1/0/all/0/1">Gandeva Bayu Satrya</a>, <a href="http://arxiv.org/find/cs/1/au:+Kamalov_F/0/1/0/all/0/1">Firuz Kamalov</a></p>
<p>The enormous demand for seafood products has led to exploitation of marine
resources and near-extinction of some species. In particular, overfishing is
one the main issues in sustainable marine development. In alignment with the
protection of marine resources and sustainable fishing, this study proposes to
advance fish classification techniques that support identifying protected fish
species using state-of-the-art machine learning. We use a custom modification
of the MobileNet model to design a lightweight classifier called M-MobileNet
that is capable of running on limited hardware. As part of the study, we
compiled a labeled dataset of 37,462 images of fish found in the waters of the
Indonesian archipelago. The proposed model is trained on the dataset to
classify images of the captured fish into their species and give
recommendations on whether they are consumable or not. Our modified MobileNet
model uses only 50\% of the top layer parameters with about 42% GTX 860M
utility and achieves up to 97% accuracy in fish classification and determining
its consumability. Given the limited computing capacity available on many
fishing vessels, the proposed model provides a practical solution to on-site
fish classification. In addition, synchronized implementation of the proposed
model on multiple vessels can supply valuable information about the movement
and location of different species of fish.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02283">DEM: A Method for Certifying Deep Neural Network Classifier Outputs in Aerospace. (arXiv:2401.02283v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Katz_G/0/1/0/all/0/1">Guy Katz</a>, <a href="http://arxiv.org/find/cs/1/au:+Levy_N/0/1/0/all/0/1">Natan Levy</a>, <a href="http://arxiv.org/find/cs/1/au:+Refaeli_I/0/1/0/all/0/1">Idan Refaeli</a>, <a href="http://arxiv.org/find/cs/1/au:+Yerushalmi_R/0/1/0/all/0/1">Raz Yerushalmi</a></p>
<p>Software development in the aerospace domain requires adhering to strict,
high-quality standards. While there exist regulatory guidelines for commercial
software in this domain (e.g., ARP-4754 and DO-178), these do not apply to
software with deep neural network (DNN) components. Consequently, it is unclear
how to allow aerospace systems to benefit from the deep learning revolution.
Our work here seeks to address this challenge with a novel, output-centric
approach for DNN certification. Our method employs statistical verification
techniques, and has the key advantage of being able to flag specific inputs for
which the DNN's output may be unreliable - so that they may be later inspected
by a human expert. To achieve this, our method conducts a statistical analysis
of the DNN's predictions for other, nearby inputs, in order to detect
inconsistencies. This is in contrast to existing techniques, which typically
attempt to certify the entire DNN, as opposed to individual outputs. Our method
uses the DNN as a black-box, and makes no assumptions about its topology. We
hope that this work constitutes another step towards integrating DNNs in
safety-critical applications - especially in the aerospace domain, where high
standards of quality and reliability are crucial.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02290">Path-based Explanation for Knowledge Graph Completion. (arXiv:2401.02290v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1">Heng Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1">Jiangnan Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Avila_A/0/1/0/all/0/1">Alejo Lopez Avila</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1">Jinhua Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jia Li</a></p>
<p>Graph Neural Networks (GNNs) have achieved great success in Knowledge Graph
Completion (KGC) by modelling how entities and relations interact in recent
years. However, the explanation of the predicted facts has not caught the
necessary attention. Proper explanations for the results of GNN-based KGC
models increase model transparency and help researchers develop more reliable
models. Existing practices for explaining KGC tasks rely on
instance/subgraph-based approaches, while in some scenarios, paths can provide
more user-friendly and interpretable explanations. Nonetheless, the methods for
generating path-based explanations for KGs have not been well-explored. To
address this gap, we propose Power-Link, the first path-based KGC explainer
that explores GNN-based models. We design a novel simplified graph-powering
technique, which enables the generation of path-based explanations with a fully
parallelisable and memory-efficient training scheme. We further introduce three
new metrics for quantitative evaluation of the explanations, together with a
qualitative human evaluation. Extensive experiments demonstrate that Power-Link
outperforms the SOTA baselines in interpretability, efficiency, and
scalability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02296">Training Single-Layer Morphological Perceptron Using Convex-Concave Programming. (arXiv:2401.02296v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cunha_I/0/1/0/all/0/1">Iara Cunha</a>, <a href="http://arxiv.org/find/cs/1/au:+Valle_M/0/1/0/all/0/1">Marcos Eduardo Valle</a></p>
<p>This paper concerns the training of a single-layer morphological perceptron
using disciplined convex-concave programming (DCCP). We introduce an algorithm
referred to as K-DDCCP, which combines the existing single-layer morphological
perceptron (SLMP) model proposed by Ritter and Urcid with the weighted
disciplined convex-concave programming (WDCCP) algorithm by Charisopoulos and
Maragos. The proposed training algorithm leverages the disciplined
convex-concave procedure (DCCP) and formulates a non-convex optimization
problem for binary classification. To tackle this problem, the constraints are
expressed as differences of convex functions, enabling the application of the
DCCP package. The experimental results confirm the effectiveness of the K-DDCCP
algorithm in solving binary classification problems. Overall, this work
contributes to the field of morphological neural networks by proposing an
algorithm that extends the capabilities of the SLMP model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02300">Robust Physics Informed Neural Networks. (arXiv:2401.02300v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Los_M/0/1/0/all/0/1">Marcin &#x141;o&#x15b;</a>, <a href="http://arxiv.org/find/cs/1/au:+Paszynski_M/0/1/0/all/0/1">Maciej Paszy&#x144;ski</a></p>
<p>We introduce a Robust version of the Physics-Informed Neural Networks
(RPINNs) to approximate the Partial Differential Equations (PDEs) solution.
Standard Physics Informed Neural Networks (PINN) takes into account the
governing physical laws described by PDE during the learning process. The
network is trained on a data set that consists of randomly selected points in
the physical domain and its boundary. PINNs have been successfully applied to
solve various problems described by PDEs with boundary conditions. The loss
function in traditional PINNs is based on the strong residuals of the PDEs.
This loss function in PINNs is generally not robust with respect to the true
error. The loss function in PINNs can be far from the true error, which makes
the training process more difficult. In particular, we do not know if the
training process has already converged to the solution with the required
accuracy. This is especially true if we do not know the exact solution, so we
cannot estimate the true error during the training. This paper introduces a
different way of defining the loss function. It incorporates the residual and
the inverse of the Gram matrix, computed using the energy norm. We test our
RPINN algorithm on two Laplace problems and one advection-diffusion problem in
two spatial dimensions. We conclude that RPINN is a robust method. The proposed
loss coincides well with the true error of the solution, as measured in the
energy norm. Thus, we know if our training process goes well, and we know when
to stop the training to obtain the neural network approximation of the solution
of the PDE with the true error of required accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02323">Multi-Agent Context Learning Strategy for Interference-Aware Beam Allocation in mmWave Vehicular Communications. (arXiv:2401.02323v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Kose_A/0/1/0/all/0/1">Abdulkadir Kose</a>, <a href="http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1">Haeyoung Lee</a>, <a href="http://arxiv.org/find/eess/1/au:+Foh_C/0/1/0/all/0/1">Chuan Heng Foh</a>, <a href="http://arxiv.org/find/eess/1/au:+Shojafar_M/0/1/0/all/0/1">Mohammad Shojafar</a></p>
<p>Millimeter wave (mmWave) has been recognized as one of key technologies for
5G and beyond networks due to its potential to enhance channel bandwidth and
network capacity. The use of mmWave for various applications including
vehicular communications has been extensively discussed. However, applying
mmWave to vehicular communications faces challenges of high mobility nodes and
narrow coverage along the mmWave beams. Due to high mobility in dense networks,
overlapping beams can cause strong interference which leads to performance
degradation. As a remedy, beam switching capability in mmWave can be utilized.
Then, frequent beam switching and cell change become inevitable to manage
interference, which increase computational and signalling complexity. In order
to deal with the complexity in interference control, we develop a new strategy
called Multi-Agent Context Learning (MACOL), which utilizes Contextual Bandit
to manage interference while allocating mmWave beams to serve vehicles in the
network. Our approach demonstrates that by leveraging knowledge of neighbouring
beam status, the machine learning agent can identify and avoid potential
interfering transmissions to other ongoing transmissions. Furthermore, we show
that even under heavy traffic loads, our proposed MACOL strategy is able to
maintain low interference levels at around 10%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02325">A Robust Quantile Huber Loss With Interpretable Parameter Adjustment In Distributional Reinforcement Learning. (arXiv:2401.02325v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Malekzadeh_P/0/1/0/all/0/1">Parvin Malekzadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Plataniotis_K/0/1/0/all/0/1">Konstantinos N. Plataniotis</a>, <a href="http://arxiv.org/find/cs/1/au:+Poulos_Z/0/1/0/all/0/1">Zissis Poulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zeyu Wang</a></p>
<p>Distributional Reinforcement Learning (RL) estimates return distribution
mainly by learning quantile values via minimizing the quantile Huber loss
function, entailing a threshold parameter often selected heuristically or via
hyperparameter search, which may not generalize well and can be suboptimal.
This paper introduces a generalized quantile Huber loss function derived from
Wasserstein distance (WD) calculation between Gaussian distributions, capturing
noise in predicted (current) and target (Bellman-updated) quantile values.
Compared to the classical quantile Huber loss, this innovative loss function
enhances robustness against outliers. Notably, the classical Huber loss
function can be seen as an approximation of our proposed loss, enabling
parameter adjustment by approximating the amount of noise in the data during
the learning process. Empirical tests on Atari games, a common application in
distributional RL, and a recent hedging strategy using distributional RL,
validate the effectiveness of our proposed loss function and its potential for
parameter adjustments in distributional RL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02329">Not all Minorities are Equal: Empty-Class-Aware Distillation for Heterogeneous Federated Learning. (arXiv:2401.02329v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1">Kuangpu Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1">Yuhe Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1">Jian Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1">Ran He</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zilei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1">Tieniu Tan</a></p>
<p>Data heterogeneity, characterized by disparities in local data distribution
across clients, poses a significant challenge in federated learning.
Substantial efforts have been devoted to addressing the heterogeneity in local
label distribution. As minority classes suffer from worse accuracy due to
overfitting on local imbalanced data, prior methods often incorporate
class-balanced learning techniques during local training. Despite the improved
mean accuracy across all classes, we observe that empty classes-referring to
categories absent from a client's data distribution-are still not well
recognized. This paper introduces FedED, a novel approach in heterogeneous
federated learning that integrates both empty-class distillation and logit
suppression simultaneously. Specifically, empty-class distillation leverages
knowledge distillation during local training on each client to retain essential
information related to empty classes from the global model. Moreover, logit
suppression directly penalizes network logits for non-label classes,
effectively addressing misclassifications in minority classes that may be
biased toward majority classes. Extensive experiments validate the efficacy of
FedED, surpassing previous state-of-the-art methods across diverse datasets
with varying degrees of label distribution shift.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02333">Beyond Extraction: Contextualising Tabular Data for Efficient Summarisation by Language Models. (arXiv:2401.02333v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Allu_U/0/1/0/all/0/1">Uday Allu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_B/0/1/0/all/0/1">Biddwan Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Tripathi_V/0/1/0/all/0/1">Vishesh Tripathi</a></p>
<p>The conventional use of the Retrieval-Augmented Generation (RAG) architecture
has proven effective for retrieving information from diverse documents.
However, challenges arise in handling complex table queries, especially within
PDF documents containing intricate tabular structures.This research introduces
an innovative approach to enhance the accuracy of complex table queries in
RAG-based systems. Our methodology involves storing PDFs in the retrieval
database and extracting tabular content separately. The extracted tables
undergo a process of context enrichment, concatenating headers with
corresponding values. To ensure a comprehensive understanding of the enriched
data, we employ a fine-tuned version of the Llama-2-chat language model for
summarisation within the RAG architecture. Furthermore, we augment the tabular
data with contextual sense using the ChatGPT 3.5 API through a one-shot prompt.
This enriched data is then fed into the retrieval database alongside other
PDFs. Our approach aims to significantly improve the precision of complex table
queries, offering a promising solution to a longstanding challenge in
information retrieval.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02342">Evasive Hardware Trojan through Adversarial Power Trace. (arXiv:2401.02342v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Omidi_B/0/1/0/all/0/1">Behnam Omidi</a>, <a href="http://arxiv.org/find/cs/1/au:+Khasawneh_K/0/1/0/all/0/1">Khaled N. Khasawneh</a>, <a href="http://arxiv.org/find/cs/1/au:+Alouani_I/0/1/0/all/0/1">Ihsen Alouani</a></p>
<p>The globalization of the Integrated Circuit (IC) supply chain, driven by
time-to-market and cost considerations, has made ICs vulnerable to hardware
Trojans (HTs). Against this threat, a promising approach is to use Machine
Learning (ML)-based side-channel analysis, which has the advantage of being a
non-intrusive method, along with efficiently detecting HTs under golden
chip-free settings. In this paper, we question the trustworthiness of ML-based
HT detection via side-channel analysis. We introduce a HT obfuscation (HTO)
approach to allow HTs to bypass this detection method. Rather than
theoretically misleading the model by simulated adversarial traces, a key
aspect of our approach is the design and implementation of adversarial noise as
part of the circuitry, alongside the HT. We detail HTO methodologies for ASICs
and FPGAs, and evaluate our approach using TrustHub benchmark. Interestingly,
we found that HTO can be implemented with only a single transistor for ASIC
designs to generate adversarial power traces that can fool the defense with
100% efficiency. We also efficiently implemented our approach on a Spartan 6
Xilinx FPGA using 2 different variants: (i) DSP slices-based, and (ii)
ring-oscillator-based design. Additionally, we assess the efficiency of
countermeasures like spectral domain analysis, and we show that an adaptive
attacker can still design evasive HTOs by constraining the design with a
spectral noise budget. In addition, while adversarial training (AT) offers
higher protection against evasive HTs, AT models suffer from a considerable
utility loss, potentially rendering them unsuitable for such security
application. We believe this research represents a significant step in
understanding and exploiting ML vulnerabilities in a hardware security context,
and we make all resources and designs openly available online:
https://dev.d18uu4lqwhbmka.amplifyapp.com
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02344">Multi-Source Domain Adaptation with Transformer-based Feature Generation for Subject-Independent EEG-based Emotion Recognition. (arXiv:2401.02344v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sartipi_S/0/1/0/all/0/1">Shadi Sartipi</a>, <a href="http://arxiv.org/find/cs/1/au:+Cetin_M/0/1/0/all/0/1">Mujdat Cetin</a></p>
<p>Although deep learning-based algorithms have demonstrated excellent
performance in automated emotion recognition via electroencephalogram (EEG)
signals, variations across brain signal patterns of individuals can diminish
the model's effectiveness when applied across different subjects. While
transfer learning techniques have exhibited promising outcomes, they still
encounter challenges related to inadequate feature representations and may
overlook the fact that source subjects themselves can possess distinct
characteristics. In this work, we propose a multi-source domain adaptation
approach with a transformer-based feature generator (MSDA-TF) designed to
leverage information from multiple sources. The proposed feature generator
retains convolutional layers to capture shallow spatial, temporal, and spectral
EEG data representations, while self-attention mechanisms extract global
dependencies within these features. During the adaptation process, we group the
source subjects based on correlation values and aim to align the moments of the
target subject with each source as well as within the sources. MSDA-TF is
validated on the SEED dataset and is shown to yield promising results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02349">A Survey Analyzing Generalization in Deep Reinforcement Learning. (arXiv:2401.02349v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Korkmaz_E/0/1/0/all/0/1">Ezgi Korkmaz</a></p>
<p>Reinforcement learning research obtained significant success and attention
with the utilization of deep neural networks to solve problems in high
dimensional state or action spaces. While deep reinforcement learning policies
are currently being deployed in many different fields from medical applications
to self driving vehicles, there are still ongoing questions the field is trying
to answer on the generalization capabilities of deep reinforcement learning
policies. In this paper, we will outline the fundamental reasons why deep
reinforcement learning policies encounter overfitting problems that limit their
robustness and generalization capabilities. Furthermore, we will formalize and
unify the diverse solution approaches to increase generalization, and overcome
overfitting in state-action value functions. We believe our study can provide a
compact systematic unified analysis for the current advancements in deep
reinforcement learning, and help to construct robust deep neural policies with
improved generalization abilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02363">Integration of physics-informed operator learning and finite element method for parametric learning of partial differential equations. (arXiv:2401.02363v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rezaei_S/0/1/0/all/0/1">Shahed Rezaei</a>, <a href="http://arxiv.org/find/cs/1/au:+Moeineddin_A/0/1/0/all/0/1">Ahmad Moeineddin</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaliske_M/0/1/0/all/0/1">Michael Kaliske</a>, <a href="http://arxiv.org/find/cs/1/au:+Apel_M/0/1/0/all/0/1">Markus Apel</a></p>
<p>We present a method that employs physics-informed deep learning techniques
for parametrically solving partial differential equations. The focus is on the
steady-state heat equations within heterogeneous solids exhibiting significant
phase contrast. Similar equations manifest in diverse applications like
chemical diffusion, electrostatics, and Darcy flow. The neural network aims to
establish the link between the complex thermal conductivity profiles and
temperature distributions, as well as heat flux components within the
microstructure, under fixed boundary conditions. A distinctive aspect is our
independence from classical solvers like finite element methods for data. A
noteworthy contribution lies in our novel approach to defining the loss
function, based on the discretized weak form of the governing equation. This
not only reduces the required order of derivatives but also eliminates the need
for automatic differentiation in the construction of loss terms, accepting
potential numerical errors from the chosen discretization method. As a result,
the loss function in this work is an algebraic equation that significantly
enhances training efficiency. We benchmark our methodology against the standard
finite element method, demonstrating accurate yet faster predictions using the
trained neural network for temperature and flux profiles. We also show higher
accuracy by using the proposed method compared to purely data-driven approaches
for unforeseen scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02398">Generating synthetic data for neural operators. (arXiv:2401.02398v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hasani_E/0/1/0/all/0/1">Erisa Hasani</a>, <a href="http://arxiv.org/find/cs/1/au:+Ward_R/0/1/0/all/0/1">Rachel A. Ward</a></p>
<p>Numerous developments in the recent literature show the promising potential
of deep learning in obtaining numerical solutions to partial differential
equations (PDEs) beyond the reach of current numerical solvers. However,
data-driven neural operators all suffer from the same problem: the data needed
to train a network depends on classical numerical solvers such as finite
difference or finite element, among others. In this paper, we propose a new
approach to generating synthetic functional training data that does not require
solving a PDE numerically. The way we do this is simple: we draw a large number
$N$ of independent and identically distributed `random functions' $u_j$ from
the underlying solution space (e.g., $H_0^1(\Omega)$) in which we know the
solution lies according to classical theory. We then plug each such random
candidate solution into the equation and get a corresponding right-hand side
function $f_j$ for the equation, and consider $(f_j, u_j)_{j=1}^N$ as
supervised training data for learning the underlying inverse problem $f
\rightarrow u$. This `backwards' approach to generating training data only
requires derivative computations, in contrast to standard `forward' approaches,
which require a numerical PDE solver, enabling us to generate a large number of
such data points quickly and efficiently. While the idea is simple, we hope
that this method will expand the potential for developing neural PDE solvers
that do not depend on classical numerical solvers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02403">Real-Time 2D Temperature Field Prediction in Metal Additive Manufacturing Using Physics-Informed Neural Networks. (arXiv:2401.02403v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sajadi_P/0/1/0/all/0/1">Pouyan Sajadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Dehaghani_M/0/1/0/all/0/1">Mostafa Rahmani Dehaghani</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1">Yifan Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">G. Gary Wang</a></p>
<p>Accurately predicting the temperature field in metal additive manufacturing
(AM) processes is critical to preventing overheating, adjusting process
parameters, and ensuring process stability. While physics-based computational
models offer precision, they are often time-consuming and unsuitable for
real-time predictions and online control in iterative design scenarios.
Conversely, machine learning models rely heavily on high-quality datasets,
which can be costly and challenging to obtain within the metal AM domain. Our
work addresses this by introducing a physics-informed neural network framework
specifically designed for temperature field prediction in metal AM. This
framework incorporates a physics-informed input, physics-informed loss
function, and a Convolutional Long Short-Term Memory (ConvLSTM) architecture.
Utilizing real-time temperature data from the process, our model predicts 2D
temperature fields for future timestamps across diverse geometries, deposition
patterns, and process parameters. We validate the proposed framework in two
scenarios: full-field temperature prediction for a thin wall and 2D temperature
field prediction for cylinder and cubic parts, demonstrating errors below 3%
and 1%, respectively. Our proposed framework exhibits the flexibility to be
applied across diverse scenarios with varying process parameters, geometries,
and deposition patterns.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02411">What You See is What You GAN: Rendering Every Pixel for High-Fidelity Geometry in 3D GANs. (arXiv:2401.02411v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Trevithick_A/0/1/0/all/0/1">Alex Trevithick</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_M/0/1/0/all/0/1">Matthew Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Takikawa_T/0/1/0/all/0/1">Towaki Takikawa</a>, <a href="http://arxiv.org/find/cs/1/au:+Iqbal_U/0/1/0/all/0/1">Umar Iqbal</a>, <a href="http://arxiv.org/find/cs/1/au:+Mello_S/0/1/0/all/0/1">Shalini De Mello</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandraker_M/0/1/0/all/0/1">Manmohan Chandraker</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramamoorthi_R/0/1/0/all/0/1">Ravi Ramamoorthi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nagano_K/0/1/0/all/0/1">Koki Nagano</a></p>
<p>3D-aware Generative Adversarial Networks (GANs) have shown remarkable
progress in learning to generate multi-view-consistent images and 3D geometries
of scenes from collections of 2D images via neural volume rendering. Yet, the
significant memory and computational costs of dense sampling in volume
rendering have forced 3D GANs to adopt patch-based training or employ
low-resolution rendering with post-processing 2D super resolution, which
sacrifices multiview consistency and the quality of resolved geometry.
Consequently, 3D GANs have not yet been able to fully resolve the rich 3D
geometry present in 2D images. In this work, we propose techniques to scale
neural volume rendering to the much higher resolution of native 2D images,
thereby resolving fine-grained 3D geometry with unprecedented detail. Our
approach employs learning-based samplers for accelerating neural rendering for
3D GAN training using up to 5 times fewer depth samples. This enables us to
explicitly "render every pixel" of the full-resolution image during training
and inference without post-processing superresolution in 2D. Together with our
strategy to learn high-quality surface geometry, our method synthesizes
high-resolution 3D geometry and strictly view-consistent images while
maintaining image quality on par with baselines relying on post-processing
super resolution. We demonstrate state-of-the-art 3D gemetric quality on FFHQ
and AFHQ, setting a new standard for unsupervised learning of 3D shapes in 3D
GANs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02412">LLM Augmented LLMs: Expanding Capabilities through Composition. (arXiv:2401.02412v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bansal_R/0/1/0/all/0/1">Rachit Bansal</a>, <a href="http://arxiv.org/find/cs/1/au:+Samanta_B/0/1/0/all/0/1">Bidisha Samanta</a>, <a href="http://arxiv.org/find/cs/1/au:+Dalmia_S/0/1/0/all/0/1">Siddharth Dalmia</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1">Nitish Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Vashishth_S/0/1/0/all/0/1">Shikhar Vashishth</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganapathy_S/0/1/0/all/0/1">Sriram Ganapathy</a>, <a href="http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1">Abhishek Bapna</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1">Prateek Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1">Partha Talukdar</a></p>
<p>Foundational models with billions of parameters which have been trained on
large corpora of data have demonstrated non-trivial skills in a variety of
domains. However, due to their monolithic structure, it is challenging and
expensive to augment them or impart new skills. On the other hand, due to their
adaptation abilities, several new instances of these models are being trained
towards new domains and tasks. In this work, we study the problem of efficient
and practical composition of existing foundation models with more specific
models to enable newer capabilities. To this end, we propose CALM --
Composition to Augment Language Models -- which introduces cross-attention
between models to compose their representations and enable new capabilities.
Salient features of CALM are: (i) Scales up LLMs on new tasks by 're-using'
existing LLMs along with a few additional parameters and data, (ii) Existing
model weights are kept intact, and hence preserves existing capabilities, and
(iii) Applies to diverse domains and settings. We illustrate that augmenting
PaLM2-S with a smaller model trained on low-resource languages results in an
absolute improvement of up to 13\% on tasks like translation into English and
arithmetic reasoning for low-resource languages. Similarly, when PaLM2-S is
augmented with a code-specific model, we see a relative improvement of 40\%
over the base model for code generation and explanation tasks -- on-par with
fully fine-tuned counterparts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02413">Simulation-Based Inference with Quantile Regression. (arXiv:2401.02413v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Jia_H/0/1/0/all/0/1">He Jia</a></p>
<p>We present Neural Quantile Estimation (NQE), a novel Simulation-Based
Inference (SBI) method based on conditional quantile regression. NQE
autoregressively learns individual one dimensional quantiles for each posterior
dimension, conditioned on the data and previous posterior dimensions. Posterior
samples are obtained by interpolating the predicted quantiles using monotonic
cubic Hermite spline, with specific treatment for the tail behavior and
multi-modal distributions. We introduce an alternative definition for the
Bayesian credible region using the local Cumulative Density Function (CDF),
offering substantially faster evaluation than the traditional Highest Posterior
Density Region (HPDR). In case of limited simulation budget and/or known model
misspecification, a post-processing broadening step can be integrated into NQE
to ensure the unbiasedness of the posterior estimation with negligible
additional computational cost. We demonstrate that the proposed NQE method
achieves state-of-the-art performance on a variety of benchmark problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02416">ODIN: A Single Model for 2D and 3D Perception. (arXiv:2401.02416v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1">Ayush Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Katara_P/0/1/0/all/0/1">Pushkal Katara</a>, <a href="http://arxiv.org/find/cs/1/au:+Gkanatsios_N/0/1/0/all/0/1">Nikolaos Gkanatsios</a>, <a href="http://arxiv.org/find/cs/1/au:+Harley_A/0/1/0/all/0/1">Adam W. Harley</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarch_G/0/1/0/all/0/1">Gabriel Sarch</a>, <a href="http://arxiv.org/find/cs/1/au:+Aggarwal_K/0/1/0/all/0/1">Kriti Aggarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Chaudhary_V/0/1/0/all/0/1">Vishrav Chaudhary</a>, <a href="http://arxiv.org/find/cs/1/au:+Fragkiadaki_K/0/1/0/all/0/1">Katerina Fragkiadaki</a></p>
<p>State-of-the-art models on contemporary 3D perception benchmarks like ScanNet
consume and label dataset-provided 3D point clouds, obtained through post
processing of sensed multiview RGB-D images. They are typically trained
in-domain, forego large-scale 2D pre-training and outperform alternatives that
featurize the posed RGB-D multiview images instead. The gap in performance
between methods that consume posed images versus post-processed 3D point clouds
has fueled the belief that 2D and 3D perception require distinct model
architectures. In this paper, we challenge this view and propose ODIN
(Omni-Dimensional INstance segmentation), a model that can segment and label
both 2D RGB images and 3D point clouds, using a transformer architecture that
alternates between 2D within-view and 3D cross-view information fusion. Our
model differentiates 2D and 3D feature operations through the positional
encodings of the tokens involved, which capture pixel coordinates for 2D patch
tokens and 3D coordinates for 3D feature tokens. ODIN achieves state-of-the-art
performance on ScanNet200, Matterport3D and AI2THOR 3D instance segmentation
benchmarks, and competitive performance on ScanNet, S3DIS and COCO. It
outperforms all previous works by a wide margin when the sensed 3D point cloud
is used in place of the point cloud sampled from 3D mesh. When used as the 3D
perception engine in an instructable embodied agent architecture, it sets a new
state-of-the-art on the TEACh action-from-dialogue benchmark. Our code and
checkpoints can be found at the project website: https://odin-seg.github.io.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02417">Task Oriented Dialogue as a Catalyst for Self-Supervised Automatic Speech Recognition. (arXiv:2401.02417v1 [eess.AS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Chan_D/0/1/0/all/0/1">David M. Chan</a>, <a href="http://arxiv.org/find/eess/1/au:+Ghosh_S/0/1/0/all/0/1">Shalini Ghosh</a>, <a href="http://arxiv.org/find/eess/1/au:+Tulsiani_H/0/1/0/all/0/1">Hitesh Tulsiani</a>, <a href="http://arxiv.org/find/eess/1/au:+Rastrow_A/0/1/0/all/0/1">Ariya Rastrow</a>, <a href="http://arxiv.org/find/eess/1/au:+Hoffmeister_B/0/1/0/all/0/1">Bj&#xf6;rn Hoffmeister</a></p>
<p>While word error rates of automatic speech recognition (ASR) systems have
consistently fallen, natural language understanding (NLU) applications built on
top of ASR systems still attribute significant numbers of failures to
low-quality speech recognition results. Existing assistant systems collect
large numbers of these unsuccessful interactions, but these systems usually
fail to learn from these interactions, even in an offline fashion. In this
work, we introduce CLC: Contrastive Learning for Conversations, a family of
methods for contrastive fine-tuning of models in a self-supervised fashion,
making use of easily detectable artifacts in unsuccessful conversations with
assistants. We demonstrate that our CLC family of approaches can improve the
performance of ASR models on OD3, a new public large-scale semi-synthetic
meta-dataset of audio task-oriented dialogues, by up to 19.2%. These gains
transfer to real-world systems as well, where we show that CLC can help to
improve performance by up to 6.7% over baselines. We make OD3 publicly
available at https://github.com/amazon-science/amazon-od3 .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2011.14956">Handling Noisy Labels via One-Step Abductive Multi-Target Learning and Its Application to Helicobacter Pylori Segmentation. (arXiv:2011.14956v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yongquan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yiming Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jie Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1">Jiayi Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Zhongxi Zheng</a></p>
<p>Learning from noisy labels is an important concern in plenty of real-world
scenarios. Various approaches for this concern first make corrections
corresponding to potentially noisy-labeled instances, and then update
predictive model with information of the made corrections. However, in specific
areas, such as medical histopathology whole slide image analysis (MHWSIA), it
is often difficult or impossible for experts to manually achieve the noisy-free
ground-truth labels which leads to labels with complex noise. This situation
raises two more difficult problems: 1) the methodology of approaches making
corrections corresponding to potentially noisy-labeled instances has
limitations due to the complex noise existing in labels; and 2) the appropriate
evaluation strategy for validation/testing is unclear because of the great
difficulty in collecting the noisy-free ground-truth labels. For the problem
1), we present one-step abductive multi-target learning (OSAMTL) that imposes a
one-step logical reasoning upon machine learning via a multi-target learning
procedure to constrain the predictions of the learning model to be subject to
our prior knowledge about the true target. For the problem 2), we propose a
logical assessment formula (LAF) that evaluates the logical rationality of the
outputs of an approach by estimating the consistencies between the predictions
of the learning model and the logical facts narrated from the results of the
one-step logical reasoning of OSAMTL. Based on the Helicobacter pylori (H.
pylori) segmentation task in MHWSIA, we show that OSAMTL enables the machine
learning model achieving logically more rational predictions, which is beyond
various state-of-the-art approaches in handling complex noisy labels.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2104.10561">Covert Channel Attack to Federated Learning Systems. (arXiv:2104.10561v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Costa_G/0/1/0/all/0/1">Gabriele Costa</a>, <a href="http://arxiv.org/find/cs/1/au:+Pinelli_F/0/1/0/all/0/1">Fabio Pinelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Soderi_S/0/1/0/all/0/1">Simone Soderi</a>, <a href="http://arxiv.org/find/cs/1/au:+Tolomei_G/0/1/0/all/0/1">Gabriele Tolomei</a></p>
<p>Federated learning (FL) goes beyond traditional, centralized machine learning
by distributing model training among a large collection of edge clients. These
clients cooperatively train a global, e.g., cloud-hosted, model without
disclosing their local, private training data. The global model is then shared
among all the participants which use it for local predictions. In this paper,
we put forward a novel attacker model aiming at turning FL systems into covert
channels to implement a stealth communication infrastructure. The main
intuition is that, during federated training, a malicious sender can poison the
global model by submitting purposely crafted examples. Although the effect of
the model poisoning is negligible to other participants, and does not alter the
overall model performance, it can be observed by a malicious receiver and used
to transmit a single bit.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2107.01752">Dynamic programming by polymorphic semiring algebraic shortcut fusion. (arXiv:2107.01752v5 [cs.DS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Little_M/0/1/0/all/0/1">Max A. Little</a>, <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xi He</a>, <a href="http://arxiv.org/find/cs/1/au:+Kayas_U/0/1/0/all/0/1">Ugur Kayas</a></p>
<p>Dynamic programming (DP) is an algorithmic design paradigm for the efficient,
exact solution of otherwise intractable, combinatorial problems. However, DP
algorithm design is often presented in an ad-hoc manner. It is sometimes
difficult to justify algorithm correctness. To address this issue, this paper
presents a rigorous algebraic formalism for systematically deriving DP
algorithms, based on semiring polymorphism. We start with a specification,
construct an algorithm to compute the required solution which is self-evidently
correct because it exhaustively generates and evaluates all possible solutions
meeting the specification. We then derive, through the use of shortcut fusion,
an implementation of this algorithm which is both efficient and correct. We
also demonstrate how, with the use of semiring lifting, the specification can
be augmented with combinatorial constraints, showing how these constraints can
be fused with the algorithm. We furthermore demonstrate how existing DP
algorithms for a given combinatorial problem can be abstracted from their
original context and re-purposed.
</p>
<p>This approach can be applied to the full scope of combinatorial problems
expressible in terms of semirings. This includes, for example: optimal
probability and Viterbi decoding, probabilistic marginalization, logical
inference, fuzzy sets, differentiable softmax, relational and provenance
queries. The approach, building on ideas from the existing literature on
constructive algorithmics, exploits generic properties of polymorphic
functions, tupling and formal sums and algebraic simplifications arising from
constraint algebras. We demonstrate the effectiveness of this formalism for
some example applications arising in signal processing, bioinformatics and
reliability engineering. Python software implementing these algorithms can be
downloaded from: <a href="http://www.maxlittle.net/software/dppolyalg.zip.">this http URL</a>
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.08364">Data Valuation for Vertical Federated Learning: A Model-free and Privacy-preserving Method. (arXiv:2112.08364v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1">Xiao Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Leye Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Junjie Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1">Xiao Fang</a></p>
<p>Vertical Federated learning (VFL) is a promising paradigm for predictive
analytics, empowering an organization (i.e., task party) to enhance its
predictive models through collaborations with multiple data suppliers (i.e.,
data parties) in a decentralized and privacy-preserving way. Despite the
fast-growing interest in VFL, the lack of effective and secure tools for
assessing the value of data owned by data parties hinders the application of
VFL in business contexts. In response, we propose FedValue, a
privacy-preserving, task-specific but model-free data valuation method for VFL,
which consists of a data valuation metric and a federated computation method.
Specifically, we first introduce a novel data valuation metric, namely
MShapley-CMI. The metric evaluates a data party's contribution to a predictive
analytics task without the need of executing a machine learning model, making
it well-suited for real-world applications of VFL. Next, we develop an
innovative federated computation method that calculates the MShapley-CMI value
for each data party in a privacy-preserving manner. Extensive experiments
conducted on six public datasets validate the efficacy of FedValue for data
valuation in the context of VFL. In addition, we illustrate the practical
utility of FedValue with a case study involving federated movie
recommendations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2201.01954">Federated Optimization of Smooth Loss Functions. (arXiv:2201.01954v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jadbabaie_A/0/1/0/all/0/1">Ali Jadbabaie</a>, <a href="http://arxiv.org/find/cs/1/au:+Makur_A/0/1/0/all/0/1">Anuran Makur</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1">Devavrat Shah</a></p>
<p>In this work, we study empirical risk minimization (ERM) within a federated
learning framework, where a central server minimizes an ERM objective function
using training data that is stored across $m$ clients. In this setting, the
Federated Averaging (FedAve) algorithm is the staple for determining
$\epsilon$-approximate solutions to the ERM problem. Similar to standard
optimization algorithms, the convergence analysis of FedAve only relies on
smoothness of the loss function in the optimization parameter. However, loss
functions are often very smooth in the training data too. To exploit this
additional smoothness, we propose the Federated Low Rank Gradient Descent
(FedLRGD) algorithm. Since smoothness in data induces an approximate low rank
structure on the loss function, our method first performs a few rounds of
communication between the server and clients to learn weights that the server
can use to approximate clients' gradients. Then, our method solves the ERM
problem at the server using inexact gradient descent. To show that FedLRGD can
have superior performance to FedAve, we present a notion of federated oracle
complexity as a counterpart to canonical oracle complexity. Under some
assumptions on the loss function, e.g., strong convexity in parameter,
$\eta$-H\"older smoothness in data, etc., we prove that the federated oracle
complexity of FedLRGD scales like $\phi m(p/\epsilon)^{\Theta(d/\eta)}$ and
that of FedAve scales like $\phi m(p/\epsilon)^{3/4}$ (neglecting sub-dominant
factors), where $\phi\gg 1$ is a "communication-to-computation ratio," $p$ is
the parameter dimension, and $d$ is the data dimension. Then, we show that when
$d$ is small and the loss function is sufficiently smooth in the data, FedLRGD
beats FedAve in federated oracle complexity. Finally, in the course of
analyzing FedLRGD, we also establish a result on low rank approximation of
latent variable models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2204.06863">ULF: Unsupervised Labeling Function Correction using Cross-Validation for Weak Supervision. (arXiv:2204.06863v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sedova_A/0/1/0/all/0/1">Anastasiia Sedova</a>, <a href="http://arxiv.org/find/cs/1/au:+Roth_B/0/1/0/all/0/1">Benjamin Roth</a></p>
<p>A cost-effective alternative to manual data labeling is weak supervision
(WS), where data samples are automatically annotated using a predefined set of
labeling functions (LFs), rule-based mechanisms that generate artificial labels
for the associated classes. In this work, we investigate noise reduction
techniques for WS based on the principle of k-fold cross-validation. We
introduce a new algorithm ULF for Unsupervised Labeling Function correction,
which denoises WS data by leveraging models trained on all but some LFs to
identify and correct biases specific to the held-out LFs. Specifically, ULF
refines the allocation of LFs to classes by re-estimating this assignment on
highly reliable cross-validated samples. Evaluation on multiple datasets
confirms ULF's effectiveness in enhancing WS learning without the need for
manual labeling.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.00652">Towards Optimization and Model Selection for Domain Generalization: A Mixup-guided Solution. (arXiv:2209.00652v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1">Wang Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jindong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yidong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xing Xie</a></p>
<p>The distribution shifts between training and test data typically undermine
the performance of models. In recent years, lots of work pays attention to
domain generalization (DG) where distribution shifts exist, and target data are
unseen. Despite the progress in algorithm design, two foundational factors have
long been ignored: 1) the optimization for regularization-based objectives, and
2) the model selection for DG since no knowledge about the target domain can be
utilized. In this paper, we propose Mixup guided optimization and selection
techniques for DG. For optimization, we utilize an adapted Mixup to generate an
out-of-distribution dataset that can guide the preference direction and
optimize with Pareto optimization. For model selection, we generate a
validation dataset with a closer distance to the target distribution, and
thereby it can better represent the target data. We also present some
theoretical insights behind our proposals. Comprehensive experiments
demonstrate that our model optimization and selection techniques can largely
improve the performance of existing domain generalization algorithms and even
achieve new state-of-the-art results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.00357">Generalized Quadratic Embeddings for Nonlinear Dynamics using Deep Learning. (arXiv:2211.00357v2 [math.DS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Goyal_P/0/1/0/all/0/1">Pawan Goyal</a>, <a href="http://arxiv.org/find/math/1/au:+Benner_P/0/1/0/all/0/1">Peter Benner</a></p>
<p>The engineering design process often relies on mathematical modeling that can
describe the underlying dynamic behavior. In this work, we present a
data-driven methodology for modeling the dynamics of nonlinear systems. To
simplify this task, we aim to identify a coordinate transformation that allows
us to represent the dynamics of nonlinear systems using a common, simple model
structure. The advantage of a common simple model is that customized design
tools developed for it can be applied to study a large variety of nonlinear
systems. The simplest common model -- one can think of -- is linear, but linear
systems often fall short in accurately capturing the complex dynamics of
nonlinear systems. In this work, we propose using quadratic systems as the
common structure, inspired by the lifting principle. According to this
principle, smooth nonlinear systems can be expressed as quadratic systems in
suitable coordinates without approximation errors. However, finding these
coordinates solely from data is challenging. Here, we leverage deep learning to
identify such lifted coordinates using only data, enabling a quadratic
dynamical system to describe the system's dynamics. Additionally, we discuss
the asymptotic stability of these quadratic dynamical systems. We illustrate
the approach using data collected from various numerical examples,
demonstrating its superior performance with the existing well-known techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.05408">Controlling Moments with Kernel Stein Discrepancies. (arXiv:2211.05408v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Kanagawa_H/0/1/0/all/0/1">Heishiro Kanagawa</a>, <a href="http://arxiv.org/find/stat/1/au:+Barp_A/0/1/0/all/0/1">Alessandro Barp</a>, <a href="http://arxiv.org/find/stat/1/au:+Gretton_A/0/1/0/all/0/1">Arthur Gretton</a>, <a href="http://arxiv.org/find/stat/1/au:+Mackey_L/0/1/0/all/0/1">Lester Mackey</a></p>
<p>Kernel Stein discrepancies (KSDs) measure the quality of a distributional
approximation and can be computed even when the target density has an
intractable normalizing constant. Notable applications include the diagnosis of
approximate MCMC samplers and goodness-of-fit tests for unnormalized
statistical models. The present work analyzes the convergence control
properties of KSDs. We first show that standard KSDs used for weak convergence
control fail to control moment convergence. To address this limitation, we next
provide sufficient conditions under which alternative diffusion KSDs control
both moment and weak convergence. As an immediate consequence we develop, for
each $q &gt; 0$, the first KSDs known to exactly characterize $q$-Wasserstein
convergence.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.13535">DeepTaster: Adversarial Perturbation-Based Fingerprinting to Identify Proprietary Dataset Use in Deep Neural Networks. (arXiv:2211.13535v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1">Seonhye Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Abuadbba_A/0/1/0/all/0/1">Alsharif Abuadbba</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Moore_K/0/1/0/all/0/1">Kristen Moore</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yansong Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">Hyoungshick Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Nepal_S/0/1/0/all/0/1">Surya Nepal</a></p>
<p>Training deep neural networks (DNNs) requires large datasets and powerful
computing resources, which has led some owners to restrict redistribution
without permission. Watermarking techniques that embed confidential data into
DNNs have been used to protect ownership, but these can degrade model
performance and are vulnerable to watermark removal attacks. Recently,
DeepJudge was introduced as an alternative approach to measuring the similarity
between a suspect and a victim model. While DeepJudge shows promise in
addressing the shortcomings of watermarking, it primarily addresses situations
where the suspect model copies the victim's architecture. In this study, we
introduce DeepTaster, a novel DNN fingerprinting technique, to address
scenarios where a victim's data is unlawfully used to build a suspect model.
DeepTaster can effectively identify such DNN model theft attacks, even when the
suspect model's architecture deviates from the victim's. To accomplish this,
DeepTaster generates adversarial images with perturbations, transforms them
into the Fourier frequency domain, and uses these transformed images to
identify the dataset used in a suspect model. The underlying premise is that
adversarial images can capture the unique characteristics of DNNs built with a
specific dataset. To demonstrate the effectiveness of DeepTaster, we evaluated
the effectiveness of DeepTaster by assessing its detection accuracy on three
datasets (CIFAR10, MNIST, and Tiny-ImageNet) across three model architectures
(ResNet18, VGG16, and DenseNet161). We conducted experiments under various
attack scenarios, including transfer learning, pruning, fine-tuning, and data
augmentation. Specifically, in the Multi-Architecture Attack scenario,
DeepTaster was able to identify all the stolen cases across all datasets, while
DeepJudge failed to detect any of the cases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.11329">Anatomy-aware and acquisition-agnostic joint registration with SynthMorph. (arXiv:2301.11329v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Hoffmann_M/0/1/0/all/0/1">Malte Hoffmann</a>, <a href="http://arxiv.org/find/eess/1/au:+Hoopes_A/0/1/0/all/0/1">Andrew Hoopes</a>, <a href="http://arxiv.org/find/eess/1/au:+Greve_D/0/1/0/all/0/1">Douglas N. Greve</a>, <a href="http://arxiv.org/find/eess/1/au:+Fischl_B/0/1/0/all/0/1">Bruce Fischl</a>, <a href="http://arxiv.org/find/eess/1/au:+Dalca_A/0/1/0/all/0/1">Adrian V. Dalca</a></p>
<p>Affine image registration is a cornerstone of medical-image analysis. While
classical algorithms can achieve excellent accuracy, they solve a
time-consuming optimization for every image pair. Deep-learning (DL) methods
learn a function that maps an image pair to an output transform. Evaluating the
function is fast, but capturing large transforms can be challenging, and
networks tend to struggle if a test-image characteristic shifts from the
training domain, such as resolution. Most affine methods are agnostic to
anatomy, meaning the registration will be inaccurate if algorithms consider all
structures in the image.
</p>
<p>We address these shortcomings with SynthMorph, an easy-to-use DL tool for
joint affine-deformable registration of any brain image without preprocessing,
right off the MRI scanner. First, we leverage a strategy to train networks with
wildly varying images synthesized from label maps, yielding robust performance
across acquisition specifics unseen at training. Second, we optimize the
spatial overlap of select anatomical labels. This enables networks to
distinguish anatomy of interest from irrelevant structures, removing the need
for preprocessing that excludes content which would impinge on anatomy-specific
registration. Third, we combine the affine model with a deformable hypernetwork
that lets users choose the optimal deformation-field regularity for their
specific data, at registration time, in a fraction of the time required by
classical methods.
</p>
<p>We rigorously analyze how competing architectures learn affine transforms and
compare state-of-the-art registration tools across an extremely diverse set of
neuroimaging data, aiming to truly capture the behavior of methods in the real
world. SynthMorph demonstrates consistent and improved accuracy. It is
available at https://w3id.org/synthmorph, as a single complete end-to-end
solution for registration of brain MRI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.00736">Approximating the Shapley Value without Marginal Contributions. (arXiv:2302.00736v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kolpaczki_P/0/1/0/all/0/1">Patrick Kolpaczki</a>, <a href="http://arxiv.org/find/cs/1/au:+Bengs_V/0/1/0/all/0/1">Viktor Bengs</a>, <a href="http://arxiv.org/find/cs/1/au:+Muschalik_M/0/1/0/all/0/1">Maximilian Muschalik</a>, <a href="http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1">Eyke H&#xfc;llermeier</a></p>
<p>The Shapley value, which is arguably the most popular approach for assigning
a meaningful contribution value to players in a cooperative game, has recently
been used intensively in explainable artificial intelligence. Its
meaningfulness is due to axiomatic properties that only the Shapley value
satisfies, which, however, comes at the expense of an exact computation growing
exponentially with the number of agents. Accordingly, a number of works are
devoted to the efficient approximation of the Shapley value, most of them
revolve around the notion of an agent's marginal contribution. In this paper,
we propose with SVARM and Stratified SVARM two parameter-free and
domain-independent approximation algorithms based on a representation of the
Shapley value detached from the notion of marginal contribution. We prove
unmatched theoretical guarantees regarding their approximation quality and
provide empirical results including synthetic games as well as common
explainability use cases comparing ourselves with state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.01078">Computational Discovery of Microstructured Composites with Optimal Stiffness-Toughness Trade-Offs. (arXiv:2302.01078v2 [cond-mat.mtrl-sci] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Li_B/0/1/0/all/0/1">Beichen Li</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Deng_B/0/1/0/all/0/1">Bolei Deng</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Shou_W/0/1/0/all/0/1">Wan Shou</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Oh_T/0/1/0/all/0/1">Tae-Hyun Oh</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Hu_Y/0/1/0/all/0/1">Yuanming Hu</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Luo_Y/0/1/0/all/0/1">Yiyue Luo</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Shi_L/0/1/0/all/0/1">Liang Shi</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Matusik_W/0/1/0/all/0/1">Wojciech Matusik</a></p>
<p>The conflict between stiffness and toughness is a fundamental problem in
engineering materials design. However, the systematic discovery of
microstructured composites with optimal stiffness-toughness trade-offs has
never been demonstrated, hindered by the discrepancies between simulation and
reality and the lack of data-efficient exploration of the entire Pareto front.
We introduce a generalizable pipeline that integrates physical experiments,
numerical simulations, and artificial neural networks to address both
challenges. Without any prescribed expert knowledge of material design, our
approach implements a nested-loop proposal-validation workflow to bridge the
simulation-to-reality gap and discover microstructured composites that are
stiff and tough with high sample efficiency. Further analysis of Pareto-optimal
designs allows us to automatically identify existing toughness enhancement
mechanisms, which were previously discovered through trial-and-error or
biomimicry. On a broader scale, our method provides a blueprint for
computational design in various research areas beyond solid mechanics, such as
polymer chemistry, fluid dynamics, meteorology, and robotics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.03390">Learning Discretized Neural Networks under Ricci Flow. (arXiv:2302.03390v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hanwen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Mengmeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_G/0/1/0/all/0/1">Guang Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1">Ivor W. Tsang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yong Liu</a></p>
<p>In this paper, we study Discretized Neural Networks (DNNs) composed of
low-precision weights and activations, which suffer from either infinite or
zero gradients due to the non-differentiable discrete function during training.
Most training-based DNNs in such scenarios employ the standard Straight-Through
Estimator (STE) to approximate the gradient w.r.t. discrete values. However,
the use of STE introduces the problem of gradient mismatch, arising from
perturbations in the approximated gradient. To address this problem, this paper
reveals that this mismatch can be interpreted as a metric perturbation in a
Riemannian manifold, viewed through the lens of duality theory. Building on
information geometry, we construct the Linearly Nearly Euclidean (LNE) manifold
for DNNs, providing a background for addressing perturbations. By introducing a
partial differential equation on metrics, i.e., the Ricci flow, we establish
the dynamical stability and convergence of the LNE metric with the $L^2$-norm
perturbation. In contrast to previous perturbation theories with convergence
rates in fractional powers, the metric perturbation under the Ricci flow
exhibits exponential decay in the LNE manifold. Experimental results across
various datasets demonstrate that our method achieves superior and more stable
performance for DNNs compared to other representative training-based methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.06114">A Comprehensive Survey on Graph Summarization with Graph Neural Networks. (arXiv:2302.06114v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shabani_N/0/1/0/all/0/1">Nasrin Shabani</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jia Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Beheshti_A/0/1/0/all/0/1">Amin Beheshti</a>, <a href="http://arxiv.org/find/cs/1/au:+Sheng_Q/0/1/0/all/0/1">Quan Z. Sheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Foo_J/0/1/0/all/0/1">Jin Foo</a>, <a href="http://arxiv.org/find/cs/1/au:+Haghighi_V/0/1/0/all/0/1">Venus Haghighi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hanif_A/0/1/0/all/0/1">Ambreen Hanif</a>, <a href="http://arxiv.org/find/cs/1/au:+Shahabikargar_M/0/1/0/all/0/1">Maryam Shahabikargar</a></p>
<p>As large-scale graphs become more widespread, more and more computational
challenges with extracting, processing, and interpreting large graph data are
being exposed. It is therefore natural to search for ways to summarize these
expansive graphs while preserving their key characteristics. In the past, most
graph summarization techniques sought to capture the most important part of a
graph statistically. However, today, the high dimensionality and complexity of
modern graph data are making deep learning techniques more popular. Hence, this
paper presents a comprehensive survey of progress in deep learning
summarization techniques that rely on graph neural networks (GNNs). Our
investigation includes a review of the current state-of-the-art approaches,
including recurrent GNNs, convolutional GNNs, graph autoencoders, and graph
attention networks. A new burgeoning line of research is also discussed where
graph reinforcement learning is being used to evaluate and improve the quality
of graph summaries. Additionally, the survey provides details of benchmark
datasets, evaluation metrics, and open-source tools that are often employed in
experimentation settings, along with a detailed comparison, discussion, and
takeaways for the research community focused on graph summarization. Finally,
the survey concludes with a number of open research challenges to motivate
further study in this area.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.09267">Stochastic Approximation Approaches to Group Distributionally Robust Optimization. (arXiv:2302.09267v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lijun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1">Peng Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1">Zhen-Hua Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1">Tianbao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhi-Hua Zhou</a></p>
<p>This paper investigates group distributionally robust optimization (GDRO),
with the purpose to learn a model that performs well over $m$ different
distributions. First, we formulate GDRO as a stochastic convex-concave
saddle-point problem, and demonstrate that stochastic mirror descent (SMD),
using $m$ samples in each iteration, achieves an $O(m (\log m)/\epsilon^2)$
sample complexity for finding an $\epsilon$-optimal solution, which matches the
$\Omega(m/\epsilon^2)$ lower bound up to a logarithmic factor. Then, we make
use of techniques from online learning to reduce the number of samples required
in each round from $m$ to $1$, keeping the same sample complexity.
Specifically, we cast GDRO as a two-players game where one player simply
performs SMD and the other executes an online algorithm for non-oblivious
multi-armed bandits. Next, we consider a more practical scenario where the
number of samples that can be drawn from each distribution is different, and
propose a novel formulation of weighted GDRO, which allows us to derive
distribution-dependent convergence rates. Denote by $n_i$ the sample budget for
the $i$-th distribution, and assume $n_1 \geq n_2 \geq \cdots \geq n_m$. In the
first approach, we incorporate non-uniform sampling into SMD such that the
sample budget is satisfied in expectation, and prove that the excess risk of
the $i$-th distribution decreases at an $O(\sqrt{n_1 \log m}/n_i)$ rate. In the
second approach, we use mini-batches to meet the budget exactly and also reduce
the variance in stochastic gradients, and then leverage stochastic mirror-prox
algorithm, which can exploit small variances, to optimize a carefully designed
weighted GDRO problem. Under appropriate conditions, it attains an $O((\log
m)/\sqrt{n_i})$ convergence rate, which almost matches the optimal
$O(\sqrt{1/n_i})$ rate of only learning from the $i$-th distribution with $n_i$
samples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.09457">Attacks in Adversarial Machine Learning: A Systematic Survey from the Life-cycle Perspective. (arXiv:2302.09457v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1">Baoyuan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Zihao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Li Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qingshan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1">Zhaofeng He</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1">Siwei Lyu</a></p>
<p>Adversarial machine learning (AML) studies the adversarial phenomenon of
machine learning, which may make inconsistent or unexpected predictions with
humans. Some paradigms have been recently developed to explore this adversarial
phenomenon occurring at different stages of a machine learning system, such as
backdoor attack occurring at the pre-training, in-training and inference stage;
weight attack occurring at the post-training, deployment and inference stage;
adversarial attack occurring at the inference stage. However, although these
adversarial paradigms share a common goal, their developments are almost
independent, and there is still no big picture of AML. In this work, we aim to
provide a unified perspective to the AML community to systematically review the
overall progress of this field. We firstly provide a general definition about
AML, and then propose a unified mathematical framework to covering existing
attack paradigms. According to the proposed unified framework, we build a full
taxonomy to systematically categorize and review existing representative
methods for each paradigm. Besides, using this unified framework, it is easy to
figure out the connections and differences among different attack paradigms,
which may inspire future researchers to develop more advanced attack paradigms.
Finally, to facilitate the viewing of the built taxonomy and the related
literature in adversarial machine learning, we further provide a website, \ie,
\url{<a href="http://adversarial-ml.com">this http URL</a>}, where the taxonomies and literature will be
continuously updated.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.10975">Improved uncertainty quantification for neural networks with Bayesian last layer. (arXiv:2302.10975v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fiedler_F/0/1/0/all/0/1">Felix Fiedler</a>, <a href="http://arxiv.org/find/cs/1/au:+Lucia_S/0/1/0/all/0/1">Sergio Lucia</a></p>
<p>Uncertainty quantification is an important task in machine learning - a task
in which standardneural networks (NNs) have traditionally not excelled. This
can be a limitation for safety-critical applications, where uncertainty-aware
methods like Gaussian processes or Bayesian linear regression are often
preferred. Bayesian neural networks are an approach to address this limitation.
They assume probability distributions for all parameters and yield distributed
predictions. However, training and inference are typically intractable and
approximations must be employed. A promising approximation is NNs with Bayesian
last layer (BLL). They assume distributed weights only in the linear output
layer and yield a normally distributed prediction. To approximate the
intractable Bayesian neural network, point estimates of the distributed weights
in all but the last layer should be obtained by maximizing the marginal
likelihood. This has previously been challenging, as the marginal likelihood is
expensive to evaluate in this setting. We present a reformulation of the
log-marginal likelihood of a NN with BLL which allows for efficient training
using backpropagation. Furthermore, we address the challenge of uncertainty
quantification for extrapolation points. We provide a metric to quantify the
degree of extrapolation and derive a method to improve the uncertainty
quantification for these points. Our methods are derived for the multivariate
case and demonstrated in a simulation study. In comparison to Bayesian linear
regression with fixed features, and a Bayesian neural network trained with
variational inference, our proposed method achieves the highest log-predictive
density on test data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.13991">Learning to Generalize towards Unseen Domains via a Content-Aware Style Invariant Model for Disease Detection from Chest X-rays. (arXiv:2302.13991v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zunaed_M/0/1/0/all/0/1">Mohammad Zunaed</a>, <a href="http://arxiv.org/find/cs/1/au:+Haque_M/0/1/0/all/0/1">Md. Aynal Haque</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasan_T/0/1/0/all/0/1">Taufiq Hasan</a></p>
<p>Performance degradation due to distribution discrepancy is a longstanding
challenge in intelligent imaging, particularly for chest X-rays (CXRs). Recent
studies have demonstrated that CNNs are biased toward styles (e.g.,
uninformative textures) rather than content (e.g., shape), in stark contrast to
the human vision system. Radiologists tend to learn visual cues from CXRs and
thus perform well across multiple domains. Motivated by this, we employ the
novel on-the-fly style randomization modules at both image (SRM-IL) and feature
(SRM-FL) levels to create rich style perturbed features while keeping the
content intact for robust cross-domain performance. Previous methods simulate
unseen domains by constructing new styles via interpolation or swapping styles
from existing data, limiting them to available source domains during training.
However, SRM-IL samples the style statistics from the possible value range of a
CXR image instead of the training data to achieve more diversified
augmentations. Moreover, we utilize pixel-wise learnable parameters in the
SRM-FL compared to pre-defined channel-wise mean and standard deviations as
style embeddings for capturing more representative style features.
Additionally, we leverage consistency regularizations on global semantic
features and predictive distributions from with and without style-perturbed
versions of the same CXR to tweak the model's sensitivity toward content
markers for accurate predictions. Our proposed method, trained on CheXpert and
MIMIC-CXR datasets, achieves 77.32$\pm$0.35, 88.38$\pm$0.19, 82.63$\pm$0.13
AUCs(%) on the unseen domain test datasets, i.e., BRAX, VinDr-CXR, and NIH
chest X-ray14, respectively, compared to 75.56$\pm$0.80, 87.57$\pm$0.46,
82.07$\pm$0.19 from state-of-the-art models on five-fold cross-validation with
statistically significant results in thoracic disease classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.02468">Lon-ea at SemEval-2023 Task 11: A Comparison of Activation Functions for Soft and Hard Label Prediction. (arXiv:2303.02468v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hosseini_P/0/1/0/all/0/1">Peyman Hosseini</a>, <a href="http://arxiv.org/find/cs/1/au:+Hosseini_M/0/1/0/all/0/1">Mehran Hosseini</a>, <a href="http://arxiv.org/find/cs/1/au:+Al_Azzawi_S/0/1/0/all/0/1">Sana Sabah Al-Azzawi</a>, <a href="http://arxiv.org/find/cs/1/au:+Liwicki_M/0/1/0/all/0/1">Marcus Liwicki</a>, <a href="http://arxiv.org/find/cs/1/au:+Castro_I/0/1/0/all/0/1">Ignacio Castro</a>, <a href="http://arxiv.org/find/cs/1/au:+Purver_M/0/1/0/all/0/1">Matthew Purver</a></p>
<p>We study the influence of different activation functions in the output layer
of deep neural network models for soft and hard label prediction in the
learning with disagreement task. In this task, the goal is to quantify the
amount of disagreement via predicting soft labels. To predict the soft labels,
we use BERT-based preprocessors and encoders and vary the activation function
used in the output layer, while keeping other parameters constant. The soft
labels are then used for the hard label prediction. The activation functions
considered are sigmoid as well as a step-function that is added to the model
post-training and a sinusoidal activation function, which is introduced for the
first time in this paper.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.06815">On Model Compression for Neural Networks: Framework, Algorithm, and Convergence Guarantee. (arXiv:2303.06815v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chenyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1">Jihoon Chung</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_B/0/1/0/all/0/1">Biao Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haimin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xianlian Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1">Bo Shen</a></p>
<p>Model compression is a crucial part of deploying neural networks (NNs),
especially when the memory and storage of computing devices are limited in many
applications. This paper focuses on two model compression techniques: low-rank
approximation and weight pruning in neural networks, which are very popular
nowadays. However, training NN with low-rank approximation and weight pruning
always suffers significant accuracy loss and convergence issues. In this paper,
a holistic framework is proposed for model compression from a novel perspective
of nonconvex optimization by designing an appropriate objective function. Then,
we introduce NN-BCD, a block coordinate descent (BCD) algorithm to solve the
nonconvex optimization. One advantage of our algorithm is that an efficient
iteration scheme can be derived with closed-form, which is gradient-free.
Therefore, our algorithm will not suffer from vanishing/exploding gradient
problems. Furthermore, with the Kurdyka-{\L}ojasiewicz (K{\L}) property of our
objective function, we show that our algorithm globally converges to a critical
point at the rate of O(1/k), where k denotes the number of iterations. Lastly,
extensive experiments with tensor train decomposition and weight pruning
demonstrate the efficiency and superior performance of the proposed framework.
Our code implementation is available at https://github.com/ChenyangLi-97/NN-BCD
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.04934">Model Sparsity Can Simplify Machine Unlearning. (arXiv:2304.04934v12 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1">Jinghan Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiancheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ram_P/0/1/0/all/0/1">Parikshit Ram</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1">Yuguang Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1">Gaowen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1">Pranay Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Sijia Liu</a></p>
<p>In response to recent data regulation requirements, machine unlearning (MU)
has emerged as a critical process to remove the influence of specific examples
from a given model. Although exact unlearning can be achieved through complete
model retraining using the remaining dataset, the associated computational
costs have driven the development of efficient, approximate unlearning
techniques. Moving beyond data-centric MU approaches, our study introduces a
novel model-based perspective: model sparsification via weight pruning, which
is capable of reducing the gap between exact unlearning and approximate
unlearning. We show in both theory and practice that model sparsity can boost
the multi-criteria unlearning performance of an approximate unlearner, closing
the approximation gap, while continuing to be efficient. This leads to a new MU
paradigm, termed prune first, then unlearn, which infuses a sparse model prior
into the unlearning process. Building on this insight, we also develop a
sparsity-aware unlearning method that utilizes sparsity regularization to
enhance the training process of approximate unlearning. Extensive experiments
show that our proposals consistently benefit MU in various unlearning
scenarios. A notable highlight is the 77% unlearning efficacy gain of
fine-tuning (one of the simplest unlearning methods) when using sparsity-aware
unlearning. Furthermore, we demonstrate the practical impact of our proposed MU
methods in addressing other machine learning challenges, such as defending
against backdoor attacks and enhancing transfer learning. Codes are available
at https://github.com/OPTML-Group/Unlearn-Sparse.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.07520">STAS: Spatial-Temporal Return Decomposition for Multi-agent Reinforcement Learning. (arXiv:2304.07520v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Sirui Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhaowei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yaodong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1">Yali Du</a></p>
<p>Centralized Training with Decentralized Execution (CTDE) has been proven to
be an effective paradigm in cooperative multi-agent reinforcement learning
(MARL). One of the major challenges is credit assignment, which aims to credit
agents by their contributions. While prior studies have shown great success,
their methods typically fail to work in episodic reinforcement learning
scenarios where global rewards are revealed only at the end of the episode.
They lack the functionality to model complicated relations of the delayed
global reward in the temporal dimension and suffer from inefficiencies. To
tackle this, we introduce Spatial-Temporal Attention with Shapley (STAS), a
novel method that learns credit assignment in both temporal and spatial
dimensions. It first decomposes the global return back to each time step, then
utilizes the Shapley Value to redistribute the individual payoff from the
decomposed global reward. To mitigate the computational complexity of the
Shapley Value, we introduce an approximation of marginal contribution and
utilize Monte Carlo sampling to estimate it. We evaluate our method on an Alice
&amp; Bob example and MPE environments across different scenarios. Our results
demonstrate that our method effectively assigns spatial-temporal credit,
outperforming all state-of-the-art baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.06432">A Generalizable Physics-informed Learning Framework for Risk Probability Estimation. (arXiv:2305.06432v2 [eess.SY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1">Zhuoyuan Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Nakahira_Y/0/1/0/all/0/1">Yorie Nakahira</a></p>
<p>Accurate estimates of long-term risk probabilities and their gradients are
critical for many stochastic safe control methods. However, computing such risk
probabilities in real-time and in unseen or changing environments is
challenging. Monte Carlo (MC) methods cannot accurately evaluate the
probabilities and their gradients as an infinitesimal devisor can amplify the
sampling noise. In this paper, we develop an efficient method to evaluate the
probabilities of long-term risk and their gradients. The proposed method
exploits the fact that long-term risk probability satisfies certain partial
differential equations (PDEs), which characterize the neighboring relations
between the probabilities, to integrate MC methods and physics-informed neural
networks. We provide theoretical guarantees of the estimation error given
certain choices of training configurations. Numerical results show the proposed
method has better sample efficiency, generalizes well to unseen regions, and
can adapt to systems with changing parameters. The proposed method can also
accurately estimate the gradients of risk probabilities, which enables first-
and second-order techniques on risk probabilities to be used for learning and
control.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.17033">The Brain Tumor Segmentation (BraTS) Challenge 2023: Focus on Pediatrics (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs). (arXiv:2305.17033v4 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Kazerooni_A/0/1/0/all/0/1">Anahita Fathi Kazerooni</a>, <a href="http://arxiv.org/find/eess/1/au:+Khalili_N/0/1/0/all/0/1">Nastaran Khalili</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1">Xinyang Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Haldar_D/0/1/0/all/0/1">Debanjan Haldar</a>, <a href="http://arxiv.org/find/eess/1/au:+Jiang_Z/0/1/0/all/0/1">Zhifan Jiang</a>, <a href="http://arxiv.org/find/eess/1/au:+Anwar_S/0/1/0/all/0/1">Syed Muhammed Anwar</a>, <a href="http://arxiv.org/find/eess/1/au:+Albrecht_J/0/1/0/all/0/1">Jake Albrecht</a>, <a href="http://arxiv.org/find/eess/1/au:+Adewole_M/0/1/0/all/0/1">Maruf Adewole</a>, <a href="http://arxiv.org/find/eess/1/au:+Anazodo_U/0/1/0/all/0/1">Udunna Anazodo</a>, <a href="http://arxiv.org/find/eess/1/au:+Anderson_H/0/1/0/all/0/1">Hannah Anderson</a>, <a href="http://arxiv.org/find/eess/1/au:+Bagheri_S/0/1/0/all/0/1">Sina Bagheri</a>, <a href="http://arxiv.org/find/eess/1/au:+Baid_U/0/1/0/all/0/1">Ujjwal Baid</a>, <a href="http://arxiv.org/find/eess/1/au:+Bergquist_T/0/1/0/all/0/1">Timothy Bergquist</a>, <a href="http://arxiv.org/find/eess/1/au:+Borja_A/0/1/0/all/0/1">Austin J. Borja</a>, <a href="http://arxiv.org/find/eess/1/au:+Calabrese_E/0/1/0/all/0/1">Evan Calabrese</a>, <a href="http://arxiv.org/find/eess/1/au:+Chung_V/0/1/0/all/0/1">Verena Chung</a>, <a href="http://arxiv.org/find/eess/1/au:+Conte_G/0/1/0/all/0/1">Gian-Marco Conte</a>, <a href="http://arxiv.org/find/eess/1/au:+Dako_F/0/1/0/all/0/1">Farouk Dako</a>, <a href="http://arxiv.org/find/eess/1/au:+Eddy_J/0/1/0/all/0/1">James Eddy</a>, <a href="http://arxiv.org/find/eess/1/au:+Ezhov_I/0/1/0/all/0/1">Ivan Ezhov</a>, <a href="http://arxiv.org/find/eess/1/au:+Familiar_A/0/1/0/all/0/1">Ariana Familiar</a>, <a href="http://arxiv.org/find/eess/1/au:+Farahani_K/0/1/0/all/0/1">Keyvan Farahani</a>, <a href="http://arxiv.org/find/eess/1/au:+Haldar_S/0/1/0/all/0/1">Shuvanjan Haldar</a>, <a href="http://arxiv.org/find/eess/1/au:+Iglesias_J/0/1/0/all/0/1">Juan Eugenio Iglesias</a>, <a href="http://arxiv.org/find/eess/1/au:+Janas_A/0/1/0/all/0/1">Anastasia Janas</a>, <a href="http://arxiv.org/find/eess/1/au:+Johansen_E/0/1/0/all/0/1">Elaine Johansen</a>, <a href="http://arxiv.org/find/eess/1/au:+Jones_B/0/1/0/all/0/1">Blaise V Jones</a>, <a href="http://arxiv.org/find/eess/1/au:+Kofler_F/0/1/0/all/0/1">Florian Kofler</a>, <a href="http://arxiv.org/find/eess/1/au:+LaBella_D/0/1/0/all/0/1">Dominic LaBella</a>, <a href="http://arxiv.org/find/eess/1/au:+Lai_H/0/1/0/all/0/1">Hollie Anne Lai</a>, <a href="http://arxiv.org/find/eess/1/au:+Leemput_K/0/1/0/all/0/1">Koen Van Leemput</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1">Hongwei Bran Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Maleki_N/0/1/0/all/0/1">Nazanin Maleki</a>, <a href="http://arxiv.org/find/eess/1/au:+McAllister_A/0/1/0/all/0/1">Aaron S McAllister</a>, <a href="http://arxiv.org/find/eess/1/au:+Meier_Z/0/1/0/all/0/1">Zeke Meier</a>, <a href="http://arxiv.org/find/eess/1/au:+Menze_B/0/1/0/all/0/1">Bjoern Menze</a>, <a href="http://arxiv.org/find/eess/1/au:+Moawad_A/0/1/0/all/0/1">Ahmed W Moawad</a>, <a href="http://arxiv.org/find/eess/1/au:+Nandolia_K/0/1/0/all/0/1">Khanak K Nandolia</a>, <a href="http://arxiv.org/find/eess/1/au:+Pavaine_J/0/1/0/all/0/1">Julija Pavaine</a>, <a href="http://arxiv.org/find/eess/1/au:+Piraud_M/0/1/0/all/0/1">Marie Piraud</a>, <a href="http://arxiv.org/find/eess/1/au:+Poussaint_T/0/1/0/all/0/1">Tina Poussaint</a>, <a href="http://arxiv.org/find/eess/1/au:+Prabhu_S/0/1/0/all/0/1">Sanjay P Prabhu</a>, <a href="http://arxiv.org/find/eess/1/au:+Reitman_Z/0/1/0/all/0/1">Zachary Reitman</a>, <a href="http://arxiv.org/find/eess/1/au:+Rodriguez_A/0/1/0/all/0/1">Andres Rodriguez</a>, <a href="http://arxiv.org/find/eess/1/au:+Rudie_J/0/1/0/all/0/1">Jeffrey D Rudie</a>, <a href="http://arxiv.org/find/eess/1/au:+Shaikh_I/0/1/0/all/0/1">Ibraheem Salman Shaikh</a>, <a href="http://arxiv.org/find/eess/1/au:+Shah_L/0/1/0/all/0/1">Lubdha M. Shah</a>, <a href="http://arxiv.org/find/eess/1/au:+Sheth_N/0/1/0/all/0/1">Nakul Sheth</a>, <a href="http://arxiv.org/find/eess/1/au:+Shinohara_R/0/1/0/all/0/1">Russel Taki Shinohara</a>, et al. (23 additional authors not shown)</p>
<p>Pediatric tumors of the central nervous system are the most common cause of
cancer-related death in children. The five-year survival rate for high-grade
gliomas in children is less than 20\%. Due to their rarity, the diagnosis of
these entities is often delayed, their treatment is mainly based on historic
treatment concepts, and clinical trials require multi-institutional
collaborations. The MICCAI Brain Tumor Segmentation (BraTS) Challenge is a
landmark community benchmark event with a successful history of 12 years of
resource creation for the segmentation and analysis of adult glioma. Here we
present the CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023 challenge, which
represents the first BraTS challenge focused on pediatric brain tumors with
data acquired across multiple international consortia dedicated to pediatric
neuro-oncology and clinical trials. The BraTS-PEDs 2023 challenge focuses on
benchmarking the development of volumentric segmentation algorithms for
pediatric brain glioma through standardized quantitative performance evaluation
metrics utilized across the BraTS 2023 cluster of challenges. Models gaining
knowledge from the BraTS-PEDs multi-parametric structural MRI (mpMRI) training
data will be evaluated on separate validation and unseen test mpMRI dataof
high-grade pediatric glioma. The CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023
challenge brings together clinicians and AI/imaging scientists to lead to
faster development of automated segmentation techniques that could benefit
clinical trials, and ultimately the care of children with brain tumors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.00876">Quantifying Deep Learning Model Uncertainty in Conformal Prediction. (arXiv:2306.00876v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Karimi_H/0/1/0/all/0/1">Hamed Karimi</a>, <a href="http://arxiv.org/find/cs/1/au:+Samavi_R/0/1/0/all/0/1">Reza Samavi</a></p>
<p>Precise estimation of predictive uncertainty in deep neural networks is a
critical requirement for reliable decision-making in machine learning and
statistical modeling, particularly in the context of medical AI. Conformal
Prediction (CP) has emerged as a promising framework for representing the model
uncertainty by providing well-calibrated confidence levels for individual
predictions. However, the quantification of model uncertainty in conformal
prediction remains an active research area, yet to be fully addressed. In this
paper, we explore state-of-the-art CP methodologies and their theoretical
foundations. We propose a probabilistic approach in quantifying the model
uncertainty derived from the produced prediction sets in conformal prediction
and provide certified boundaries for the computed uncertainty. By doing so, we
allow model uncertainty measured by CP to be compared by other uncertainty
quantification methods such as Bayesian (e.g., MC-Dropout and DeepEnsemble) and
Evidential approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.04502">Learning with Noisy Labels by Adaptive Gradient-Based Outlier Removal. (arXiv:2306.04502v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sedova_A/0/1/0/all/0/1">Anastasiia Sedova</a>, <a href="http://arxiv.org/find/cs/1/au:+Zellinger_L/0/1/0/all/0/1">Lena Zellinger</a>, <a href="http://arxiv.org/find/cs/1/au:+Roth_B/0/1/0/all/0/1">Benjamin Roth</a></p>
<p>An accurate and substantial dataset is essential for training a reliable and
well-performing model. However, even manually annotated datasets contain label
errors, not to mention automatically labeled ones. Previous methods for label
denoising have primarily focused on detecting outliers and their permanent
removal - a process that is likely to over- or underfilter the dataset. In this
work, we propose AGRA: a new method for learning with noisy labels by using
Adaptive GRAdient-based outlier removal. Instead of cleaning the dataset prior
to model training, the dataset is dynamically adjusted during the training
process. By comparing the aggregated gradient of a batch of samples and an
individual example gradient, our method dynamically decides whether a
corresponding example is helpful for the model at this point or is
counter-productive and should be left out for the current update. Extensive
evaluation on several datasets demonstrates AGRA's effectiveness, while a
comprehensive results analysis supports our initial hypothesis: permanent hard
outlier removal is not always what model benefits the most from.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.10759">SGFormer: Simplifying and Empowering Transformers for Large-Graph Representations. (arXiv:2306.10759v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1">Qitian Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Wentao Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Chenxiao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hengrui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_F/0/1/0/all/0/1">Fan Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1">Haitian Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bian_Y/0/1/0/all/0/1">Yatao Bian</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1">Junchi Yan</a></p>
<p>Learning representations on large-sized graphs is a long-standing challenge
due to the inter-dependence nature involved in massive data points.
Transformers, as an emerging class of foundation encoders for graph-structured
data, have shown promising performance on small graphs due to its global
attention capable of capturing all-pair influence beyond neighboring nodes.
Even so, existing approaches tend to inherit the spirit of Transformers in
language and vision tasks, and embrace complicated models by stacking deep
multi-head attentions. In this paper, we critically demonstrate that even using
a one-layer attention can bring up surprisingly competitive performance across
node property prediction benchmarks where node numbers range from
thousand-level to billion-level. This encourages us to rethink the design
philosophy for Transformers on large graphs, where the global attention is a
computation overhead hindering the scalability. We frame the proposed scheme as
Simplified Graph Transformers (SGFormer), which is empowered by a simple
attention model that can efficiently propagate information among arbitrary
nodes in one layer. SGFormer requires none of positional encodings,
feature/graph pre-processing or augmented loss. Empirically, SGFormer
successfully scales to the web-scale graph ogbn-papers100M and yields up to
141x inference acceleration over SOTA Transformers on medium-sized graphs.
Beyond current results, we believe the proposed methodology alone enlightens a
new technical path of independent interest for building Transformers on large
graphs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.11586">Provably Powerful Graph Neural Networks for Directed Multigraphs. (arXiv:2306.11586v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Egressy_B/0/1/0/all/0/1">B&#xe9;ni Egressy</a>, <a href="http://arxiv.org/find/cs/1/au:+Niederhausern_L/0/1/0/all/0/1">Luc von Niederh&#xe4;usern</a>, <a href="http://arxiv.org/find/cs/1/au:+Blanusa_J/0/1/0/all/0/1">Jovan Blanusa</a>, <a href="http://arxiv.org/find/cs/1/au:+Altman_E/0/1/0/all/0/1">Erik Altman</a>, <a href="http://arxiv.org/find/cs/1/au:+Wattenhofer_R/0/1/0/all/0/1">Roger Wattenhofer</a>, <a href="http://arxiv.org/find/cs/1/au:+Atasu_K/0/1/0/all/0/1">Kubilay Atasu</a></p>
<p>This paper analyses a set of simple adaptations that transform standard
message-passing Graph Neural Networks (GNN) into provably powerful directed
multigraph neural networks. The adaptations include multigraph port numbering,
ego IDs, and reverse message passing. We prove that the combination of these
theoretically enables the detection of any directed subgraph pattern. To
validate the effectiveness of our proposed adaptations in practice, we conduct
experiments on synthetic subgraph detection tasks, which demonstrate
outstanding performance with almost perfect results. Moreover, we apply our
proposed adaptations to two financial crime analysis tasks. We observe dramatic
improvements in detecting money laundering transactions, improving the
minority-class F1 score of a standard message-passing GNN by up to 30%, and
closely matching or outperforming tree-based and GNN baselines. Similarly
impressive results are observed on a real-world phishing detection dataset,
boosting three standard GNNs' F1 scores by around 15% and outperforming all
baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.02535">Learning to Generate Training Datasets for Robust Semantic Segmentation. (arXiv:2308.02535v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hariat_M/0/1/0/all/0/1">Marwane Hariat</a>, <a href="http://arxiv.org/find/cs/1/au:+Laurent_O/0/1/0/all/0/1">Olivier Laurent</a>, <a href="http://arxiv.org/find/cs/1/au:+Kazmierczak_R/0/1/0/all/0/1">R&#xe9;mi Kazmierczak</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shihao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bursuc_A/0/1/0/all/0/1">Andrei Bursuc</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1">Angela Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Franchi_G/0/1/0/all/0/1">Gianni Franchi</a></p>
<p>Semantic segmentation methods have advanced significantly. Still, their
robustness to real-world perturbations and object types not seen during
training remains a challenge, particularly in safety-critical applications. We
propose a novel approach to improve the robustness of semantic segmentation
techniques by leveraging the synergy between label-to-image generators and
image-to-label segmentation models. Specifically, we design Robusta, a novel
robust conditional generative adversarial network to generate realistic and
plausible perturbed images that can be used to train reliable segmentation
models. We conduct in-depth studies of the proposed generative model, assess
the performance and robustness of the downstream segmentation network, and
demonstrate that our approach can significantly enhance the robustness in the
face of real-world perturbations, distribution shifts, and out-of-distribution
samples. Our results suggest that this approach could be valuable in
safety-critical applications, where the reliability of perception modules such
as semantic segmentation is of utmost importance and comes with a limited
computational budget in inference. We release our code at
https://github.com/ENSTA-U2IS/robusta.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04669">A General Implicit Framework for Fast NeRF Composition and Rendering. (arXiv:2308.04669v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xinyu Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Ziyi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yunlu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yuxiang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1">Xiaogang Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_C/0/1/0/all/0/1">Changqing Zou</a></p>
<p>A variety of Neural Radiance Fields (NeRF) methods have recently achieved
remarkable success in high render speed. However, current accelerating methods
are specialized and incompatible with various implicit methods, preventing
real-time composition over various types of NeRF works. Because NeRF relies on
sampling along rays, it is possible to provide general guidance for
acceleration. To that end, we propose a general implicit pipeline for composing
NeRF objects quickly. Our method enables the casting of dynamic shadows within
or between objects using analytical light sources while allowing multiple NeRF
objects to be seamlessly placed and rendered together with any arbitrary rigid
transformations. Mainly, our work introduces a new surface representation known
as Neural Depth Fields (NeDF) that quickly determines the spatial relationship
between objects by allowing direct intersection computation between rays and
implicit surfaces. It leverages an intersection neural network to query NeRF
for acceleration instead of depending on an explicit spatial structure.Our
proposed method is the first to enable both the progressive and interactive
composition of NeRF objects. Additionally, it also serves as a previewing
plugin for a range of existing NeRF works.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.06911">GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text. (arXiv:2308.06911v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1">Pengfei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1">Yiming Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1">Zhixiang Ren</a></p>
<p>Large language models have made significant strides in natural language
processing, enabling innovative applications in molecular science by processing
textual representations of molecules. However, most existing language models
cannot capture the rich information with complex molecular structures or
images. In this paper, we introduce GIT-Mol, a multi-modal large language model
that integrates the Graph, Image, and Text information. To facilitate the
integration of multi-modal molecular data, we propose GIT-Former, a novel
architecture that is capable of aligning all modalities into a unified latent
space. We achieve a 5%-10% accuracy increase in properties prediction and a
20.2% boost in molecule generation validity compared to the baselines. With the
any-to-language molecular translation strategy, our model has the potential to
perform more downstream tasks, such as compound name recognition and chemical
reaction prediction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12517">Not Only Rewards But Also Constraints: Applications on Legged Robot Locomotion. (arXiv:2308.12517v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Yunho Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_H/0/1/0/all/0/1">Hyunsik Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jeonghyun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1">Jinhyeok Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1">Gwanghyeon Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_M/0/1/0/all/0/1">Moonkyu Jung</a>, <a href="http://arxiv.org/find/cs/1/au:+Youm_D/0/1/0/all/0/1">Donghoon Youm</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwangbo_J/0/1/0/all/0/1">Jemin Hwangbo</a></p>
<p>Several earlier studies have shown impressive control performance in complex
robotic systems by designing the controller using a neural network and training
it with model-free reinforcement learning. However, these outstanding
controllers with natural motion style and high task performance are developed
through extensive reward engineering, which is a highly laborious and
time-consuming process of designing numerous reward terms and determining
suitable reward coefficients. In this work, we propose a novel reinforcement
learning framework for training neural network controllers for complex robotic
systems consisting of both rewards and constraints. To let the engineers
appropriately reflect their intent to constraints and handle them with minimal
computation overhead, two constraint types and an efficient policy optimization
algorithm are suggested. The learning framework is applied to train locomotion
controllers for several legged robots with different morphology and physical
attributes to traverse challenging terrains. Extensive simulation and
real-world experiments demonstrate that performant controllers can be trained
with significantly less reward engineering, by tuning only a single reward
coefficient. Furthermore, a more straightforward and intuitive engineering
process can be utilized, thanks to the interpretability and generalizability of
constraints. The summary video is available at https://youtu.be/KAlm3yskhvM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.15256">Let There Be Sound: Reconstructing High Quality Speech from Silent Videos. (arXiv:2308.15256v2 [eess.AS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1">Ji-Hoon Kim</a>, <a href="http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1">Jaehun Kim</a>, <a href="http://arxiv.org/find/eess/1/au:+Chung_J/0/1/0/all/0/1">Joon Son Chung</a></p>
<p>The goal of this work is to reconstruct high quality speech from lip motions
alone, a task also known as lip-to-speech. A key challenge of lip-to-speech
systems is the one-to-many mapping caused by (1) the existence of homophenes
and (2) multiple speech variations, resulting in a mispronounced and
over-smoothed speech. In this paper, we propose a novel lip-to-speech system
that significantly improves the generation quality by alleviating the
one-to-many mapping problem from multiple perspectives. Specifically, we
incorporate (1) self-supervised speech representations to disambiguate
homophenes, and (2) acoustic variance information to model diverse speech
styles. Additionally, to better solve the aforementioned problem, we employ a
flow based post-net which captures and refines the details of the generated
speech. We perform extensive experiments on two datasets, and demonstrate that
our method achieves the generation quality close to that of real human
utterance, outperforming existing methods in terms of speech naturalness and
intelligibility by a large margin. Synthesised samples are available at our
demo page: https://mm.kaist.ac.kr/projects/LTBS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11319">WFTNet: Exploiting Global and Local Periodicity in Long-term Time Series Forecasting. (arXiv:2309.11319v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1">Peiyuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1">Beiliang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1">Naiqi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_T/0/1/0/all/0/1">Tao Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_F/0/1/0/all/0/1">Fengmao Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1">Jigang Bao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yong Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1">Shu-Tao Xia</a></p>
<p>Recent CNN and Transformer-based models tried to utilize frequency and
periodicity information for long-term time series forecasting. However, most
existing work is based on Fourier transform, which cannot capture fine-grained
and local frequency structure. In this paper, we propose a Wavelet-Fourier
Transform Network (WFTNet) for long-term time series forecasting. WFTNet
utilizes both Fourier and wavelet transforms to extract comprehensive
temporal-frequency information from the signal, where Fourier transform
captures the global periodic patterns and wavelet transform captures the local
ones. Furthermore, we introduce a Periodicity-Weighted Coefficient (PWC) to
adaptively balance the importance of global and local frequency patterns.
Extensive experiments on various time series datasets show that WFTNet
consistently outperforms other state-of-the-art baseline. Code is available at
https://github.com/Hank0626/WFTNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03585">Smoothing Methods for Automatic Differentiation Across Conditional Branches. (arXiv:2310.03585v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kreikemeyer_J/0/1/0/all/0/1">Justin N. Kreikemeyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Andelfinger_P/0/1/0/all/0/1">Philipp Andelfinger</a></p>
<p>Programs involving discontinuities introduced by control flow constructs such
as conditional branches pose challenges to mathematical optimization methods
that assume a degree of smoothness in the objective function's response
surface. Smooth interpretation (SI) is a form of abstract interpretation that
approximates the convolution of a program's output with a Gaussian kernel, thus
smoothing its output in a principled manner. Here, we combine SI with automatic
differentiation (AD) to efficiently compute gradients of smoothed programs. In
contrast to AD across a regular program execution, these gradients also capture
the effects of alternative control flow paths. The combination of SI with AD
enables the direct gradient-based parameter synthesis for branching programs,
allowing for instance the calibration of simulation models or their combination
with neural network models in machine learning pipelines. We detail the effects
of the approximations made for tractability in SI and propose a novel Monte
Carlo estimator that avoids the underlying assumptions by estimating the
smoothed programs' gradients through a combination of AD and sampling. Using
DiscoGrad, our tool for automatically translating simple C++ programs to a
smooth differentiable form, we perform an extensive evaluation. We compare the
combination of SI with AD and our Monte Carlo estimator to existing
gradient-free and stochastic methods on four non-trivial and originally
discontinuous problems ranging from classical simulation-based optimization to
neural network-driven control. While the optimization progress with the
SI-based estimator depends on the complexity of the program's control flow, our
Monte Carlo estimator is competitive in all problems, exhibiting the fastest
convergence by a substantial margin in our highest-dimensional problem.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.06289">Better and Simpler Lower Bounds for Differentially Private Statistical Estimation. (arXiv:2310.06289v2 [math.ST] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Narayanan_S/0/1/0/all/0/1">Shyam Narayanan</a></p>
<p>We provide optimal lower bounds for two well-known parameter estimation (also
known as statistical estimation) tasks in high dimensions with approximate
differential privacy. First, we prove that for any $\alpha \le O(1)$,
estimating the covariance of a Gaussian up to spectral error $\alpha$ requires
$\tilde{\Omega}\left(\frac{d^{3/2}}{\alpha \varepsilon} +
\frac{d}{\alpha^2}\right)$ samples, which is tight up to logarithmic factors.
This result improves over previous work which established this for $\alpha \le
O\left(\frac{1}{\sqrt{d}}\right)$, and is also simpler than previous work.
Next, we prove that estimating the mean of a heavy-tailed distribution with
bounded $k$th moments requires $\tilde{\Omega}\left(\frac{d}{\alpha^{k/(k-1)}
\varepsilon} + \frac{d}{\alpha^2}\right)$ samples. Previous work for this
problem was only able to establish this lower bound against pure differential
privacy, or in the special case of $k = 2$.
</p>
<p>Our techniques follow the method of fingerprinting and are generally quite
simple. Our lower bound for heavy-tailed estimation is based on a black-box
reduction from privately estimating identity-covariance Gaussians. Our lower
bound for covariance estimation utilizes a Bayesian approach to show that,
under an Inverse Wishart prior distribution for the covariance matrix, no
private estimator can be accurate even in expectation, without sufficiently
many samples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17498">CBD: A Certified Backdoor Detector Based on Local Dominant Probability. (arXiv:2310.17498v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiang_Z/0/1/0/all/0/1">Zhen Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1">Zidi Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bo Li</a></p>
<p>Backdoor attack is a common threat to deep neural networks. During testing,
samples embedded with a backdoor trigger will be misclassified as an
adversarial target by a backdoored model, while samples without the backdoor
trigger will be correctly classified. In this paper, we present the first
certified backdoor detector (CBD), which is based on a novel, adjustable
conformal prediction scheme based on our proposed statistic local dominant
probability. For any classifier under inspection, CBD provides 1) a detection
inference, 2) the condition under which the attacks are guaranteed to be
detectable for the same classification domain, and 3) a probabilistic upper
bound for the false positive rate. Our theoretical results show that attacks
with triggers that are more resilient to test-time noise and have smaller
perturbation magnitudes are more likely to be detected with guarantees.
Moreover, we conduct extensive experiments on four benchmark datasets
considering various backdoor types, such as BadNet, CB, and Blend. CBD achieves
comparable or even higher detection accuracy than state-of-the-art detectors,
and it in addition provides detection certification. Notably, for backdoor
attacks with random perturbation triggers bounded by $\ell_2\leq0.75$ which
achieves more than 90\% attack success rate, CBD achieves 100\% (98\%), 100\%
(84\%), 98\% (98\%), and 72\% (40\%) empirical (certified) detection true
positive rates on the four benchmark datasets GTSRB, SVHN, CIFAR-10, and
TinyImageNet, respectively, with low false positive rates.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.20065">LinFlo-Net: A two-stage deep learning method to generate simulation ready meshes of the heart. (arXiv:2310.20065v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Narayanan_A/0/1/0/all/0/1">Arjun Narayanan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kong_F/0/1/0/all/0/1">Fanwei Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Shadden_S/0/1/0/all/0/1">Shawn Shadden</a></p>
<p>We present a deep learning model to automatically generate computer models of
the human heart from patient imaging data with an emphasis on its capability to
generate thin-walled cardiac structures. Our method works by deforming a
template mesh to fit the cardiac structures to the given image. Compared with
prior deep learning methods that adopted this approach, our framework is
designed to minimize mesh self-penetration, which typically arises when
deforming surface meshes separated by small distances. We achieve this by using
a two-stage diffeomorphic deformation process along with a novel loss function
derived from the kinematics of motion that penalizes surface contact and
interpenetration. Our model demonstrates comparable accuracy with
state-of-the-art methods while additionally producing meshes free of
self-intersections. The resultant meshes are readily usable in physics based
simulation, minimizing the need for post-processing and cleanup.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01520">Entropy and the Kullback-Leibler Divergence for Bayesian Networks: Computational Complexity and Efficient Implementation. (arXiv:2312.01520v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Scutari_M/0/1/0/all/0/1">Marco Scutari</a></p>
<p>Bayesian networks (BNs) are a foundational model in machine learning and
causal inference. Their graphical structure can handle high-dimensional
problems, divide them into a sparse collection of smaller ones, underlies Judea
Pearl's causality, and determines their explainability and interpretability.
Despite their popularity, there are almost no resources in the literature on
how to compute Shannon's entropy and the Kullback-Leibler (KL) divergence for
BNs under their most common distributional assumptions. In this paper, we
provide computationally efficient algorithms for both by leveraging BNs'
graphical structure, and we illustrate them with a complete set of numerical
examples. In the process, we show it is possible to reduce the computational
complexity of KL from cubic to quadratic for Gaussian BNs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.09015">Uncertainty in GNN Learning Evaluations: A Comparison Between Measures for Quantifying Randomness in GNN Community Detection. (arXiv:2312.09015v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Leeney_W/0/1/0/all/0/1">William Leeney</a>, <a href="http://arxiv.org/find/cs/1/au:+McConville_R/0/1/0/all/0/1">Ryan McConville</a></p>
<p>(1) The enhanced capability of Graph Neural Networks (GNNs) in unsupervised
community detection of clustered nodes is attributed to their capacity to
encode both the connectivity and feature information spaces of graphs. The
identification of latent communities holds practical significance in various
domains, from social networks to genomics. Current real-world performance
benchmarks are perplexing due to the multitude of decisions influencing GNN
evaluations for this task. (2) Three metrics are compared to assess the
consistency of algorithm rankings in the presence of randomness. The
consistency and quality of performance between the results under a
hyperparameter optimisation with the default hyperparameters is evaluated. (3)
The results compare hyperparameter optimisation with default hyperparameters,
revealing a significant performance loss when neglecting hyperparameter
investigation. A comparison of metrics indicates that ties in ranks can
substantially alter the quantification of randomness. (4) Ensuring adherence to
the same evaluation criteria may result in notable differences in the reported
performance of methods for this task. The $W$ Randomness coefficient, based on
the Wasserstein distance, is identified as providing the most robust assessment
of randomness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11671">Evaluating Language-Model Agents on Realistic Autonomous Tasks. (arXiv:2312.11671v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kinniment_M/0/1/0/all/0/1">Megan Kinniment</a>, <a href="http://arxiv.org/find/cs/1/au:+Sato_L/0/1/0/all/0/1">Lucas Jun Koba Sato</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1">Haoxing Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Goodrich_B/0/1/0/all/0/1">Brian Goodrich</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasin_M/0/1/0/all/0/1">Max Hasin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_L/0/1/0/all/0/1">Lawrence Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Miles_L/0/1/0/all/0/1">Luke Harold Miles</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1">Tao R. Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wijk_H/0/1/0/all/0/1">Hjalmar Wijk</a>, <a href="http://arxiv.org/find/cs/1/au:+Burget_J/0/1/0/all/0/1">Joel Burget</a>, <a href="http://arxiv.org/find/cs/1/au:+Ho_A/0/1/0/all/0/1">Aaron Ho</a>, <a href="http://arxiv.org/find/cs/1/au:+Barnes_E/0/1/0/all/0/1">Elizabeth Barnes</a>, <a href="http://arxiv.org/find/cs/1/au:+Christiano_P/0/1/0/all/0/1">Paul Christiano</a></p>
<p>In this report, we explore the ability of language model agents to acquire
resources, create copies of themselves, and adapt to novel challenges they
encounter in the wild. We refer to this cluster of capabilities as "autonomous
replication and adaptation" or ARA. We believe that systems capable of ARA
could have wide-reaching and hard-to-anticipate consequences, and that
measuring and forecasting ARA may be useful for informing measures around
security, monitoring, and alignment. Additionally, once a system is capable of
ARA, placing bounds on a system's capabilities may become significantly more
difficult.
</p>
<p>We construct four simple example agents that combine language models with
tools that allow them to take actions in the world. We then evaluate these
agents on 12 tasks relevant to ARA. We find that these language model agents
can only complete the easiest tasks from this list, although they make some
progress on the more challenging tasks. Unfortunately, these evaluations are
not adequate to rule out the possibility that near-future agents will be
capable of ARA. In particular, we do not think that these evaluations provide
good assurance that the ``next generation'' of language models (e.g. 100x
effective compute scaleup on existing models) will not yield agents capable of
ARA, unless intermediate evaluations are performed during pretraining.
Relatedly, we expect that fine-tuning of the existing models could produce
substantially more competent agents, even if the fine-tuning is not directly
targeted at ARA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11973">Continual Learning: Forget-free Winning Subnetworks for Video Representations. (arXiv:2312.11973v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1">Haeyong Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1">Jaehong Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1">Sung Ju Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoo_C/0/1/0/all/0/1">Chang D. Yoo</a></p>
<p>Inspired by the Lottery Ticket Hypothesis (LTH), which highlights the
existence of efficient subnetworks within larger, dense networks, a
high-performing Winning Subnetwork (WSN) in terms of task performance under
appropriate sparsity conditions is considered for various continual learning
tasks. It leverages pre-existing weights from dense networks to achieve
efficient learning in Task Incremental Learning (TIL) scenarios. In Few-Shot
Class Incremental Learning (FSCIL), a variation of WSN referred to as the Soft
subnetwork (SoftNet) is designed to prevent overfitting when the data samples
are scarce. Furthermore, the sparse reuse of WSN weights is considered for
Video Incremental Learning (VIL). The use of Fourier Subneural Operator (FSO)
within WSN is considered. It enables compact encoding of videos and identifies
reusable subnetworks across varying bandwidths. We have integrated FSO into
different architectural frameworks for continual learning, including VIL, TIL,
and FSCIL. Our comprehensive experiments demonstrate FSO's effectiveness,
significantly improving task performance at various convolutional
representational levels. Specifically, FSO enhances higher-layer performance in
TIL and FSCIL and lower-layer performance in VIL
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12728">Lookahead: An Inference Acceleration Framework for Large Language Model with Lossless Generation Accuracy. (arXiv:2312.12728v2 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yao Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1">Zhitian Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_C/0/1/0/all/0/1">Chenyi Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jinjie Gu</a></p>
<p>As Large Language Models (LLMs) have made significant advancements across
various tasks, such as question answering, translation, text summarization, and
dialogue systems, the need for accuracy in information becomes crucial,
especially for serious financial products serving billions of users like
Alipay. To address this, Alipay has developed a Retrieval-Augmented Generation
(RAG) system that grounds LLMs on the most accurate and up-to-date information.
However, for a real-world product serving millions of users, the inference
speed of LLMs becomes a critical factor compared to a mere experimental model.
</p>
<p>Hence, this paper presents a generic framework for accelerating the inference
process, resulting in a substantial increase in speed and cost reduction for
our RAG system, with lossless generation accuracy. In the traditional inference
process, each token is generated sequentially by the LLM, leading to a time
consumption proportional to the number of generated tokens. To enhance this
process, our framework, named \textit{lookahead}, introduces a
\textit{multi-branch} strategy. Instead of generating a single token at a time,
we propose a \textit{Trie-based Retrieval} (TR) process that enables the
generation of multiple branches simultaneously, each of which is a sequence of
tokens. Subsequently, for each branch, a \textit{Verification and Accept} (VA)
process is performed to identify the longest correct sub-sequence as the final
output. Our strategy offers two distinct advantages: (1) it guarantees absolute
correctness of the output, avoiding any approximation algorithms, and (2) the
worst-case performance of our approach is equivalent to the conventional
process. We conduct extensive experiments to demonstrate the significant
improvements achieved by applying our inference acceleration framework. Code is
avaliable: https://github.com/alipay/PainlessInferenceAcceleration.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12789">SLP-Net:An efficient lightweight network for segmentation of skin lesions. (arXiv:2312.12789v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Yang_B/0/1/0/all/0/1">Bo Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Peng_H/0/1/0/all/0/1">Hong Peng</a>, <a href="http://arxiv.org/find/eess/1/au:+Guo_C/0/1/0/all/0/1">Chenggang Guo</a>, <a href="http://arxiv.org/find/eess/1/au:+Luo_X/0/1/0/all/0/1">Xiaohui Luo</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1">Jun Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Long_X/0/1/0/all/0/1">Xianzhong Long</a></p>
<p>Prompt treatment for melanoma is crucial. To assist physicians in identifying
lesion areas precisely in a quick manner, we propose a novel skin lesion
segmentation technique namely SLP-Net, an ultra-lightweight segmentation
network based on the spiking neural P(SNP) systems type mechanism. Most
existing convolutional neural networks achieve high segmentation accuracy while
neglecting the high hardware cost. SLP-Net, on the contrary, has a very small
number of parameters and a high computation speed. We design a lightweight
multi-scale feature extractor without the usual encoder-decoder structure.
Rather than a decoder, a feature adaptation module is designed to replace it
and implement multi-scale information decoding. Experiments at the ISIC2018
challenge demonstrate that the proposed model has the highest Acc and DSC among
the state-of-the-art methods, while experiments on the PH2 dataset also
demonstrate a favorable generalization ability. Finally, we compare the
computational complexity as well as the computational speed of the models in
experiments, where SLP-Net has the highest overall superiority
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13143">Underwater Acoustic Signal Recognition Based on Salient Feature. (arXiv:2312.13143v2 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Minghao Chen</a></p>
<p>With the rapid advancement of technology, the recognition of underwater
acoustic signals in complex environments has become increasingly crucial.
Currently, mainstream underwater acoustic signal recognition relies primarily
on time-frequency analysis to extract spectral features, finding widespread
applications in the field. However, existing recognition methods heavily depend
on expert systems, facing limitations such as restricted knowledge bases and
challenges in handling complex relationships. These limitations stem from the
complexity and maintenance difficulties associated with rules or inference
engines. Recognizing the potential advantages of deep learning in handling
intricate relationships, this paper proposes a method utilizing neural networks
for underwater acoustic signal recognition. The proposed approach involves
continual learning of features extracted from spectra for the classification of
underwater acoustic signals. Deep learning models can automatically learn
abstract features from data and continually adjust weights during training to
enhance classification performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14504">Theory of Hallucinations based on Equivariance. (arXiv:2312.14504v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shibata_H/0/1/0/all/0/1">Hisaichi Shibata</a></p>
<p>This study aims to acquire knowledge for creating very large language models
that are immune to hallucinations. Hallucinations in contemporary large
language models are often attributed to a misunderstanding of real-world social
relationships. Therefore, I hypothesize that very large language models capable
of thoroughly grasping all these relationships will be free from
hallucinations. Additionally, I propose that certain types of equivariant
language models are adept at learning and understanding these relationships.
Building on this, I have developed a specialized cross-entropy error function
to create a hallucination scale for language models, which measures their
extent of equivariance acquisition. Utilizing this scale, I tested language
models for their ability to acquire character-level equivariance. In
particular, I introduce and employ a novel technique based on T5 (Text To Text
Transfer Transformer) that efficiently understands permuted input texts without
the need for explicit dictionaries to convert token IDs (integers) to texts
(strings). This T5 model demonstrated a moderate ability to acquire
character-level equivariance. Additionally, I discovered scale laws that can
aid in developing hallucination-free language models at the character level.
This methodology can be extended to assess equivariance acquisition at the word
level, paving the way for very large language models that can comprehensively
understand relationships and, consequently, avoid hallucinations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15045">Probabilistic Modeling for Sequences of Sets in Continuous-Time. (arXiv:2312.15045v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1">Yuxin Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Boyd_A/0/1/0/all/0/1">Alex Boyd</a>, <a href="http://arxiv.org/find/cs/1/au:+Smyth_P/0/1/0/all/0/1">Padhraic Smyth</a></p>
<p>Neural marked temporal point processes have been a valuable addition to the
existing toolbox of statistical parametric models for continuous-time event
data. These models are useful for sequences where each event is associated with
a single item (a single type of event or a "mark") -- but such models are not
suited for the practical situation where each event is associated with a set of
items. In this work, we develop a general framework for modeling set-valued
data in continuous-time, compatible with any intensity-based recurrent neural
point process model. In addition, we develop inference methods that can use
such models to answer probabilistic queries such as "the probability of item
$A$ being observed before item $B$," conditioned on sequence history. Computing
exact answers for such queries is generally intractable for neural models due
to both the continuous-time nature of the problem setting and the
combinatorially-large space of potential outcomes for each event. To address
this, we develop a class of importance sampling methods for querying with
set-based sequences and demonstrate orders-of-magnitude improvements in
efficiency over direct sampling via systematic experiments with four real-world
datasets. We also illustrate how to use this framework to perform model
selection using likelihoods that do not involve one-step-ahead prediction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15228">Adversarial Data Poisoning for Fake News Detection: How to Make a Model Misclassify a Target News without Modifying It. (arXiv:2312.15228v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Siciliano_F/0/1/0/all/0/1">Federico Siciliano</a>, <a href="http://arxiv.org/find/cs/1/au:+Maiano_L/0/1/0/all/0/1">Luca Maiano</a>, <a href="http://arxiv.org/find/cs/1/au:+Papa_L/0/1/0/all/0/1">Lorenzo Papa</a>, <a href="http://arxiv.org/find/cs/1/au:+Baccini_F/0/1/0/all/0/1">Federica Baccini</a>, <a href="http://arxiv.org/find/cs/1/au:+Amerini_I/0/1/0/all/0/1">Irene Amerini</a>, <a href="http://arxiv.org/find/cs/1/au:+Silvestri_F/0/1/0/all/0/1">Fabrizio Silvestri</a></p>
<p>Fake news detection models are critical to countering disinformation but can
be manipulated through adversarial attacks. In this position paper, we analyze
how an attacker can compromise the performance of an online learning detector
on specific news content without being able to manipulate the original target
news. In some contexts, such as social networks, where the attacker cannot
exert complete control over all the information, this scenario can indeed be
quite plausible. Therefore, we show how an attacker could potentially introduce
poisoning data into the training data to manipulate the behavior of an online
learning method. Our initial findings reveal varying susceptibility of logistic
regression models based on complexity and attack type.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15665">A Multi-Modal Contrastive Diffusion Model for Therapeutic Peptide Generation. (arXiv:2312.15665v2 [q-bio.QM] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Wang_Y/0/1/0/all/0/1">Yongkang Wang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Liu_X/0/1/0/all/0/1">Xuan Liu</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Huang_F/0/1/0/all/0/1">Feng Huang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Xiong_Z/0/1/0/all/0/1">Zhankun Xiong</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Zhang_W/0/1/0/all/0/1">Wen Zhang</a></p>
<p>Therapeutic peptides represent a unique class of pharmaceutical agents
crucial for the treatment of human diseases. Recently, deep generative models
have exhibited remarkable potential for generating therapeutic peptides, but
they only utilize sequence or structure information alone, which hinders the
performance in generation. In this study, we propose a Multi-Modal Contrastive
Diffusion model (MMCD), fusing both sequence and structure modalities in a
diffusion framework to co-generate novel peptide sequences and structures.
Specifically, MMCD constructs the sequence-modal and structure-modal diffusion
models, respectively, and devises a multi-modal contrastive learning strategy
with intercontrastive and intra-contrastive in each diffusion timestep, aiming
to capture the consistency between two modalities and boost model performance.
The inter-contrastive aligns sequences and structures of peptides by maximizing
the agreement of their embeddings, while the intra-contrastive differentiates
therapeutic and non-therapeutic peptides by maximizing the disagreement of
their sequence/structure embeddings simultaneously. The extensive experiments
demonstrate that MMCD performs better than other state-of-theart deep
generative methods in generating therapeutic peptides across various metrics,
including antimicrobial/anticancer score, diversity, and peptide-docking.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16430">Preference as Reward, Maximum Preference Optimization with Importance Sampling. (arXiv:2312.16430v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1">Zaifan Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xing Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1">Chao Wei</a></p>
<p>Preference learning is a key technology for aligning language models with
human values. Reinforcement Learning from Human Feedback (RLHF) is a model
based algorithm to optimize preference learning, which first fitting a reward
model for preference score, and then optimizing generating policy with
on-policy PPO algorithm to maximize the reward. The processing of RLHF is
complex, time-consuming and unstable. Direct Preference Optimization (DPO)
algorithm using off-policy algorithm to direct optimize generating policy and
eliminating the need for reward model, which is data efficient and stable. DPO
use Bradley-Terry model and log-loss which leads to over-fitting to the
preference data at the expense of ignoring KL-regularization term when
preference near deterministic. IPO uses a root-finding pairwise MSE loss to
solve the ignoring KL-regularization problem, and learning an optimal policy.
But IPO's pairwise loss still can't s make the KL-regularization to work. In
this paper, we design a simple and intuitive off-policy preferences
optimization algorithm from an importance sampling view, and add an off-policy
KL-regularization term which makes KL-regularization truly effective. To
simplify the learning process and save memory usage, we can generate
regularization data in advance, which eliminate the needs for both reward model
and reference policy in the stage of optimization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16713">Knowledge Enhanced Conditional Imputation for Healthcare Time-series. (arXiv:2312.16713v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qian_L/0/1/0/all/0/1">Linglong Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Ibrahim_Z/0/1/0/all/0/1">Zina Ibrahim</a>, <a href="http://arxiv.org/find/cs/1/au:+Ellis_H/0/1/0/all/0/1">Hugh Logan Ellis</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1">Ao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuezhou Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dobson_R/0/1/0/all/0/1">Richard Dobson</a></p>
<p>This study presents a novel approach to addressing the challenge of missing
data in multivariate time series, with a particular focus on the complexities
of healthcare data. Our Conditional Self-Attention Imputation (CSAI) model,
grounded in a transformer-based framework, introduces a conditional hidden
state initialization tailored to the intricacies of medical time series data.
This methodology diverges from traditional imputation techniques by
specifically targeting the imbalance in missing data distribution, a crucial
aspect often overlooked in healthcare datasets. By integrating advanced
knowledge embedding and a non-uniform masking strategy, CSAI adeptly adjusts to
the distinct patterns of missing data in Electronic Health Records (EHRs).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.00413">Real-Time FJ/MAC PDE Solvers via Tensorized, Back-Propagation-Free Optical PINN Training. (arXiv:2401.00413v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yequan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1">Xian Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1">Xinling Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Ziyue Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhixiong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Kurczveil_G/0/1/0/all/0/1">Geza Kurczveil</a>, <a href="http://arxiv.org/find/cs/1/au:+Beausoleil_R/0/1/0/all/0/1">Raymond G. Beausoleil</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zheng Zhang</a></p>
<p>Solving partial differential equations (PDEs) numerically often requires huge
computing time, energy cost, and hardware resources in practical applications.
This has limited their applications in many scenarios (e.g., autonomous
systems, supersonic flows) that have a limited energy budget and require near
real-time response. Leveraging optical computing, this paper develops an
on-chip training framework for physics-informed neural networks (PINNs), aiming
to solve high-dimensional PDEs with fJ/MAC photonic power consumption and
ultra-low latency. Despite the ultra-high speed of optical neural networks,
training a PINN on an optical chip is hard due to (1) the large size of
photonic devices, and (2) the lack of scalable optical memory devices to store
the intermediate results of back-propagation (BP). To enable realistic optical
PINN training, this paper presents a scalable method to avoid the BP process.
We also employ a tensor-compressed approach to improve the convergence and
scalability of our optical PINN training. This training framework is designed
with tensorized optical neural networks (TONN) for scalable inference
acceleration and MZI phase-domain tuning for \textit{in-situ} optimization. Our
simulation results of a 20-dim HJB PDE show that our photonic accelerator can
reduce the number of MZIs by a factor of $1.17\times 10^3$, with only $1.36$ J
and $1.15$ s to solve this equation. This is the first real-size optical PINN
training framework that can be applied to solve high-dimensional PDEs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.00625">Beyond Efficiency: A Systematic Survey of Resource-Efficient Large Language Models. (arXiv:2401.00625v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bai_G/0/1/0/all/0/1">Guangji Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Chai_Z/0/1/0/all/0/1">Zheng Chai</a>, <a href="http://arxiv.org/find/cs/1/au:+Ling_C/0/1/0/all/0/1">Chen Ling</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shiyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Jiaying Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Nan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_T/0/1/0/all/0/1">Tingwei Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Ziyang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1">Mengdan Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yifei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Carl Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1">Yue Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1">Liang Zhao</a></p>
<p>The burgeoning field of Large Language Models (LLMs), exemplified by
sophisticated models like OpenAI's ChatGPT, represents a significant
advancement in artificial intelligence. These models, however, bring forth
substantial challenges in the high consumption of computational, memory,
energy, and financial resources, especially in environments with limited
resource capabilities. This survey aims to systematically address these
challenges by reviewing a broad spectrum of techniques designed to enhance the
resource efficiency of LLMs. We categorize methods based on their optimization
focus: computational, memory, energy, financial, and network resources and
their applicability across various stages of an LLM's lifecycle, including
architecture design, pretraining, finetuning, and system design. Additionally,
the survey introduces a nuanced categorization of resource efficiency
techniques by their specific resource types, which uncovers the intricate
relationships and mappings between various resources and corresponding
optimization techniques. A standardized set of evaluation metrics and datasets
is also presented to facilitate consistent and fair comparisons across
different models and techniques. By offering a comprehensive overview of the
current sota and identifying open research avenues, this survey serves as a
foundational reference for researchers and practitioners, aiding them in
developing more sustainable and efficient LLMs in a rapidly evolving landscape.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01641">Towards a Foundation Purchasing Model: Pretrained Generative Autoregression on Transaction Sequences. (arXiv:2401.01641v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Skalski_P/0/1/0/all/0/1">Piotr Skalski</a>, <a href="http://arxiv.org/find/cs/1/au:+Sutton_D/0/1/0/all/0/1">David Sutton</a>, <a href="http://arxiv.org/find/cs/1/au:+Burrell_S/0/1/0/all/0/1">Stuart Burrell</a>, <a href="http://arxiv.org/find/cs/1/au:+Perez_I/0/1/0/all/0/1">Iker Perez</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1">Jason Wong</a></p>
<p>Machine learning models underpin many modern financial systems for use cases
such as fraud detection and churn prediction. Most are based on supervised
learning with hand-engineered features, which relies heavily on the
availability of labelled data. Large self-supervised generative models have
shown tremendous success in natural language processing and computer vision,
yet so far they haven't been adapted to multivariate time series of financial
transactions. In this paper, we present a generative pretraining method that
can be used to obtain contextualised embeddings of financial transactions.
Benchmarks on public datasets demonstrate that it outperforms state-of-the-art
self-supervised methods on a range of downstream tasks. We additionally perform
large-scale pretraining of an embedding model using a corpus of data from 180
issuing banks containing 5.1 billion transactions and apply it to the card
fraud detection problem on hold-out datasets. The embedding model significantly
improves value detection rate at high precision thresholds and transfers well
to out-of-domain distributions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01783">Approximating Numerical Flux by Fourier Neural Operators for the Hyperbolic Conservation Laws. (arXiv:2401.01783v2 [math.NA] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Kim_T/0/1/0/all/0/1">Taeyoung Kim</a>, <a href="http://arxiv.org/find/math/1/au:+Kang_M/0/1/0/all/0/1">Myungjoo Kang</a></p>
<p>Classical numerical schemes exist for solving PDEs numerically, and recently,
neural network-based methods have been developed. However, methodologies using
neural networks, such as PINN and neural operators, lack robustness and
generalization power. To compensate for such drawbacks, there are many types of
research combining classical numerical schemes and machine learning methods by
replacing a small portion of the numerical schemes with neural networks. In
this work, we focus on hyperbolic conservation laws and replace numerical
fluxes in the numerical schemes by neural operator. For this, we construct
losses that are motivated by numerical schemes for conservation laws and
approximate numerical flux by FNO. Through experiments, we show that our
methodology has advantages of both numerical schemes and FNO by comparing with
original methods. For instance, we demonstrate our method gains robustness,
resolution invariance property, and feasibility of a data-driven method. Our
method especially has the ability to predict continuously in time and
generalization power on the out-of-distribution samples, which are challenges
to be tackled for existing neural operator methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.15216">Diabetic Retinopathy Using Gaussian Filter. (arXiv:2309.15216v2 [cs.LG] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Muddaluru_R/0/1/0/all/0/1">Roshan Vasu Muddaluru</a>, <a href="http://arxiv.org/find/cs/1/au:+Thoguluva_S/0/1/0/all/0/1">Sharvaani Ravikumar Thoguluva</a>, <a href="http://arxiv.org/find/cs/1/au:+Prabha_S/0/1/0/all/0/1">Shruti Prabha</a>, <a href="http://arxiv.org/find/cs/1/au:+Reddy_T/0/1/0/all/0/1">Tanuja Konda Reddy</a>, <a href="http://arxiv.org/find/cs/1/au:+P_D/0/1/0/all/0/1">Dr. Suja P</a></p>
<p>The retina is an essential component of the visual system, and maintaining
eyesight depends on the timely and correct detection of disorders. This
research specifically addresses the early-stage detection and severity
classification of diabetic retinopathy (DR), a serious public health hazard. We
compare the results of different deep learning models such as InceptionV3,
DenseNet121 and other CNN based models by using different image filters, such
as Gaussian, grayscale and Gabor. These models could detect subtle pathological
alterations and use that information to estimate the risk of retinal illnesses.
The objective is to improve the diagnostic processes for diabetic retinopathy,
the primary cause of diabetes-related blindness, by utilizing deep learning
models. A comparative analysis between Greyscale, Gaussian and Gabor filters
has been provided after applying these filters on the retinal images. The
Gaussian filter resulted to be the most promising filter giving the best
accuracies for all the models. The best performing model was InceptionV3 which
gave an accuracy of 96% on Gaussian images, therefore Gaussian filter emerged
as our most promising filter.
</p>
</p>
</div>

    </div>
    </body>
    