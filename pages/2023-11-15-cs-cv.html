<!DOCTYPE html>
<html>
<head>
<title>2023-11-15-cs-cv</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2311.06276">Enhancing the machine vision performance with multi-spectral light sources. (arXiv:2311.06276v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1">Feng Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Bao_R/0/1/0/all/0/1">Rui Bao</a>, <a href="http://arxiv.org/find/eess/1/au:+Dai_C/0/1/0/all/0/1">Congqi Dai</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1">Wanlu Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1">Shu Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Guo_R/0/1/0/all/0/1">Ruiqian Guo</a></p>
<p>This study mainly focuses on the performance of different multi-spectral
light sources on different object colors in machine vision and tries to enhance
machine vision with multi-spectral light sources. Using different color pencils
as samples, by recognizing the collected images with two classical neural
networks, AlexNet and VGG19, the performance was investigated under 35
different multi-spectral light sources. The results show that for both models
there are always some non-pure white light sources, whose accuracy is better
than pure white light, which suggests the potential of multi-spectral light
sources to further enhance the effectiveness of machine vision. The comparison
of both models is also performed, and surprised to find that the overall
performance of VGG19 is lower than that of AlexNet, which shows that the
importance of the choice of multi-spectral light sources and models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06285">Sounding Bodies: Modeling 3D Spatial Sound of Humans Using Body Pose and Audio. (arXiv:2311.06285v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xudong Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Markovic_D/0/1/0/all/0/1">Dejan Markovic</a>, <a href="http://arxiv.org/find/cs/1/au:+Sandakly_J/0/1/0/all/0/1">Jacob Sandakly</a>, <a href="http://arxiv.org/find/cs/1/au:+Keebler_T/0/1/0/all/0/1">Todd Keebler</a>, <a href="http://arxiv.org/find/cs/1/au:+Krenn_S/0/1/0/all/0/1">Steven Krenn</a>, <a href="http://arxiv.org/find/cs/1/au:+Richard_A/0/1/0/all/0/1">Alexander Richard</a></p>
<p>While 3D human body modeling has received much attention in computer vision,
modeling the acoustic equivalent, i.e. modeling 3D spatial audio produced by
body motion and speech, has fallen short in the community. To close this gap,
we present a model that can generate accurate 3D spatial audio for full human
bodies. The system consumes, as input, audio signals from headset microphones
and body pose, and produces, as output, a 3D sound field surrounding the
transmitter's body, from which spatial audio can be rendered at any arbitrary
position in the 3D space. We collect a first-of-its-kind multimodal dataset of
human bodies, recorded with multiple cameras and a spherical array of 345
microphones. In an empirical evaluation, we demonstrate that our model can
produce accurate body-induced sound fields when trained with a suitable loss.
Dataset and code are available online.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06322">Post-training Quantization with Progressive Calibration and Activation Relaxing for Text-to-Image Diffusion Models. (arXiv:2311.06322v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1">Siao Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1">Chaoyu Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zewen Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1">Yansong Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1">Wenwu Zhu</a></p>
<p>Diffusion models have achieved great success due to their remarkable
generation ability. However, their high computational overhead is still a
troublesome problem. Recent studies have leveraged post-training quantization
(PTQ) to compress diffusion models. However, most of them only focus on
unconditional models, leaving the quantization of widely used large pretrained
text-to-image models, e.g., Stable Diffusion, largely unexplored. In this
paper, we propose a novel post-training quantization method PCR (Progressive
Calibration and Relaxing) for text-to-image diffusion models, which consists of
a progressive calibration strategy that considers the accumulated quantization
error across timesteps, and an activation relaxing strategy that improves the
performance with negligible cost. Additionally, we demonstrate the previous
metrics for text-to-image diffusion model quantization are not accurate due to
the distribution gap. To tackle the problem, we propose a novel QDiffBench
benchmark, which utilizes data in the same domain for more accurate evaluation.
Besides, QDiffBench also considers the generalization performance of the
quantized model outside the calibration dataset. Extensive experiments on
Stable Diffusion and Stable Diffusion XL demonstrate the superiority of our
method and benchmark. Moreover, we are the first to achieve quantization for
Stable Diffusion XL while maintaining the performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06329">A Survey of AI Text-to-Image and AI Text-to-Video Generators. (arXiv:2311.06329v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1">Aditi Singh</a></p>
<p>Text-to-Image and Text-to-Video AI generation models are revolutionary
technologies that use deep learning and natural language processing (NLP)
techniques to create images and videos from textual descriptions. This paper
investigates cutting-edge approaches in the discipline of Text-to-Image and
Text-to-Video AI generations. The survey provides an overview of the existing
literature as well as an analysis of the approaches used in various studies. It
covers data preprocessing techniques, neural network types, and evaluation
metrics used in the field. In addition, the paper discusses the challenges and
limitations of Text-to-Image and Text-to-Video AI generations, as well as
future research directions. Overall, these models have promising potential for
a wide range of applications such as video production, content creation, and
digital marketing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06375">Image Classification using Combination of Topological Features and Neural Networks. (arXiv:2311.06375v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lima_M/0/1/0/all/0/1">Mariana D&#xf3;ria Prata Lima</a>, <a href="http://arxiv.org/find/cs/1/au:+Giraldi_G/0/1/0/all/0/1">Gilson Antonio Giraldi</a>, <a href="http://arxiv.org/find/cs/1/au:+Junior_G/0/1/0/all/0/1">Gast&#xe3;o Flor&#xea;ncio Miranda Junior</a></p>
<p>In this work we use the persistent homology method, a technique in
topological data analysis (TDA), to extract essential topological features from
the data space and combine them with deep learning features for classification
tasks. In TDA, the concepts of complexes and filtration are building blocks.
Firstly, a filtration is constructed from some complex. Then, persistent
homology classes are computed, and their evolution along the filtration is
visualized through the persistence diagram. Additionally, we applied
vectorization techniques to the persistence diagram to make this topological
information compatible with machine learning algorithms. This was carried out
with the aim of classifying images from multiple classes in the MNIST dataset.
Our approach inserts topological features into deep learning approaches
composed by single and two-streams neural networks architectures based on a
multi-layer perceptron (MLP) and a convolutional neral network (CNN) taylored
for multi-class classification in the MNIST dataset. In our analysis, we
evaluated the obtained results and compared them with the outcomes achieved
through the baselines that are available in the TensorFlow library. The main
conclusion is that topological information may increase neural network accuracy
in multi-class classification tasks with the price of computational complexity
of persistent homology calculation. Up to the best of our knowledge, it is the
first work that combines deep learning features and the combination of
topological features for multi-class classification tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06386">Towards A Unified Neural Architecture for Visual Recognition and Reasoning. (arXiv:2311.06386v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1">Calvin Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1">Boqing Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Ting Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1">Chen Sun</a></p>
<p>Recognition and reasoning are two pillars of visual understanding. However,
these tasks have an imbalance in focus; whereas recent advances in neural
networks have shown strong empirical performance in visual recognition, there
has been comparably much less success in solving visual reasoning. Intuitively,
unifying these two tasks under a singular framework is desirable, as they are
mutually dependent and beneficial. Motivated by the recent success of
multi-task transformers for visual recognition and language understanding, we
propose a unified neural architecture for visual recognition and reasoning with
a generic interface (e.g., tokens) for both. Our framework enables the
principled investigation of how different visual recognition tasks, datasets,
and inductive biases can help enable spatiotemporal reasoning capabilities.
Noticeably, we find that object detection, which requires spatial localization
of individual objects, is the most beneficial recognition task for reasoning.
We further demonstrate via probing that implicit object-centric representations
emerge automatically inside our framework. Intriguingly, we discover that
certain architectural choices such as the backbone model of the visual encoder
have a significant impact on visual reasoning, but little on object detection.
Given the results of our experiments, we believe that visual reasoning should
be considered as a first-class citizen alongside visual recognition, as they
are strongly correlated but benefit from potentially different design choices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06394">A design of Convolutional Neural Network model for the Diagnosis of the COVID-19. (arXiv:2311.06394v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Song_X/0/1/0/all/0/1">Xinyuan Song</a></p>
<p>With the spread of COVID-19 around the globe over the past year, the usage of
artificial intelligence (AI) algorithms and image processing methods to analyze
the X-ray images of patients' chest with COVID-19 has become essential. The
COVID-19 virus recognition in the lung area of a patient is one of the basic
and essential needs of clicical centers and hospitals. Most research in this
field has been devoted to papers on the basis of deep learning methods
utilizing CNNs (Convolutional Neural Network), which mainly deal with the
screening of sick and healthy people.In this study, a new structure of a
19-layer CNN has been recommended for accurately recognition of the COVID-19
from the X-ray pictures of chest. The offered CNN is developed to serve as a
precise diagnosis system for a three class (viral pneumonia, Normal, COVID) and
a four classclassification (Lung opacity, Normal, COVID-19, and pneumonia). A
comparison is conducted among the outcomes of the offered procedure and some
popular pretrained networks, including Inception, Alexnet, ResNet50,
Squeezenet, and VGG19 and based on Specificity, Accuracy, Precision,
Sensitivity, Confusion Matrix, and F1-score. The experimental results of the
offered CNN method specify its dominance over the existing published
procedures. This method can be a useful tool for clinicians in deciding
properly about COVID-19.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06400">EviPrompt: A Training-Free Evidential Prompt Generation Method for Segment Anything Model in Medical Images. (arXiv:2311.06400v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yinsong Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jiaqi Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Men_A/0/1/0/all/0/1">Aidong Men</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Qingchao Chen</a></p>
<p>Medical image segmentation has immense clinical applicability but remains a
challenge despite advancements in deep learning. The Segment Anything Model
(SAM) exhibits potential in this field, yet the requirement for expertise
intervention and the domain gap between natural and medical images poses
significant obstacles. This paper introduces a novel training-free evidential
prompt generation method named EviPrompt to overcome these issues. The proposed
method, built on the inherent similarities within medical images, requires only
a single reference image-annotation pair, making it a training-free solution
that significantly reduces the need for extensive labeling and computational
resources. First, to automatically generate prompts for SAM in medical images,
we introduce an evidential method based on uncertainty estimation without the
interaction of clinical experts. Then, we incorporate the human prior into the
prompts, which is vital for alleviating the domain gap between natural and
medical images and enhancing the applicability and usefulness of SAM in medical
scenarios. EviPrompt represents an efficient and robust approach to medical
image segmentation, with evaluations across a broad range of tasks and
modalities confirming its efficacy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06411">Analyzing Modular Approaches for Visual Question Decomposition. (arXiv:2311.06411v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khandelwal_A/0/1/0/all/0/1">Apoorv Khandelwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1">Ellie Pavlick</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1">Chen Sun</a></p>
<p>Modular neural networks without additional training have recently been shown
to surpass end-to-end neural networks on challenging vision-language tasks. The
latest such methods simultaneously introduce LLM-based code generation to build
programs and a number of skill-specific, task-oriented modules to execute them.
In this paper, we focus on ViperGPT and ask where its additional performance
comes from and how much is due to the (state-of-art, end-to-end) BLIP-2 model
it subsumes vs. additional symbolic components. To do so, we conduct a
controlled study (comparing end-to-end, modular, and prompting-based methods
across several VQA benchmarks). We find that ViperGPT's reported gains over
BLIP-2 can be attributed to its selection of task-specific modules, and when we
run ViperGPT using a more task-agnostic selection of modules, these gains go
away. Additionally, ViperGPT retains much of its performance if we make
prominent alterations to its selection of modules: e.g. removing or retaining
only BLIP-2. Finally, we compare ViperGPT against a prompting-based
decomposition strategy and find that, on some benchmarks, modular approaches
significantly benefit by representing subtasks with natural language, instead
of code.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06423">Flatness-aware Adversarial Attack. (arXiv:2311.06423v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1">Mingyuan Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaodan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Cen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yinggui Wang</a></p>
<p>The transferability of adversarial examples can be exploited to launch
black-box attacks. However, adversarial examples often present poor
transferability. To alleviate this issue, by observing that the diversity of
inputs can boost transferability, input regularization based methods are
proposed, which craft adversarial examples by combining several transformed
inputs. We reveal that input regularization based methods make resultant
adversarial examples biased towards flat extreme regions. Inspired by this, we
propose an attack called flatness-aware adversarial attack (FAA) which
explicitly adds a flatness-aware regularization term in the optimization target
to promote the resultant adversarial examples towards flat extreme regions. The
flatness-aware regularization term involves gradients of samples around the
resultant adversarial examples but optimizing gradients requires the evaluation
of Hessian matrix in high-dimension spaces which generally is intractable. To
address the problem, we derive an approximate solution to circumvent the
construction of Hessian matrix, thereby making FAA practical and cheap.
Extensive experiments show the transferability of adversarial examples crafted
by FAA can be considerably boosted compared with state-of-the-art baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06443">CVTHead: One-shot Controllable Head Avatar with Vertex-feature Transformer. (arXiv:2311.06443v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1">Haoyu Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1">Shanlin Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1">Xiangyi Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1">Kun Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xiaohui Xie</a></p>
<p>Reconstructing personalized animatable head avatars has significant
implications in the fields of AR/VR. Existing methods for achieving explicit
face control of 3D Morphable Models (3DMM) typically rely on multi-view images
or videos of a single subject, making the reconstruction process complex.
Additionally, the traditional rendering pipeline is time-consuming, limiting
real-time animation possibilities. In this paper, we introduce CVTHead, a novel
approach that generates controllable neural head avatars from a single
reference image using point-based neural rendering. CVTHead considers the
sparse vertices of mesh as the point set and employs the proposed
Vertex-feature Transformer to learn local feature descriptors for each vertex.
This enables the modeling of long-range dependencies among all the vertices.
Experimental results on the VoxCeleb dataset demonstrate that CVTHead achieves
comparable performance to state-of-the-art graphics-based methods. Moreover, it
enables efficient rendering of novel human heads with various expressions, head
poses, and camera views. These attributes can be explicitly controlled using
the coefficients of 3DMMs, facilitating versatile and realistic animation in
real-time scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06455">Aria-NeRF: Multimodal Egocentric View Synthesis. (arXiv:2311.06455v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jiankai Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1">Jianing Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1">Chuanyang Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Tucker_J/0/1/0/all/0/1">John Tucker</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Javier Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Schwager_M/0/1/0/all/0/1">Mac Schwager</a></p>
<p>We seek to accelerate research in developing rich, multimodal scene models
trained from egocentric data, based on differentiable volumetric ray-tracing
inspired by Neural Radiance Fields (NeRFs). The construction of a NeRF-like
model from an egocentric image sequence plays a pivotal role in understanding
human behavior and holds diverse applications within the realms of VR/AR. Such
egocentric NeRF-like models may be used as realistic simulations, contributing
significantly to the advancement of intelligent agents capable of executing
tasks in the real-world. The future of egocentric view synthesis may lead to
novel environment representations going beyond today's NeRFs by augmenting
visual data with multimodal sensors such as IMU for egomotion tracking, audio
sensors to capture surface texture and human language context, and eye-gaze
trackers to infer human attention patterns in the scene. To support and
facilitate the development and evaluation of egocentric multimodal scene
modeling, we present a comprehensive multimodal egocentric video dataset. This
dataset offers a comprehensive collection of sensory data, featuring RGB
images, eye-tracking camera footage, audio recordings from a microphone,
atmospheric pressure readings from a barometer, positional coordinates from
GPS, connectivity details from Wi-Fi and Bluetooth, and information from
dual-frequency IMU datasets (1kHz and 800Hz) paired with a magnetometer. The
dataset was collected with the Meta Aria Glasses wearable device platform. The
diverse data modalities and the real-world context captured within this dataset
serve as a robust foundation for furthering our understanding of human behavior
and enabling more immersive and intelligent experiences in the realms of VR,
AR, and robotics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06495">LayoutPrompter: Awaken the Design Ability of Large Language Models. (arXiv:2311.06495v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Jiawei Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jiaqi Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1">Shizhao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zijiang James Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1">Jian-Guang Lou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dongmei Zhang</a></p>
<p>Conditional graphic layout generation, which automatically maps user
constraints to high-quality layouts, has attracted widespread attention today.
Although recent works have achieved promising performance, the lack of
versatility and data efficiency hinders their practical applications. In this
work, we propose LayoutPrompter, which leverages large language models (LLMs)
to address the above problems through in-context learning. LayoutPrompter is
made up of three key components, namely input-output serialization, dynamic
exemplar selection and layout ranking. Specifically, the input-output
serialization component meticulously designs the input and output formats for
each layout generation task. Dynamic exemplar selection is responsible for
selecting the most helpful prompting exemplars for a given input. And a layout
ranker is used to pick the highest quality layout from multiple outputs of
LLMs. We conduct experiments on all existing layout generation tasks using four
public datasets. Despite the simplicity of our approach, experimental results
show that LayoutPrompter can compete with or even outperform state-of-the-art
approaches on these tasks without any model training or fine-tuning. This
demonstrates the effectiveness of this versatile and training-free approach. In
addition, the ablation studies show that LayoutPrompter is significantly
superior to the training-based baseline in a low-data regime, further
indicating the data efficiency of LayoutPrompter. Our project is available at
https://github.com/microsoft/LayoutGeneration/tree/main/LayoutPrompter.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06497">DRUformer: Enhancing the driving scene Important object detection with driving relationship self-understanding. (arXiv:2311.06497v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1">Yingjie Niu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1">Ming Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Fujii_K/0/1/0/all/0/1">Keisuke Fujii</a>, <a href="http://arxiv.org/find/cs/1/au:+Ohtani_K/0/1/0/all/0/1">Kento Ohtani</a>, <a href="http://arxiv.org/find/cs/1/au:+Carballo_A/0/1/0/all/0/1">Alexander Carballo</a>, <a href="http://arxiv.org/find/cs/1/au:+Takeda_K/0/1/0/all/0/1">Kazuya Takeda</a></p>
<p>Traffic accidents frequently lead to fatal injuries, contributing to over 50
million deaths until 2023. To mitigate driving hazards and ensure personal
safety, it is crucial to assist vehicles in anticipating important objects
during travel. Previous research on important object detection primarily
assessed the importance of individual participants, treating them as
independent entities and frequently overlooking the connections between these
participants. Unfortunately, this approach has proven less effective in
detecting important objects in complex scenarios. In response, we introduce
Driving scene Relationship self-Understanding transformer (DRUformer), designed
to enhance the important object detection task. The DRUformer is a
transformer-based multi-modal important object detection model that takes into
account the relationships between all the participants in the driving scenario.
Recognizing that driving intention also significantly affects the detection of
important objects during driving, we have incorporated a module for embedding
driving intention. To assess the performance of our approach, we conducted a
comparative experiment on the DRAMA dataset, pitting our model against other
state-of-the-art (SOTA) models. The results demonstrated a noteworthy 16.2\%
improvement in mIoU and a substantial 12.3\% boost in ACC compared to SOTA
methods. Furthermore, we conducted a qualitative analysis of our model's
ability to detect important objects across different road scenarios and
classes, highlighting its effectiveness in diverse contexts. Finally, we
conducted various ablation studies to assess the efficiency of the proposed
modules in our DRUformer model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06504">Self-supervised Context Learning for Visual Inspection of Industrial Defects. (arXiv:2311.06504v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Peng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1">Haiming Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1">Wenyong Yu</a></p>
<p>The unsupervised visual inspection of defects in industrial products poses a
significant challenge due to substantial variations in product surfaces.
Current unsupervised models struggle to strike a balance between detecting
texture and object defects, lacking the capacity to discern latent
representations and intricate features. In this paper, we present a novel
self-supervised learning algorithm designed to derive an optimal encoder by
tackling the renowned jigsaw puzzle. Our approach involves dividing the target
image into nine patches, tasking the encoder with predicting the relative
position relationships between any two patches to extract rich semantics.
Subsequently, we introduce an affinity-augmentation method to accentuate
differences between normal and abnormal latent representations. Leveraging the
classic support vector data description algorithm yields final detection
results. Experimental outcomes demonstrate that our proposed method achieves
outstanding detection and segmentation performance on the widely used MVTec AD
dataset, with rates of 95.8% and 96.8%, respectively, establishing a
state-of-the-art benchmark for both texture and object defects. Comprehensive
experimentation underscores the effectiveness of our approach in diverse
industrial applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06510">Band-wise Hyperspectral Image Pansharpening using CNN Model Propagation. (arXiv:2311.06510v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Guarino_G/0/1/0/all/0/1">Giuseppe Guarino</a>, <a href="http://arxiv.org/find/eess/1/au:+Ciotola_M/0/1/0/all/0/1">Matteo Ciotola</a>, <a href="http://arxiv.org/find/eess/1/au:+Vivone_G/0/1/0/all/0/1">Gemine Vivone</a>, <a href="http://arxiv.org/find/eess/1/au:+Scarpa_G/0/1/0/all/0/1">Giuseppe Scarpa</a></p>
<p>Hyperspectral pansharpening is receiving a growing interest since the last
few years as testified by a large number of research papers and challenges. It
consists in a pixel-level fusion between a lower-resolution hyperspectral
datacube and a higher-resolution single-band image, the panchromatic image,
with the goal of providing a hyperspectral datacube at panchromatic resolution.
Thanks to their powerful representational capabilities, deep learning models
have succeeded to provide unprecedented results on many general purpose image
processing tasks. However, when moving to domain specific problems, as in this
case, the advantages with respect to traditional model-based approaches are
much lesser clear-cut due to several contextual reasons. Scarcity of training
data, lack of ground-truth, data shape variability, are some such factors that
limit the generalization capacity of the state-of-the-art deep learning
networks for hyperspectral pansharpening. To cope with these limitations, in
this work we propose a new deep learning method which inherits a simple
single-band unsupervised pansharpening model nested in a sequential band-wise
adaptive scheme, where each band is pansharpened refining the model tuned on
the preceding one. By doing so, a simple model is propagated along the
wavelength dimension, adaptively and flexibly, with no need to have a fixed
number of spectral bands, and, with no need to dispose of large, expensive and
labeled training datasets. The proposed method achieves very good results on
our datasets, outperforming both traditional and deep learning reference
methods. The implementation of the proposed method can be found on
https://github.com/giu-guarino/R-PNN
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06536">CrashCar101: Procedural Generation for Damage Assessment. (arXiv:2311.06536v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Parslov_J/0/1/0/all/0/1">Jens Parslov</a>, <a href="http://arxiv.org/find/cs/1/au:+Riise_E/0/1/0/all/0/1">Erik Riise</a>, <a href="http://arxiv.org/find/cs/1/au:+Papadopoulos_D/0/1/0/all/0/1">Dim P. Papadopoulos</a></p>
<p>In this paper, we are interested in addressing the problem of damage
assessment for vehicles, such as cars. This task requires not only detecting
the location and the extent of the damage but also identifying the damaged
part. To train a computer vision system for the semantic part and damage
segmentation in images, we need to manually annotate images with costly pixel
annotations for both part categories and damage types. To overcome this need,
we propose to use synthetic data to train these models. Synthetic data can
provide samples with high variability, pixel-accurate annotations, and
arbitrarily large training sets without any human intervention. We propose a
procedural generation pipeline that damages 3D car models and we obtain
synthetic 2D images of damaged cars paired with pixel-accurate annotations for
part and damage categories. To validate our idea, we execute our pipeline and
render our CrashCar101 dataset. We run experiments on three real datasets for
the tasks of part and damage segmentation. For part segmentation, we show that
the segmentation models trained on a combination of real data and our synthetic
data outperform all models trained only on real data. For damage segmentation,
we show the sim2real transfer ability of CrashCar101.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06542">Generation Of Colors using Bidirectional Long Short Term Memory Networks. (arXiv:2311.06542v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1">A. Sinha</a></p>
<p>Human vision can distinguish between a vast spectrum of colours, estimated to
be between 2 to 7 million discernible shades. However, this impressive range
does not inherently imply that all these colours have been precisely named and
described within our lexicon. We often associate colours with familiar objects
and concepts in our daily lives. This research endeavors to bridge the gap
between our visual perception of countless shades and our ability to articulate
and name them accurately. A novel model has been developed to achieve this
goal, leveraging Bidirectional Long Short-Term Memory (BiLSTM) networks with
Active learning. This model operates on a proprietary dataset meticulously
curated for this study. The primary objective of this research is to create a
versatile tool for categorizing and naming previously unnamed colours or
identifying intermediate shades that elude traditional colour terminology. The
findings underscore the potential of this innovative approach in
revolutionizing our understanding of colour perception and language. Through
rigorous experimentation and analysis, this study illuminates a promising
avenue for Natural Language Processing (NLP) applications in diverse
industries. By facilitating the exploration of the vast colour spectrum the
potential applications of NLP are extended beyond conventional boundaries.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06551">FDNet: Feature Decoupled Segmentation Network for Tooth CBCT Image. (arXiv:2311.06551v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1">Xiang Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chengkai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1">Chengyu Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yunxiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yongbo He</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yaiqi Wang</a></p>
<p>Precise Tooth Cone Beam Computed Tomography (CBCT) image segmentation is
crucial for orthodontic treatment planning. In this paper, we propose FDNet, a
Feature Decoupled Segmentation Network, to excel in the face of the variable
dental conditions encountered in CBCT scans, such as complex artifacts and
indistinct tooth boundaries. The Low-Frequency Wavelet Transform (LF-Wavelet)
is employed to enrich the semantic content by emphasizing the global structural
integrity of the teeth, while the SAM encoder is leveraged to refine the
boundary delineation, thus improving the contrast between adjacent dental
structures. By integrating these dual aspects, FDNet adeptly addresses the
semantic gap, providing a detailed and accurate segmentation. The framework's
effectiveness is validated through rigorous benchmarks, achieving the top Dice
and IoU scores of 85.28% and 75.23%, respectively. This innovative decoupling
of semantic and boundary features capitalizes on the unique strengths of each
element to significantly elevate the quality of segmentation performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06552">Stain Consistency Learning: Handling Stain Variation for Automatic Digital Pathology Segmentation. (arXiv:2311.06552v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Yeung_M/0/1/0/all/0/1">Michael Yeung</a>, <a href="http://arxiv.org/find/eess/1/au:+Watts_T/0/1/0/all/0/1">Todd Watts</a>, <a href="http://arxiv.org/find/eess/1/au:+Tan_S/0/1/0/all/0/1">Sean YW Tan</a>, <a href="http://arxiv.org/find/eess/1/au:+Ferreira_P/0/1/0/all/0/1">Pedro F. Ferreira</a>, <a href="http://arxiv.org/find/eess/1/au:+Scott_A/0/1/0/all/0/1">Andrew D. Scott</a>, <a href="http://arxiv.org/find/eess/1/au:+Nielles_Vallespin_S/0/1/0/all/0/1">Sonia Nielles-Vallespin</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1">Guang Yang</a></p>
<p>Stain variation is a unique challenge associated with automated analysis of
digital pathology. Numerous methods have been developed to improve the
robustness of machine learning methods to stain variation, but comparative
studies have demonstrated limited benefits to performance. Moreover, methods to
handle stain variation were largely developed for H&amp;E stained data, with
evaluation generally limited to classification tasks. Here we propose Stain
Consistency Learning, a novel framework combining stain-specific augmentation
with a stain consistency loss function to learn stain colour invariant
features. We perform the first, extensive comparison of methods to handle stain
variation for segmentation tasks, comparing ten methods on Masson's trichrome
and H&amp;E stained cell and nuclei datasets, respectively. We observed that stain
normalisation methods resulted in equivalent or worse performance, while stain
augmentation or stain adversarial methods demonstrated improved performance,
with the best performance consistently achieved by our proposed approach. The
code is available at: https://github.com/mlyg/stain_consistency_learning
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06553">Visual Commonsense based Heterogeneous Graph Contrastive Learning. (arXiv:2311.06553v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zongzhao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiangyu Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhaoxiang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1">Zhen Lei</a></p>
<p>How to select relevant key objects and reason about the complex relationships
cross vision and linguistic domain are two key issues in many multi-modality
applications such as visual question answering (VQA). In this work, we
incorporate the visual commonsense information and propose a heterogeneous
graph contrastive learning method to better finish the visual reasoning task.
Our method is designed as a plug-and-play way, so that it can be quickly and
easily combined with a wide range of representative methods. Specifically, our
model contains two key components: the Commonsense-based Contrastive Learning
and the Graph Relation Network. Using contrastive learning, we guide the model
concentrate more on discriminative objects and relevant visual commonsense
attributes. Besides, thanks to the introduction of the Graph Relation Network,
the model reasons about the correlations between homogeneous edges and the
similarities between heterogeneous edges, which makes information transmission
more effective. Extensive experiments on four benchmarks show that our method
greatly improves seven representative VQA models, demonstrating its
effectiveness and generalizability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06557">Identification of vortex in unstructured mesh with graph neural networks. (arXiv:2311.06557v1 [physics.flu-dyn])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Wang_L/0/1/0/all/0/1">Lianfa Wang</a>, <a href="http://arxiv.org/find/physics/1/au:+Fournier_Y/0/1/0/all/0/1">Yvan Fournier</a>, <a href="http://arxiv.org/find/physics/1/au:+Wald_J/0/1/0/all/0/1">Jean-Francois Wald</a>, <a href="http://arxiv.org/find/physics/1/au:+Mesri_Y/0/1/0/all/0/1">Youssef Mesri</a></p>
<p>Deep learning has been employed to identify flow characteristics from
Computational Fluid Dynamics (CFD) databases to assist the researcher to better
understand the flow field, to optimize the geometry design and to select the
correct CFD configuration for corresponding flow characteristics. Convolutional
Neural Network (CNN) is one of the most popular algorithms used to extract and
identify flow features. However its use, without any additional flow field
interpolation, is limited to the simple domain geometry and regular meshes
which limits its application to real industrial cases where complex geometry
and irregular meshes are usually used. Aiming at the aforementioned problems,
we present a Graph Neural Network (GNN) based model with U-Net architecture to
identify the vortex in CFD results on unstructured meshes. The graph generation
and graph hierarchy construction using algebraic multigrid method from CFD
meshes are introduced. A vortex auto-labeling method is proposed to label
vortex regions in 2D CFD meshes. We precise our approach by firstly optimizing
the input set on CNNs, then benchmarking current GNN kernels against CNN model
and evaluating the performances of GNN kernels in terms of classification
accuracy, training efficiency and identified vortex morphology. Finally, we
demonstrate the adaptability of our approach to unstructured meshes and
generality to unseen cases with different turbulence models at different
Reynolds numbers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06567">SCADI: Self-supervised Causal Disentanglement in Latent Variable Models. (arXiv:2311.06567v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nam_H/0/1/0/all/0/1">Heejeong Nam</a></p>
<p>Causal disentanglement has great potential for capturing complex situations.
However, there is a lack of practical and efficient approaches. It is already
known that most unsupervised disentangling methods are unable to produce
identifiable results without additional information, often leading to randomly
disentangled output. Therefore, most existing models for disentangling are
weakly supervised, providing information about intrinsic factors, which incurs
excessive costs. Therefore, we propose a novel model, SCADI(SElf-supervised
CAusal DIsentanglement), that enables the model to discover semantic factors
and learn their causal relationships without any supervision. This model
combines a masked structural causal model (SCM) with a pseudo-label generator
for causal disentanglement, aiming to provide a new direction for
self-supervised causal disentanglement models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06570">OR Residual Connection Achieving Comparable Accuracy to ADD Residual Connection in Deep Residual Spiking Neural Networks. (arXiv:2311.06570v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1">Yimeng Shan</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1">Xuerui Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1">Rui-jie Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1">Ruike Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Meng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1">Haicheng Qu</a></p>
<p>Spiking Neural Networks (SNNs) have garnered substantial attention in
brain-like computing for their biological fidelity and the capacity to execute
energy-efficient spike-driven operations. As the demand for heightened
performance in SNNs surges, the trend towards training deeper networks becomes
imperative, while residual learning stands as a pivotal method for training
deep neural networks. In our investigation, we identified that the SEW-ResNet,
a prominent representative of deep residual spiking neural networks,
incorporates non-event-driven operations. To rectify this, we introduce the OR
Residual connection (ORRC) to the architecture. Additionally, we propose the
Synergistic Attention (SynA) module, an amalgamation of the Inhibitory
Attention (IA) module and the Multi-dimensional Attention (MA) module, to
offset energy loss stemming from high quantization. When integrating SynA into
the network, we observed the phenomenon of "natural pruning", where after
training, some or all of the shortcuts in the network naturally drop out
without affecting the model's classification accuracy. This significantly
reduces computational overhead and makes it more suitable for deployment on
edge devices. Experimental results on various public datasets confirmed that
the SynA enhanced OR-Spiking ResNet achieved single-sample classification with
as little as 0.8 spikes per neuron. Moreover, when compared to other spike
residual models, it exhibited higher accuracy and lower power consumption.
Codes are available at https://github.com/Ym-Shan/ORRC-SynA-natural-pruning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06572">Swin UNETR++: Advancing Transformer-Based Dense Dose Prediction Towards Fully Automated Radiation Oncology Treatments. (arXiv:2311.06572v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Wang_K/0/1/0/all/0/1">Kuancheng Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Tan_H/0/1/0/all/0/1">Hai Siong Tan</a>, <a href="http://arxiv.org/find/eess/1/au:+Mcbeth_R/0/1/0/all/0/1">Rafe Mcbeth</a></p>
<p>The field of Radiation Oncology is uniquely positioned to benefit from the
use of artificial intelligence to fully automate the creation of radiation
treatment plans for cancer therapy. This time-consuming and specialized task
combines patient imaging with organ and tumor segmentation to generate a 3D
radiation dose distribution to meet clinical treatment goals, similar to
voxel-level dense prediction. In this work, we propose Swin UNETR++, that
contains a lightweight 3D Dual Cross-Attention (DCA) module to capture the
intra and inter-volume relationships of each patient's unique anatomy, which
fully convolutional neural networks lack. Our model was trained, validated, and
tested on the Open Knowledge-Based Planning dataset. In addition to metrics of
Dose Score $\overline{S_{\text{Dose}}}$ and DVH Score
$\overline{S_{\text{DVH}}}$ that quantitatively measure the difference between
the predicted and ground-truth 3D radiation dose distribution, we propose the
qualitative metrics of average volume-wise acceptance rate
$\overline{R_{\text{VA}}}$ and average patient-wise clinical acceptance rate
$\overline{R_{\text{PA}}}$ to assess the clinical reliability of the
predictions. Swin UNETR++ demonstrates near-state-of-the-art performance on
validation and test dataset (validation: $\overline{S_{\text{DVH}}}$=1.492 Gy,
$\overline{S_{\text{Dose}}}$=2.649 Gy, $\overline{R_{\text{VA}}}$=88.58%,
$\overline{R_{\text{PA}}}$=100.0%; test: $\overline{S_{\text{DVH}}}$=1.634 Gy,
$\overline{S_{\text{Dose}}}$=2.757 Gy, $\overline{R_{\text{VA}}}$=90.50%,
$\overline{R_{\text{PA}}}$=98.0%), establishing a basis for future studies to
translate 3D dose predictions into a deliverable treatment plan, facilitating
full automation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06607">Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models. (arXiv:2311.06607v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1">Biao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qiang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1">Zhiyin Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shuo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jingxu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yabo Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuliang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1">Xiang Bai</a></p>
<p>Large Multimodal Models have demonstrated impressive capabilities in
understanding general vision-language tasks. However, due to the limitation of
supported input resolution (e.g., 448 x 448) as well as the inexhaustive
description of the training image-text pair, these models often encounter
challenges when dealing with intricate scene understandings and narratives.
Here we address the problem by proposing the Monkey. Our contributions are
two-fold: 1) without pretraining from the start, our method can be built upon
an existing vision encoder (e.g., vit-BigHuge) to effectively improve the input
resolution capacity up to 896 x 1344 pixels; 2) we propose a multi-level
description generation method, which automatically provides rich information
that can guide model to learn contextual association between scenes and
objects. Our extensive testing across more than 16 distinct datasets reveals
that Monkey achieves consistently competitive performance over the existing
LMMs on fundamental tasks, such as Image Captioning, General Visual Question
Answering (VQA), and Document-oriented VQA. Models, interactive demo, and the
source code are provided at the following
https://github.com/Yuliang-Liu/Monkey.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06612">PerceptionGPT: Effectively Fusing Visual Perception into LLM. (arXiv:2311.06612v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pi_R/0/1/0/all/0/1">Renjie Pi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1">Lewei Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jiahui Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jipeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tong Zhang</a></p>
<p>The integration of visual inputs with large language models (LLMs) has led to
remarkable advancements in multi-modal capabilities, giving rise to visual
large language models (VLLMs). However, effectively harnessing VLLMs for
intricate visual perception tasks remains a challenge. In this paper, we
present a novel end-to-end framework named PerceptionGPT, which efficiently and
effectively equips the VLLMs with visual perception abilities by leveraging the
representation power of LLMs' token embedding. Our proposed method treats the
token embedding of the LLM as the carrier of spatial information, then leverage
lightweight visual task encoders and decoders to perform visual perception
tasks (e.g., detection, segmentation). Our approach significantly alleviates
the training difficulty suffered by previous approaches that formulate the
visual outputs as discrete tokens, and enables achieving superior performance
with fewer trainable parameters, less training data and shorted training time.
Moreover, as only one token embedding is required to decode the visual outputs,
the resulting sequence length during inference is significantly reduced.
Consequently, our approach enables accurate and flexible representations,
seamless integration of visual perception tasks, and efficient handling of a
multiple of visual outputs. We validate the effectiveness and efficiency of our
approach through extensive experiments. The results demonstrate significant
improvements over previous methods with much fewer trainable parameters and GPU
hours, which facilitates future research in enabling LLMs with visual
perception abilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06613">Computer Vision for Particle Size Analysis of Coarse-Grained Soils. (arXiv:2311.06613v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Youwai_S/0/1/0/all/0/1">Sompote Youwai</a>, <a href="http://arxiv.org/find/cs/1/au:+Makam_P/0/1/0/all/0/1">Parchya Makam</a></p>
<p>Particle size analysis (PSA) is a fundamental technique for evaluating the
physical characteristics of soils. However, traditional methods like sieving
can be time-consuming and labor-intensive. In this study, we present a novel
approach that utilizes computer vision (CV) and the Python programming language
for PSA of coarse-grained soils, employing a standard mobile phone camera. By
eliminating the need for a high-performance camera, our method offers
convenience and cost savings. Our methodology involves using the OPENCV library
to detect and measure soil particles in digital photographs taken under
ordinary lighting conditions. For accurate particle size determination, a
calibration target with known dimensions is placed on a plain paper alongside
20 different sand samples. The proposed method is compared with traditional
sieve analysis and exhibits satisfactory performance for soil particles larger
than 2 mm, with a mean absolute percent error (MAPE) of approximately 6%.
However, particles smaller than 2 mm result in higher MAPE, reaching up to 60%.
To address this limitation, we recommend using a higher-resolution camera to
capture images of the smaller soil particles. Furthermore, we discuss the
advantages, limitations, and potential future improvements of our method.
Remarkably, the program can be executed on a mobile phone, providing immediate
results without the need to send soil samples to a laboratory. This
field-friendly feature makes our approach highly convenient for on-site usage,
outside of a traditional laboratory setting. Ultimately, this novel method
represents an initial disruption to the industry, enabling efficient particle
size analysis of soil without the reliance on laboratory-based sieve analysis.
KEYWORDS: Computer vision, Grain size, ARUCO
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06623">VT-Former: A Transformer-based Vehicle Trajectory Prediction Approach For Intelligent Highway Transportation Systems. (arXiv:2311.06623v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pazho_A/0/1/0/all/0/1">Armin Danesh Pazho</a>, <a href="http://arxiv.org/find/cs/1/au:+Katariya_V/0/1/0/all/0/1">Vinit Katariya</a>, <a href="http://arxiv.org/find/cs/1/au:+Noghre_G/0/1/0/all/0/1">Ghazal Alinezhad Noghre</a>, <a href="http://arxiv.org/find/cs/1/au:+Tabkhi_H/0/1/0/all/0/1">Hamed Tabkhi</a></p>
<p>Enhancing roadway safety and traffic management has become an essential focus
area for a broad range of modern cyber-physical systems and intelligent
transportation systems. Vehicle Trajectory Prediction is a pivotal element
within numerous applications for highway and road safety. These applications
encompass a wide range of use cases, spanning from traffic management and
accident prevention to enhancing work-zone safety and optimizing energy
conservation. The ability to implement intelligent management in this context
has been greatly advanced by the developments in the field of Artificial
Intelligence (AI), alongside the increasing deployment of surveillance cameras
across road networks. In this paper, we introduce a novel transformer-based
approach for vehicle trajectory prediction for highway safety and surveillance,
denoted as VT-Former. In addition to utilizing transformers to capture
long-range temporal patterns, a new Graph Attentive Tokenization (GAT) module
has been proposed to capture intricate social interactions among vehicles.
Combining these two core components culminates in a precise approach for
vehicle trajectory prediction. Our study on three benchmark datasets with three
different viewpoints demonstrates the State-of-The-Art (SoTA) performance of
VT-Former in vehicle trajectory prediction and its generalizability and
robustness. We also evaluate VT-Former's efficiency on embedded boards and
explore its potential for vehicle anomaly detection as a sample application,
showcasing its broad applicability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06631">A 3D Conditional Diffusion Model for Image Quality Transfer -- An Application to Low-Field MRI. (arXiv:2311.06631v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1">Seunghoi Kim</a>, <a href="http://arxiv.org/find/eess/1/au:+Tregidgo_H/0/1/0/all/0/1">Henry F. J. Tregidgo</a>, <a href="http://arxiv.org/find/eess/1/au:+Eldaly_A/0/1/0/all/0/1">Ahmed K. Eldaly</a>, <a href="http://arxiv.org/find/eess/1/au:+Figini_M/0/1/0/all/0/1">Matteo Figini</a>, <a href="http://arxiv.org/find/eess/1/au:+Alexander_D/0/1/0/all/0/1">Daniel C. Alexander</a></p>
<p>Low-field (LF) MRI scanners (&lt;1T) are still prevalent in settings with
limited resources or unreliable power supply. However, they often yield images
with lower spatial resolution and contrast than high-field (HF) scanners. This
quality disparity can result in inaccurate clinician interpretations. Image
Quality Transfer (IQT) has been developed to enhance the quality of images by
learning a mapping function between low and high-quality images. Existing IQT
models often fail to restore high-frequency features, leading to blurry output.
In this paper, we propose a 3D conditional diffusion model to improve 3D
volumetric data, specifically LF MR images. Additionally, we incorporate a
cross-batch mechanism into the self-attention and padding of our network,
ensuring broader contextual awareness even under small 3D patches. Experiments
on the publicly available Human Connectome Project (HCP) dataset for IQT and
brain parcellation demonstrate that our model outperforms existing methods both
quantitatively and qualitatively. The code is publicly available at
\url{https://github.com/edshkim98/DiffusionIQT}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06634">Back to Basics: Fast Denoising Iterative Algorithm. (arXiv:2311.06634v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Pereg_D/0/1/0/all/0/1">Deborah Pereg</a></p>
<p>We introduce Back to Basics (BTB), a fast iterative algorithm for noise
reduction. Our method is computationally efficient, does not require training
or ground truth data, and can be applied in the presence of independent noise,
as well as correlated (coherent) noise, where the noise level is unknown. We
examine three study cases: natural image denoising in the presence of additive
white Gaussian noise, Poisson-distributed image denoising, and speckle
suppression in optical coherence tomography (OCT). Experimental results
demonstrate that the proposed approach can effectively improve image quality,
in challenging noise settings. Theoretical guarantees are provided for
convergence stability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/1902.11122">Deep Learning in Cardiology. (arXiv:1902.11122v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bizopoulos_P/0/1/0/all/0/1">Paschalis Bizopoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Koutsouris_D/0/1/0/all/0/1">Dimitrios Koutsouris</a></p>
<p>The medical field is creating large amount of data that physicians are unable
to decipher and use efficiently. Moreover, rule-based expert systems are
inefficient in solving complicated medical tasks or for creating insights using
big data. Deep learning has emerged as a more accurate and effective technology
in a wide range of medical problems such as diagnosis, prediction and
intervention. Deep learning is a representation learning method that consists
of layers that transform the data non-linearly, thus, revealing hierarchical
relationships and structures. In this review we survey deep learning
application papers that use structured data, signal and imaging modalities from
cardiology. We discuss the advantages and limitations of applying deep learning
in cardiology that also apply in medicine in general, while proposing certain
directions as the most viable for clinical use.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/1904.13216">Signal2Image Modules in Deep Neural Networks for EEG Classification. (arXiv:1904.13216v9 [eess.SP] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Bizopoulos_P/0/1/0/all/0/1">Paschalis Bizopoulos</a>, <a href="http://arxiv.org/find/eess/1/au:+Lambrou_G/0/1/0/all/0/1">George I Lambrou</a>, <a href="http://arxiv.org/find/eess/1/au:+Koutsouris_D/0/1/0/all/0/1">Dimitrios Koutsouris</a></p>
<p>Deep learning has revolutionized computer vision utilizing the increased
availability of big data and the power of parallel computational units such as
graphical processing units. The vast majority of deep learning research is
conducted using images as training data, however the biomedical domain is rich
in physiological signals that are used for diagnosis and prediction problems.
It is still an open research question how to best utilize signals to train deep
neural networks.
</p>
<p>In this paper we define the term Signal2Image (S2Is) as trainable or
non-trainable prefix modules that convert signals, such as
Electroencephalography (EEG), to image-like representations making them
suitable for training image-based deep neural networks defined as `base
models'. We compare the accuracy and time performance of four S2Is (`signal as
image', spectrogram, one and two layer Convolutional Neural Networks (CNNs))
combined with a set of `base models' (LeNet, AlexNet, VGGnet, ResNet, DenseNet)
along with the depth-wise and 1D variations of the latter. We also provide
empirical evidence that the one layer CNN S2I performs better in eleven out of
fifteen tested models than non-trainable S2Is for classifying EEG signals and
we present visual comparisons of the outputs of the S2Is.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/1907.06592">Sparsely Activated Networks. (arXiv:1907.06592v10 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bizopoulos_P/0/1/0/all/0/1">Paschalis Bizopoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Koutsouris_D/0/1/0/all/0/1">Dimitrios Koutsouris</a></p>
<p>Previous literature on unsupervised learning focused on designing structural
priors with the aim of learning meaningful features. However, this was done
without considering the description length of the learned representations which
is a direct and unbiased measure of the model complexity. In this paper, first
we introduce the $\varphi$ metric that evaluates unsupervised models based on
their reconstruction accuracy and the degree of compression of their internal
representations. We then present and define two activation functions (Identity,
ReLU) as base of reference and three sparse activation functions (top-k
absolutes, Extrema-Pool indices, Extrema) as candidate structures that minimize
the previously defined $\varphi$. We lastly present Sparsely Activated Networks
(SANs) that consist of kernels with shared weights that, during encoding, are
convolved with the input and then passed through a sparse activation function.
During decoding, the same weights are convolved with the sparse activation map
and subsequently the partial reconstructions from each weight are summed to
reconstruct the input. We compare SANs using the five previously defined
activation functions on a variety of datasets (Physionet, UCI-epilepsy, MNIST,
FMNIST) and show that models that are selected using $\varphi$ have small
description representation length and consist of interpretable kernels.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/1912.00042">Learning Likelihoods with Conditional Normalizing Flows. (arXiv:1912.00042v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Winkler_C/0/1/0/all/0/1">Christina Winkler</a>, <a href="http://arxiv.org/find/cs/1/au:+Worrall_D/0/1/0/all/0/1">Daniel Worrall</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoogeboom_E/0/1/0/all/0/1">Emiel Hoogeboom</a>, <a href="http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1">Max Welling</a></p>
<p>Normalizing Flows (NFs) are able to model complicated distributions p(y) with
strong inter-dimensional correlations and high multimodality by transforming a
simple base density p(z) through an invertible neural network under the change
of variables formula. Such behavior is desirable in multivariate structured
prediction tasks, where handcrafted per-pixel loss-based methods inadequately
capture strong correlations between output dimensions. We present a study of
conditional normalizing flows (CNFs), a class of NFs where the base density to
output space mapping is conditioned on an input x, to model conditional
densities p(y|x). CNFs are efficient in sampling and inference, they can be
trained with a likelihood-based objective, and CNFs, being generative flows, do
not suffer from mode collapse or training instabilities. We provide an
effective method to train continuous CNFs for binary problems and in
particular, we apply these CNFs to super-resolution and vessel segmentation
tasks demonstrating competitive performance on standard benchmark datasets in
terms of likelihood and conventional metrics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2012.08026">Classification of Smoking and Calling using Deep Learning. (arXiv:2012.08026v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Miaowei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohacey_A/0/1/0/all/0/1">Alexander William Mohacey</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hongyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Apfel_J/0/1/0/all/0/1">James Apfel</a></p>
<p>Since 2014, very deep convolutional neural networks have been proposed and
become the must-have weapon for champions in all kinds of competition. In this
report, a pipeline is introduced to perform the classification of smoking and
calling by modifying the pretrained inception V3. Brightness enhancing based on
deep learning is implemented to improve the classification of this
classification task along with other useful training tricks. Based on the
quality and quantity results, it can be concluded that this pipeline with small
biased samples is practical and useful with high accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.05456">Monitoring and Adapting the Physical State of a Camera for Autonomous Vehicles. (arXiv:2112.05456v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wischow_M/0/1/0/all/0/1">Maik Wischow</a>, <a href="http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1">Guillermo Gallego</a>, <a href="http://arxiv.org/find/cs/1/au:+Ernst_I/0/1/0/all/0/1">Ines Ernst</a>, <a href="http://arxiv.org/find/cs/1/au:+Borner_A/0/1/0/all/0/1">Anko B&#xf6;rner</a></p>
<p>Autonomous vehicles and robots require increasingly more robustness and
reliability to meet the demands of modern tasks. These requirements specially
apply to cameras onboard such vehicles because they are the predominant sensors
to acquire information about the environment and support actions. Cameras must
maintain proper functionality and take automatic countermeasures if necessary.
Existing solutions are typically tailored to specific problems or detached from
the downstream computer vision tasks of the machines, which, however, determine
the requirements on the quality of the produced camera images. We propose a
generic and task-oriented self-health-maintenance framework for cameras based
on data- and physically-grounded models. To this end, we determine two
reliable, real-time capable estimators for typical image effects of a camera in
poor condition (blur, noise phenomena and most common combinations) by
evaluating traditional and customized machine learning-based approaches in
extensive experiments. Furthermore, we implement the framework on a real-world
ground vehicle and demonstrate how a camera can adjust its parameters to
counter an identified poor condition to achieve optimal application capability
based on experimental (non-linear and non-monotonic) input-output performance
curves. Object detection is chosen as target application, and the image effects
motion blur and sensor noise as conditioning examples. Our framework not only
provides a practical ready-to-use solution to monitor and maintain the health
of cameras, but can also serve as a basis for extensions to tackle more
sophisticated problems that combine additional data sources (e.g., sensor or
environment parameters) empirically in order to attain fully reliable and
robust machines. Code:
https://github.com/MaikWischow/Camera-Condition-Monitoring
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.03062">Object Scan Context: Object-centric Spatial Descriptor for Place Recognition within 3D Point Cloud Map. (arXiv:2206.03062v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1">Haodong Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yudong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1">Shengyin Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xue Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jian Wang</a></p>
<p>The integration of a SLAM algorithm with place recognition technology
empowers it with the ability to mitigate accumulated errors and to relocalize
itself. However, existing methods for point cloud-based place recognition
predominantly rely on the matching of descriptors, which are mostly
lidar-centric. These methods suffer from two major drawbacks: first, they
cannot perform place recognition when the distance between two point clouds is
significant, and second, they can only calculate the rotation angle without
considering the offset in the X and Y directions. To overcome these
limitations, we propose a novel local descriptor that is constructed around the
Main Object. By using a geometric method, we can accurately calculate the
relative pose. We have provided a theoretical analysis to demonstrate that this
method can overcome the aforementioned limitations. Furthermore, we conducted
extensive experiments on KITTI Odometry and KITTI360, which indicate that our
proposed method has significant advantages over state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.13086">RankSEG: A Consistent Ranking-based Framework for Segmentation. (arXiv:2206.13086v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Dai_B/0/1/0/all/0/1">Ben Dai</a>, <a href="http://arxiv.org/find/stat/1/au:+Li_C/0/1/0/all/0/1">Chunlin Li</a></p>
<p>Segmentation has emerged as a fundamental field of computer vision and
natural language processing, which assigns a label to every pixel/feature to
extract regions of interest from an image/text. To evaluate the performance of
segmentation, the Dice and IoU metrics are used to measure the degree of
overlap between the ground truth and the predicted segmentation. In this paper,
we establish a theoretical foundation of segmentation with respect to the
Dice/IoU metrics, including the Bayes rule and Dice-/IoU-calibration, analogous
to classification-calibration or Fisher consistency in classification. We prove
that the existing thresholding-based framework with most operating losses are
not consistent with respect to the Dice/IoU metrics, and thus may lead to a
suboptimal solution. To address this pitfall, we propose a novel consistent
ranking-based framework, namely RankDice/RankIoU, inspired by plug-in rules of
the Bayes segmentation rule. Three numerical algorithms with GPU parallel
execution are developed to implement the proposed framework in large-scale and
high-dimensional segmentation. We study statistical properties of the proposed
framework. We show it is Dice-/IoU-calibrated, and its excess risk bounds and
the rate of convergence are also provided. The numerical effectiveness of
RankDice/mRankDice is demonstrated in various simulated examples and
Fine-annotated CityScapes, Pascal VOC and Kvasir-SEG datasets with
state-of-the-art deep learning architectures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.11450">Hybrid Fusion Based Interpretable Multimodal Emotion Recognition with Limited Labelled Data. (arXiv:2208.11450v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1">Puneet Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Malik_S/0/1/0/all/0/1">Sarthak Malik</a>, <a href="http://arxiv.org/find/cs/1/au:+Raman_B/0/1/0/all/0/1">Balasubramanian Raman</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaobai Li</a></p>
<p>This paper proposes a multimodal emotion recognition system, VIsual Spoken
Textual Additive Net (VISTA Net), to classify emotions reflected by multimodal
input containing image, speech, and text into discrete classes. A new
interpretability technique, K-Average Additive exPlanation (KAAP), has also
been developed that identifies important visual, spoken, and textual features
leading to predicting a particular emotion class. The VISTA Net fuses
information from image, speech, and text modalities using a hybrid of early and
late fusion. It automatically adjusts the weights of their intermediate outputs
while computing the weighted average. The KAAP technique computes the
contribution of each modality and corresponding features toward predicting a
particular emotion class. To mitigate the insufficiency of multimodal emotion
datasets labeled with discrete emotion classes, we have constructed a
large-scale IIT-R MMEmoRec dataset consisting of images, corresponding speech
and text, and emotion labels ('angry,' 'happy,' 'hate,' and 'sad'). The VISTA
Net has resulted in 95.99\% emotion recognition accuracy on the IIT-R MMEmoRec
dataset on using visual, audio, and textual modalities, outperforming when
using any one or two modalities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.08355">Towards Connectivity-Aware Pulmonary Airway Segmentation. (arXiv:2209.08355v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Minghui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1">Guang-Zhong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1">Yun Gu</a></p>
<p>Detailed pulmonary airway segmentation is a clinically important task for
endobronchial intervention and treatment of peripheral located lung cancer
lesions. Convolutional Neural Networks (CNNs) are promising tools for medical
image analysis but have been performing poorly for cases when existing a
significant imbalanced feature distribution, which is true for the airway data
as the trachea and principal bronchi dominate most of the voxels whereas the
lobar bronchi and distal segmental bronchi occupy a small proportion. In this
paper, we propose a Differentiable Topology-Preserved Distance Transform
(DTPDT) framework to improve the performance of airway segmentation. A
Topology-Preserved Surrogate (TPS) learning strategy is first proposed to
balance the training progress within-class distribution. Furthermore, a
Convolutional Distance Transform (CDT) is designed to identify the breakage
phenomenon with superior sensitivity and minimize the variation of the distance
map between the predictionand ground-truth. The proposed method is validated
with the publically available reference airway segmentation datasets. The
detected rate of branch and length on public EXACT'09 and BAS datasets are
82.1%/79.6% and 96.5%/91.5% respectively, demonstrating the reliability and
efficiency of the method in terms of improving the topology completeness of the
segmentation performance while maintaining the overall topology accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.03461">FastCLIPstyler: Optimisation-free Text-based Image Style Transfer Using Style Representations. (arXiv:2210.03461v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Suresh_A/0/1/0/all/0/1">Ananda Padhmanabhan Suresh</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1">Sanjana Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Noinongyao_P/0/1/0/all/0/1">Pavit Noinongyao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganguly_A/0/1/0/all/0/1">Ankush Ganguly</a></p>
<p>In recent years, language-driven artistic style transfer has emerged as a new
type of style transfer technique, eliminating the need for a reference style
image by using natural language descriptions of the style. The first model to
achieve this, called CLIPstyler, has demonstrated impressive stylisation
results. However, its lengthy optimisation procedure at runtime for each query
limits its suitability for many practical applications. In this work, we
present FastCLIPstyler, a generalised text-based image style transfer model
capable of stylising images in a single forward pass for arbitrary text inputs.
Furthermore, we introduce EdgeCLIPstyler, a lightweight model designed for
compatibility with resource-constrained devices. Through quantitative and
qualitative comparisons with state-of-the-art approaches, we demonstrate that
our models achieve superior stylisation quality based on measurable metrics
while offering significantly improved runtime efficiency, particularly on edge
devices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.01368">Fast Non-Rigid Radiance Fields from Monocularized Data. (arXiv:2212.01368v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kappel_M/0/1/0/all/0/1">Moritz Kappel</a>, <a href="http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1">Vladislav Golyanik</a>, <a href="http://arxiv.org/find/cs/1/au:+Castillo_S/0/1/0/all/0/1">Susana Castillo</a>, <a href="http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1">Christian Theobalt</a>, <a href="http://arxiv.org/find/cs/1/au:+Magnor_M/0/1/0/all/0/1">Marcus Magnor</a></p>
<p>The reconstruction and novel view synthesis of dynamic scenes recently gained
increased attention. As reconstruction from large-scale multi-view data
involves immense memory and computational requirements, recent benchmark
datasets provide collections of single monocular views per timestamp sampled
from multiple (virtual) cameras. We refer to this form of inputs as
"monocularized" data. Existing work shows impressive results for synthetic
setups and forward-facing real-world data, but is often limited in the training
speed and angular range for generating novel views. This paper addresses these
limitations and proposes a new method for full 360{\deg} inward-facing novel
view synthesis of non-rigidly deforming scenes. At the core of our method are:
1) An efficient deformation module that decouples the processing of spatial and
temporal information for accelerated training and inference; and 2) A static
module representing the canonical scene as a fast hash-encoded neural radiance
field. In addition to existing synthetic monocularized data, we systematically
analyze the performance on real-world inward-facing scenes using a newly
recorded challenging dataset sampled from a synchronized large-scale multi-view
rig. In both cases, our method is significantly faster than previous methods,
converging in less than 7 minutes and achieving real-time framerates at 1K
resolution, while obtaining a higher visual accuracy for generated novel views.
Our source code and data is available at our project page
https://graphics.tu-bs.de/publications/kappel2022fast.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.04155">Latent Graph Representations for Critical View of Safety Assessment. (arXiv:2212.04155v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Murali_A/0/1/0/all/0/1">Aditya Murali</a>, <a href="http://arxiv.org/find/cs/1/au:+Alapatt_D/0/1/0/all/0/1">Deepak Alapatt</a>, <a href="http://arxiv.org/find/cs/1/au:+Mascagni_P/0/1/0/all/0/1">Pietro Mascagni</a>, <a href="http://arxiv.org/find/cs/1/au:+Vardazaryan_A/0/1/0/all/0/1">Armine Vardazaryan</a>, <a href="http://arxiv.org/find/cs/1/au:+Garcia_A/0/1/0/all/0/1">Alain Garcia</a>, <a href="http://arxiv.org/find/cs/1/au:+Okamoto_N/0/1/0/all/0/1">Nariaki Okamoto</a>, <a href="http://arxiv.org/find/cs/1/au:+Mutter_D/0/1/0/all/0/1">Didier Mutter</a>, <a href="http://arxiv.org/find/cs/1/au:+Padoy_N/0/1/0/all/0/1">Nicolas Padoy</a></p>
<p>Assessing the critical view of safety in laparoscopic cholecystectomy
requires accurate identification and localization of key anatomical structures,
reasoning about their geometric relationships to one another, and determining
the quality of their exposure. Prior works have approached this task by
including semantic segmentation as an intermediate step, using predicted
segmentation masks to then predict the CVS. While these methods are effective,
they rely on extremely expensive ground-truth segmentation annotations and tend
to fail when the predicted segmentation is incorrect, limiting generalization.
In this work, we propose a method for CVS prediction wherein we first represent
a surgical image using a disentangled latent scene graph, then process this
representation using a graph neural network. Our graph representations
explicitly encode semantic information - object location, class information,
geometric relations - to improve anatomy-driven reasoning, as well as visual
features to retain differentiability and thereby provide robustness to semantic
errors. Finally, to address annotation cost, we propose to train our method
using only bounding box annotations, incorporating an auxiliary image
reconstruction objective to learn fine-grained object boundaries. We show that
our method not only outperforms several baseline methods when trained with
bounding box annotations, but also scales effectively when trained with
segmentation masks, maintaining state-of-the-art performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.04673">MSI: Maximize Support-Set Information for Few-Shot Segmentation. (arXiv:2212.04673v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1">Seonghyeon Moon</a>, <a href="http://arxiv.org/find/cs/1/au:+Sohn_S/0/1/0/all/0/1">Samuel S. Sohn</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Honglu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1">Sejong Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Pavlovic_V/0/1/0/all/0/1">Vladimir Pavlovic</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1">Muhammad Haris Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kapadia_M/0/1/0/all/0/1">Mubbasir Kapadia</a></p>
<p>FSS(Few-shot segmentation) aims to segment a target class using a small
number of labeled images(support set). To extract information relevant to the
target class, a dominant approach in best-performing FSS methods removes
background features using a support mask. We observe that this feature excision
through a limiting support mask introduces an information bottleneck in several
challenging FSS cases, e.g., for small targets and/or inaccurate target
boundaries. To this end, we present a novel method(MSI), which maximizes the
support-set information by exploiting two complementary sources of features to
generate super correlation maps. We validate the effectiveness of our approach
by instantiating it into three recent and strong FSS methods. Experimental
results on several publicly available FSS benchmarks show that our proposed
method consistently improves performance by visible margins and leads to faster
convergence. Our code and trained models are available at:
https://github.com/moonsh/MSI-Maximize-Support-Set-Information
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.11146">The Challenges of HTR Model Training: Feedback from the Project Donner le gout de l&#x27;archive a l&#x27;ere numerique. (arXiv:2212.11146v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Couture_B/0/1/0/all/0/1">Beatrice Couture</a> (Universit&#xe9; de Montr&#xe9;al), <a href="http://arxiv.org/find/cs/1/au:+Verret_F/0/1/0/all/0/1">Farah Verret</a> (Universit&#xe9; de Montr&#xe9;al), <a href="http://arxiv.org/find/cs/1/au:+Gohier_M/0/1/0/all/0/1">Maxime Gohier</a> (Universit&#xe9; du Qu&#xe9;bec &#xe0; Rimouski), <a href="http://arxiv.org/find/cs/1/au:+Deslandres_D/0/1/0/all/0/1">Dominique Deslandres</a> (Universit&#xe9; de Montr&#xe9;al)</p>
<p>The arrival of handwriting recognition technologies offers new possibilities
for research in heritage studies. However, it is now necessary to reflect on
the experiences and the practices developed by research teams. Our use of the
Transkribus platform since 2018 has led us to search for the most significant
ways to improve the performance of our handwritten text recognition (HTR)
models which are made to transcribe French handwriting dating from the 17th
century. This article therefore reports on the impacts of creating transcribing
protocols, using the language model at full scale and determining the best way
to use base models in order to help increase the performance of HTR models.
Combining all of these elements can indeed increase the performance of a single
model by more than 20% (reaching a Character Error Rate below 5%). This article
also discusses some challenges regarding the collaborative nature of HTR
platforms such as Transkribus and the way researchers can share their data
generated in the process of creating or training handwritten text recognition
models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.05206">ImMesh: An Immediate LiDAR Localization and Meshing Framework. (arXiv:2301.05206v3 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Jiarong Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1">Chongjiang Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1">Yixi Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Haotian Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1">Yunfan Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1">Yuying Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_X/0/1/0/all/0/1">Xiaoping Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1">Fu Zhang</a></p>
<p>In this paper, we propose a novel LiDAR(-inertial) odometry and mapping
framework to achieve the goal of simultaneous localization and meshing in
real-time. This proposed framework termed ImMesh comprises four tightly-coupled
modules: receiver, localization, meshing, and broadcaster. The localization
module utilizes the prepossessed sensor data from the receiver, estimates the
sensor pose online by registering LiDAR scans to maps, and dynamically grows
the map. Then, our meshing module takes the registered LiDAR scan for
incrementally reconstructing the triangle mesh on the fly. Finally, the
real-time odometry, map, and mesh are published via our broadcaster. The key
contribution of this work is the meshing module, which represents a scene by an
efficient hierarchical voxels structure, performs fast finding of voxels
observed by new scans, and reconstructs triangle facets in each voxel in an
incremental manner. This voxel-wise meshing operation is delicately designed
for the purpose of efficiency; it first performs a dimension reduction by
projecting 3D points to a 2D local plane contained in the voxel, and then
executes the meshing operation with pull, commit and push steps for incremental
reconstruction of triangle facets. To the best of our knowledge, this is the
first work in literature that can reconstruct online the triangle mesh of
large-scale scenes, just relying on a standard CPU without GPU acceleration. To
share our findings and make contributions to the community, we make our code
publicly available on our GitHub: https://github.com/hku-mars/ImMesh.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.00290">MS-DETR: Multispectral Pedestrian Detection Transformer with Loosely Coupled Fusion and Modality-Balanced Optimization. (arXiv:2302.00290v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xing_Y/0/1/0/all/0/1">Yinghui Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Song Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shizhou Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_G/0/1/0/all/0/1">Guoqiang Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiuwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yanning Zhang</a></p>
<p>Multispectral pedestrian detection is an important task for many
around-the-clock applications, since the visible and thermal modalities can
provide complementary information especially under low light conditions. Most
of the available multispectral pedestrian detectors are based on non-end-to-end
detectors, while in this paper, we propose MultiSpectral pedestrian DEtection
TRansformer (MS-DETR), an end-to-end multispectral pedestrian detector, which
extends DETR into the field of multi-modal detection. MS-DETR consists of two
modality-specific backbones and Transformer encoders, followed by a multi-modal
Transformer decoder, and the visible and thermal features are fused in the
multi-modal Transformer decoder. To well resist the misalignment between
multi-modal images, we design a loosely coupled fusion strategy by sparsely
sampling some keypoints from multi-modal features independently and fusing them
with adaptively learned attention weights. Moreover, based on the insight that
not only different modalities, but also different pedestrian instances tend to
have different confidence scores to final detection, we further propose an
instance-aware modality-balanced optimization strategy, which preserves visible
and thermal decoder branches and aligns their predicted slots through an
instance-wise dynamic loss. Our end-to-end MS-DETR shows superior performance
on the challenging KAIST, CVC-14 and LLVIP benchmark datasets. The source code
is available at https://github.com/YinghuiXing/MS-DETR .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.01602">A Feature Selection Method for Driver Stress Detection Using Heart Rate Variability and Breathing Rate. (arXiv:2302.01602v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Parsi_A/0/1/0/all/0/1">Ashkan Parsi</a>, <a href="http://arxiv.org/find/cs/1/au:+OCallaghan_D/0/1/0/all/0/1">David O&#x27;Callaghan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lemley_J/0/1/0/all/0/1">Joseph Lemley</a></p>
<p>Driver stress is a major cause of car accidents and death worldwide.
Furthermore, persistent stress is a health problem, contributing to
hypertension and other diseases of the cardiovascular system. Stress has a
measurable impact on heart and breathing rates and stress levels can be
inferred from such measurements. Galvanic skin response is a common test to
measure the perspiration caused by both physiological and psychological stress,
as well as extreme emotions. In this paper, galvanic skin response is used to
estimate the ground truth stress levels. A feature selection technique based on
the minimal redundancy-maximal relevance method is then applied to multiple
heart rate variability and breathing rate metrics to identify a novel and
optimal combination for use in detecting stress. The support vector machine
algorithm with a radial basis function kernel was used along with these
features to reliably predict stress. The proposed method has achieved a high
level of accuracy on the target dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.11728">A Convolutional-Transformer Network for Crack Segmentation with Boundary Awareness. (arXiv:2302.11728v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tao_H/0/1/0/all/0/1">Huaqi Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bingxi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1">Jinqiang Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hong Zhang</a></p>
<p>Cracks play a crucial role in assessing the safety and durability of
manufactured buildings. However, the long and sharp topological features and
complex background of cracks make the task of crack segmentation extremely
challenging. In this paper, we propose a novel convolutional-transformer
network based on encoder-decoder architecture to solve this challenge.
Particularly, we designed a Dilated Residual Block (DRB) and a Boundary
Awareness Module (BAM). The DRB pays attention to the local detail of cracks
and adjusts the feature dimension for other blocks as needed. And the BAM
learns the boundary features from the dilated crack label. Furthermore, the DRB
is combined with a lightweight transformer that captures global information to
serve as an effective encoder. Experimental results show that the proposed
network performs better than state-of-the-art algorithms on two typical
datasets. Datasets, code, and trained models are available for research at
https://github.com/HqiTao/CT-crackseg.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.07271">Provably Convergent Plug-and-Play Quasi-Newton Methods. (arXiv:2303.07271v4 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Tan_H/0/1/0/all/0/1">Hong Ye Tan</a>, <a href="http://arxiv.org/find/math/1/au:+Mukherjee_S/0/1/0/all/0/1">Subhadip Mukherjee</a>, <a href="http://arxiv.org/find/math/1/au:+Tang_J/0/1/0/all/0/1">Junqi Tang</a>, <a href="http://arxiv.org/find/math/1/au:+Schonlieb_C/0/1/0/all/0/1">Carola-Bibiane Sch&#xf6;nlieb</a></p>
<p>Plug-and-Play (PnP) methods are a class of efficient iterative methods that
aim to combine data fidelity terms and deep denoisers using classical
optimization algorithms, such as ISTA or ADMM, with applications in inverse
problems and imaging. Provable PnP methods are a subclass of PnP methods with
convergence guarantees, such as fixed point convergence or convergence to
critical points of some energy function. Many existing provable PnP methods
impose heavy restrictions on the denoiser or fidelity function, such as
non-expansiveness or strict convexity, respectively. In this work, we propose a
novel algorithmic approach incorporating quasi-Newton steps into a provable PnP
framework based on proximal denoisers, resulting in greatly accelerated
convergence while retaining light assumptions on the denoiser. By
characterizing the denoiser as the proximal operator of a weakly convex
function, we show that the fixed points of the proposed quasi-Newton PnP
algorithm are critical points of a weakly convex function. Numerical
experiments on image deblurring and super-resolution demonstrate 2--8x faster
convergence as compared to other provable PnP methods with similar
reconstruction quality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.09447">Steering Prototypes with Prompt-tuning for Rehearsal-free Continual Learning. (arXiv:2303.09447v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhuowei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1">Long Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zizhao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Han Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1">Di Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Ting Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1">Dimitris N. Metaxas</a></p>
<p>In the context of continual learning, prototypes-as representative class
embeddings-offer advantages in memory conservation and the mitigation of
catastrophic forgetting. However, challenges related to semantic drift and
prototype interference persist. In this study, we introduce the Contrastive
Prototypical Prompt (CPP) approach. Through task-specific prompt-tuning,
underpinned by a contrastive learning objective, we effectively address both
aforementioned challenges. Our evaluations on four challenging
class-incremental benchmarks reveal that CPP achieves a significant 4% to 6%
improvement over state-of-the-art methods. Importantly, CPP operates without a
rehearsal buffer and narrows the performance divergence between continual and
offline joint-learning, suggesting an innovative scheme for Transformer-based
continual learning systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.00466">Learning Robust Medical Image Segmentation from Multi-source Annotations. (arXiv:2304.00466v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1">Yifeng Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Luo_L/0/1/0/all/0/1">Luyang Luo</a>, <a href="http://arxiv.org/find/eess/1/au:+Wu_M/0/1/0/all/0/1">Mingxiang Wu</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1">Qiong Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1">Hao Chen</a></p>
<p>Collecting annotations from multiple independent sources could mitigate the
impact of potential noises and biases from a single source, which is a common
practice in medical image segmentation. Learning segmentation networks from
multi-source annotations remains a challenge due to the uncertainties brought
by the variance of annotations and the quality of images. In this paper, we
propose an Uncertainty-guided Multi-source Annotation Network (UMA-Net), which
guides the training process by uncertainty estimation at both the pixel and the
image levels. First, we developed the annotation uncertainty estimation module
(AUEM) to learn the pixel-wise uncertainty of each annotation, which then
guided the network to learn from reliable pixels by weighted segmentation loss.
Second, a quality assessment module (QAM) was proposed to assess the
image-level quality of the input samples based on the former assessed
annotation uncertainties. Importantly, we introduced an auxiliary predictor to
learn from the low-quality samples instead of discarding them, which ensured
the preservation of their representation knowledge in the backbone without
directly accumulating errors within the primary predictor. Extensive
experiments demonstrated the effectiveness and feasibility of our proposed
UMA-Net on various datasets, including 2D chest X-ray segmentation, fundus
image segmentation, and 3D breast DCE-MRI segmentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.02725">FMG-Net and W-Net: Multigrid Inspired Deep Learning Architectures For Medical Imaging Segmentation. (arXiv:2304.02725v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Celaya_A/0/1/0/all/0/1">Adrian Celaya</a>, <a href="http://arxiv.org/find/eess/1/au:+Riviere_B/0/1/0/all/0/1">Beatrice Riviere</a>, <a href="http://arxiv.org/find/eess/1/au:+Fuentes_D/0/1/0/all/0/1">David Fuentes</a></p>
<p>Accurate medical imaging segmentation is critical for precise and effective
medical interventions. However, despite the success of convolutional neural
networks (CNNs) in medical image segmentation, they still face challenges in
handling fine-scale features and variations in image scales. These challenges
are particularly evident in complex and challenging segmentation tasks, such as
the BraTS multi-label brain tumor segmentation challenge. In this task,
accurately segmenting the various tumor sub-components, which vary
significantly in size and shape, remains a significant challenge, with even
state-of-the-art methods producing substantial errors. Therefore, we propose
two architectures, FMG-Net and W-Net, that incorporate the principles of
geometric multigrid methods for solving linear systems of equations into CNNs
to address these challenges. Our experiments on the BraTS 2020 dataset
demonstrate that both FMG-Net and W-Net outperform the widely used U-Net
architecture regarding tumor subcomponent segmentation accuracy and training
efficiency. These findings highlight the potential of incorporating the
principles of multigrid methods into CNNs to improve the accuracy and
efficiency of medical imaging segmentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.02848">Patch-aware Batch Normalization for Improving Cross-domain Robustness. (arXiv:2304.02848v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1">Lei Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1">Dongjia Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yinghuan Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1">Xin Geng</a></p>
<p>Despite the significant success of deep learning in computer vision tasks,
cross-domain tasks still present a challenge in which the model's performance
will degrade when the training set and the test set follow different
distributions. Most existing methods employ adversarial learning or instance
normalization for achieving data augmentation to solve this task. In contrast,
considering that the batch normalization (BN) layer may not be robust for
unseen domains and there exist the differences between local patches of an
image, we propose a novel method called patch-aware batch normalization (PBN).
To be specific, we first split feature maps of a batch into non-overlapping
patches along the spatial dimension, and then independently normalize each
patch to jointly optimize the shared BN parameter at each iteration. By
exploiting the differences between local patches of an image, our proposed PBN
can effectively enhance the robustness of the model's parameters. Besides,
considering the statistics from each patch may be inaccurate due to their
smaller size compared to the global feature maps, we incorporate the globally
accumulated statistics with the statistics from each batch to obtain the final
statistics for normalizing each patch. Since the proposed PBN can replace the
typical BN, it can be integrated into most existing state-of-the-art methods.
Extensive experiments and analysis demonstrate the effectiveness of our PBN in
multiple computer vision tasks, including classification, object detection,
instance retrieval, and semantic segmentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.08946">Image Matching by Bare Homography. (arXiv:2305.08946v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bellavia_F/0/1/0/all/0/1">Fabio Bellavia</a></p>
<p>This paper presents Slime, a novel non-deep image matching framework which
models the scene as rough local overlapping planes. This intermediate
representation sits in-between the local affine approximation of the keypoint
patches and the global matching based on both spatial and similarity
constraints, providing a progressive pruning of the correspondences, as planes
are easier to handle with respect to general scenes.
</p>
<p>Slime decomposes the images into overlapping regions at different scales and
computes loose planar homographies. Planes are mutually extended by compatible
matches and the images are split into fixed tiles, with only the best
homographies retained for each pair of tiles. Stable matches are identified
according to the consensus of the admissible stereo configurations provided by
pairwise homographies. Within tiles, the rough planes are then merged according
to their overlap in terms of matches and further consistent correspondences are
extracted.
</p>
<p>The whole process only involves homography constraints. As a result, both the
coverage and the stability of correct matches over the scene are amplified,
together with the ability to spot matches in challenging scenes, allowing
traditional hybrid matching pipelines to make up lost ground against recent
end-to-end deep matching methods.
</p>
<p>In addition, the paper gives a thorough comparative analysis of recent
state-of-the-art in image matching represented by end-to-end deep networks and
hybrid pipelines. The evaluation considers both planar and non-planar scenes,
taking into account critical and challenging scenarios including abrupt
temporal image changes and strong variations in relative image rotations.
According to this analysis, although the impressive progress done in this
field, there is still a wide room for improvements to be investigated in future
research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14053">Parts of Speech-Grounded Subspaces in Vision-Language Models. (arXiv:2305.14053v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oldfield_J/0/1/0/all/0/1">James Oldfield</a>, <a href="http://arxiv.org/find/cs/1/au:+Tzelepis_C/0/1/0/all/0/1">Christos Tzelepis</a>, <a href="http://arxiv.org/find/cs/1/au:+Panagakis_Y/0/1/0/all/0/1">Yannis Panagakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Nicolaou_M/0/1/0/all/0/1">Mihalis A. Nicolaou</a>, <a href="http://arxiv.org/find/cs/1/au:+Patras_I/0/1/0/all/0/1">Ioannis Patras</a></p>
<p>Latent image representations arising from vision-language models have proved
immensely useful for a variety of downstream tasks. However, their utility is
limited by their entanglement with respect to different visual attributes. For
instance, recent work has shown that CLIP image representations are often
biased toward specific visual properties (such as objects or actions) in an
unpredictable manner. In this paper, we propose to separate representations of
the different visual modalities in CLIP's joint vision-language space by
leveraging the association between parts of speech and specific visual modes of
variation (e.g. nouns relate to objects, adjectives describe appearance). This
is achieved by formulating an appropriate component analysis model that learns
subspaces capturing variability corresponding to a specific part of speech,
while jointly minimising variability to the rest. Such a subspace yields
disentangled representations of the different visual properties of an image or
text in closed form while respecting the underlying geometry of the manifold on
which the representations lie. What's more, we show the proposed model
additionally facilitates learning subspaces corresponding to specific visual
appearances (e.g. artists' painting styles), which enables the selective
removal of entire visual themes from CLIP-based text-to-image synthesis. We
validate the model both qualitatively, by visualising the subspace projections
with a text-to-image model and by preventing the imitation of artists' styles,
and quantitatively, through class invariance metrics and improvements to
baseline zero-shot classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14575">Towards Early Prediction of Human iPSC Reprogramming Success. (arXiv:2305.14575v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1">Abhineet Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Jasra_I/0/1/0/all/0/1">Ila Jasra</a>, <a href="http://arxiv.org/find/cs/1/au:+Mouhammed_O/0/1/0/all/0/1">Omar Mouhammed</a>, <a href="http://arxiv.org/find/cs/1/au:+Dadheech_N/0/1/0/all/0/1">Nidheesh Dadheech</a>, <a href="http://arxiv.org/find/cs/1/au:+Ray_N/0/1/0/all/0/1">Nilanjan Ray</a>, <a href="http://arxiv.org/find/cs/1/au:+Shapiro_J/0/1/0/all/0/1">James Shapiro</a></p>
<p>This paper presents advancements in automated early-stage prediction of the
success of reprogramming human induced pluripotent stem cells (iPSCs) as a
potential source for regenerative cell therapies.The minuscule success rate of
iPSC-reprogramming of around $ 0.01% $ to $ 0.1% $ makes it labor-intensive,
time-consuming, and exorbitantly expensive to generate a stable iPSC line.
Since that requires culturing of millions of cells and intense biological
scrutiny of multiple clones to identify a single optimal clone. The ability to
reliably predict which cells are likely to establish as an optimal iPSC line at
an early stage of pluripotency would therefore be ground-breaking in rendering
this a practical and cost-effective approach to personalized medicine. Temporal
information about changes in cellular appearance over time is crucial for
predicting its future growth outcomes. In order to generate this data, we first
performed continuous time-lapse imaging of iPSCs in culture using an ultra-high
resolution microscope. We then annotated the locations and identities of cells
in late-stage images where reliable manual identification is possible. Next, we
propagated these labels backwards in time using a semi-automated tracking
system to obtain labels for early stages of growth. Finally, we used this data
to train deep neural networks to perform automatic cell segmentation and
classification. Our code and data are available at
https://github.com/abhineet123/ipsc_prediction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16963">Semantic segmentation of sparse irregular point clouds for leaf/wood discrimination. (arXiv:2305.16963v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1">Yuchen Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Durand_J/0/1/0/all/0/1">Jean-Baptiste Durand</a>, <a href="http://arxiv.org/find/cs/1/au:+Forbes_F/0/1/0/all/0/1">Florence Forbes</a>, <a href="http://arxiv.org/find/cs/1/au:+Vincent_G/0/1/0/all/0/1">Gr&#xe9;goire Vincent</a></p>
<p>LiDAR (Light Detection and Ranging) has become an essential part of the
remote sensing toolbox used for biosphere monitoring. In particular, LiDAR
provides the opportunity to map forest leaf area with unprecedented accuracy,
while leaf area has remained an important source of uncertainty affecting
models of gas exchanges between the vegetation and the atmosphere. Unmanned
Aerial Vehicles (UAV) are easy to mobilize and therefore allow frequent
revisits to track the response of vegetation to climate change. However,
miniature sensors embarked on UAVs usually provide point clouds of limited
density, which are further affected by a strong decrease in density from top to
bottom of the canopy due to progressively stronger occlusion. In such a
context, discriminating leaf points from wood points presents a significant
challenge due in particular to strong class imbalance and spatially irregular
sampling intensity. Here we introduce a neural network model based on the
Pointnet ++ architecture which makes use of point geometry only (excluding any
spectral information). To cope with local data sparsity, we propose an
innovative sampling scheme which strives to preserve local important geometric
information. We also propose a loss function adapted to the severe class
imbalance. We show that our model outperforms state-of-the-art alternatives on
UAV point clouds. We discuss future possible improvements, particularly
regarding much denser point clouds acquired from below the canopy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.17600">NashFormer: Leveraging Local Nash Equilibria for Semantically Diverse Trajectory Prediction. (arXiv:2305.17600v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lidard_J/0/1/0/all/0/1">Justin Lidard</a>, <a href="http://arxiv.org/find/cs/1/au:+So_O/0/1/0/all/0/1">Oswin So</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yanxia Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+DeCastro_J/0/1/0/all/0/1">Jonathan DeCastro</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1">Xiongyi Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuo_Y/0/1/0/all/0/1">Yen-Ling Kuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Leonard_J/0/1/0/all/0/1">John Leonard</a>, <a href="http://arxiv.org/find/cs/1/au:+Balachandran_A/0/1/0/all/0/1">Avinash Balachandran</a>, <a href="http://arxiv.org/find/cs/1/au:+Leonard_N/0/1/0/all/0/1">Naomi Leonard</a>, <a href="http://arxiv.org/find/cs/1/au:+Rosman_G/0/1/0/all/0/1">Guy Rosman</a></p>
<p>Interactions between road agents present a significant challenge in
trajectory prediction, especially in cases involving multiple agents. Because
existing diversity-aware predictors do not account for the interactive nature
of multi-agent predictions, they may miss these important interaction outcomes.
In this paper, we propose NashFormer, a framework for trajectory prediction
that leverages game-theoretic inverse reinforcement learning to improve
coverage of multi-modal predictions. We use a training-time game-theoretic
analysis as an auxiliary loss resulting in improved coverage and accuracy
without presuming a taxonomy of actions for the agents. We demonstrate our
approach on the interactive split of the Waymo Open Motion Dataset, including
four subsets involving scenarios with high interaction complexity. Experiment
results show that our predictor produces accurate predictions while covering
$33\%$ more potential interactions versus a baseline model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.18259">GlyphControl: Glyph Conditional Control for Visual Text Generation. (arXiv:2305.18259v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yukang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gui_D/0/1/0/all/0/1">Dongnan Gui</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1">Yuhui Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1">Weicong Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1">Haisong Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1">Han Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Kai Chen</a></p>
<p>Recently, there has been an increasing interest in developing diffusion-based
text-to-image generative models capable of generating coherent and well-formed
visual text. In this paper, we propose a novel and efficient approach called
GlyphControl to address this task. Unlike existing methods that rely on
character-aware text encoders like ByT5 and require retraining of text-to-image
models, our approach leverages additional glyph conditional information to
enhance the performance of the off-the-shelf Stable-Diffusion model in
generating accurate visual text. By incorporating glyph instructions, users can
customize the content, location, and size of the generated text according to
their specific requirements. To facilitate further research in visual text
generation, we construct a training benchmark dataset called LAION-Glyph. We
evaluate the effectiveness of our approach by measuring OCR-based metrics, CLIP
score, and FID of the generated visual text. Our empirical evaluations
demonstrate that GlyphControl outperforms the recent DeepFloyd IF approach in
terms of OCR accuracy, CLIP score, and FID, highlighting the efficacy of our
method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.18414">StEik: Stabilizing the Optimization of Neural Signed Distance Functions and Finer Shape Representation. (arXiv:2305.18414v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Huizong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yuxin Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Sundaramoorthi_G/0/1/0/all/0/1">Ganesh Sundaramoorthi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yezzi_A/0/1/0/all/0/1">Anthony Yezzi</a></p>
<p>We present new insights and a novel paradigm (StEik) for learning implicit
neural representations (INR) of shapes. In particular, we shed light on the
popular eikonal loss used for imposing a signed distance function constraint in
INR. We show analytically that as the representation power of the network
increases, the optimization approaches a partial differential equation (PDE) in
the continuum limit that is unstable. We show that this instability can
manifest in existing network optimization, leading to irregularities in the
reconstructed surface and/or convergence to sub-optimal local minima, and thus
fails to capture fine geometric and topological structure. We show analytically
how other terms added to the loss, currently used in the literature for other
purposes, can actually eliminate these instabilities. However, such terms can
over-regularize the surface, preventing the representation of fine shape
detail. Based on a similar PDE theory for the continuum limit, we introduce a
new regularization term that still counteracts the eikonal instability but
without over-regularizing. Furthermore, since stability is now guaranteed in
the continuum limit, this stabilization also allows for considering new network
structures that are able to represent finer shape detail. We introduce such a
structure based on quadratic layers. Experiments on multiple benchmark data
sets show that our new regularization and network are able to capture more
precise shape details and more accurate topology than existing
state-of-the-art.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.02500">Systematic Visual Reasoning through Object-Centric Relational Abstraction. (arXiv:2306.02500v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Webb_T/0/1/0/all/0/1">Taylor W. Webb</a>, <a href="http://arxiv.org/find/cs/1/au:+Mondal_S/0/1/0/all/0/1">Shanka Subhra Mondal</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_J/0/1/0/all/0/1">Jonathan D. Cohen</a></p>
<p>Human visual reasoning is characterized by an ability to identify abstract
patterns from only a small number of examples, and to systematically generalize
those patterns to novel inputs. This capacity depends in large part on our
ability to represent complex visual inputs in terms of both objects and
relations. Recent work in computer vision has introduced models with the
capacity to extract object-centric representations, leading to the ability to
process multi-object visual inputs, but falling short of the systematic
generalization displayed by human reasoning. Other recent models have employed
inductive biases for relational abstraction to achieve systematic
generalization of learned abstract rules, but have generally assumed the
presence of object-focused inputs. Here, we combine these two approaches,
introducing Object-Centric Relational Abstraction (OCRA), a model that extracts
explicit representations of both objects and abstract relations, and achieves
strong systematic generalization in tasks (including a novel dataset,
CLEVR-ART, with greater visual complexity) involving complex visual displays.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.06874">VillanDiffusion: A Unified Backdoor Attack Framework for Diffusion Models. (arXiv:2306.06874v4 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chou_S/0/1/0/all/0/1">Sheng-Yen Chou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1">Pin-Yu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ho_T/0/1/0/all/0/1">Tsung-Yi Ho</a></p>
<p>Diffusion Models (DMs) are state-of-the-art generative models that learn a
reversible corruption process from iterative noise addition and denoising. They
are the backbone of many generative AI applications, such as text-to-image
conditional generation. However, recent studies have shown that basic
unconditional DMs (e.g., DDPM and DDIM) are vulnerable to backdoor injection, a
type of output manipulation attack triggered by a maliciously embedded pattern
at model input. This paper presents a unified backdoor attack framework
(VillanDiffusion) to expand the current scope of backdoor analysis for DMs. Our
framework covers mainstream unconditional and conditional DMs (denoising-based
and score-based) and various training-free samplers for holistic evaluations.
Experiments show that our unified framework facilitates the backdoor analysis
of different DM configurations and provides new insights into caption-based
backdoor attacks on DMs. Our code is available on GitHub:
\url{https://github.com/IBM/villandiffusion}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.08233">OT-Net: A Reusable Neural Optimal Transport Solver. (arXiv:2306.08233v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zezeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shenghao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1">Lianbao Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_N/0/1/0/all/0/1">Na Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1">Zhongxuan Luo</a></p>
<p>With the widespread application of optimal transport (OT), its calculation
becomes essential, and various algorithms have emerged. However, the existing
methods either have low efficiency or cannot represent discontinuous maps. A
novel reusable neural OT solver OT-Net is thus presented, which first learns
Brenier's height representation via the neural network to obtain its potential,
and then gained the OT map by computing the gradient of the potential. The
algorithm has two merits, 1) it can easily represent discontinuous maps, which
allows it to match any target distribution with discontinuous supports and
achieve sharp boundaries. This can well eliminate mode collapse in the
generated models. 2) The OT map can be calculated straightly by the proposed
algorithm when new target samples are added, which greatly improves the
efficiency and reusability of the map. Moreover, the theoretical error bound of
the algorithm is analyzed, and we have demonstrated the empirical success of
our approach in image generation, color transfer, and domain adaptation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.09338">Understanding Optimization of Deep Learning via Jacobian Matrix and Lipschitz Constant. (arXiv:2306.09338v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1">Xianbiao Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jianan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lei Zhang</a></p>
<p>This article provides a comprehensive understanding of optimization in deep
learning, with a primary focus on the challenges of gradient vanishing and
gradient exploding, which normally lead to diminished model representational
ability and training instability, respectively. We analyze these two challenges
through several strategic measures, including the improvement of gradient flow
and the imposition of constraints on a network's Lipschitz constant. To help
understand the current optimization methodologies, we categorize them into two
classes: explicit optimization and implicit optimization. Explicit optimization
methods involve direct manipulation of optimizer parameters, including weight,
gradient, learning rate, and weight decay. Implicit optimization methods, by
contrast, focus on improving the overall landscape of a network by enhancing
its modules, such as residual shortcuts, normalization methods, attention
mechanisms, and activations. In this article, we provide an in-depth analysis
of these two optimization classes and undertake a thorough examination of the
Jacobian matrices and the Lipschitz constants of many widely used deep learning
modules, highlighting existing issues as well as potential improvements.
Moreover, we also conduct a series of analytical experiments to substantiate
our theoretical discussions. This article does not aim to propose a new
optimizer or network. Rather, our intention is to present a comprehensive
understanding of optimization in deep learning. We hope that this article will
assist readers in gaining a deeper insight in this field and encourages the
development of more robust, efficient, and high-performing models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.11977">Encoding Enhanced Complex CNN for Accurate and Highly Accelerated MRI. (arXiv:2306.11977v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1">Zimeng Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Xiao_S/0/1/0/all/0/1">Sa Xiao</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1">Cheng Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1">Haidong Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhao_X/0/1/0/all/0/1">Xiuchao Zhao</a>, <a href="http://arxiv.org/find/eess/1/au:+Duan_C/0/1/0/all/0/1">Caohui Duan</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_Q/0/1/0/all/0/1">Qian Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Rao_Q/0/1/0/all/0/1">Qiuchen Rao</a>, <a href="http://arxiv.org/find/eess/1/au:+Fang_Y/0/1/0/all/0/1">Yuan Fang</a>, <a href="http://arxiv.org/find/eess/1/au:+Xie_J/0/1/0/all/0/1">Junshuai Xie</a>, <a href="http://arxiv.org/find/eess/1/au:+Shi_L/0/1/0/all/0/1">Lei Shi</a>, <a href="http://arxiv.org/find/eess/1/au:+Guo_F/0/1/0/all/0/1">Fumin Guo</a>, <a href="http://arxiv.org/find/eess/1/au:+Ye_C/0/1/0/all/0/1">Chaohui Ye</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1">Xin Zhou</a></p>
<p>Magnetic resonance imaging (MRI) using hyperpolarized noble gases provides a
way to visualize the structure and function of human lung, but the long imaging
time limits its broad research and clinical applications. Deep learning has
demonstrated great potential for accelerating MRI by reconstructing images from
undersampled data. However, most existing deep conventional neural networks
(CNN) directly apply square convolution to k-space data without considering the
inherent properties of k-space sampling, limiting k-space learning efficiency
and image reconstruction quality. In this work, we propose an encoding enhanced
(EN2) complex CNN for highly undersampled pulmonary MRI reconstruction. EN2
employs convolution along either the frequency or phase-encoding direction,
resembling the mechanisms of k-space sampling, to maximize the utilization of
the encoding correlation and integrity within a row or column of k-space. We
also employ complex convolution to learn rich representations from the complex
k-space data. In addition, we develop a feature-strengthened modularized unit
to further boost the reconstruction performance. Experiments demonstrate that
our approach can accurately reconstruct hyperpolarized 129Xe and 1H lung MRI
from 6-fold undersampled k-space data and provide lung function measurements
with minimal biases compared with fully-sampled image. These results
demonstrate the effectiveness of the proposed algorithmic components and
indicate that the proposed approach could be used for accelerated pulmonary MRI
in research and clinical lung disease patient care.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.16045">OpenNDD: Open Set Recognition for Neurodevelopmental Disorders Detection. (arXiv:2306.16045v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Jiaming Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_Z/0/1/0/all/0/1">Zihao Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1">Xinyue Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shujie Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1">Zhenshan Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiumei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Changcai Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1">Riqing Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1">Lanyan Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1">Lifang Wei</a></p>
<p>Since the strong comorbid similarity in NDDs, such as attention-deficit
hyperactivity disorder, can interfere with the accurate diagnosis of autism
spectrum disorder (ASD), identifying unknown classes is extremely crucial and
challenging from NDDs. We design a novel open set recognition framework for
ASD-aided diagnosis (OpenNDD), which trains a model by combining autoencoder
and adversarial reciprocal points learning to distinguish in-distribution and
out-of-distribution categories as well as identify ASD accurately. Considering
the strong similarities between NDDs, we present a joint scaling method by
Min-Max scaling combined with Standardization (MMS) to increase the differences
between classes for better distinguishing unknown NDDs. We conduct the
experiments in the hybrid datasets from Autism Brain Imaging Data Exchange I
(ABIDE I) and THE ADHD-200 SAMPLE (ADHD-200) with 791 samples from four sites
and the results demonstrate the superiority on various metrics. Our OpenNDD
achieves promising performance, where the accuracy is 77.38%, AUROC is 75.53%
and the open set classification rate is as high as 59.43%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.17010">milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing. (arXiv:2306.17010v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1">Fangqiang Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1">Zhen Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1">Peijun Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1">Chris Xiaoxuan Lu</a></p>
<p>Approaching the era of ubiquitous computing, human motion sensing plays a
crucial role in smart systems for decision making, user interaction, and
personalized services. Extensive research has been conducted on human tracking,
pose estimation, gesture recognition, and activity recognition, which are
predominantly based on cameras in traditional methods. However, the intrusive
nature of cameras limits their use in smart home applications. To address this,
mmWave radars have gained popularity due to their privacy-friendly features. In
this work, we propose milliFlow, a novel deep learning method for scene flow
estimation as a complementary motion information for mmWave point cloud,
serving as an intermediate level of features and directly benefiting downstream
human motion sensing tasks. Experimental results demonstrate the superior
performance of our method with an average 3D endpoint error of 4.6cm,
significantly surpassing the competing approaches. Furthermore, by
incorporating scene flow information, we achieve remarkable improvements in
human activity recognition, human parsing, and human body part tracking. To
foster further research in this area, we will provide our codebase and dataset
for open access upon acceptance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.00562">A MIL Approach for Anomaly Detection in Surveillance Videos from Multiple Camera Views. (arXiv:2307.00562v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pereira_S/0/1/0/all/0/1">Silas Santiago Lopes Pereira</a>, <a href="http://arxiv.org/find/cs/1/au:+Maia_J/0/1/0/all/0/1">Jos&#xe9; Everardo Bessa Maia</a></p>
<p>Occlusion and clutter are two scene states that make it difficult to detect
anomalies in surveillance video. Furthermore, anomaly events are rare and, as a
consequence, class imbalance and lack of labeled anomaly data are also key
features of this task. Therefore, weakly supervised methods are heavily
researched for this application. In this paper, we tackle these typical
problems of anomaly detection in surveillance video by combining Multiple
Instance Learning (MIL) to deal with the lack of labels and Multiple Camera
Views (MC) to reduce occlusion and clutter effects. In the resulting MC-MIL
algorithm we apply a multiple camera combined loss function to train a
regression network with Sultani's MIL ranking function. To evaluate the MC-MIL
algorithm first proposed here, the multiple camera PETS-2009 benchmark dataset
was re-labeled for the anomaly detection task from multiple camera views. The
result shows a significant performance improvement in F1 score compared to the
single-camera configuration.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01097">MVDiffusion: Enabling Holistic Multi-view Image Generation with Correspondence-Aware Diffusion. (arXiv:2307.01097v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1">Shitao Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1">Fuyang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiacheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Peng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Furukawa_Y/0/1/0/all/0/1">Yasutaka Furukawa</a></p>
<p>This paper introduces MVDiffusion, a simple yet effective method for
generating consistent multi-view images from text prompts given pixel-to-pixel
correspondences (e.g., perspective crops from a panorama or multi-view images
given depth maps and poses). Unlike prior methods that rely on iterative image
warping and inpainting, MVDiffusion simultaneously generates all images with a
global awareness, effectively addressing the prevalent error accumulation
issue. At its core, MVDiffusion processes perspective images in parallel with a
pre-trained text-to-image diffusion model, while integrating novel
correspondence-aware attention layers to facilitate cross-view interactions.
For panorama generation, while only trained with 10k panoramas, MVDiffusion is
able to generate high-resolution photorealistic images for arbitrary texts or
extrapolate one perspective image to a 360-degree view. For multi-view
depth-to-image generation, MVDiffusion demonstrates state-of-the-art
performance for texturing a scene mesh. The project page is at
https://mvdiffusion.github.io/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01465">AdAM: Few-Shot Image Generation via Adaptation-Aware Kernel Modulation. (arXiv:2307.01465v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yunqing Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandrasegaran_K/0/1/0/all/0/1">Keshigeyan Chandrasegaran</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdollahzadeh_M/0/1/0/all/0/1">Milad Abdollahzadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1">Chao Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1">Tianyu Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1">Ruoteng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1">Henghui Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheung_N/0/1/0/all/0/1">Ngai-Man Cheung</a></p>
<p>Few-shot image generation (FSIG) aims to learn to generate new and diverse
images given few (e.g., 10) training samples. Recent work has addressed FSIG by
leveraging a GAN pre-trained on a large-scale source domain and adapting it to
the target domain with few target samples. Central to recent FSIG methods are
knowledge preservation criteria, which select and preserve a subset of source
knowledge to the adapted model. However, a major limitation of existing methods
is that their knowledge preserving criteria consider only source domain/task
and fail to consider target domain/adaptation in selecting source knowledge,
casting doubt on their suitability for setups of different proximity between
source and target domain. Our work makes two contributions. Firstly, we revisit
recent FSIG works and their experiments. We reveal that under setups which
assumption of close proximity between source and target domains is relaxed,
many existing state-of-the-art (SOTA) methods which consider only source domain
in knowledge preserving perform no better than a baseline method. As our second
contribution, we propose Adaptation-Aware kernel Modulation (AdAM) for general
FSIG of different source-target domain proximity. Extensive experiments show
that AdAM consistently achieves SOTA performance in FSIG, including challenging
setups where source and target domains are more apart.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.02881">Probabilistic and Semantic Descriptions of Image Manifolds and Their Applications. (arXiv:2307.02881v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tu_P/0/1/0/all/0/1">Peter Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhaoyuan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hartley_R/0/1/0/all/0/1">Richard Hartley</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zhiwei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1">Yiwei Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Campbell_D/0/1/0/all/0/1">Dylan Campbell</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1">Jaskirat Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tianyu Wang</a></p>
<p>This paper begins with a description of methods for estimating image
probability density functions that reflects the observation that such data is
usually constrained to lie in restricted regions of the high-dimensional image
space-not every pattern of pixels is an image. It is common to say that images
lie on a lower-dimensional manifold in the high-dimensional space. However, it
is not the case that all points on the manifold have an equal probability of
being images. Images are unevenly distributed on the manifold, and our task is
to devise ways to model this distribution as a probability distribution. We
therefore consider popular generative models. For our purposes,
generative/probabilistic models should have the properties of 1) sample
generation: the possibility to sample from this distribution with the modelled
density function, and 2) probability computation: given a previously unseen
sample from the dataset of interest, one should be able to compute its
probability, at least up to a normalising constant. To this end, we investigate
the use of methods such as normalising flow and diffusion models. We then show
how semantic interpretations are used to describe points on the manifold. To
achieve this, we consider an emergent language framework that uses variational
encoders for a disentangled representation of points that reside on a given
manifold. Trajectories between points on a manifold can then be described as
evolving semantic descriptions. We also show that such probabilistic
descriptions (bounded) can be used to improve semantic consistency by
constructing defences against adversarial attacks. We evaluate our methods with
improved semantic robustness and OoD detection capability, explainable and
editable semantic interpolation, and improved classification accuracy under
patch attacks. We also discuss the limitation in diffusion models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08286">Going Beyond Linear Mode Connectivity: The Layerwise Linear Feature Connectivity. (arXiv:2307.08286v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhanpeng Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yongyi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xiaojiang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1">Junchi Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1">Wei Hu</a></p>
<p>Recent work has revealed many intriguing empirical phenomena in neural
network training, despite the poorly understood and highly complex loss
landscapes and training dynamics. One of these phenomena, Linear Mode
Connectivity (LMC), has gained considerable attention due to the intriguing
observation that different solutions can be connected by a linear path in the
parameter space while maintaining near-constant training and test losses. In
this work, we introduce a stronger notion of linear connectivity, Layerwise
Linear Feature Connectivity (LLFC), which says that the feature maps of every
layer in different trained networks are also linearly connected. We provide
comprehensive empirical evidence for LLFC across a wide range of settings,
demonstrating that whenever two trained networks satisfy LMC (via either
spawning or permutation methods), they also satisfy LLFC in nearly all the
layers. Furthermore, we delve deeper into the underlying factors contributing
to LLFC, which reveal new insights into the spawning and permutation
approaches. The study of LLFC transcends and advances our understanding of LMC
by adopting a feature-learning perspective.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10455">A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset. (arXiv:2307.10455v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gharaee_Z/0/1/0/all/0/1">Zahra Gharaee</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1">ZeMing Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Pellegrino_N/0/1/0/all/0/1">Nicholas Pellegrino</a>, <a href="http://arxiv.org/find/cs/1/au:+Zarubiieva_I/0/1/0/all/0/1">Iuliia Zarubiieva</a>, <a href="http://arxiv.org/find/cs/1/au:+Haurum_J/0/1/0/all/0/1">Joakim Bruslund Haurum</a>, <a href="http://arxiv.org/find/cs/1/au:+Lowe_S/0/1/0/all/0/1">Scott C. Lowe</a>, <a href="http://arxiv.org/find/cs/1/au:+McKeown_J/0/1/0/all/0/1">Jaclyn T.A. McKeown</a>, <a href="http://arxiv.org/find/cs/1/au:+Ho_C/0/1/0/all/0/1">Chris C.Y. Ho</a>, <a href="http://arxiv.org/find/cs/1/au:+McLeod_J/0/1/0/all/0/1">Joschka McLeod</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1">Yi-Yun C Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Agda_J/0/1/0/all/0/1">Jireh Agda</a>, <a href="http://arxiv.org/find/cs/1/au:+Ratnasingham_S/0/1/0/all/0/1">Sujeevan Ratnasingham</a>, <a href="http://arxiv.org/find/cs/1/au:+Steinke_D/0/1/0/all/0/1">Dirk Steinke</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_A/0/1/0/all/0/1">Angel X. Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Taylor_G/0/1/0/all/0/1">Graham W. Taylor</a>, <a href="http://arxiv.org/find/cs/1/au:+Fieguth_P/0/1/0/all/0/1">Paul Fieguth</a></p>
<p>In an effort to catalog insect biodiversity, we propose a new large dataset
of hand-labelled insect images, the BIOSCAN-Insect Dataset. Each record is
taxonomically classified by an expert, and also has associated genetic
information including raw nucleotide barcode sequences and assigned barcode
index numbers, which are genetically-based proxies for species classification.
This paper presents a curated million-image dataset, primarily to train
computer-vision models capable of providing image-based taxonomic assessment,
however, the dataset also presents compelling characteristics, the study of
which would be of interest to the broader machine learning community. Driven by
the biological nature inherent to the dataset, a characteristic long-tailed
class-imbalance distribution is exhibited. Furthermore, taxonomic labelling is
a hierarchical classification scheme, presenting a highly fine-grained
classification problem at lower levels. Beyond spurring interest in
biodiversity research within the machine learning community, progress on
creating an image-based taxonomic classifier will also further the ultimate
goal of all BIOSCAN research: to lay the foundation for a comprehensive survey
of global biodiversity. This paper introduces the dataset and explores the
classification task through the implementation and analysis of a baseline
classifier.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10499">Mining Conditional Part Semantics with Occluded Extrapolation for Human-Object Interaction Detection. (arXiv:2307.10499v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guangzhi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yangyang Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Kankanhalli_M/0/1/0/all/0/1">Mohan Kankanhalli</a></p>
<p>Human-Object Interaction Detection is a crucial aspect of human-centric scene
understanding, with important applications in various domains. Despite recent
progress in this field, recognizing subtle and detailed interactions remains
challenging. Existing methods try to use human-related clues to alleviate the
difficulty, but rely heavily on external annotations or knowledge, limiting
their practical applicability in real-world scenarios. In this work, we propose
a novel Part Semantic Network (PSN) to solve this problem. The core of PSN is a
Conditional Part Attention (CPA) mechanism, where human features are taken as
keys and values, and the object feature is used as query for the computation in
a cross-attention mechanism. In this way, our model learns to automatically
focus on the most informative human parts conditioned on the involved object,
generating more semantically meaningful features for interaction recognition.
Additionally, we propose an Occluded Part Extrapolation (OPE) strategy to
facilitate interaction recognition under occluded scenarios, which teaches the
model to extrapolate detailed features from partially occluded ones. Our method
consistently outperforms prior approaches on the V-COCO and HICO-DET datasets,
without external data or extra annotations. Additional ablation studies
validate the effectiveness of each component of our proposed method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.16532">Echoes Beyond Points: Unleashing the Power of Raw Radar Data in Multi-modality Fusion. (arXiv:2307.16532v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Feng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1">Naiyan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhaoxiang Zhang</a></p>
<p>Radar is ubiquitous in autonomous driving systems due to its low cost and
good adaptability to bad weather. Nevertheless, the radar detection performance
is usually inferior because its point cloud is sparse and not accurate due to
the poor azimuth and elevation resolution. Moreover, point cloud generation
algorithms already drop weak signals to reduce the false targets which may be
suboptimal for the use of deep fusion. In this paper, we propose a novel method
named EchoFusion to skip the existing radar signal processing pipeline and then
incorporate the radar raw data with other sensors. Specifically, we first
generate the Bird's Eye View (BEV) queries and then take corresponding spectrum
features from radar to fuse with other sensors. By this approach, our method
could utilize both rich and lossless distance and speed clues from radar echoes
and rich semantic clues from images, making our method surpass all existing
methods on the RADIal dataset, and approach the performance of LiDAR. The code
will be released on https://github.com/tusen-ai/EchoFusion.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01412">Achieving state-of-the-art performance in the Medical Out-of-Distribution (MOOD) challenge using plausible synthetic anomalies. (arXiv:2308.01412v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Marimont_S/0/1/0/all/0/1">Sergio Naval Marimont</a>, <a href="http://arxiv.org/find/cs/1/au:+Tarroni_G/0/1/0/all/0/1">Giacomo Tarroni</a></p>
<p>The detection and localization of anomalies is one important medical image
analysis task. Most commonly, Computer Vision anomaly detection approaches rely
on manual annotations that are both time consuming and expensive to obtain.
Unsupervised anomaly detection, or Out-of-Distribution detection, aims at
identifying anomalous samples relying only on unannotated samples considered
normal. In this study we present a new unsupervised anomaly detection method.
Our method builds upon the self-supervised strategy consisting on training a
segmentation network to identify local synthetic anomalies. Our contributions
improve the synthetic anomaly generation process, making synthetic anomalies
more heterogeneous and challenging by 1) using complex random shapes and 2)
smoothing the edges of synthetic anomalies so networks cannot rely on the high
gradient between image and synthetic anomalies. In our implementation we
adopted standard practices in 3D medical image segmentation, including 3D U-Net
architecture, patch-wise training and model ensembling. Our method was
evaluated using a validation set with different types of synthetic anomalies.
Our experiments show that our method improved substantially the baseline method
performance. Additionally, we evaluated our method by participating in the
Medical Out-of-Distribution (MOOD) Challenge held at MICCAI in 2022 and
achieved first position in both sample-wise and pixel-wise tasks. Our
experiments and results in the latest MOOD challenge show that our simple yet
effective approach can substantially improve the performance of
Out-of-Distribution detection techniques which rely on synthetic anomalies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12469">Diffuse, Attend, and Segment: Unsupervised Zero-Shot Segmentation using Stable Diffusion. (arXiv:2308.12469v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1">Junjiao Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Aggarwal_L/0/1/0/all/0/1">Lavisha Aggarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Colaco_A/0/1/0/all/0/1">Andrea Colaco</a>, <a href="http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1">Zsolt Kira</a>, <a href="http://arxiv.org/find/cs/1/au:+Gonzalez_Franco_M/0/1/0/all/0/1">Mar Gonzalez-Franco</a></p>
<p>Producing quality segmentation masks for images is a fundamental problem in
computer vision. Recent research has explored large-scale supervised training
to enable zero-shot segmentation on virtually any image style and unsupervised
training to enable segmentation without dense annotations. However,
constructing a model capable of segmenting anything in a zero-shot manner
without any annotations is still challenging. In this paper, we propose to
utilize the self-attention layers in stable diffusion models to achieve this
goal because the pre-trained stable diffusion model has learned inherent
concepts of objects within its attention layers. Specifically, we introduce a
simple yet effective iterative merging process based on measuring KL divergence
among attention maps to merge them into valid segmentation masks. The proposed
method does not require any training or language dependency to extract quality
segmentation for any images. On COCO-Stuff-27, our method surpasses the prior
unsupervised zero-shot SOTA method by an absolute 26% in pixel accuracy and 17%
in mean IoU. The project page is at
\url{https://sites.google.com/view/diffseg/home}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.13488">Temporal Uncertainty Localization to Enable Human-in-the-loop Analysis of Dynamic Contrast-enhanced Cardiac MRI Datasets. (arXiv:2308.13488v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Yalcinkaya_D/0/1/0/all/0/1">Dilek M. Yalcinkaya</a>, <a href="http://arxiv.org/find/eess/1/au:+Youssef_K/0/1/0/all/0/1">Khalid Youssef</a>, <a href="http://arxiv.org/find/eess/1/au:+Heydari_B/0/1/0/all/0/1">Bobak Heydari</a>, <a href="http://arxiv.org/find/eess/1/au:+Simonetti_O/0/1/0/all/0/1">Orlando Simonetti</a>, <a href="http://arxiv.org/find/eess/1/au:+Dharmakumar_R/0/1/0/all/0/1">Rohan Dharmakumar</a>, <a href="http://arxiv.org/find/eess/1/au:+Raman_S/0/1/0/all/0/1">Subha Raman</a>, <a href="http://arxiv.org/find/eess/1/au:+Sharif_B/0/1/0/all/0/1">Behzad Sharif</a></p>
<p>Dynamic contrast-enhanced (DCE) cardiac magnetic resonance imaging (CMRI) is
a widely used modality for diagnosing myocardial blood flow (perfusion)
abnormalities. During a typical free-breathing DCE-CMRI scan, close to 300
time-resolved images of myocardial perfusion are acquired at various contrast
"wash in/out" phases. Manual segmentation of myocardial contours in each
time-frame of a DCE image series can be tedious and time-consuming,
particularly when non-rigid motion correction has failed or is unavailable.
While deep neural networks (DNNs) have shown promise for analyzing DCE-CMRI
datasets, a "dynamic quality control" (dQC) technique for reliably detecting
failed segmentations is lacking. Here we propose a new space-time uncertainty
metric as a dQC tool for DNN-based segmentation of free-breathing DCE-CMRI
datasets by validating the proposed metric on an external dataset and
establishing a human-in-the-loop framework to improve the segmentation results.
In the proposed approach, we referred the top 10% most uncertain segmentations
as detected by our dQC tool to the human expert for refinement. This approach
resulted in a significant increase in the Dice score (p&lt;0.001) and a notable
decrease in the number of images with failed segmentation (16.2% to 11.3%)
whereas the alternative approach of randomly selecting the same number of
segmentations for human referral did not achieve any significant improvement.
Our results suggest that the proposed dQC framework has the potential to
accurately identify poor-quality segmentations and may enable efficient
DNN-based analysis of DCE-CMRI in a human-in-the-loop pipeline for clinical
interpretation and reporting of dynamic CMRI datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.03179">SLiMe: Segment Like Me. (arXiv:2309.03179v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khani_A/0/1/0/all/0/1">Aliasghar Khani</a>, <a href="http://arxiv.org/find/cs/1/au:+Taghanaki_S/0/1/0/all/0/1">Saeid Asgari Taghanaki</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanghi_A/0/1/0/all/0/1">Aditya Sanghi</a>, <a href="http://arxiv.org/find/cs/1/au:+Amiri_A/0/1/0/all/0/1">Ali Mahdavi Amiri</a>, <a href="http://arxiv.org/find/cs/1/au:+Hamarneh_G/0/1/0/all/0/1">Ghassan Hamarneh</a></p>
<p>Significant strides have been made using large vision-language models, like
Stable Diffusion (SD), for a variety of downstream tasks, including image
editing, image correspondence, and 3D shape generation. Inspired by these
advancements, we explore leveraging these extensive vision-language models for
segmenting images at any desired granularity using as few as one annotated
sample by proposing SLiMe. SLiMe frames this problem as an optimization task.
Specifically, given a single training image and its segmentation mask, we first
extract attention maps, including our novel "weighted accumulated
self-attention map" from the SD prior. Then, using the extracted attention
maps, the text embeddings of Stable Diffusion are optimized such that, each of
them, learn about a single segmented region from the training image. These
learned embeddings then highlight the segmented region in the attention maps,
which in turn can then be used to derive the segmentation map. This enables
SLiMe to segment any real-world image during inference with the granularity of
the segmented region in the training image, using just one example. Moreover,
leveraging additional training data when available, i.e. few-shot, improves the
performance of SLiMe. We carried out a knowledge-rich set of experiments
examining various design factors and showed that SLiMe outperforms other
existing one-shot and few-shot segmentation methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.03493">SAM3D: Segment Anything Model in Volumetric Medical Images. (arXiv:2309.03493v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Bui_N/0/1/0/all/0/1">Nhat-Tan Bui</a>, <a href="http://arxiv.org/find/eess/1/au:+Hoang_D/0/1/0/all/0/1">Dinh-Hieu Hoang</a>, <a href="http://arxiv.org/find/eess/1/au:+Tran_M/0/1/0/all/0/1">Minh-Triet Tran</a>, <a href="http://arxiv.org/find/eess/1/au:+Doretto_G/0/1/0/all/0/1">Gianfranco Doretto</a>, <a href="http://arxiv.org/find/eess/1/au:+Adjeroh_D/0/1/0/all/0/1">Donald Adjeroh</a>, <a href="http://arxiv.org/find/eess/1/au:+Patel_B/0/1/0/all/0/1">Brijesh Patel</a>, <a href="http://arxiv.org/find/eess/1/au:+Choudhary_A/0/1/0/all/0/1">Arabinda Choudhary</a>, <a href="http://arxiv.org/find/eess/1/au:+Le_N/0/1/0/all/0/1">Ngan Le</a></p>
<p>Image segmentation remains a pivotal component in medical image analysis,
aiding in the extraction of critical information for precise diagnostic
practices. With the advent of deep learning, automated image segmentation
methods have risen to prominence, showcasing exceptional proficiency in
processing medical imagery. Motivated by the Segment Anything Model (SAM)-a
foundational model renowned for its remarkable precision and robust
generalization capabilities in segmenting 2D natural images-we introduce SAM3D,
an innovative adaptation tailored for 3D volumetric medical image analysis.
Unlike current SAM-based methods that segment volumetric data by converting the
volume into separate 2D slices for individual analysis, our SAM3D model
processes the entire 3D volume image in a unified approach. Extensive
experiments are conducted on multiple medical image datasets to demonstrate
that our network attains competitive results compared with other
state-of-the-art methods in 3D medical segmentation tasks while being
significantly efficient in terms of parameters. Code and checkpoints are
available at https://github.com/UARK-AICV/SAM3D.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.10987">SpikingNeRF: Making Bio-inspired Neural Networks See through the Real World. (arXiv:2309.10987v3 [cs.NE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1">Xingting Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1">Qinghao Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tielong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mo_Z/0/1/0/all/0/1">Zitao Mo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Zeyu Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuge_Z/0/1/0/all/0/1">Zhengyang Zhuge</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1">Jian Cheng</a></p>
<p>Spiking neural networks (SNNs) have been thriving on numerous tasks to
leverage their promising energy efficiency and exploit their potentialities as
biologically plausible intelligence. Meanwhile, the Neural Radiance Fields
(NeRF) render high-quality 3D scenes with massive energy consumption, but few
works delve into the energy-saving solution with a bio-inspired approach. In
this paper, we propose SpikingNeRF, which aligns the radiance ray with the
temporal dimension of SNN, to naturally accommodate the SNN to the
reconstruction of Radiance Fields. Thus, the computation turns into a
spike-based, multiplication-free manner, reducing the energy consumption. In
SpikingNeRF, each sampled point on the ray is matched onto a particular time
step, and represented in a hybrid manner where the voxel grids are maintained
as well. Based on the voxel grids, sampled points are determined whether to be
masked for better training and inference. However, this operation also incurs
irregular temporal length. We propose the temporal padding strategy to tackle
the masked samples to maintain regular temporal length, i.e., regular tensors,
and the temporal condensing strategy to form a denser data structure for
hardware-friendly computation. Extensive experiments on various datasets
demonstrate that our method reduces the 70.79% energy consumption on average
and obtains comparable synthesis quality with the ANN baseline.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.14303">Dataset Diffusion: Diffusion-based Synthetic Dataset Generation for Pixel-Level Semantic Segmentation. (arXiv:2309.14303v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1">Quang Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1">Truong Vu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_A/0/1/0/all/0/1">Anh Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1">Khoi Nguyen</a></p>
<p>Preparing training data for deep vision models is a labor-intensive task. To
address this, generative models have emerged as an effective solution for
generating synthetic data. While current generative models produce image-level
category labels, we propose a novel method for generating pixel-level semantic
segmentation labels using the text-to-image generative model Stable Diffusion
(SD). By utilizing the text prompts, cross-attention, and self-attention of SD,
we introduce three new techniques: class-prompt appending, class-prompt
cross-attention, and self-attention exponentiation. These techniques enable us
to generate segmentation maps corresponding to synthetic images. These maps
serve as pseudo-labels for training semantic segmenters, eliminating the need
for labor-intensive pixel-wise annotation. To account for the imperfections in
our pseudo-labels, we incorporate uncertainty regions into the segmentation,
allowing us to disregard loss from those regions. We conduct evaluations on two
datasets, PASCAL VOC and MSCOCO, and our approach significantly outperforms
concurrent work. Our benchmarks and code will be released at
https://github.com/VinAIResearch/Dataset-Diffusion
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.17105">Continual Action Assessment via Task-Consistent Score-Discriminative Feature Distribution Modeling. (arXiv:2309.17105v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuan-Ming Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_L/0/1/0/all/0/1">Ling-An Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_J/0/1/0/all/0/1">Jing-Ke Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1">Wei-Shi Zheng</a></p>
<p>Action Quality Assessment (AQA) is a task that tries to answer how well an
action is carried out. While remarkable progress has been achieved, existing
works on AQA assume that all the training data are visible for training in one
time, but do not enable continual learning on assessing new technical actions.
In this work, we address such a Continual Learning problem in AQA
(Continual-AQA), which urges a unified model to learn AQA tasks sequentially
without forgetting. Our idea for modeling Continual-AQA is to sequentially
learn a task-consistent score-discriminative feature distribution, in which the
latent features express a strong correlation with the score labels regardless
of the task or action types. From this perspective, we aim to mitigate the
forgetting in Continual-AQA from two aspects. Firstly, to fuse the features of
new and previous data into a score-discriminative distribution, a novel
Feature-Score Correlation-Aware Rehearsal is proposed to store and reuse data
from previous tasks with limited memory size. Secondly, an Action
General-Specific Graph is developed to learn and decouple the action-general
and action-specific knowledge so that the task-consistent score-discriminative
features can be better extracted across various tasks. Extensive experiments
are conducted to evaluate the contributions of proposed components. The
comparisons with the existing continual learning methods additionally verify
the effectiveness and versatility of our approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00723">HOH: Markerless Multimodal Human-Object-Human Handover Dataset with Large Object Count. (arXiv:2310.00723v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wiederhold_N/0/1/0/all/0/1">Noah Wiederhold</a>, <a href="http://arxiv.org/find/cs/1/au:+Megyeri_A/0/1/0/all/0/1">Ava Megyeri</a>, <a href="http://arxiv.org/find/cs/1/au:+Paris_D/0/1/0/all/0/1">DiMaggio Paris</a>, <a href="http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1">Sean Banerjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Banerjee_N/0/1/0/all/0/1">Natasha Kholgade Banerjee</a></p>
<p>We present the HOH (Human-Object-Human) Handover Dataset, a large object
count dataset with 136 objects, to accelerate data-driven research on handover
studies, human-robot handover implementation, and artificial intelligence (AI)
on handover parameter estimation from 2D and 3D data of person interactions.
HOH contains multi-view RGB and depth data, skeletons, fused point clouds,
grasp type and handedness labels, object, giver hand, and receiver hand 2D and
3D segmentations, giver and receiver comfort ratings, and paired object
metadata and aligned 3D models for 2,720 handover interactions spanning 136
objects and 20 giver-receiver pairs-40 with role-reversal-organized from 40
participants. We also show experimental results of neural networks trained
using HOH to perform grasp, orientation, and trajectory prediction. As the only
fully markerless handover capture dataset, HOH represents natural human-human
handover interactions, overcoming challenges with markered datasets that
require specific suiting for body tracking, and lack high-resolution hand
tracking. To date, HOH is the largest handover dataset in number of objects,
participants, pairs with role reversal accounted for, and total interactions
captured.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03059">Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models. (arXiv:2310.03059v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_I/0/1/0/all/0/1">Ivan Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Ray Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Zoey Guo</a></p>
<p>The popularity of pre-trained large models has revolutionized downstream
tasks across diverse fields, such as language, vision, and multi-modality. To
minimize the adaption cost for downstream tasks, many Parameter-Efficient
Fine-Tuning (PEFT) techniques are proposed for language and 2D image
pre-trained models. However, the specialized PEFT method for 3D pre-trained
models is still under-explored. To this end, we introduce Point-PEFT, a novel
framework for adapting point cloud pre-trained models with minimal learnable
parameters. Specifically, for a pre-trained 3D model, we freeze most of its
parameters, and only tune the newly added PEFT modules on downstream tasks,
which consist of a Point-prior Prompt and a Geometry-aware Adapter. The
Point-prior Prompt adopts a set of learnable prompt tokens, for which we
propose to construct a memory bank with domain-specific knowledge, and utilize
a parameter-free attention to enhance the prompt tokens. The Geometry-aware
Adapter aims to aggregate point cloud features within spatial neighborhoods to
capture fine-grained geometric information through local interactions.
Extensive experiments indicate that our Point-PEFT can achieve better
performance than the full fine-tuning on various downstream tasks, while using
only 5% of the trainable parameters, demonstrating the efficiency and
effectiveness of our approach. Code will be released at
https://github.com/Even-JK/PEFT-3D.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04816">Hacking Generative Models with Differentiable Network Bending. (arXiv:2310.04816v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aldegheri_G/0/1/0/all/0/1">Giacomo Aldegheri</a>, <a href="http://arxiv.org/find/cs/1/au:+Rogalska_A/0/1/0/all/0/1">Alina Rogalska</a>, <a href="http://arxiv.org/find/cs/1/au:+Youssef_A/0/1/0/all/0/1">Ahmed Youssef</a>, <a href="http://arxiv.org/find/cs/1/au:+Iofinova_E/0/1/0/all/0/1">Eugenia Iofinova</a></p>
<p>In this work, we propose a method to 'hack' generative models, pushing their
outputs away from the original training distribution towards a new objective.
We inject a small-scale trainable module between the intermediate layers of the
model and train it for a low number of iterations, keeping the rest of the
network frozen. The resulting output images display an uncanny quality, given
by the tension between the original and new objectives that can be exploited
for artistic purposes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05375">IPDreamer: Appearance-Controllable 3D Object Generation with Image Prompts. (arXiv:2310.05375v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zeng_B/0/1/0/all/0/1">Bohan Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shanglin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yutang Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1">Sicheng Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiaming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Huaxia Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1">Xu Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jianzhuang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Baochang Zhang</a></p>
<p>Recent advances in text-to-3D generation have been remarkable, with methods
such as DreamFusion leveraging large-scale text-to-image diffusion-based models
to supervise 3D generation. These methods, including the variational score
distillation proposed by ProlificDreamer, enable the synthesis of detailed and
photorealistic textured meshes. However, the appearance of 3D objects generated
by these methods is often random and uncontrollable, posing a challenge in
achieving appearance-controllable 3D objects. To address this challenge, we
introduce IPDreamer, a novel approach that incorporates image prompts to
provide specific and comprehensive appearance information for 3D object
generation. Our results demonstrate that IPDreamer effectively generates
high-quality 3D objects that are consistent with both the provided text and
image prompts, demonstrating its promising capability in
appearance-controllable 3D object generation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.06403">Boundary Discretization and Reliable Classification Network for Temporal Action Detection. (arXiv:2310.06403v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1">Zhenying Fang</a></p>
<p>Temporal action detection aims to recognize the action category and determine
the starting and ending time of each action instance in untrimmed videos. The
mixed methods have achieved remarkable performance by simply merging
anchor-based and anchor-free approaches. However, there are still two crucial
issues in the mixed framework: (1) Brute-force merging and handcrafted anchors
design affect the performance and practical application of the mixed methods.
(2) A large number of false positives in action category predictions further
impact the detection performance. In this paper, we propose a novel Boundary
Discretization and Reliable Classification Network (BDRC-Net) that addresses
the above issues by introducing boundary discretization and reliable
classification modules. Specifically, the boundary discretization module (BDM)
elegantly merges anchor-based and anchor-free approaches in the form of
boundary discretization, avoiding the handcrafted anchors design required by
traditional mixed methods. Furthermore, the reliable classification module
(RCM) predicts reliable action categories to reduce false positives in action
category predictions. Extensive experiments conducted on different benchmarks
demonstrate that our proposed method achieves favorable performance compared
with the state-of-the-art. For example, BDRC-Net hits an average mAP of 68.6%
on THUMOS'14, outperforming the previous best by 1.5%. The code will be
released at https://github.com/zhenyingfang/BDRC-Net.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.11864">VQ-NeRF: Neural Reflectance Decomposition and Editing with Vector Quantization. (arXiv:2310.11864v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1">Hongliang Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jingbo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1">Jing Liao</a></p>
<p>We propose VQ-NeRF, a two-branch neural network model that incorporates
Vector Quantization (VQ) to decompose and edit reflectance fields in 3D scenes.
Conventional neural reflectance fields use only continuous representations to
model 3D scenes, despite the fact that objects are typically composed of
discrete materials in reality. This lack of discretization can result in noisy
material decomposition and complicated material editing. To address these
limitations, our model consists of a continuous branch and a discrete branch.
The continuous branch follows the conventional pipeline to predict decomposed
materials, while the discrete branch uses the VQ mechanism to quantize
continuous materials into individual ones. By discretizing the materials, our
model can reduce noise in the decomposition process and generate a segmentation
map of discrete materials. Specific materials can be easily selected for
further editing by clicking on the corresponding area of the segmentation
outcomes. Additionally, we propose a dropout-based VQ codeword ranking strategy
to predict the number of materials in a scene, which reduces redundancy in the
material segmentation process. To improve usability, we also develop an
interactive interface to further assist material editing. We evaluate our model
on both computer-generated and real-world scenes, demonstrating its superior
performance. To the best of our knowledge, our model is the first to enable
discrete material editing in 3D scenes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.13135">LeTFuser: Light-weight End-to-end Transformer-Based Sensor Fusion for Autonomous Driving with Multi-Task Learning. (arXiv:2310.13135v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Agand_P/0/1/0/all/0/1">Pedram Agand</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahdavian_M/0/1/0/all/0/1">Mohammad Mahdavian</a>, <a href="http://arxiv.org/find/cs/1/au:+Savva_M/0/1/0/all/0/1">Manolis Savva</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Mo Chen</a></p>
<p>In end-to-end autonomous driving, the utilization of existing sensor fusion
techniques for imitation learning proves inadequate in challenging situations
that involve numerous dynamic agents. To address this issue, we introduce
LeTFuser, a \mmm{lightweight} transformer-based algorithm for fusing multiple
RGB-D camera representations. To perform perception and control tasks
simultaneously, we utilize multi-task learning. Our model comprises of two
modules, the first being the perception module that is responsible for encoding
the observation data obtained from the RGB-D cameras. It carries out tasks such
as semantic segmentation, semantic depth cloud mapping (SDC), and traffic light
state recognition. Our approach employs the Convolutional vision Transformer
(CvT) \cite{wu2021cvt} to better extract and fuse features from multiple RGB
cameras due to local and global feature extraction capability of convolution
and transformer modules, respectively. Following this, the control module
undertakes the decoding of the encoded characteristics together with
supplementary data, comprising a rough simulator for static and dynamic
environments, as well as various measurements, in order to anticipate the
waypoints associated with a latent feature space. We use two methods to process
these outputs and generate the vehicular controls (e.g. steering, throttle, and
brake) levels. The first method uses a PID algorithm to follow the waypoints on
the fly, whereas the second one directly predicts the control policy using the
measurement features and environmental state. We evaluate the model and conduct
a comparative analysis with recent models on the CARLA simulator using various
scenarios, ranging from normal to adversarial conditions, to simulate
real-world scenarios. Our code is available at
\url{https://github.com/pagand/e2etransfuser/tree/cvpr-w} to facilitate future
studies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.15590">Facial Data Minimization: Shallow Model as Your Privacy Filter. (arXiv:2310.15590v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pu_Y/0/1/0/all/0/1">Yuwen Pu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiahao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1">Jiayu Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+li_H/0/1/0/all/0/1">Hao li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_D/0/1/0/all/0/1">Diqun Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xuhong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1">Shouling Ji</a></p>
<p>Face recognition service has been used in many fields and brings much
convenience to people. However, once the user's facial data is transmitted to a
service provider, the user will lose control of his/her private data. In recent
years, there exist various security and privacy issues due to the leakage of
facial data. Although many privacy-preserving methods have been proposed, they
usually fail when they are not accessible to adversaries' strategies or
auxiliary data. Hence, in this paper, by fully considering two cases of
uploading facial images and facial features, which are very typical in face
recognition service systems, we proposed a data privacy minimization
transformation (PMT) method. This method can process the original facial data
based on the shallow model of authorized services to obtain the obfuscated
data. The obfuscated data can not only maintain satisfactory performance on
authorized models and restrict the performance on other unauthorized models but
also prevent original privacy data from leaking by AI methods and human visual
theft. Additionally, since a service provider may execute preprocessing
operations on the received data, we also propose an enhanced perturbation
method to improve the robustness of PMT. Besides, to authorize one facial image
to multiple service models simultaneously, a multiple restriction mechanism is
proposed to improve the scalability of PMT. Finally, we conduct extensive
experiments and evaluate the effectiveness of the proposed PMT in defending
against face reconstruction, data abuse, and face attribute estimation attacks.
These experimental results demonstrate that PMT performs well in preventing
facial data abuse and privacy leakage while maintaining face recognition
accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19542">Exploiting Image-Related Inductive Biases in Single-Branch Visual Tracking. (arXiv:2310.19542v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1">Chuanming Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1">Joost van de Weijer</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jianlin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yongmei Huang</a></p>
<p>Despite achieving state-of-the-art performance in visual tracking, recent
single-branch trackers tend to overlook the weak prior assumptions associated
with the Vision Transformer (ViT) encoder and inference pipeline. Moreover, the
effectiveness of discriminative trackers remains constrained due to the
adoption of the dual-branch pipeline. To tackle the inferior effectiveness of
the vanilla ViT, we propose an Adaptive ViT Model Prediction tracker (AViTMP)
to bridge the gap between single-branch network and discriminative models.
Specifically, in the proposed encoder AViT-Enc, we introduce an adaptor module
and joint target state embedding to enrich the dense embedding paradigm based
on ViT. Then, we combine AViT-Enc with a dense-fusion decoder and a
discriminative target model to predict accurate location. Further, to mitigate
the limitations of conventional inference practice, we present a novel
inference pipeline called CycleTrack, which bolsters the tracking robustness in
the presence of distractors via bidirectional cycle tracking verification.
Lastly, we propose a dual-frame update inference strategy that adeptively
handles significant challenges in long-term scenarios. In the experiments, we
evaluate AViTMP on ten tracking benchmarks for a comprehensive assessment,
including LaSOT, LaSOTExtSub, AVisT, etc. The experimental results
unequivocally establish that AViTMP attains state-of-the-art performance,
especially on long-time tracking and robustness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00282">TLMCM Network for Medical Image Hierarchical Multi-Label Classification. (arXiv:2311.00282v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1">Meng Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1">Siyan Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1">Qiyu Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1">Wenbin Ouyang</a></p>
<p>Medical Image Hierarchical Multi-Label Classification (MI-HMC) is of
paramount importance in modern healthcare, presenting two significant
challenges: data imbalance and \textit{hierarchy constraint}. Existing
solutions involve complex model architecture design or domain-specific
preprocessing, demanding considerable expertise or effort in implementation. To
address these limitations, this paper proposes Transfer Learning with Maximum
Constraint Module (TLMCM) network for the MI-HMC task. The TLMCM network offers
a novel approach to overcome the aforementioned challenges, outperforming
existing methods based on the Area Under the Average Precision and Recall
Curve($AU\overline{(PRC)}$) metric. In addition, this research proposes two
novel accuracy metrics, $EMR$ and $HammingAccuracy$, which have not been
extensively explored in the context of the MI-HMC task. Experimental results
demonstrate that the TLMCM network achieves high multi-label prediction
accuracy($80\%$-$90\%$) for MI-HMC tasks, making it a valuable contribution to
healthcare domain applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00562">MNN: Mixed Nearest-Neighbors for Self-Supervised Learning. (arXiv:2311.00562v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Long_X/0/1/0/all/0/1">Xianzhong Long</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1">Chen Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yun Li</a></p>
<p>In contrastive self-supervised learning, positive samples are typically drawn
from the same image but in different augmented views, resulting in a relatively
limited source of positive samples. An effective way to alleviate this problem
is to incorporate the relationship between samples, which involves including
the top-K nearest neighbors of positive samples. However, the problem of false
neighbors (i.e., neighbors that do not belong to the same category as the
positive sample) is an objective but often overlooked challenge due to the
query of neighbor samples without supervision information. In this paper, we
present a simple self-supervised learning framework called Mixed
Nearest-Neighbors for Self-Supervised Learning (MNN). MNN optimizes the
influence of neighbor samples on the semantics of positive samples through an
intuitive weighting approach and image mixture operations. The results
demonstrate that MNN exhibits exceptional generalization performance and
training efficiency on four benchmark datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00567">A Robust Deep Learning Method with Uncertainty Estimation for the Pathological Classification of Renal Cell Carcinoma based on CT Images. (arXiv:2311.00567v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Yao_N/0/1/0/all/0/1">Ni Yao</a>, <a href="http://arxiv.org/find/eess/1/au:+Hu_H/0/1/0/all/0/1">Hang Hu</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_K/0/1/0/all/0/1">Kaicong Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhao_C/0/1/0/all/0/1">Chen Zhao</a>, <a href="http://arxiv.org/find/eess/1/au:+Guo_Y/0/1/0/all/0/1">Yuan Guo</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1">Boya Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Nan_J/0/1/0/all/0/1">Jiaofen Nan</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1">Yanting Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Han_C/0/1/0/all/0/1">Chuang Han</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhu_F/0/1/0/all/0/1">Fubao Zhu</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_W/0/1/0/all/0/1">Weihua Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Tian_L/0/1/0/all/0/1">Li Tian</a></p>
<p>Objectives To develop and validate a deep learning-based diagnostic model
incorporating uncertainty estimation so as to facilitate radiologists in the
preoperative differentiation of the pathological subtypes of renal cell
carcinoma (RCC) based on CT images. Methods Data from 668 consecutive patients,
pathologically proven RCC, were retrospectively collected from Center 1. By
using five-fold cross-validation, a deep learning model incorporating
uncertainty estimation was developed to classify RCC subtypes into clear cell
RCC (ccRCC), papillary RCC (pRCC), and chromophobe RCC (chRCC). An external
validation set of 78 patients from Center 2 further evaluated the model's
performance. Results In the five-fold cross-validation, the model's area under
the receiver operating characteristic curve (AUC) for the classification of
ccRCC, pRCC, and chRCC was 0.868 (95% CI: 0.826-0.923), 0.846 (95% CI:
0.812-0.886), and 0.839 (95% CI: 0.802-0.88), respectively. In the external
validation set, the AUCs were 0.856 (95% CI: 0.838-0.882), 0.787 (95% CI:
0.757-0.818), and 0.793 (95% CI: 0.758-0.831) for ccRCC, pRCC, and chRCC,
respectively. Conclusions The developed deep learning model demonstrated robust
performance in predicting the pathological subtypes of RCC, while the
incorporated uncertainty emphasized the importance of understanding model
confidence, which is crucial for assisting clinical decision-making for
patients with renal tumors. Clinical relevance statement Our deep learning
approach, integrated with uncertainty estimation, offers clinicians a dual
advantage: accurate RCC subtype predictions complemented by diagnostic
confidence references, promoting informed decision-making for patients with
RCC.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01455">RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation. (arXiv:2311.01455v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yufei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xian_Z/0/1/0/all/0/1">Zhou Xian</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1">Feng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tsun-Hsuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Erickson_Z/0/1/0/all/0/1">Zackory Erickson</a>, <a href="http://arxiv.org/find/cs/1/au:+Held_D/0/1/0/all/0/1">David Held</a>, <a href="http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1">Chuang Gan</a></p>
<p>We present RoboGen, a generative robotic agent that automatically learns
diverse robotic skills at scale via generative simulation. RoboGen leverages
the latest advancements in foundation and generative models. Instead of
directly using or adapting these models to produce policies or low-level
actions, we advocate for a generative scheme, which uses these models to
automatically generate diversified tasks, scenes, and training supervisions,
thereby scaling up robotic skill learning with minimal human supervision. Our
approach equips a robotic agent with a self-guided propose-generate-learn
cycle: the agent first proposes interesting tasks and skills to develop, and
then generates corresponding simulation environments by populating pertinent
objects and assets with proper spatial configurations. Afterwards, the agent
decomposes the proposed high-level task into sub-tasks, selects the optimal
learning approach (reinforcement learning, motion planning, or trajectory
optimization), generates required training supervision, and then learns
policies to acquire the proposed skill. Our work attempts to extract the
extensive and versatile knowledge embedded in large-scale models and transfer
them to the field of robotics. Our fully generative pipeline can be queried
repeatedly, producing an endless stream of skill demonstrations associated with
diverse tasks and environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02332">Multimodal Machine Learning in Image-Based and Clinical Biomedicine: Survey and Prospects. (arXiv:2311.02332v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Warner_E/0/1/0/all/0/1">Elisa Warner</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Joonsang Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1">William Hsu</a>, <a href="http://arxiv.org/find/cs/1/au:+Syeda_Mahmood_T/0/1/0/all/0/1">Tanveer Syeda-Mahmood</a>, <a href="http://arxiv.org/find/cs/1/au:+Kahn_C/0/1/0/all/0/1">Charles Kahn</a>, <a href="http://arxiv.org/find/cs/1/au:+Gevaert_O/0/1/0/all/0/1">Olivier Gevaert</a>, <a href="http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1">Arvind Rao</a></p>
<p>Machine learning (ML) applications in medical artificial intelligence (AI)
systems have shifted from traditional and statistical methods to increasing
application of deep learning models. This survey navigates the current
landscape of multimodal ML, focusing on its profound impact on medical image
analysis and clinical decision support systems. Emphasizing challenges and
innovations in addressing multimodal representation, fusion, translation,
alignment, and co-learning, the paper explores the transformative potential of
multimodal models for clinical predictions. It also questions practical
implementation of such models, bringing attention to the dynamics between
decision support systems and healthcare providers. Despite advancements,
challenges such as data biases and the scarcity of "big data" in many
biomedical domains persist. We conclude with a discussion on effective
innovation and collaborative efforts to further the miss
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02358">Domain Transfer in Latent Space (DTLS) Wins on Image Super-Resolution $-$ a Non-Denoising Model. (arXiv:2311.02358v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Hui_C/0/1/0/all/0/1">Chun-Chuen Hui</a>, <a href="http://arxiv.org/find/eess/1/au:+Siu_W/0/1/0/all/0/1">Wan-Chi Siu</a>, <a href="http://arxiv.org/find/eess/1/au:+Law_N/0/1/0/all/0/1">Ngai-Fong Law</a></p>
<p>Large scale image super-resolution is a challenging computer vision task,
since vast information is missing in a highly degraded image, say for example
forscale x16 super-resolution. Diffusion models are used successfully in recent
years in extreme super-resolution applications, in which Gaussian noise is used
as a means to form a latent photo-realistic space, and acts as a link between
the space of latent vectors and the latent photo-realistic space. There are
quite a few sophisticated mathematical derivations on mapping the statistics of
Gaussian noises making Diffusion Models successful. In this paper we propose a
simple approach which gets away from using Gaussian noise but adopts some basic
structures of diffusion models for efficient image super-resolution.
Essentially, we propose a DNN to perform domain transfer between neighbor
domains, which can learn the differences in statistical properties to
facilitate gradual interpolation with results of reasonable quality. Further
quality improvement is achieved by conditioning the domain transfer with
reference to the input LR image. Experimental results show that our method
outperforms not only state-of-the-art large scale super resolution models, but
also the current diffusion models for image super-resolution. The approach can
readily be extended to other image-to-image tasks, such as image enlightening,
inpainting, denoising, etc.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02782">Towards Generic Anomaly Detection and Understanding: Large-scale Visual-linguistic Model (GPT-4V) Takes the Lead. (arXiv:2311.02782v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yunkang Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiaohao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1">Chen Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xiaonan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1">Weiming Shen</a></p>
<p>Anomaly detection is a crucial task across different domains and data types.
However, existing anomaly detection models are often designed for specific
domains and modalities. This study explores the use of GPT-4V(ision), a
powerful visual-linguistic model, to address anomaly detection tasks in a
generic manner. We investigate the application of GPT-4V in multi-modality,
multi-domain anomaly detection tasks, including image, video, point cloud, and
time series data, across multiple application areas, such as industrial,
medical, logical, video, 3D anomaly detection, and localization tasks. To
enhance GPT-4V's performance, we incorporate different kinds of additional cues
such as class information, human expertise, and reference images as
prompts.Based on our experiments, GPT-4V proves to be highly effective in
detecting and explaining global and fine-grained semantic patterns in
zero/one-shot anomaly detection. This enables accurate differentiation between
normal and abnormal instances. Although we conducted extensive evaluations in
this study, there is still room for future evaluation to further exploit
GPT-4V's generic anomaly detection capacity from different aspects. These
include exploring quantitative metrics, expanding evaluation benchmarks,
incorporating multi-round interactions, and incorporating human feedback loops.
Nevertheless, GPT-4V exhibits promising performance in generic anomaly
detection and understanding, thus opening up a new avenue for anomaly
detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02877">Inner-IoU: More Effective Intersection over Union Loss with Auxiliary Bounding Box. (arXiv:2311.02877v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Cong Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shuaijie Zhang</a></p>
<p>With the rapid development of detectors, Bounding Box Regression (BBR) loss
function has constantly updated and optimized. However, the existing IoU-based
BBR still focus on accelerating convergence by adding new loss terms, ignoring
the limitations of IoU loss term itself. Although theoretically IoU loss can
effectively describe the state of bounding box regression,in practical
applications, it cannot adjust itself according to different detectors and
detection tasks, and does not have strong generalization. Based on the above,
we first analyzed the BBR model and concluded that distinguishing different
regression samples and using different scales of auxiliary bounding boxes to
calculate losses can effectively accelerate the bounding box regression
process. For high IoU samples, using smaller auxiliary bounding boxes to
calculate losses can accelerate convergence, while larger auxiliary bounding
boxes are suitable for low IoU samples. Then, we propose Inner-IoU loss, which
calculates IoU loss through auxiliary bounding boxes. For different datasets
and detectors, we introduce a scaling factor ratio to control the scale size of
the auxiliary bounding boxes for calculating losses. Finally, integrate
Inner-IoU into the existing IoU-based loss functions for simulation and
comparative experiments. The experiment result demonstrate a further
enhancement in detection performance with the utilization of the method
proposed in this paper, verifying the effectiveness and generalization ability
of Inner-IoU loss. Code is available at https://github.com/Instinct323/wiou.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04414">Learning the What and How of Annotation in Video Object Segmentation. (arXiv:2311.04414v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Delatolas_T/0/1/0/all/0/1">Thanos Delatolas</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalogeiton_V/0/1/0/all/0/1">Vicky Kalogeiton</a>, <a href="http://arxiv.org/find/cs/1/au:+Papadopoulos_D/0/1/0/all/0/1">Dim P. Papadopoulos</a></p>
<p>Video Object Segmentation (VOS) is crucial for several applications, from
video editing to video data generation. Training a VOS model requires an
abundance of manually labeled training videos. The de-facto traditional way of
annotating objects requires humans to draw detailed segmentation masks on the
target objects at each video frame. This annotation process, however, is
tedious and time-consuming. To reduce this annotation cost, in this paper, we
propose EVA-VOS, a human-in-the-loop annotation framework for video object
segmentation. Unlike the traditional approach, we introduce an agent that
predicts iteratively both which frame ("What") to annotate and which annotation
type ("How") to use. Then, the annotator annotates only the selected frame that
is used to update a VOS module, leading to significant gains in annotation
time. We conduct experiments on the MOSE and the DAVIS datasets and we show
that: (a) EVA-VOS leads to masks with accuracy close to the human agreement
3.5x faster than the standard way of annotating videos; (b) our frame selection
achieves state-of-the-art performance; (c) EVA-VOS yields significant
performance gains in terms of annotation time compared to all other methods and
baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04498">NExT-Chat: An LMM for Chat, Detection and Segmentation. (arXiv:2311.04498v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1">Ao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1">Wei Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1">Tat-Seng Chua</a></p>
<p>The development of large language models (LLMs) has greatly advanced the
field of multimodal understanding, leading to the emergence of large multimodal
models (LMMs). In order to enhance the level of visual comprehension, recent
studies have equipped LMMs with region-level understanding capabilities by
representing object bounding box coordinates as a series of text sequences
(pixel2seq). In this paper, we introduce a novel paradigm for object location
modeling called pixel2emb method, where we ask the LMM to output the location
embeddings and then decoded by different decoders. This paradigm allows for
different location formats (such as bounding boxes and masks) to be used in
multimodal conversations Furthermore, this kind of embedding based location
modeling enables the utilization of existing practices in localization tasks,
such as detection and segmentation. In scenarios with limited resources, our
pixel2emb demonstrates superior performance compared to existing
state-of-the-art (SOTA) approaches in both the location input and output tasks
under fair comparison. Leveraging the proposed pixel2emb method, we train an
LMM named NExT-Chat and demonstrate its capability of handling multiple tasks
like visual grounding, region caption, and grounded reasoning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04584">Weakly-supervised deepfake localization in diffusion-generated images. (arXiv:2311.04584v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tantaru_D/0/1/0/all/0/1">Dragos Tantaru</a>, <a href="http://arxiv.org/find/cs/1/au:+Oneata_E/0/1/0/all/0/1">Elisabeta Oneata</a>, <a href="http://arxiv.org/find/cs/1/au:+Oneata_D/0/1/0/all/0/1">Dan Oneata</a></p>
<p>The remarkable generative capabilities of denoising diffusion models have
raised new concerns regarding the authenticity of the images we see every day
on the Internet. However, the vast majority of existing deepfake detection
models are tested against previous generative approaches (e.g. GAN) and usually
provide only a "fake" or "real" label per image. We believe a more informative
output would be to augment the per-image label with a localization map
indicating which regions of the input have been manipulated. To this end, we
frame this task as a weakly-supervised localization problem and identify three
main categories of methods (based on either explanations, local scores or
attention), which we compare on an equal footing by using the Xception network
as the common backbone architecture. We provide a careful analysis of all the
main factors that parameterize the design space: choice of method, type of
supervision, dataset and generator used in the creation of manipulated images;
our study is enabled by constructing datasets in which only one of the
components is varied. Our results show that weakly-supervised localization is
attainable, with the best performing detection method (based on local scores)
being less sensitive to the looser supervision than to the mismatch in terms of
dataset or generator.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04678">Weakly supervised cross-modal learning in high-content screening. (arXiv:2311.04678v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gabriel_W/0/1/0/all/0/1">Watkinson Gabriel</a>, <a href="http://arxiv.org/find/cs/1/au:+Ethan_C/0/1/0/all/0/1">Cohen Ethan</a>, <a href="http://arxiv.org/find/cs/1/au:+Nicolas_B/0/1/0/all/0/1">Bourriez Nicolas</a>, <a href="http://arxiv.org/find/cs/1/au:+Ihab_B/0/1/0/all/0/1">Bendidi Ihab</a>, <a href="http://arxiv.org/find/cs/1/au:+Guillaume_B/0/1/0/all/0/1">Bollot Guillaume</a>, <a href="http://arxiv.org/find/cs/1/au:+Auguste_G/0/1/0/all/0/1">Genovesio Auguste</a></p>
<p>With the surge in available data from various modalities, there is a growing
need to bridge the gap between different data types. In this work, we introduce
a novel approach to learn cross-modal representations between image data and
molecular representations for drug discovery. We propose EMM and IMM, two
innovative loss functions built on top of CLIP that leverage weak supervision
and cross sites replicates in High-Content Screening. Evaluating our model
against known baseline on cross-modal retrieval, we show that our proposed
approach allows to learn better representations and mitigate batch effect. In
addition, we also present a preprocessing method for the JUMP-CP dataset that
effectively reduce the required space from 85Tb to a mere usable 7Tb size,
still retaining all perturbations and most of the information content.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04766">DualTalker: A Cross-Modal Dual Learning Approach for Speech-Driven 3D Facial Animation. (arXiv:2311.04766v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Su_G/0/1/0/all/0/1">Guinan Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yanwu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhifeng Li</a></p>
<p>In recent years, audio-driven 3D facial animation has gained significant
attention, particularly in applications such as virtual reality, gaming, and
video conferencing. However, accurately modeling the intricate and subtle
dynamics of facial expressions remains a challenge. Most existing studies
approach the facial animation task as a single regression problem, which often
fail to capture the intrinsic inter-modal relationship between speech signals
and 3D facial animation and overlook their inherent consistency. Moreover, due
to the limited availability of 3D-audio-visual datasets, approaches learning
with small-size samples have poor generalizability that decreases the
performance. To address these issues, in this study, we propose a cross-modal
dual-learning framework, termed DualTalker, aiming at improving data usage
efficiency as well as relating cross-modal dependencies. The framework is
trained jointly with the primary task (audio-driven facial animation) and its
dual task (lip reading) and shares common audio/motion encoder components. Our
joint training framework facilitates more efficient data usage by leveraging
information from both tasks and explicitly capitalizing on the complementary
relationship between facial motion and audio to improve performance.
Furthermore, we introduce an auxiliary cross-modal consistency loss to mitigate
the potential over-smoothing underlying the cross-modal complementary
representations, enhancing the mapping of subtle facial expression dynamics.
Through extensive experiments and a perceptual user study conducted on the VOCA
and BIWI datasets, we demonstrate that our approach outperforms current
state-of-the-art methods both qualitatively and quantitatively. We have made
our code and video demonstrations available at
https://github.com/sabrina-su/iadf.git.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05698">Mirasol3B: A Multimodal Autoregressive model for time-aligned and contextual modalities. (arXiv:2311.05698v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Piergiovanni_A/0/1/0/all/0/1">AJ Piergiovanni</a>, <a href="http://arxiv.org/find/cs/1/au:+Noble_I/0/1/0/all/0/1">Isaac Noble</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1">Dahun Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Ryoo_M/0/1/0/all/0/1">Michael S. Ryoo</a>, <a href="http://arxiv.org/find/cs/1/au:+Gomes_V/0/1/0/all/0/1">Victor Gomes</a>, <a href="http://arxiv.org/find/cs/1/au:+Angelova_A/0/1/0/all/0/1">Anelia Angelova</a></p>
<p>One of the main challenges of multimodal learning is the need to combine
heterogeneous modalities (e.g., video, audio, text). For example, video and
audio are obtained at much higher rates than text and are roughly aligned in
time. They are often not synchronized with text, which comes as a global
context, e.g., a title, or a description. Furthermore, video and audio inputs
are of much larger volumes, and grow as the video length increases, which
naturally requires more compute dedicated to these modalities and makes
modeling of long-range dependencies harder.
</p>
<p>We here decouple the multimodal modeling, dividing it into separate, focused
autoregressive models, processing the inputs according to the characteristics
of the modalities. We propose a multimodal model, called Mirasol3B, consisting
of an autoregressive component for the time-synchronized modalities (audio and
video), and an autoregressive component for the context modalities which are
not necessarily aligned in time but are still sequential. To address the
long-sequences of the video-audio inputs, we propose to further partition the
video and audio sequences in consecutive snippets and autoregressively process
their representations. To that end, we propose a Combiner mechanism, which
models the audio-video information jointly within a timeframe. The Combiner
learns to extract audio and video features from raw spatio-temporal signals,
and then learns to fuse these features producing compact but expressive
representations per snippet.
</p>
<p>Our approach achieves the state-of-the-art on well established multimodal
benchmarks, outperforming much larger models. It effectively addresses the high
computational demand of media inputs by both learning compact representations,
controlling the sequence length of the audio-video feature representations, and
modeling their dependencies in time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05919">Inter-object Discriminative Graph Modeling for Indoor Scene Recognition. (arXiv:2311.05919v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1">Chuanxin Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Hanbo Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xin Ma</a></p>
<p>Variable scene layouts and coexisting objects across scenes make indoor scene
recognition still a challenging task. Leveraging object information within
scenes to enhance the distinguishability of feature representations has emerged
as a key approach in this domain. Currently, most object-assisted methods use a
separate branch to process object information, combining object and scene
features heuristically. However, few of them pay attention to interpretably
handle the hidden discriminative knowledge within object information. In this
paper, we propose to leverage discriminative object knowledge to enhance scene
feature representations. Initially, we capture the object-scene discriminative
relationships from a probabilistic perspective, which are transformed into an
Inter-Object Discriminative Prototype (IODP). Given the abundant prior
knowledge from IODP, we subsequently construct a Discriminative Graph Network
(DGN), in which pixel-level scene features are defined as nodes and the
discriminative relationships between node features are encoded as edges. DGN
aims to incorporate inter-object discriminative knowledge into the image
representation through graph convolution. With the proposed IODP and DGN, we
obtain state-of-the-art results on several widely used scene datasets,
demonstrating the effectiveness of the proposed approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05927">Automated Sperm Assessment Framework and Neural Network Specialized for Sperm Video Recognition. (arXiv:2311.05927v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fujii_T/0/1/0/all/0/1">Takuro Fujii</a>, <a href="http://arxiv.org/find/cs/1/au:+Nakagawa_H/0/1/0/all/0/1">Hayato Nakagawa</a>, <a href="http://arxiv.org/find/cs/1/au:+Takeshima_T/0/1/0/all/0/1">Teppei Takeshima</a>, <a href="http://arxiv.org/find/cs/1/au:+Yumura_Y/0/1/0/all/0/1">Yasushi Yumura</a>, <a href="http://arxiv.org/find/cs/1/au:+Hamagami_T/0/1/0/all/0/1">Tomoki Hamagami</a></p>
<p>Infertility is a global health problem, and an increasing number of couples
are seeking medical assistance to achieve reproduction, at least half of which
are caused by men. The success rate of assisted reproductive technologies
depends on sperm assessment, in which experts determine whether sperm can be
used for reproduction based on morphology and motility of sperm. Previous sperm
assessment studies with deep learning have used datasets comprising images that
include only sperm heads, which cannot consider motility and other morphologies
of sperm. Furthermore, the labels of the dataset are one-hot, which provides
insufficient support for experts, because assessment results are inconsistent
between experts, and they have no absolute answer. Therefore, we constructed
the video dataset for sperm assessment whose videos include sperm head as well
as neck and tail, and its labels were annotated with soft-label. Furthermore,
we proposed the sperm assessment framework and the neural network, RoSTFine,
for sperm video recognition. Experimental results showed that RoSTFine could
improve the sperm assessment performances compared to existing video
recognition models and focus strongly on important sperm parts (i.e., head and
neck).
</p>
</p>
</div>

    </div>
    </body>
    