<!DOCTYPE html>
<html>
<head>
<title>2024-01-07-cs-cv</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2401.01911">Backdoor Attack on Unpaired Medical Image-Text Foundation Models: A Pilot Study on MedCLIP. (arXiv:2401.01911v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1">Ruinan Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1">Chun-Yin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1">Chenyu You</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaoxiao Li</a></p>
<p>In recent years, foundation models (FMs) have solidified their role as
cornerstone advancements in the deep learning domain. By extracting intricate
patterns from vast datasets, these models consistently achieve state-of-the-art
results across a spectrum of downstream tasks, all without necessitating
extensive computational resources. Notably, MedCLIP, a vision-language
contrastive learning-based medical FM, has been designed using unpaired
image-text training. While the medical domain has often adopted unpaired
training to amplify data, the exploration of potential security concerns linked
to this approach hasn't kept pace with its practical usage. Notably, the
augmentation capabilities inherent in unpaired training also indicate that
minor label discrepancies can result in significant model deviations. In this
study, we frame this label discrepancy as a backdoor attack problem. We further
analyze its impact on medical FMs throughout the FM supply chain. Our
evaluation primarily revolves around MedCLIP, emblematic of medical FM
employing the unpaired strategy. We begin with an exploration of
vulnerabilities in MedCLIP stemming from unpaired image-text matching, termed
BadMatch. BadMatch is achieved using a modest set of wrongly labeled data.
Subsequently, we disrupt MedCLIP's contrastive learning through
BadDist-assisted BadMatch by introducing a Bad-Distance between the embeddings
of clean and poisoned data. Additionally, combined with BadMatch and BadDist,
the attacking pipeline consistently fends off backdoor assaults across diverse
model designs, datasets, and triggers. Also, our findings reveal that current
defense strategies are insufficient in detecting these latent threats in
medical FMs' supply chains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01912">Shrinking Your TimeStep: Towards Low-Latency Neuromorphic Object Recognition with Spiking Neural Network. (arXiv:2401.01912v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1">Yongqi Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Zuo_L/0/1/0/all/0/1">Lin Zuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Jing_M/0/1/0/all/0/1">Mengmeng Jing</a>, <a href="http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1">Pei He</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1">Yongjun Xiao</a></p>
<p>Neuromorphic object recognition with spiking neural networks (SNNs) is the
cornerstone of low-power neuromorphic computing. However, existing SNNs suffer
from significant latency, utilizing 10 to 40 timesteps or more, to recognize
neuromorphic objects. At low latencies, the performance of existing SNNs is
drastically degraded. In this work, we propose the Shrinking SNN (SSNN) to
achieve low-latency neuromorphic object recognition without reducing
performance. Concretely, we alleviate the temporal redundancy in SNNs by
dividing SNNs into multiple stages with progressively shrinking timesteps,
which significantly reduces the inference latency. During timestep shrinkage,
the temporal transformer smoothly transforms the temporal scale and preserves
the information maximally. Moreover, we add multiple early classifiers to the
SNN during training to mitigate the mismatch between the surrogate gradient and
the true gradient, as well as the gradient vanishing/exploding, thus
eliminating the performance degradation at low latency. Extensive experiments
on neuromorphic datasets, CIFAR10-DVS, N-Caltech101, and DVS-Gesture have
revealed that SSNN is able to improve the baseline accuracy by 6.55% ~ 21.41%.
With only 5 average timesteps and without any data augmentation, SSNN is able
to achieve an accuracy of 73.63% on CIFAR10-DVS. This work presents a
heterogeneous temporal scale SNN and provides valuable insights into the
development of high-performance, low-latency SNNs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01918">Distilling Temporal Knowledge with Masked Feature Reconstruction for 3D Object Detection. (arXiv:2401.01918v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1">Haowen Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_D/0/1/0/all/0/1">Dong Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jintao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ai_R/0/1/0/all/0/1">Rui Ai</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_W/0/1/0/all/0/1">Weihao Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1">Yanyan Liang</a></p>
<p>Striking a balance between precision and efficiency presents a prominent
challenge in the bird's-eye-view (BEV) 3D object detection. Although previous
camera-based BEV methods achieved remarkable performance by incorporating
long-term temporal information, most of them still face the problem of low
efficiency. One potential solution is knowledge distillation. Existing
distillation methods only focus on reconstructing spatial features, while
overlooking temporal knowledge. To this end, we propose TempDistiller, a
Temporal knowledge Distiller, to acquire long-term memory from a teacher
detector when provided with a limited number of frames. Specifically, a
reconstruction target is formulated by integrating long-term temporal knowledge
through self-attention operation applied to feature teachers. Subsequently,
novel features are generated for masked student features via a generator.
Ultimately, we utilize this reconstruction target to reconstruct the student
features. In addition, we also explore temporal relational knowledge when
inputting full frames for the student model. We verify the effectiveness of the
proposed method on the nuScenes benchmark. The experimental results show our
method obtain an enhancement of +1.6 mAP and +1.1 NDS compared to the baseline,
a speed improvement of approximately 6 FPS after compressing temporal
knowledge, and the most accurate velocity estimation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01922">Unsupervised Object-Centric Learning from Multiple Unspecified Viewpoints. (arXiv:2401.01922v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1">Jinyang Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tonglin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1">Zhimeng Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1">Xiangyang Xue</a></p>
<p>Visual scenes are extremely diverse, not only because there are infinite
possible combinations of objects and backgrounds but also because the
observations of the same scene may vary greatly with the change of viewpoints.
When observing a multi-object visual scene from multiple viewpoints, humans can
perceive the scene compositionally from each viewpoint while achieving the
so-called ``object constancy'' across different viewpoints, even though the
exact viewpoints are untold. This ability is essential for humans to identify
the same object while moving and to learn from vision efficiently. It is
intriguing to design models that have a similar ability. In this paper, we
consider a novel problem of learning compositional scene representations from
multiple unspecified (i.e., unknown and unrelated) viewpoints without using any
supervision and propose a deep generative model which separates latent
representations into a viewpoint-independent part and a viewpoint-dependent
part to solve this problem. During the inference, latent representations are
randomly initialized and iteratively updated by integrating the information in
different viewpoints with neural networks. Experiments on several specifically
designed synthetic datasets have shown that the proposed method can effectively
learn from multiple unspecified viewpoints.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01951">Can We Generate Realistic Hands Only Using Convolution?. (arXiv:2401.01951v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hosseini_M/0/1/0/all/0/1">Mehran Hosseini</a>, <a href="http://arxiv.org/find/cs/1/au:+Hosseini_P/0/1/0/all/0/1">Peyman Hosseini</a></p>
<p>The enduring inability of image generative models to recreate intricate
geometric features, such as those present in human hands and fingers has been
an ongoing problem in image generation for nearly a decade. While strides have
been made by increasing model sizes and diversifying training datasets, this
issue remains prevalent across all models, from denoising diffusion models to
Generative Adversarial Networks (GAN), pointing to a fundamental shortcoming in
the underlying architectures. In this paper, we demonstrate how this problem
can be mitigated by augmenting convolution layers geometric capabilities
through providing them with a single input channel incorporating the relative
$n$-dimensional Cartesian coordinate system. We show that this drastically
improves quality of hand and face images generated by GANs and Variational
AutoEncoders (VAE).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01952">Instruct-Imagen: Image Generation with Multi-modal Instruction. (arXiv:2401.01952v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1">Hexiang Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_K/0/1/0/all/0/1">Kelvin C.K. Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1">Yu-Chuan Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wenhu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yandong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1">Kihyuk Sohn</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ben_X/0/1/0/all/0/1">Xue Ben</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1">Boqing Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1">William Cohen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1">Ming-Wei Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1">Xuhui Jia</a></p>
<p>This paper presents instruct-imagen, a model that tackles heterogeneous image
generation tasks and generalizes across unseen tasks. We introduce *multi-modal
instruction* for image generation, a task representation articulating a range
of generation intents with precision. It uses natural language to amalgamate
disparate modalities (e.g., text, edge, style, subject, etc.), such that
abundant generation intents can be standardized in a uniform format.
</p>
<p>We then build instruct-imagen by fine-tuning a pre-trained text-to-image
diffusion model with a two-stage framework. First, we adapt the model using the
retrieval-augmented training, to enhance model's capabilities to ground its
generation on external multimodal context. Subsequently, we fine-tune the
adapted model on diverse image generation tasks that requires vision-language
understanding (e.g., subject-driven generation, etc.), each paired with a
multi-modal instruction encapsulating the task's essence. Human evaluation on
various image generation datasets reveals that instruct-imagen matches or
surpasses prior task-specific models in-domain and demonstrates promising
generalization to unseen and more complex tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01970">FMGS: Foundation Model Embedded 3D Gaussian Splatting for Holistic 3D Scene Understanding. (arXiv:2401.01970v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1">Xingxing Zuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Samangouei_P/0/1/0/all/0/1">Pouya Samangouei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yunwen Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Di_Y/0/1/0/all/0/1">Yan Di</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Mingyang Li</a></p>
<p>Precisely perceiving the geometric and semantic properties of real-world 3D
objects is crucial for the continued evolution of augmented reality and robotic
applications. To this end, we present \algfull{} (\algname{}), which
incorporates vision-language embeddings of foundation models into 3D Gaussian
Splatting (GS). The key contribution of this work is an efficient method to
reconstruct and represent 3D vision-language models. This is achieved by
distilling feature maps generated from image-based foundation models into those
rendered from our 3D model. To ensure high-quality rendering and fast training,
we introduce a novel scene representation by integrating strengths from both GS
and multi-resolution hash encodings (MHE). Our effective training procedure
also introduces a pixel alignment loss that makes the rendered feature distance
of same semantic entities close, following the pixel-level semantic boundaries.
Our results demonstrate remarkable multi-view semantic consistency,
facilitating diverse downstream tasks, beating state-of-the-art methods by
$\mathbf{10.2}$ percent on open-vocabulary language-based object detection,
despite that we are $\mathbf{851\times}$ faster for inference. This research
explores the intersection of vision, language, and 3D scene representation,
paving the way for enhanced scene understanding in uncontrolled real-world
environments. We plan to release the code upon paper acceptance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01974">Towards Truly Zero-shot Compositional Visual Reasoning with LLMs as Programmers. (arXiv:2401.01974v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stanic_A/0/1/0/all/0/1">Aleksandar Stani&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Caelles_S/0/1/0/all/0/1">Sergi Caelles</a>, <a href="http://arxiv.org/find/cs/1/au:+Tschannen_M/0/1/0/all/0/1">Michael Tschannen</a></p>
<p>Visual reasoning is dominated by end-to-end neural networks scaled to
billions of model parameters and training examples. However, even the largest
models struggle with compositional reasoning, generalization, fine-grained
spatial and temporal reasoning, and counting. Visual reasoning with large
language models (LLMs) as controllers can, in principle, address these
limitations by decomposing the task and solving subtasks by orchestrating a set
of (visual) tools. Recently, these models achieved great performance on tasks
such as compositional visual question answering, visual grounding, and video
temporal reasoning. Nevertheless, in their current form, these models heavily
rely on human engineering of in-context examples in the prompt, which are often
dataset- and task-specific and require significant labor by highly skilled
programmers. In this work, we present a framework that mitigates these issues
by introducing spatially and temporally abstract routines and by leveraging a
small number of labeled examples to automatically generate in-context examples,
thereby avoiding human-created in-context examples. On a number of visual
reasoning tasks, we show that our framework leads to consistent gains in
performance, makes LLMs as controllers setup more robust, and removes the need
for human engineering of in-context examples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01984">AUPIMO: Redefining Visual Anomaly Detection Benchmarks with High Speed and Low Tolerance. (arXiv:2401.01984v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bertoldo_J/0/1/0/all/0/1">Joao P. C. Bertoldo</a>, <a href="http://arxiv.org/find/cs/1/au:+Ameln_D/0/1/0/all/0/1">Dick Ameln</a>, <a href="http://arxiv.org/find/cs/1/au:+Vaidya_A/0/1/0/all/0/1">Ashwin Vaidya</a>, <a href="http://arxiv.org/find/cs/1/au:+Akcay_S/0/1/0/all/0/1">Samet Ak&#xe7;ay</a></p>
<p>Recent advances in visual anomaly detection research have seen AUROC and
AUPRO scores on public benchmark datasets such as MVTec and VisA converge
towards perfect recall, giving the impression that these benchmarks are
near-solved. However, high AUROC and AUPRO scores do not always reflect
qualitative performance, which limits the validity of these metrics in
real-world applications. We argue that the artificial ceiling imposed by the
lack of an adequate evaluation metric restrains progression of the field, and
it is crucial that we revisit the evaluation metrics used to rate our
algorithms. In response, we introduce Per-IMage Overlap (PIMO), a novel metric
that addresses the shortcomings of AUROC and AUPRO. PIMO retains the
recall-based nature of the existing metrics but introduces two distinctions:
the assignment of curves (and respective area under the curve) is per-image,
and its X-axis relies solely on normal images. Measuring recall per image
simplifies instance score indexing and is more robust to noisy annotations. As
we show, it also accelerates computation and enables the usage of statistical
tests to compare models. By imposing low tolerance for false positives on
normal images, PIMO provides an enhanced model validation procedure and
highlights performance variations across datasets. Our experiments demonstrate
that PIMO offers practical advantages and nuanced performance insights that
redefine anomaly detection benchmarks -- notably challenging the perception
that MVTec AD and VisA datasets have been solved by contemporary models.
Available on GitHub: https://github.com/jpcbertoldo/aupimo.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01990">GPS-SSL: Guided Positive Sampling to Inject Prior Into Self-Supervised Learning. (arXiv:2401.01990v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feizi_A/0/1/0/all/0/1">Aarash Feizi</a>, <a href="http://arxiv.org/find/cs/1/au:+Balestriero_R/0/1/0/all/0/1">Randall Balestriero</a>, <a href="http://arxiv.org/find/cs/1/au:+Romero_Soriano_A/0/1/0/all/0/1">Adriana Romero-Soriano</a>, <a href="http://arxiv.org/find/cs/1/au:+Rabbany_R/0/1/0/all/0/1">Reihaneh Rabbany</a></p>
<p>We propose Guided Positive Sampling Self-Supervised Learning (GPS-SSL), a
general method to inject a priori knowledge into Self-Supervised Learning (SSL)
positive samples selection. Current SSL methods leverage Data-Augmentations
(DA) for generating positive samples and incorporate prior knowledge - an
incorrect, or too weak DA will drastically reduce the quality of the learned
representation. GPS-SSL proposes instead to design a metric space where
Euclidean distances become a meaningful proxy for semantic relationship. In
that space, it is now possible to generate positive samples from nearest
neighbor sampling. Any prior knowledge can now be embedded into that metric
space independently from the employed DA. From its simplicity, GPS-SSL is
applicable to any SSL method, e.g. SimCLR or BYOL. A key benefit of GPS-SSL is
in reducing the pressure in tailoring strong DAs. For example GPS-SSL reaches
85.58% on Cifar10 with weak DA while the baseline only reaches 37.51%. We
therefore move a step forward towards the goal of making SSL less reliant on
DA. We also show that even when using strong DAs, GPS-SSL outperforms the
baselines on under-studied domains. We evaluate GPS-SSL along with multiple
baseline SSL methods on numerous downstream datasets from different domains
when the models use strong or minimal data augmentations. We hope that GPS-SSL
will open new avenues in studying how to inject a priori knowledge into SSL in
a principled manner.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02015">Improving Diffusion-Based Image Synthesis with Context Prediction. (arXiv:2401.02015v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Ling Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jingwei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1">Shenda Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhilong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zhilin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1">Zheming Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wentao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1">Bin Cui</a></p>
<p>Diffusion models are a new class of generative models, and have dramatically
promoted image generation with unprecedented quality and diversity. Existing
diffusion models mainly try to reconstruct input image from a corrupted one
with a pixel-wise or feature-wise constraint along spatial axes. However, such
point-based reconstruction may fail to make each predicted pixel/feature fully
preserve its neighborhood context, impairing diffusion-based image synthesis.
As a powerful source of automatic supervisory signal, context has been well
studied for learning representations. Inspired by this, we for the first time
propose ConPreDiff to improve diffusion-based image synthesis with context
prediction. We explicitly reinforce each point to predict its neighborhood
context (i.e., multi-stride features/tokens/pixels) with a context decoder at
the end of diffusion denoising blocks in training stage, and remove the decoder
for inference. In this way, each point can better reconstruct itself by
preserving its semantic connections with neighborhood context. This new
paradigm of ConPreDiff can generalize to arbitrary discrete and continuous
diffusion backbones without introducing extra parameters in sampling procedure.
Extensive experiments are conducted on unconditional image generation,
text-to-image generation and image inpainting tasks. Our ConPreDiff
consistently outperforms previous methods and achieves a new SOTA text-to-image
generation results on MS-COCO, with a zero-shot FID score of 6.21.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02020">Spikformer V2: Join the High Accuracy Club on ImageNet with an SNN Ticket. (arXiv:2401.02020v1 [cs.NE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhaokun Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Che_K/0/1/0/all/0/1">Kaiwei Che</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1">Wei Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_K/0/1/0/all/0/1">Keyu Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yuesheng Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1">Shuicheng Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yonghong Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1">Li Yuan</a></p>
<p>Spiking Neural Networks (SNNs), known for their biologically plausible
architecture, face the challenge of limited performance. The self-attention
mechanism, which is the cornerstone of the high-performance Transformer and
also a biologically inspired structure, is absent in existing SNNs. To this
end, we explore the potential of leveraging both self-attention capability and
biological properties of SNNs, and propose a novel Spiking Self-Attention (SSA)
and Spiking Transformer (Spikformer). The SSA mechanism eliminates the need for
softmax and captures the sparse visual feature employing spike-based Query,
Key, and Value. This sparse computation without multiplication makes SSA
efficient and energy-saving. Further, we develop a Spiking Convolutional Stem
(SCS) with supplementary convolutional layers to enhance the architecture of
Spikformer. The Spikformer enhanced with the SCS is referred to as Spikformer
V2. To train larger and deeper Spikformer V2, we introduce a pioneering
exploration of Self-Supervised Learning (SSL) within the SNN. Specifically, we
pre-train Spikformer V2 with masking and reconstruction style inspired by the
mainstream self-supervised Transformer, and then finetune the Spikformer V2 on
the image classification on ImageNet. Extensive experiments show that
Spikformer V2 outperforms other previous surrogate training and ANN2SNN
methods. An 8-layer Spikformer V2 achieves an accuracy of 80.38% using 4 time
steps, and after SSL, a 172M 16-layer Spikformer V2 reaches an accuracy of
81.10% with just 1 time step. To the best of our knowledge, this is the first
time that the SNN achieves 80+% accuracy on ImageNet. The code will be
available at Spikformer V2.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02031">Spy-Watermark: Robust Invisible Watermarking for Backdoor Attack. (arXiv:2401.02031v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Ruofei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_R/0/1/0/all/0/1">Renjie Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Zongyu Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1">Qing Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1">Rui Huang</a></p>
<p>Backdoor attack aims to deceive a victim model when facing backdoor instances
while maintaining its performance on benign data. Current methods use manual
patterns or special perturbations as triggers, while they often overlook the
robustness against data corruption, making backdoor attacks easy to defend in
practice. To address this issue, we propose a novel backdoor attack method
named Spy-Watermark, which remains effective when facing data collapse and
backdoor defense. Therein, we introduce a learnable watermark embedded in the
latent domain of images, serving as the trigger. Then, we search for a
watermark that can withstand collapse during image decoding, cooperating with
several anti-collapse operations to further enhance the resilience of our
trigger against data corruption. Extensive experiments are conducted on
CIFAR10, GTSRB, and ImageNet datasets, demonstrating that Spy-Watermark
overtakes ten state-of-the-art methods in terms of robustness and stealthiness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02032">DiffusionEdge: Diffusion Probabilistic Model for Crisp Edge Detection. (arXiv:2401.02032v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1">Yunfan Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Kai Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yuhang Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_R/0/1/0/all/0/1">Renjiao Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1">Zhiping Cai</a></p>
<p>Limited by the encoder-decoder architecture, learning-based edge detectors
usually have difficulty predicting edge maps that satisfy both correctness and
crispness. With the recent success of the diffusion probabilistic model (DPM),
we found it is especially suitable for accurate and crisp edge detection since
the denoising process is directly applied to the original image size.
Therefore, we propose the first diffusion model for the task of general edge
detection, which we call DiffusionEdge. To avoid expensive computational
resources while retaining the final performance, we apply DPM in the latent
space and enable the classic cross-entropy loss which is uncertainty-aware in
pixel level to directly optimize the parameters in latent space in a
distillation manner. We also adopt a decoupled architecture to speed up the
denoising process and propose a corresponding adaptive Fourier filter to adjust
the latent features of specific frequencies. With all the technical designs,
DiffusionEdge can be stably trained with limited resources, predicting crisp
and accurate edge maps with much fewer augmentation strategies. Extensive
experiments on four edge detection benchmarks demonstrate the superiority of
DiffusionEdge both in correctness and crispness. On the NYUDv2 dataset,
compared to the second best, we increase the ODS, OIS (without post-processing)
and AC by 30.2%, 28.1% and 65.1%, respectively. Code:
https://github.com/GuHuangAI/DiffusionEdge.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02041">Efficient Cloud-edge Collaborative Inference for Object Re-identification. (arXiv:2401.02041v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chuanming Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yuxin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_M/0/1/0/all/0/1">Mengshi Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1">Huadong Ma</a></p>
<p>Current object re-identification (ReID) system follows the centralized
processing paradigm, i.e., all computations are conducted in the cloud server
and edge devices are only used to capture and send images. As the number of
videos experiences a rapid escalation, this paradigm has become impractical due
to the finite computational resources. In such a scenario, the ReID system
should be converted to fit in the cloud-edge collaborative processing paradigm,
which is crucial to boost the scalability and practicality of ReID systems.
However, current relevant work lacks research on this issue, making it
challenging for ReID methods to be adapted effectively. Therefore, we pioneer a
cloud-edge collaborative inference framework for ReID systems and particularly
propose a distribution-aware correlation modeling network (DaCM) to make the
desired image return to the cloud server as soon as possible via learning to
model the spatial-temporal correlations among instances. DaCM embeds the
spatial-temporal correlations implicitly included in the timestamps into a
graph structure, and it can be applied in the cloud to regulate the size of the
upload window and on the edge device to adjust the sequence of images,
respectively. Traditional ReID methods can be combined with DaCM seamlessly,
enabling their application within our proposed edge-cloud collaborative
framework. Extensive experiments demonstrate that our method obviously reduces
transmission overhead and significantly improves performance. We will release
our code and model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02044">Generalizable vision-language pre-training for annotation-free pathology localization. (arXiv:2401.02044v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Hao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Hong-Yu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Cheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1">Weijian Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiarun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shanshan Wang</a></p>
<p>Locating pathologies automatically from medical images aids the understanding
of the emergence and progression of diseases, and such an ability can
significantly benefit clinical diagnostics. However, existing deep learning
models heavily rely on expert annotations and lack generalization capabilities
in open clinical environments. In this study, we present a generalizable
vision-language pre-training model for Annotation-Free pathology Localization
(AFLoc). The core strength of AFLoc lies in its image annotation-free
multi-level semantic structure-based contrastive learning, which
comprehensively aligns multi-granularity medical concepts from reports with
abundant image features, to adapt to the diverse expressions of observed and
emerging unseen pathologies. We conducted extensive experimental validation
across 4 distinct external datasets, encompassing 11 types of chest
pathologies, to verify its generalization ability. The results demonstrate that
AFLoc surpasses 6 state-of-the-art methods and even outperforms the human
benchmark in locating 5 different pathologies, underscoring its suitability for
complex clinical environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02052">Encoder-Decoder Based Long Short-Term Memory (LSTM) Model for Video Captioning. (arXiv:2401.02052v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Adewale_S/0/1/0/all/0/1">Sikiru Adewale</a>, <a href="http://arxiv.org/find/cs/1/au:+Ige_T/0/1/0/all/0/1">Tosin Ige</a>, <a href="http://arxiv.org/find/cs/1/au:+Matti_B/0/1/0/all/0/1">Bolanle Hafiz Matti</a></p>
<p>This work demonstrates the implementation and use of an encoder-decoder model
to perform a many-to-many mapping of video data to text captions. The
many-to-many mapping occurs via an input temporal sequence of video frames to
an output sequence of words to form a caption sentence. Data preprocessing,
model construction, and model training are discussed. Caption correctness is
evaluated using 2-gram BLEU scores across the different splits of the dataset.
Specific examples of output captions were shown to demonstrate model generality
over the video temporal dimension. Predicted captions were shown to generalize
over video action, even in instances where the video scene changed
dramatically. Model architecture changes are discussed to improve sentence
grammar and correctness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02076">Leveraging SAM for Single-Source Domain Generalization in Medical Image Segmentation. (arXiv:2401.02076v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hanhui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1">Huaize Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1">Yi Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xueyan Zhang</a></p>
<p>Domain Generalization (DG) aims to reduce domain shifts between domains to
achieve promising performance on the unseen target domain, which has been
widely practiced in medical image segmentation. Single-source domain
generalization (SDG) is the most challenging setting that trains on only one
source domain. Although existing methods have made considerable progress on SDG
of medical image segmentation, the performances are still far from the
applicable standards when faced with a relatively large domain shift. In this
paper, we leverage the Segment Anything Model (SAM) to SDG to greatly improve
the ability of generalization. Specifically, we introduce a parallel framework,
the source images are sent into the SAM module and normal segmentation module
respectively. To reduce the calculation resources, we apply a merging strategy
before sending images to the SAM module. We extract the bounding boxes from the
segmentation module and send the refined version as prompts to the SAM module.
We evaluate our model on a classic DG dataset and achieve competitive results
compared to other state-of-the-art DG methods. Furthermore, We conducted a
series of ablation experiments to prove the effectiveness of the proposed
method. The code is publicly available at https://github.com/SARIHUST/SAMMed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02094">Federated Class-Incremental Learning with Prototype Guided Transformer. (arXiv:2401.02094v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1">Haiyang Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1">Fei Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wenzhuo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xu-Yao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Cheng-Lin Liu</a></p>
<p>Existing federated learning methods have effectively addressed decentralized
learning in scenarios involving data privacy and non-IID data. However, in
real-world situations, each client dynamically learns new classes, requiring
the global model to maintain discriminative capabilities for both new and old
classes. To effectively mitigate the effects of catastrophic forgetting and
data heterogeneity under low communication costs, we designed a simple and
effective method named PLoRA. On the one hand, we adopt prototype learning to
learn better feature representations and leverage the heuristic information
between prototypes and class features to design a prototype re-weight module to
solve the classifier bias caused by data heterogeneity without retraining the
classification layer. On the other hand, our approach utilizes a pre-trained
model as the backbone and utilizes LoRA to fine-tune with a tiny amount of
parameters when learning new classes. Moreover, PLoRA does not rely on
similarity-based module selection strategies, thereby further reducing
communication overhead. Experimental results on standard datasets indicate that
our method outperforms the state-of-the-art approaches significantly. More
importantly, our method exhibits strong robustness and superiority in various
scenarios and degrees of data heterogeneity. Our code will be publicly
available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02097">Preserving Image Properties Through Initializations in Diffusion Models. (arXiv:2401.02097v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jeffrey Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1">Shao-Yu Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Kedan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Forsyth_D/0/1/0/all/0/1">David Forsyth</a></p>
<p>Retail photography imposes specific requirements on images. For instance,
images may need uniform background colors, consistent model poses, centered
products, and consistent lighting. Minor deviations from these standards impact
a site's aesthetic appeal, making the images unsuitable for use. We show that
Stable Diffusion methods, as currently applied, do not respect these
requirements. The usual practice of training the denoiser with a very noisy
image and starting inference with a sample of pure noise leads to inconsistent
generated images during inference. This inconsistency occurs because it is easy
to tell the difference between samples of the training and inference
distributions. As a result, a network trained with centered retail product
images with uniform backgrounds generates images with erratic backgrounds. The
problem is easily fixed by initializing inference with samples from an
approximation of noisy images. However, in using such an approximation, the
joint distribution of text and noisy image at inference time still slightly
differs from that at training time. This discrepancy is corrected by training
the network with samples from the approximate noisy image distribution.
Extensive experiments on real application data show significant qualitative and
quantitative improvements in performance from adopting these procedures.
Finally, our procedure can interact well with other control-based methods to
further enhance the controllability of diffusion-based methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02099">CLAPP: Contrastive Language-Audio Pre-training in Passive Underwater Vessel Classification. (arXiv:2401.02099v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zeyu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jingsheng Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1">Tong Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_S/0/1/0/all/0/1">Suncheng Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruan_J/0/1/0/all/0/1">Jiacheng Ruan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Ting Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1">Yuzhuo Fu</a></p>
<p>Existing research on audio classification faces challenges in recognizing
attributes of passive underwater vessel scenarios and lacks well-annotated
datasets due to data privacy concerns. In this study, we introduce CLAPP
(Contrastive Language-Audio Pre-training in Passive Underwater Vessel
Classification), a novel model. Our aim is to train a neural network using a
wide range of vessel audio and vessel state text pairs obtained from an
oceanship dataset. CLAPP is capable of directly learning from raw vessel audio
data and, when available, from carefully curated labels, enabling improved
recognition of vessel attributes in passive underwater vessel scenarios.
Model's zero-shot capability allows predicting the most relevant vessel state
description for a given vessel audio, without directly optimizing for the task.
Our approach aims to solve 2 challenges: vessel audio-text classification and
passive underwater vessel audio attribute recognition. The proposed method
achieves new state-of-the-art results on both Deepship and Shipsear public
datasets, with a notable margin of about 7%-13% for accuracy compared to prior
methods on zero-shot task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02110">Significance of Anatomical Constraints in Virtual Try-On. (arXiv:2401.02110v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Roy_D/0/1/0/all/0/1">Debapriya Roy</a>, <a href="http://arxiv.org/find/cs/1/au:+Santra_S/0/1/0/all/0/1">Sanchayan Santra</a>, <a href="http://arxiv.org/find/cs/1/au:+Mukherjee_D/0/1/0/all/0/1">Diganta Mukherjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Chanda_B/0/1/0/all/0/1">Bhabatosh Chanda</a></p>
<p>The system of Virtual Try-ON (VTON) allows a user to try a product virtually.
In general, a VTON system takes a clothing source and a person's image to
predict the try-on output of the person in the given clothing. Although
existing methods perform well for simple poses, in case of bent or crossed arms
posture or when there is a significant difference between the alignment of the
source clothing and the pose of the target person, these methods fail by
generating inaccurate clothing deformations. In the VTON methods that employ
Thin Plate Spline (TPS) based clothing transformations, this mainly occurs for
two reasons - (1)~the second-order smoothness constraint of TPS that restricts
the bending of the object plane. (2)~Overlaps among different clothing parts
(e.g., sleeves and torso) can not be modeled by a single TPS transformation, as
it assumes the clothing as a single planar object; therefore, disregards the
independence of movement of different clothing parts. To this end, we make two
major contributions. Concerning the bending limitations of TPS, we propose a
human AnaTomy-Aware Geometric (ATAG) transformation. Regarding the overlap
issue, we propose a part-based warping approach that divides the clothing into
independently warpable parts to warp them separately and later combine them.
Extensive analysis shows the efficacy of this approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02113">Source-Free Online Domain Adaptive Semantic Segmentation of Satellite Images under Image Degradation. (arXiv:2401.02113v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Niloy_F/0/1/0/all/0/1">Fahim Faisal Niloy</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhaumik_K/0/1/0/all/0/1">Kishor Kumar Bhaumik</a>, <a href="http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1">Simon S. Woo</a></p>
<p>Online adaptation to distribution shifts in satellite image segmentation
stands as a crucial yet underexplored problem. In this paper, we address
source-free and online domain adaptation, i.e., test-time adaptation (TTA), for
satellite images, with the focus on mitigating distribution shifts caused by
various forms of image degradation. Towards achieving this goal, we propose a
novel TTA approach involving two effective strategies. First, we progressively
estimate the global Batch Normalization (BN) statistics of the target
distribution with incoming data stream. Leveraging these statistics during
inference has the ability to effectively reduce domain gap. Furthermore, we
enhance prediction quality by refining the predicted masks using global class
centers. Both strategies employ dynamic momentum for fast and stable
convergence. Notably, our method is backpropagation-free and hence fast and
lightweight, making it highly suitable for on-the-fly adaptation to new domain.
Through comprehensive experiments across various domain adaptation scenarios,
we demonstrate the robust performance of our method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02117">Mobile ALOHA: Learning Bimanual Mobile Manipulation with Low-Cost Whole-Body Teleoperation. (arXiv:2401.02117v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1">Zipeng Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1">Tony Z. Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1">Chelsea Finn</a></p>
<p>Imitation learning from human demonstrations has shown impressive performance
in robotics. However, most results focus on table-top manipulation, lacking the
mobility and dexterity necessary for generally useful tasks. In this work, we
develop a system for imitating mobile manipulation tasks that are bimanual and
require whole-body control. We first present Mobile ALOHA, a low-cost and
whole-body teleoperation system for data collection. It augments the ALOHA
system with a mobile base, and a whole-body teleoperation interface. Using data
collected with Mobile ALOHA, we then perform supervised behavior cloning and
find that co-training with existing static ALOHA datasets boosts performance on
mobile manipulation tasks. With 50 demonstrations for each task, co-training
can increase success rates by up to 90%, allowing Mobile ALOHA to autonomously
complete complex mobile manipulation tasks such as sauteing and serving a piece
of shrimp, opening a two-door wall cabinet to store heavy cooking pots, calling
and entering an elevator, and lightly rinsing a used pan using a kitchen
faucet. Project website: https://mobile-aloha.github.io
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02126">Unified Diffusion-Based Rigid and Non-Rigid Editing with Text and Image Guidance. (arXiv:2401.02126v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiacheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1">Ping Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Wei Xu</a></p>
<p>Existing text-to-image editing methods tend to excel either in rigid or
non-rigid editing but encounter challenges when combining both, resulting in
misaligned outputs with the provided text prompts. In addition, integrating
reference images for control remains challenging. To address these issues, we
present a versatile image editing framework capable of executing both rigid and
non-rigid edits, guided by either textual prompts or reference images. We
leverage a dual-path injection scheme to handle diverse editing scenarios and
introduce an integrated self-attention mechanism for fusion of appearance and
structural information. To mitigate potential visual artifacts, we further
employ latent fusion techniques to adjust intermediate latents. Compared to
previous work, our approach represents a significant advance in achieving
precise and versatile image editing. Comprehensive experiments validate the
efficacy of our method, showcasing competitive or superior results in
text-based editing and appearance transfer tasks, encompassing both rigid and
non-rigid settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02137">SyCoCa: Symmetrizing Contrastive Captioners with Attentive Masking for Multimodal Alignment. (arXiv:2401.02137v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1">Ziping Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1">Furong Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jian Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Ming Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1">Qingpei Guo</a></p>
<p>Multimodal alignment between language and vision is the fundamental topic in
current vision-language model research. Contrastive Captioners (CoCa), as a
representative method, integrates Contrastive Language-Image Pretraining (CLIP)
and Image Caption (IC) into a unified framework, resulting in impressive
results. CLIP imposes a bidirectional constraints on global representation of
entire images and sentences. Although IC conducts an unidirectional
image-to-text generation on local representation, it lacks any constraint on
local text-to-image reconstruction, which limits the ability to understand
images at a fine-grained level when aligned with texts. To achieve multimodal
alignment from both global and local perspectives, this paper proposes
Symmetrizing Contrastive Captioners (SyCoCa), which introduces bidirectional
interactions on images and texts across the global and local representation
levels. Specifically, we expand a Text-Guided Masked Image Modeling (TG-MIM)
head based on ITC and IC heads. The improved SyCoCa can further leverage
textual cues to reconstruct contextual images and visual cues to predict
textual contents. When implementing bidirectional local interactions, the local
contents of images tend to be cluttered or unrelated to their textual
descriptions. Thus, we employ an attentive masking strategy to select effective
image patches for interaction. Extensive experiments on five vision-language
tasks, including image-text retrieval, image-captioning, visual question
answering, and zero-shot/finetuned image classification, validate the
effectiveness of our proposed method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02138">Explore Human Parsing Modality for Action Recognition. (arXiv:2401.02138v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jinfu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1">Runwei Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1">Yuhang Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_N/0/1/0/all/0/1">Nan Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1">Fanyang Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1">Shen Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Mengyuan Liu</a></p>
<p>Multimodal-based action recognition methods have achieved high success using
pose and RGB modality. However, skeletons sequences lack appearance depiction
and RGB images suffer irrelevant noise due to modality limitations. To address
this, we introduce human parsing feature map as a novel modality, since it can
selectively retain effective semantic features of the body parts, while
filtering out most irrelevant noise. We propose a new dual-branch framework
called Ensemble Human Parsing and Pose Network (EPP-Net), which is the first to
leverage both skeletons and human parsing modalities for action recognition.
The first human pose branch feeds robust skeletons in graph convolutional
network to model pose features, while the second human parsing branch also
leverages depictive parsing feature maps to model parsing festures via
convolutional backbones. The two high-level features will be effectively
combined through a late fusion strategy for better action recognition.
Extensive experiments on NTU RGB+D and NTU RGB+D 120 benchmarks consistently
verify the effectiveness of our proposed EPP-Net, which outperforms the
existing action recognition methods. Our code is available at:
https://github.com/liujf69/EPP-Net-Action.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02141">Bayesian Intrinsic Groupwise Image Registration: Unsupervised Disentanglement of Anatomy and Geometry. (arXiv:2401.02141v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1">Xinzhe Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shapiro_L/0/1/0/all/0/1">Linda Shapiro</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1">Chun Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1">Jianfeng Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_X/0/1/0/all/0/1">Xiahai Zhuang</a></p>
<p>This article presents a general Bayesian learning framework for multi-modal
groupwise registration on medical images. The method builds on probabilistic
modelling of the image generative process, where the underlying common anatomy
and geometric variations of the observed images are explicitly disentangled as
latent variables. Thus, groupwise registration is achieved through the solution
to Bayesian inference. We propose a novel hierarchical variational
auto-encoding architecture to realize the inference procedure of the latent
variables, where the registration parameters can be calculated in a
mathematically interpretable fashion. Remarkably, this new paradigm can learn
groupwise registration in an unsupervised closed-loop self-reconstruction
process, sparing the burden of designing complex intensity-based similarity
measures. The computationally efficient disentangled architecture is also
inherently scalable and flexible, allowing for groupwise registration on
large-scale image groups with variable sizes. Furthermore, the inferred
structural representations from disentanglement learning are capable of
capturing the latent anatomy of the observations with visual semantics.
Extensive experiments were conducted to validate the proposed framework,
including four datasets from cardiac, brain and abdominal medical images. The
results have demonstrated the superiority of our method over conventional
similarity-based approaches in terms of accuracy, efficiency, scalability and
interpretability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02142">GUESS:GradUally Enriching SyntheSis for Text-Driven Human Motion Generation. (arXiv:2401.02142v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xuehao Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1">Zhenyu Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1">Shaoyi Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1">Zhongqian Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yang Wu</a></p>
<p>In this paper, we propose a novel cascaded diffusion-based generative
framework for text-driven human motion synthesis, which exploits a strategy
named GradUally Enriching SyntheSis (GUESS as its abbreviation). The strategy
sets up generation objectives by grouping body joints of detailed skeletons in
close semantic proximity together and then replacing each of such joint group
with a single body-part node. Such an operation recursively abstracts a human
pose to coarser and coarser skeletons at multiple granularity levels. With
gradually increasing the abstraction level, human motion becomes more and more
concise and stable, significantly benefiting the cross-modal motion synthesis
task. The whole text-driven human motion synthesis problem is then divided into
multiple abstraction levels and solved with a multi-stage generation framework
with a cascaded latent diffusion model: an initial generator first generates
the coarsest human motion guess from a given text description; then, a series
of successive generators gradually enrich the motion details based on the
textual description and the previous synthesized results. Notably, we further
integrate GUESS with the proposed dynamic multi-condition fusion mechanism to
dynamically balance the cooperative effects of the given textual condition and
synthesized coarse motion prompt in different generation stages. Extensive
experiments on large-scale datasets verify that GUESS outperforms existing
state-of-the-art methods by large margins in terms of accuracy, realisticness,
and diversity. Code is available at https://github.com/Xuehao-Gao/GUESS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02147">Exploring Boundary of GPT-4V on Marine Analysis: A Preliminary Case Study. (arXiv:2401.02147v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Ziqiang Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yiwei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jipeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1">Tuan-Anh Vu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1">Huimin Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Tim_Y/0/1/0/all/0/1">Yue Him Wong Tim</a>, <a href="http://arxiv.org/find/cs/1/au:+Yeung_S/0/1/0/all/0/1">Sai-Kit Yeung</a></p>
<p>Large language models (LLMs) have demonstrated a powerful ability to answer
various queries as a general-purpose assistant. The continuous multi-modal
large language models (MLLM) empower LLMs with the ability to perceive visual
signals. The launch of GPT-4 (Generative Pre-trained Transformers) has
generated significant interest in the research communities. GPT-4V(ison) has
demonstrated significant power in both academia and industry fields, as a focal
point in a new artificial intelligence generation. Though significant success
was achieved by GPT-4V, exploring MLLMs in domain-specific analysis (e.g.,
marine analysis) that required domain-specific knowledge and expertise has
gained less attention. In this study, we carry out the preliminary and
comprehensive case study of utilizing GPT-4V for marine analysis. This report
conducts a systematic evaluation of existing GPT-4V, assessing the performance
of GPT-4V on marine research and also setting a new standard for future
developments in MLLMs. The experimental results of GPT-4V show that the
responses generated by GPT-4V are still far away from satisfying the
domain-specific requirements of the marine professions. All images and prompts
used in this study will be available at
https://github.com/hkust-vgd/Marine_GPT-4V_Eval
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02150">Marginal Debiased Network for Fair Visual Recognition. (arXiv:2401.02150v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Mei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1">Weihong Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1">Sen Su</a></p>
<p>Deep neural networks (DNNs) are often prone to learn the spurious
correlations between target classes and bias attributes, like gender and race,
inherent in a major portion of training data (bias-aligned samples), thus
showing unfair behavior and arising controversy in the modern pluralistic and
egalitarian society. In this paper, we propose a novel marginal debiased
network (MDN) to learn debiased representations. More specifically, a marginal
softmax loss (MSL) is designed by introducing the idea of margin penalty into
the fairness problem, which assigns a larger margin for bias-conflicting
samples (data without spurious correlations) than for bias-aligned ones, so as
to deemphasize the spurious correlations and improve generalization on unbiased
test criteria. To determine the margins, our MDN is optimized through a meta
learning framework. We propose a meta equalized loss (MEL) to perceive the
model fairness, and adaptively update the margin parameters by metaoptimization
which requires the trained model guided by the optimal margins should minimize
MEL computed on an unbiased meta-validation set. Extensive experiments on
BiasedMNIST, Corrupted CIFAR-10, CelebA and UTK-Face datasets demonstrate that
our MDN can achieve a remarkable performance on under-represented samples and
obtain superior debiased results against the previous approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02151">Frequency-Adaptive Pan-Sharpening with Mixture of Experts. (arXiv:2401.02151v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xuanhua He</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1">Keyu Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1">Rui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1">Chengjun Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jie Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1">Man Zhou</a></p>
<p>Pan-sharpening involves reconstructing missing high-frequency information in
multi-spectral images with low spatial resolution, using a higher-resolution
panchromatic image as guidance. Although the inborn connection with frequency
domain, existing pan-sharpening research has not almost investigated the
potential solution upon frequency domain. To this end, we propose a novel
Frequency Adaptive Mixture of Experts (FAME) learning framework for
pan-sharpening, which consists of three key components: the Adaptive Frequency
Separation Prediction Module, the Sub-Frequency Learning Expert Module, and the
Expert Mixture Module. In detail, the first leverages the discrete cosine
transform to perform frequency separation by predicting the frequency mask. On
the basis of generated mask, the second with low-frequency MOE and
high-frequency MOE takes account for enabling the effective low-frequency and
high-frequency information reconstruction. Followed by, the final fusion module
dynamically weights high-frequency and low-frequency MOE knowledge to adapt to
remote sensing images with significant content variations. Quantitative and
qualitative experiments over multiple datasets demonstrate that our method
performs the best against other state-of-the-art ones and comprises a strong
generalization ability for real-world scenes. Code will be made publicly at
\url{https://github.com/alexhe101/FAME-Net}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02161">Enhancing RAW-to-sRGB with Decoupled Style Structure in Fourier Domain. (arXiv:2401.02161v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xuanhua He</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1">Tao Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guoli Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zejin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Run Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1">Keyu Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Ziyi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1">Rui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1">Chenjun Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jie Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1">Man Zhou</a></p>
<p>RAW to sRGB mapping, which aims to convert RAW images from smartphones into
RGB form equivalent to that of Digital Single-Lens Reflex (DSLR) cameras, has
become an important area of research. However, current methods often ignore the
difference between cell phone RAW images and DSLR camera RGB images, a
difference that goes beyond the color matrix and extends to spatial structure
due to resolution variations. Recent methods directly rebuild color mapping and
spatial structure via shared deep representation, limiting optimal performance.
Inspired by Image Signal Processing (ISP) pipeline, which distinguishes image
restoration and enhancement, we present a novel Neural ISP framework, named
FourierISP. This approach breaks the image down into style and structure within
the frequency domain, allowing for independent optimization. FourierISP is
comprised of three subnetworks: Phase Enhance Subnet for structural refinement,
Amplitude Refine Subnet for color learning, and Color Adaptation Subnet for
blending them in a smooth manner. This approach sharpens both color and
structure, and extensive evaluations across varied datasets confirm that our
approach realizes state-of-the-art results. Code will be available at
~\url{https://github.com/alexhe101/FourierISP}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02162">Frequency Domain Nuances Mining for Visible-Infrared Person Re-identification. (arXiv:2401.02162v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yukang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yang Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1">Yan Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hanzi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xuelong Li</a></p>
<p>The key of visible-infrared person re-identification (VIReID) lies in how to
minimize the modality discrepancy between visible and infrared images. Existing
methods mainly exploit the spatial information while ignoring the
discriminative frequency information. To address this issue, this paper aims to
reduce the modality discrepancy from the frequency domain perspective.
Specifically, we propose a novel Frequency Domain Nuances Mining (FDNM) method
to explore the cross-modality frequency domain information, which mainly
includes an amplitude guided phase (AGP) module and an amplitude nuances mining
(ANM) module. These two modules are mutually beneficial to jointly explore
frequency domain visible-infrared nuances, thereby effectively reducing the
modality discrepancy in the frequency domain. Besides, we propose a
center-guided nuances mining loss to encourage the ANM module to preserve
discriminative identity information while discovering diverse cross-modality
nuances. To the best of our knowledge, this is the first work that explores the
potential frequency information for VIReID research. Extensive experiments show
that the proposed FDNM has significant advantages in improving the performance
of VIReID. Specifically, our method outperforms the second-best method by 5.2\%
in Rank-1 accuracy and 5.8\% in mAP on the SYSU-MM01 dataset under the indoor
search mode, respectively. Besides, we also validate the effectiveness and
generalization of our method on the challenging visible-infrared face
recognition task. \textcolor{magenta}{The code will be available.}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02173">Prompt Decoupling for Text-to-Image Person Re-identification. (arXiv:2401.02173v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Weihao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1">Lei Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_P/0/1/0/all/0/1">Pingyang Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yan Zhang</a></p>
<p>Text-to-image person re-identification (TIReID) aims to retrieve the target
person from an image gallery via a textual description query. Recently,
pre-trained vision-language models like CLIP have attracted significant
attention and have been widely utilized for this task due to their robust
capacity for semantic concept learning and rich multi-modal knowledge. However,
recent CLIP-based TIReID methods commonly rely on direct fine-tuning of the
entire network to adapt the CLIP model for the TIReID task. Although these
methods show competitive performance on this topic, they are suboptimal as they
necessitate simultaneous domain adaptation and task adaptation. To address this
issue, we attempt to decouple these two processes during the training stage.
Specifically, we introduce the prompt tuning strategy to enable domain
adaptation and propose a two-stage training approach to disentangle domain
adaptation from task adaptation. In the first stage, we freeze the two encoders
from CLIP and solely focus on optimizing the prompts to alleviate domain gap
between the original training data of CLIP and downstream tasks. In the second
stage, we maintain the fixed prompts and fine-tune the CLIP model to prioritize
capturing fine-grained information, which is more suitable for TIReID task.
Finally, we evaluate the effectiveness of our method on three widely used
datasets. Compared to the directly fine-tuned approach, our method achieves
significant improvements.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02192">Nodule detection and generation on chest X-rays: NODE21 Challenge. (arXiv:2401.02192v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Sogancioglu_E/0/1/0/all/0/1">Ecem Sogancioglu</a>, <a href="http://arxiv.org/find/eess/1/au:+Ginneken_B/0/1/0/all/0/1">Bram van Ginneken</a>, <a href="http://arxiv.org/find/eess/1/au:+Behrendt_F/0/1/0/all/0/1">Finn Behrendt</a>, <a href="http://arxiv.org/find/eess/1/au:+Bengs_M/0/1/0/all/0/1">Marcel Bengs</a>, <a href="http://arxiv.org/find/eess/1/au:+Schlaefer_A/0/1/0/all/0/1">Alexander Schlaefer</a>, <a href="http://arxiv.org/find/eess/1/au:+Radu_M/0/1/0/all/0/1">Miron Radu</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_D/0/1/0/all/0/1">Di Xu</a>, <a href="http://arxiv.org/find/eess/1/au:+Sheng_K/0/1/0/all/0/1">Ke Sheng</a>, <a href="http://arxiv.org/find/eess/1/au:+Scalzo_F/0/1/0/all/0/1">Fabien Scalzo</a>, <a href="http://arxiv.org/find/eess/1/au:+Marcus_E/0/1/0/all/0/1">Eric Marcus</a>, <a href="http://arxiv.org/find/eess/1/au:+Papa_S/0/1/0/all/0/1">Samuele Papa</a>, <a href="http://arxiv.org/find/eess/1/au:+Teuwen_J/0/1/0/all/0/1">Jonas Teuwen</a>, <a href="http://arxiv.org/find/eess/1/au:+Scholten_E/0/1/0/all/0/1">Ernst Th. Scholten</a>, <a href="http://arxiv.org/find/eess/1/au:+Schalekamp_S/0/1/0/all/0/1">Steven Schalekamp</a>, <a href="http://arxiv.org/find/eess/1/au:+Hendrix_N/0/1/0/all/0/1">Nils Hendrix</a>, <a href="http://arxiv.org/find/eess/1/au:+Jacobs_C/0/1/0/all/0/1">Colin Jacobs</a>, <a href="http://arxiv.org/find/eess/1/au:+Hendrix_W/0/1/0/all/0/1">Ward Hendrix</a>, <a href="http://arxiv.org/find/eess/1/au:+Sanchez_C/0/1/0/all/0/1">Clara I S&#xe1;nchez</a>, <a href="http://arxiv.org/find/eess/1/au:+Murphy_K/0/1/0/all/0/1">Keelin Murphy</a></p>
<p>Pulmonary nodules may be an early manifestation of lung cancer, the leading
cause of cancer-related deaths among both men and women. Numerous studies have
established that deep learning methods can yield high-performance levels in the
detection of lung nodules in chest X-rays. However, the lack of gold-standard
public datasets slows down the progression of the research and prevents
benchmarking of methods for this task. To address this, we organized a public
research challenge, NODE21, aimed at the detection and generation of lung
nodules in chest X-rays. While the detection track assesses state-of-the-art
nodule detection systems, the generation track determines the utility of nodule
generation algorithms to augment training data and hence improve the
performance of the detection systems. This paper summarizes the results of the
NODE21 challenge and performs extensive additional experiments to examine the
impact of the synthetically generated nodule training images on the detection
algorithm performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02241">Slot-guided Volumetric Object Radiance Fields. (arXiv:2401.02241v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qi_D/0/1/0/all/0/1">Di Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1">Tong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiangyu Zhang</a></p>
<p>We present a novel framework for 3D object-centric representation learning.
Our approach effectively decomposes complex scenes into individual objects from
a single image in an unsupervised fashion. This method, called slot-guided
Volumetric Object Radiance Fields (sVORF), composes volumetric object radiance
fields with object slots as a guidance to implement unsupervised 3D scene
decomposition. Specifically, sVORF obtains object slots from a single image via
a transformer module, maps these slots to volumetric object radiance fields
with a hypernetwork and composes object radiance fields with the guidance of
object slots at a 3D location. Moreover, sVORF significantly reduces memory
requirement due to small-sized pixel rendering during training. We demonstrate
the effectiveness of our approach by showing top results in scene decomposition
and generation tasks of complex synthetic datasets (e.g., Room-Diverse).
Furthermore, we also confirm the potential of sVORF to segment objects in
real-world scenes (e.g., the LLFF dataset). We hope our approach can provide
preliminary understanding of the physical world and help ease future research
in 3D object-centric representation learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02274">ShapeAug: Occlusion Augmentation for Event Camera Data. (arXiv:2401.02274v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bendig_K/0/1/0/all/0/1">Katharina Bendig</a>, <a href="http://arxiv.org/find/cs/1/au:+Schuster_R/0/1/0/all/0/1">Ren&#xe9; Schuster</a>, <a href="http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1">Didier Stricker</a></p>
<p>Recently, Dynamic Vision Sensors (DVSs) sparked a lot of interest due to
their inherent advantages over conventional RGB cameras. These advantages
include a low latency, a high dynamic range and a low energy consumption.
Nevertheless, the processing of DVS data using Deep Learning (DL) methods
remains a challenge, particularly since the availability of event training data
is still limited. This leads to a need for event data augmentation techniques
in order to improve accuracy as well as to avoid over-fitting on the training
data. Another challenge especially in real world automotive applications is
occlusion, meaning one object is hindering the view onto the object behind it.
In this paper, we present a novel event data augmentation approach, which
addresses this problem by introducing synthetic events for randomly moving
objects in a scene. We test our method on multiple DVS classification datasets,
resulting in an relative improvement of up to 6.5 % in top1-accuracy. Moreover,
we apply our augmentation technique on the real world Gen1 Automotive Event
Dataset for object detection, where we especially improve the detection of
pedestrians by up to 5 %.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02278">Lightweight Fish Classification Model for Sustainable Marine Management: Indonesian Case. (arXiv:2401.02278v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kurniawan_F/0/1/0/all/0/1">Febrian Kurniawan</a>, <a href="http://arxiv.org/find/cs/1/au:+Satrya_G/0/1/0/all/0/1">Gandeva Bayu Satrya</a>, <a href="http://arxiv.org/find/cs/1/au:+Kamalov_F/0/1/0/all/0/1">Firuz Kamalov</a></p>
<p>The enormous demand for seafood products has led to exploitation of marine
resources and near-extinction of some species. In particular, overfishing is
one the main issues in sustainable marine development. In alignment with the
protection of marine resources and sustainable fishing, this study proposes to
advance fish classification techniques that support identifying protected fish
species using state-of-the-art machine learning. We use a custom modification
of the MobileNet model to design a lightweight classifier called M-MobileNet
that is capable of running on limited hardware. As part of the study, we
compiled a labeled dataset of 37,462 images of fish found in the waters of the
Indonesian archipelago. The proposed model is trained on the dataset to
classify images of the captured fish into their species and give
recommendations on whether they are consumable or not. Our modified MobileNet
model uses only 50\% of the top layer parameters with about 42% GTX 860M
utility and achieves up to 97% accuracy in fish classification and determining
its consumability. Given the limited computing capacity available on many
fishing vessels, the proposed model provides a practical solution to on-site
fish classification. In addition, synchronized implementation of the proposed
model on multiple vessels can supply valuable information about the movement
and location of different species of fish.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02281">PEGASUS: Physically Enhanced Gaussian Splatting Simulation System for 6DOF Object Pose Dataset Generation. (arXiv:2401.02281v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Meyer_L/0/1/0/all/0/1">Lukas Meyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Erich_F/0/1/0/all/0/1">Floris Erich</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoshiyasu_Y/0/1/0/all/0/1">Yusuke Yoshiyasu</a>, <a href="http://arxiv.org/find/cs/1/au:+Stamminger_M/0/1/0/all/0/1">Marc Stamminger</a>, <a href="http://arxiv.org/find/cs/1/au:+Ando_N/0/1/0/all/0/1">Noriaki Ando</a>, <a href="http://arxiv.org/find/cs/1/au:+Domae_Y/0/1/0/all/0/1">Yukiyasu Domae</a></p>
<p>We introduce Physically Enhanced Gaussian Splatting Simulation System
(PEGASUS) for 6DOF object pose dataset generation, a versatile dataset
generator based on 3D Gaussian Splatting. Environment and object
representations can be easily obtained using commodity cameras to reconstruct
with Gaussian Splatting. PEGASUS allows the composition of new scenes by
merging the respective underlying Gaussian Splatting point cloud of an
environment with one or multiple objects. Leveraging a physics engine enables
the simulation of natural object placement within a scene through interaction
between meshes extracted for the objects and the environment. Consequently, an
extensive amount of new scenes - static or dynamic - can be created by
combining different environments and objects. By rendering scenes from various
perspectives, diverse data points such as RGB images, depth maps, semantic
masks, and 6DoF object poses can be extracted. Our study demonstrates that
training on data generated by PEGASUS enables pose estimation networks to
successfully transfer from synthetic data to real-world data. Moreover, we
introduce the Ramen dataset, comprising 30 Japanese cup noodle items. This
dataset includes spherical scans that captures images from both object
hemisphere and the Gaussian Splatting reconstruction, making them compatible
with PEGASUS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02287">Distillation-based fabric anomaly detection. (arXiv:2401.02287v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Thomine_S/0/1/0/all/0/1">Simon Thomine</a>, <a href="http://arxiv.org/find/cs/1/au:+Snoussi_H/0/1/0/all/0/1">Hichem Snoussi</a></p>
<p>Unsupervised texture anomaly detection has been a concerning topic in a vast
amount of industrial processes. Patterned textures inspection, particularly in
the context of fabric defect detection, is indeed a widely encountered use
case. This task involves handling a diverse spectrum of colors and textile
types, encompassing a wide range of fabrics. Given the extensive variability in
colors, textures, and defect types, fabric defect detection poses a complex and
challenging problem in the field of patterned textures inspection. In this
article, we propose a knowledge distillation-based approach tailored
specifically for addressing the challenge of unsupervised anomaly detection in
textures resembling fabrics. Our method aims to redefine the recently
introduced reverse distillation approach, which advocates for an
encoder-decoder design to mitigate classifier bias and to prevent the student
from reconstructing anomalies. In this study, we present a new reverse
distillation technique for the specific task of fabric defect detection. Our
approach involves a meticulous design selection that strategically highlights
high-level features. To demonstrate the capabilities of our approach both in
terms of performance and inference speed, we conducted a series of experiments
on multiple texture datasets, including MVTEC AD, AITEX, and TILDA, alongside
conducting experiments on a dataset acquired from a textile manufacturing
facility. The main contributions of this paper are the following: a robust
texture anomaly detector utilizing a reverse knowledge-distillation technique
suitable for both anomaly detection and domain generalization and a novel
dataset encompassing a diverse range of fabrics and defects.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02292">GridFormer: Point-Grid Transformer for Surface Reconstruction. (arXiv:2401.02292v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shengtao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1">Ge Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yudong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yu-Shen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_M/0/1/0/all/0/1">Ming Gu</a></p>
<p>Implicit neural networks have emerged as a crucial technology in 3D surface
reconstruction. To reconstruct continuous surfaces from discrete point clouds,
encoding the input points into regular grid features (plane or volume) has been
commonly employed in existing approaches. However, these methods typically use
the grid as an index for uniformly scattering point features. Compared with the
irregular point features, the regular grid features may sacrifice some
reconstruction details but improve efficiency. To take full advantage of these
two types of features, we introduce a novel and high-efficiency attention
mechanism between the grid and point features named Point-Grid Transformer
(GridFormer). This mechanism treats the grid as a transfer point connecting the
space and point cloud. Our method maximizes the spatial expressiveness of grid
features and maintains computational efficiency. Furthermore, optimizing
predictions over the entire space could potentially result in blurred
boundaries. To address this issue, we further propose a boundary optimization
strategy incorporating margin binary cross-entropy loss and boundary sampling.
This approach enables us to achieve a more precise representation of the object
structure. Our experiments validate that our method is effective and
outperforms the state-of-the-art approaches under widely used benchmarks by
producing more precise geometry reconstructions. The code is available at
https://github.com/list17/GridFormer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02309">TR-DETR: Task-Reciprocal Transformer for Joint Moment Retrieval and Highlight Detection. (arXiv:2401.02309v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Hao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1">Mingyao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wenjing Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1">Wei Xie</a></p>
<p>Video moment retrieval (MR) and highlight detection (HD) based on natural
language queries are two highly related tasks, which aim to obtain relevant
moments within videos and highlight scores of each video clip. Recently,
several methods have been devoted to building DETR-based networks to solve both
MR and HD jointly. These methods simply add two separate task heads after
multi-modal feature extraction and feature interaction, achieving good
performance. Nevertheless, these approaches underutilize the reciprocal
relationship between two tasks. In this paper, we propose a task-reciprocal
transformer based on DETR (TR-DETR) that focuses on exploring the inherent
reciprocity between MR and HD. Specifically, a local-global multi-modal
alignment module is first built to align features from diverse modalities into
a shared latent space. Subsequently, a visual feature refinement is designed to
eliminate query-irrelevant information from visual features for modal
interaction. Finally, a task cooperation module is constructed to refine the
retrieval pipeline and the highlight score prediction process by utilizing the
reciprocity between MR and HD. Comprehensive experiments on QVHighlights,
Charades-STA and TVSum datasets demonstrate that TR-DETR outperforms existing
state-of-the-art methods. Codes are available at
\url{https://github.com/mingyao1120/TR-DETR}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02313">SuperEdge: Towards a Generalization Model for Self-Supervised Edge Detection. (arXiv:2401.02313v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kai_L/0/1/0/all/0/1">Leng Kai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhijie_Z/0/1/0/all/0/1">Zhang Zhijie</a>, <a href="http://arxiv.org/find/cs/1/au:+Jie_L/0/1/0/all/0/1">Liu Jie</a>, <a href="http://arxiv.org/find/cs/1/au:+Boukhers_Z/0/1/0/all/0/1">Zed Boukhers</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1">Sui Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Cong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhijun_L/0/1/0/all/0/1">Li Zhijun</a></p>
<p>Edge detection is a fundamental technique in various computer vision tasks.
Edges are indeed effectively delineated by pixel discontinuity and can offer
reliable structural information even in textureless areas. State-of-the-art
heavily relies on pixel-wise annotations, which are labor-intensive and subject
to inconsistencies when acquired manually. In this work, we propose a novel
self-supervised approach for edge detection that employs a multi-level,
multi-homography technique to transfer annotations from synthetic to real-world
datasets. To fully leverage the generated edge annotations, we developed
SuperEdge, a streamlined yet efficient model capable of concurrently extracting
edges at pixel-level and object-level granularity. Thanks to self-supervised
training, our method eliminates the dependency on manual annotated edge labels,
thereby enhancing its generalizability across diverse datasets. Comparative
evaluations reveal that SuperEdge advances edge detection, demonstrating
improvements of 4.9% in ODS and 3.3% in OIS over the existing STEdge method on
BIPEDv2.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02317">BA-SAM: Scalable Bias-Mode Attention Mask for Segment Anything Model. (arXiv:2401.02317v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Yiran Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1">Qianyu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiangtai Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1">Deng-Ping Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1">Xuequan Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1">Lizhuang Ma</a></p>
<p>In this paper, we address the challenge of image resolution variation for the
Segment Anything Model (SAM). SAM, known for its zero-shot generalizability,
exhibits a performance degradation when faced with datasets with varying image
sizes. Previous approaches tend to resize the image to a fixed size or adopt
structure modifications, hindering the preservation of SAM's rich prior
knowledge. Besides, such task-specific tuning necessitates a complete
retraining of the model, which is cost-expensive and unacceptable for
deployment in the downstream tasks. In this paper, we reformulate this issue as
a length extrapolation problem, where token sequence length varies while
maintaining a consistent patch size for images of different sizes. To this end,
we propose Scalable Bias-Mode Attention Mask (BA-SAM) to enhance SAM's
adaptability to varying image resolutions while eliminating the need for
structure modifications. Firstly, we introduce a new scaling factor to ensure
consistent magnitude in the attention layer's dot product values when the token
sequence length changes. Secondly, we present a bias-mode attention mask that
allows each token to prioritize neighboring information, mitigating the impact
of untrained distant information. Our BA-SAM demonstrates efficacy in two
scenarios: zero-shot and fine-tuning. Extensive evaluation on diverse datasets,
including DIS5K, DUTS, ISIC, COD10K, and COCO, reveals its ability to
significantly mitigate performance degradation in the zero-shot setting and
achieve state-of-the-art performance with minimal fine-tuning. Furthermore, we
propose a generalized model and benchmark, showcasing BA-SAM's generalizability
across all four datasets simultaneously.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02326">ClassWise-SAM-Adapter: Parameter Efficient Fine-tuning Adapts Segment Anything to SAR Domain for Semantic Segmentation. (arXiv:2401.02326v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pu_X/0/1/0/all/0/1">Xinyang Pu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_H/0/1/0/all/0/1">Hecheng Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1">Linghao Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Feng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1">Feng Xu</a></p>
<p>In the realm of artificial intelligence, the emergence of foundation models,
backed by high computing capabilities and extensive data, has been
revolutionary. Segment Anything Model (SAM), built on the Vision Transformer
(ViT) model with millions of parameters and vast training dataset SA-1B, excels
in various segmentation scenarios relying on its significance of semantic
information and generalization ability. Such achievement of visual foundation
model stimulates continuous researches on specific downstream tasks in computer
vision. The ClassWise-SAM-Adapter (CWSAM) is designed to adapt the
high-performing SAM for landcover classification on space-borne Synthetic
Aperture Radar (SAR) images. The proposed CWSAM freezes most of SAM's
parameters and incorporates lightweight adapters for parameter efficient
fine-tuning, and a classwise mask decoder is designed to achieve semantic
segmentation task. This adapt-tuning method allows for efficient landcover
classification of SAR images, balancing the accuracy with computational demand.
In addition, the task specific input module injects low frequency information
of SAR images by MLP-based layers to improve the model performance. Compared to
conventional state-of-the-art semantic segmentation algorithms by extensive
experiments, CWSAM showcases enhanced performance with fewer computing
resources, highlighting the potential of leveraging foundational models like
SAM for specific downstream tasks in the SAR domain. The source code is
available at: https://github.com/xypu98/CWSAM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02330">LLaVA-$\phi$: Efficient Multi-Modal Assistant with Small Language Model. (arXiv:2401.02330v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yichen Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1">Minjie Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1">Ning Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ou_Z/0/1/0/all/0/1">Zhicai Ou</a>, <a href="http://arxiv.org/find/cs/1/au:+Mou_X/0/1/0/all/0/1">Xiaofeng Mou</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jian Tang</a></p>
<p>In this paper, we introduce LLaVA-$\phi$ (LLaVA-Phi), an efficient
multi-modal assistant that harnesses the power of the recently advanced small
language model, Phi-2, to facilitate multi-modal dialogues. LLaVA-Phi marks a
notable advancement in the realm of compact multi-modal models. It demonstrates
that even smaller language models, with as few as 2.7B parameters, can
effectively engage in intricate dialogues that integrate both textual and
visual elements, provided they are trained with high-quality corpora. Our model
delivers commendable performance on publicly available benchmarks that
encompass visual comprehension, reasoning, and knowledge-based perception.
Beyond its remarkable performance in multi-modal dialogue tasks, our model
opens new avenues for applications in time-sensitive environments and systems
that require real-time interaction, such as embodied agents. It highlights the
potential of smaller language models to achieve sophisticated levels of
understanding and interaction, while maintaining greater resource
efficiency.The project is available at {https://github.com/zhuyiche/llava-phi}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02335">Linguistic Profiling of Deepfakes: An Open Database for Next-Generation Deepfake Detection. (arXiv:2401.02335v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yabin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zhiwu Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1">Zhiheng Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_X/0/1/0/all/0/1">Xiaopeng Hong</a></p>
<p>The emergence of text-to-image generative models has revolutionized the field
of deepfakes, enabling the creation of realistic and convincing visual content
directly from textual descriptions. However, this advancement presents
considerably greater challenges in detecting the authenticity of such content.
Existing deepfake detection datasets and methods often fall short in
effectively capturing the extensive range of emerging deepfakes and offering
satisfactory explanatory information for detection. To address the significant
issue, this paper introduces a deepfake database (DFLIP-3K) for the development
of convincing and explainable deepfake detection. It encompasses about 300K
diverse deepfake samples from approximately 3K generative models, which boasts
the largest number of deepfake models in the literature. Moreover, it collects
around 190K linguistic footprints of these deepfakes. The two distinguished
features enable DFLIP-3K to develop a benchmark that promotes progress in
linguistic profiling of deepfakes, which includes three sub-tasks namely
deepfake detection, model identification, and prompt prediction. The deepfake
model and prompt are two essential components of each deepfake, and thus
dissecting them linguistically allows for an invaluable exploration of
trustworthy and interpretable evidence in deepfake detection, which we believe
is the key for the next-generation deepfake detection. Furthermore, DFLIP-3K is
envisioned as an open database that fosters transparency and encourages
collaborative efforts to further enhance its growth. Our extensive experiments
on the developed benchmark verify that our DFLIP-3K database is capable of
serving as a standardized resource for evaluating and comparing
linguistic-based deepfake detection, identification, and prompt prediction
techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02347">Mining Fine-Grained Image-Text Alignment for Zero-Shot Captioning via Text-Only Training. (arXiv:2401.02347v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1">Longtian Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ning_S/0/1/0/all/0/1">Shan Ning</a>, <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xuming He</a></p>
<p>Image captioning aims at generating descriptive and meaningful textual
descriptions of images, enabling a broad range of vision-language applications.
Prior works have demonstrated that harnessing the power of Contrastive Image
Language Pre-training (CLIP) offers a promising approach to achieving zero-shot
captioning, eliminating the need for expensive caption annotations. However,
the widely observed modality gap in the latent space of CLIP harms the
performance of zero-shot captioning by breaking the alignment between paired
image-text features. To address this issue, we conduct an analysis on the CLIP
latent space which leads to two findings. Firstly, we observe that the CLIP's
visual feature of image subregions can achieve closer proximity to the paired
caption due to the inherent information loss in text descriptions. In addition,
we show that the modality gap between a paired image-text can be empirically
modeled as a zero-mean Gaussian distribution. Motivated by the findings, we
propose a novel zero-shot image captioning framework with text-only training to
reduce the modality gap. In particular, we introduce a subregion feature
aggregation to leverage local region information, which produces a compact
visual representation for matching text representation. Moreover, we
incorporate a noise injection and CLIP reranking strategy to boost captioning
performance. We also extend our framework to build a zero-shot VQA pipeline,
demonstrating its generality. Through extensive experiments on common
captioning and VQA datasets such as MSCOCO, Flickr30k and VQAV2, we show that
our method achieves remarkable performance improvements. Code is available at
https://github.com/Artanic30/MacCap.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02357">Fit-NGP: Fitting Object Models to Neural Graphics Primitives. (arXiv:2401.02357v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Taher_M/0/1/0/all/0/1">Marwan Taher</a>, <a href="http://arxiv.org/find/cs/1/au:+Alzugaray_I/0/1/0/all/0/1">Ignacio Alzugaray</a>, <a href="http://arxiv.org/find/cs/1/au:+Davison_A/0/1/0/all/0/1">Andrew J. Davison</a></p>
<p>Accurate 3D object pose estimation is key to enabling many robotic
applications that involve challenging object interactions. In this work, we
show that the density field created by a state-of-the-art efficient radiance
field reconstruction method is suitable for highly accurate and robust pose
estimation for objects with known 3D models, even when they are very small and
with challenging reflective surfaces. We present a fully automatic object pose
estimation system based on a robot arm with a single wrist-mounted camera,
which can scan a scene from scratch, detect and estimate the 6-Degrees of
Freedom (DoF) poses of multiple objects within a couple of minutes of
operation. Small objects such as bolts and nuts are estimated with accuracy on
order of 1mm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02358">A novel method to enhance pneumonia detection via a model-level ensembling of CNN and vision transformer. (arXiv:2401.02358v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Angara_S/0/1/0/all/0/1">Sandeep Angara</a>, <a href="http://arxiv.org/find/eess/1/au:+Mannuru_N/0/1/0/all/0/1">Nishith Reddy Mannuru</a>, <a href="http://arxiv.org/find/eess/1/au:+Mannuru_A/0/1/0/all/0/1">Aashrith Mannuru</a>, <a href="http://arxiv.org/find/eess/1/au:+Thirunagaru_S/0/1/0/all/0/1">Sharath Thirunagaru</a></p>
<p>Pneumonia remains a leading cause of morbidity and mortality worldwide. Chest
X-ray (CXR) imaging is a fundamental diagnostic tool, but traditional analysis
relies on time-intensive expert evaluation. Recently, deep learning has shown
immense potential for automating pneumonia detection from CXRs. This paper
explores applying neural networks to improve CXR-based pneumonia diagnosis. We
developed a novel model fusing Convolution Neural networks (CNN) and Vision
Transformer networks via model-level ensembling. Our fusion architecture
combines a ResNet34 variant and a Multi-Axis Vision Transformer small model.
Both base models are initialized with ImageNet pre-trained weights. The output
layers are removed, and features are combined using a flattening layer before
final classification. Experiments used the Kaggle pediatric pneumonia dataset
containing 1,341 normal and 3,875 pneumonia CXR images. We compared our model
against standalone ResNet34, Vision Transformer, and Swin Transformer Tiny
baseline models using identical training procedures. Extensive data
augmentation, Adam optimization, learning rate warmup, and decay were employed.
The fusion model achieved a state-of-the-art accuracy of 94.87%, surpassing the
baselines. We also attained excellent sensitivity, specificity, kappa score,
and positive predictive value. Confusion matrix analysis confirms fewer
misclassifications. The ResNet34 and Vision Transformer combination enables
jointly learning robust features from CNNs and Transformer paradigms. This
model-level ensemble technique effectively integrates their complementary
strengths for enhanced pneumonia classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02361">An Open and Comprehensive Pipeline for Unified Object Grounding and Detection. (arXiv:2401.02361v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xiangyu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yicheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Shilin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiangtai Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinjiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yining Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Haian Huang</a></p>
<p>Grounding-DINO is a state-of-the-art open-set detection model that tackles
multiple vision tasks including Open-Vocabulary Detection (OVD), Phrase
Grounding (PG), and Referring Expression Comprehension (REC). Its effectiveness
has led to its widespread adoption as a mainstream architecture for various
downstream applications. However, despite its significance, the original
Grounding-DINO model lacks comprehensive public technical details due to the
unavailability of its training code. To bridge this gap, we present
MM-Grounding-DINO, an open-source, comprehensive, and user-friendly baseline,
which is built with the MMDetection toolbox. It adopts abundant vision datasets
for pre-training and various detection and grounding datasets for fine-tuning.
We give a comprehensive analysis of each reported result and detailed settings
for reproduction. The extensive experiments on the benchmarks mentioned
demonstrate that our MM-Grounding-DINO-Tiny outperforms the Grounding-DINO-Tiny
baseline. We release all our models to the research community. Codes and
trained models are released at
https://github.com/open-mmlab/mmdetection/configs/mm_grounding_dino.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02383">Survey of 3D Human Body Pose and Shape Estimation Methods for Contemporary Dance Applications. (arXiv:2401.02383v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Venkatrayappa_D/0/1/0/all/0/1">Darshan Venkatrayappa</a>, <a href="http://arxiv.org/find/cs/1/au:+Tremeau_A/0/1/0/all/0/1">Alain Tremeau</a>, <a href="http://arxiv.org/find/cs/1/au:+Muselet_D/0/1/0/all/0/1">Damien Muselet</a>, <a href="http://arxiv.org/find/cs/1/au:+Colantoni_P/0/1/0/all/0/1">Philippe Colantoni</a></p>
<p>3D human body shape and pose estimation from RGB images is a challenging
problem with potential applications in augmented/virtual reality, healthcare
and fitness technology and virtual retail. Recent solutions have focused on
three types of inputs: i) single images, ii) multi-view images and iii) videos.
In this study, we surveyed and compared 3D body shape and pose estimation
methods for contemporary dance and performing arts, with a special focus on
human body pose and dressing, camera viewpoint, illumination conditions and
background conditions. We demonstrated that multi-frame methods, such as PHALP,
provide better results than single-frame method for pose estimation when
dancers are performing contemporary dances.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02384">ChartAssisstant: A Universal Chart Multimodal Language Model via Chart-to-Table Pre-training and Multitask Instruction Tuning. (arXiv:2401.02384v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1">Fanqing Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1">Wenqi Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1">Quanfeng Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1">Peng Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kaipeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1">Ping Luo</a></p>
<p>Charts play a vital role in data visualization, understanding data patterns,
and informed decision-making. However, their unique combination of graphical
elements (e.g., bars, lines) and textual components (e.g., labels, legends)
poses challenges for general-purpose multimodal models. While vision-language
models trained on chart data excel in comprehension, they struggle with
generalization and require task-specific fine-tuning. To address these
challenges, we propose ChartAssistant, a chart-based vision-language model for
universal chart comprehension and reasoning. ChartAssistant leverages ChartSFT,
a comprehensive dataset covering diverse chart-related tasks with basic and
specialized chart types. It undergoes a two-stage training process, starting
with pre-training on chart-to-table parsing to align chart and text, followed
by multitask instruction-following fine-tuning. This approach enables
ChartAssistant to achieve competitive performance across various chart tasks
without task-specific fine-tuning. Experimental results demonstrate significant
performance gains over the state-of-the-art UniChart method, outperforming
OpenAI's GPT-4V(ision) on real-world chart data. The code and data are
available at https://github.com/OpenGVLab/ChartAst.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02400">Learning the 3D Fauna of the Web. (arXiv:2401.02400v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zizhang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Litvak_D/0/1/0/all/0/1">Dor Litvak</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1">Ruining Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yunzhi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jakab_T/0/1/0/all/0/1">Tomas Jakab</a>, <a href="http://arxiv.org/find/cs/1/au:+Rupprecht_C/0/1/0/all/0/1">Christian Rupprecht</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Shangzhe Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1">Andrea Vedaldi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jiajun Wu</a></p>
<p>Learning 3D models of all animals on the Earth requires massively scaling up
existing solutions. With this ultimate goal in mind, we develop 3D-Fauna, an
approach that learns a pan-category deformable 3D animal model for more than
100 animal species jointly. One crucial bottleneck of modeling animals is the
limited availability of training data, which we overcome by simply learning
from 2D Internet images. We show that prior category-specific attempts fail to
generalize to rare species with limited training images. We address this
challenge by introducing the Semantic Bank of Skinned Models (SBSM), which
automatically discovers a small set of base animal shapes by combining
geometric inductive priors with semantic knowledge implicitly captured by an
off-the-shelf self-supervised feature extractor. To train such a model, we also
contribute a new large-scale dataset of diverse animal species. At inference
time, given a single image of any quadruped animal, our model reconstructs an
articulated 3D mesh in a feed-forward fashion within seconds.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02402">3D Open-Vocabulary Panoptic Segmentation with 2D-3D Vision-Language Distillation. (arXiv:2401.02402v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1">Zihao Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1">Longlong Jing</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Shangxuan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_A/0/1/0/all/0/1">Alex Zihao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1">Jingwei Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1">Chiyu Max Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hung_W/0/1/0/all/0/1">Wei-Chih Hung</a>, <a href="http://arxiv.org/find/cs/1/au:+Funkhouser_T/0/1/0/all/0/1">Thomas Funkhouser</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuo_W/0/1/0/all/0/1">Weicheng Kuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Angelova_A/0/1/0/all/0/1">Anelia Angelova</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yin Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Sheng_S/0/1/0/all/0/1">Shiwei Sheng</a></p>
<p>3D panoptic segmentation is a challenging perception task, which aims to
predict both semantic and instance annotations for 3D points in a scene.
Although prior 3D panoptic segmentation approaches have achieved great
performance on closed-set benchmarks, generalizing to novel categories remains
an open problem. For unseen object categories, 2D open-vocabulary segmentation
has achieved promising results that solely rely on frozen CLIP backbones and
ensembling multiple classification outputs. However, we find that simply
extending these 2D models to 3D does not achieve good performance due to poor
per-mask classification quality on novel categories. In this paper, we propose
the first method to tackle 3D open-vocabulary panoptic segmentation. Our model
takes advantage of the fusion between learnable LiDAR features and dense frozen
vision CLIP features, using a single classification head to make predictions
for both base and novel classes. To further improve the classification
performance on novel classes and leverage the CLIP model, we propose two novel
loss functions: object-level distillation loss and voxel-level distillation
loss. Our experiments on the nuScenes and SemanticKITTI datasets show that our
method outperforms strong baselines by a large margin.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02411">What You See is What You GAN: Rendering Every Pixel for High-Fidelity Geometry in 3D GANs. (arXiv:2401.02411v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Trevithick_A/0/1/0/all/0/1">Alex Trevithick</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_M/0/1/0/all/0/1">Matthew Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Takikawa_T/0/1/0/all/0/1">Towaki Takikawa</a>, <a href="http://arxiv.org/find/cs/1/au:+Iqbal_U/0/1/0/all/0/1">Umar Iqbal</a>, <a href="http://arxiv.org/find/cs/1/au:+Mello_S/0/1/0/all/0/1">Shalini De Mello</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandraker_M/0/1/0/all/0/1">Manmohan Chandraker</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramamoorthi_R/0/1/0/all/0/1">Ravi Ramamoorthi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nagano_K/0/1/0/all/0/1">Koki Nagano</a></p>
<p>3D-aware Generative Adversarial Networks (GANs) have shown remarkable
progress in learning to generate multi-view-consistent images and 3D geometries
of scenes from collections of 2D images via neural volume rendering. Yet, the
significant memory and computational costs of dense sampling in volume
rendering have forced 3D GANs to adopt patch-based training or employ
low-resolution rendering with post-processing 2D super resolution, which
sacrifices multiview consistency and the quality of resolved geometry.
Consequently, 3D GANs have not yet been able to fully resolve the rich 3D
geometry present in 2D images. In this work, we propose techniques to scale
neural volume rendering to the much higher resolution of native 2D images,
thereby resolving fine-grained 3D geometry with unprecedented detail. Our
approach employs learning-based samplers for accelerating neural rendering for
3D GAN training using up to 5 times fewer depth samples. This enables us to
explicitly "render every pixel" of the full-resolution image during training
and inference without post-processing superresolution in 2D. Together with our
strategy to learn high-quality surface geometry, our method synthesizes
high-resolution 3D geometry and strictly view-consistent images while
maintaining image quality on par with baselines relying on post-processing
super resolution. We demonstrate state-of-the-art 3D gemetric quality on FFHQ
and AFHQ, setting a new standard for unsupervised learning of 3D shapes in 3D
GANs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02412">LLM Augmented LLMs: Expanding Capabilities through Composition. (arXiv:2401.02412v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bansal_R/0/1/0/all/0/1">Rachit Bansal</a>, <a href="http://arxiv.org/find/cs/1/au:+Samanta_B/0/1/0/all/0/1">Bidisha Samanta</a>, <a href="http://arxiv.org/find/cs/1/au:+Dalmia_S/0/1/0/all/0/1">Siddharth Dalmia</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1">Nitish Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Vashishth_S/0/1/0/all/0/1">Shikhar Vashishth</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganapathy_S/0/1/0/all/0/1">Sriram Ganapathy</a>, <a href="http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1">Abhishek Bapna</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1">Prateek Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1">Partha Talukdar</a></p>
<p>Foundational models with billions of parameters which have been trained on
large corpora of data have demonstrated non-trivial skills in a variety of
domains. However, due to their monolithic structure, it is challenging and
expensive to augment them or impart new skills. On the other hand, due to their
adaptation abilities, several new instances of these models are being trained
towards new domains and tasks. In this work, we study the problem of efficient
and practical composition of existing foundation models with more specific
models to enable newer capabilities. To this end, we propose CALM --
Composition to Augment Language Models -- which introduces cross-attention
between models to compose their representations and enable new capabilities.
Salient features of CALM are: (i) Scales up LLMs on new tasks by 're-using'
existing LLMs along with a few additional parameters and data, (ii) Existing
model weights are kept intact, and hence preserves existing capabilities, and
(iii) Applies to diverse domains and settings. We illustrate that augmenting
PaLM2-S with a smaller model trained on low-resource languages results in an
absolute improvement of up to 13\% on tasks like translation into English and
arithmetic reasoning for low-resource languages. Similarly, when PaLM2-S is
augmented with a code-specific model, we see a relative improvement of 40\%
over the base model for code generation and explanation tasks -- on-par with
fully fine-tuned counterparts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02414">Bring Metric Functions into Diffusion Models. (arXiv:2401.02414v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+An_J/0/1/0/all/0/1">Jie An</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhengyuan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jianfeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Linjie Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zicheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lijuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1">Jiebo Luo</a></p>
<p>We introduce a Cascaded Diffusion Model (Cas-DM) that improves a Denoising
Diffusion Probabilistic Model (DDPM) by effectively incorporating additional
metric functions in training. Metric functions such as the LPIPS loss have been
proven highly effective in consistency models derived from the score matching.
However, for the diffusion counterparts, the methodology and efficacy of adding
extra metric functions remain unclear. One major challenge is the mismatch
between the noise predicted by a DDPM at each step and the desired clean image
that the metric function works well on. To address this problem, we propose
Cas-DM, a network architecture that cascades two network modules to effectively
apply metric functions to the diffusion model training. The first module,
similar to a standard DDPM, learns to predict the added noise and is unaffected
by the metric function. The second cascaded module learns to predict the clean
image, thereby facilitating the metric function computation. Experiment results
show that the proposed diffusion model backbone enables the effective use of
the LPIPS loss, leading to state-of-the-art image quality (FID, sFID, IS) on
various established benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02416">ODIN: A Single Model for 2D and 3D Perception. (arXiv:2401.02416v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1">Ayush Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Katara_P/0/1/0/all/0/1">Pushkal Katara</a>, <a href="http://arxiv.org/find/cs/1/au:+Gkanatsios_N/0/1/0/all/0/1">Nikolaos Gkanatsios</a>, <a href="http://arxiv.org/find/cs/1/au:+Harley_A/0/1/0/all/0/1">Adam W. Harley</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarch_G/0/1/0/all/0/1">Gabriel Sarch</a>, <a href="http://arxiv.org/find/cs/1/au:+Aggarwal_K/0/1/0/all/0/1">Kriti Aggarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Chaudhary_V/0/1/0/all/0/1">Vishrav Chaudhary</a>, <a href="http://arxiv.org/find/cs/1/au:+Fragkiadaki_K/0/1/0/all/0/1">Katerina Fragkiadaki</a></p>
<p>State-of-the-art models on contemporary 3D perception benchmarks like ScanNet
consume and label dataset-provided 3D point clouds, obtained through post
processing of sensed multiview RGB-D images. They are typically trained
in-domain, forego large-scale 2D pre-training and outperform alternatives that
featurize the posed RGB-D multiview images instead. The gap in performance
between methods that consume posed images versus post-processed 3D point clouds
has fueled the belief that 2D and 3D perception require distinct model
architectures. In this paper, we challenge this view and propose ODIN
(Omni-Dimensional INstance segmentation), a model that can segment and label
both 2D RGB images and 3D point clouds, using a transformer architecture that
alternates between 2D within-view and 3D cross-view information fusion. Our
model differentiates 2D and 3D feature operations through the positional
encodings of the tokens involved, which capture pixel coordinates for 2D patch
tokens and 3D coordinates for 3D feature tokens. ODIN achieves state-of-the-art
performance on ScanNet200, Matterport3D and AI2THOR 3D instance segmentation
benchmarks, and competitive performance on ScanNet, S3DIS and COCO. It
outperforms all previous works by a wide margin when the sensed 3D point cloud
is used in place of the point cloud sampled from 3D mesh. When used as the 3D
perception engine in an instructable embodied agent architecture, it sets a new
state-of-the-art on the TEACh action-from-dialogue benchmark. Our code and
checkpoints can be found at the project website: https://odin-seg.github.io.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02418">Learning to Prompt with Text Only Supervision for Vision-Language Models. (arXiv:2401.02418v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khattak_M/0/1/0/all/0/1">Muhammad Uzair Khattak</a>, <a href="http://arxiv.org/find/cs/1/au:+Naeem_M/0/1/0/all/0/1">Muhammad Ferjad Naeem</a>, <a href="http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1">Muzammal Naseer</a>, <a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1">Luc Van Gool</a>, <a href="http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1">Federico Tombari</a></p>
<p>Foundational vision-language models such as CLIP are becoming a new paradigm
in vision, due to their excellent generalization abilities. However, adapting
these models for downstream tasks while maintaining their generalization
remains a challenge. In literature, one branch of methods adapts CLIP by
learning prompts using visual information. While effective, most of these works
require labeled data which is not practical, and often struggle to generalize
towards new datasets due to over-fitting on the source data. An alternative
approach resorts to training-free methods by generating class descriptions from
large language models (LLMs) and perform prompt ensembling. However, these
methods often generate class specific prompts that cannot be transferred to
other classes, which incur higher costs by generating LLM descriptions for each
class separately. In this work, we propose to combine the strengths of these
both streams of methods by learning prompts using only text data derived from
LLMs. As supervised training of prompts is not trivial due to absence of
images, we develop a training approach that allows prompts to extract rich
contextual knowledge from LLM data. Moreover, with LLM contextual data mapped
within the learned prompts, it enables zero-shot transfer of prompts to new
classes and datasets potentially cutting the LLM prompt engineering cost. To
the best of our knowledge, this is the first work that learns generalized
prompts using text only data. We perform extensive evaluations on 4 benchmarks
where our method improves over prior ensembling works while being competitive
to those utilizing labeled images. Our code and pre-trained models are
available at https://github.com/muzairkhattak/ProText.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2011.14956">Handling Noisy Labels via One-Step Abductive Multi-Target Learning and Its Application to Helicobacter Pylori Segmentation. (arXiv:2011.14956v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yongquan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yiming Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jie Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1">Jiayi Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Zhongxi Zheng</a></p>
<p>Learning from noisy labels is an important concern in plenty of real-world
scenarios. Various approaches for this concern first make corrections
corresponding to potentially noisy-labeled instances, and then update
predictive model with information of the made corrections. However, in specific
areas, such as medical histopathology whole slide image analysis (MHWSIA), it
is often difficult or impossible for experts to manually achieve the noisy-free
ground-truth labels which leads to labels with complex noise. This situation
raises two more difficult problems: 1) the methodology of approaches making
corrections corresponding to potentially noisy-labeled instances has
limitations due to the complex noise existing in labels; and 2) the appropriate
evaluation strategy for validation/testing is unclear because of the great
difficulty in collecting the noisy-free ground-truth labels. For the problem
1), we present one-step abductive multi-target learning (OSAMTL) that imposes a
one-step logical reasoning upon machine learning via a multi-target learning
procedure to constrain the predictions of the learning model to be subject to
our prior knowledge about the true target. For the problem 2), we propose a
logical assessment formula (LAF) that evaluates the logical rationality of the
outputs of an approach by estimating the consistencies between the predictions
of the learning model and the logical facts narrated from the results of the
one-step logical reasoning of OSAMTL. Based on the Helicobacter pylori (H.
pylori) segmentation task in MHWSIA, we show that OSAMTL enables the machine
learning model achieving logically more rational predictions, which is beyond
various state-of-the-art approaches in handling complex noisy labels.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2204.03842">From 2D Images to 3D Model:Weakly Supervised Multi-View Face Reconstruction with Deep Fusion. (arXiv:2204.03842v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Weiguang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Chaolong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1">Jianan Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Rui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1">Yuyao Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1">Bin Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Hussain_A/0/1/0/all/0/1">Amir Hussain</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1">Kaizhu Huang</a></p>
<p>While weakly supervised multi-view face reconstruction (MVR) is garnering
increased attention, one critical issue still remains open: how to effectively
fuse multiple image information to reconstruct high-precision 3D models. In
this regard, we propose a novel model called Deep Fusion MVR (DF-MVR) and
design a multi-view encoding to single decoding framework with skip
connections, able to extract, integrate, and compensate deep features with
attention from multi-view images. Furthermore, we adopt the involution kernel
to enrich deep fusion features with channel features. In addition, we develop
the face parse network to learn, identify, and emphasize the critical common
face area within multi-view images. Experiments on Pixel-Face and Bosphorus
datasets indicate the superiority of our model. Without 3D annotation, DF-MVR
achieves 5.2% and 3.0% RMSE improvement over the existing weakly supervised
MVRs respectively on Pixel-Face and Bosphorus dataset. Code will be available
publicly at https://github.com/weiguangzhao/DF_MVR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.05922">Audiovisual Masked Autoencoders. (arXiv:2212.05922v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Georgescu_M/0/1/0/all/0/1">Mariana-Iuliana Georgescu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fonseca_E/0/1/0/all/0/1">Eduardo Fonseca</a>, <a href="http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1">Radu Tudor Ionescu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lucic_M/0/1/0/all/0/1">Mario Lucic</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1">Cordelia Schmid</a>, <a href="http://arxiv.org/find/cs/1/au:+Arnab_A/0/1/0/all/0/1">Anurag Arnab</a></p>
<p>Can we leverage the audiovisual information already present in video to
improve self-supervised representation learning? To answer this question, we
study various pretraining architectures and objectives within the masked
autoencoding framework, motivated by the success of similar methods in natural
language and image understanding. We show that we can achieve significant
improvements on audiovisual downstream classification tasks, surpassing the
state-of-the-art on VGGSound and AudioSet. Furthermore, we can leverage our
audiovisual pretraining scheme for multiple unimodal downstream tasks using a
single audiovisual pretrained model. We additionally demonstrate the
transferability of our representations, achieving state-of-the-art audiovisual
results on Epic Kitchens without pretraining specifically for this dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.02008">Expressive Speech-driven Facial Animation with controllable emotions. (arXiv:2301.02008v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yutong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Junhong Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wei-Qiang Zhang</a></p>
<p>It is in high demand to generate facial animation with high realism, but it
remains a challenging task. Existing approaches of speech-driven facial
animation can produce satisfactory mouth movement and lip synchronization, but
show weakness in dramatic emotional expressions and flexibility in emotion
control. This paper presents a novel deep learning-based approach for
expressive facial animation generation from speech that can exhibit
wide-spectrum facial expressions with controllable emotion type and intensity.
We propose an emotion controller module to learn the relationship between the
emotion variations (e.g., types and intensity) and the corresponding facial
expression parameters. It enables emotion-controllable facial animation, where
the target expression can be continuously adjusted as desired. The qualitative
and quantitative evaluations show that the animation generated by our method is
rich in facial emotional expressiveness while retaining accurate lip movement,
outperforming other state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.11329">Anatomy-aware and acquisition-agnostic joint registration with SynthMorph. (arXiv:2301.11329v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Hoffmann_M/0/1/0/all/0/1">Malte Hoffmann</a>, <a href="http://arxiv.org/find/eess/1/au:+Hoopes_A/0/1/0/all/0/1">Andrew Hoopes</a>, <a href="http://arxiv.org/find/eess/1/au:+Greve_D/0/1/0/all/0/1">Douglas N. Greve</a>, <a href="http://arxiv.org/find/eess/1/au:+Fischl_B/0/1/0/all/0/1">Bruce Fischl</a>, <a href="http://arxiv.org/find/eess/1/au:+Dalca_A/0/1/0/all/0/1">Adrian V. Dalca</a></p>
<p>Affine image registration is a cornerstone of medical-image analysis. While
classical algorithms can achieve excellent accuracy, they solve a
time-consuming optimization for every image pair. Deep-learning (DL) methods
learn a function that maps an image pair to an output transform. Evaluating the
function is fast, but capturing large transforms can be challenging, and
networks tend to struggle if a test-image characteristic shifts from the
training domain, such as resolution. Most affine methods are agnostic to
anatomy, meaning the registration will be inaccurate if algorithms consider all
structures in the image.
</p>
<p>We address these shortcomings with SynthMorph, an easy-to-use DL tool for
joint affine-deformable registration of any brain image without preprocessing,
right off the MRI scanner. First, we leverage a strategy to train networks with
wildly varying images synthesized from label maps, yielding robust performance
across acquisition specifics unseen at training. Second, we optimize the
spatial overlap of select anatomical labels. This enables networks to
distinguish anatomy of interest from irrelevant structures, removing the need
for preprocessing that excludes content which would impinge on anatomy-specific
registration. Third, we combine the affine model with a deformable hypernetwork
that lets users choose the optimal deformation-field regularity for their
specific data, at registration time, in a fraction of the time required by
classical methods.
</p>
<p>We rigorously analyze how competing architectures learn affine transforms and
compare state-of-the-art registration tools across an extremely diverse set of
neuroimaging data, aiming to truly capture the behavior of methods in the real
world. SynthMorph demonstrates consistent and improved accuracy. It is
available at https://w3id.org/synthmorph, as a single complete end-to-end
solution for registration of brain MRI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.13656">A Survey and Benchmark of Automatic Surface Reconstruction from Point Clouds. (arXiv:2301.13656v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sulzer_R/0/1/0/all/0/1">Raphael Sulzer</a>, <a href="http://arxiv.org/find/cs/1/au:+Marlet_R/0/1/0/all/0/1">Renaud Marlet</a>, <a href="http://arxiv.org/find/cs/1/au:+Vallet_B/0/1/0/all/0/1">Bruno Vallet</a>, <a href="http://arxiv.org/find/cs/1/au:+Landrieu_L/0/1/0/all/0/1">Loic Landrieu</a></p>
<p>We present a comprehensive survey and benchmark of both traditional and
learning-based methods for surface reconstruction from point clouds. This task
is particularly challenging for real-world acquisitions due to factors like
noise, outliers, non-uniform sampling, and missing data. Traditional approaches
often simplify the problem by imposing handcrafted priors on either the input
point clouds or the resulting surface, a process that can necessitate tedious
hyperparameter tuning. Conversely, deep learning models have the capability to
directly learn the properties of input point clouds and desired surfaces from
data. We study the influence of these handcrafted and learned priors on the
precision and robustness of surface reconstruction techniques. We evaluate
various time-tested and contemporary methods in a standardized manner. When
both trained and evaluated on point clouds with identical characteristics, the
learning-based models consistently produce superior surfaces compared to their
traditional counterparts$\unicode{x2013}$even in scenarios involving novel
shape categories. However, traditional methods demonstrate greater resilience
to the diverse array of point cloud anomalies commonly found in real-world 3D
acquisitions. For the benefit of the research community, we make our code and
datasets available, inviting further enhancements to learning-based surface
reconstruction. This can be accessed at
https://github.com/raphaelsulzer/dsr-benchmark .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.13991">Learning to Generalize towards Unseen Domains via a Content-Aware Style Invariant Model for Disease Detection from Chest X-rays. (arXiv:2302.13991v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zunaed_M/0/1/0/all/0/1">Mohammad Zunaed</a>, <a href="http://arxiv.org/find/cs/1/au:+Haque_M/0/1/0/all/0/1">Md. Aynal Haque</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasan_T/0/1/0/all/0/1">Taufiq Hasan</a></p>
<p>Performance degradation due to distribution discrepancy is a longstanding
challenge in intelligent imaging, particularly for chest X-rays (CXRs). Recent
studies have demonstrated that CNNs are biased toward styles (e.g.,
uninformative textures) rather than content (e.g., shape), in stark contrast to
the human vision system. Radiologists tend to learn visual cues from CXRs and
thus perform well across multiple domains. Motivated by this, we employ the
novel on-the-fly style randomization modules at both image (SRM-IL) and feature
(SRM-FL) levels to create rich style perturbed features while keeping the
content intact for robust cross-domain performance. Previous methods simulate
unseen domains by constructing new styles via interpolation or swapping styles
from existing data, limiting them to available source domains during training.
However, SRM-IL samples the style statistics from the possible value range of a
CXR image instead of the training data to achieve more diversified
augmentations. Moreover, we utilize pixel-wise learnable parameters in the
SRM-FL compared to pre-defined channel-wise mean and standard deviations as
style embeddings for capturing more representative style features.
Additionally, we leverage consistency regularizations on global semantic
features and predictive distributions from with and without style-perturbed
versions of the same CXR to tweak the model's sensitivity toward content
markers for accurate predictions. Our proposed method, trained on CheXpert and
MIMIC-CXR datasets, achieves 77.32$\pm$0.35, 88.38$\pm$0.19, 82.63$\pm$0.13
AUCs(%) on the unseen domain test datasets, i.e., BRAX, VinDr-CXR, and NIH
chest X-ray14, respectively, compared to 75.56$\pm$0.80, 87.57$\pm$0.46,
82.07$\pm$0.19 from state-of-the-art models on five-fold cross-validation with
statistically significant results in thoracic disease classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.15403">Training-free Content Injection using h-space in Diffusion Models. (arXiv:2303.15403v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1">Jaeseok Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwon_M/0/1/0/all/0/1">Mingi Kwon</a>, <a href="http://arxiv.org/find/cs/1/au:+Uh_Y/0/1/0/all/0/1">Youngjung Uh</a></p>
<p>Diffusion models (DMs) synthesize high-quality images in various domains.
However, controlling their generative process is still hazy because the
intermediate variables in the process are not rigorously studied. Recently, the
bottleneck feature of the U-Net, namely $h$-space, is found to convey the
semantics of the resulting image. It enables StyleCLIP-like latent editing
within DMs. In this paper, we explore further usage of $h$-space beyond
attribute editing, and introduce a method to inject the content of one image
into another image by combining their features in the generative processes.
Briefly, given the original generative process of the other image, 1) we
gradually blend the bottleneck feature of the content with proper
normalization, and 2) we calibrate the skip connections to match the injected
content. Unlike custom-diffusion approaches, our method does not require
time-consuming optimization or fine-tuning. Instead, our method manipulates
intermediate features within a feed-forward generative process. Furthermore,
our method does not require supervision from external networks. The code is
available at https://curryjung.github.io/InjectFusion/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.17599">Zero-Shot Video Editing Using Off-The-Shelf Image Diffusion Models. (arXiv:2303.17599v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yan Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_K/0/1/0/all/0/1">Kangyang Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zide Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yue Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinlong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1">Chunhua Shen</a></p>
<p>Large-scale text-to-image diffusion models achieve unprecedented success in
image generation and editing. However, how to extend such success to video
editing is unclear. Recent initial attempts at video editing require
significant text-to-video data and computation resources for training, which is
often not accessible. In this work, we propose vid2vid-zero, a simple yet
effective method for zero-shot video editing. Our vid2vid-zero leverages
off-the-shelf image diffusion models, and doesn't require training on any
video. At the core of our method is a null-text inversion module for
text-to-video alignment, a cross-frame modeling module for temporal
consistency, and a spatial regularization module for fidelity to the original
video. Without any training, we leverage the dynamic nature of the attention
mechanism to enable bi-directional temporal modeling at test time. Experiments
and analyses show promising results in editing attributes, subjects, places,
etc., in real-world videos. Code is made available at
\url{https://github.com/baaivision/vid2vid-zero}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.06355">VideoChat: Chat-Centric Video Understanding. (arXiv:2305.06355v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">KunChang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yinan He</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yizhuo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenhai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1">Ping Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yali Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Limin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a></p>
<p>In this paper, we initiate an attempt of developing an end-to-end
chat-centric video understanding system, coined as VideoChat. It integrates
video foundation models and large language models via a learnable neural
interface, excelling in spatiotemporal reasoning, event localization, and
causal relationship inference. To instructively tune this system, we build a
video-centric instruction dataset, composed of thousands of videos associated
with detailed descriptions and conversations. This dataset emphasizes
spatiotemporal reasoning and captures causal relationships, providing a
valuable asset for training our chat-centric video understanding system.
Preliminary qualitative experiments demonstrate the potential of our system
across a broad spectrum of video applications, which could serve as a simple
prototype system for future research on chat-centric video understanding.
Access our code and data at https://github.com/OpenGVLab/Ask-Anything
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.17033">The Brain Tumor Segmentation (BraTS) Challenge 2023: Focus on Pediatrics (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs). (arXiv:2305.17033v4 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Kazerooni_A/0/1/0/all/0/1">Anahita Fathi Kazerooni</a>, <a href="http://arxiv.org/find/eess/1/au:+Khalili_N/0/1/0/all/0/1">Nastaran Khalili</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1">Xinyang Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Haldar_D/0/1/0/all/0/1">Debanjan Haldar</a>, <a href="http://arxiv.org/find/eess/1/au:+Jiang_Z/0/1/0/all/0/1">Zhifan Jiang</a>, <a href="http://arxiv.org/find/eess/1/au:+Anwar_S/0/1/0/all/0/1">Syed Muhammed Anwar</a>, <a href="http://arxiv.org/find/eess/1/au:+Albrecht_J/0/1/0/all/0/1">Jake Albrecht</a>, <a href="http://arxiv.org/find/eess/1/au:+Adewole_M/0/1/0/all/0/1">Maruf Adewole</a>, <a href="http://arxiv.org/find/eess/1/au:+Anazodo_U/0/1/0/all/0/1">Udunna Anazodo</a>, <a href="http://arxiv.org/find/eess/1/au:+Anderson_H/0/1/0/all/0/1">Hannah Anderson</a>, <a href="http://arxiv.org/find/eess/1/au:+Bagheri_S/0/1/0/all/0/1">Sina Bagheri</a>, <a href="http://arxiv.org/find/eess/1/au:+Baid_U/0/1/0/all/0/1">Ujjwal Baid</a>, <a href="http://arxiv.org/find/eess/1/au:+Bergquist_T/0/1/0/all/0/1">Timothy Bergquist</a>, <a href="http://arxiv.org/find/eess/1/au:+Borja_A/0/1/0/all/0/1">Austin J. Borja</a>, <a href="http://arxiv.org/find/eess/1/au:+Calabrese_E/0/1/0/all/0/1">Evan Calabrese</a>, <a href="http://arxiv.org/find/eess/1/au:+Chung_V/0/1/0/all/0/1">Verena Chung</a>, <a href="http://arxiv.org/find/eess/1/au:+Conte_G/0/1/0/all/0/1">Gian-Marco Conte</a>, <a href="http://arxiv.org/find/eess/1/au:+Dako_F/0/1/0/all/0/1">Farouk Dako</a>, <a href="http://arxiv.org/find/eess/1/au:+Eddy_J/0/1/0/all/0/1">James Eddy</a>, <a href="http://arxiv.org/find/eess/1/au:+Ezhov_I/0/1/0/all/0/1">Ivan Ezhov</a>, <a href="http://arxiv.org/find/eess/1/au:+Familiar_A/0/1/0/all/0/1">Ariana Familiar</a>, <a href="http://arxiv.org/find/eess/1/au:+Farahani_K/0/1/0/all/0/1">Keyvan Farahani</a>, <a href="http://arxiv.org/find/eess/1/au:+Haldar_S/0/1/0/all/0/1">Shuvanjan Haldar</a>, <a href="http://arxiv.org/find/eess/1/au:+Iglesias_J/0/1/0/all/0/1">Juan Eugenio Iglesias</a>, <a href="http://arxiv.org/find/eess/1/au:+Janas_A/0/1/0/all/0/1">Anastasia Janas</a>, <a href="http://arxiv.org/find/eess/1/au:+Johansen_E/0/1/0/all/0/1">Elaine Johansen</a>, <a href="http://arxiv.org/find/eess/1/au:+Jones_B/0/1/0/all/0/1">Blaise V Jones</a>, <a href="http://arxiv.org/find/eess/1/au:+Kofler_F/0/1/0/all/0/1">Florian Kofler</a>, <a href="http://arxiv.org/find/eess/1/au:+LaBella_D/0/1/0/all/0/1">Dominic LaBella</a>, <a href="http://arxiv.org/find/eess/1/au:+Lai_H/0/1/0/all/0/1">Hollie Anne Lai</a>, <a href="http://arxiv.org/find/eess/1/au:+Leemput_K/0/1/0/all/0/1">Koen Van Leemput</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1">Hongwei Bran Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Maleki_N/0/1/0/all/0/1">Nazanin Maleki</a>, <a href="http://arxiv.org/find/eess/1/au:+McAllister_A/0/1/0/all/0/1">Aaron S McAllister</a>, <a href="http://arxiv.org/find/eess/1/au:+Meier_Z/0/1/0/all/0/1">Zeke Meier</a>, <a href="http://arxiv.org/find/eess/1/au:+Menze_B/0/1/0/all/0/1">Bjoern Menze</a>, <a href="http://arxiv.org/find/eess/1/au:+Moawad_A/0/1/0/all/0/1">Ahmed W Moawad</a>, <a href="http://arxiv.org/find/eess/1/au:+Nandolia_K/0/1/0/all/0/1">Khanak K Nandolia</a>, <a href="http://arxiv.org/find/eess/1/au:+Pavaine_J/0/1/0/all/0/1">Julija Pavaine</a>, <a href="http://arxiv.org/find/eess/1/au:+Piraud_M/0/1/0/all/0/1">Marie Piraud</a>, <a href="http://arxiv.org/find/eess/1/au:+Poussaint_T/0/1/0/all/0/1">Tina Poussaint</a>, <a href="http://arxiv.org/find/eess/1/au:+Prabhu_S/0/1/0/all/0/1">Sanjay P Prabhu</a>, <a href="http://arxiv.org/find/eess/1/au:+Reitman_Z/0/1/0/all/0/1">Zachary Reitman</a>, <a href="http://arxiv.org/find/eess/1/au:+Rodriguez_A/0/1/0/all/0/1">Andres Rodriguez</a>, <a href="http://arxiv.org/find/eess/1/au:+Rudie_J/0/1/0/all/0/1">Jeffrey D Rudie</a>, <a href="http://arxiv.org/find/eess/1/au:+Shaikh_I/0/1/0/all/0/1">Ibraheem Salman Shaikh</a>, <a href="http://arxiv.org/find/eess/1/au:+Shah_L/0/1/0/all/0/1">Lubdha M. Shah</a>, <a href="http://arxiv.org/find/eess/1/au:+Sheth_N/0/1/0/all/0/1">Nakul Sheth</a>, <a href="http://arxiv.org/find/eess/1/au:+Shinohara_R/0/1/0/all/0/1">Russel Taki Shinohara</a>, et al. (23 additional authors not shown)</p>
<p>Pediatric tumors of the central nervous system are the most common cause of
cancer-related death in children. The five-year survival rate for high-grade
gliomas in children is less than 20\%. Due to their rarity, the diagnosis of
these entities is often delayed, their treatment is mainly based on historic
treatment concepts, and clinical trials require multi-institutional
collaborations. The MICCAI Brain Tumor Segmentation (BraTS) Challenge is a
landmark community benchmark event with a successful history of 12 years of
resource creation for the segmentation and analysis of adult glioma. Here we
present the CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023 challenge, which
represents the first BraTS challenge focused on pediatric brain tumors with
data acquired across multiple international consortia dedicated to pediatric
neuro-oncology and clinical trials. The BraTS-PEDs 2023 challenge focuses on
benchmarking the development of volumentric segmentation algorithms for
pediatric brain glioma through standardized quantitative performance evaluation
metrics utilized across the BraTS 2023 cluster of challenges. Models gaining
knowledge from the BraTS-PEDs multi-parametric structural MRI (mpMRI) training
data will be evaluated on separate validation and unseen test mpMRI dataof
high-grade pediatric glioma. The CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023
challenge brings together clinicians and AI/imaging scientists to lead to
faster development of automated segmentation techniques that could benefit
clinical trials, and ultimately the care of children with brain tumors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.17423">Accelerating Text-to-Image Editing via Cache-Enabled Sparse Diffusion Inference. (arXiv:2305.17423v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Zihao Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Haoyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_F/0/1/0/all/0/1">Fangcheng Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Miao_X/0/1/0/all/0/1">Xupeng Miao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1">Bin Cui</a></p>
<p>Due to the recent success of diffusion models, text-to-image generation is
becoming increasingly popular and achieves a wide range of applications. Among
them, text-to-image editing, or continuous text-to-image generation, attracts
lots of attention and can potentially improve the quality of generated images.
It's common to see that users may want to slightly edit the generated image by
making minor modifications to their input textual descriptions for several
rounds of diffusion inference. However, such an image editing process suffers
from the low inference efficiency of many existing diffusion models even using
GPU accelerators. To solve this problem, we introduce Fast Image Semantically
Edit (FISEdit), a cached-enabled sparse diffusion model inference engine for
efficient text-to-image editing. The key intuition behind our approach is to
utilize the semantic mapping between the minor modifications on the input text
and the affected regions on the output image. For each text editing step,
FISEdit can automatically identify the affected image regions and utilize the
cached unchanged regions' feature map to accelerate the inference process.
Extensive empirical results show that FISEdit can be $3.4\times$ and
$4.4\times$ faster than existing methods on NVIDIA TITAN RTX and A100 GPUs
respectively, and even generates more satisfactory images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.00876">Quantifying Deep Learning Model Uncertainty in Conformal Prediction. (arXiv:2306.00876v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Karimi_H/0/1/0/all/0/1">Hamed Karimi</a>, <a href="http://arxiv.org/find/cs/1/au:+Samavi_R/0/1/0/all/0/1">Reza Samavi</a></p>
<p>Precise estimation of predictive uncertainty in deep neural networks is a
critical requirement for reliable decision-making in machine learning and
statistical modeling, particularly in the context of medical AI. Conformal
Prediction (CP) has emerged as a promising framework for representing the model
uncertainty by providing well-calibrated confidence levels for individual
predictions. However, the quantification of model uncertainty in conformal
prediction remains an active research area, yet to be fully addressed. In this
paper, we explore state-of-the-art CP methodologies and their theoretical
foundations. We propose a probabilistic approach in quantifying the model
uncertainty derived from the produced prediction sets in conformal prediction
and provide certified boundaries for the computed uncertainty. By doing so, we
allow model uncertainty measured by CP to be compared by other uncertainty
quantification methods such as Bayesian (e.g., MC-Dropout and DeepEnsemble) and
Evidential approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.07716">Dynamically Masked Discriminator for Generative Adversarial Networks. (arXiv:2306.07716v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wentian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Haozhe Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bing Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1">Jinheng Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yawen Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuexiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1">Yefeng Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1">Bernard Ghanem</a></p>
<p>Training Generative Adversarial Networks (GANs) remains a challenging
problem. The discriminator trains the generator by learning the distribution of
real/generated data. However, the distribution of generated data changes
throughout the training process, which is difficult for the discriminator to
learn. In this paper, we propose a novel method for GANs from the viewpoint of
online continual learning. We observe that the discriminator model, trained on
historically generated data, often slows down its adaptation to the changes in
the new arrival generated data, which accordingly decreases the quality of
generated results. By treating the generated data in training as a stream, we
propose to detect whether the discriminator slows down the learning of new
knowledge in generated data. Therefore, we can explicitly enforce the
discriminator to learn new knowledge fast. Particularly, we propose a new
discriminator, which automatically detects its retardation and then dynamically
masks its features, such that the discriminator can adaptively learn the
temporally-vary distribution of generated data. Experimental results show our
method outperforms the state-of-the-art approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.17358">Shadow Generation with Decomposed Mask Prediction and Attentive Shadow Filling. (arXiv:2306.17358v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tao_X/0/1/0/all/0/1">Xinhao Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1">Junyan Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1">Yan Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1">Li Niu</a></p>
<p>Image composition refers to inserting a foreground object into a background
image to obtain a composite image. In this work, we focus on generating
plausible shadows for the inserted foreground object to make the composite
image more realistic. To supplement the existing small-scale dataset, we create
a large-scale dataset called RdSOBA with rendering techniques. Moreover, we
design a two-stage network named DMASNet with decomposed mask prediction and
attentive shadow filling. Specifically, in the first stage, we decompose shadow
mask prediction into box prediction and shape prediction. In the second stage,
we attend to reference background shadow pixels to fill the foreground shadow.
Abundant experiments prove that our DMASNet achieves better visual effects and
generalizes well to real composite images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06942">InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation. (arXiv:2307.06942v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yinan He</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yizhuo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Kunchang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Jiashuo Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xin Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xinhao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1">Guo Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xinyuan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yaohui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1">Conghui He</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1">Ping Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Ziwei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yali Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Limin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a></p>
<p>This paper introduces InternVid, a large-scale video-centric multimodal
dataset that enables learning powerful and transferable video-text
representations for multimodal understanding and generation. The InternVid
dataset contains over 7 million videos lasting nearly 760K hours, yielding 234M
video clips accompanied by detailed descriptions of total 4.1B words. Our core
contribution is to develop a scalable approach to autonomously build a
high-quality video-text dataset with large language models (LLM), thereby
showcasing its efficacy in learning video-language representation at scale.
Specifically, we utilize a multi-scale approach to generate video-related
descriptions. Furthermore, we introduce ViCLIP, a video-text representation
learning model based on ViT-L. Learned on InternVid via contrastive learning,
this model demonstrates leading zero-shot action recognition and competitive
video retrieval performance. Beyond basic video understanding tasks like
recognition and retrieval, our dataset and model have broad applications. They
are particularly beneficial for generating interleaved video-text data for
learning a video-centric dialogue system, advancing video-to-text and
text-to-video generation research. These proposed resources provide a tool for
researchers and practitioners interested in multimodal video understanding and
generation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.02535">Learning to Generate Training Datasets for Robust Semantic Segmentation. (arXiv:2308.02535v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hariat_M/0/1/0/all/0/1">Marwane Hariat</a>, <a href="http://arxiv.org/find/cs/1/au:+Laurent_O/0/1/0/all/0/1">Olivier Laurent</a>, <a href="http://arxiv.org/find/cs/1/au:+Kazmierczak_R/0/1/0/all/0/1">R&#xe9;mi Kazmierczak</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shihao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bursuc_A/0/1/0/all/0/1">Andrei Bursuc</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1">Angela Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Franchi_G/0/1/0/all/0/1">Gianni Franchi</a></p>
<p>Semantic segmentation methods have advanced significantly. Still, their
robustness to real-world perturbations and object types not seen during
training remains a challenge, particularly in safety-critical applications. We
propose a novel approach to improve the robustness of semantic segmentation
techniques by leveraging the synergy between label-to-image generators and
image-to-label segmentation models. Specifically, we design Robusta, a novel
robust conditional generative adversarial network to generate realistic and
plausible perturbed images that can be used to train reliable segmentation
models. We conduct in-depth studies of the proposed generative model, assess
the performance and robustness of the downstream segmentation network, and
demonstrate that our approach can significantly enhance the robustness in the
face of real-world perturbations, distribution shifts, and out-of-distribution
samples. Our results suggest that this approach could be valuable in
safety-critical applications, where the reliability of perception modules such
as semantic segmentation is of utmost importance and comes with a limited
computational budget in inference. We release our code at
https://github.com/ENSTA-U2IS/robusta.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04074">Exploiting Spatial-Temporal Context for Interacting Hand Reconstruction on Monocular RGB Video. (arXiv:2308.04074v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Weichao Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1">Hezhen Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1">Wengang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+li_L/0/1/0/all/0/1">Li li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Houqiang Li</a></p>
<p>Reconstructing interacting hands from monocular RGB data is a challenging
task, as it involves many interfering factors, e.g. self- and mutual occlusion
and similar textures. Previous works only leverage information from a single
RGB image without modeling their physically plausible relation, which leads to
inferior reconstruction results. In this work, we are dedicated to explicitly
exploiting spatial-temporal information to achieve better interacting hand
reconstruction. On one hand, we leverage temporal context to complement
insufficient information provided by the single frame, and design a novel
temporal framework with a temporal constraint for interacting hand motion
smoothness. On the other hand, we further propose an interpenetration detection
module to produce kinetically plausible interacting hands without physical
collisions. Extensive experiments are performed to validate the effectiveness
of our proposed framework, which achieves new state-of-the-art performance on
public benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04669">A General Implicit Framework for Fast NeRF Composition and Rendering. (arXiv:2308.04669v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xinyu Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Ziyi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yunlu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yuxiang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1">Xiaogang Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_C/0/1/0/all/0/1">Changqing Zou</a></p>
<p>A variety of Neural Radiance Fields (NeRF) methods have recently achieved
remarkable success in high render speed. However, current accelerating methods
are specialized and incompatible with various implicit methods, preventing
real-time composition over various types of NeRF works. Because NeRF relies on
sampling along rays, it is possible to provide general guidance for
acceleration. To that end, we propose a general implicit pipeline for composing
NeRF objects quickly. Our method enables the casting of dynamic shadows within
or between objects using analytical light sources while allowing multiple NeRF
objects to be seamlessly placed and rendered together with any arbitrary rigid
transformations. Mainly, our work introduces a new surface representation known
as Neural Depth Fields (NeDF) that quickly determines the spatial relationship
between objects by allowing direct intersection computation between rays and
implicit surfaces. It leverages an intersection neural network to query NeRF
for acceleration instead of depending on an explicit spatial structure.Our
proposed method is the first to enable both the progressive and interactive
composition of NeRF objects. Additionally, it also serves as a previewing
plugin for a range of existing NeRF works.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.02742">MLN-net: A multi-source medical image segmentation method for clustered microcalcifications using multiple layer normalization. (arXiv:2309.02742v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Ke Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1">Zanting Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xiang Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1">Haidong Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Banteng Liu</a></p>
<p>Accurate segmentation of clustered microcalcifications in mammography is
crucial for the diagnosis and treatment of breast cancer. Despite exhibiting
expert-level accuracy, recent deep learning advancements in medical image
segmentation provide insufficient contribution to practical applications, due
to the domain shift resulting from differences in patient postures, individual
gland density, and imaging modalities of mammography etc. In this paper, a
novel framework named MLN-net, which can accurately segment multi-source images
using only single source images, is proposed for clustered microcalcification
segmentation. We first propose a source domain image augmentation method to
generate multi-source images, leading to improved generalization. And a
structure of multiple layer normalization (LN) layers is used to construct the
segmentation network, which can be found efficient for clustered
microcalcification segmentation in different domains. Additionally, a branch
selection strategy is designed for measuring the similarity of the source
domain data and the target domain data. To validate the proposed MLN-net,
extensive analyses including ablation experiments are performed, comparison of
12 baseline methods. Extensive experiments validate the effectiveness of
MLN-net in segmenting clustered microcalcifications from different domains and
the its segmentation accuracy surpasses state-of-the-art methods. Code will be
available at https://github.com/yezanting/MLN-NET-VERSON1.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.04967">Towards Fully Decoupled End-to-End Person Search. (arXiv:2309.04967v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Pengcheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1">Xiao Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1">Jin Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ning_X/0/1/0/all/0/1">Xin Ning</a></p>
<p>End-to-end person search aims to jointly detect and re-identify a target
person in raw scene images with a unified model. The detection task unifies all
persons while the re-id task discriminates different identities, resulting in
conflict optimal objectives. Existing works proposed to decouple end-to-end
person search to alleviate such conflict. Yet these methods are still
sub-optimal on one or two of the sub-tasks due to their partially decoupled
models, which limits the overall person search performance. In this paper, we
propose to fully decouple person search towards optimal person search. A
task-incremental person search network is proposed to incrementally construct
an end-to-end model for the detection and re-id sub-task, which decouples the
model architecture for the two sub-tasks. The proposed task-incremental network
allows task-incremental training for the two conflicting tasks. This enables
independent learning for different objectives thus fully decoupled the model
for persons earch. Comprehensive experimental evaluations demonstrate the
effectiveness of the proposed fully decoupled models for end-to-end person
search.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.09431">FactoFormer: Factorized Hyperspectral Transformers with Self-Supervised Pretraining. (arXiv:2309.09431v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mohamed_S/0/1/0/all/0/1">Shaheer Mohamed</a>, <a href="http://arxiv.org/find/cs/1/au:+Haghighat_M/0/1/0/all/0/1">Maryam Haghighat</a>, <a href="http://arxiv.org/find/cs/1/au:+Fernando_T/0/1/0/all/0/1">Tharindu Fernando</a>, <a href="http://arxiv.org/find/cs/1/au:+Sridharan_S/0/1/0/all/0/1">Sridha Sridharan</a>, <a href="http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1">Clinton Fookes</a>, <a href="http://arxiv.org/find/cs/1/au:+Moghadam_P/0/1/0/all/0/1">Peyman Moghadam</a></p>
<p>Hyperspectral images (HSIs) contain rich spectral and spatial information.
Motivated by the success of transformers in the field of natural language
processing and computer vision where they have shown the ability to learn long
range dependencies within input data, recent research has focused on using
transformers for HSIs. However, current state-of-the-art hyperspectral
transformers only tokenize the input HSI sample along the spectral dimension,
resulting in the under-utilization of spatial information. Moreover,
transformers are known to be data-hungry and their performance relies heavily
on large-scale pretraining, which is challenging due to limited annotated
hyperspectral data. Therefore, the full potential of HSI transformers has not
been fully realized. To overcome these limitations, we propose a novel
factorized spectral-spatial transformer that incorporates factorized
self-supervised pretraining procedures, leading to significant improvements in
performance. The factorization of the inputs allows the spectral and spatial
transformers to better capture the interactions within the hyperspectral data
cubes. Inspired by masked image modeling pretraining, we also devise efficient
masking strategies for pretraining each of the spectral and spatial
transformers. We conduct experiments on six publicly available datasets for HSI
classification task and demonstrate that our model achieves state-of-the-art
performance in all the datasets. The code for our model will be made available
at https://github.com/csiro-robotics/factoformer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.13505">Rewrite Caption Semantics: Bridging Semantic Gaps for Language-Supervised Semantic Segmentation. (arXiv:2309.13505v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xing_Y/0/1/0/all/0/1">Yun Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1">Jian Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1">Aoran Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1">Jiahao Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1">Ling Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1">Shijian Lu</a></p>
<p>Vision-Language Pre-training has demonstrated its remarkable zero-shot
recognition ability and potential to learn generalizable visual representations
from language supervision. Taking a step ahead, language-supervised semantic
segmentation enables spatial localization of textual inputs by learning pixel
grouping solely from image-text pairs. Nevertheless, the state-of-the-art
suffers from clear semantic gaps between visual and textual modality: plenty of
visual concepts appeared in images are missing in their paired captions. Such
semantic misalignment circulates in pre-training, leading to inferior zero-shot
performance in dense predictions due to insufficient visual concepts captured
in textual representations. To close such semantic gap, we propose Concept
Curation (CoCu), a pipeline that leverages CLIP to compensate for the missing
semantics. For each image-text pair, we establish a concept archive that
maintains potential visually-matched concepts with our proposed vision-driven
expansion and text-to-vision-guided ranking. Relevant concepts can thus be
identified via cluster-guided sampling and fed into pre-training, thereby
bridging the gap between visual and textual semantics. Extensive experiments
over a broad suite of 8 segmentation benchmarks show that CoCu achieves superb
zero-shot transfer performance and greatly boosts language-supervised
segmentation baseline by a large margin, suggesting the value of bridging
semantic gap in pre-training data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.20065">LinFlo-Net: A two-stage deep learning method to generate simulation ready meshes of the heart. (arXiv:2310.20065v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Narayanan_A/0/1/0/all/0/1">Arjun Narayanan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kong_F/0/1/0/all/0/1">Fanwei Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Shadden_S/0/1/0/all/0/1">Shawn Shadden</a></p>
<p>We present a deep learning model to automatically generate computer models of
the human heart from patient imaging data with an emphasis on its capability to
generate thin-walled cardiac structures. Our method works by deforming a
template mesh to fit the cardiac structures to the given image. Compared with
prior deep learning methods that adopted this approach, our framework is
designed to minimize mesh self-penetration, which typically arises when
deforming surface meshes separated by small distances. We achieve this by using
a two-stage diffeomorphic deformation process along with a novel loss function
derived from the kinematics of motion that penalizes surface contact and
interpenetration. Our model demonstrates comparable accuracy with
state-of-the-art methods while additionally producing meshes free of
self-intersections. The resultant meshes are readily usable in physics based
simulation, minimizing the need for post-processing and cleanup.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09024">Fast Certification of Vision-Language Models Using Incremental Randomized Smoothing. (arXiv:2311.09024v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nirala_A/0/1/0/all/0/1">A K Nirala</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1">A Joshi</a> (2), <a href="http://arxiv.org/find/cs/1/au:+Hegde_C/0/1/0/all/0/1">C Hegde</a> (2), <a href="http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1">S Sarkar</a> (1) ((1) Iowa State University, (2) New York University)</p>
<p>A key benefit of deep vision-language models such as CLIP is that they enable
zero-shot open vocabulary classification; the user has the ability to define
novel class labels via natural language prompts at inference time. However,
while CLIP-based zero-shot classifiers have demonstrated competitive
performance across a range of domain shifts, they remain highly vulnerable to
adversarial attacks. Therefore, ensuring the robustness of such models is
crucial for their reliable deployment in the wild.
</p>
<p>In this work, we introduce Open Vocabulary Certification (OVC), a fast
certification method designed for open-vocabulary models like CLIP via
randomized smoothing techniques. Given a base "training" set of prompts and
their corresponding certified CLIP classifiers, OVC relies on the observation
that a classifier with a novel prompt can be viewed as a perturbed version of
nearby classifiers in the base training set. Therefore, OVC can rapidly certify
the novel classifier using a variation of incremental randomized smoothing. By
using a caching trick, we achieve approximately two orders of magnitude
acceleration in the certification process for novel prompts. To achieve further
(heuristic) speedups, OVC approximates the embedding space at a given input
using a multivariate normal distribution bypassing the need for sampling via
forward passes through the vision backbone. We demonstrate the effectiveness of
OVC on through experimental evaluation using multiple vision-language backbones
on the CIFAR-10 and ImageNet test datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09077">Spiking NeRF: Representing the Real-World Geometry by a Discontinuous Representation. (arXiv:2311.09077v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liao_Z/0/1/0/all/0/1">Zhanfeng Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1">Qian Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_G/0/1/0/all/0/1">Gang Pan</a></p>
<p>A crucial reason for the success of existing NeRF-based methods is to build a
neural density field for the geometry representation via multiple perceptron
layers (MLPs). MLPs are continuous functions, however, real geometry or density
field is frequently discontinuous at the interface between the air and the
surface. Such a contrary brings the problem of unfaithful geometry
representation. To this end, this paper proposes spiking NeRF, which leverages
spiking neurons and a hybrid Artificial Neural Network (ANN)-Spiking Neural
Network (SNN) framework to build a discontinuous density field for faithful
geometry representation. Specifically, we first demonstrate the reason why
continuous density fields will bring inaccuracy. Then, we propose to use the
spiking neurons to build a discontinuous density field. We conduct a
comprehensive analysis for the problem of existing spiking neuron models and
then provide the numerical relationship between the parameter of the spiking
neuron and the theoretical accuracy of geometry. Based on this, we propose a
bounded spiking neuron to build the discontinuous density field. Our method
achieves SOTA performance. The source code and the supplementary material are
available at https://github.com/liaozhanfeng/Spiking-NeRF.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13398">Depth-Regularized Optimization for 3D Gaussian Splatting in Few-Shot Images. (arXiv:2311.13398v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1">Jaeyoung Chung</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1">Jeongtaek Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kyoung Mu Lee</a></p>
<p>In this paper, we present a method to optimize Gaussian splatting with a
limited number of images while avoiding overfitting. Representing a 3D scene by
combining numerous Gaussian splats has yielded outstanding visual quality.
However, it tends to overfit the training views when only a small number of
images are available. To address this issue, we introduce a dense depth map as
a geometry guide to mitigate overfitting. We obtained the depth map using a
pre-trained monocular depth estimation model and aligning the scale and offset
using sparse COLMAP feature points. The adjusted depth aids in the color-based
optimization of 3D Gaussian splatting, mitigating floating artifacts, and
ensuring adherence to geometric constraints. We verify the proposed method on
the NeRF-LLFF dataset with varying numbers of few images. Our approach
demonstrates robust geometry compared to the original method that relies solely
on images. Project page: robot0321.github.io/DepthRegGS
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17515">Fusion of Single and Integral Multispectral Aerial Images. (arXiv:2311.17515v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Youssef_M/0/1/0/all/0/1">Mohamed Youssef</a>, <a href="http://arxiv.org/find/eess/1/au:+Bimber_O/0/1/0/all/0/1">Oliver Bimber</a></p>
<p>A novel hybrid (model- and learning-based) architecture is presented for
fusing the most significant features from conventional aerial images with the
ones from integral aerial images that are the result of synthetic aperture
sensing for removing occlusion. It combines the environment's spatial
references with features of unoccluded targets that would normally be hidden by
dense vegetation. Our method out-beats state-of-the-art two-channel and
multi-channel fusion approaches visually and quantitatively in common metrics,
such as mutual information, visual information fidelity, and peak
signal-to-noise ratio. The proposed model does not require manually tuned
parameters, can be extended to an arbitrary number and combinations of spectral
channels, and is reconfigurable for addressing different use cases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01886">InstructTA: Instruction-Tuned Targeted Attack for Large Vision-Language Models. (arXiv:2312.01886v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xunguang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1">Zhenlan Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_P/0/1/0/all/0/1">Pingchuan Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zongjie Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuai Wang</a></p>
<p>Large vision-language models (LVLMs) have demonstrated their incredible
capability in image understanding and response generation. However, this rich
visual interaction also makes LVLMs vulnerable to adversarial examples. In this
paper, we formulate a novel and practical gray-box attack scenario that the
adversary can only access the visual encoder of the victim LVLM, without the
knowledge of its prompts (which are often proprietary for service providers and
not publicly available) and its underlying large language model (LLM). This
practical setting poses challenges to the cross-prompt and cross-model
transferability of targeted adversarial attack, which aims to confuse the LVLM
to output a response that is semantically similar to the attacker's chosen
target text. To this end, we propose an instruction-tuned targeted attack
(dubbed InstructTA) to deliver the targeted adversarial attack on LVLMs with
high transferability. Initially, we utilize a public text-to-image generative
model to "reverse" the target response into a target image, and employ GPT-4 to
infer a reasonable instruction $\boldsymbol{p}^\prime$ from the target
response. We then form a local surrogate model (sharing the same visual encoder
with the victim LVLM) to extract instruction-aware features of an adversarial
image example and the target image, and minimize the distance between these two
features to optimize the adversarial example. To further improve the
transferability, we augment the instruction $\boldsymbol{p}^\prime$ with
instructions paraphrased from an LLM. Extensive experiments demonstrate the
superiority of our proposed method in targeted attack performance and
transferability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06661">UpFusion: Novel View Diffusion from Unposed Sparse View Observations. (arXiv:2312.06661v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kani_B/0/1/0/all/0/1">Bharath Raj Nagoor Kani</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hsin-Ying Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1">Sergey Tulyakov</a>, <a href="http://arxiv.org/find/cs/1/au:+Tulsiani_S/0/1/0/all/0/1">Shubham Tulsiani</a></p>
<p>We propose UpFusion, a system that can perform novel view synthesis and infer
3D representations for an object given a sparse set of reference images without
corresponding pose information. Current sparse-view 3D inference methods
typically rely on camera poses to geometrically aggregate information from
input views, but are not robust in-the-wild when such information is
unavailable/inaccurate. In contrast, UpFusion sidesteps this requirement by
learning to implicitly leverage the available images as context in a
conditional generative model for synthesizing novel views. We incorporate two
complementary forms of conditioning into diffusion models for leveraging the
input views: a) via inferring query-view aligned features using a scene-level
transformer, b) via intermediate attentional layers that can directly observe
the input image tokens. We show that this mechanism allows generating
high-fidelity novel views while improving the synthesis quality given
additional (unposed) images. We evaluate our approach on the Co3Dv2 and Google
Scanned Objects datasets and demonstrate the benefits of our method over
pose-reliant sparse-view methods as well as single-view methods that cannot
leverage additional views. Finally, we also show that our learned model can
generalize beyond the training categories and even allow reconstruction from
self-captured images of generic objects in-the-wild.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06978">CLASS-M: Adaptive stain separation-based contrastive learning with pseudo-labeling for histopathological image classification. (arXiv:2312.06978v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Bodong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Manoochehri_H/0/1/0/all/0/1">Hamid Manoochehri</a>, <a href="http://arxiv.org/find/cs/1/au:+Ho_M/0/1/0/all/0/1">Man Minh Ho</a>, <a href="http://arxiv.org/find/cs/1/au:+Fooladgar_F/0/1/0/all/0/1">Fahimeh Fooladgar</a>, <a href="http://arxiv.org/find/cs/1/au:+Chong_Y/0/1/0/all/0/1">Yosep Chong</a>, <a href="http://arxiv.org/find/cs/1/au:+Knudsen_B/0/1/0/all/0/1">Beatrice S. Knudsen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sirohi_D/0/1/0/all/0/1">Deepika Sirohi</a>, <a href="http://arxiv.org/find/cs/1/au:+Tasdizen_T/0/1/0/all/0/1">Tolga Tasdizen</a></p>
<p>Histopathological image classification is an important task in medical image
analysis. Recent approaches generally rely on weakly supervised learning due to
the ease of acquiring case-level labels from pathology reports. However,
patch-level classification is preferable in applications where only a limited
number of cases are available or when local prediction accuracy is critical. On
the other hand, acquiring extensive datasets with localized labels for training
is not feasible. In this paper, we propose a semi-supervised patch-level
histopathological image classification model, named CLASS-M, that does not
require extensively labeled datasets. CLASS-M is formed by two main parts: a
contrastive learning module that uses separated Hematoxylin and Eosin images
generated through an adaptive stain separation process, and a module with
pseudo-labels using MixUp. We compare our model with other state-of-the-art
models on two clear cell renal cell carcinoma datasets. We demonstrate that our
CLASS-M model has the best performance on both datasets. Our code is available
at github.com/BzhangURU/Paper_CLASS-M/tree/main
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08774">VSFormer: Visual-Spatial Fusion Transformer for Correspondence Pruning. (arXiv:2312.08774v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liao_T/0/1/0/all/0/1">Tangfei Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaoqin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1">Li Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_G/0/1/0/all/0/1">Guobao Xiao</a></p>
<p>Correspondence pruning aims to find correct matches (inliers) from an initial
set of putative correspondences, which is a fundamental task for many
applications. The process of finding is challenging, given the varying inlier
ratios between scenes/image pairs due to significant visual differences.
However, the performance of the existing methods is usually limited by the
problem of lacking visual cues (\eg texture, illumination, structure) of
scenes. In this paper, we propose a Visual-Spatial Fusion Transformer
(VSFormer) to identify inliers and recover camera poses accurately. Firstly, we
obtain highly abstract visual cues of a scene with the cross attention between
local features of two-view images. Then, we model these visual cues and
correspondences by a joint visual-spatial fusion module, simultaneously
embedding visual cues into correspondences for pruning. Additionally, to mine
the consistency of correspondences, we also design a novel module that combines
the KNN-based graph and the transformer, effectively capturing both local and
global contexts. Extensive experiments have demonstrated that the proposed
VSFormer outperforms state-of-the-art methods on outdoor and indoor benchmarks.
Our code is provided at the following repository:
https://github.com/sugar-fly/VSFormer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11973">Continual Learning: Forget-free Winning Subnetworks for Video Representations. (arXiv:2312.11973v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1">Haeyong Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1">Jaehong Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1">Sung Ju Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoo_C/0/1/0/all/0/1">Chang D. Yoo</a></p>
<p>Inspired by the Lottery Ticket Hypothesis (LTH), which highlights the
existence of efficient subnetworks within larger, dense networks, a
high-performing Winning Subnetwork (WSN) in terms of task performance under
appropriate sparsity conditions is considered for various continual learning
tasks. It leverages pre-existing weights from dense networks to achieve
efficient learning in Task Incremental Learning (TIL) scenarios. In Few-Shot
Class Incremental Learning (FSCIL), a variation of WSN referred to as the Soft
subnetwork (SoftNet) is designed to prevent overfitting when the data samples
are scarce. Furthermore, the sparse reuse of WSN weights is considered for
Video Incremental Learning (VIL). The use of Fourier Subneural Operator (FSO)
within WSN is considered. It enables compact encoding of videos and identifies
reusable subnetworks across varying bandwidths. We have integrated FSO into
different architectural frameworks for continual learning, including VIL, TIL,
and FSCIL. Our comprehensive experiments demonstrate FSO's effectiveness,
significantly improving task performance at various convolutional
representational levels. Specifically, FSO enhances higher-layer performance in
TIL and FSCIL and lower-layer performance in VIL
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12789">SLP-Net:An efficient lightweight network for segmentation of skin lesions. (arXiv:2312.12789v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Yang_B/0/1/0/all/0/1">Bo Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Peng_H/0/1/0/all/0/1">Hong Peng</a>, <a href="http://arxiv.org/find/eess/1/au:+Guo_C/0/1/0/all/0/1">Chenggang Guo</a>, <a href="http://arxiv.org/find/eess/1/au:+Luo_X/0/1/0/all/0/1">Xiaohui Luo</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1">Jun Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Long_X/0/1/0/all/0/1">Xianzhong Long</a></p>
<p>Prompt treatment for melanoma is crucial. To assist physicians in identifying
lesion areas precisely in a quick manner, we propose a novel skin lesion
segmentation technique namely SLP-Net, an ultra-lightweight segmentation
network based on the spiking neural P(SNP) systems type mechanism. Most
existing convolutional neural networks achieve high segmentation accuracy while
neglecting the high hardware cost. SLP-Net, on the contrary, has a very small
number of parameters and a high computation speed. We design a lightweight
multi-scale feature extractor without the usual encoder-decoder structure.
Rather than a decoder, a feature adaptation module is designed to replace it
and implement multi-scale information decoding. Experiments at the ISIC2018
challenge demonstrate that the proposed model has the highest Acc and DSC among
the state-of-the-art methods, while experiments on the PH2 dataset also
demonstrate a favorable generalization ability. Finally, we compare the
computational complexity as well as the computational speed of the models in
experiments, where SLP-Net has the highest overall superiority
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12804">Multi-stages attention Breast cancer classification based on nonlinear spiking neural P neurons with autapses. (arXiv:2312.12804v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1">Bo Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1">Hong Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1">Xiaohui Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jun Wang</a></p>
<p>Breast cancer(BC) is a prevalent type of malignant tumor in women. Early
diagnosis and treatment are vital for enhancing the patients' survival rate.
Downsampling in deep networks may lead to loss of information, so for
compensating the detail and edge information and allowing convolutional neural
networks to pay more attention to seek the lesion region, we propose a
multi-stages attention architecture based on NSNP neurons with autapses. First,
unlike the single-scale attention acquisition methods of existing methods, we
set up spatial attention acquisition at each feature map scale of the
convolutional network to obtain an fusion global information on attention
guidance. Then we introduce a new type of NSNP variants called NSNP neurons
with autapses. Specifically, NSNP systems are modularized as feature encoders,
recoding the features extracted from convolutional neural network as well as
the fusion of attention information and preserve the key characteristic
elements in feature maps. This ensures the retention of valuable data while
gradually transforming high-dimensional complicated info into low-dimensional
ones. The proposed method is evaluated on the public dataset BreakHis at
various magnifications and classification tasks. It achieves a classification
accuracy of 96.32% at all magnification cases, outperforming state-of-the-art
methods. Ablation studies are also performed, verifying the proposed model's
efficacy. The source code is available at
XhuBobYoung/Breast-cancer-Classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15707">High-Fidelity Diffusion-based Image Editing. (arXiv:2312.15707v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hou_C/0/1/0/all/0/1">Chen Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_G/0/1/0/all/0/1">Guoqiang Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhibo Chen</a></p>
<p>Diffusion models have attained remarkable success in the domains of image
generation and editing. It is widely recognized that employing larger inversion
and denoising steps in diffusion model leads to improved image reconstruction
quality. However, the editing performance of diffusion models tends to be no
more satisfactory even with increasing denoising steps. The deficiency in
editing could be attributed to the conditional Markovian property of the
editing process, where errors accumulate throughout denoising steps. To tackle
this challenge, we first propose an innovative framework where a rectifier
module is incorporated to modulate diffusion model weights with residual
features, thereby providing compensatory information to bridge the fidelity
gap. Furthermore, we introduce a novel learning paradigm aimed at minimizing
error propagation during the editing process, which trains the editing
procedure in a manner similar to denoising score-matching. Extensive
experiments demonstrate that our proposed framework and training strategy
achieve high-fidelity reconstruction and editing results across various levels
of denoising steps, meanwhile exhibits exceptional performance in terms of both
quantitative metric and qualitative assessments. Moreover, we explore our
model's generalization through several applications like image-to-image
translation and out-of-domain image editing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.17432">Video Understanding with Large Language Models: A Survey. (arXiv:2312.17432v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1">Yunlong Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1">Jing Bi</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Siting Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1">Luchuan Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1">Susan Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Teng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Daoan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+An_J/0/1/0/all/0/1">Jie An</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Jingyang Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1">Rongyi Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Vosoughi_A/0/1/0/all/0/1">Ali Vosoughi</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1">Chao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zeliang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1">Feng Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jianguo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1">Ping Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1">Jiebo Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Chenliang Xu</a></p>
<p>With the burgeoning growth of online video platforms and the escalating
volume of video content, the demand for proficient video understanding tools
has intensified markedly. Given the remarkable capabilities of Large Language
Models (LLMs) in language and multimodal tasks, this survey provides a detailed
overview of the recent advancements in video understanding harnessing the power
of LLMs (Vid-LLMs). The emergent capabilities of Vid-LLMs are surprisingly
advanced, particularly their ability for open-ended spatial-temporal reasoning
combined with commonsense knowledge, suggesting a promising path for future
video understanding. We examine the unique characteristics and capabilities of
Vid-LLMs, categorizing the approaches into four main types: LLM-based Video
Agents, Vid-LLMs Pretraining, Vid-LLMs Instruction Tuning, and Hybrid Methods.
Furthermore, this survey presents a comprehensive study of the tasks, datasets,
and evaluation methodologies for Vid-LLMs. Additionally, it explores the
expansive applications of Vid-LLMs across various domains, highlighting their
remarkable scalability and versatility in real-world video understanding
challenges. Finally, it summarizes the limitations of existing Vid-LLMs and
outlines directions for future research. For more information, readers are
recommended to visit the repository at
https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.17492">HEAP: Unsupervised Object Discovery and Localization with Contrastive Grouping. (arXiv:2312.17492v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1">Jinheng Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1">Yuan Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Mi_M/0/1/0/all/0/1">Michael Bi Mi</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_R/0/1/0/all/0/1">Robby T. Tan</a></p>
<p>Unsupervised object discovery and localization aims to detect or segment
objects in an image without any supervision. Recent efforts have demonstrated a
notable potential to identify salient foreground objects by utilizing
self-supervised transformer features. However, their scopes only build upon
patch-level features within an image, neglecting region/image-level and
cross-image relationships at a broader scale. Moreover, these methods cannot
differentiate various semantics from multiple instances. To address these
problems, we introduce Hierarchical mErging framework via contrAstive grouPing
(HEAP). Specifically, a novel lightweight head with cross-attention mechanism
is designed to adaptively group intra-image patches into semantically coherent
regions based on correlation among self-supervised features. Further, to ensure
the distinguishability among various regions, we introduce a region-level
contrastive clustering loss to pull closer similar regions across images. Also,
an image-level contrastive loss is present to push foreground and background
representations apart, with which foreground objects and background are
accordingly discovered. HEAP facilitates efficient hierarchical image
decomposition, which contributes to more accurate object discovery while also
enabling differentiation among objects of various classes. Extensive
experimental results on semantic segmentation retrieval, unsupervised object
discovery, and saliency detection tasks demonstrate that HEAP achieves
state-of-the-art performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.00440">TSGAN: An Optical-to-SAR Dual Conditional GAN for Optical based SAR Temporal Shifting. (arXiv:2401.00440v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rangzan_M/0/1/0/all/0/1">Moien Rangzan</a>, <a href="http://arxiv.org/find/cs/1/au:+Attarchi_S/0/1/0/all/0/1">Sara Attarchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gloaguen_R/0/1/0/all/0/1">Richard Gloaguen</a>, <a href="http://arxiv.org/find/cs/1/au:+Alavipanah_S/0/1/0/all/0/1">Seyed Kazem Alavipanah</a></p>
<p>In contrast to the well-investigated field of SAR-to-Optical translation,
this study explores the lesser-investigated domain of Optical-to-SAR
translation, a challenging field due to the ill-posed nature of this
translation. The complexity arises as a single optical data can have multiple
SAR representations based on the SAR viewing geometry. We propose a novel
approach, termed SAR Temporal Shifting, which inputs an optical data from the
desired timestamp along with a SAR data from a different temporal point but
with a consistent viewing geometry as the expected SAR data, both complemented
with a change map of optical data during the intervening period. This model
modifies the SAR data based on the changes observed in optical data to generate
the SAR data for the desired timestamp. Our model, a dual conditional
Generative Adversarial Network (GAN), named Temporal Shifting GAN (TSGAN),
incorporates a siamese encoder in both the Generator and the Discriminator. To
prevent the model from overfitting on the input SAR data, we employed a change
weighted loss function. Our approach surpasses traditional translation methods
by eliminating the GAN's fiction phenomenon, particularly in unchanged regions,
resulting in higher SSIM and PSNR in these areas. Additionally, modifications
to the Pix2Pix architecture and the inclusion of attention mechanisms have
enhanced the model's performance on all regions of the data. This research
paves the way for leveraging legacy optical datasets, the most abundant and
longstanding source of Earth imagery data, extending their use to SAR domains
and temporal analyses. To foster further research, we provide the code,
datasets used in our study, and a framework for generating paired SAR-Optical
datasets for new regions of interest. These resources are available on
github.com/moienr/TemporalGAN
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.00926">Accurate Leukocyte Detection Based on Deformable-DETR and Multi-Level Feature Fusion for Aiding Diagnosis of Blood Diseases. (arXiv:2401.00926v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yifei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chenyan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Ben Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yiyu Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yifei Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Changmiao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1">Xianjun Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1">Yuxing Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_F/0/1/0/all/0/1">Feiwei Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1">Yong Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yu Gao</a></p>
<p>In standard hospital blood tests, the traditional process requires doctors to
manually isolate leukocytes from microscopic images of patients' blood using
microscopes. These isolated leukocytes are then categorized via automatic
leukocyte classifiers to determine the proportion and volume of different types
of leukocytes present in the blood samples, aiding disease diagnosis. This
methodology is not only time-consuming and labor-intensive, but it also has a
high propensity for errors due to factors such as image quality and
environmental conditions, which could potentially lead to incorrect subsequent
classifications and misdiagnosis. To address these issues, this paper proposes
an innovative method of leukocyte detection: the Multi-level Feature Fusion and
Deformable Self-attention DETR (MFDS-DETR). To tackle the issue of leukocyte
scale disparity, we designed the High-level Screening-feature Fusion Pyramid
(HS-FPN), enabling multi-level fusion. This model uses high-level features as
weights to filter low-level feature information via a channel attention module
and then merges the screened information with the high-level features, thus
enhancing the model's feature expression capability. Further, we address the
issue of leukocyte feature scarcity by incorporating a multi-scale deformable
self-attention module in the encoder and using the self-attention and
cross-deformable attention mechanisms in the decoder, which aids in the
extraction of the global features of the leukocyte feature maps. The
effectiveness, superiority, and generalizability of the proposed MFDS-DETR
method are confirmed through comparisons with other cutting-edge leukocyte
detection models using the private WBCDD, public LISC and BCCD datasets. Our
source code and private WBCCD dataset are available at
https://github.com/JustlfC03/MFDS-DETR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01642">BLADE: Box-Level Supervised Amodal Segmentation through Directed Expansion. (arXiv:2401.01642v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhaochen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhixuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1">Tingting Jiang</a></p>
<p>Perceiving the complete shape of occluded objects is essential for human and
machine intelligence. While the amodal segmentation task is to predict the
complete mask of partially occluded objects, it is time-consuming and
labor-intensive to annotate the pixel-level ground truth amodal masks.
Box-level supervised amodal segmentation addresses this challenge by relying
solely on ground truth bounding boxes and instance classes as supervision,
thereby alleviating the need for exhaustive pixel-level annotations.
Nevertheless, current box-level methodologies encounter limitations in
generating low-resolution masks and imprecise boundaries, failing to meet the
demands of practical real-world applications. We present a novel solution to
tackle this problem by introducing a directed expansion approach from visible
masks to corresponding amodal masks. Our approach involves a hybrid end-to-end
network based on the overlapping region - the area where different instances
intersect. Diverse segmentation strategies are applied for overlapping regions
and non-overlapping regions according to distinct characteristics. To guide the
expansion of visible masks, we introduce an elaborately-designed connectivity
loss for overlapping regions, which leverages correlations with visible masks
and facilitates accurate amodal segmentation. Experiments are conducted on
several challenging datasets and the results show that our proposed method can
outperform existing state-of-the-art methods with large margins.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01736">Few-shot Adaptation of Multi-modal Foundation Models: A Survey. (arXiv:2401.01736v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1">Fan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tianshu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1">Wenwen Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1">Wenwen Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xiaocong Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Delong Chen</a></p>
<p>Multi-modal (vision-language) models, such as CLIP, are replacing traditional
supervised pre-training models (e.g., ImageNet-based pre-training) as the new
generation of visual foundation models. These models with robust and aligned
semantic representations learned from billions of internet image-text pairs and
can be applied to various downstream tasks in a zero-shot manner. However, in
some fine-grained domains like medical imaging and remote sensing, the
performance of multi-modal foundation models often leaves much to be desired.
Consequently, many researchers have begun to explore few-shot adaptation
methods for these models, gradually deriving three main technical approaches:
1) prompt-based methods, 2) adapter-based methods, and 3) external
knowledge-based methods. Nevertheless, this rapidly developing field has
produced numerous results without a comprehensive survey to systematically
organize the research progress. Therefore, in this survey, we introduce and
analyze the research advancements in few-shot adaptation methods for
multi-modal models, summarizing commonly used datasets and experimental setups,
and comparing the results of different methods. In addition, due to the lack of
reliable theoretical support for existing methods, we derive the few-shot
adaptation generalization error bound for multi-modal models. The theorem
reveals that the generalization error of multi-modal foundation models is
constrained by three factors: domain gap, model capacity, and sample size.
Based on this, we propose three possible solutions from the following aspects:
1) adaptive domain generalization, 2) adaptive model selection, and 3) adaptive
knowledge utilization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01822">HawkRover: An Autonomous mmWave Vehicular Communication Testbed with Multi-sensor Fusion and Deep Learning. (arXiv:2401.01822v2 [cs.IT] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_E/0/1/0/all/0/1">Ethan Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Haijian Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_M/0/1/0/all/0/1">Mingyue Ji</a></p>
<p>Connected and automated vehicles (CAVs) have become a transformative
technology that can change our daily life. Currently, millimeter-wave (mmWave)
bands are identified as the promising CAV connectivity solution. While it can
provide high data rate, their realization faces many challenges such as high
attenuation during mmWave signal propagation and mobility management. Existing
solution has to initiate pilot signal to measure channel information, then
apply signal processing to calculate the best narrow beam towards the receiver
end to guarantee sufficient signal power. This process takes significant
overhead and time, hence not suitable for vehicles. In this study, we propose
an autonomous and low-cost testbed to collect extensive co-located mmWave
signal and other sensors data such as LiDAR (Light Detection and Ranging),
cameras, ultrasonic, etc, traditionally for ``automated'', to facilitate mmWave
vehicular communications. Intuitively, these sensors can build a 3D map around
the vehicle and signal propagation path can be estimated, eliminating iterative
the process via pilot signals. This multimodal data fusion, together with AI,
is expected to bring significant advances in ``connected'' research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01839">Frequency Domain Modality-invariant Feature Learning for Visible-infrared Person Re-Identification. (arXiv:2401.01839v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yulin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tianzhu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yongdong Zhang</a></p>
<p>Visible-infrared person re-identification (VI-ReID) is challenging due to the
significant cross-modality discrepancies between visible and infrared images.
While existing methods have focused on designing complex network architectures
or using metric learning constraints to learn modality-invariant features, they
often overlook which specific component of the image causes the modality
discrepancy problem. In this paper, we first reveal that the difference in the
amplitude component of visible and infrared images is the primary factor that
causes the modality discrepancy and further propose a novel Frequency Domain
modality-invariant feature learning framework (FDMNet) to reduce modality
discrepancy from the frequency domain perspective. Our framework introduces two
novel modules, namely the Instance-Adaptive Amplitude Filter (IAF) module and
the Phrase-Preserving Normalization (PPNorm) module, to enhance the
modality-invariant amplitude component and suppress the modality-specific
component at both the image- and feature-levels. Extensive experimental results
on two standard benchmarks, SYSU-MM01 and RegDB, demonstrate the superior
performance of our FDMNet against state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.15216">Diabetic Retinopathy Using Gaussian Filter. (arXiv:2309.15216v2 [cs.LG] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Muddaluru_R/0/1/0/all/0/1">Roshan Vasu Muddaluru</a>, <a href="http://arxiv.org/find/cs/1/au:+Thoguluva_S/0/1/0/all/0/1">Sharvaani Ravikumar Thoguluva</a>, <a href="http://arxiv.org/find/cs/1/au:+Prabha_S/0/1/0/all/0/1">Shruti Prabha</a>, <a href="http://arxiv.org/find/cs/1/au:+Reddy_T/0/1/0/all/0/1">Tanuja Konda Reddy</a>, <a href="http://arxiv.org/find/cs/1/au:+P_D/0/1/0/all/0/1">Dr. Suja P</a></p>
<p>The retina is an essential component of the visual system, and maintaining
eyesight depends on the timely and correct detection of disorders. This
research specifically addresses the early-stage detection and severity
classification of diabetic retinopathy (DR), a serious public health hazard. We
compare the results of different deep learning models such as InceptionV3,
DenseNet121 and other CNN based models by using different image filters, such
as Gaussian, grayscale and Gabor. These models could detect subtle pathological
alterations and use that information to estimate the risk of retinal illnesses.
The objective is to improve the diagnostic processes for diabetic retinopathy,
the primary cause of diabetes-related blindness, by utilizing deep learning
models. A comparative analysis between Greyscale, Gaussian and Gabor filters
has been provided after applying these filters on the retinal images. The
Gaussian filter resulted to be the most promising filter giving the best
accuracies for all the models. The best performing model was InceptionV3 which
gave an accuracy of 96% on Gaussian images, therefore Gaussian filter emerged
as our most promising filter.
</p>
</p>
</div>

    </div>
    </body>
    