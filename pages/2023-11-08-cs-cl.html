<!DOCTYPE html>
<html>
<head>
<title>2023-11-08-cs-cl</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2311.02084">ITEm: Unsupervised Image-Text Embedding Learning for eCommerce. (arXiv:2311.02084v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liao_B/0/1/0/all/0/1">Baohao Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Kozielski_M/0/1/0/all/0/1">Michael Kozielski</a>, <a href="http://arxiv.org/find/cs/1/au:+Hewavitharana_S/0/1/0/all/0/1">Sanjika Hewavitharana</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1">Jiangbo Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Khadivi_S/0/1/0/all/0/1">Shahram Khadivi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lancewicki_T/0/1/0/all/0/1">Tomer Lancewicki</a></p>
<p>Product embedding serves as a cornerstone for a wide range of applications in
eCommerce. The product embedding learned from multiple modalities shows
significant improvement over that from a single modality, since different
modalities provide complementary information. However, some modalities are more
informatively dominant than others. How to teach a model to learn embedding
from different modalities without neglecting information from the less dominant
modality is challenging. We present an image-text embedding model (ITEm), an
unsupervised learning method that is designed to better attend to image and
text modalities. We extend BERT by (1) learning an embedding from text and
image without knowing the regions of interest; (2) training a global
representation to predict masked words and to construct masked image patches
without their individual representations. We evaluate the pre-trained ITEm on
two tasks: the search for extremely similar products and the prediction of
product categories, showing substantial gains compared to strong baseline
models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02089">LlamaRec: Two-Stage Recommendation using Large Language Models for Ranking. (arXiv:2311.02089v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yue_Z/0/1/0/all/0/1">Zhenrui Yue</a>, <a href="http://arxiv.org/find/cs/1/au:+Rabhi_S/0/1/0/all/0/1">Sara Rabhi</a>, <a href="http://arxiv.org/find/cs/1/au:+Moreira_G/0/1/0/all/0/1">Gabriel de Souza Pereira Moreira</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Oldridge_E/0/1/0/all/0/1">Even Oldridge</a></p>
<p>Recently, large language models (LLMs) have exhibited significant progress in
language understanding and generation. By leveraging textual features,
customized LLMs are also applied for recommendation and demonstrate
improvements across diverse recommendation scenarios. Yet the majority of
existing methods perform training-free recommendation that heavily relies on
pretrained knowledge (e.g., movie recommendation). In addition, inference on
LLMs is slow due to autoregressive generation, rendering existing methods less
effective for real-time recommendation. As such, we propose a two-stage
framework using large language models for ranking-based recommendation
(LlamaRec). In particular, we use small-scale sequential recommenders to
retrieve candidates based on the user interaction history. Then, both history
and retrieved items are fed to the LLM in text via a carefully designed prompt
template. Instead of generating next-item titles, we adopt a verbalizer-based
approach that transforms output logits into probability distributions over the
candidate items. Therefore, the proposed LlamaRec can efficiently rank items
without generating long text. To validate the effectiveness of the proposed
framework, we compare against state-of-the-art baseline methods on benchmark
datasets. Our experimental results demonstrate the performance of LlamaRec,
which consistently achieves superior performance in both recommendation
performance and efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02192">Automating Governing Knowledge Commons and Contextual Integrity (GKC-CI) Privacy Policy Annotations with Large Language Models. (arXiv:2311.02192v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chanenson_J/0/1/0/all/0/1">Jake Chanenson</a>, <a href="http://arxiv.org/find/cs/1/au:+Pickering_M/0/1/0/all/0/1">Madison Pickering</a>, <a href="http://arxiv.org/find/cs/1/au:+Apthorpe_N/0/1/0/all/0/1">Noah Apthorpe</a></p>
<p>Identifying contextual integrity (CI) and governing knowledge commons (GKC)
parameters in privacy policy texts can facilitate normative privacy analysis.
However, GKC-CI annotation has heretofore required manual or crowdsourced
effort. This paper demonstrates that high-accuracy GKC-CI parameter annotation
of privacy policies can be performed automatically using large language models.
We fine-tune 18 open-source and proprietary models on 21,588 GKC-CI annotations
from 16 ground truth privacy policies. Our best-performing model (fine-tuned
GPT-3.5 Turbo with prompt engineering) has an accuracy of 86%, exceeding the
performance of prior crowdsourcing approaches despite the complexity of privacy
policy texts and the nuance of the GKC-CI annotation task. We apply our
best-performing model to privacy policies from 164 popular online services,
demonstrating the effectiveness of scaling GKC-CI annotation for data
exploration. We make all annotated policies as well as the training data and
scripts needed to fine-tune our best-performing model publicly available for
future research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02205">An Introduction to Natural Language Processing Techniques and Framework for Clinical Implementation in Radiation Oncology. (arXiv:2311.02205v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khanmohammadi_R/0/1/0/all/0/1">Reza Khanmohammadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghassemi_M/0/1/0/all/0/1">Mohammad M. Ghassemi</a>, <a href="http://arxiv.org/find/cs/1/au:+Verdecchia_K/0/1/0/all/0/1">Kyle Verdecchia</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghanem_A/0/1/0/all/0/1">Ahmed I. Ghanem</a>, <a href="http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1">Luo Bing</a>, <a href="http://arxiv.org/find/cs/1/au:+Chetty_I/0/1/0/all/0/1">Indrin J. Chetty</a>, <a href="http://arxiv.org/find/cs/1/au:+Bagher_Ebadian_H/0/1/0/all/0/1">Hassan Bagher-Ebadian</a>, <a href="http://arxiv.org/find/cs/1/au:+Siddiqui_F/0/1/0/all/0/1">Farzan Siddiqui</a>, <a href="http://arxiv.org/find/cs/1/au:+Elshaikh_M/0/1/0/all/0/1">Mohamed Elshaikh</a>, <a href="http://arxiv.org/find/cs/1/au:+Movsas_B/0/1/0/all/0/1">Benjamin Movsas</a>, <a href="http://arxiv.org/find/cs/1/au:+Thind_K/0/1/0/all/0/1">Kundan Thind</a></p>
<p>Natural Language Processing (NLP) is a key technique for developing Medical
Artificial Intelligence (AI) systems that leverage Electronic Health Record
(EHR) data to build diagnostic and prognostic models. NLP enables the
conversion of unstructured clinical text into structured data that can be fed
into AI algorithms. The emergence of the transformer architecture and large
language models (LLMs) has led to remarkable advances in NLP for various
healthcare tasks, such as entity recognition, relation extraction, sentence
similarity, text summarization, and question answering. In this article, we
review the major technical innovations that underpin modern NLP models and
present state-of-the-art NLP applications that employ LLMs in radiation
oncology research. However, these LLMs are prone to many errors such as
hallucinations, biases, and ethical violations, which necessitate rigorous
evaluation and validation before clinical deployment. As such, we propose a
comprehensive framework for assessing the NLP models based on their purpose and
clinical fit, technical performance, bias and trust, legal and ethical
implications, and quality assurance, prior to implementation in clinical
radiation oncology. Our article aims to provide guidance and insights for
researchers and clinicians who are interested in developing and using NLP
models in clinical radiation oncology.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02216">Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data. (arXiv:2311.02216v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1">Mubashara Akhtar</a>, <a href="http://arxiv.org/find/cs/1/au:+Shankarampeta_A/0/1/0/all/0/1">Abhilash Shankarampeta</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1">Vivek Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Patil_A/0/1/0/all/0/1">Arpit Patil</a>, <a href="http://arxiv.org/find/cs/1/au:+Cocarascu_O/0/1/0/all/0/1">Oana Cocarascu</a>, <a href="http://arxiv.org/find/cs/1/au:+Simperl_E/0/1/0/all/0/1">Elena Simperl</a></p>
<p>Numbers are crucial for various real-world domains such as finance,
economics, and science. Thus, understanding and reasoning with numbers are
essential skills for language models to solve different tasks. While different
numerical benchmarks have been introduced in recent years, they are limited to
specific numerical aspects mostly. In this paper, we propose a hierarchical
taxonomy for numerical reasoning skills with more than ten reasoning types
across four levels: representation, number sense, manipulation, and complex
reasoning. We conduct a comprehensive evaluation of state-of-the-art models to
identify reasoning challenges specific to them. Henceforth, we develop a
diverse set of numerical probes employing a semi-automated approach. We focus
on the tabular Natural Language Inference (TNLI) task as a case study and
measure models' performance shifts. Our results show that no model consistently
excels across all numerical reasoning types. Among the probed models, FlanT5
(few-/zero-shot) and GPT-3.5 (few-shot) demonstrate strong overall numerical
reasoning skills compared to other models. Label-flipping probes indicate that
models often exploit dataset artifacts to predict the correct labels.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02236">Robust Fine-Tuning of Vision-Language Models for Domain Generalization. (arXiv:2311.02236v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vogt_Lowell_K/0/1/0/all/0/1">Kevin Vogt-Lowell</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1">Noah Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsiligkaridis_T/0/1/0/all/0/1">Theodoros Tsiligkaridis</a>, <a href="http://arxiv.org/find/cs/1/au:+Vaillant_M/0/1/0/all/0/1">Marc Vaillant</a></p>
<p>Transfer learning enables the sharing of common knowledge among models for a
variety of downstream tasks, but traditional methods suffer in limited training
data settings and produce narrow models incapable of effectively generalizing
under distribution shifts. Foundation models have recently demonstrated
impressive zero-shot inference capabilities and robustness under distribution
shifts. However, zero-shot evaluation for these models has been predominantly
confined to benchmarks with simple distribution shifts, limiting our
understanding of their effectiveness under the more realistic shifts found in
practice. Moreover, common fine-tuning methods for these models have yet to be
evaluated against vision models in few-shot scenarios where training data is
limited. To address these gaps, we present a new recipe for few-shot
fine-tuning of the popular vision-language foundation model CLIP and evaluate
its performance on challenging benchmark datasets with realistic distribution
shifts from the WILDS collection. Our experimentation demonstrates that, while
zero-shot CLIP fails to match performance of trained vision models on more
complex benchmarks, few-shot CLIP fine-tuning outperforms its vision-only
counterparts in terms of in-distribution and out-of-distribution accuracy at
all levels of training data availability. This provides a strong incentive for
adoption of foundation models within few-shot learning applications operating
with real-world data. Code is available at
https://github.com/mit-ll/robust-vision-language-finetuning
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02248">COSMIC: Data Efficient Instruction-tuning For Speech In-Context Learning. (arXiv:2311.02248v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1">Jing Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jian Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gaur_Y/0/1/0/all/0/1">Yashesh Gaur</a>, <a href="http://arxiv.org/find/cs/1/au:+Sivasankaran_S/0/1/0/all/0/1">Sunit Sivasankaran</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhuo Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shujie Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jinyu Li</a></p>
<p>We present a data and cost efficient way of incorporating the speech modality
into a large language model (LLM). The resulting multi-modal LLM is a
COntextual Speech Model with Instruction-following/in-context-learning
Capabilities - COSMIC. Speech comprehension test question-answer (SQA) pairs
are generated using GPT-3.5 based on the speech transcriptions as a part of the
supervision for the instruction tuning. With fewer than 20M trainable
parameters and as little as 450 hours of English speech data for SQA
generation, COSMIC exhibits emergent instruction-following and in-context
learning capabilities in speech-to-text tasks. The model is able to follow the
given text instructions to generate text response even on the unseen EN$\to$X
speech-to-text translation (S2TT) task with zero-shot setting. We evaluate the
model's in-context learning via various tasks such as EN$\to$X S2TT and
few-shot domain adaptation. And instruction-following capabilities are
evaluated through a contextual biasing benchmark. Our results demonstrate the
efficacy of the proposed low cost recipe for building a speech LLM and that
with the new instruction-tuning data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02262">Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs. (arXiv:2311.02262v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qingru Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_C/0/1/0/all/0/1">Chandan Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Liyuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiaodong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1">Bin Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jianfeng Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1">Tuo Zhao</a></p>
<p>In human-written articles, we often leverage the subtleties of text style,
such as bold and italics, to guide the attention of readers. These textual
emphases are vital for the readers to grasp the conveyed information. When
interacting with large language models (LLMs), we have a similar need -
steering the model to pay closer attention to user-specified information, e.g.,
an instruction. Existing methods, however, are constrained to process plain
text and do not support such a mechanism. This motivates us to introduce PASTA
- Post-hoc Attention STeering Approach, a method that allows LLMs to read text
with user-specified emphasis marks. To this end, PASTA identifies a small
subset of attention heads and applies precise attention reweighting on them,
directing the model attention to user-specified parts. Like prompting, PASTA is
applied at inference time and does not require changing any model parameters.
Experiments demonstrate that PASTA can substantially enhance an LLM's ability
to follow user instructions or integrate new knowledge from user inputs,
leading to a significant performance improvement on a variety of tasks, e.g.,
an average accuracy improvement of 22% for LLAMA-7B. Our code is publicly
available at https://github.com/QingruZhang/PASTA .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02265">Not all layers are equally as important: Every Layer Counts BERT. (arXiv:2311.02265v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Charpentier_L/0/1/0/all/0/1">Lucas Georges Gabriel Charpentier</a>, <a href="http://arxiv.org/find/cs/1/au:+Samuel_D/0/1/0/all/0/1">David Samuel</a></p>
<p>This paper introduces a novel modification of the transformer architecture,
tailored for the data-efficient pretraining of language models. This aspect is
evaluated by participating in the BabyLM challenge, where our solution won both
the \textsc{strict} and \textsc{strict-small} tracks. Our approach allows each
transformer layer to select which outputs of previous layers to process. The
empirical results verify the potential of this simple modification and show
that not all layers are equally as important.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02271">FaMeSumm: Investigating and Improving Faithfulness of Medical Summarization. (arXiv:2311.02271v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Nan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yusen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1">Wu Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitra_P/0/1/0/all/0/1">Prasenjit Mitra</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Rui Zhang</a></p>
<p>Summaries of medical text shall be faithful by being consistent and factual
with source inputs, which is an important but understudied topic for safety and
efficiency in healthcare. In this paper, we investigate and improve
faithfulness in summarization on a broad range of medical summarization tasks.
Our investigation reveals that current summarization models often produce
unfaithful outputs for medical input text. We then introduce FAMESUMM, a
framework to improve faithfulness by fine-tuning pre-trained language models
based on medical knowledge. FAMESUMM performs contrastive learning on designed
sets of faithful and unfaithful summaries, and it incorporates medical terms
and their contexts to encourage faithful generation of medical terms. We
conduct comprehensive experiments on three datasets in two languages: health
question and radiology report summarization datasets in English, and a
patient-doctor dialogue dataset in Chinese. Results demonstrate that FAMESUMM
is flexible and effective by delivering consistent improvements over mainstream
language models such as BART, T5, mT5, and PEGASUS, yielding state-of-the-art
performances on metrics for faithfulness and general quality. Human evaluation
by doctors also shows that FAMESUMM generates more faithful outputs. Our code
is available at https: //github.com/psunlpgroup/FaMeSumm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02294">LLMs grasp morality in concept. (arXiv:2311.02294v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pock_M/0/1/0/all/0/1">Mark Pock</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_A/0/1/0/all/0/1">Andre Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Moore_J/0/1/0/all/0/1">Jared Moore</a></p>
<p>Work in AI ethics and fairness has made much progress in regulating LLMs to
reflect certain values, such as fairness, truth, and diversity. However, it has
taken the problem of how LLMs might 'mean' anything at all for granted. Without
addressing this, it is not clear what imbuing LLMs with such values even means.
In response, we provide a general theory of meaning that extends beyond humans.
We use this theory to explicate the precise nature of LLMs as meaning-agents.
We suggest that the LLM, by virtue of its position as a meaning-agent, already
grasps the constructions of human society (e.g. morality, gender, and race) in
concept. Consequently, under certain ethical frameworks, currently popular
methods for model alignment are limited at best and counterproductive at worst.
Moreover, unaligned models may help us better develop our moral and social
philosophy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02310">Narrowing the Gap between Zero- and Few-shot Machine Translation by Matching Styles. (arXiv:2311.02310v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1">Weiting Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Haoran Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1">Lingfeng Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shuyue Stella Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Murray_K/0/1/0/all/0/1">Kenton Murray</a>, <a href="http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1">Philipp Koehn</a>, <a href="http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1">Benjamin Van Durme</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yunmo Chen</a></p>
<p>Large language models trained primarily in a monolingual setting have
demonstrated their ability to generalize to machine translation using zero- and
few-shot examples with in-context learning. However, even though zero-shot
translations are relatively good, there remains a discernible gap comparing
their performance with the few-shot setting. In this paper, we investigate the
factors contributing to this gap and find that this gap can largely be closed
(for about 70%) by matching the writing styles of the target corpus.
Additionally, we explore potential approaches to enhance zero-shot baselines
without the need for parallel demonstration examples, providing valuable
insights into how these methods contribute to improving translation metrics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02321">Identifying Context-Dependent Translations for Evaluation Set Production. (arXiv:2311.02321v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wicks_R/0/1/0/all/0/1">Rachel Wicks</a>, <a href="http://arxiv.org/find/cs/1/au:+Post_M/0/1/0/all/0/1">Matt Post</a></p>
<p>A major impediment to the transition to context-aware machine translation is
the absence of good evaluation metrics and test sets. Sentences that require
context to be translated correctly are rare in test sets, reducing the utility
of standard corpus-level metrics such as COMET or BLEU. On the other hand,
datasets that annotate such sentences are also rare, small in scale, and
available for only a few languages. To address this, we modernize, generalize,
and extend previous annotation pipelines to produce CTXPRO, a tool that
identifies subsets of parallel documents containing sentences that require
context to correctly translate five phenomena: gender, formality, and animacy
for pronouns, verb phrase ellipsis, and ambiguous noun inflections. The input
to the pipeline is a set of hand-crafted, per-language, linguistically-informed
rules that select contextual sentence pairs using coreference, part-of-speech,
and morphological features provided by state-of-the-art tools. We apply this
pipeline to seven languages pairs (EN into and out-of DE, ES, FR, IT, PL, PT,
and RU) and two datasets (OpenSubtitles and WMT test sets), and validate its
performance using both overlap with previous work and its ability to
discriminate a contextual MT system from a sentence-based one. We release the
CTXPRO pipeline and data as open source.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2007.01777">ProtoryNet - Interpretable Text Classification Via Prototype Trajectories. (arXiv:2007.01777v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1">Dat Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Baek_S/0/1/0/all/0/1">Stephen S. Baek</a></p>
<p>We propose a novel interpretable deep neural network for text classification,
called ProtoryNet, based on a new concept of prototype trajectories. Motivated
by the prototype theory in modern linguistics, ProtoryNet makes a prediction by
finding the most similar prototype for each sentence in a text sequence and
feeding an RNN backbone with the proximity of each sentence to the
corresponding active prototype. The RNN backbone then captures the temporal
pattern of the prototypes, which we refer to as prototype trajectories.
Prototype trajectories enable intuitive and fine-grained interpretation of the
reasoning process of the RNN model, in resemblance to how humans analyze texts.
We also design a prototype pruning procedure to reduce the total number of
prototypes used by the model for better interpretability. Experiments on
multiple public data sets show that ProtoryNet is more accurate than the
baseline prototype-based deep neural net and reduces the performance gap
compared to state-of-the-art black-box models. In addition, after prototype
pruning, the resulting ProtoryNet models only need less than or around 20
prototypes for all datasets, which significantly benefits interpretability.
Furthermore, we report a survey result indicating that human users find
ProtoryNet more intuitive and easier to understand than other prototype-based
methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.00638">Composable Text Controls in Latent Space with ODEs. (arXiv:2208.00638v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1">Guangyi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1">Zeyu Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yuan Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zichao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1">Xiaodan Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1">Junwei Bao</a>, <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xiaodong He</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1">Shuguang Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1">Zhiting Hu</a></p>
<p>Real-world text applications often involve composing a wide range of text
control operations, such as editing the text w.r.t. an attribute, manipulating
keywords and structure, and generating new text of desired properties. Prior
work typically learns/finetunes a language model (LM) to perform individual or
specific subsets of operations. Recent research has studied combining
operations in a plug-and-play manner, often with costly search or optimization
in the complex sequence space. This paper proposes a new efficient approach for
composable text operations in the compact latent space of text. The
low-dimensionality and differentiability of the text latent vector allow us to
develop an efficient sampler based on ordinary differential equations (ODEs)
given arbitrary plug-in operators (e.g., attribute classifiers). By connecting
pretrained LMs (e.g., GPT2) to the latent space through efficient adaption, we
then decode the sampled vectors into desired text sequences. The flexible
approach permits diverse control operators (sentiment, tense, formality,
keywords, etc.) acquired using any relevant data from different domains.
Experiments show that composing those operators within our approach manages to
generate or edit high-quality text, substantially improving over previous
methods in terms of generation quality and efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.11081">A Theory of Unsupervised Translation Motivated by Understanding Animal Communication. (arXiv:2211.11081v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Goldwasser_S/0/1/0/all/0/1">Shafi Goldwasser</a>, <a href="http://arxiv.org/find/cs/1/au:+Gruber_D/0/1/0/all/0/1">David F. Gruber</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalai_A/0/1/0/all/0/1">Adam Tauman Kalai</a>, <a href="http://arxiv.org/find/cs/1/au:+Paradise_O/0/1/0/all/0/1">Orr Paradise</a></p>
<p>Neural networks are capable of translating between languages -- in some cases
even between two languages where there is little or no access to parallel
translations, in what is known as Unsupervised Machine Translation (UMT). Given
this progress, it is intriguing to ask whether machine learning tools can
ultimately enable understanding animal communication, particularly that of
highly intelligent animals. We propose a theoretical framework for analyzing
UMT when no parallel translations are available and when it cannot be assumed
that the source and target corpora address related subject domains or posses
similar linguistic structure. We exemplify this theory with two stylized models
of language, for which our framework provides bounds on necessary sample
complexity; the bounds are formally proven and experimentally verified on
synthetic data. These bounds show that the error rates are inversely related to
the language complexity and amount of common ground. This suggests that
unsupervised translation of animal communication may be feasible if the
communication system is sufficiently complex.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.09577">CiteBench: A benchmark for Scientific Citation Text Generation. (arXiv:2212.09577v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Funkquist_M/0/1/0/all/0/1">Martin Funkquist</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuznetsov_I/0/1/0/all/0/1">Ilia Kuznetsov</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1">Yufang Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1">Iryna Gurevych</a></p>
<p>Science progresses by building upon the prior body of knowledge documented in
scientific publications. The acceleration of research makes it hard to stay
up-to-date with the recent developments and to summarize the ever-growing body
of prior work. To address this, the task of citation text generation aims to
produce accurate textual summaries given a set of papers-to-cite and the citing
paper context. Due to otherwise rare explicit anchoring of cited documents in
the citing paper, citation text generation provides an excellent opportunity to
study how humans aggregate and synthesize textual knowledge from sources. Yet,
existing studies are based upon widely diverging task definitions, which makes
it hard to study this task systematically. To address this challenge, we
propose CiteBench: a benchmark for citation text generation that unifies
multiple diverse datasets and enables standardized evaluation of citation text
generation models across task designs and domains. Using the new benchmark, we
investigate the performance of multiple strong baselines, test their
transferability between the datasets, and deliver new insights into the task
definition and evaluation to guide future research in citation text generation.
We make the code for CiteBench publicly available at
https://github.com/UKPLab/citebench.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.06627">Dissociating language and thought in large language models. (arXiv:2301.06627v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mahowald_K/0/1/0/all/0/1">Kyle Mahowald</a>, <a href="http://arxiv.org/find/cs/1/au:+Ivanova_A/0/1/0/all/0/1">Anna A. Ivanova</a>, <a href="http://arxiv.org/find/cs/1/au:+Blank_I/0/1/0/all/0/1">Idan A. Blank</a>, <a href="http://arxiv.org/find/cs/1/au:+Kanwisher_N/0/1/0/all/0/1">Nancy Kanwisher</a>, <a href="http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1">Joshua B. Tenenbaum</a>, <a href="http://arxiv.org/find/cs/1/au:+Fedorenko_E/0/1/0/all/0/1">Evelina Fedorenko</a></p>
<p>Large language models (LLMs) have come closest among all models to date to
mastering human language, yet opinions about their linguistic and cognitive
capabilities remain split. Here, we evaluate LLMs using a distinction between
formal linguistic competence--knowledge of linguistic rules and patterns--and
functional linguistic competence--understanding and using language in the
world. We ground this distinction in human neuroscience, showing that formal
and functional competence rely on different neural mechanisms. Although LLMs
are surprisingly good at formal competence, their performance on functional
competence tasks remains spotty and often requires specialized fine-tuning
and/or coupling with external modules. In short, LLMs are good models of
language but incomplete models of human thought.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.12608">Evaluating Neuron Interpretation Methods of NLP Models. (arXiv:2301.12608v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1">Yimin Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Dalvi_F/0/1/0/all/0/1">Fahim Dalvi</a>, <a href="http://arxiv.org/find/cs/1/au:+Durrani_N/0/1/0/all/0/1">Nadir Durrani</a>, <a href="http://arxiv.org/find/cs/1/au:+Sajjad_H/0/1/0/all/0/1">Hassan Sajjad</a></p>
<p>Neuron Interpretation has gained traction in the field of interpretability,
and have provided fine-grained insights into what a model learns and how
language knowledge is distributed amongst its different components. However,
the lack of evaluation benchmark and metrics have led to siloed progress within
these various methods, with very little work comparing them and highlighting
their strengths and weaknesses. The reason for this discrepancy is the
difficulty of creating ground truth datasets, for example, many neurons within
a given model may learn the same phenomena, and hence there may not be one
correct answer. Moreover, a learned phenomenon may spread across several
neurons that work together -- surfacing these to create a gold standard
challenging. In this work, we propose an evaluation framework that measures the
compatibility of a neuron analysis method with other methods. We hypothesize
that the more compatible a method is with the majority of the methods, the more
confident one can be about its performance. We systematically evaluate our
proposed framework and present a comparative analysis of a large set of neuron
interpretation methods. We make the evaluation framework available to the
community. It enables the evaluation of any new method using 20 concepts and
across three pre-trained models.The code is released at
https://github.com/fdalvi/neuron-comparative-analysis
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.08956">AfriSenti: A Twitter Sentiment Analysis Benchmark for African Languages. (arXiv:2302.08956v5 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Muhammad_S/0/1/0/all/0/1">Shamsuddeen Hassan Muhammad</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdulmumin_I/0/1/0/all/0/1">Idris Abdulmumin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ayele_A/0/1/0/all/0/1">Abinew Ali Ayele</a>, <a href="http://arxiv.org/find/cs/1/au:+Ousidhoum_N/0/1/0/all/0/1">Nedjma Ousidhoum</a>, <a href="http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1">David Ifeoluwa Adelani</a>, <a href="http://arxiv.org/find/cs/1/au:+Yimam_S/0/1/0/all/0/1">Seid Muhie Yimam</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmad_I/0/1/0/all/0/1">Ibrahim Sa&#x27;id Ahmad</a>, <a href="http://arxiv.org/find/cs/1/au:+Beloucif_M/0/1/0/all/0/1">Meriem Beloucif</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1">Saif M. Mohammad</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1">Sebastian Ruder</a>, <a href="http://arxiv.org/find/cs/1/au:+Hourrane_O/0/1/0/all/0/1">Oumaima Hourrane</a>, <a href="http://arxiv.org/find/cs/1/au:+Brazdil_P/0/1/0/all/0/1">Pavel Brazdil</a>, <a href="http://arxiv.org/find/cs/1/au:+Ali_F/0/1/0/all/0/1">Felermino D&#xe1;rio M&#xe1;rio Ant&#xf3;nio Ali</a>, <a href="http://arxiv.org/find/cs/1/au:+David_D/0/1/0/all/0/1">Davis David</a>, <a href="http://arxiv.org/find/cs/1/au:+Osei_S/0/1/0/all/0/1">Salomey Osei</a>, <a href="http://arxiv.org/find/cs/1/au:+Bello_B/0/1/0/all/0/1">Bello Shehu Bello</a>, <a href="http://arxiv.org/find/cs/1/au:+Ibrahim_F/0/1/0/all/0/1">Falalu Ibrahim</a>, <a href="http://arxiv.org/find/cs/1/au:+Gwadabe_T/0/1/0/all/0/1">Tajuddeen Gwadabe</a>, <a href="http://arxiv.org/find/cs/1/au:+Rutunda_S/0/1/0/all/0/1">Samuel Rutunda</a>, <a href="http://arxiv.org/find/cs/1/au:+Belay_T/0/1/0/all/0/1">Tadesse Belay</a>, <a href="http://arxiv.org/find/cs/1/au:+Messelle_W/0/1/0/all/0/1">Wendimu Baye Messelle</a>, <a href="http://arxiv.org/find/cs/1/au:+Balcha_H/0/1/0/all/0/1">Hailu Beshada Balcha</a>, <a href="http://arxiv.org/find/cs/1/au:+Chala_S/0/1/0/all/0/1">Sisay Adugna Chala</a>, <a href="http://arxiv.org/find/cs/1/au:+Gebremichael_H/0/1/0/all/0/1">Hagos Tesfahun Gebremichael</a>, <a href="http://arxiv.org/find/cs/1/au:+Opoku_B/0/1/0/all/0/1">Bernard Opoku</a>, <a href="http://arxiv.org/find/cs/1/au:+Arthur_S/0/1/0/all/0/1">Steven Arthur</a></p>
<p>Africa is home to over 2,000 languages from more than six language families
and has the highest linguistic diversity among all continents. These include 75
languages with at least one million speakers each. Yet, there is little NLP
research conducted on African languages. Crucial to enabling such research is
the availability of high-quality annotated datasets. In this paper, we
introduce AfriSenti, a sentiment analysis benchmark that contains a total of
&gt;110,000 tweets in 14 African languages (Amharic, Algerian Arabic, Hausa, Igbo,
Kinyarwanda, Moroccan Arabic, Mozambican Portuguese, Nigerian Pidgin, Oromo,
Swahili, Tigrinya, Twi, Xitsonga, and Yor\`ub\'a) from four language families.
The tweets were annotated by native speakers and used in the AfriSenti-SemEval
shared task (The AfriSenti Shared Task had over 200 participants. See website
at https://afrisenti-semeval.github.io). We describe the data collection
methodology, annotation process, and the challenges we dealt with when curating
each dataset. We further report baseline experiments conducted on the different
datasets and discuss their usefulness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.03004">xCodeEval: A Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval. (arXiv:2303.03004v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1">Mohammad Abdullah Matin Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bari_M/0/1/0/all/0/1">M Saiful Bari</a>, <a href="http://arxiv.org/find/cs/1/au:+Do_X/0/1/0/all/0/1">Xuan Long Do</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Weishi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Parvez_M/0/1/0/all/0/1">Md Rizwan Parvez</a>, <a href="http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1">Shafiq Joty</a></p>
<p>Recently, pre-trained large language models (LLMs) have shown impressive
abilities in generating codes from natural language descriptions, repairing
buggy codes, translating codes between languages, and retrieving relevant code
segments. However, the evaluation of these models has often been performed in a
scattered way on only one or two specific tasks, in a few languages, at a
partial granularity (e.g., function) level, and in many cases without proper
training data. Even more concerning is that in most cases the evaluation of
generated codes has been done in terms of mere lexical overlap with a reference
code rather than actual execution. We introduce xCodeEval, the largest
executable multilingual multitask benchmark to date consisting of $25$M
document-level coding examples ($16.5$B tokens) from about $7.5$K unique
problems covering up to $11$ programming languages with execution-level
parallelism. It features a total of $7$ tasks involving code understanding,
generation, translation and retrieval. xCodeEval adopts an execution-based
evaluation and offers a multilingual code execution engine, ExecEval that
supports unit test based execution in all the $11$ languages. To address the
challenge of balancing the distributions of text-code samples over multiple
attributes in validation/test sets, we propose a novel data splitting and a
data selection schema based on the geometric mean and graph-theoretic
principle. Our experiments with OpenAI's LLMs (zero-shot) and open-LLMs
(zero-shot and fine-tuned) on the tasks and languages demonstrate **xCodeEval**
to be quite challenging as per the current advancements in language models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.03480">Can an Embodied Agent Find Your &quot;Cat-shaped Mug&quot;? LLM-Guided Exploration for Zero-Shot Object Navigation. (arXiv:2303.03480v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dorbala_V/0/1/0/all/0/1">Vishnu Sashank Dorbala</a>, <a href="http://arxiv.org/find/cs/1/au:+Mullen_J/0/1/0/all/0/1">James F. Mullen Jr.</a>, <a href="http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1">Dinesh Manocha</a></p>
<p>We present LGX (Language-guided Exploration), a novel algorithm for
Language-Driven Zero-Shot Object Goal Navigation (L-ZSON), where an embodied
agent navigates to a uniquely described target object in a previously unseen
environment. Our approach makes use of Large Language Models (LLMs) for this
task by leveraging the LLM's commonsense reasoning capabilities for making
sequential navigational decisions. Simultaneously, we perform generalized
target object detection using a pre-trained Vision-Language grounding model. We
achieve state-of-the-art zero-shot object navigation results on RoboTHOR with a
success rate (SR) improvement of over 27% over the current baseline of the
OWL-ViT CLIP on Wheels (OWL CoW). Furthermore, we study the usage of LLMs for
robot navigation and present an analysis of various prompting strategies
affecting the model output. Finally, we showcase the benefits of our approach
via \textit{real-world} experiments that indicate the superior performance of
LGX in detecting and navigating to visually unique objects.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.12513">Is BERT Blind? Exploring the Effect of Vision-and-Language Pretraining on Visual Language Understanding. (arXiv:2303.12513v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alper_M/0/1/0/all/0/1">Morris Alper</a>, <a href="http://arxiv.org/find/cs/1/au:+Fiman_M/0/1/0/all/0/1">Michael Fiman</a>, <a href="http://arxiv.org/find/cs/1/au:+Averbuch_Elor_H/0/1/0/all/0/1">Hadar Averbuch-Elor</a></p>
<p>Most humans use visual imagination to understand and reason about language,
but models such as BERT reason about language using knowledge acquired during
text-only pretraining. In this work, we investigate whether vision-and-language
pretraining can improve performance on text-only tasks that involve implicit
visual reasoning, focusing primarily on zero-shot probing methods. We propose a
suite of visual language understanding (VLU) tasks for probing the visual
reasoning abilities of text encoder models, as well as various non-visual
natural language understanding (NLU) tasks for comparison. We also contribute a
novel zero-shot knowledge probing method, Stroop probing, for applying models
such as CLIP to text-only tasks without needing a prediction head such as the
masked language modelling head of models like BERT. We show that SOTA
multimodally trained text encoders outperform unimodally trained text encoders
on the VLU tasks while being underperformed by them on the NLU tasks, lending
new context to previously mixed results regarding the NLU capabilities of
multimodal models. We conclude that exposure to images during pretraining
affords inherent visual reasoning knowledge that is reflected in language-only
tasks that require implicit visual reasoning. Our findings bear importance in
the broader context of multimodal learning, providing principled guidelines for
the choice of text encoders used in such contexts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.15987">Sentiment Analysis Dataset in Moroccan Dialect: Bridging the Gap Between Arabic and Latin Scripted dialect. (arXiv:2303.15987v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jbel_M/0/1/0/all/0/1">Mouad Jbel</a>, <a href="http://arxiv.org/find/cs/1/au:+Hafidi_I/0/1/0/all/0/1">Imad Hafidi</a>, <a href="http://arxiv.org/find/cs/1/au:+Metrane_A/0/1/0/all/0/1">Abdulmutallib Metrane</a></p>
<p>Sentiment analysis, the automated process of determining emotions or opinions
expressed in text, has seen extensive exploration in the field of natural
language processing. However, one aspect that has remained underrepresented is
the sentiment analysis of the Moroccan dialect, which boasts a unique
linguistic landscape and the coexistence of multiple scripts. Previous works in
sentiment analysis primarily targeted dialects employing Arabic script. While
these efforts provided valuable insights, they may not fully capture the
complexity of Moroccan web content, which features a blend of Arabic and Latin
script. As a result, our study emphasizes the importance of extending sentiment
analysis to encompass the entire spectrum of Moroccan linguistic diversity.
Central to our research is the creation of the largest public dataset for
Moroccan dialect sentiment analysis that incorporates not only Moroccan dialect
written in Arabic script but also in Latin letters. By assembling a diverse
range of textual data, we were able to construct a dataset with a range of 20
000 manually labeled text in Moroccan dialect and also publicly available lists
of stop words in Moroccan dialect. To dive into sentiment analysis, we
conducted a comparative study on multiple Machine learning models to assess
their compatibility with our dataset. Experiments were performed using both raw
and preprocessed data to show the importance of the preprocessing step. We were
able to achieve 92% accuracy in our model and to further prove its liability we
tested our model on smaller publicly available datasets of Moroccan dialect and
the results were favorable.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.01246">Safety Analysis in the Era of Large Language Models: A Case Study of STPA using ChatGPT. (arXiv:2304.01246v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1">Yi Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xingyu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Khastgir_S/0/1/0/all/0/1">Siddartha Khastgir</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xiaowei Huang</a></p>
<p>Can safety analysis make use of Large Language Models (LLMs)? A case study
explores Systems Theoretic Process Analysis (STPA) applied to Automatic
Emergency Brake (AEB) and Electricity Demand Side Management (DSM) systems
using ChatGPT. We investigate how collaboration schemes, input semantic
complexity, and prompt guidelines influence STPA results. Comparative results
show that using ChatGPT without human intervention may be inadequate due to
reliability related issues, but with careful design, it may outperform human
experts. No statistically significant differences are found when varying the
input semantic complexity or using common prompt guidelines, which suggests the
necessity for developing domain-specific prompt engineering. We also highlight
future challenges, including concerns about LLM trustworthiness and the
necessity for standardisation and regulation in this domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.02386">Approximating CKY with Transformers. (arXiv:2305.02386v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khalighinejad_G/0/1/0/all/0/1">Ghazal Khalighinejad</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_O/0/1/0/all/0/1">Ollie Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wiseman_S/0/1/0/all/0/1">Sam Wiseman</a></p>
<p>We investigate the ability of transformer models to approximate the CKY
algorithm, using them to directly predict a sentence's parse and thus avoid the
CKY algorithm's cubic dependence on sentence length. We find that on standard
constituency parsing benchmarks this approach achieves competitive or better
performance than comparable parsers that make use of CKY, while being faster.
We also evaluate the viability of this approach for parsing under
\textit{random} PCFGs. Here we find that performance declines as the grammar
becomes more ambiguous, suggesting that the transformer is not fully capturing
the CKY computation. However, we also find that incorporating additional
inductive bias is helpful, and we propose a novel approach that makes use of
gradients with respect to chart representations in predicting the parse, in
analogy with the CKY algorithm being a subgradient of a partition function
variant with respect to the chart.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.02820">Semantic Space Grounded Weighted Decoding for Multi-Attribute Controllable Dialogue Generation. (arXiv:2305.02820v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhiling Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1">Mengyue Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1">Kenny Q. Zhu</a></p>
<p>Controlling chatbot utterance generation with multiple attributes such as
personalities, emotions and dialogue acts is a practically useful but
under-studied problem. We propose a novel framework called DASC that possesses
strong controllability with a weighted decoding paradigm, while improving
generation quality with the grounding in an attribute semantics space.
Generation with multiple attributes is then intuitively implemented with an
interpolation of multiple attribute embeddings, which results in substantial
reduction in the model sizes. Experiments show that DASC can achieve high
control accuracy in generation task with the simultaneous control of 3 aspects
while also producing interesting and reasonably sensible responses, even in an
out-of-distribution robustness test.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.06349">RECKONING: Reasoning through Dynamic Knowledge Encoding. (arXiv:2305.06349v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zeming Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Weiss_G/0/1/0/all/0/1">Gail Weiss</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitchell_E/0/1/0/all/0/1">Eric Mitchell</a>, <a href="http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1">Asli Celikyilmaz</a>, <a href="http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1">Antoine Bosselut</a></p>
<p>Recent studies on transformer-based language models show that they can answer
questions by reasoning over knowledge provided as part of the context (i.e.,
in-context reasoning). However, since the available knowledge is often not
filtered for a particular question, in-context reasoning can be sensitive to
distractor facts, additional content that is irrelevant to a question but that
may be relevant for a different question (i.e., not necessarily random noise).
In these situations, the model fails to distinguish the knowledge that is
necessary to answer the question, leading to spurious reasoning and degraded
performance. This reasoning failure contrasts with the model's apparent ability
to distinguish its contextual knowledge from all the knowledge it has memorized
during pre-training. Following this observation, we propose teaching the model
to reason more robustly by folding the provided contextual knowledge into the
model's parameters before presenting it with a question. Our method, RECKONING,
is a bi-level learning algorithm that teaches language models to reason by
updating their parametric knowledge through back-propagation, allowing them to
then answer questions using the updated parameters. During training, the inner
loop rapidly adapts a copy of the model weights to encode contextual knowledge
into its parameters. In the outer loop, the model learns to use the updated
weights to reproduce and answer reasoning questions about the memorized
knowledge. Our experiments on two multi-hop reasoning datasets show that
RECKONING's performance improves over the in-context reasoning baseline (by up
to 4.5%). We also find that compared to in-context reasoning, RECKONING
generalizes better to longer reasoning chains unseen during training, is more
robust to distractors in the context, and is more computationally efficient
when multiple questions are asked about the same knowledge.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.08322">C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models. (arXiv:2305.08322v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yuzhen Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1">Yuzhuo Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Zhihao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Junlei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jinghan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_T/0/1/0/all/0/1">Tangjun Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Junteng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_C/0/1/0/all/0/1">Chuancheng Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yikai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1">Jiayi Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1">Yao Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1">Maosong Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Junxian He</a></p>
<p>New NLP benchmarks are urgently needed to align with the rapid development of
large language models (LLMs). We present C-Eval, the first comprehensive
Chinese evaluation suite designed to assess advanced knowledge and reasoning
abilities of foundation models in a Chinese context. C-Eval comprises
multiple-choice questions across four difficulty levels: middle school, high
school, college, and professional. The questions span 52 diverse disciplines,
ranging from humanities to science and engineering. C-Eval is accompanied by
C-Eval Hard, a subset of very challenging subjects in C-Eval that requires
advanced reasoning abilities to solve. We conduct a comprehensive evaluation of
the most advanced LLMs on C-Eval, including both English- and Chinese-oriented
models. Results indicate that only GPT-4 could achieve an average accuracy of
over 60%, suggesting that there is still significant room for improvement for
current LLMs. We anticipate C-Eval will help analyze important strengths and
shortcomings of foundation models, and foster their development and growth for
Chinese users.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.10927">Causal Document-Grounded Dialogue Pre-training. (arXiv:2305.10927v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yingxiu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1">Bowen Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Haiyang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bowen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jinyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1">Fei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yongbin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Nevin L. Zhang</a></p>
<p>The goal of document-grounded dialogue (DocGD) is to generate a response by
grounding the evidence in a supporting document in accordance with the dialogue
context. This process involves four variables that are causally connected.
Recently, task-specific pre-training has greatly boosted performances on many
downstream tasks. Existing DocGD methods, however, continue to rely on general
pre-trained language models without a specifically tailored pre-training
approach that explicitly captures the causal relationships. To tackle this
issue, we are the first to present a causally-complete dataset construction
strategy for building million-level DocGD pre-training corpora. To better
capture causality, we further propose a causally-perturbed pre-training
strategy, which introduces causal perturbations on the variables and optimizes
the overall causal effect. Experiments on three benchmark datasets demonstrate
that our causal pre-training achieves considerable and consistent improvements
under fully-supervised, low-resource, few-shot, and zero-shot settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.12473">Continually Improving Extractive QA via Human Feedback. (arXiv:2305.12473v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1">Ge Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hung-Ting Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1">Yoav Artzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1">Eunsol Choi</a></p>
<p>We study continually improving an extractive question answering (QA) system
via human user feedback. We design and deploy an iterative approach, where
information-seeking users ask questions, receive model-predicted answers, and
provide feedback. We conduct experiments involving thousands of user
interactions under diverse setups to broaden the understanding of learning from
feedback over time. Our experiments show effective improvement from user
feedback of extractive QA models over time across different data regimes,
including significant potential for domain adaptation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13903">Let&#x27;s Think Frame by Frame with VIP: A Video Infilling and Prediction Dataset for Evaluating Video Chain-of-Thought. (arXiv:2305.13903v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Himakunthala_V/0/1/0/all/0/1">Vaishnavi Himakunthala</a>, <a href="http://arxiv.org/find/cs/1/au:+Ouyang_A/0/1/0/all/0/1">Andy Ouyang</a>, <a href="http://arxiv.org/find/cs/1/au:+Rose_D/0/1/0/all/0/1">Daniel Rose</a>, <a href="http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1">Ryan He</a>, <a href="http://arxiv.org/find/cs/1/au:+Mei_A/0/1/0/all/0/1">Alex Mei</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yujie Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sonar_C/0/1/0/all/0/1">Chinmay Sonar</a>, <a href="http://arxiv.org/find/cs/1/au:+Saxon_M/0/1/0/all/0/1">Michael Saxon</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">William Yang Wang</a></p>
<p>Despite exciting recent results showing vision-language systems' capacity to
reason about images using natural language, their capacity for video reasoning
remains under-explored. We motivate framing video reasoning as the sequential
understanding of a small number of keyframes, thereby leveraging the power and
robustness of vision-language while alleviating the computational complexities
of processing videos. To evaluate this novel application, we introduce VIP, an
inference-time challenge dataset designed to explore models' reasoning
capabilities through video chain-of-thought. Inspired by visually descriptive
scene plays, we propose two formats for keyframe description: unstructured
dense captions and structured scene descriptions that identify the focus,
action, mood, objects, and setting (FAMOuS) of the keyframe. To evaluate video
reasoning, we propose two tasks: Video Infilling and Video Prediction, which
test abilities to generate multiple intermediate keyframes and predict future
keyframes, respectively. We benchmark GPT-4, GPT-3, and VICUNA on VIP,
demonstrate the performance gap in these complex video reasoning tasks, and
encourage future work to prioritize language models for efficient and
generalized video reasoning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14202">Fine-tuned LLMs Know More, Hallucinate Less with Few-Shot Sequence-to-Sequence Semantic Parsing over Wikidata. (arXiv:2305.14202v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Silei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shicheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Culhane_T/0/1/0/all/0/1">Theo Culhane</a>, <a href="http://arxiv.org/find/cs/1/au:+Pertseva_E/0/1/0/all/0/1">Elizaveta Pertseva</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1">Meng-Hsi Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Semnani_S/0/1/0/all/0/1">Sina J. Semnani</a>, <a href="http://arxiv.org/find/cs/1/au:+Lam_M/0/1/0/all/0/1">Monica S. Lam</a></p>
<p>While large language models (LLMs) can answer many questions correctly, they
can also hallucinate and give wrong answers. Wikidata, with its over 12 billion
facts, can be used to ground LLMs to improve their factuality. This paper
presents WikiWebQuestions, a high-quality question answering benchmark for
Wikidata. Ported over from WebQuestions for Freebase, it consists of real-world
data with SPARQL annotation. This paper presents a few-shot
sequence-to-sequence semantic parser for Wikidata. We modify SPARQL to use the
unique domain and property names instead of their IDs. We train the parser to
use either the results from an entity linker or mentions in the query. We
fine-tune LLaMA by adding the few-shot training data to that used to fine-tune
Alpaca. Our experimental results demonstrate the effectiveness of this
methodology, establishing a strong baseline of 76% and 65% answer accuracy in
the dev and test sets of WikiWebQuestions, respectively. By pairing our
semantic parser with GPT-3, we combine verifiable results with qualified GPT-3
guesses to provide useful answers to 96% of the questions in dev. We also show
that our method outperforms the state-of-the-art for the QALD-7 Wikidata
dataset by 3.6% in F1 score.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14263">LIMIT: Language Identification, Misidentification, and Translation using Hierarchical Models in 350+ Languages. (arXiv:2305.14263v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Agarwal_M/0/1/0/all/0/1">Milind Agarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1">Md Mahfuz Ibn Alam</a>, <a href="http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1">Antonios Anastasopoulos</a></p>
<p>Knowing the language of an input text/audio is a necessary first step for
using almost every NLP tool such as taggers, parsers, or translation systems.
Language identification is a well-studied problem, sometimes even considered
solved; in reality, due to lack of data and computational challenges, current
systems cannot accurately identify most of the world's 7000 languages. To
tackle this bottleneck, we first compile a corpus, MCS-350, of 50K multilingual
and parallel children's stories in 350+ languages. MCS-350 can serve as a
benchmark for language identification of short texts and for 1400+ new
translation directions in low-resource Indian and African languages. Second, we
propose a novel misprediction-resolution hierarchical model, LIMIt, for
language identification that reduces error by 55% (from 0.71 to 0.32) on our
compiled children's stories dataset and by 40% (from 0.23 to 0.14) on the
FLORES-200 benchmark. Our method can expand language identification coverage
into low-resource languages by relying solely on systemic misprediction
patterns, bypassing the need to retrain large models from scratch.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14323">ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models. (arXiv:2305.14323v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhipeng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1">Kun Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Beichen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1">Zheng Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Wayne Xin Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1">Ji-Rong Wen</a></p>
<p>Although large language models (LLMs) have achieved excellent performance in
a variety of evaluation benchmarks, they still struggle in complex reasoning
tasks which require specific knowledge and multi-hop reasoning. To improve the
reasoning abilities, we propose ChatCoT, a tool-augmented chain-of-thought
reasoning framework for chat-based LLMs (e.g., ChatGPT). In ChatCoT, we model
the chain-of-thought (CoT) reasoning as multi-turn conversations, to utilize
tools in a more natural way through chatting. At each turn, LLMs can either
interact with tools or perform the reasoning. Our approach can effectively
leverage the multi-turn conversation ability of chat-based LLMs, and integrate
the thought chain following and tools manipulation in a unified way. Specially,
we initialize the early turns of the conversation by the knowledge about tools,
tasks, and reasoning format, and propose an iterative tool-augmented reasoning
step to perform step-by-step tool-augmented reasoning. The experiment results
on two complex reasoning datasets (MATH and HotpotQA) have shown the
effectiveness of ChatCoT on complex reasoning tasks, achieving a 7.9% relative
improvement over the state-of-the-art baseline. Our code and data are available
at: \url{https://github.com/RUCAIBOX/ChatCoT}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14481">FOCUS: Effective Embedding Initialization for Monolingual Specialization of Multilingual Models. (arXiv:2305.14481v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dobler_K/0/1/0/all/0/1">Konstantin Dobler</a>, <a href="http://arxiv.org/find/cs/1/au:+Melo_G/0/1/0/all/0/1">Gerard de Melo</a></p>
<p>Using model weights pretrained on a high-resource language as a warm start
can reduce the need for data and compute to obtain high-quality language models
for other, especially low-resource, languages. However, if we want to use a new
tokenizer specialized for the target language, we cannot transfer the source
model's embedding matrix. In this paper, we propose FOCUS - Fast Overlapping
Token Combinations Using Sparsemax, a novel embedding initialization method
that initializes the embedding matrix effectively for a new tokenizer based on
information in the source model's embedding matrix. FOCUS represents newly
added tokens as combinations of tokens in the overlap of the source and target
vocabularies. The overlapping tokens are selected based on semantic similarity
in an auxiliary static token embedding space. We focus our study on using the
multilingual XLM-R as a source model and empirically show that FOCUS
outperforms random initialization and previous work in language modeling and on
a range of downstream tasks (NLI, QA, and NER).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14599">Bridging Continuous and Discrete Spaces: Interpretable Sentence Representation Learning via Compositional Operations. (arXiv:2305.14599v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">James Y. Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1">Wenlin Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1">Kaiqiang Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Muhao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1">Dong Yu</a></p>
<p>Traditional sentence embedding models encode sentences into vector
representations to capture useful properties such as the semantic similarity
between sentences. However, in addition to similarity, sentence semantics can
also be interpreted via compositional operations such as sentence fusion or
difference. It is unclear whether the compositional semantics of sentences can
be directly reflected as compositional operations in the embedding space. To
more effectively bridge the continuous embedding and discrete text spaces, we
explore the plausibility of incorporating various compositional properties into
the sentence embedding space that allows us to interpret embedding
transformations as compositional sentence operations. We propose InterSent, an
end-to-end framework for learning interpretable sentence embeddings that
supports compositional sentence operations in the embedding space. Our method
optimizes operator networks and a bottleneck encoder-decoder model to produce
meaningful and interpretable sentence embeddings. Experimental results
demonstrate that our method significantly improves the interpretability of
sentence embeddings on four textual generation tasks over existing approaches
while maintaining strong performance on traditional semantic similarity tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14788">Adapting Language Models to Compress Contexts. (arXiv:2305.14788v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chevalier_A/0/1/0/all/0/1">Alexis Chevalier</a>, <a href="http://arxiv.org/find/cs/1/au:+Wettig_A/0/1/0/all/0/1">Alexander Wettig</a>, <a href="http://arxiv.org/find/cs/1/au:+Ajith_A/0/1/0/all/0/1">Anirudh Ajith</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Danqi Chen</a></p>
<p>Transformer-based language models (LMs) are powerful and widely-applicable
tools, but their usefulness is constrained by a finite context window and the
expensive computational cost of processing long text documents. We propose to
adapt pre-trained LMs into AutoCompressors. These language models are capable
of compressing long contexts into compact summary vectors, which are then
accessible to the model as soft prompts. Summary vectors are trained with an
unsupervised objective, whereby long documents are processed in segments, and
summary vectors from all previous segments are used in language modeling. We
fine-tune OPT and Llama-2 models on sequences of up to 30,720 tokens and show
that AutoCompressors can utilize long contexts to improve perplexity. We
evaluate AutoCompressors on in-context learning by compressing task
demonstrations and find that summary vectors are good substitutes for
plain-text demonstrations, increasing accuracy while reducing inference costs.
Finally, we explore the benefits of pre-computing summary vectors for large
corpora by applying summary vectors to retrievalaugmented language modeling and
a passage re-ranking task. Overall, AutoCompressors emerge as a simple and
inexpensive solution to extend the context window of LMs while speeding up
inference over long contexts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14871">ClusterLLM: Large Language Models as a Guide for Text Clustering. (arXiv:2305.14871v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zihan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1">Jingbo Shang</a></p>
<p>We introduce ClusterLLM, a novel text clustering framework that leverages
feedback from an instruction-tuned large language model, such as ChatGPT.
Compared with traditional unsupervised methods that builds upon "small"
embedders, ClusterLLM exhibits two intriguing advantages: (1) it enjoys the
emergent capability of LLM even if its embeddings are inaccessible; and (2) it
understands the user's preference on clustering through textual instruction
and/or a few annotated data. First, we prompt ChatGPT for insights on
clustering perspective by constructing hard triplet questions &lt;does A better
correspond to B than C&gt;, where A, B and C are similar data points that belong
to different clusters according to small embedder. We empirically show that
this strategy is both effective for fine-tuning small embedder and
cost-efficient to query ChatGPT. Second, we prompt ChatGPT for helps on
clustering granularity by carefully designed pairwise questions &lt;do A and B
belong to the same category&gt;, and tune the granularity from cluster hierarchies
that is the most consistent with the ChatGPT answers. Extensive experiments on
14 datasets show that ClusterLLM consistently improves clustering quality, at
an average cost of ~$0.6 per dataset. The code will be available at
https://github.com/zhang-yu-wei/ClusterLLM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14919">Frugal Prompting for Dialog Models. (arXiv:2305.14919v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Santra_B/0/1/0/all/0/1">Bishal Santra</a>, <a href="http://arxiv.org/find/cs/1/au:+Basak_S/0/1/0/all/0/1">Sakya Basak</a>, <a href="http://arxiv.org/find/cs/1/au:+De_A/0/1/0/all/0/1">Abhinandan De</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1">Manish Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1">Pawan Goyal</a></p>
<p>The use of large language models (LLMs) in natural language processing (NLP)
tasks is rapidly increasing, leading to changes in how researchers approach
problems in the field. To fully utilize these models' abilities, a better
understanding of their behavior for different input protocols is required. With
LLMs, users can directly interact with the models through a text-based
interface to define and solve various tasks. Hence, understanding the
conversational abilities of these LLMs, which may not have been specifically
trained for dialog modeling, is also important. This study examines different
approaches for building dialog systems using LLMs by considering various
aspects of the prompt. As part of prompt tuning, we experiment with various
ways of providing instructions, exemplars, current query and additional
context. The research also analyzes the representations of dialog history that
have the optimal usable-information density. Based on the findings, the paper
suggests more compact ways of providing dialog history information while
ensuring good performance and reducing model's inference-API costs. The
research contributes to a better understanding of how LLMs can be effectively
used for building interactive systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15093">C-STS: Conditional Semantic Textual Similarity. (arXiv:2305.15093v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1">Ameet Deshpande</a>, <a href="http://arxiv.org/find/cs/1/au:+Jimenez_C/0/1/0/all/0/1">Carlos E. Jimenez</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Howard Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Murahari_V/0/1/0/all/0/1">Vishvak Murahari</a>, <a href="http://arxiv.org/find/cs/1/au:+Graf_V/0/1/0/all/0/1">Victoria Graf</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajpurohit_T/0/1/0/all/0/1">Tanmay Rajpurohit</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalyan_A/0/1/0/all/0/1">Ashwin Kalyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Danqi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1">Karthik Narasimhan</a></p>
<p>Semantic textual similarity (STS), a cornerstone task in NLP, measures the
degree of similarity between a pair of sentences, and has broad application in
fields such as information retrieval and natural language understanding.
However, sentence similarity can be inherently ambiguous, depending on the
specific aspect of interest. We resolve this ambiguity by proposing a novel
task called Conditional STS (C-STS) which measures sentences' similarity
conditioned on an feature described in natural language (hereon, condition). As
an example, the similarity between the sentences "The NBA player shoots a
three-pointer." and "A man throws a tennis ball into the air to serve." is
higher for the condition "The motion of the ball" (both upward) and lower for
"The size of the ball" (one large and one small). C-STS's advantages are
two-fold: (1) it reduces the subjectivity and ambiguity of STS and (2) enables
fine-grained language model evaluation through diverse natural language
conditions. We put several state-of-the-art models to the test, and even those
performing well on STS (e.g. SimCSE, Flan-T5, and GPT-4) find C-STS
challenging; all with Spearman correlation scores below 50. To encourage a more
comprehensive evaluation of semantic similarity and natural language
understanding, we make nearly 19K C-STS examples and code available for others
to train and test their models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15269">Testing the General Deductive Reasoning Capacity of Large Language Models Using OOD Examples. (arXiv:2305.15269v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Saparov_A/0/1/0/all/0/1">Abulhair Saparov</a>, <a href="http://arxiv.org/find/cs/1/au:+Pang_R/0/1/0/all/0/1">Richard Yuanzhe Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Padmakumar_V/0/1/0/all/0/1">Vishakh Padmakumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1">Nitish Joshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kazemi_S/0/1/0/all/0/1">Seyed Mehran Kazemi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1">Najoung Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1">He He</a></p>
<p>Given the intractably large size of the space of proofs, any model that is
capable of general deductive reasoning must generalize to proofs of greater
complexity. Recent studies have shown that large language models (LLMs) possess
some abstract deductive reasoning ability given chain-of-thought prompts.
However, they have primarily been tested on proofs using modus ponens or of a
specific size, and from the same distribution as the in-context examples. To
measure the general deductive reasoning ability of LLMs, we test on a broad set
of deduction rules and measure their ability to generalize to more complex
proofs from simpler demonstrations from multiple angles: depth-, width-, and
compositional generalization. To facilitate systematic exploration, we
construct a new synthetic and programmable reasoning dataset that enables
control over deduction rules and proof complexity. Our experiments on four LLMs
of various sizes and training objectives show that they are able to generalize
to compositional proofs. However, they have difficulty generalizing to longer
proofs, and they require explicit demonstrations to produce hypothetical
subproofs, specifically in proof by cases and proof by contradiction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15408">Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective. (arXiv:2305.15408v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_G/0/1/0/all/0/1">Guhao Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Bohang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1">Yuntian Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1">Haotian Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1">Di He</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Liwei Wang</a></p>
<p>Recent studies have discovered that Chain-of-Thought prompting (CoT) can
dramatically improve the performance of Large Language Models (LLMs),
particularly when dealing with complex tasks involving mathematics or
reasoning. Despite the enormous empirical success, the underlying mechanisms
behind CoT and how it unlocks the potential of LLMs remain elusive. In this
paper, we take a first step towards theoretically answering these questions.
Specifically, we examine the expressivity of LLMs with CoT in solving
fundamental mathematical and decision-making problems. By using circuit
complexity theory, we first give impossibility results showing that
bounded-depth Transformers are unable to directly produce correct answers for
basic arithmetic/equation tasks unless the model size grows super-polynomially
with respect to the input length. In contrast, we then prove by construction
that autoregressive Transformers of constant size suffice to solve both tasks
by generating CoT derivations using a commonly used math language format.
Moreover, we show LLMs with CoT can handle a general class of decision-making
problems known as Dynamic Programming, thus justifying its power in tackling
complex real-world tasks. Finally, an extensive set of experiments show that,
while Transformers always fail to directly predict the answers, they can
consistently learn to generate correct solutions step-by-step given sufficient
CoT demonstrations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.02213">Evaluating Emotion Arcs Across Languages: Bridging the Global Divide in Sentiment Analysis. (arXiv:2306.02213v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Teodorescu_D/0/1/0/all/0/1">Daniela Teodorescu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1">Saif M. Mohammad</a></p>
<p>Emotion arcs capture how an individual (or a population) feels over time.
They are widely used in industry and research; however, there is little work on
evaluating the automatically generated arcs. This is because of the difficulty
of establishing the true (gold) emotion arc. Our work, for the first time,
systematically and quantitatively evaluates automatically generated emotion
arcs. We also compare two common ways of generating emotion arcs:
Machine-Learning (ML) models and Lexicon-Only (LexO) methods. By running
experiments on 18 diverse datasets in 9 languages, we show that despite being
markedly poor at instance level emotion classification, LexO methods are highly
accurate at generating emotion arcs when aggregating information from hundreds
of instances. We also show, through experiments on six indigenous African
languages, as well as Arabic, and Spanish, that automatic translations of
English emotion lexicons can be used to generate high-quality emotion arcs in
less-resource languages. This opens up avenues for work on emotions in
languages from around the world; which is crucial for commerce, public policy,
and health research in service of speakers often left behind. Code and
resources: https://github.com/dteodore/EmotionArcs
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.03535">SciLit: A Platform for Joint Scientific Literature Discovery, Summarization and Citation Generation. (arXiv:2306.03535v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gu_N/0/1/0/all/0/1">Nianlong Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hahnloser_R/0/1/0/all/0/1">Richard H.R. Hahnloser</a></p>
<p>Scientific writing involves retrieving, summarizing, and citing relevant
papers, which can be time-consuming processes in large and rapidly evolving
fields. By making these processes inter-operable, natural language processing
(NLP) provides opportunities for creating end-to-end assistive writing tools.
We propose SciLit, a pipeline that automatically recommends relevant papers,
extracts highlights, and suggests a reference sentence as a citation of a
paper, taking into consideration the user-provided context and keywords. SciLit
efficiently recommends papers from large databases of hundreds of millions of
papers using a two-stage pre-fetching and re-ranking literature search system
that flexibly deals with addition and removal of a paper database. We provide a
convenient user interface that displays the recommended papers as extractive
summaries and that offers abstractively-generated citing sentences which are
aligned with the provided context and which mention the chosen keyword(s). Our
assistive tool for literature discovery and scientific writing is available at
https://scilit.vercel.app
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.03774">Exploring Hybrid Linguistic Features for Turkish Text Readability. (arXiv:2306.03774v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Uluslu_A/0/1/0/all/0/1">Ahmet Yavuz Uluslu</a>, <a href="http://arxiv.org/find/cs/1/au:+Schneider_G/0/1/0/all/0/1">Gerold Schneider</a></p>
<p>This paper presents the first comprehensive study on automatic readability
assessment of Turkish texts. We combine state-of-the-art neural network models
with linguistic features at lexical, morphosyntactic, syntactic and discourse
levels to develop an advanced readability tool. We evaluate the effectiveness
of traditional readability formulas compared to modern automated methods and
identify key linguistic features that determine the readability of Turkish
texts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.04181">Benchmarking Foundation Models with Language-Model-as-an-Examiner. (arXiv:2306.04181v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1">Yushi Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Ying_J/0/1/0/all/0/1">Jiahao Ying</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yixin Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_X/0/1/0/all/0/1">Xin Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yuze He</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaozhi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Jifan Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_K/0/1/0/all/0/1">Kaisheng Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1">Yijia Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_H/0/1/0/all/0/1">Haozhe Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiayin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Juanzi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1">Lei Hou</a></p>
<p>Numerous benchmarks have been established to assess the performance of
foundation models on open-ended question answering, which serves as a
comprehensive test of a model's ability to understand and generate language in
a manner similar to humans. Most of these works focus on proposing new
datasets, however, we see two main issues within previous benchmarking
pipelines, namely testing leakage and evaluation automation. In this paper, we
propose a novel benchmarking framework, Language-Model-as-an-Examiner, where
the LM serves as a knowledgeable examiner that formulates questions based on
its knowledge and evaluates responses in a reference-free manner. Our framework
allows for effortless extensibility as various LMs can be adopted as the
examiner, and the questions can be constantly updated given more diverse
trigger topics. For a more comprehensive and equitable evaluation, we devise
three strategies: (1) We instruct the LM examiner to generate questions across
a multitude of domains to probe for a broad acquisition, and raise follow-up
questions to engage in a more in-depth assessment. (2) Upon evaluation, the
examiner combines both scoring and ranking measurements, providing a reliable
result as it aligns closely with human annotations. (3) We additionally propose
a decentralized Peer-examination method to address the biases in a single
examiner. Our data and benchmarking results are available at:
<a href="http://lmexam.xlore.cn.">this http URL</a>
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.09869">Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models. (arXiv:2306.09869v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Park_G/0/1/0/all/0/1">Geon Yeong Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jeongsol Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1">Beomsu Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Sang Wan Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1">Jong Chul Ye</a></p>
<p>Despite the remarkable performance of text-to-image diffusion models in image
generation tasks, recent studies have raised the issue that generated images
sometimes cannot capture the intended semantic contents of the text prompts,
which phenomenon is often called semantic misalignment. To address this, here
we present a novel energy-based model (EBM) framework for adaptive context
control by modeling the posterior of context vectors. Specifically, we first
formulate EBMs of latent image representations and text embeddings in each
cross-attention layer of the denoising autoencoder. Then, we obtain the
gradient of the log posterior of context vectors, which can be updated and
transferred to the subsequent cross-attention layer, thereby implicitly
minimizing a nested hierarchy of energy functions. Our latent EBMs further
allow zero-shot compositional generation as a linear combination of
cross-attention outputs from different contexts. Using extensive experiments,
we demonstrate that the proposed method is highly effective in handling various
image generation tasks, including multi-concept generation, text-guided image
inpainting, and real and synthetic image editing. Code:
https://github.com/EnergyAttention/Energy-Based-CrossAttention.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.11197">Sparse Modular Activation for Efficient Sequence Modeling. (arXiv:2306.11197v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1">Liliang Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuohang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yichong Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1">Chenguang Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1">ChengXiang Zhai</a></p>
<p>Recent hybrid models combining Linear State Space Models (SSMs) with
self-attention mechanisms have demonstrated impressive results across a range
of sequence modeling tasks. However, current approaches apply attention modules
statically and uniformly to all elements in the input sequences, leading to
sub-optimal quality-efficiency trade-offs. To address this limitation, we
introduce Sparse Modular Activation (SMA), a general mechanism enabling neural
networks to sparsely and dynamically activate sub-modules for sequence elements
in a differentiable manner. Through allowing each element to skip non-activated
sub-modules, SMA reduces computation and memory consumption of neural networks
at both training and inference stages. To validate the effectiveness of SMA on
sequence modeling, we design a novel neural architecture, SeqBoat, which
employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the
state representations learned from an SSM. By constraining the GAU to only
conduct local attention on the activated inputs, SeqBoat can achieve linear
inference complexity with theoretically infinite attention span, and provide
substantially better quality-efficiency trade-off than the chunking-based
models. With experiments on a wide range of tasks, including long sequence
modeling, speech classification and language modeling, SeqBoat brings new
state-of-the-art results among hybrid models with linear complexity, and
reveals the amount of attention needed for each task through the learned sparse
activation patterns. Our code is publicly available at
https://github.com/renll/SeqBoat.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06775">A Novel Site-Agnostic Multimodal Deep Learning Model to Identify Pro-Eating Disorder Content on Social Media. (arXiv:2307.06775v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feldman_J/0/1/0/all/0/1">Jonathan Feldman</a></p>
<p>Over the last decade, there has been a vast increase in eating disorder
diagnoses and eating disorder-attributed deaths, reaching their zenith during
the Covid-19 pandemic. This immense growth derived in part from the stressors
of the pandemic but also from increased exposure to social media, which is rife
with content that promotes eating disorders. This study aimed to create a
multimodal deep learning model that can determine if a given social media post
promotes eating disorders based on a combination of visual and textual data. A
labeled dataset of Tweets was collected from Twitter, recently rebranded as X,
upon which twelve deep learning models were trained and evaluated. Based on
model performance, the most effective deep learning model was the multimodal
fusion of the RoBERTa natural language processing model and the MaxViT image
classification model, attaining accuracy and F1 scores of 95.9% and 0.959,
respectively. The RoBERTa and MaxViT fusion model, deployed to classify an
unlabeled dataset of posts from the social media sites Tumblr and Reddit,
generated results akin to those of previous research studies that did not
employ artificial intelligence-based techniques, indicating that deep learning
models can develop insights congruent to those of researchers. Additionally,
the model was used to conduct a time-series analysis of yet unseen Tweets from
eight Twitter hashtags, uncovering that, since 2014, the relative abundance of
content that promotes eating disorders has decreased drastically within those
communities. Despite this reduction, by 2018, content that promotes eating
disorders had either stopped declining or increased in ampleness anew on those
hashtags.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08701">AlpaGasus: Training A Better Alpaca with Fewer Data. (arXiv:2307.08701v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lichang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shiyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1">Jun Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gunaratna_K/0/1/0/all/0/1">Kalpa Gunaratna</a>, <a href="http://arxiv.org/find/cs/1/au:+Yadav_V/0/1/0/all/0/1">Vikas Yadav</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1">Zheng Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Srinivasan_V/0/1/0/all/0/1">Vijay Srinivasan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1">Tianyi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Heng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1">Hongxia Jin</a></p>
<p>Large language models~(LLMs) strengthen instruction-following capability
through instruction-finetuning (IFT) on supervised instruction/response data.
However, widely used IFT datasets (e.g., Alpaca's 52k data) surprisingly
contain many low-quality instances with incorrect or irrelevant responses,
which are misleading and detrimental to IFT. In this paper, we propose a simple
and effective data selection strategy that automatically identifies and filters
out low-quality data using a strong LLM (e.g., ChatGPT). To this end, we
introduce AlpaGasus, which is finetuned on only 9k high-quality data filtered
from the 52k Alpaca data. AlpaGasus significantly outperforms the original
Alpaca as evaluated by GPT-4 on multiple test sets and the controlled human
evaluation. Its 13B variant matches $&gt;90\%$ performance of its teacher LLM
(i.e., Text-Davinci-003 generating the 52k data) on test tasks. It also
provides 5.7x faster training, reducing the training time for a 7B variant from
80 minutes (for Alpaca) to 14 minutes. Moreover, the experiments prove the
efficacy of our method across diverse datasets, base models, and LLM filters.
Overall, AlpaGasus demonstrates a novel data-centric IFT paradigm that can be
generally applied to instruction-tuning data, leading to faster training and
better instruction-following models. Our project page is available at:
\url{https://lichang-chen.github.io/AlpaGasus/}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.11760">Large Language Models Understand and Can be Enhanced by Emotional Stimuli. (arXiv:2307.11760v6 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Cheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jindong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yixuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1">Kaijie Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_W/0/1/0/all/0/1">Wenxin Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Lian_J/0/1/0/all/0/1">Jianxun Lian</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1">Fang Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1">Qiang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xing Xie</a></p>
<p>Emotional intelligence significantly impacts our daily behaviors and
interactions. Although Large Language Models (LLMs) are increasingly viewed as
a stride toward artificial general intelligence, exhibiting impressive
performance in numerous tasks, it is still uncertain if LLMs can genuinely
grasp psychological emotional stimuli. Understanding and responding to
emotional cues gives humans a distinct advantage in problem-solving. In this
paper, we take the first step towards exploring the ability of LLMs to
understand emotional stimuli. To this end, we first conduct automatic
experiments on 45 tasks using various LLMs, including Flan-T5-Large, Vicuna,
Llama 2, BLOOM, ChatGPT, and GPT-4. Our tasks span deterministic and generative
applications that represent comprehensive evaluation scenarios. Our automatic
experiments show that LLMs have a grasp of emotional intelligence, and their
performance can be improved with emotional prompts (which we call
"EmotionPrompt" that combines the original prompt with emotional stimuli),
e.g., 8.00% relative performance improvement in Instruction Induction and 115%
in BIG-Bench. In addition to those deterministic tasks that can be
automatically evaluated using existing metrics, we conducted a human study with
106 participants to assess the quality of generative tasks using both vanilla
and emotional prompts. Our human study results demonstrate that EmotionPrompt
significantly boosts the performance of generative tasks (10.9% average
improvement in terms of performance, truthfulness, and responsibility metrics).
We provide an in-depth discussion regarding why EmotionPrompt works for LLMs
and the factors that may influence its performance. We posit that EmotionPrompt
heralds a novel avenue for exploring interdisciplinary knowledge for human-LLMs
interaction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14850">Turkish Native Language Identification. (arXiv:2307.14850v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Uluslu_A/0/1/0/all/0/1">Ahmet Yavuz Uluslu</a>, <a href="http://arxiv.org/find/cs/1/au:+Schneider_G/0/1/0/all/0/1">Gerold Schneider</a></p>
<p>In this paper, we present the first application of Native Language
Identification (NLI) for the Turkish language. NLI involves predicting the
writer's first language by analysing their writing in different languages.
While most NLI research has focused on English, our study extends its scope to
Turkish. We used the recently constructed Turkish Learner Corpus and employed a
combination of three syntactic features (CFG production rules, part-of-speech
n-grams, and function words) with L2 texts to demonstrate their effectiveness
in this task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.15936">A Theory for Emergence of Complex Skills in Language Models. (arXiv:2307.15936v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1">Sanjeev Arora</a>, <a href="http://arxiv.org/find/cs/1/au:+Goyal_A/0/1/0/all/0/1">Anirudh Goyal</a></p>
<p>A major driver of AI products today is the fact that new skills emerge in
language models when their parameter set and training corpora are scaled up.
This phenomenon is poorly understood, and a mechanistic explanation via
mathematical analysis of gradient-based training seems difficult. The current
paper takes a different approach, analysing emergence using the famous (and
empirical) Scaling Laws of LLMs and a simple statistical framework.
Contributions include: (a) A statistical framework that relates cross-entropy
loss of LLMs to competence on the basic skills that underlie language tasks.
(b) Mathematical analysis showing that the Scaling Laws imply a strong form of
inductive bias that allows the pre-trained model to learn very efficiently. We
informally call this {\em slingshot generalization} since naively viewed it
appears to give competence levels at skills that violate usual generalization
theory. (c) A key example of slingshot generalization, that competence at
executing tasks involving $k$-tuples of skills emerges essentially at the same
scaling and same rate as competence on the elementary skills themselves.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.00158">Predictive Data Analytics with AI: assessing the need for post-editing of MT output by fine-tuning OpenAI LLMs. (arXiv:2308.00158v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gladkoff_S/0/1/0/all/0/1">Serge Gladkoff</a>, <a href="http://arxiv.org/find/cs/1/au:+Erofeev_G/0/1/0/all/0/1">Gleb Erofeev</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1">Lifeng Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Nenadic_G/0/1/0/all/0/1">Goran Nenadic</a></p>
<p>Translation Quality Evaluation (TQE) is an essential step of the modern
translation production process. TQE is critical in assessing both machine
translation (MT) and human translation (HT) quality without reference
translations. The ability to evaluate or even simply estimate the quality of
translation automatically may open significant efficiency gains through process
optimisation. This work examines whether the state-of-the-art large language
models (LLMs) can be used for this purpose. We take OpenAI models as the best
state-of-the-art technology and approach TQE as a binary classification task.
On \textbf{eight language pairs} including English to Italian, German, French,
Japanese, Dutch, Portuguese, Turkish, and Chinese, our experimental results
show that fine-tuned \textbf{\textit{gpt3.5}} can demonstrate good performance
on translation quality prediction tasks, i.e. \textit{whether the translation
needs to be edited}. Another finding is that simply increasing the sizes of
LLMs does not lead to apparent better performances on this task by comparing
the performance of three different versions of OpenAI models:
\textbf{\textit{curie}}, \textbf{\textit{davinci}}, and
\textbf{\textit{gpt3.5}} with 13B, 175B, and 175B parameters, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.06032">Large Language Models in Cryptocurrency Securities Cases: Can ChatGPT Replace Lawyers?. (arXiv:2308.06032v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Trozze_A/0/1/0/all/0/1">Arianna Trozze</a>, <a href="http://arxiv.org/find/cs/1/au:+Davies_T/0/1/0/all/0/1">Toby Davies</a>, <a href="http://arxiv.org/find/cs/1/au:+Kleinberg_B/0/1/0/all/0/1">Bennett Kleinberg</a></p>
<p>Large Language Models (LLMs) could enhance access to the legal system.
However, empirical research on their effectiveness in conducting legal tasks is
scant. We study securities cases involving cryptocurrencies as one of numerous
contexts where AI could support the legal process, studying LLMs' legal
reasoning and drafting capabilities. We examine whether a) an LLM can
accurately determine which laws are potentially being violated from a fact
pattern, and b) whether there is a difference in juror decision-making based on
complaints written by a lawyer compared to an LLM. We feed fact patterns from
real-life cases to GPT-3.5 and evaluate its ability to determine correct
potential violations from the scenario and exclude spurious violations. Second,
we had mock jurors assess complaints written by the LLM and lawyers. GPT-3.5's
legal reasoning skills proved weak, though we expect improvement in future
models, particularly given the violations it suggested tended to be correct (it
merely missed additional, correct violations). GPT-3.5 performed better at
legal drafting, and jurors' decisions were not statistically significantly
associated with the author of the document upon which they based their
decisions. Because LLMs cannot satisfactorily conduct legal reasoning tasks,
they would be unable to replace lawyers at this stage. However, their drafting
skills (though, perhaps, still inferior to lawyers), could provide access to
justice for more individuals by reducing the cost of legal services. Our
research is the first to systematically study LLMs' legal drafting and
reasoning capabilities in litigation, as well as in securities law and
cryptocurrency-related misconduct.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12877">DS4DH at #SMM4H 2023: Zero-Shot Adverse Drug Events Normalization using Sentence Transformers and Reciprocal-Rank Fusion. (arXiv:2308.12877v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yazdani_A/0/1/0/all/0/1">Anthony Yazdani</a>, <a href="http://arxiv.org/find/cs/1/au:+Rouhizadeh_H/0/1/0/all/0/1">Hossein Rouhizadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Alvarez_D/0/1/0/all/0/1">David Vicente Alvarez</a>, <a href="http://arxiv.org/find/cs/1/au:+Teodoro_D/0/1/0/all/0/1">Douglas Teodoro</a></p>
<p>This paper outlines the performance evaluation of a system for adverse drug
event normalization, developed by the Data Science for Digital Health (DS4DH)
group for the Social Media Mining for Health Applications (SMM4H) 2023 shared
task 5. Shared task 5 targeted the normalization of adverse drug event mentions
in Twitter to standard concepts of the Medical Dictionary for Regulatory
Activities terminology. Our system hinges on a two-stage approach: BERT
fine-tuning for entity recognition, followed by zero-shot normalization using
sentence transformers and reciprocal-rank fusion. The approach yielded a
precision of 44.9%, recall of 40.5%, and an F1-score of 42.6%. It outperformed
the median performance in shared task 5 by 10% and demonstrated the highest
performance among all participants. These results substantiate the
effectiveness of our approach and its potential application for adverse drug
event normalization in the realm of social media text mining.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.14132">Detecting Language Model Attacks with Perplexity. (arXiv:2308.14132v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alon_G/0/1/0/all/0/1">Gabriel Alon</a>, <a href="http://arxiv.org/find/cs/1/au:+Kamfonas_M/0/1/0/all/0/1">Michael Kamfonas</a></p>
<p>A novel hack involving Large Language Models (LLMs) has emerged, leveraging
adversarial suffixes to trick models into generating perilous responses. This
method has garnered considerable attention from reputable media outlets such as
the New York Times and Wired, thereby influencing public perception regarding
the security and safety of LLMs. In this study, we advocate the utilization of
perplexity as one of the means to recognize such potential attacks. The
underlying concept behind these hacks revolves around appending an unusually
constructed string of text to a harmful query that would otherwise be blocked.
This maneuver confuses the protective mechanisms and tricks the model into
generating a forbidden response. Such scenarios could result in providing
detailed instructions to a malicious user for constructing explosives or
orchestrating a bank heist. Our investigation demonstrates the feasibility of
employing perplexity, a prevalent natural language processing metric, to detect
these adversarial tactics before generating a forbidden response. By evaluating
the perplexity of queries with and without such adversarial suffixes using an
open-source LLM, we discovered that nearly 90 percent were above a perplexity
of 1000. This contrast underscores the efficacy of perplexity for detecting
this type of exploit.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.15727">Quantifying and Analyzing Entity-level Memorization in Large Language Models. (arXiv:2308.15727v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhenhong Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_J/0/1/0/all/0/1">Jiuyang Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chaomeng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1">Sen Su</a></p>
<p>Large language models (LLMs) have been proven capable of memorizing their
training data, which can be extracted through specifically designed prompts. As
the scale of datasets continues to grow, privacy risks arising from
memorization have attracted increasing attention. Quantifying language model
memorization helps evaluate potential privacy risks. However, prior works on
quantifying memorization require access to the precise original data or incur
substantial computational overhead, making it difficult for applications in
real-world language models. To this end, we propose a fine-grained,
entity-level definition to quantify memorization with conditions and metrics
closer to real-world scenarios. In addition, we also present an approach for
efficiently extracting sensitive entities from autoregressive language models.
We conduct extensive experiments based on the proposed, probing language
models' ability to reconstruct sensitive entities under different settings. We
find that language models have strong memorization at the entity level and are
able to reproduce the training data even with partial leakages. The results
demonstrate that LLMs not only memorize their training data but also understand
associations between entities. These findings necessitate that trainers of LLMs
exercise greater prudence regarding model memorization, adopting memorization
mitigation techniques to preclude privacy violations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.00312">Comparative Topic Modeling for Determinants of Divergent Report Results Applied to Macular Degeneration Studies. (arXiv:2309.00312v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jacaruso_L/0/1/0/all/0/1">Lucas Cassiel Jacaruso</a></p>
<p>Topic modeling and text mining are subsets of Natural Language Processing
(NLP) with relevance for conducting meta-analysis (MA) and systematic review
(SR). For evidence synthesis, the above NLP methods are conventionally used for
topic-specific literature searches or extracting values from reports to
automate essential phases of SR and MA. Instead, this work proposes a
comparative topic modeling approach to analyze reports of contradictory results
on the same general research question. Specifically, the objective is to
identify topics exhibiting distinct associations with significant results for
an outcome of interest by ranking them according to their proportional
occurrence in (and consistency of distribution across) reports of significant
effects. The proposed method was tested on broad-scope studies addressing
whether supplemental nutritional compounds significantly benefit macular
degeneration (MD). Six compounds were identified as having a particular
association with reports of significant results for benefiting MD. Four of
these were further supported in terms of effectiveness upon conducting a
follow-up literature search for validation (omega-3 fatty acids, copper,
zeaxanthin, and nitrates). The two not supported by the follow-up literature
search (niacin and molybdenum) also had the lowest scores under the proposed
scoring system, suggesting that the proposed method's score for a given topic
is a viable proxy for its degree of association with the outcome of interest
and is helpful in the search for potentially causal relationships. These
results underpin the proposed methods potential to add specificity in
understanding effects from broad-scope reports, elucidate topics of interest
for future research, and guide evidence synthesis in a systematic and scalable
way.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.03412">From Base to Conversational: Japanese Instruction Dataset and Tuning Large Language Models. (arXiv:2309.03412v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Suzuki_M/0/1/0/all/0/1">Masahiro Suzuki</a>, <a href="http://arxiv.org/find/cs/1/au:+Hirano_M/0/1/0/all/0/1">Masanori Hirano</a>, <a href="http://arxiv.org/find/cs/1/au:+Sakaji_H/0/1/0/all/0/1">Hiroki Sakaji</a></p>
<p>Instruction tuning is essential for large language models (LLMs) to become
interactive. While many instruction tuning datasets exist in English, there is
a noticeable lack in other languages. Also, their effectiveness has not been
well verified in non-English languages. We construct a Japanese instruction
dataset by expanding and filtering existing datasets and apply the dataset to a
Japanese pre-trained base model. We performed Low-Rank Adaptation (LoRA) tuning
on both Japanese and English existing models using our instruction dataset. We
evaluated these models from both quantitative and qualitative perspectives. As
a result, the effectiveness of Japanese instruction datasets is confirmed. The
results also indicate that even with relatively small LLMs, performances in
downstream tasks would be improved through instruction tuning. Our instruction
dataset, tuned models, and implementation are publicly available online.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.05454">Flesch or Fumble? Evaluating Readability Standard Alignment of Instruction-Tuned Language Models. (arXiv:2309.05454v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Imperial_J/0/1/0/all/0/1">Joseph Marvin Imperial</a>, <a href="http://arxiv.org/find/cs/1/au:+Madabushi_H/0/1/0/all/0/1">Harish Tayyar Madabushi</a></p>
<p>Readability metrics and standards such as Flesch Kincaid Grade Level (FKGL)
and the Common European Framework of Reference for Languages (CEFR) exist to
guide teachers and educators to properly assess the complexity of educational
materials before administering them for classroom use. In this study, we select
a diverse set of open and closed-source instruction-tuned language models and
investigate their performances in writing story completions and simplifying
narratives--tasks that teachers perform--using standard-guided prompts
controlling text readability. Our extensive findings provide empirical proof of
how globally recognized models like ChatGPT may be considered less effective
and may require more refined prompts for these generative tasks compared to
other open-sourced models such as BLOOMZ and FlanT5--which have shown promising
results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00836">Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models. (arXiv:2310.00836v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1">Man Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumbhar_S/0/1/0/all/0/1">Shrinidhi Kumbhar</a>, <a href="http://arxiv.org/find/cs/1/au:+shen_M/0/1/0/all/0/1">Ming shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Parmar_M/0/1/0/all/0/1">Mihir Parmar</a>, <a href="http://arxiv.org/find/cs/1/au:+Varshney_N/0/1/0/all/0/1">Neeraj Varshney</a>, <a href="http://arxiv.org/find/cs/1/au:+Banerjee_P/0/1/0/all/0/1">Pratyay Banerjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Aditya_S/0/1/0/all/0/1">Somak Aditya</a>, <a href="http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1">Chitta Baral</a></p>
<p>Logical reasoning is fundamental for humans yet presents a substantial
challenge in the domain of Artificial Intelligence. Initially, researchers used
Knowledge Representation and Reasoning (KR) systems that did not scale and
required non trivial manual effort. Recently, the emergence of large language
models (LLMs) has demonstrated the ability to overcome various limitations of
formal Knowledge Representation (KR) systems. Consequently, there is a growing
interest in using LLMs for logical reasoning via natural language. This work
strives to understand the proficiency of LLMs in logical reasoning by offering
a brief review of the latest progress in this area; with a focus on the logical
reasoning datasets, tasks, and the methods adopted to utilize LLMs for
reasoning. To offer a thorough analysis, we have compiled a benchmark titled
LogiGLUE. This includes 24 varied datasets encompassing deductive, abductive,
and inductive reasoning. We have standardized these datasets into Seq2Seq tasks
to facilitate straightforward training and evaluation for future research.
Utilizing LogiGLUE as a foundation, we have trained an instruction fine tuned
language model, resulting in LogiT5. We study single task training, multi task
training, and a chain of thought knowledge distillation fine tuning technique
to assess the performance of model across the different logical reasoning
categories. By this comprehensive process, we aim to shed light on the
capabilities and potential pathways for enhancing logical reasoning proficiency
in LLMs, paving the way for more advanced and nuanced developments in this
critical field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01352">RA-DIT: Retrieval-Augmented Dual Instruction Tuning. (arXiv:2310.01352v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1">Xi Victoria Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xilun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Mingda Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1">Weijia Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lomeli_M/0/1/0/all/0/1">Maria Lomeli</a>, <a href="http://arxiv.org/find/cs/1/au:+James_R/0/1/0/all/0/1">Rich James</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_P/0/1/0/all/0/1">Pedro Rodriguez</a>, <a href="http://arxiv.org/find/cs/1/au:+Kahn_J/0/1/0/all/0/1">Jacob Kahn</a>, <a href="http://arxiv.org/find/cs/1/au:+Szilvasy_G/0/1/0/all/0/1">Gergely Szilvasy</a>, <a href="http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1">Mike Lewis</a>, <a href="http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1">Luke Zettlemoyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Yih_S/0/1/0/all/0/1">Scott Yih</a></p>
<p>Retrieval-augmented language models (RALMs) improve performance by accessing
long-tail and up-to-date knowledge from external data stores, but are
challenging to build. Existing approaches require either expensive
retrieval-specific modifications to LM pre-training or use post-hoc integration
of the data store that leads to suboptimal performance. We introduce
Retrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning
methodology that provides a third option by retrofitting any LLM with retrieval
capabilities. Our approach operates in two distinct fine-tuning steps: (1) one
updates a pre-trained LM to better use retrieved information, while (2) the
other updates the retriever to return more relevant results, as preferred by
the LM. By fine-tuning over tasks that require both knowledge utilization and
contextual awareness, we demonstrate that each stage yields significant
performance improvements, and using both leads to additional gains. Our best
model, RA-DIT 65B, achieves state-of-the-art performance across a range of
knowledge-intensive zero- and few-shot learning benchmarks, significantly
outperforming existing in-context RALM approaches by up to +8.9% in 0-shot
setting and +1.4% in 5-shot setting on average.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01381">DiffAR: Denoising Diffusion Autoregressive Model for Raw Speech Waveform Generation. (arXiv:2310.01381v2 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Benita_R/0/1/0/all/0/1">Roi Benita</a>, <a href="http://arxiv.org/find/cs/1/au:+Elad_M/0/1/0/all/0/1">Michael Elad</a>, <a href="http://arxiv.org/find/cs/1/au:+Keshet_J/0/1/0/all/0/1">Joseph Keshet</a></p>
<p>Diffusion models have recently been shown to be relevant for high-quality
speech generation. Most work has been focused on generating spectrograms, and
as such, they further require a subsequent model to convert the spectrogram to
a waveform (i.e., a vocoder). This work proposes a diffusion probabilistic
end-to-end model for generating a raw speech waveform. The proposed model is
autoregressive, generating overlapping frames sequentially, where each frame is
conditioned on a portion of the previously generated one. Hence, our model can
effectively synthesize an unlimited speech duration while preserving
high-fidelity synthesis and temporal coherence. We implemented the proposed
model for unconditional and conditional speech generation, where the latter can
be driven by an input sequence of phonemes, amplitudes, and pitch values.
Working on the waveform directly has some empirical advantages. Specifically,
it allows the creation of local acoustic behaviors, like vocal fry, which makes
the overall waveform sounds more natural. Furthermore, the proposed diffusion
model is stochastic and not deterministic; therefore, each inference generates
a slightly different waveform variation, enabling abundance of valid
realizations. Experiments show that the proposed model generates speech with
superior quality compared with other state-of-the-art neural speech generation
systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04027">Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models. (arXiv:2310.04027v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Boyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Hongyang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1">Tianyu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Babar_A/0/1/0/all/0/1">Ali Babar</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiao-Yang Liu</a></p>
<p>Financial sentiment analysis is critical for valuation and investment
decision-making. Traditional NLP models, however, are limited by their
parameter size and the scope of their training datasets, which hampers their
generalization capabilities and effectiveness in this field. Recently, Large
Language Models (LLMs) pre-trained on extensive corpora have demonstrated
superior performance across various NLP tasks due to their commendable
zero-shot abilities. Yet, directly applying LLMs to financial sentiment
analysis presents challenges: The discrepancy between the pre-training
objective of LLMs and predicting the sentiment label can compromise their
predictive performance. Furthermore, the succinct nature of financial news,
often devoid of sufficient context, can significantly diminish the reliability
of LLMs' sentiment analysis. To address these challenges, we introduce a
retrieval-augmented LLMs framework for financial sentiment analysis. This
framework includes an instruction-tuned LLMs module, which ensures LLMs behave
as predictors of sentiment labels, and a retrieval-augmentation module which
retrieves additional context from reliable external sources. Benchmarked
against traditional models and LLMs like ChatGPT and LLaMA, our approach
achieves 15\% to 48\% performance gain in accuracy and F1 score.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05199">Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning from Human Feedback. (arXiv:2310.05199v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1">Wei Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1">Rui Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhan_W/0/1/0/all/0/1">Wenyu Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jun Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Dou_S/0/1/0/all/0/1">Shihan Dou</a>, <a href="http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1">Tao Gui</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xuanjing Huang</a></p>
<p>Reinforcement learning from human feedback serves as a crucial bridge,
aligning large language models with human and societal values. This alignment
requires a vast corpus of human feedback to learn a reward model, which is
subsequently used to finetune language models. However, we have identified that
the reward model often finds shortcuts to bypass its intended objectives,
misleadingly assuming that humans prefer longer responses. The emergence of
length bias often induces the model to favor longer outputs, yet it doesn't
equate to an increase in helpful information within these outputs. In this
paper, we propose an innovative solution, applying the Product-of-Experts (PoE)
technique to separate reward modeling from the influence of sequence length. In
our framework, the main expert concentrates on understanding human intents,
while the biased expert targets the identification and capture of length bias.
To further enhance the learning of bias, we introduce perturbations into the
bias-focused expert, disrupting the flow of semantic information. Experimental
results validate the effectiveness of our approach, indicating that language
model performance is improved, irrespective of sequence length.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.10118">Learning to Rank Context for Named Entity Recognition Using a Synthetic Dataset. (arXiv:2310.10118v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Amalvy_A/0/1/0/all/0/1">Arthur Amalvy</a> (LIA), <a href="http://arxiv.org/find/cs/1/au:+Labatut_V/0/1/0/all/0/1">Vincent Labatut</a> (LIA), <a href="http://arxiv.org/find/cs/1/au:+Dufour_R/0/1/0/all/0/1">Richard Dufour</a> (LS2N - &#xe9;quipe TALN )</p>
<p>While recent pre-trained transformer-based models can perform named entity
recognition (NER) with great accuracy, their limited range remains an issue
when applied to long documents such as whole novels. To alleviate this issue, a
solution is to retrieve relevant context at the document level. Unfortunately,
the lack of supervision for such a task means one has to settle for
unsupervised approaches. Instead, we propose to generate a synthetic context
retrieval training dataset using Alpaca, an instructiontuned large language
model (LLM). Using this dataset, we train a neural context retriever based on a
BERT model that is able to find relevant context for NER. We show that our
method outperforms several retrieval baselines for the NER task on an English
literary dataset composed of the first chapter of 40 books.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.10191">VIBE: Topic-Driven Temporal Adaptation for Twitter Classification. (arXiv:2310.10191v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuji Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jing Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wenjie Li</a></p>
<p>Language features are evolving in real-world social media, resulting in the
deteriorating performance of text classification in dynamics. To address this
challenge, we study temporal adaptation, where models trained on past data are
tested in the future. Most prior work focused on continued pretraining or
knowledge updating, which may compromise their performance on noisy social
media data. To tackle this issue, we reflect feature change via modeling latent
topic evolution and propose a novel model, VIBE: Variational Information
Bottleneck for Evolutions. Concretely, we first employ two Information
Bottleneck (IB) regularizers to distinguish past and future topics. Then, the
distinguished topics work as adaptive features via multi-task training with
timestamp and class label prediction. In adaptive learning, VIBE utilizes
retrieved unlabeled data from online streams created posterior to training data
time. Substantial Twitter experiments on three classification tasks show that
our model, with only 3% of data, significantly outperforms previous
state-of-the-art continued-pretraining methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.11441">Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V. (arXiv:2310.11441v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jianwei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1">Feng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1">Xueyan Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chunyuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jianfeng Gao</a></p>
<p>We present Set-of-Mark (SoM), a new visual prompting method, to unleash the
visual grounding abilities of large multimodal models (LMMs), such as GPT-4V.
As illustrated in Fig. 1 (right), we employ off-the-shelf interactive
segmentation models, such as SEEM/SAM, to partition an image into regions at
different levels of granularity, and overlay these regions with a set of marks
e.g., alphanumerics, masks, boxes. Using the marked image as input, GPT-4V can
answer the questions that require visual grounding. We perform a comprehensive
empirical study to validate the effectiveness of SoM on a wide range of
fine-grained vision and multimodal tasks. For example, our experiments show
that GPT-4V with SoM in zero-shot setting outperforms the state-of-the-art
fully-finetuned referring expression comprehension and segmentation model on
RefCOCOg. Code for SoM prompting is made public at:
https://github.com/microsoft/SoM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.13505">Robust Training for Conversational Question Answering Models with Reinforced Reformulation Generation. (arXiv:2310.13505v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kaiser_M/0/1/0/all/0/1">Magdalena Kaiser</a>, <a href="http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1">Rishiraj Saha Roy</a>, <a href="http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1">Gerhard Weikum</a></p>
<p>Models for conversational question answering (ConvQA) over knowledge graphs
(KGs) are usually trained and tested on benchmarks of gold QA pairs. This
implies that training is limited to surface forms seen in the respective
datasets, and evaluation is on a small set of held-out questions. Through our
proposed framework REIGN, we take several steps to remedy this restricted
learning setup. First, we systematically generate reformulations of training
questions to increase robustness of models to surface form variations. This is
a particularly challenging problem, given the incomplete nature of such
questions. Second, we guide ConvQA models towards higher performance by feeding
it only those reformulations that help improve their answering quality, using
deep reinforcement learning. Third, we demonstrate the viability of training
major model components on one benchmark and applying them zero-shot to another.
Finally, for a rigorous evaluation of robustness for trained models, we use and
release large numbers of diverse reformulations generated by prompting GPT for
benchmark test sets (resulting in 20x increase in sizes). Our findings show
that ConvQA models with robust training via reformulations, significantly
outperform those with standard training from gold QA pairs only.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.13682">Optimizing Retrieval-augmented Reader Models via Token Elimination. (arXiv:2310.13682v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Berchansky_M/0/1/0/all/0/1">Moshe Berchansky</a>, <a href="http://arxiv.org/find/cs/1/au:+Izsak_P/0/1/0/all/0/1">Peter Izsak</a>, <a href="http://arxiv.org/find/cs/1/au:+Caciularu_A/0/1/0/all/0/1">Avi Caciularu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dagan_I/0/1/0/all/0/1">Ido Dagan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wasserblat_M/0/1/0/all/0/1">Moshe Wasserblat</a></p>
<p>Fusion-in-Decoder (FiD) is an effective retrieval-augmented language model
applied across a variety of open-domain tasks, such as question answering, fact
checking, etc. In FiD, supporting passages are first retrieved and then
processed using a generative model (Reader), which can cause a significant
bottleneck in decoding time, particularly with long outputs. In this work, we
analyze the contribution and necessity of all the retrieved passages to the
performance of reader models, and propose eliminating some of the retrieved
information, at the token level, that might not contribute essential
information to the answer generation process. We demonstrate that our method
can reduce run-time by up to 62.2%, with only a 2% reduction in performance,
and in some cases, even improve the performance results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.14265">CT-GAT: Cross-Task Generative Adversarial Attack based on Transferability. (arXiv:2310.14265v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lv_M/0/1/0/all/0/1">Minxuan Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_C/0/1/0/all/0/1">Chengwei Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Kun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1">Wei Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1">Songlin Hu</a></p>
<p>Neural network models are vulnerable to adversarial examples, and adversarial
transferability further increases the risk of adversarial attacks. Current
methods based on transferability often rely on substitute models, which can be
impractical and costly in real-world scenarios due to the unavailability of
training data and the victim model's structural details. In this paper, we
propose a novel approach that directly constructs adversarial examples by
extracting transferable features across various tasks. Our key insight is that
adversarial transferability can extend across different tasks. Specifically, we
train a sequence-to-sequence generative model named CT-GAT using adversarial
sample data collected from multiple tasks to acquire universal adversarial
features and generate adversarial examples for different tasks. We conduct
experiments on ten distinct datasets, and the results demonstrate that our
method achieves superior attack performance with small cost.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.15494">TRAMS: Training-free Memory Selection for Long-range Language Modeling. (arXiv:2310.15494v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Haofei Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Cunxiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yue Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bi_W/0/1/0/all/0/1">Wei Bi</a></p>
<p>The Transformer architecture is crucial for numerous AI models, but it still
faces challenges in long-range language modeling. Though several specific
transformer architectures have been designed to tackle issues of long-range
dependencies, existing methods like Transformer-XL are plagued by a high
percentage of ineffective memories. In this study, we present a plug-and-play
strategy, known as TRAining-free Memory Selection (TRAMS), that selects tokens
participating in attention calculation based on one simple metric. This
strategy allows us to keep tokens that are likely to have a high attention
score with the current queries and ignore the other ones. We have tested our
approach on the word-level benchmark (WikiText-103) and the character-level
benchmark (enwik8), and the results indicate an improvement without having
additional training or adding additional parameters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16248">GlotLID: Language Identification for Low-Resource Languages. (arXiv:2310.16248v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kargaran_A/0/1/0/all/0/1">Amir Hossein Kargaran</a>, <a href="http://arxiv.org/find/cs/1/au:+Imani_A/0/1/0/all/0/1">Ayyoob Imani</a>, <a href="http://arxiv.org/find/cs/1/au:+Yvon_F/0/1/0/all/0/1">Fran&#xe7;ois Yvon</a>, <a href="http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1">Hinrich Sch&#xfc;tze</a></p>
<p>Several recent papers have published good solutions for language
identification (LID) for about 300 high-resource and medium-resource languages.
However, there is no LID available that (i) covers a wide range of low-resource
languages, (ii) is rigorously evaluated and reliable and (iii) efficient and
easy to use. Here, we publish GlotLID-M, an LID model that satisfies the
desiderata of wide coverage, reliability and efficiency. It identifies 1665
languages, a large increase in coverage compared to prior work. In our
experiments, GlotLID-M outperforms four baselines (CLD3, FT176, OpenLID and
NLLB) when balancing F1 and false positive rate (FPR). We analyze the unique
challenges that low-resource LID poses: incorrect corpus metadata, leakage from
high-resource languages, difficulty separating closely related languages,
handling of macrolanguage vs varieties and in general noisy data. We hope that
integrating GlotLID-M into dataset creation pipelines will improve quality and
enhance accessibility of NLP technology for low-resource languages and
cultures. GlotLID-M model, code, and list of data sources are available:
https://github.com/cisnlp/GlotLID.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16787">The Data Provenance Initiative: A Large Scale Audit of Dataset Licensing &amp; Attribution in AI. (arXiv:2310.16787v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Longpre_S/0/1/0/all/0/1">Shayne Longpre</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahari_R/0/1/0/all/0/1">Robert Mahari</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1">Anthony Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Obeng_Marnu_N/0/1/0/all/0/1">Naana Obeng-Marnu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sileo_D/0/1/0/all/0/1">Damien Sileo</a>, <a href="http://arxiv.org/find/cs/1/au:+Brannon_W/0/1/0/all/0/1">William Brannon</a>, <a href="http://arxiv.org/find/cs/1/au:+Muennighoff_N/0/1/0/all/0/1">Niklas Muennighoff</a>, <a href="http://arxiv.org/find/cs/1/au:+Khazam_N/0/1/0/all/0/1">Nathan Khazam</a>, <a href="http://arxiv.org/find/cs/1/au:+Kabbara_J/0/1/0/all/0/1">Jad Kabbara</a>, <a href="http://arxiv.org/find/cs/1/au:+Perisetla_K/0/1/0/all/0/1">Kartik Perisetla</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xinyi Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shippole_E/0/1/0/all/0/1">Enrico Shippole</a>, <a href="http://arxiv.org/find/cs/1/au:+Bollacker_K/0/1/0/all/0/1">Kurt Bollacker</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1">Tongshuang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Villa_L/0/1/0/all/0/1">Luis Villa</a>, <a href="http://arxiv.org/find/cs/1/au:+Pentland_S/0/1/0/all/0/1">Sandy Pentland</a>, <a href="http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1">Sara Hooker</a></p>
<p>The race to train language models on vast, diverse, and inconsistently
documented datasets has raised pressing concerns about the legal and ethical
risks for practitioners. To remedy these practices threatening data
transparency and understanding, we convene a multi-disciplinary effort between
legal and machine learning experts to systematically audit and trace 1800+ text
datasets. We develop tools and standards to trace the lineage of these
datasets, from their source, creators, series of license conditions,
properties, and subsequent use. Our landscape analysis highlights the sharp
divides in composition and focus of commercially open vs closed datasets, with
closed datasets monopolizing important categories: lower resource languages,
more creative tasks, richer topic variety, newer and more synthetic training
data. This points to a deepening divide in the types of data that are made
available under different license conditions, and heightened implications for
jurisdictional legal interpretations of copyright and fair use. We also observe
frequent miscategorization of licenses on widely used dataset hosting sites,
with license omission of 70%+ and error rates of 50%+. This points to a crisis
in misattribution and informed use of the most popular datasets driving many
recent breakthroughs. As a contribution to ongoing improvements in dataset
transparency and responsible use, we release our entire audit, with an
interactive UI, the Data Provenance Explorer, which allows practitioners to
trace and filter on data provenance for the most popular open source finetuning
data collections: www.dataprovenance.org.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17369">Language and Mental Health: Measures of Emotion Dynamics from Text as Linguistic Biosocial Markers. (arXiv:2310.17369v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Teodorescu_D/0/1/0/all/0/1">Daniela Teodorescu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_T/0/1/0/all/0/1">Tiffany Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Fyshe_A/0/1/0/all/0/1">Alona Fyshe</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1">Saif M. Mohammad</a></p>
<p>Research in psychopathology has shown that, at an aggregate level, the
patterns of emotional change over time -- emotion dynamics -- are indicators of
one's mental health. One's patterns of emotion change have traditionally been
determined through self-reports of emotions; however, there are known issues
with accuracy, bias, and ease of data collection. Recent approaches to
determining emotion dynamics from one's everyday utterances addresses many of
these concerns, but it is not yet known whether these measures of utterance
emotion dynamics (UED) correlate with mental health diagnoses. Here, for the
first time, we study the relationship between tweet emotion dynamics and mental
health disorders. We find that each of the UED metrics studied varied by the
user's self-disclosed diagnosis. For example: average valence was significantly
higher (i.e., more positive text) in the control group compared to users with
ADHD, MDD, and PTSD. Valence variability was significantly lower in the control
group compared to ADHD, depression, bipolar disorder, MDD, PTSD, and OCD but
not PPD. Rise and recovery rates of valence also exhibited significant
differences from the control. This work provides important early evidence for
how linguistic cues pertaining to emotion dynamics can play a crucial role as
biosocial markers for mental illnesses and aid in the understanding, diagnosis,
and management of mental health disorders.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.18152">Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs. (arXiv:2310.18152v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1">Yijian Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Ziwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1">Wenwu Zhu</a></p>
<p>Text-attributed graphs (TAGs) are prevalent on the web and research over TAGs
such as citation networks, e-commerce networks and social networks has
attracted considerable attention in the web community. Recently, large language
models (LLMs) have demonstrated exceptional capabilities across a wide range of
tasks. However, the existing works focus on harnessing the potential of LLMs
solely relying on prompts to convey graph structure information to LLMs, thus
suffering from insufficient understanding of the complex structural
relationships within TAGs. To address this problem, in this paper we present
the Disentangled Graph-Text Learner (DGTL) model, which is able to enhance the
reasoning and predicting capabilities of LLMs for TAGs. Our proposed DGTL model
incorporates graph structure information through tailored disentangled graph
neural network (GNN) layers, enabling LLMs to capture the intricate
relationships hidden in text-attributed graphs from multiple structural
factors. Furthermore, DGTL operates with frozen pre-trained LLMs, reducing
computational costs and allowing much more flexibility in combining with
different LLM models. Experimental evaluations demonstrate the effectiveness of
the proposed DGTL model on achieving superior or comparable performance over
state-of-the-art baselines. Additionally, we also demonstrate that our DGTL
model can offer natural language explanations for predictions, thereby
significantly enhancing model interpretability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.18208">ArcheType: A Novel Framework for Open-Source Column Type Annotation using Large Language Models. (arXiv:2310.18208v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feuer_B/0/1/0/all/0/1">Benjamin Feuer</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yurong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hegde_C/0/1/0/all/0/1">Chinmay Hegde</a>, <a href="http://arxiv.org/find/cs/1/au:+Freire_J/0/1/0/all/0/1">Juliana Freire</a></p>
<p>Existing deep-learning approaches to semantic column type annotation (CTA)
have important shortcomings: they rely on semantic types which are fixed at
training time; require a large number of training samples per type and incur
large run-time inference costs; and their performance can degrade when
evaluated on novel datasets, even when types remain constant. Large language
models have exhibited strong zero-shot classification performance on a wide
range of tasks and in this paper we explore their use for CTA. We introduce
ArcheType, a simple, practical method for context sampling, prompt
serialization, model querying, and label remapping, which enables large
language models to solve CTA problems in a fully zero-shot manner. We ablate
each component of our method separately, and establish that improvements to
context sampling and label remapping provide the most consistent gains.
ArcheType establishes a new state-of-the-art performance on zero-shot CTA
benchmarks (including three new domain-specific benchmarks which we release
along with this paper), and when used in conjunction with classical CTA
techniques, it outperforms a SOTA DoDuo model on the fine-tuned SOTAB
benchmark. Our code is available at https://github.com/penfever/ArcheType.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.18343">PHD: Pixel-Based Language Modeling of Historical Documents. (arXiv:2310.18343v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Borenstein_N/0/1/0/all/0/1">Nadav Borenstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Rust_P/0/1/0/all/0/1">Phillip Rust</a>, <a href="http://arxiv.org/find/cs/1/au:+Elliott_D/0/1/0/all/0/1">Desmond Elliott</a>, <a href="http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1">Isabelle Augenstein</a></p>
<p>The digitisation of historical documents has provided historians with
unprecedented research opportunities. Yet, the conventional approach to
analysing historical documents involves converting them from images to text
using OCR, a process that overlooks the potential benefits of treating them as
images and introduces high levels of noise. To bridge this gap, we take
advantage of recent advancements in pixel-based language models trained to
reconstruct masked patches of pixels instead of predicting token distributions.
Due to the scarcity of real historical scans, we propose a novel method for
generating synthetic scans to resemble real historical documents. We then
pre-train our model, PHD, on a combination of synthetic scans and real
historical newspapers from the 1700-1900 period. Through our experiments, we
demonstrate that PHD exhibits high proficiency in reconstructing masked image
patches and provide evidence of our model's noteworthy language understanding
capabilities. Notably, we successfully apply our model to a historical QA task,
highlighting its usefulness in this domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19233">Building Real-World Meeting Summarization Systems using Large Language Models: A Practical Perspective. (arXiv:2310.19233v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Laskar_M/0/1/0/all/0/1">Md Tahmid Rahman Laskar</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1">Xue-Yong Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Cheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+TN_S/0/1/0/all/0/1">Shashi Bhushan TN</a></p>
<p>This paper studies how to effectively build meeting summarization systems for
real-world usage using large language models (LLMs). For this purpose, we
conduct an extensive evaluation and comparison of various closed-source and
open-source LLMs, namely, GPT-4, GPT- 3.5, PaLM-2, and LLaMA-2. Our findings
reveal that most closed-source LLMs are generally better in terms of
performance. However, much smaller open-source models like LLaMA- 2 (7B and
13B) could still achieve performance comparable to the large closed-source
models even in zero-shot scenarios. Considering the privacy concerns of
closed-source models for only being accessible via API, alongside the high cost
associated with using fine-tuned versions of the closed-source models, the
opensource models that can achieve competitive performance are more
advantageous for industrial use. Balancing performance with associated costs
and privacy concerns, the LLaMA-2-7B model looks more promising for industrial
usage. In sum, this paper offers practical insights on using LLMs for
real-world business meeting summarization, shedding light on the trade-offs
between performance and cost.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19975">BioInstruct: Instruction Tuning of Large Language Models for Biomedical Natural Language Processing. (arXiv:2310.19975v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1">Hieu Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhichao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1">Zonghai Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Hong Yu</a></p>
<p>To enhance the performance of large language models (LLMs) in biomedical
natural language processing (BioNLP) by introducing a domain-specific
instruction dataset and examining its impact when combined with multi-task
learning principles. We created the BioInstruct, comprising 25,005 instructions
to instruction-tune LLMs(LLaMA 1 &amp; 2, 7B &amp; 13B version). The instructions were
created by prompting the GPT-4 language model with three-seed samples randomly
drawn from an 80 human curated instructions. We employed Low-Rank
Adaptation(LoRA) for parameter-efficient fine-tuning. We then evaluated these
instruction-tuned LLMs on several BioNLP tasks, which can be grouped into three
major categories: question answering(QA), information extraction(IE), and text
generation(GEN). We also examined whether categories(e.g., QA, IE, and
generation) of instructions impact model performance. Comparing with LLMs
without instruction-tuned, our instruction-tuned LLMs demonstrated marked
performance gains: 17.3% in QA, 5.7% in IE, and 96% in Generation tasks. Our
7B-parameter instruction-tuned LLaMA 1 model was competitive or even surpassed
other LLMs in the biomedical domain that were also fine-tuned from LLaMA 1 with
vast domain-specific data or a variety of tasks. Our results also show that the
performance gain is significantly higher when instruction fine-tuning is
conducted with closely related tasks. Our findings align with the observations
of multi-task learning, suggesting the synergies between two tasks. The
BioInstruct dataset serves as a valuable resource and instruction tuned LLMs
lead to the best performing BioNLP applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.20195">Generating Continuations in Multilingual Idiomatic Contexts. (arXiv:2310.20195v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pokharel_R/0/1/0/all/0/1">Rhitabrat Pokharel</a>, <a href="http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1">Ameeta Agrawal</a></p>
<p>The ability to process idiomatic or literal multiword expressions is a
crucial aspect of understanding and generating any language. The task of
generating contextually relevant continuations for narratives containing
idiomatic (or literal) expressions can allow us to test the ability of
generative language models (LMs) in understanding nuanced language containing
non-compositional figurative text. We conduct a series of experiments using
datasets in two distinct languages (English and Portuguese) under three
different training settings (zero-shot, few-shot, and fine-tuned). Our results
suggest that the models are only slightly better at generating continuations
for literal contexts than idiomatic contexts, with exceedingly small margins.
Furthermore, the models studied in this work perform equally well across both
languages, indicating the robustness of generative models in performing this
task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.20256">PsyCoT: Psychological Questionnaire as Powerful Chain-of-Thought for Personality Detection. (arXiv:2310.20256v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1">Tao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_T/0/1/0/all/0/1">Tianyuan Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_F/0/1/0/all/0/1">Fanqi Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Quan_X/0/1/0/all/0/1">Xiaojun Quan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qifan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1">Bingzhe Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jiaxiang Wu</a></p>
<p>Recent advances in large language models (LLMs), such as ChatGPT, have
showcased remarkable zero-shot performance across various NLP tasks. However,
the potential of LLMs in personality detection, which involves identifying an
individual's personality from their written texts, remains largely unexplored.
Drawing inspiration from Psychological Questionnaires, which are carefully
designed by psychologists to evaluate individual personality traits through a
series of targeted items, we argue that these items can be regarded as a
collection of well-structured chain-of-thought (CoT) processes. By
incorporating these processes, LLMs can enhance their capabilities to make more
reasonable inferences on personality from textual input. In light of this, we
propose a novel personality detection method, called PsyCoT, which mimics the
way individuals complete psychological questionnaires in a multi-turn dialogue
manner. In particular, we employ a LLM as an AI assistant with a specialization
in text analysis. We prompt the assistant to rate individual items at each turn
and leverage the historical rating results to derive a conclusive personality
preference. Our experiments demonstrate that PsyCoT significantly improves the
performance and robustness of GPT-3.5 in personality detection, achieving an
average F1 score improvement of 4.23/10.63 points on two benchmark datasets
compared to the standard prompting method. Our code is available at
https://github.com/TaoYang225/PsyCoT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.20499">Leveraging Word Guessing Games to Assess the Intelligence of Large Language Models. (arXiv:2310.20499v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liang_T/0/1/0/all/0/1">Tian Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1">Zhiwei He</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jen-tse Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenxuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiao_W/0/1/0/all/0/1">Wenxiang Jiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Rui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yujiu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1">Zhaopeng Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1">Shuming Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xing Wang</a></p>
<p>The automatic evaluation of LLM-based agent intelligence is critical in
developing advanced LLM-based agents. Although considerable effort has been
devoted to developing human-annotated evaluation datasets, such as AlpacaEval,
existing techniques are costly, time-consuming, and lack adaptability. In this
paper, inspired by the popular language game ``Who is Spy'', we propose to use
the word guessing game to assess the intelligence performance of LLMs. Given a
word, the LLM is asked to describe the word and determine its identity (spy or
not) based on its and other players' descriptions. Ideally, an advanced agent
should possess the ability to accurately describe a given word using an
aggressive description while concurrently maximizing confusion in the
conservative description, enhancing its participation in the game. To this end,
we first develop DEEP to evaluate LLMs' expression and disguising abilities.
DEEP requires LLM to describe a word in aggressive and conservative modes. We
then introduce SpyGame, an interactive multi-agent framework designed to assess
LLMs' intelligence through participation in a competitive language-based board
game. Incorporating multi-agent interaction, SpyGame requires the target LLM to
possess linguistic skills and strategic thinking, providing a more
comprehensive evaluation of LLMs' human-like cognitive abilities and
adaptability in complex communication situations. The proposed evaluation
framework is very easy to implement. We collected words from multiple sources,
domains, and languages and used the proposed evaluation framework to conduct
experiments. Extensive experiments demonstrate that the proposed DEEP and
SpyGame effectively evaluate the capabilities of various LLMs, capturing their
ability to adapt to novel situations and engage in strategic communication.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01766">Support or Refute: Analyzing the Stance of Evidence to Detect Out-of-Context Mis- and Disinformation. (arXiv:2311.01766v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1">Xin Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jie Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1">Weidong Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zheng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shujun Li</a></p>
<p>Mis- and disinformation online have become a major societal problem as major
sources of online harms of different kinds. One common form of mis- and
disinformation is out-of-context (OOC) information, where different pieces of
information are falsely associated, e.g., a real image combined with a false
textual caption or a misleading textual description. Although some past studies
have attempted to defend against OOC mis- and disinformation through external
evidence, they tend to disregard the role of different pieces of evidence with
different stances. Motivated by the intuition that the stance of evidence
represents a bias towards different detection results, we propose a stance
extraction network (SEN) that can extract the stances of different pieces of
multi-modal evidence in a unified framework. Moreover, we introduce a
support-refutation score calculated based on the co-occurrence relations of
named entities into the textual SEN. Extensive experiments on a public
large-scale dataset demonstrated that our proposed method outperformed the
state-of-the-art baselines, with the best model achieving a performance gain of
3.2% in accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01825">Large Language Models to the Rescue: Reducing the Complexity in Scientific Workflow Development Using ChatGPT. (arXiv:2311.01825v2 [cs.DC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sanger_M/0/1/0/all/0/1">Mario S&#xe4;nger</a>, <a href="http://arxiv.org/find/cs/1/au:+Mecquenem_N/0/1/0/all/0/1">Ninon De Mecquenem</a>, <a href="http://arxiv.org/find/cs/1/au:+Lewinska_K/0/1/0/all/0/1">Katarzyna Ewa Lewi&#x144;ska</a>, <a href="http://arxiv.org/find/cs/1/au:+Bountris_V/0/1/0/all/0/1">Vasilis Bountris</a>, <a href="http://arxiv.org/find/cs/1/au:+Lehmann_F/0/1/0/all/0/1">Fabian Lehmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Leser_U/0/1/0/all/0/1">Ulf Leser</a>, <a href="http://arxiv.org/find/cs/1/au:+Kosch_T/0/1/0/all/0/1">Thomas Kosch</a></p>
<p>Scientific workflow systems are increasingly popular for expressing and
executing complex data analysis pipelines over large datasets, as they offer
reproducibility, dependability, and scalability of analyses by automatic
parallelization on large compute clusters. However, implementing workflows is
difficult due to the involvement of many black-box tools and the deep
infrastructure stack necessary for their execution. Simultaneously,
user-supporting tools are rare, and the number of available examples is much
lower than in classical programming languages. To address these challenges, we
investigate the efficiency of Large Language Models (LLMs), specifically
ChatGPT, to support users when dealing with scientific workflows. We performed
three user studies in two scientific domains to evaluate ChatGPT for
comprehending, adapting, and extending workflows. Our results indicate that
LLMs efficiently interpret workflows but achieve lower performance for
exchanging components or purposeful workflow extensions. We characterize their
limitations in these challenging scenarios and suggest future research
directions.
</p>
</p>
</div>

    </div>
    </body>
    