<!DOCTYPE html>
<html>
<head>
<title>2024-01-31-cs-ai</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2401.15081">Can generative AI and ChatGPT outperform humans on cognitive-demanding problem-solving tasks in science?. (arXiv:2401.15081v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1">Xiaoming Zhai</a>, <a href="http://arxiv.org/find/cs/1/au:+Nyaaba_M/0/1/0/all/0/1">Matthew Nyaaba</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1">Wenchao Ma</a></p>
<p>This study aimed to examine an assumption that generative artificial
intelligence (GAI) tools can overcome the cognitive intensity that humans
suffer when solving problems. We compared the performance of ChatGPT and GPT-4
on 2019 NAEP science assessments with students by cognitive demands of the
items. Fifty-four tasks were coded by experts using a two-dimensional cognitive
load framework, including task cognitive complexity and dimensionality. ChatGPT
and GPT-4 responses were scored using the scoring keys of NAEP. The analysis of
the available data was based on the average student ability scores for students
who answered each item correctly and the percentage of students who responded
to individual items. Results showed that both ChatGPT and GPT-4 consistently
outperformed most students who answered the NAEP science assessments. As the
cognitive demand for NAEP tasks increases, statistically higher average student
ability scores are required to correctly address the questions. This pattern
was observed for students in grades 4, 8, and 12, respectively. However,
ChatGPT and GPT-4 were not statistically sensitive to the increase in cognitive
demands of the tasks, except for Grade 4. As the first study focusing on
comparing GAI and K-12 students in problem-solving in science, this finding
implies the need for changes to educational objectives to prepare students with
competence to work with GAI tools in the future. Education ought to emphasize
the cultivation of advanced cognitive skills rather than depending solely on
tasks that demand cognitive intensity. This approach would foster critical
thinking, analytical skills, and the application of knowledge in novel
contexts. Findings also suggest the need for innovative assessment practices by
moving away from cognitive intensity tasks toward creativity and analytical
skills to avoid the negative effects of GAI on testing more efficiently.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15098">Hi-Core: Hierarchical Knowledge Transfer for Continual Reinforcement Learning. (arXiv:2401.15098v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1">Chaofan Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1">Wei Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1">Tianrui Li</a></p>
<p>Continual reinforcement learning (CRL) empowers RL agents with the ability to
learn from a sequence of tasks, preserving previous knowledge and leveraging it
to facilitate future learning. However, existing methods often focus on
transferring low-level knowledge across similar tasks, which neglects the
hierarchical structure of human cognitive control, resulting in insufficient
knowledge transfer across diverse tasks. To enhance high-level knowledge
transfer, we propose a novel framework named Hi-Core (Hierarchical knowledge
transfer for Continual reinforcement learning), which is structured in two
layers: 1) the high-level policy formulation which utilizes the powerful
reasoning ability of the Large Language Model (LLM) to set goals and 2) the
low-level policy learning through RL which is oriented by high-level goals.
Moreover, the knowledge base (policy library) is constructed to store policies
that can be retrieved for hierarchical knowledge transfer. Experiments
conducted in MiniGrid have demonstrated the effectiveness of Hi-Core in
handling diverse CRL tasks, outperforming popular baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15103">PruneSymNet: A Symbolic Neural Network and Pruning Algorithm for Symbolic Regression. (arXiv:2401.15103v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1">Min Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Weijun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1">Lina Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wenqiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jingyi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yanjie Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hao_M/0/1/0/all/0/1">Meilan Hao</a></p>
<p>Symbolic regression aims to derive interpretable symbolic expressions from
data in order to better understand and interpret data. %which plays an
important role in knowledge discovery and interpretable machine learning.
</p>
<p>In this study, a symbolic network called PruneSymNet is proposed for symbolic
regression. This is a novel neural network whose activation function consists
of common elementary functions and operators. The whole network is
differentiable and can be trained by gradient descent method. Each subnetwork
in the network corresponds to an expression, and our goal is to extract such
subnetworks to get the desired symbolic expression.
</p>
<p>Therefore, a greedy pruning algorithm is proposed to prune the network into a
subnetwork while ensuring the accuracy of data fitting. The proposed greedy
pruning algorithm preserves the edge with the least loss in each pruning, but
greedy algorithm often can not get the optimal solution. In order to alleviate
this problem, we combine beam search during pruning to obtain multiple
candidate expressions each time, and finally select the expression with the
smallest loss as the final result. It was tested on the public data set and
compared with the current popular algorithms. The results showed that the
proposed algorithm had better accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15106">Decision Theoretic Foundations for Experiments Evaluating Human Decisions. (arXiv:2401.15106v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hullman_J/0/1/0/all/0/1">Jessica Hullman</a>, <a href="http://arxiv.org/find/cs/1/au:+Kale_A/0/1/0/all/0/1">Alex Kale</a>, <a href="http://arxiv.org/find/cs/1/au:+Hartline_J/0/1/0/all/0/1">Jason Hartline</a></p>
<p>Decision-making with information displays is a key focus of research in areas
like explainable AI, human-AI teaming, and data visualization. However, what
constitutes a decision problem, and what is required for an experiment to be
capable of concluding that human decisions are flawed in some way, remain open
to speculation. We present a widely applicable definition of a decision problem
synthesized from statistical decision theory and information economics. We
argue that to attribute loss in human performance to forms of bias, an
experiment must provide participants with the information that a rational agent
would need to identify the normative decision. We evaluate the extent to which
recent evaluations of decision-making from the literature on AI-assisted
decisions achieve this criteria. We find that only 6 (17\%) of 35 studies that
claim to identify biased behavior present participants with sufficient
information to characterize their behavior as deviating from good
decision-making. We motivate the value of studying well-defined decision
problems by describing a characterization of performance losses they allow us
to conceive. In contrast, the ambiguities of a poorly communicated decision
problem preclude normative interpretation. We conclude with recommendations for
practice.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15108">Multi-agent Deep Reinforcement Learning for Dynamic Pricing by Fast-charging Electric Vehicle Hubs in ccompetition. (arXiv:2401.15108v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1">Diwas Paudel</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_T/0/1/0/all/0/1">Tapas K. Das</a></p>
<p>Fast-charging hubs for electric vehicles will soon become part of the newly
built infrastructure for transportation electrification across the world. These
hubs are expected to host many DC fast-charging stations and will admit EVs
only for charging. Like the gasoline refueling stations, fast-charging hubs in
a neighborhood will dynamically vary their prices to compete for the same pool
of EV owners. These hubs will interact with the electric power network by
making purchase commitments for a significant part of their power needs in the
day-ahead (DA) electricity market and meeting the difference from the real-time
(RT) market. Hubs may have supplemental battery storage systems (BSS), which
they will use for arbitrage. In this paper, we develop a two-step data-driven
dynamic pricing methodology for hubs in price competition. We first obtain the
DA commitment by solving a stochastic DA commitment model. Thereafter we obtain
the hub pricing strategies by modeling the game as a competitive Markov
decision process (CMDP) and solving it using a multi-agent deep reinforcement
learning (MADRL) approach. We develop a numerical case study for a pricing game
between two charging hubs. We solve the case study with our methodology by
using combinations of two different DRL algorithms, DQN and SAC, and two
different neural networks (NN) architectures, a feed-forward (FF) neural
network, and a multi-head attention (MHA) neural network. We construct a
measure of collusion (index) using the hub profits. A value of zero for this
index indicates no collusion (perfect competition) and a value of one indicates
full collusion (monopolistic behavior). Our results show that the collusion
index varies approximately between 0.14 and 0.45 depending on the combinations
of the algorithms and the architectures chosen by the hubs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15109">Towards Collective Superintelligence: Amplifying Group IQ using Conversational Swarms. (arXiv:2401.15109v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rosenberg_L/0/1/0/all/0/1">Louis Rosenberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Willcox_G/0/1/0/all/0/1">Gregg Willcox</a>, <a href="http://arxiv.org/find/cs/1/au:+Schumann_H/0/1/0/all/0/1">Hans Schumann</a>, <a href="http://arxiv.org/find/cs/1/au:+Mani_G/0/1/0/all/0/1">Ganesh Mani</a></p>
<p>Swarm Intelligence (SI) is a natural phenomenon that enables biological
groups to amplify their combined intellect by forming real-time systems.
Artificial Swarm Intelligence (or Swarm AI) is a technology that enables
networked human groups to amplify their combined intelligence by forming
similar systems. In the past, swarm-based methods were constrained to narrowly
defined tasks like probabilistic forecasting and multiple-choice decision
making. A new technology called Conversational Swarm Intelligence (CSI) was
developed in 2023 that amplifies the decision-making accuracy of networked
human groups through natural conversational deliberations. The current study
evaluated the ability of real-time groups using a CSI platform to take a common
IQ test known as Raven's Advanced Progressive Matrices (RAPM). First, a
baseline group of participants took the Raven's IQ test by traditional survey.
This group averaged 45.6% correct. Then, groups of approximately 35 individuals
answered IQ test questions together using a CSI platform called Thinkscape.
These groups averaged 80.5% correct. This places the CSI groups in the 97th
percentile of IQ test-takers and corresponds to an effective IQ increase of 28
points (p&lt;0.001). This is an encouraging result and suggests that CSI is a
powerful method for enabling conversational collective intelligence in large,
networked groups. In addition, because CSI is scalable across groups of
potentially any size, this technology may provide a viable pathway to building
a Collective Superintelligence.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15118">GeoDecoder: Empowering Multimodal Map Understanding. (arXiv:2401.15118v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1">Feng Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_M/0/1/0/all/0/1">Mian Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Zixian Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chao Wang</a></p>
<p>This paper presents GeoDecoder, a dedicated multimodal model designed for
processing geospatial information in maps. Built on the BeitGPT architecture,
GeoDecoder incorporates specialized expert modules for image and text
processing. On the image side, GeoDecoder utilizes GaoDe Amap as the underlying
base map, which inherently encompasses essential details about road and
building shapes, relative positions, and other attributes. Through the
utilization of rendering techniques, the model seamlessly integrates external
data and features such as symbol markers, drive trajectories, heatmaps, and
user-defined markers, eliminating the need for extra feature engineering. The
text module of GeoDecoder accepts various context texts and question prompts,
generating text outputs in the style of GPT. Furthermore, the GPT-based model
allows for the training and execution of multiple tasks within the same model
in an end-to-end manner. To enhance map cognition and enable GeoDecoder to
acquire knowledge about the distribution of geographic entities in Beijing, we
devised eight fundamental geospatial tasks and conducted pretraining of the
model using large-scale text-image samples. Subsequently, rapid fine-tuning was
performed on three downstream tasks, resulting in significant performance
improvements. The GeoDecoder model demonstrates a comprehensive understanding
of map elements and their associated operations, enabling efficient and
high-quality application of diverse geospatial tasks in different business
scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15119">Interpreting Time Series Transformer Models and Sensitivity Analysis of Population Age Groups to COVID-19 Infections. (arXiv:2401.15119v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1">Md Khairul Islam</a>, <a href="http://arxiv.org/find/cs/1/au:+Valentine_T/0/1/0/all/0/1">Tyler Valentine</a>, <a href="http://arxiv.org/find/cs/1/au:+Sue_T/0/1/0/all/0/1">Timothy Joowon Sue</a>, <a href="http://arxiv.org/find/cs/1/au:+Karmacharya_A/0/1/0/all/0/1">Ayush Karmacharya</a>, <a href="http://arxiv.org/find/cs/1/au:+Benham_L/0/1/0/all/0/1">Luke Neil Benham</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhengguang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1">Kingsley Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Fox_J/0/1/0/all/0/1">Judy Fox</a></p>
<p>Interpreting deep learning time series models is crucial in understanding the
model's behavior and learning patterns from raw data for real-time
decision-making. However, the complexity inherent in transformer-based time
series models poses challenges in explaining the impact of individual features
on predictions. In this study, we leverage recent local interpretation methods
to interpret state-of-the-art time series models. To use real-world datasets,
we collected three years of daily case data for 3,142 US counties. Firstly, we
compare six transformer-based models and choose the best prediction model for
COVID-19 infection. Using 13 input features from the last two weeks, we can
predict the cases for the next two weeks. Secondly, we present an innovative
way to evaluate the prediction sensitivity to 8 population age groups over
highly dynamic multivariate infection data. Thirdly, we compare our proposed
perturbation-based interpretation method with related work, including a total
of eight local interpretation methods. Finally, we apply our framework to
traffic and electricity datasets, demonstrating that our approach is generic
and can be applied to other time-series domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15120">Context-driven self-supervised visual learning: Harnessing the environment as a data source. (arXiv:2401.15120v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1">Lizhen Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">James Z. Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1">Wonseuk Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Wyble_B/0/1/0/all/0/1">Brad Wyble</a></p>
<p>Visual learning often occurs in a specific context, where an agent acquires
skills through exploration and tracking of its location in a consistent
environment. The historical spatial context of the agent provides a similarity
signal for self-supervised contrastive learning. We present a unique approach,
termed Environmental Spatial Similarity (ESS), that complements existing
contrastive learning methods. Using images from simulated, photorealistic
environments as an experimental setting, we demonstrate that ESS outperforms
traditional instance discrimination approaches. Moreover, sampling additional
data from the same environment substantially improves accuracy and provides new
augmentations. ESS allows remarkable proficiency in room classification and
spatial prediction tasks, especially in unfamiliar environments. This learning
paradigm has the potential to enable rapid visual learning in agents operating
in new environments with unique visual characteristics. Potentially
transformative applications span from robotics to space exploration. Our proof
of concept demonstrates improved efficiency over methods that rely on
extensive, disconnected datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15121">Expressive Power of ReLU and Step Networks under Floating-Point Operations. (arXiv:2401.15121v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1">Yeachan Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_G/0/1/0/all/0/1">Geonho Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1">Wonyeol Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1">Sejun Park</a></p>
<p>The study of the expressive power of neural networks has investigated the
fundamental limits of neural networks. Most existing results assume real-valued
inputs and parameters as well as exact operations during the evaluation of
neural networks. However, neural networks are typically executed on computers
that can only represent a tiny subset of the reals and apply inexact
operations. In this work, we analyze the expressive power of neural networks
under a more realistic setup: when we use floating-point numbers and
operations. Our first set of results assumes floating-point operations where
the significand of a float is represented by finite bits but its exponent can
take any integer value. Under this setup, we show that neural networks using a
binary threshold unit or ReLU can memorize any finite input/output pairs and
can approximate any continuous function within a small error. We also show
similar results on memorization and universal approximation when floating-point
operations use finite bits for both significand and exponent; these results are
applicable to many popular floating-point formats such as those defined in the
IEEE 754 standard (e.g., 32-bit single-precision format) and bfloat16.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15122">A Multi-Grained Symmetric Differential Equation Model for Learning Protein-Ligand Binding Dynamics. (arXiv:2401.15122v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shengchao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1">Weitao Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yanjing Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhuoxinran Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhethanabotla_V/0/1/0/all/0/1">Vignesh Bhethanabotla</a>, <a href="http://arxiv.org/find/cs/1/au:+Rampal_N/0/1/0/all/0/1">Nakul Rampal</a>, <a href="http://arxiv.org/find/cs/1/au:+Yaghi_O/0/1/0/all/0/1">Omar Yaghi</a>, <a href="http://arxiv.org/find/cs/1/au:+Borgs_C/0/1/0/all/0/1">Christian Borgs</a>, <a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1">Anima Anandkumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1">Hongyu Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chayes_J/0/1/0/all/0/1">Jennifer Chayes</a></p>
<p>In drug discovery, molecular dynamics (MD) simulation for protein-ligand
binding provides a powerful tool for predicting binding affinities, estimating
transport properties, and exploring pocket sites. There has been a long history
of improving the efficiency of MD simulations through better numerical methods
and, more recently, by augmenting them with machine learning (ML) methods. Yet,
challenges remain, such as accurate modeling of extended-timescale simulations.
To address this issue, we propose NeuralMD, the first ML surrogate that can
facilitate numerical MD and provide accurate simulations of protein-ligand
binding dynamics. We propose a principled approach that incorporates a novel
physics-informed multi-grained group symmetric framework. Specifically, we
propose (1) a BindingNet model that satisfies group symmetry using vector
frames and captures the multi-level protein-ligand interactions, and (2) an
augmented neural differential equation solver that learns the trajectory under
Newtonian mechanics. For the experiment, we design ten single-trajectory and
three multi-trajectory binding simulation tasks. We show the efficiency and
effectiveness of NeuralMD, with a 2000$\times$ speedup over standard numerical
MD simulation and outperforming all other ML approaches by up to 80\% under the
stability metric. We further qualitatively show that NeuralMD reaches more
stable binding predictions compared to other machine learning methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15123">Large Language Model Guided Knowledge Distillation for Time Series Anomaly Detection. (arXiv:2401.15123v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1">Shibo He</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1">Qihang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shizhong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_W/0/1/0/all/0/1">Wenchao Meng</a></p>
<p>Self-supervised methods have gained prominence in time series anomaly
detection due to the scarcity of available annotations. Nevertheless, they
typically demand extensive training data to acquire a generalizable
representation map, which conflicts with scenarios of a few available samples,
thereby limiting their performance. To overcome the limitation, we propose
\textbf{AnomalyLLM}, a knowledge distillation-based time series anomaly
detection approach where the student network is trained to mimic the features
of the large language model (LLM)-based teacher network that is pretrained on
large-scale datasets. During the testing phase, anomalies are detected when the
discrepancy between the features of the teacher and student networks is large.
To circumvent the student network from learning the teacher network's feature
of anomalous samples, we devise two key strategies. 1) Prototypical signals are
incorporated into the student network to consolidate the normal feature
extraction. 2) We use synthetic anomalies to enlarge the representation gap
between the two networks. AnomalyLLM demonstrates state-of-the-art performance
on 15 datasets, improving accuracy by at least 14.5\% in the UCR dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15124">Sensor-Based Data Acquisition via Ubiquitous Device to Detect Muscle Strength Training Activities. (arXiv:2401.15124v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wianto_E/0/1/0/all/0/1">E. Wianto</a>, <a href="http://arxiv.org/find/cs/1/au:+Toba_H/0/1/0/all/0/1">H. Toba</a>, <a href="http://arxiv.org/find/cs/1/au:+Malinda_M/0/1/0/all/0/1">M. Malinda</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chien-Hsu Chen</a></p>
<p>Maintaining a high quality of life through physical activities (PA) to
prevent health decline is crucial. However, the relationship between
individuals health status, PA preferences, and motion factors is complex. PA
discussions consistently show a positive correlation with healthy aging
experiences, but no explicit relation to specific types of musculoskeletal
exercises. Taking advantage of the increasingly widespread existence of
smartphones, especially in Indonesia, this research utilizes embedded sensors
for Human Activity Recognition (HAR). Based on 25 participants data, performing
nine types of selected motion, this study has successfully identified important
sensor attributes that play important roles in the right and left hands for
muscle strength motions as the basis for developing machine learning models
with the LSTM algorithm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15132">On the Emergence of Symmetrical Reality. (arXiv:2401.15132v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhenliang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zeyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiao_Z/0/1/0/all/0/1">Ziyuan Jiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1">Yao Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hangxin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1">Song-Chun Zhu</a></p>
<p>Artificial intelligence (AI) has revolutionized human cognitive abilities and
facilitated the development of new AI entities capable of interacting with
humans in both physical and virtual environments. Despite the existence of
virtual reality, mixed reality, and augmented reality for several years,
integrating these technical fields remains a formidable challenge due to their
disparate application directions. The advent of AI agents, capable of
autonomous perception and action, further compounds this issue by exposing the
limitations of traditional human-centered research approaches. It is imperative
to establish a comprehensive framework that accommodates the dual perceptual
centers of humans and AI agents in both physical and virtual worlds. In this
paper, we introduce the symmetrical reality framework, which offers a unified
representation encompassing various forms of physical-virtual amalgamations.
This framework enables researchers to better comprehend how AI agents can
collaborate with humans and how distinct technical pathways of physical-virtual
integration can be consolidated from a broader perspective. We then delve into
the coexistence of humans and AI, demonstrating a prototype system that
exemplifies the operation of symmetrical reality systems for specific tasks,
such as pouring water. Subsequently, we propose an instance of an AI-driven
active assistance service that illustrates the potential applications of
symmetrical reality. This paper aims to offer beneficial perspectives and
guidance for researchers and practitioners in different fields, thus
contributing to the ongoing research about human-AI coexistence in both
physical and virtual environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15170">Scalable Qualitative Coding with LLMs: Chain-of-Thought Reasoning Matches Human Performance in Some Hermeneutic Tasks. (arXiv:2401.15170v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dunivin_Z/0/1/0/all/0/1">Zackary Okun Dunivin</a></p>
<p>Qualitative coding, or content analysis, extracts meaning from text to
discern quantitative patterns across a corpus of texts. Recently, advances in
the interpretive abilities of large language models (LLMs) offer potential for
automating the coding process (applying category labels to texts), thereby
enabling human researchers to concentrate on more creative research aspects,
while delegating these interpretive tasks to AI. Our case study comprises a set
of socio-historical codes on dense, paragraph-long passages representative of a
humanistic study. We show that GPT-4 is capable of human-equivalent
interpretations, whereas GPT-3.5 is not. Compared to our human-derived gold
standard, GPT-4 delivers excellent intercoder reliability (Cohen's $\kappa \geq
0.79$) for 3 of 9 codes, and substantial reliability ($\kappa \geq 0.6$) for 8
of 9 codes. In contrast, GPT-3.5 greatly underperforms for all codes
($mean(\kappa) = 0.34$; $max(\kappa) = 0.55$). Importantly, we find that coding
fidelity improves considerably when the LLM is prompted to give rationale
justifying its coding decisions (chain-of-thought reasoning). We present these
and other findings along with a set of best practices for adapting traditional
codebooks for LLMs. Our results indicate that for certain codebooks,
state-of-the-art LLMs are already adept at large-scale content analysis.
Furthermore, they suggest the next generation of models will likely render AI
coding a viable option for a majority of codebooks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15188">CAREForMe: Contextual Multi-Armed Bandit Recommendation Framework for Mental Health. (arXiv:2401.15188v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1">Sheng Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Nourzad_N/0/1/0/all/0/1">Narjes Nourzad</a>, <a href="http://arxiv.org/find/cs/1/au:+Semple_R/0/1/0/all/0/1">Randye J. Semple</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yixue Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_E/0/1/0/all/0/1">Emily Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishnamachari_B/0/1/0/all/0/1">Bhaskar Krishnamachari</a></p>
<p>The COVID-19 pandemic has intensified the urgency for effective and
accessible mental health interventions in people's daily lives. Mobile Health
(mHealth) solutions, such as AI Chatbots and Mindfulness Apps, have gained
traction as they expand beyond traditional clinical settings to support daily
life. However, the effectiveness of current mHealth solutions is impeded by the
lack of context-awareness, personalization, and modularity to foster their
reusability. This paper introduces CAREForMe, a contextual multi-armed bandit
(CMAB) recommendation framework for mental health. Designed with
context-awareness, personalization, and modularity at its core, CAREForMe
harnesses mobile sensing and integrates online learning algorithms with user
clustering capability to deliver timely, personalized recommendations. With its
modular design, CAREForMe serves as both a customizable recommendation
framework to guide future research, and a collaborative platform to facilitate
interdisciplinary contributions in mHealth research. We showcase CAREForMe's
versatility through its implementation across various platforms (e.g., Discord,
Telegram) and its customization to diverse recommendation features.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15196">Regularized Q-Learning with Linear Function Approximation. (arXiv:2401.15196v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xi_J/0/1/0/all/0/1">Jiachen Xi</a>, <a href="http://arxiv.org/find/cs/1/au:+Garcia_A/0/1/0/all/0/1">Alfredo Garcia</a>, <a href="http://arxiv.org/find/cs/1/au:+Momcilovic_P/0/1/0/all/0/1">Petar Momcilovic</a></p>
<p>Several successful reinforcement learning algorithms make use of
regularization to promote multi-modal policies that exhibit enhanced
exploration and robustness. With functional approximation, the convergence
properties of some of these algorithms (e.g. soft Q-learning) are not well
understood. In this paper, we consider a single-loop algorithm for minimizing
the projected Bellman error with finite time convergence guarantees in the case
of linear function approximation. The algorithm operates on two scales: a
slower scale for updating the target network of the state-action values, and a
faster scale for approximating the Bellman backups in the subspace of the span
of basis vectors. We show that, under certain assumptions, the proposed
algorithm converges to a stationary point in the presence of Markovian noise.
In addition, we provide a performance guarantee for the policies derived from
the proposed algorithm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15199">SCANIA Component X Dataset: A Real-World Multivariate Time Series Dataset for Predictive Maintenance. (arXiv:2401.15199v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kharazian_Z/0/1/0/all/0/1">Zahra Kharazian</a>, <a href="http://arxiv.org/find/cs/1/au:+Lindgren_T/0/1/0/all/0/1">Tony Lindgren</a>, <a href="http://arxiv.org/find/cs/1/au:+Magnusson_S/0/1/0/all/0/1">Sindri Magn&#xfa;sson</a>, <a href="http://arxiv.org/find/cs/1/au:+Steinert_O/0/1/0/all/0/1">Olof Steinert</a>, <a href="http://arxiv.org/find/cs/1/au:+Reyna_O/0/1/0/all/0/1">Oskar Andersson Reyna</a></p>
<p>This paper presents a description of a real-world, multivariate time series
dataset collected from an anonymized engine component (called Component X) of a
fleet of trucks from SCANIA, Sweden. This dataset includes diverse variables
capturing detailed operational data, repair records, and specifications of
trucks while maintaining confidentiality by anonymization. It is well-suited
for a range of machine learning applications, such as classification,
regression, survival analysis, and anomaly detection, particularly when applied
to predictive maintenance scenarios. The large population size and variety of
features in the format of histograms and numerical counters, along with the
inclusion of temporal information, make this real-world dataset unique in the
field. The objective of releasing this dataset is to give a broad range of
researchers the possibility of working with real-world data from an
internationally well-known company and introduce a standard benchmark to the
predictive maintenance field, fostering reproducible research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15210">Roq: Robust Query Optimization Based on a Risk-aware Learned Cost Model. (arXiv:2401.15210v1 [cs.DB])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kamali_A/0/1/0/all/0/1">Amin Kamali</a>, <a href="http://arxiv.org/find/cs/1/au:+Kantere_V/0/1/0/all/0/1">Verena Kantere</a>, <a href="http://arxiv.org/find/cs/1/au:+Zuzarte_C/0/1/0/all/0/1">Calisto Zuzarte</a>, <a href="http://arxiv.org/find/cs/1/au:+Corvinelli_V/0/1/0/all/0/1">Vincent Corvinelli</a></p>
<p>Query optimizers in relational database management systems (RDBMSs) search
for execution plans expected to be optimal for a given queries. They use
parameter estimates, often inaccurate, and make assumptions that may not hold
in practice. Consequently, they may select execution plans that are suboptimal
at runtime, when these estimates and assumptions are not valid, which may
result in poor query performance. Therefore, query optimizers do not
sufficiently support robust query optimization. Recent years have seen a surge
of interest in using machine learning (ML) to improve efficiency of data
systems and reduce their maintenance overheads, with promising results obtained
in the area of query optimization in particular. In this paper, inspired by
these advancements, and based on several years of experience of IBM Db2 in this
journey, we propose Robust Optimization of Queries, (Roq), a holistic framework
that enables robust query optimization based on a risk-aware learning approach.
Roq includes a novel formalization of the notion of robustness in the context
of query optimization and a principled approach for its quantification and
measurement based on approximate probabilistic ML. It also includes novel
strategies and algorithms for query plan evaluation and selection. Roq also
includes a novel learned cost model that is designed to predict query execution
cost and the associated risks and performs query optimization accordingly. We
demonstrate experimentally that Roq provides significant improvements to robust
query optimization compared to the state-of-the-art.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15222">Transfer Learning for the Prediction of Entity Modifiers in Clinical Text: Application to Opioid Use Disorder Case Detection. (arXiv:2401.15222v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Almudaifer_A/0/1/0/all/0/1">Abdullateef I. Almudaifer</a>, <a href="http://arxiv.org/find/cs/1/au:+O%60Leary_T/0/1/0/all/0/1">Tobias O`Leary</a>, <a href="http://arxiv.org/find/cs/1/au:+Covington_W/0/1/0/all/0/1">Whitney Covington</a>, <a href="http://arxiv.org/find/cs/1/au:+Hairston_J/0/1/0/all/0/1">JaMor Hairston</a>, <a href="http://arxiv.org/find/cs/1/au:+Deitch_Z/0/1/0/all/0/1">Zachary Deitch</a>, <a href="http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1">Ankit Anand</a>, <a href="http://arxiv.org/find/cs/1/au:+Carroll_C/0/1/0/all/0/1">Caleb M. Carroll</a>, <a href="http://arxiv.org/find/cs/1/au:+Crisan_E/0/1/0/all/0/1">Estera Crisan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bradford_W/0/1/0/all/0/1">William Bradford</a>, <a href="http://arxiv.org/find/cs/1/au:+Walter_L/0/1/0/all/0/1">Lauren Walter</a>, <a href="http://arxiv.org/find/cs/1/au:+Ellen_E/0/1/0/all/0/1">Eaton Ellen</a>, <a href="http://arxiv.org/find/cs/1/au:+Feldman_S/0/1/0/all/0/1">Sue S. Feldman</a>, <a href="http://arxiv.org/find/cs/1/au:+Osborne_J/0/1/0/all/0/1">John D. Osborne</a></p>
<p>Background: The semantics of entities extracted from a clinical text can be
dramatically altered by modifiers, including entity negation, uncertainty,
conditionality, severity, and subject. Existing models for determining
modifiers of clinical entities involve regular expression or features weights
that are trained independently for each modifier.
</p>
<p>Methods: We develop and evaluate a multi-task transformer architecture design
where modifiers are learned and predicted jointly using the publicly available
SemEval 2015 Task 14 corpus and a new Opioid Use Disorder (OUD) data set that
contains modifiers shared with SemEval as well as novel modifiers specific for
OUD. We evaluate the effectiveness of our multi-task learning approach versus
previously published systems and assess the feasibility of transfer learning
for clinical entity modifiers when only a portion of clinical modifiers are
shared.
</p>
<p>Results: Our approach achieved state-of-the-art results on the ShARe corpus
from SemEval 2015 Task 14, showing an increase of 1.1% on weighted accuracy,
1.7% on unweighted accuracy, and 10% on micro F1 scores.
</p>
<p>Conclusions: We show that learned weights from our shared model can be
effectively transferred to a new partially matched data set, validating the use
of transfer learning for clinical text modifiers
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15238">Deep Learning with Tabular Data: A Self-supervised Approach. (arXiv:2401.15238v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vyas_T/0/1/0/all/0/1">Tirth Kiranbhai Vyas</a></p>
<p>We have described a novel approach for training tabular data using the
TabTransformer model with self-supervised learning. Traditional machine
learning models for tabular data, such as GBDT are being widely used though our
paper examines the effectiveness of the TabTransformer which is a Transformer
based model optimised specifically for tabular data. The TabTransformer
captures intricate relationships and dependencies among features in tabular
data by leveraging the self-attention mechanism of Transformers. We have used a
self-supervised learning approach in this study, where the TabTransformer
learns from unlabelled data by creating surrogate supervised tasks, eliminating
the need for the labelled data. The aim is to find the most effective
TabTransformer model representation of categorical and numerical features. To
address the challenges faced during the construction of various input settings
into the Transformers. Furthermore, a comparative analysis is also been
conducted to examine performance of the TabTransformer model against baseline
models such as MLP and supervised TabTransformer.
</p>
<p>The research has presented with a novel approach by creating various variants
of TabTransformer model namely, Binned-TT, Vanilla-MLP-TT, MLP- based-TT which
has helped to increase the effective capturing of the underlying relationship
between various features of the tabular dataset by constructing optimal inputs.
And further we have employed a self-supervised learning approach in the form of
a masking-based unsupervised setting for tabular data. The findings shed light
on the best way to represent categorical and numerical features, emphasizing
the TabTransormer performance when compared to established machine learning
models and other self-supervised learning methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15241">Unlearning Reveals the Influential Training Data of Language Models. (arXiv:2401.15241v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Isonuma_M/0/1/0/all/0/1">Masaru Isonuma</a>, <a href="http://arxiv.org/find/cs/1/au:+Titov_I/0/1/0/all/0/1">Ivan Titov</a></p>
<p>In order to enhance the performance of language models while mitigating the
risks of generating harmful content, it is crucial to identify which training
dataset affects the model's outputs. Ideally, we can measure the influence of
each dataset by removing it from training; however, it is prohibitively
expensive to retrain a model multiple times. This paper presents UnTrac, which
estimates the influence of a training dataset by unlearning it from the trained
model. UnTrac is extremely simple; each training dataset is unlearned by
gradient ascent, and we evaluate how much the model's predictions change after
unlearning. We empirically examine if our methods can assess the influence of
pretraining datasets on generating toxic, biased, and untruthful content.
Experimental results demonstrate that our method estimates their influence much
more accurately than existing methods while requiring neither excessive memory
space nor multiple model checkpoints.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15245">GenPluSSS: A Genetic Algorithm Based Plugin for Measured Subsurface Scattering Representation. (arXiv:2401.15245v1 [cs.GR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yildirim_B/0/1/0/all/0/1">Bar&#x131;&#x15f; Y&#x131;ld&#x131;r&#x131;m</a>, <a href="http://arxiv.org/find/cs/1/au:+Kurt_M/0/1/0/all/0/1">Murat Kurt</a></p>
<p>This paper presents a plugin that adds a representation of homogeneous and
heterogeneous, optically thick, translucent materials on the Blender 3D
modeling tool. The working principle of this plugin is based on a combination
of Genetic Algorithm (GA) and Singular Value Decomposition (SVD)-based
subsurface scattering method (GenSSS). The proposed plugin has been implemented
using Mitsuba renderer, which is an open source rendering software. The
proposed plugin has been validated on measured subsurface scattering data. It's
shown that the proposed plugin visualizes homogeneous and heterogeneous
subsurface scattering effects, accurately, compactly and computationally
efficiently.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15268">Towards Stable Preferences for Stakeholder-aligned Machine Learning. (arXiv:2401.15268v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sheraz_H/0/1/0/all/0/1">Haleema Sheraz</a>, <a href="http://arxiv.org/find/cs/1/au:+Kremer_S/0/1/0/all/0/1">Stefan C. Kremer</a>, <a href="http://arxiv.org/find/cs/1/au:+Skorburg_J/0/1/0/all/0/1">Joshua August Skorburg</a>, <a href="http://arxiv.org/find/cs/1/au:+Taylor_G/0/1/0/all/0/1">Graham Taylor</a>, <a href="http://arxiv.org/find/cs/1/au:+Sinnott_Armstrong_W/0/1/0/all/0/1">Walter Sinnott-Armstrong</a>, <a href="http://arxiv.org/find/cs/1/au:+Boerstler_K/0/1/0/all/0/1">Kyle Boerstler</a></p>
<p>In response to the pressing challenge of kidney allocation, characterized by
growing demands for organs, this research sets out to develop a data-driven
solution to this problem, which also incorporates stakeholder values. The
primary objective of this study is to create a method for learning both
individual and group-level preferences pertaining to kidney allocations.
Drawing upon data from the 'Pairwise Kidney Patient Online Survey.' Leveraging
two distinct datasets and evaluating across three levels - Individual, Group
and Stability - we employ machine learning classifiers assessed through several
metrics. The Individual level model predicts individual participant
preferences, the Group level model aggregates preferences across participants,
and the Stability level model, an extension of the Group level, evaluates the
stability of these preferences over time. By incorporating stakeholder
preferences into the kidney allocation process, we aspire to advance the
ethical dimensions of organ transplantation, contributing to more transparent
and equitable practices while promoting the integration of moral values into
algorithmic decision-making.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15269">Improving Medical Reasoning through Retrieval and Self-Reflection with Retrieval-Augmented Large Language Models. (arXiv:2401.15269v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jeong_M/0/1/0/all/0/1">Minbyul Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Sohn_J/0/1/0/all/0/1">Jiwoong Sohn</a>, <a href="http://arxiv.org/find/cs/1/au:+Sung_M/0/1/0/all/0/1">Mujeen Sung</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1">Jaewoo Kang</a></p>
<p>Recent proprietary large language models (LLMs), such as GPT-4, have achieved
a milestone in tackling diverse challenges in the biomedical domain, ranging
from multiple-choice questions to long-form generations. To address challenges
that still cannot be handled with the encoded knowledge of LLMs, various
retrieval-augmented generation (RAG) methods have been developed by searching
documents from the knowledge corpus and appending them unconditionally or
selectively to the input of LLMs for generation. However, when applying
existing methods to different domain-specific problems, poor generalization
becomes apparent, leading to fetching incorrect documents or making inaccurate
judgments. In this paper, we introduce Self-BioRAG, a framework reliable for
biomedical text that specializes in generating explanations, retrieving
domain-specific documents, and self-reflecting generated responses. We utilize
84k filtered biomedical instruction sets to train Self-BioRAG that can assess
its generated explanations with customized reflective tokens. Our work proves
that domain-specific components, such as a retriever, domain-related document
corpus, and instruction sets are necessary for adhering to domain-related
instructions. Using three major medical question-answering benchmark datasets,
experimental results of Self-BioRAG demonstrate significant performance gains
by achieving a 7.2% absolute improvement on average over the state-of-the-art
open-foundation model with a parameter size of 7B or less. Overall, we analyze
that Self-BioRAG finds the clues in the question, retrieves relevant documents
if needed, and understands how to answer with information from retrieved
documents and encoded knowledge as a medical expert does. We release our data
and code for training our framework components and model weights (7B and 13B)
to enhance capabilities in biomedical and clinical domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15270">SimFair: Physics-Guided Fairness-Aware Learning with Simulation Models. (arXiv:2401.15270v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhihao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1">Yiqun Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhili Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1">Xiaowei Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1">Zhe Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_A/0/1/0/all/0/1">Aolin Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Shuo Xu</a></p>
<p>Fairness-awareness has emerged as an essential building block for the
responsible use of artificial intelligence in real applications. In many cases,
inequity in performance is due to the change in distribution over different
regions. While techniques have been developed to improve the transferability of
fairness, a solution to the problem is not always feasible with no samples from
the new regions, which is a bottleneck for pure data-driven attempts.
Fortunately, physics-based mechanistic models have been studied for many
problems with major social impacts. We propose SimFair, a physics-guided
fairness-aware learning framework, which bridges the data limitation by
integrating physical-rule-based simulation and inverse modeling into the
training design. Using temperature prediction as an example, we demonstrate the
effectiveness of the proposed SimFair in fairness preservation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15284">Building ethical guidelines for generative AI in scientific research. (arXiv:2401.15284v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zhicheng Lin</a></p>
<p>Generative artificial intelligence tools like large language models are
rapidly transforming academic research and real world applications. However,
discussions on ethical guidelines for generative AI in science remain
fragmented, underscoring the urgent need for consensus based standards. This
paper offers an initial framework by developing analyses and mitigation
strategies across five key themes: understanding model limitations regarding
truthfulness and bias; respecting privacy, confidentiality, and copyright;
avoiding plagiarism and policy violations when incorporating model output;
ensuring applications provide overall benefit; and using AI transparently and
reproducibly. Common scenarios are outlined to demonstrate potential ethical
violations. We argue that global consensus coupled with professional training
and reasonable enforcement are critical to promoting the benefits of AI while
safeguarding research integrity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15293">SkipViT: Speeding Up Vision Transformers with a Token-Level Skip Connection. (arXiv:2401.15293v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ataiefard_F/0/1/0/all/0/1">Foozhan Ataiefard</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_W/0/1/0/all/0/1">Walid Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Hajimolahoseini_H/0/1/0/all/0/1">Habib Hajimolahoseini</a>, <a href="http://arxiv.org/find/cs/1/au:+Asani_S/0/1/0/all/0/1">Saina Asani</a>, <a href="http://arxiv.org/find/cs/1/au:+Javadi_F/0/1/0/all/0/1">Farnoosh Javadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hassanpour_M/0/1/0/all/0/1">Mohammad Hassanpour</a>, <a href="http://arxiv.org/find/cs/1/au:+Awad_O/0/1/0/all/0/1">Omar Mohamed Awad</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_A/0/1/0/all/0/1">Austin Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1">Kangling Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a></p>
<p>Vision transformers are known to be more computationally and data-intensive
than CNN models. These transformer models such as ViT, require all the input
image tokens to learn the relationship among them. However, many of these
tokens are not informative and may contain irrelevant information such as
unrelated background or unimportant scenery. These tokens are overlooked by the
multi-head self-attention (MHSA), resulting in many redundant and unnecessary
computations in MHSA and the feed-forward network (FFN). In this work, we
propose a method to optimize the amount of unnecessary interactions between
unimportant tokens by separating and sending them through a different low-cost
computational path. Our method does not add any parameters to the ViT model and
aims to find the best trade-off between training throughput and achieving a 0%
loss in the Top-1 accuracy of the final model. Our experimental results on
training ViT-small from scratch show that SkipViT is capable of effectively
dropping 55% of the tokens while gaining more than 13% training throughput and
maintaining classification accuracy at the level of the baseline model on
Huawei Ascend910A.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15296">A Survey on 3D Skeleton Based Person Re-Identification: Approaches, Designs, Challenges, and Future Directions. (arXiv:2401.15296v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rao_H/0/1/0/all/0/1">Haocong Rao</a>, <a href="http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1">Chunyan Miao</a></p>
<p>Person re-identification via 3D skeletons is an important emerging research
area that triggers great interest in the pattern recognition community. With
distinctive advantages for many application scenarios, a great diversity of 3D
skeleton based person re-identification (SRID) methods have been proposed in
recent years, effectively addressing prominent problems in skeleton modeling
and feature learning. Despite recent advances, to the best of our knowledge,
little effort has been made to comprehensively summarize these studies and
their challenges. In this paper, we attempt to fill this gap by providing a
systematic survey on current SRID approaches, model designs, challenges, and
future directions. Specifically, we first formulate the SRID problem, and
propose a taxonomy of SRID research with a summary of benchmark datasets,
commonly-used model architectures, and an analytical review of different
methods' characteristics. Then, we elaborate on the design principles of SRID
models from multiple aspects to offer key insights for model improvement.
Finally, we identify critical challenges confronting current studies and
discuss several promising directions for future research of SRID.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15299">SupplyGraph: A Benchmark Dataset for Supply Chain Planning using Graph Neural Networks. (arXiv:2401.15299v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wasi_A/0/1/0/all/0/1">Azmine Toushik Wasi</a>, <a href="http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1">MD Shafikul Islam</a>, <a href="http://arxiv.org/find/cs/1/au:+Akib_A/0/1/0/all/0/1">Adipto Raihan Akib</a></p>
<p>Graph Neural Networks (GNNs) have gained traction across different domains
such as transportation, bio-informatics, language processing, and computer
vision. However, there is a noticeable absence of research on applying GNNs to
supply chain networks. Supply chain networks are inherently graph-like in
structure, making them prime candidates for applying GNN methodologies. This
opens up a world of possibilities for optimizing, predicting, and solving even
the most complex supply chain problems. A major setback in this approach lies
in the absence of real-world benchmark datasets to facilitate the research and
resolution of supply chain problems using GNNs. To address the issue, we
present a real-world benchmark dataset for temporal tasks, obtained from one of
the leading FMCG companies in Bangladesh, focusing on supply chain planning for
production purposes. The dataset includes temporal data as node features to
enable sales predictions, production planning, and the identification of
factory issues. By utilizing this dataset, researchers can employ GNNs to
address numerous supply chain problems, thereby advancing the field of supply
chain analytics and planning. Source: https://github.com/CIOL-SUST/SupplyGraph
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15318">Gaussian Splashing: Dynamic Fluid Synthesis with Gaussian Splatting. (arXiv:2401.15318v1 [cs.GR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yutao Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1">Xiang Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_Y/0/1/0/all/0/1">Yintong Shang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Ying Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1">Chang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zong_Z/0/1/0/all/0/1">Zeshun Zong</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_T/0/1/0/all/0/1">Tianjia Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Hongzhi Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1">Kun Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1">Chenfanfu Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yin Yang</a></p>
<p>We demonstrate the feasibility of integrating physics-based animations of
solids and fluids with 3D Gaussian Splatting (3DGS) to create novel effects in
virtual scenes reconstructed using 3DGS. Leveraging the coherence of the
Gaussian splatting and position-based dynamics (PBD) in the underlying
representation, we manage rendering, view synthesis, and the dynamics of solids
and fluids in a cohesive manner. Similar to Gaussian shader, we enhance each
Gaussian kernel with an added normal, aligning the kernel's orientation with
the surface normal to refine the PBD simulation. This approach effectively
eliminates spiky noises that arise from rotational deformation in solids. It
also allows us to integrate physically based rendering to augment the dynamic
surface reflections on fluids. Consequently, our framework is capable of
realistically reproducing surface highlights on dynamic fluids and facilitating
interactions between scene objects and fluids from new views. For more
information, please visit our project page at
\url{https://amysteriouscat.github.io/GaussianSplashing/}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15323">Music Auto-Tagging with Robust Music Representation Learned via Domain Adversarial Training. (arXiv:2401.15323v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Joung_H/0/1/0/all/0/1">Haesun Joung</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kyogu Lee</a></p>
<p>Music auto-tagging is crucial for enhancing music discovery and
recommendation. Existing models in Music Information Retrieval (MIR) struggle
with real-world noise such as environmental and speech sounds in multimedia
content. This study proposes a method inspired by speech-related tasks to
enhance music auto-tagging performance in noisy settings. The approach
integrates Domain Adversarial Training (DAT) into the music domain, enabling
robust music representations that withstand noise. Unlike previous research,
this approach involves an additional pretraining phase for the domain
classifier, to avoid performance degradation in the subsequent phase. Adding
various synthesized noisy music data improves the model's generalization across
different noise levels. The proposed architecture demonstrates enhanced
performance in music auto-tagging by effectively utilizing unlabeled noisy
music data. Additional experiments with supplementary unlabeled data further
improves the model's performance, underscoring its robust generalization
capabilities and broad applicability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15335">L-AutoDA: Leveraging Large Language Models for Automated Decision-based Adversarial Attacks. (arXiv:2401.15335v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1">Ping Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1">Fei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1">Xi Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1">Qingchuan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qingfu Zhang</a></p>
<p>In the rapidly evolving field of machine learning, adversarial attacks
present a significant challenge to model robustness and security.
Decision-based attacks, which only require feedback on the decision of a model
rather than detailed probabilities or scores, are particularly insidious and
difficult to defend against. This work introduces L-AutoDA (Large Language
Model-based Automated Decision-based Adversarial Attacks), a novel approach
leveraging the generative capabilities of Large Language Models (LLMs) to
automate the design of these attacks. By iteratively interacting with LLMs in
an evolutionary framework, L-AutoDA automatically designs competitive attack
algorithms efficiently without much human effort. We demonstrate the efficacy
of L-AutoDA on CIFAR-10 dataset, showing significant improvements over baseline
methods in both success rate and computational efficiency. Our findings
underscore the potential of language models as tools for adversarial attack
generation and highlight new avenues for the development of robust AI systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15337">Deep Learning with Information Fusion and Model Interpretation for Health Monitoring of Fetus based on Long-term Prenatal Electronic Fetal Heart Rate Monitoring Data. (arXiv:2401.15337v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zenghui Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xintong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1">Nan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1">Ruichen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qingao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1">Jingying Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Liwei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1">Shenda Hong</a></p>
<p>Long-term fetal heart rate (FHR) monitoring during the antepartum period,
increasingly popularized by electronic FHR monitoring, represents a growing
approach in FHR monitoring. This kind of continuous monitoring, in contrast to
the short-term one, collects an extended period of fetal heart data. This
offers a more comprehensive understanding of fetus's conditions. However, the
interpretation of long-term antenatal fetal heart monitoring is still in its
early stages, lacking corresponding clinical standards. Furthermore, the
substantial amount of data generated by continuous monitoring imposes a
significant burden on clinical work when analyzed manually. To address above
challenges, this study develops an automatic analysis system named LARA
(Long-term Antepartum Risk Analysis system) for continuous FHR monitoring,
combining deep learning and information fusion methods. LARA's core is a
well-established convolutional neural network (CNN) model. It processes
long-term FHR data as input and generates a Risk Distribution Map (RDM) and
Risk Index (RI) as the analysis results. We evaluate LARA on inner test
dataset, the performance metrics are as follows: AUC 0.872, accuracy 0.816,
specificity 0.811, sensitivity 0.806, precision 0.271, and F1 score 0.415. In
our study, we observe that long-term FHR monitoring data with higher RI is more
likely to result in adverse outcomes (p=0.0021). In conclusion, this study
introduces LARA, the first automated analysis system for long-term FHR
monitoring, initiating the further explorations into its clinical value in the
future.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15347">A Comprehensive Survey of Compression Algorithms for Language Models. (arXiv:2401.15347v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1">Seungcheol Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1">Jaehyeon Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Sojin Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_U/0/1/0/all/0/1">U Kang</a></p>
<p>How can we compress language models without sacrificing accuracy? The number
of compression algorithms for language models is rapidly growing to benefit
from remarkable advances of recent language models without side effects due to
the gigantic size of language models, such as increased carbon emissions and
expensive maintenance fees. While numerous compression algorithms have shown
remarkable progress in compressing language models, it ironically becomes
challenging to capture emerging trends and identify the fundamental concepts
underlying them due to the excessive number of algorithms. In this paper, we
survey and summarize diverse compression algorithms including pruning,
quantization, knowledge distillation, low-rank approximation, parameter
sharing, and efficient architecture design. We not only summarize the overall
trend of diverse compression algorithms but also select representative
algorithms and provide in-depth analyses of them. We discuss the value of each
category of compression algorithms, and the desired properties of low-cost
compression algorithms which have a significant impact due to the emergence of
large language models. Finally, we introduce promising future research topics
based on our survey results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15351">A Survey on Neural Topic Models: Methods, Applications, and Challenges. (arXiv:2401.15351v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xiaobao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Thong Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Luu_A/0/1/0/all/0/1">Anh Tuan Luu</a></p>
<p>Topic models have been prevalent for decades to discover latent topics and
infer topic proportions of documents in an unsupervised fashion. They have been
widely used in various applications like text analysis and context
recommendation. Recently, the rise of neural networks has facilitated the
emergence of a new research field -- Neural Topic Models (NTMs). Different from
conventional topic models, NTMs directly optimize parameters without requiring
model-specific derivations. This endows NTMs with better scalability and
flexibility, resulting in significant research attention and plentiful new
methods and applications. In this paper, we present a comprehensive survey on
neural topic models concerning methods, applications, and challenges.
Specifically, we systematically organize current NTM methods according to their
network structures and introduce the NTMs for various scenarios like short
texts and cross-lingual documents. We also discuss a wide range of popular
applications built on NTMs. Finally, we highlight the challenges confronted by
NTMs to inspire future research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15356">A Statistical Framework for Measuring AI Reliance. (arXiv:2401.15356v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Ziyang Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yifan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hartline_J/0/1/0/all/0/1">Jason Hartline</a>, <a href="http://arxiv.org/find/cs/1/au:+Hullman_J/0/1/0/all/0/1">Jessica Hullman</a></p>
<p>Humans frequently make decisions with the aid of artificially intelligent
(AI) systems. A common pattern is for the AI to recommend an action to the
human who retains control over the final decision. Researchers have identified
ensuring that a human has appropriate reliance on an AI as a critical component
of achieving complementary performance. We argue that the current definition of
appropriate reliance used in such research lacks formal statistical grounding
and can lead to contradictions. We propose a formal definition of reliance,
based on statistical decision theory, which separates the concepts of reliance
as the probability the decision-maker follows the AI's prediction from
challenges a human may face in differentiating the signals and forming accurate
beliefs about the situation. Our definition gives rise to a framework that can
be used to guide the design and interpretation of studies on human-AI
complementarity and reliance. Using recent AI-advised decision making studies
from literature, we demonstrate how our framework can be used to separate the
loss due to mis-reliance from the loss due to not accurately differentiating
the signals. We evaluate these losses by comparing to a baseline and a
benchmark for complementary performance defined by the expected payoff achieved
by a rational agent facing the same decision task as the behavioral agents.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15378">A RAG-based Question Answering System Proposal for Understanding Islam: MufassirQAS LLM. (arXiv:2401.15378v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alan_A/0/1/0/all/0/1">Ahmet Yusuf Alan</a>, <a href="http://arxiv.org/find/cs/1/au:+Karaarslan_E/0/1/0/all/0/1">Enis Karaarslan</a>, <a href="http://arxiv.org/find/cs/1/au:+Aydin_O/0/1/0/all/0/1">Omer Aydin</a></p>
<p>There exist challenges in learning and understanding religions as the
presence of complexity and depth of religious doctrines and teachings. Chatbots
as question-answering systems can help in solving these challenges. LLM
chatbots use NLP techniques to establish connections between topics and
accurately respond to complex questions. These capabilities make it perfect to
be used in enlightenment on religion as a question answering chatbot. However,
LLMs also have a tendency to generate false information, known as
hallucination. The responses of the chatbots can include content that insults
personal religious beliefs, interfaith conflicts, and controversial or
sensitive topics. It needs to avoid such cases without promoting hate speech or
offending certain groups of people or their beliefs. This study uses a vector
database-based Retrieval Augmented Generation (RAG) approach to enhance the
accuracy and transparency of LLMs. Our question-answering system is called as
"MufassirQAS". We created a vector database with several open-access books that
include Turkish context. These are Turkish translations, and interpretations on
Islam. We worked on creating system prompts with care, ensuring they provide
instructions that prevent harmful, offensive, or disrespectful responses. We
also tested the MufassirQAS and ChatGPT with sensitive questions. We got better
performance with our system. Study and enhancements are still in progress.
Results and future works are given.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15390">A microservice architecture for real-time IoT data processing: A reusable Web of things approach for smart ports. (arXiv:2401.15390v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ortiz_G/0/1/0/all/0/1">Guadalupe Ortiz</a>, <a href="http://arxiv.org/find/cs/1/au:+Boubeta_Puig_J/0/1/0/all/0/1">Juan Boubeta-Puig</a>, <a href="http://arxiv.org/find/cs/1/au:+Criado_J/0/1/0/all/0/1">Javier Criado</a>, <a href="http://arxiv.org/find/cs/1/au:+Corral_Plaza_D/0/1/0/all/0/1">David Corral-Plaza</a>, <a href="http://arxiv.org/find/cs/1/au:+Garcia_de_Prado_A/0/1/0/all/0/1">Alfonso Garcia-de-Prado</a>, <a href="http://arxiv.org/find/cs/1/au:+Medina_Bulo_I/0/1/0/all/0/1">Inmaculada Medina-Bulo</a>, <a href="http://arxiv.org/find/cs/1/au:+Iribarne_L/0/1/0/all/0/1">Luis Iribarne</a></p>
<p>Major advances in telecommunications and the Internet of Things have given
rise to numerous smart city scenarios in which smart services are provided.
What was once a dream for the future has now become reality. However, the need
to provide these smart services quickly, efficiently, in an interoperable
manner and in real time is a cutting-edge technological challenge. Although
some software architectures offer solutions in this area, these are often
limited in terms of reusability and maintenance by independent modules,
involving the need for system downtime when maintaining or evolving, as well as
by a lack of standards in terms of the interoperability of their interface. In
this paper, we propose a fully reusable microservice architecture, standardized
through the use of the Web of things paradigm, and with high efficiency in
real-time data processing, supported by complex event processing techniques. To
illustrate the proposal, we present a fully reusable implementation of the
microservices necessary for the deployment of the architecture in the field of
air quality monitoring and alerting in smart ports. The performance evaluation
of this architecture shows excellent results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15443">DiffuserLite: Towards Real-time Diffusion Planning. (arXiv:2401.15443v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1">Zibin Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1">Jianye Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1">Yifu Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ni_F/0/1/0/all/0/1">Fei Ni</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yitian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Pengyi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1">Yan Zheng</a></p>
<p>Diffusion planning has been recognized as an effective decision-making
paradigm in various domains. The high-quality conditional generation capability
of long-horizon trajectories makes it a promising research direction. However,
existing diffusion planning methods suffer from low decision-making frequencies
because of the expensive iterative sampling cost. To address this issue, we
introduce DiffuserLite, a fast and lightweight diffusion planning framework.
DiffuserLite employs a planning refinement process (PRP) to generate
coarse-to-fine-grained trajectories, which significantly reduces the modeling
of redundant information and leads to notable increases in decision-making
frequency. Our experimental results demonstrate that DiffuserLite incurs only
$0.88\%$ of the runtime cost compared to previous frameworks, achieves an
average decision-making frequency of $122$Hz, and reaches state-of-the-art
performance on D4RL benchmarks. In addition, our clean DiffuserLite framework
can serve as a flexible plugin to enhance decision frequency in other diffusion
planning algorithms, providing a structural design reference for future works.
More details and visualizations are available at [project
website](https://diffuserlite.github.io/).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/1611.06189">Query Complexity of Tournament Solutions. (arXiv:1611.06189v4 [cs.DS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maiti_A/0/1/0/all/0/1">Arnab Maiti</a>, <a href="http://arxiv.org/find/cs/1/au:+Dey_P/0/1/0/all/0/1">Palash Dey</a></p>
<p>A directed graph where there is exactly one edge between every pair of
vertices is called a {\em tournament}. Finding the "best" set of vertices of a
tournament is a well studied problem in social choice theory. A {\em tournament
solution} takes a tournament as input and outputs a subset of vertices of the
input tournament. However, in many applications, for example, choosing the best
set of drugs from a given set of drugs, the edges of the tournament are given
only implicitly and knowing the orientation of an edge is costly. In such
scenarios, we would like to know the best set of vertices (according to some
tournament solution) by "querying" as few edges as possible. We, in this paper,
precisely study this problem for commonly used tournament solutions: given an
oracle access to the edges of a tournament T, find $f(T)$ by querying as few
edges as possible, for a tournament solution f. We first show that the set of
Condorcet non-losers in a tournament can be found by querying $2n-\lfloor \log
n \rfloor -2$ edges only and this is tight in the sense that every algorithm
for finding the set of Condorcet non-losers needs to query at least $2n-\lfloor
\log n \rfloor -2$ edges in the worst case, where $n$ is the number of vertices
in the input tournament. We then move on to study other popular tournament
solutions and show that any algorithm for finding the Copeland set, the Slater
set, the Markov set, the bipartisan set, the uncovered set, the Banks set, and
the top cycle must query $\Omega(n^2)$ edges in the worst case. On the positive
side, we are able to circumvent our strong query complexity lower bound results
by proving that, if the size of the top cycle of the input tournament is at
most $k$, then we can find all the tournament solutions mentioned above by
querying $O(nk + \frac{n\log n}{\log(1-\frac{1}{k})})$ edges only.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2009.07497">One head is better than two: a polynomial restriction for propositional definite Horn forgetting. (arXiv:2009.07497v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liberatore_P/0/1/0/all/0/1">Paolo Liberatore</a></p>
<p>Logical forgetting is \np-complete even in the simple case of propositional
Horn formulae, and may exponentially increase their size. A way to forget is to
replace each variable to forget with the body of each clause whose head is the
variable. It takes polynomial time in the single-head case: each variable is at
most the head of a clause. Some formulae are not single-head but can be made so
to simplify forgetting. They are single-head equivalent. The first contribution
of this article is the study of a semantical characterization of single-head
equivalence. Two necessary conditions are given. They are sufficient when the
formula is inequivalent: it makes two sets of variables equivalent only if they
are also equivalent to their intersection. All acyclic formulae are
inequivalent. The second contribution of this article is an incomplete
algorithm for turning a formula single-head. In case of success, forgetting
becomes possible in polynomial time and produces a polynomial-size formula,
none of which is otherwise guaranteed. The algorithm is complete on
inequivalent formulae.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2109.00783">Computer Vision Self-supervised Learning Methods on Time Series. (arXiv:2109.00783v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1">Daesoo Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Aune_E/0/1/0/all/0/1">Erlend Aune</a></p>
<p>Self-supervised learning (SSL) has had great success in both computer vision.
Most of the current mainstream computer vision SSL frameworks are based on
Siamese network architecture. These approaches often rely on cleverly crafted
loss functions and training setups to avoid feature collapse. In this study, we
evaluate if those computer-vision SSL frameworks are also effective on a
different modality (\textit{i.e.,} time series). The effectiveness is
experimented and evaluated on the UCR and UEA archives, and we show that the
computer vision SSL frameworks can be effective even for time series. In
addition, we propose a new method that improves on the recently proposed VICReg
method. Our method improves on a \textit{covariance} term proposed in VICReg,
and in addition we augment the head of the architecture by an iterative
normalization layer that accelerates the convergence of the model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.00999">DeepAutoPIN: An automorphism orbits based deep neural network for characterizing the organizational diversity of protein interactomes across the tree of life. (arXiv:2203.00999v2 [q-bio.MN] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Singh_V/0/1/0/all/0/1">Vikram Singh</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Singh_V/0/1/0/all/0/1">Vikram Singh</a></p>
<p>The enormous diversity of life forms thriving in drastically different
environmental milieus involves a complex interplay among constituent proteins
interacting with each other. However, the organizational principles
characterizing the evolution of protein interaction networks (PINs) across the
tree of life are largely unknown. Here we study 4,738 PINs belonging to 16
phyla to discover phyla-specific architectural features and examine if there
are some evolutionary constraints imposed on the networks' topologies. We
utilized positional information of a network's nodes by normalizing the
frequencies of automorphism orbits appearing in graphlets of sizes 2-5. We
report that orbit usage profiles (OUPs) of networks belonging to the three
domains of life are contrastingly different not only at the domain level but
also at the scale of phyla. Integrating the information related to protein
families, domains, subcellular location, gene ontology, and pathways, our
results indicate that wiring patterns of PINs in different phyla are not
randomly generated rather they are shaped by evolutionary constraints imposed
on them. There exist subtle but substantial variations in the wiring patterns
of PINs that enable OUPs to differentiate among different superfamilies. A deep
neural network was trained on differentially expressed orbits resulting in a
prediction accuracy of 85%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.10469">Formal Contracts Mitigate Social Dilemmas in Multi-Agent RL. (arXiv:2208.10469v4 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Haupt_A/0/1/0/all/0/1">Andreas A. Haupt</a>, <a href="http://arxiv.org/find/cs/1/au:+Christoffersen_P/0/1/0/all/0/1">Phillip J.K. Christoffersen</a>, <a href="http://arxiv.org/find/cs/1/au:+Damani_M/0/1/0/all/0/1">Mehul Damani</a>, <a href="http://arxiv.org/find/cs/1/au:+Hadfield_Menell_D/0/1/0/all/0/1">Dylan Hadfield-Menell</a></p>
<p>Multi-agent Reinforcement Learning (MARL) is a powerful tool for training
autonomous agents acting independently in a common environment. However, it can
lead to sub-optimal behavior when individual incentives and group incentives
diverge. Humans are remarkably capable at solving these social dilemmas. It is
an open problem in MARL to replicate such cooperative behaviors in selfish
agents. In this work, we draw upon the idea of formal contracting from
economics to overcome diverging incentives between agents in MARL. We propose
an augmentation to a Markov game where agents voluntarily agree to binding
transfers of reward, under pre-specified conditions. Our contributions are
theoretical and empirical. First, we show that this augmentation makes all
subgame-perfect equilibria of all Fully Observable Markov Games exhibit
socially optimal behavior, given a sufficiently rich space of contracts. Next,
we show that for general contract spaces, and even under partial observability,
richer contract spaces lead to higher welfare. Hence, contract space design
solves an exploration-exploitation tradeoff, sidestepping incentive issues. We
complement our theoretical analysis with experiments. Issues of exploration in
the contracting augmentation are mitigated using a training methodology
inspired by multi-objective reinforcement learning: Multi-Objective Contract
Augmentation Learning (MOCA). We test our methodology in static, single-move
games, as well as dynamic domains that simulate traffic, pollution management
and common pool resource management.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.03563">SSL-WM: A Black-Box Watermarking Approach for Encoders Pre-trained by Self-supervised Learning. (arXiv:2209.03563v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lv_P/0/1/0/all/0/1">Peizhuo Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Pan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1">Shenchen Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shengzhi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Kai Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_R/0/1/0/all/0/1">Ruigang Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yue_C/0/1/0/all/0/1">Chang Yue</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_F/0/1/0/all/0/1">Fan Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1">Yuling Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1">Hualong Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yingjun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_G/0/1/0/all/0/1">Guozhu Meng</a></p>
<p>Recent years have witnessed tremendous success in Self-Supervised Learning
(SSL), which has been widely utilized to facilitate various downstream tasks in
Computer Vision (CV) and Natural Language Processing (NLP) domains. However,
attackers may steal such SSL models and commercialize them for profit, making
it crucial to verify the ownership of the SSL models. Most existing ownership
protection solutions (e.g., backdoor-based watermarks) are designed for
supervised learning models and cannot be used directly since they require that
the models' downstream tasks and target labels be known and available during
watermark embedding, which is not always possible in the domain of SSL. To
address such a problem, especially when downstream tasks are diverse and
unknown during watermark embedding, we propose a novel black-box watermarking
solution, named SSL-WM, for verifying the ownership of SSL models. SSL-WM maps
watermarked inputs of the protected encoders into an invariant representation
space, which causes any downstream classifier to produce expected behavior,
thus allowing the detection of embedded watermarks. We evaluate SSL-WM on
numerous tasks, such as CV and NLP, using different SSL models both
contrastive-based and generative-based. Experimental results demonstrate that
SSL-WM can effectively verify the ownership of stolen SSL models in various
downstream tasks. Furthermore, SSL-WM is robust against model fine-tuning,
pruning, and input preprocessing attacks. Lastly, SSL-WM can also evade
detection from evaluated watermark detection approaches, demonstrating its
promising application in protecting the ownership of SSL models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.07661">On the Relation between Sensitivity and Accuracy in In-context Learning. (arXiv:2209.07661v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yanda Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1">Chen Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Zhou Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1">Kathleen McKeown</a>, <a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1">He He</a></p>
<p>In-context learning (ICL) suffers from oversensitivity to the prompt, making
it unreliable in real-world scenarios. We study the sensitivity of ICL with
respect to multiple perturbation types. First, we find that label bias obscures
the true sensitivity, and therefore prior work may have significantly
underestimated ICL sensitivity. Second, we observe a strong negative
correlation between ICL sensitivity and accuracy: predictions sensitive to
perturbations are less likely to be correct. Motivated by these findings, we
propose \textsc{SenSel}, a few-shot selective prediction method that abstains
from sensitive predictions. Experiments on ten classification datasets show
that \textsc{SenSel} consistently outperforms two commonly used
confidence-based and entropy-based baselines on abstention decisions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.01751">Proportoids. (arXiv:2210.01751v6 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Antic_C/0/1/0/all/0/1">Christian Anti&#x107;</a></p>
<p>Analogical proportions are expressions of the form ``$a$ is to $b$ what $c$
is to $d$'' at the core of analogical reasoning. This paper contributes to the
mathematical foundations of analogical proportions in the axiomatic tradition
as initiated by Yves Lepage two decades ago. For this we introduce proportoids
as sets endowed with a 4-ary analogical proportion relation $a:b::c:d$
satisfying a suitable set of axioms and study different kinds of
proportion-preserving mappings and relations and their properties.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.14164">No-Box Attacks on 3D Point Cloud Classification. (arXiv:2210.14164v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Naderi_H/0/1/0/all/0/1">Hanieh Naderi</a>, <a href="http://arxiv.org/find/cs/1/au:+Dinesh_C/0/1/0/all/0/1">Chinthaka Dinesh</a>, <a href="http://arxiv.org/find/cs/1/au:+Bajic_I/0/1/0/all/0/1">Ivan V. Bajic</a>, <a href="http://arxiv.org/find/cs/1/au:+Kasaei_S/0/1/0/all/0/1">Shohreh Kasaei</a></p>
<p>Adversarial attacks pose serious challenges for deep neural network
(DNN)-based analysis of various input signals. In the case of 3D point clouds,
methods have been developed to identify points that play a key role in network
decision, and these become crucial in generating existing adversarial attacks.
For example, a saliency map approach is a popular method for identifying
adversarial drop points, whose removal would significantly impact the network
decision. Generally, methods for identifying adversarial points rely on the
access to the DNN model itself to determine which points are critically
important for the model's decision. This paper aims to provide a novel
viewpoint on this problem, where adversarial points can be predicted without
access to the target DNN model, which is referred to as a ``no-box'' attack. To
this end, we define 14 point cloud features and use multiple linear regression
to examine whether these features can be used for adversarial point prediction,
and which combination of features is best suited for this purpose. Experiments
show that a suitable combination of features is able to predict adversarial
points of four different networks -- PointNet, PointNet++, DGCNN, and PointConv
-- significantly better than a random guess and comparable to white-box
attacks. Additionally, we show that no-box attack is transferable to unseen
models. The results also provide further insight into DNNs for point cloud
classification, by showing which features play key roles in their
decision-making process.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.06236">Efficient Deep Reinforcement Learning with Predictive Processing Proximal Policy Optimization. (arXiv:2211.06236v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kucukoglu_B/0/1/0/all/0/1">Burcu K&#xfc;&#xe7;&#xfc;ko&#x11f;lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Borkent_W/0/1/0/all/0/1">Walraaf Borkent</a>, <a href="http://arxiv.org/find/cs/1/au:+Rueckauer_B/0/1/0/all/0/1">Bodo Rueckauer</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmad_N/0/1/0/all/0/1">Nasir Ahmad</a>, <a href="http://arxiv.org/find/cs/1/au:+Guclu_U/0/1/0/all/0/1">Umut G&#xfc;&#xe7;l&#xfc;</a>, <a href="http://arxiv.org/find/cs/1/au:+Gerven_M/0/1/0/all/0/1">Marcel van Gerven</a></p>
<p>Advances in reinforcement learning (RL) often rely on massive compute
resources and remain notoriously sample inefficient. In contrast, the human
brain is able to efficiently learn effective control strategies using limited
resources. This raises the question whether insights from neuroscience can be
used to improve current RL methods. Predictive processing is a popular
theoretical framework which maintains that the human brain is actively seeking
to minimize surprise. We show that recurrent neural networks which predict
their own sensory states can be leveraged to minimise surprise, yielding
substantial gains in cumulative reward. Specifically, we present the Predictive
Processing Proximal Policy Optimization (P4O) agent; an actor-critic
reinforcement learning agent that applies predictive processing to a recurrent
variant of the PPO algorithm by integrating a world model in its hidden state.
Even without hyperparameter tuning, P4O significantly outperforms a baseline
recurrent variant of the PPO algorithm on multiple Atari games using a single
GPU. It also outperforms other state-of-the-art agents given the same
wall-clock time and exceeds human gamer performance on multiple games including
Seaquest, which is a particularly challenging environment in the Atari domain.
Altogether, our work underscores how insights from the field of neuroscience
may support the development of more capable and efficient artificial agents.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.12132">AutoPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning. (arXiv:2301.12132v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Han Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1">Xingchen Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1">Ivan Vuli&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1">Anna Korhonen</a></p>
<p>Large pretrained language models are widely used in downstream NLP tasks via
task-specific fine-tuning, but such procedures can be costly. Recently,
Parameter-Efficient Fine-Tuning (PEFT) methods have achieved strong task
performance while updating much fewer parameters than full model fine-tuning
(FFT). However, it is non-trivial to make informed design choices on the PEFT
configurations, such as their architecture, the number of tunable parameters,
and even the layers in which the PEFT modules are inserted. Consequently, it is
highly likely that the current, manually designed configurations are suboptimal
in terms of their performance-efficiency trade-off. Inspired by advances in
neural architecture search, we propose AutoPEFT for automatic PEFT
configuration selection: we first design an expressive configuration search
space with multiple representative PEFT modules as building blocks. Using
multi-objective Bayesian optimisation in a low-cost setup, we then discover a
Pareto-optimal set of configurations with strong performance-cost trade-offs
across different numbers of parameters that are also highly transferable across
different tasks. Empirically, on GLUE and SuperGLUE tasks, we show that
AutoPEFT-discovered configurations significantly outperform existing PEFT
methods and are on par or better than FFT without incurring substantial
training efficiency costs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.13359">IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing. (arXiv:2301.13359v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1">Guoyang Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jinbao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiaqi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1">Jiayi Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chengjie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1">Feng Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1">Yaochu Jin</a></p>
<p>Image anomaly detection (IAD) is an emerging and vital computer vision task
in industrial manufacturing (IM). Recently, many advanced algorithms have been
reported, but their performance deviates considerably with various IM settings.
We realize that the lack of a uniform IM benchmark is hindering the development
and usage of IAD methods in real-world applications. In addition, it is
difficult for researchers to analyze IAD algorithms without a uniform
benchmark. To solve this problem, we propose a uniform IM benchmark, for the
first time, to assess how well these algorithms perform, which includes various
levels of supervision (unsupervised versus fully supervised), learning
paradigms (few-shot, continual and noisy label), and efficiency (memory usage
and inference speed). Then, we construct a comprehensive image anomaly
detection benchmark (IM-IAD), which includes 19 algorithms on seven major
datasets with a uniform setting. Extensive experiments (17,017 total) on IM-IAD
provide in-depth insights into IAD algorithm redesign or selection. Moreover,
the proposed IM-IAD benchmark challenges existing algorithms and suggests
future research directions. To foster reproducibility and accessibility, the
source code of IM-IAD is uploaded on the website,
https://github.com/M-3LAB/IM-IAD.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.01928">Aligning Robot and Human Representations. (arXiv:2302.01928v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bobu_A/0/1/0/all/0/1">Andreea Bobu</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_A/0/1/0/all/0/1">Andi Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1">Pulkit Agrawal</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_J/0/1/0/all/0/1">Julie Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Dragan_A/0/1/0/all/0/1">Anca D. Dragan</a></p>
<p>To act in the world, robots rely on a representation of salient task aspects:
for example, to carry a coffee mug, a robot may consider movement efficiency or
mug orientation in its behavior. However, if we want robots to act for and with
people, their representations must not be just functional but also reflective
of what humans care about, i.e. they must be aligned. We observe that current
learning approaches suffer from representation misalignment, where the robot's
learned representation does not capture the human's representation. We suggest
that because humans are the ultimate evaluator of robot performance, we must
explicitly focus our efforts on aligning learned representations with humans,
in addition to learning the downstream task. We advocate that current
representation learning approaches in robotics should be studied from the
perspective of how well they accomplish the objective of representation
alignment. We mathematically define the problem, identify its key desiderata,
and situate current methods within this formalism. We conclude by suggesting
future directions for exploring open challenges.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.02759">Detecting Reddit Users with Depression Using a Hybrid Neural Network SBERT-CNN. (arXiv:2302.02759v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Ziyi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1">Ren Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1">Sunyang Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zong_N/0/1/0/all/0/1">Nansu Zong</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hongfang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1">Ming Huang</a></p>
<p>Depression is a widespread mental health issue, affecting an estimated 3.8%
of the global population. It is also one of the main contributors to disability
worldwide. Recently it is becoming popular for individuals to use social media
platforms (e.g., Reddit) to express their difficulties and health issues (e.g.,
depression) and seek support from other users in online communities. It opens
great opportunities to automatically identify social media users with
depression by parsing millions of posts for potential interventions. Deep
learning methods have begun to dominate in the field of machine learning and
natural language processing (NLP) because of their ease of use, efficient
processing, and state-of-the-art results on many NLP tasks. In this work, we
propose a hybrid deep learning model which combines a pretrained sentence BERT
(SBERT) and convolutional neural network (CNN) to detect individuals with
depression with their Reddit posts. The sentence BERT is used to learn the
meaningful representation of semantic information in each post. CNN enables the
further transformation of those embeddings and the temporal identification of
behavioral patterns of users. We trained and evaluated the model performance to
identify Reddit users with depression by utilizing the Self-reported Mental
Health Diagnoses (SMHD) data. The hybrid deep learning model achieved an
accuracy of 0.86 and an F1 score of 0.86 and outperformed the state-of-the-art
documented result (F1 score of 0.79) by other machine learning models in the
literature. The results show the feasibility of the hybrid model to identify
individuals with depression. Although the hybrid model is validated to detect
depression with Reddit posts, it can be easily tuned and applied to other text
classification tasks and different clinical applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.04914">Flexible, Model-Agnostic Method for Materials Data Extraction from Text Using General Purpose Language Models. (arXiv:2302.04914v2 [cond-mat.mtrl-sci] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Polak_M/0/1/0/all/0/1">Maciej P. Polak</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Modi_S/0/1/0/all/0/1">Shrey Modi</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Latosinska_A/0/1/0/all/0/1">Anna Latosinska</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Zhang_J/0/1/0/all/0/1">Jinming Zhang</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Wang_C/0/1/0/all/0/1">Ching-Wen Wang</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Wang_S/0/1/0/all/0/1">Shanonan Wang</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Hazra_A/0/1/0/all/0/1">Ayan Deep Hazra</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Morgan_D/0/1/0/all/0/1">Dane Morgan</a></p>
<p>Accurate and comprehensive material databases extracted from research papers
are critical for materials science and engineering but require significant
human effort to develop. In this paper we present a simple method of extracting
materials data from full texts of research papers suitable for quickly
developing modest-sized databases. The method requires minimal to no coding,
prior knowledge about the extracted property, or model training, and provides
high recall and almost perfect precision in the resultant database. The method
is fully automated except for one human-assisted step, which typically requires
just a few hours of human labor. The method builds on top of natural language
processing and large general language models but can work with almost any such
model. The language models GPT-3/3.5, bart and DeBERTaV3 are evaluated here for
comparison. We provide a detailed detailed analysis of the methods performance
in extracting bulk modulus data, obtaining up to 90% precision at 96% recall,
depending on the amount of human effort involved. We then demonstrate the
methods broader effectiveness by developing a database of critical cooling
rates for metallic glasses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.06582">A Convex Hull Cheapest Insertion Heuristic for the Non-Euclidean TSP. (arXiv:2302.06582v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Goutham_M/0/1/0/all/0/1">Mithun Goutham</a>, <a href="http://arxiv.org/find/cs/1/au:+Menon_M/0/1/0/all/0/1">Meghna Menon</a>, <a href="http://arxiv.org/find/cs/1/au:+Garrow_S/0/1/0/all/0/1">Sarah Garrow</a>, <a href="http://arxiv.org/find/cs/1/au:+Stockar_S/0/1/0/all/0/1">Stephanie Stockar</a></p>
<p>The convex hull cheapest insertion heuristic is known to generate good
solutions to the Traveling Salesperson Problem in Euclidean spaces, but it has
not been extended to the non-Euclidean case. To address the difficulty of
dealing with obstacles in the non-Euclidean space, the proposed adaptation uses
multidimensional scaling to first approximate these points in a Euclidean
space, thereby enabling the generation of the convex hull that initializes the
algorithm. To evaluate the proposed algorithm, the TSPLIB benchmark data-set is
modified by adding impassable separators that produce non-Euclidean spaces. The
algorithm is demonstrated to outperform the commonly used Nearest Neighbor
algorithm in 96% of the cases studied.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.10096">Similarity. (arXiv:2302.10096v5 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Antic_C/0/1/0/all/0/1">Christian Anti&#x107;</a></p>
<p>Detecting and exploiting similarities between seemingly distant objects is
without doubt an important human ability. This paper develops \textit{from the
ground up} an abstract algebraic and qualitative justification-based notion of
similarity based on the observation that sets of generalizations encode
important properties of elements. We show that similarity defined in this way
has appealing mathematical properties. As we construct our notion of similarity
from first principles using only elementary concepts of universal algebra, to
convince the reader of its plausibility, we show that it can be naturally
embedded into first-order logic via model-theoretic types.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.12094">Evaluating explainability for machine learning predictions using model-agnostic metrics. (arXiv:2302.12094v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Munoz_C/0/1/0/all/0/1">Cristian Munoz</a>, <a href="http://arxiv.org/find/cs/1/au:+Costa_K/0/1/0/all/0/1">Kleyton da Costa</a>, <a href="http://arxiv.org/find/cs/1/au:+Modenesi_B/0/1/0/all/0/1">Bernardo Modenesi</a>, <a href="http://arxiv.org/find/cs/1/au:+Koshiyama_A/0/1/0/all/0/1">Adriano Koshiyama</a></p>
<p>Rapid advancements in artificial intelligence (AI) technology have brought
about a plethora of new challenges in terms of governance and regulation. AI
systems are being integrated into various industries and sectors, creating a
demand from decision-makers to possess a comprehensive and nuanced
understanding of the capabilities and limitations of these systems. One
critical aspect of this demand is the ability to explain the results of machine
learning models, which is crucial to promoting transparency and trust in AI
systems, as well as fundamental in helping machine learning models to be
trained ethically. In this paper, we present novel metrics to quantify the
degree of which AI model predictions can be easily explainable by its features.
Our metrics summarize different aspects of explainability into scalars,
providing a more comprehensive understanding of model predictions and
facilitating communication between decision-makers and stakeholders, thereby
increasing the overall transparency and accountability of AI systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.04488">Magnushammer: A Transformer-based Approach to Premise Selection. (arXiv:2303.04488v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mikula_M/0/1/0/all/0/1">Maciej Miku&#x142;a</a>, <a href="http://arxiv.org/find/cs/1/au:+Antoniak_S/0/1/0/all/0/1">Szymon Antoniak</a>, <a href="http://arxiv.org/find/cs/1/au:+Tworkowski_S/0/1/0/all/0/1">Szymon Tworkowski</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_A/0/1/0/all/0/1">Albert Qiaochu Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jin Peng Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Szegedy_C/0/1/0/all/0/1">Christian Szegedy</a>, <a href="http://arxiv.org/find/cs/1/au:+Kucinski_L/0/1/0/all/0/1">&#x141;ukasz Kuci&#x144;ski</a>, <a href="http://arxiv.org/find/cs/1/au:+Milos_P/0/1/0/all/0/1">Piotr Mi&#x142;o&#x15b;</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yuhuai Wu</a></p>
<p>Premise selection is a fundamental problem of automated theorem proving.
Previous works often use intricate symbolic methods, rely on domain knowledge,
and require significant engineering effort to solve this task. In this work, we
show that Magnushammer, a neural transformer-based approach, can outperform
traditional symbolic systems by a large margin. Tested on the PISA benchmark,
Magnushammer achieves $59.5\%$ proof rate compared to a $38.3\%$ proof rate of
Sledgehammer, the most mature and popular symbolic-based solver. Furthermore,
by combining Magnushammer with a neural formal prover based on a language
model, we significantly improve the previous state-of-the-art proof rate from
$57.0\%$ to $71.0\%$.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.09026">Towards Commonsense Knowledge based Fuzzy Systems for Supporting Size-Related Fine-Grained Object Detection. (arXiv:2303.09026v7 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Pu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tianhua Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bin Liu</a></p>
<p>Deep learning has become the dominating approach for object detection. To
achieve accurate fine-grained detection, one needs to employ a large enough
model and a vast amount of data annotations. In this paper, we propose a
commonsense knowledge inference module (CKIM) which leverages commonsense
knowledge to assist a lightweight deep neural network base coarse-grained
object detector to achieve accurate fine-grained detection. Specifically, we
focus on a scenario where a single image contains objects of similar categories
but varying sizes, and we establish a size-related commonsense knowledge
inference module (CKIM) that maps the coarse-grained labels produced by the DL
detector to size-related fine-grained labels. Considering that rule-based
systems are one of the popular methods of knowledge representation and
reasoning, our experiments explored two types of rule-based CKIMs, implemented
using crisp-rule and fuzzy-rule approaches, respectively. Experimental results
demonstrate that compared with baseline methods, our approach achieves accurate
fine-grained detection with a reduced amount of annotated data and smaller
model size. Our code is available at: https://github.com/ZJLAB-AMMI/CKIM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.01295">Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks using Prompt-Tuning. (arXiv:2304.01295v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tu_L/0/1/0/all/0/1">Lifu Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qu_J/0/1/0/all/0/1">Jin Qu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yavuz_S/0/1/0/all/0/1">Semih Yavuz</a>, <a href="http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1">Shafiq Joty</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wenhao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1">Caiming Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yingbo Zhou</a></p>
<p>Cross-lingual transfer of language models trained on high-resource languages
like English has been widely studied for many NLP tasks, but focus on
conversational tasks has been rather limited. This is partly due to the high
cost of obtaining non-English conversational data, which results in limited
coverage. In this work, we introduce XSGD for cross-lingual alignment
pretraining, a parallel and large-scale multilingual conversation dataset that
we created by translating the English-only Schema-Guided Dialogue (SGD) dataset
(Rastogi et al., 2020) into 105 other languages. XSGD contains approximately
330k utterances per language. To facilitate aligned cross-lingual
representations, we develop an efficient prompt-tuning-based method for
learning alignment prompts. We also investigate two different classifiers:
NLI-based and vanilla classifiers, and test cross-lingual capability enabled by
the aligned prompts. We evaluate our model's cross-lingual generalization
capabilities on two conversation tasks: slot-filling and intent classification.
Our results demonstrate the strong and efficient modeling ability of NLI-based
classifiers and the large cross-lingual transfer improvements achieved by our
aligned prompts, particularly in few-shot settings. In addition, we highlight
the nice results of our approach compared to LLMs such as text-davinci-003 and
ChatGPT in both zero-shot and few-shot settings. While LLMs exhibit impressive
performance in English, their cross-lingual capabilities in other languages,
particularly low-resource languages, are limited.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.08767">Masked Language Model Based Textual Adversarial Example Detection. (arXiv:2304.08767v3 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaomei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhaoxi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_Q/0/1/0/all/0/1">Qi Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1">Xufei Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yanjun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1">Shengshan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Leo Yu Zhang</a></p>
<p>Adversarial attacks are a serious threat to the reliable deployment of
machine learning models in safety-critical applications. They can misguide
current models to predict incorrectly by slightly modifying the inputs.
Recently, substantial work has shown that adversarial examples tend to deviate
from the underlying data manifold of normal examples, whereas pre-trained
masked language models can fit the manifold of normal NLP data. To explore how
to use the masked language model in adversarial detection, we propose a novel
textual adversarial example detection method, namely Masked Language
Model-based Detection (MLMD), which can produce clearly distinguishable signals
between normal examples and adversarial examples by exploring the changes in
manifolds induced by the masked language model. MLMD features a plug and play
usage (i.e., no need to retrain the victim model) for adversarial defense and
it is agnostic to classification tasks, victim model's architectures, and
to-be-defended attack methods. We evaluate MLMD on various benchmark textual
datasets, widely studied machine learning models, and state-of-the-art (SOTA)
adversarial attacks (in total $3*4*4 = 48$ settings). Experimental results show
that MLMD can achieve strong performance, with detection accuracy up to 0.984,
0.967, and 0.901 on AG-NEWS, IMDB, and SST-2 datasets, respectively.
Additionally, MLMD is superior, or at least comparable to, the SOTA detection
defenses in detection accuracy and F1 score. Among many defenses based on the
off-manifold assumption of adversarial examples, this work offers a new angle
for capturing the manifold change. The code for this work is openly accessible
at \url{https://github.com/mlmddetection/MLMDdetection}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.09452">A sequential transit network design algorithm with optimal learning under correlated beliefs. (arXiv:2305.09452v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yoon_G/0/1/0/all/0/1">Gyugeun Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Chow_J/0/1/0/all/0/1">Joseph Y. J. Chow</a></p>
<p>Mobility service route design requires demand information to operate in a
service region. Transit planners and operators can access various data sources
including household travel survey data and mobile device location logs.
However, when implementing a mobility system with emerging technologies,
estimating demand becomes harder because of limited data resulting in
uncertainty. This study proposes an artificial intelligence-driven algorithm
that combines sequential transit network design with optimal learning to
address the operation under limited data. An operator gradually expands its
route system to avoid risks from inconsistency between designed routes and
actual travel demand. At the same time, observed information is archived to
update the knowledge that the operator currently uses. Three learning policies
are compared within the algorithm: multi-armed bandit, knowledge gradient, and
knowledge gradient with correlated beliefs. For validation, a new route system
is designed on an artificial network based on public use microdata areas in New
York City. Prior knowledge is reproduced from the regional household travel
survey data. The results suggest that exploration considering correlations can
achieve better performance compared to greedy choices in general. In future
work, the problem may incorporate more complexities such as demand elasticity
to travel time, no limitations to the number of transfers, and costs for
expansion.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.10163">Large Language Models Leverage External Knowledge to Extend Clinical Insight Beyond Language Boundaries. (arXiv:2305.10163v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jiageng Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xian Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1">Zhaopeng Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Minghui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yingying Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1">Yefeng Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jie Yang</a></p>
<p>Objectives: Large Language Models (LLMs) such as ChatGPT and Med-PaLM have
excelled in various medical question-answering tasks. However, these
English-centric models encounter challenges in non-English clinical settings,
primarily due to limited clinical knowledge in respective languages, a
consequence of imbalanced training corpora. We systematically evaluate LLMs in
the Chinese medical context and develop a novel in-context learning framework
to enhance their performance.
</p>
<p>Materials and Methods: The latest China National Medical Licensing
Examination (CNMLE-2022) served as the benchmark. We collected 53 medical books
and 381,149 medical questions to construct the medical knowledge base and
question bank. The proposed Knowledge and Few-shot Enhancement In-context
Learning (KFE) framework leverages the in-context learning ability of LLMs to
integrate diverse external clinical knowledge sources. We evaluated KFE with
ChatGPT(GPT3.5), GPT4, Baichuan2-7b, and Baichuan2-13B in CNMLE-2022 and
further investigated the effectiveness of different pathways for incorporating
LLMs with medical knowledge from seven distinct perspectives.
</p>
<p>Results: Directly applying ChatGPT failed to qualify for the CNMLE-2022 at a
score of 51. Cooperated with the KFE framework, the LLMs with varying sizes
yielded consistent and significant improvements. The ChatGPT's performance
surged to 70.04 and GPT-4 achieved the highest score of 82.59. This surpasses
the qualification threshold (60) and exceeds the average human score of 68.70,
affirming the effectiveness and robustness of the framework. It also enabled a
smaller Baichuan2-13B to pass the examination, showcasing the great potential
in low-resource settings. This study shed light on the optimal practices to
enhance the capabilities of LLMs in non-English medical scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.12971">Neural Cellular Automata Can Respond to Signals. (arXiv:2305.12971v2 [cs.NE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stovold_J/0/1/0/all/0/1">James Stovold</a></p>
<p>Neural Cellular Automata (NCAs) are a model of morphogenesis, capable of
growing two-dimensional artificial organisms from a single seed cell. In this
paper, we show that NCAs can be trained to respond to signals. Two types of
signal are used: internal (genomically-coded) signals, and external
(environmental) signals. Signals are presented to a single pixel for a single
timestep.
</p>
<p>Results show NCAs are able to grow into multiple distinct forms based on
internal signals, and are able to change colour based on external signals.
Overall these contribute to the development of NCAs as a model of artificial
morphogenesis, and pave the way for future developments embedding dynamic
behaviour into the NCA model.
</p>
<p>Code and target images are available through GitHub:
https://github.com/jstovold/ALIFE2023
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.11886">SPRINT: Scalable Policy Pre-Training via Language Instruction Relabeling. (arXiv:2306.11886v3 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jesse Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pertsch_K/0/1/0/all/0/1">Karl Pertsch</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiahui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1">Joseph J. Lim</a></p>
<p>Pre-training robot policies with a rich set of skills can substantially
accelerate the learning of downstream tasks. Prior works have defined
pre-training tasks via natural language instructions, but doing so requires
tedious human annotation of hundreds of thousands of instructions. Thus, we
propose SPRINT, a scalable offline policy pre-training approach which
substantially reduces the human effort needed for pre-training a diverse set of
skills. Our method uses two core ideas to automatically expand a base set of
pre-training tasks: instruction relabeling via large language models and
cross-trajectory skill chaining through offline reinforcement learning. As a
result, SPRINT pre-training equips robots with a much richer repertoire of
skills. Experimental results in a household simulator and on a real robot
kitchen manipulation task show that SPRINT leads to substantially faster
learning of new long-horizon tasks than previous pre-training approaches.
Website at https://clvrai.com/sprint.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.15749">To Spike or Not To Spike: A Digital Hardware Perspective on Deep Learning Acceleration. (arXiv:2306.15749v5 [cs.NE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ottati_F/0/1/0/all/0/1">Fabrizio Ottati</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1">Chang Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Qinyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Brignone_G/0/1/0/all/0/1">Giovanni Brignone</a>, <a href="http://arxiv.org/find/cs/1/au:+Casu_M/0/1/0/all/0/1">Mario R. Casu</a>, <a href="http://arxiv.org/find/cs/1/au:+Eshraghian_J/0/1/0/all/0/1">Jason K. Eshraghian</a>, <a href="http://arxiv.org/find/cs/1/au:+Lavagno_L/0/1/0/all/0/1">Luciano Lavagno</a></p>
<p>As deep learning models scale, they become increasingly competitive from
domains spanning from computer vision to natural language processing; however,
this happens at the expense of efficiency since they require increasingly more
memory and computing power. The power efficiency of the biological brain
outperforms any large-scale deep learning ( DL ) model; thus, neuromorphic
computing tries to mimic the brain operations, such as spike-based information
processing, to improve the efficiency of DL models. Despite the benefits of the
brain, such as efficient information transmission, dense neuronal
interconnects, and the co-location of computation and memory, the available
biological substrate has severely constrained the evolution of biological
brains. Electronic hardware does not have the same constraints; therefore,
while modeling spiking neural networks ( SNNs) might uncover one piece of the
puzzle, the design of efficient hardware backends for SNN s needs further
investigation, potentially taking inspiration from the available work done on
the artificial neural networks ( ANNs) side. As such, when is it wise to look
at the brain while designing new hardware, and when should it be ignored? To
answer this question, we quantitatively compare the digital hardware
acceleration techniques and platforms of ANNs and SNN s. As a result, we
provide the following insights: (i) ANNs currently process static data more
efficiently, (ii) applications targeting data produced by neuromorphic sensors,
such as event-based cameras and silicon cochleas, need more investigation since
the behavior of these sensors might naturally fit the SNN paradigm, and (iii)
hybrid approaches combining SNN s and ANNs might lead to the best solutions and
should be investigated further at the hardware level, accounting for both
efficiency and loss optimization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.16048">Benchmarking Zero-Shot Recognition with Vision-Language Models: Challenges on Granularity and Specificity. (arXiv:2306.16048v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zhenlin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yi Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_T/0/1/0/all/0/1">Tiffany Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1">Abhay Mittal</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yanbei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Manchen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Favaro_P/0/1/0/all/0/1">Paolo Favaro</a>, <a href="http://arxiv.org/find/cs/1/au:+Tighe_J/0/1/0/all/0/1">Joseph Tighe</a>, <a href="http://arxiv.org/find/cs/1/au:+Modolo_D/0/1/0/all/0/1">Davide Modolo</a></p>
<p>This paper introduces innovative benchmarks to evaluate Vision-Language
Models (VLMs) in real-world zero-shot recognition tasks, focusing on the
granularity and specificity of prompting text. We propose a unique evaluation
protocol using adapted ImageNet and MS-COCO datasets to assess models'
consistency in recognizing concepts at varying granularity levels and their
sensitivity to the specificity of language inputs. Our extensive evaluation
reveals that state-of-the-art VLMs, including contrastive models like CLIP,
struggle with granularity and are sensitive to text specificity, impacting
their effectiveness in open-world settings. This comprehensive study, a first
in evaluating VLMs from these perspectives, provides valuable insights and
tools for the community, highlighting the limitations and paving the way for
enhanced models with better generalization in zero-shot recognition.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.00012">FlakyFix: Using Large Language Models for Predicting Flaky Test Fix Categories and Test Code Repair. (arXiv:2307.00012v2 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fatima_S/0/1/0/all/0/1">Sakina Fatima</a>, <a href="http://arxiv.org/find/cs/1/au:+Hemmati_H/0/1/0/all/0/1">Hadi Hemmati</a>, <a href="http://arxiv.org/find/cs/1/au:+Briand_L/0/1/0/all/0/1">Lionel Briand</a></p>
<p>Flaky tests are problematic because they non-deterministically pass or fail
for the same software version under test, causing confusion and wasting
development effort. While machine learning models have been used to predict
flakiness and its root causes, there is much less work on providing support to
fix the problem. To address this gap, in this paper, we focus on predicting the
type of fix that is required to remove flakiness and then repair the test code
on that basis. We do this for a subset of flaky test cases where the root cause
of flakiness is in the test case itself and not in the production code. Our key
idea is to guide the repair process with additional knowledge about the test's
flakiness in the form of its predicted fix category. Thus, we first propose a
framework that automatically generates labeled datasets for 13 fix categories
and trains models to predict the fix category of a flaky test by analyzing the
test code only. Our experimental results using code models and few-shot
learning show that we can correctly predict most of the fix categories. To show
the usefulness of such fix category labels for automatically repairing
flakiness, in addition to informing testers, we augment a Large Language Model
(LLM) like GPT with such extra knowledge to ask the LLM for repair suggestions.
The results show that our suggested fix category labels significantly enhance
the capability of GPT 3.5 Turbo, in generating fixes for flaky tests.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08962">REX: Rapid Exploration and eXploitation for AI Agents. (arXiv:2307.08962v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Murthy_R/0/1/0/all/0/1">Rithesh Murthy</a>, <a href="http://arxiv.org/find/cs/1/au:+Heinecke_S/0/1/0/all/0/1">Shelby Heinecke</a>, <a href="http://arxiv.org/find/cs/1/au:+Niebles_J/0/1/0/all/0/1">Juan Carlos Niebles</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhiwei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1">Le Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1">Weiran Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yihao Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zeyuan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gokul_A/0/1/0/all/0/1">Akash Gokul</a>, <a href="http://arxiv.org/find/cs/1/au:+Arpit_D/0/1/0/all/0/1">Devansh Arpit</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">Ran Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mui_P/0/1/0/all/0/1">Phil Mui</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Huan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1">Caiming Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1">Silvio Savarese</a></p>
<p>In this paper, we propose an enhanced approach for Rapid Exploration and
eXploitation for AI Agents called REX. Existing AutoGPT-style techniques have
inherent limitations, such as a heavy reliance on precise descriptions for
decision-making, and the lack of a systematic approach to leverage try-and-fail
procedures akin to traditional Reinforcement Learning (RL). REX introduces an
additional layer of rewards and integrates concepts similar to Upper Confidence
Bound (UCB) scores, leading to more robust and efficient AI agent performance.
This approach has the advantage of enabling the utilization of offline
behaviors from logs and allowing seamless integration with existing foundation
models while it does not require any model fine-tuning. Through comparative
analysis with existing methods such as Chain-of-Thoughts(CoT) and Reasoning viA
Planning(RAP), REX-based methods demonstrate comparable performance and, in
certain cases, even surpass the results achieved by these existing techniques.
Notably, REX-based methods exhibit remarkable reductions in execution time,
enhancing their practical applicability across a diverse set of scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.16348">Rating-based Reinforcement Learning. (arXiv:2307.16348v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+White_D/0/1/0/all/0/1">Devin White</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1">Mingkang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Novoseller_E/0/1/0/all/0/1">Ellen Novoseller</a>, <a href="http://arxiv.org/find/cs/1/au:+Lawhern_V/0/1/0/all/0/1">Vernon J. Lawhern</a>, <a href="http://arxiv.org/find/cs/1/au:+Waytowich_N/0/1/0/all/0/1">Nicholas Waytowich</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yongcan Cao</a></p>
<p>This paper develops a novel rating-based reinforcement learning approach that
uses human ratings to obtain human guidance in reinforcement learning.
Different from the existing preference-based and ranking-based reinforcement
learning paradigms, based on human relative preferences over sample pairs, the
proposed rating-based reinforcement learning approach is based on human
evaluation of individual trajectories without relative comparisons between
sample pairs. The rating-based reinforcement learning approach builds on a new
prediction model for human ratings and a novel multi-class loss function. We
conduct several experimental studies based on synthetic ratings and real human
ratings to evaluate the effectiveness and benefits of the new rating-based
reinforcement learning approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04586">Bootstrapping Developmental AIs: From Simple Competences to Intelligent Human-Compatible AIs. (arXiv:2308.04586v18 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stefik_M/0/1/0/all/0/1">Mark Stefik</a>, <a href="http://arxiv.org/find/cs/1/au:+Price_R/0/1/0/all/0/1">Robert Price</a></p>
<p>Developmental AI is a bootstrapping approach where embodied AIs start with
innate competences and learn by interacting with the world. They develop
abilities in small steps along a bio-inspired trajectory. However,
developmental AIs have not yet reached the abilities of young children. In
contrast, mainstream approaches for creating AIs have led to valuable AI
systems and impressive feats. These approaches include deep learning and
generative approaches (e.g., large language models) and manually constructed
symbolic approaches. Manually constructed AIs are brittle even in circumscribed
domains. Generative AIs can make strange mistakes and not notice them. Taken
together, mainstream approaches lack common sense, curiosity, and social
alignment. This position paper lays out prospects, gaps, and challenges for
augmenting the AI mainstream approaches with developmental AI. The ambition is
to create data-rich experientially based foundation models for
human-compatible, resilient, and trustworthy AIs. This research aims to produce
developmental AIs that learn to communicate, establish common ground, read
critically, consider the provenance of information, test hypotheses, and
collaborate. A virtuous multidisciplinary research cycle has led to
developmental AIs with capabilities for multimodal perception, object
recognition, and manipulation. Computational models for hierarchical planning,
abstraction discovery, curiosity, and language acquisition exist but need to be
adapted to an embodied learning approach. They need to bridge competence gaps
involving nonverbal communication, speech, reading, and writing.
Aspirationally, developmental AIs would learn, share what they learn, and
collaborate to achieve high standards. The approach would make the creation of
AIs more democratic, enabling more people to train, test, build on, and
replicate AIs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.08334">Learning logic programs by discovering higher-order abstractions. (arXiv:2308.08334v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hocquette_C/0/1/0/all/0/1">C&#xe9;line Hocquette</a>, <a href="http://arxiv.org/find/cs/1/au:+Dumancic_S/0/1/0/all/0/1">Sebastijan Duman&#x10d;i&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Cropper_A/0/1/0/all/0/1">Andrew Cropper</a></p>
<p>We introduce the higher-order refactoring problem, where the goal is to
compress a logic program by discovering higher-order abstractions, such as map,
filter, and fold. We implement our approach in Stevie, which formulates the
refactoring problem as a constraint optimisation problem. Our experiments on
multiple domains, including program synthesis and visual reasoning, show that
refactoring can improve the learning performance of an inductive logic
programming system, specifically improving predictive accuracies by 27% and
reducing learning times by 47%. We also show that Stevie can discover
abstractions that transfer to multiple domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.10335">Can ChatGPT replace StackOverflow? A Study on Robustness and Reliability of Large Language Model Code Generation. (arXiv:2308.10335v5 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1">Li Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zilong Wang</a></p>
<p>Recently, the large language models (LLMs) have shown extraordinary ability
in understanding natural language and generating programming code. It has been
a common practice of software engineers to consult LLMs when encountering
coding questions. Although efforts have been made to avoid syntax errors and
align the code with the intended semantics, the reliability and robustness of
the code generationfrom LLMs have not yet been thoroughly studied. The
executable code is not equivalent to the reliable and robust code, especially
in the context of real-world software development. The misuse of APIs in the
generated code could lead to severe problem, such as resource leaks, program
crashes. To make things worse, the users of LLM code generation services are
actually the developers that are most vulnerable to these code that seems right
-- They are always novice developers that are not familiar with the APIs that
LLMs generate code for them. Therefore, they could hardly tell the misuse in
the code generated by LLMs, which further facilitates the incorrect code
applied in real-world software. Existing code evaluation benchmark and datasets
focus on crafting small tasks such as programming questions in coding
interviews, which however deviates from the problem that developers would ask
LLM for real-world coding help. To fill the missing piece, in this work, we
propose a dataset RobustAPI for evaluating the reliability and robustness of
code generated by LLMs. We collect 1208 coding questions from StackOverflow on
24 representative Java APIs. We summarize thecommon misuse patterns of these
APIs and evaluate them oncurrent popular LLMs. The evaluation results show that
evenfor GPT-4, 62% of the generated code contains API misuses,which would cause
unexpected consequences if the code isintroduced into real-world software.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.13420">Reinforcement Learning-assisted Evolutionary Algorithm: A Survey and Research Opportunities. (arXiv:2308.13420v3 [cs.NE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Yanjie Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yutong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yangyang Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1">Ran Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Suganthan_P/0/1/0/all/0/1">P. N. Suganthan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yue Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pedrycz_W/0/1/0/all/0/1">Witold Pedrycz</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1">Swagatam Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Mallipeddi_R/0/1/0/all/0/1">Rammohan Mallipeddi</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_O/0/1/0/all/0/1">Oladayo Solomon Ajani. Qiang Feng</a></p>
<p>Evolutionary algorithms (EA), a class of stochastic search methods based on
the principles of natural evolution, have received widespread acclaim for their
exceptional performance in various real-world optimization problems. While
researchers worldwide have proposed a wide variety of EAs, certain limitations
remain, such as slow convergence speed and poor generalization capabilities.
Consequently, numerous scholars actively explore improvements to algorithmic
structures, operators, search patterns, etc., to enhance their optimization
performance. Reinforcement learning (RL) integrated as a component in the EA
framework has demonstrated superior performance in recent years. This paper
presents a comprehensive survey on integrating reinforcement learning into the
evolutionary algorithm, referred to as reinforcement learning-assisted
evolutionary algorithm (RL-EA). We begin with the conceptual outlines of
reinforcement learning and the evolutionary algorithm. We then provide a
taxonomy of RL-EA. Subsequently, we discuss the RL-EA integration method, the
RL-assisted strategy adopted by RL-EA, and its applications according to the
existing literature. The RL-assisted procedure is divided according to the
implemented functions including solution generation, learnable objective
function, algorithm/operator/sub-population selection, parameter adaptation,
and other strategies. Additionally, different attribute settings of RL in RL-EA
are discussed. In the applications of RL-EA section, we also demonstrate the
excellent performance of RL-EA on several benchmarks and a range of public
datasets to facilitate a quick comparative study. Finally, we analyze potential
directions for future research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.04174">Manifold-based Verbalizer Space Re-embedding for Tuning-free Prompt-based Classification. (arXiv:2309.04174v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haochun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1">Sendong Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xi_N/0/1/0/all/0/1">Nuwa Xi</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_M/0/1/0/all/0/1">Muzhen Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1">Bing Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Ting Liu</a></p>
<p>Prompt-based classification adapts tasks to a cloze question format utilizing
the [MASK] token and the filled tokens are then mapped to labels through
pre-defined verbalizers. Recent studies have explored the use of verbalizer
embeddings to reduce labor in this process. However, all existing studies
require a tuning process for either the pre-trained models or additional
trainable embeddings. Meanwhile, the distance between high-dimensional
verbalizer embeddings should not be measured by Euclidean distance due to the
potential for non-linear manifolds in the representation space. In this study,
we propose a tuning-free manifold-based space re-embedding method called
Locally Linear Embedding with Intra-class Neighborhood Constraint (LLE-INC) for
verbalizer embeddings, which preserves local properties within the same class
as guidance for classification. Experimental results indicate that even without
tuning any parameters, our LLE-INC is on par with automated verbalizers with
parameter tuning. And with the parameter updating, our approach further
enhances prompt-based tuning by up to 3.2%. Furthermore, experiments with the
LLaMA-7B&amp;13B indicate that LLE-INC is an efficient tuning-free classification
approach for the hyper-scale language models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.13043">E(2)-Equivariant Graph Planning for Navigation. (arXiv:2309.13043v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1">Linfeng Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hongyu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Padir_T/0/1/0/all/0/1">Taskin Padir</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1">Huaizu Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_L/0/1/0/all/0/1">Lawson L.S. Wong</a></p>
<p>Learning for robot navigation presents a critical and challenging task. The
scarcity and costliness of real-world datasets necessitate efficient learning
approaches. In this letter, we exploit Euclidean symmetry in planning for 2D
navigation, which originates from Euclidean transformations between reference
frames and enables parameter sharing. To address the challenges of unstructured
environments, we formulate the navigation problem as planning on a geometric
graph and develop an equivariant message passing network to perform value
iteration. Furthermore, to handle multi-camera input, we propose a learnable
equivariant layer to lift features to a desired space. We conduct comprehensive
evaluations across five diverse tasks encompassing structured and unstructured
environments, along with maps of known and unknown, given point goals or
semantic goals. Our experiments confirm the substantial benefits on training
efficiency, stability, and generalization. More details can be found at the
project website: https://lhy.xyz/e2-planning/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.13340">Towards LLM-guided Causal Explainability for Black-box Text Classifiers. (arXiv:2309.13340v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bhattacharjee_A/0/1/0/all/0/1">Amrita Bhattacharjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Moraffah_R/0/1/0/all/0/1">Raha Moraffah</a>, <a href="http://arxiv.org/find/cs/1/au:+Garland_J/0/1/0/all/0/1">Joshua Garland</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Huan Liu</a></p>
<p>With the advent of larger and more complex deep learning models, such as in
Natural Language Processing (NLP), model qualities like explainability and
interpretability, albeit highly desirable, are becoming harder challenges to
tackle and solve. For example, state-of-the-art models in text classification
are black-box by design. Although standard explanation methods provide some
degree of explainability, these are mostly correlation-based methods and do not
provide much insight into the model. The alternative of causal explainability
is more desirable to achieve but extremely challenging in NLP due to a variety
of reasons. Inspired by recent endeavors to utilize Large Language Models
(LLMs) as experts, in this work, we aim to leverage the instruction-following
and textual understanding capabilities of recent state-of-the-art LLMs to
facilitate causal explainability via counterfactual explanation generation for
black-box text classifiers. To do this, we propose a three-step pipeline via
which, we use an off-the-shelf LLM to: (1) identify the latent or unobserved
features in the input text, (2) identify the input features associated with the
latent features, and finally (3) use the identified input features to generate
a counterfactual explanation. We experiment with our pipeline on multiple NLP
text classification datasets, with several recent LLMs, and present interesting
and promising findings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.13500">Enhancing Student Performance Prediction on Learnersourced Questions with SGNN-LLM Synergy. (arXiv:2309.13500v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ni_L/0/1/0/all/0/1">Lin Ni</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Sijie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zeyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaoxuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1">Xianda Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Denny_P/0/1/0/all/0/1">Paul Denny</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiamou Liu</a></p>
<p>Learnersourcing offers great potential for scalable education through student
content creation. However, predicting student performance on learnersourced
questions, which is essential for personalizing the learning experience, is
challenging due to the inherent noise in student-generated data. Moreover,
while conventional graph-based methods can capture the complex network of
student and question interactions, they often fall short under cold start
conditions where limited student engagement with questions yields sparse data.
To address both challenges, we introduce an innovative strategy that synergizes
the potential of integrating Signed Graph Neural Networks (SGNNs) and Large
Language Model (LLM) embeddings. Our methodology employs a signed bipartite
graph to comprehensively model student answers, complemented by a contrastive
learning framework that enhances noise resilience. Furthermore, LLM's
contribution lies in generating foundational question embeddings, proving
especially advantageous in addressing cold start scenarios characterized by
limited graph data. Validation across five real-world datasets sourced from the
PeerWise platform underscores our approach's effectiveness. Our method
outperforms baselines, showcasing enhanced predictive accuracy and robustness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.14053">Revisiting LARS for Large Batch Training Generalization of Neural Networks. (arXiv:2309.14053v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Do_K/0/1/0/all/0/1">Khoi Do</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1">Duong Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1">Hoa Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_Thanh_L/0/1/0/all/0/1">Long Tran-Thanh</a>, <a href="http://arxiv.org/find/cs/1/au:+Pham_Q/0/1/0/all/0/1">Quoc-Viet Pham</a></p>
<p>This paper explores Large Batch Training techniques using layer-wise adaptive
scaling ratio (LARS) across diverse settings, uncovering insights. LARS
algorithms with warm-up tend to be trapped in sharp minimizers early on due to
redundant ratio scaling. Additionally, a fixed steep decline in the latter
phase restricts deep neural networks from effectively navigating early-phase
sharp minimizers. Building on these findings, we propose Time Varying LARS
(TVLARS), a novel algorithm that replaces warm-up with a configurable
sigmoid-like function for robust training in the initial phase. TVLARS promotes
gradient exploration early on, surpassing sharp optimizers and gradually
transitioning to LARS for robustness in later phases. Extensive experiments
demonstrate that TVLARS consistently outperforms LARS and LAMB in most cases,
with up to 2\% improvement in classification scenarios. Notably, in all
self-supervised learning cases, TVLARS dominates LARS and LAMB with performance
improvements of up to 10\%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.15560">Identifiability Matters: Revealing the Hidden Recoverable Condition in Unbiased Learning to Rank. (arXiv:2309.15560v2 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Mouxiang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chenghao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zemin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhuo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jianling Sun</a></p>
<p>Unbiased Learning to Rank (ULTR) aims to train unbiased ranking models from
biased click logs, by explicitly modeling a generation process for user
behavior and fitting click data based on examination hypothesis. Previous
research found empirically that the true latent relevance is mostly recoverable
through perfect click fitting. However, we demonstrate that this is not always
achievable, resulting in a significant reduction in ranking performance. This
research investigates the conditions under which relevance can be recovered
from click data at a foundational level. We initially characterize a ranking
model as identifiable if it can recover the true relevance up to a scaling
transformation, a criterion sufficient for the pairwise ranking objective.
Subsequently, we investigate an equivalent condition for identifiability,
articulated as a graph connectivity test problem: the recovery of relevance is
feasible if and only if the identifiability graph (IG), derived from the
underlying structure of the dataset, is connected. The presence of a
disconnected IG may lead to degenerate cases and suboptimal ranking
performance. To tackle this challenge, we introduce two methods, namely node
intervention and node merging, designed to modify the dataset and restore the
connectivity of the IG. Empirical results derived from a simulated dataset and
two real-world LTR benchmark datasets not only validate our proposed theorems
but also demonstrate the effectiveness of our methods in alleviating data bias
when the relevance model is unidentifiable.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16742">Supervised Learning Models for Early Detection of Albuminuria Risk in Type-2 Diabetes Mellitus Patients. (arXiv:2309.16742v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Muharram_A/0/1/0/all/0/1">Arief Purnama Muharram</a>, <a href="http://arxiv.org/find/cs/1/au:+Tahapary_D/0/1/0/all/0/1">Dicky Levenus Tahapary</a>, <a href="http://arxiv.org/find/cs/1/au:+Lestari_Y/0/1/0/all/0/1">Yeni Dwi Lestari</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarayar_R/0/1/0/all/0/1">Randy Sarayar</a>, <a href="http://arxiv.org/find/cs/1/au:+Dirjayanto_V/0/1/0/all/0/1">Valerie Josephine Dirjayanto</a></p>
<p>Diabetes, especially T2DM, continues to be a significant health problem. One
of the major concerns associated with diabetes is the development of its
complications. Diabetic nephropathy, one of the chronic complication of
diabetes, adversely affects the kidneys, leading to kidney damage. Diagnosing
diabetic nephropathy involves considering various criteria, one of which is the
presence of a pathologically significant quantity of albumin in urine, known as
albuminuria. Thus, early prediction of albuminuria in diabetic patients holds
the potential for timely preventive measures. This study aimed to develop a
supervised learning model to predict the risk of developing albuminuria in T2DM
patients. The selected supervised learning algorithms included Na\"ive Bayes,
Support Vector Machine (SVM), decision tree, random forest, AdaBoost, XGBoost,
and Multi-Layer Perceptron (MLP). Our private dataset, comprising 184 entries
of diabetes complications risk factors, was used to train the algorithms. It
consisted of 10 attributes as features and 1 attribute as the target
(albuminuria). Upon conducting the experiments, the MLP demonstrated superior
performance compared to the other algorithms. It achieved accuracy and f1-score
values as high as 0.74 and 0.75, respectively, making it suitable for screening
purposes in predicting albuminuria in T2DM. Nonetheless, further studies are
warranted to enhance the model's performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00863">Syllable-level lyrics generation from melody exploiting character-level language model. (arXiv:2310.00863v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhe Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lasocki_K/0/1/0/all/0/1">Karol Lasocki</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yi Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Takasu_A/0/1/0/all/0/1">Atsuhiro Takasu</a></p>
<p>The generation of lyrics tightly connected to accompanying melodies involves
establishing a mapping between musical notes and syllables of lyrics. This
process requires a deep understanding of music constraints and semantic
patterns at syllable-level, word-level, and sentence-level semantic meanings.
However, pre-trained language models specifically designed at the syllable
level are publicly unavailable. To solve these challenging issues, we propose
to exploit fine-tuning character-level language models for syllable-level
lyrics generation from symbolic melody. In particular, our method endeavors to
incorporate linguistic knowledge of the language model into the beam search
process of a syllable-level Transformer generator network. Additionally, by
exploring ChatGPT-based evaluation for generated lyrics, along with human
subjective evaluation, we demonstrate that our approach enhances the coherence
and correctness of the generated lyrics, eliminating the need to train
expensive new language models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01728">Time-LLM: Time Series Forecasting by Reprogramming Large Language Models. (arXiv:2310.01728v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1">Ming Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shiyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1">Lintao Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1">Zhixuan Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">James Y. Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1">Xiaoming Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1">Pin-Yu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1">Yuxuan Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuan-Fang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1">Shirui Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_Q/0/1/0/all/0/1">Qingsong Wen</a></p>
<p>Time series forecasting holds significant importance in many real-world
dynamic systems and has been extensively studied. Unlike natural language
process (NLP) and computer vision (CV), where a single large model can tackle
multiple tasks, models for time series forecasting are often specialized,
necessitating distinct designs for different tasks and applications. While
pre-trained foundation models have made impressive strides in NLP and CV, their
development in time series domains has been constrained by data sparsity.
Recent studies have revealed that large language models (LLMs) possess robust
pattern recognition and reasoning abilities over complex sequences of tokens.
However, the challenge remains in effectively aligning the modalities of time
series data and natural language to leverage these capabilities. In this work,
we present Time-LLM, a reprogramming framework to repurpose LLMs for general
time series forecasting with the backbone language models kept intact. We begin
by reprogramming the input time series with text prototypes before feeding it
into the frozen LLM to align the two modalities. To augment the LLM's ability
to reason with time series data, we propose Prompt-as-Prefix (PaP), which
enriches the input context and directs the transformation of reprogrammed input
patches. The transformed time series patches from the LLM are finally projected
to obtain the forecasts. Our comprehensive evaluations demonstrate that
Time-LLM is a powerful time series learner that outperforms state-of-the-art,
specialized forecasting models. Moreover, Time-LLM excels in both few-shot and
zero-shot learning scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02161">Selenite: Scaffolding Online Sensemaking with Comprehensive Overviews Elicited from Large Language Models. (arXiv:2310.02161v4 [cs.HC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Michael Xieyang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1">Tongshuang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tianying Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1">Franklin Mingzhe Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Kittur_A/0/1/0/all/0/1">Aniket Kittur</a>, <a href="http://arxiv.org/find/cs/1/au:+Myers_B/0/1/0/all/0/1">Brad A. Myers</a></p>
<p>Sensemaking in unfamiliar domains can be challenging, demanding considerable
user effort to compare different options with respect to various criteria.
Prior research and our formative study found that people would benefit from
reading an overview of an information space upfront, including the criteria
others previously found useful. However, existing sensemaking tools struggle
with the "cold-start" problem -- it not only requires significant input from
previous users to generate and share these overviews, but such overviews may
also turn out to be biased and incomplete. In this work, we introduce a novel
system, Selenite, which leverages Large Language Models (LLMs) as reasoning
machines and knowledge retrievers to automatically produce a comprehensive
overview of options and criteria to jumpstart users' sensemaking processes.
Subsequently, Selenite also adapts as people use it, helping users find, read,
and navigate unfamiliar information in a systematic yet personalized manner.
Through three studies, we found that Selenite produced accurate and
high-quality overviews reliably, significantly accelerated users' information
processing, and effectively improved their overall comprehension and
sensemaking experience.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02446">Low-Resource Languages Jailbreak GPT-4. (arXiv:2310.02446v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yong_Z/0/1/0/all/0/1">Zheng-Xin Yong</a>, <a href="http://arxiv.org/find/cs/1/au:+Menghini_C/0/1/0/all/0/1">Cristina Menghini</a>, <a href="http://arxiv.org/find/cs/1/au:+Bach_S/0/1/0/all/0/1">Stephen H. Bach</a></p>
<p>AI safety training and red-teaming of large language models (LLMs) are
measures to mitigate the generation of unsafe content. Our work exposes the
inherent cross-lingual vulnerability of these safety mechanisms, resulting from
the linguistic inequality of safety training data, by successfully
circumventing GPT-4's safeguard through translating unsafe English inputs into
low-resource languages. On the AdvBenchmark, GPT-4 engages with the unsafe
translated inputs and provides actionable items that can get the users towards
their harmful goals 79% of the time, which is on par with or even surpassing
state-of-the-art jailbreaking attacks. Other high-/mid-resource languages have
significantly lower attack success rate, which suggests that the cross-lingual
vulnerability mainly applies to low-resource languages. Previously, limited
training on low-resource languages primarily affects speakers of those
languages, causing technological disparities. However, our work highlights a
crucial shift: this deficiency now poses a risk to all LLMs users. Publicly
available translation APIs enable anyone to exploit LLMs' safety
vulnerabilities. Therefore, our work calls for a more holistic red-teaming
efforts to develop robust multilingual safeguards with wide language coverage.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05969">Automated Chest X-Ray Report Generator Using Multi-Model Deep Learning Approach. (arXiv:2310.05969v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Muharram_A/0/1/0/all/0/1">Arief Purnama Muharram</a>, <a href="http://arxiv.org/find/eess/1/au:+Haryono_H/0/1/0/all/0/1">Hollyana Puteri Haryono</a>, <a href="http://arxiv.org/find/eess/1/au:+Juma_A/0/1/0/all/0/1">Abassi Haji Juma</a>, <a href="http://arxiv.org/find/eess/1/au:+Puspasari_I/0/1/0/all/0/1">Ira Puspasari</a>, <a href="http://arxiv.org/find/eess/1/au:+Utama_N/0/1/0/all/0/1">Nugraha Priya Utama</a></p>
<p>Reading and interpreting chest X-ray images is one of the most radiologist's
routines. However, it still can be challenging, even for the most experienced
ones. Therefore, we proposed a multi-model deep learning-based automated chest
X-ray report generator system designed to assist radiologists in their work.
The basic idea of the proposed system is by utilizing multi
binary-classification models for detecting multi abnormalities, with each model
responsible for detecting one abnormality, in a single image. In this study, we
limited the radiology abnormalities detection to only cardiomegaly, lung
effusion, and consolidation. The system generates a radiology report by
performing the following three steps: image pre-processing, utilizing deep
learning models to detect abnormalities, and producing a report. The aim of the
image pre-processing step is to standardize the input by scaling it to 128x128
pixels and slicing it into three segments, which covers the upper, lower, and
middle parts of the lung. After pre-processing, each corresponding model
classifies the image, resulting in a 0 (zero) for no abnormality detected and a
1 (one) for the presence of an abnormality. The prediction outputs of each
model are then concatenated to form a 'result code'. The 'result code' is used
to construct a report by selecting the appropriate pre-determined sentence for
each detected abnormality in the report generation step. The proposed system is
expected to reduce the workload of radiologists and increase the accuracy of
chest X-ray diagnosis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07276">BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations. (arXiv:2310.07276v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pei_Q/0/1/0/all/0/1">Qizhi Pei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jinhua Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1">Kehan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1">Kaiyuan Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1">Lijun Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1">Yingce Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1">Rui Yan</a></p>
<p>Recent advancements in biological research leverage the integration of
molecules, proteins, and natural language to enhance drug discovery. However,
current models exhibit several limitations, such as the generation of invalid
molecular SMILES, underutilization of contextual information, and equal
treatment of structured and unstructured knowledge. To address these issues, we
propose $\mathbf{BioT5}$, a comprehensive pre-training framework that enriches
cross-modal integration in biology with chemical knowledge and natural language
associations. $\mathbf{BioT5}$ utilizes SELFIES for $100%$ robust molecular
representations and extracts knowledge from the surrounding context of
bio-entities in unstructured biological literature. Furthermore,
$\mathbf{BioT5}$ distinguishes between structured and unstructured knowledge,
leading to more effective utilization of information. After fine-tuning, BioT5
shows superior performance across a wide range of tasks, demonstrating its
strong capability of capturing underlying relations and properties of
bio-entities. Our code is available at
$\href{https://github.com/QizhiPei/BioT5}{Github}$.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.10705">Empirical and Experimental Insights into Machine Learning-Based Defect Classification in Semiconductor Wafers. (arXiv:2310.10705v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Taha_K/0/1/0/all/0/1">Kamal Taha</a></p>
<p>This survey paper offers a comprehensive review of methodologies utilizing
machine learning (ML) classification techniques for identifying wafer defects
in semiconductor manufacturing. Despite the growing body of research
demonstrating the effectiveness of ML in wafer defect identification, there is
a noticeable absence of comprehensive reviews on this subject. This survey
attempts to fill this void by amalgamating available literature and providing
an in-depth analysis of the advantages, limitations, and potential applications
of various ML classification algorithms in the realm of wafer defect detection.
An innovative taxonomy of methodologies that we present provides a detailed
classification of algorithms into more refined categories and techniques. This
taxonomy follows a three-tier structure, starting from broad methodology
categories and ending with specific techniques. It aids researchers in
comprehending the complex relationships between different algorithms and their
techniques. We employ a rigorous empirical and experimental evaluation to rank
these varying techniques. For the empirical evaluation, we assess techniques
based on a set of five criteria. The experimental evaluation ranks the
algorithms employing the same techniques, sub-categories, and categories. Also
the paper illuminates the future prospects of ML classification techniques for
wafer defect identification, underscoring potential advancements and
opportunities for further research in this field
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.13833">GraphMaker: Can Diffusion Models Generate Large Attributed Graphs?. (arXiv:2310.13833v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Mufei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Kreacic_E/0/1/0/all/0/1">Eleonora Krea&#x10d;i&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Potluru_V/0/1/0/all/0/1">Vamsi K. Potluru</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Pan Li</a></p>
<p>Large-scale graphs with node attributes are increasingly common in various
real-world applications. Creating synthetic, attribute-rich graphs that mirror
real-world examples is crucial, especially for sharing graph data for analysis
and developing learning models when original data is restricted to be shared.
Traditional graph generation methods are limited in their capacity to handle
these complex structures. Recent advances in diffusion models have shown
potential in generating graph structures without attributes and smaller
molecular graphs. However, these models face challenges in generating large
attributed graphs due to the complex attribute-structure correlations and the
large size of these graphs. This paper introduces a novel diffusion model,
GraphMaker, specifically designed for generating large attributed graphs. We
explore various combinations of node attribute and graph structure generation
processes, finding that an asynchronous approach more effectively captures the
intricate attribute-structure correlations. We also address scalability issues
through edge mini-batching generation. To demonstrate the practicality of our
approach in graph data dissemination, we introduce a new evaluation pipeline.
The evaluation demonstrates that synthetic graphs generated by GraphMaker can
be used to develop competitive graph machine learning models for the tasks
defined over the original graphs without actually accessing these graphs, while
many leading graph generation methods fall short in this evaluation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.14526">Towards Zero Shot Learning in Restless Multi-armed Bandits. (arXiv:2310.14526v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yunfan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Behari_N/0/1/0/all/0/1">Nikhil Behari</a>, <a href="http://arxiv.org/find/cs/1/au:+Hughes_E/0/1/0/all/0/1">Edward Hughes</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1">Edwin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Nagaraj_D/0/1/0/all/0/1">Dheeraj Nagaraj</a>, <a href="http://arxiv.org/find/cs/1/au:+Tuyls_K/0/1/0/all/0/1">Karl Tuyls</a>, <a href="http://arxiv.org/find/cs/1/au:+Taneja_A/0/1/0/all/0/1">Aparna Taneja</a>, <a href="http://arxiv.org/find/cs/1/au:+Tambe_M/0/1/0/all/0/1">Milind Tambe</a></p>
<p>Restless multi-arm bandits (RMABs), a class of resource allocation problems
with broad application in areas such as healthcare, online advertising, and
anti-poaching, have recently been studied from a multi-agent reinforcement
learning perspective. Prior RMAB research suffers from several limitations,
e.g., it fails to adequately address continuous states, and requires retraining
from scratch when arms opt-in and opt-out over time, a common challenge in many
real world applications. We address these limitations by developing a neural
network-based pre-trained model (PreFeRMAB) that has general zero-shot ability
on a wide range of previously unseen RMABs, and which can be fine-tuned on
specific instances in a more sample-efficient way than retraining from scratch.
Our model also accommodates general multi-action settings and discrete or
continuous state spaces. To enable fast generalization, we learn a novel single
policy network model that utilizes feature information and employs a training
procedure in which arms opt-in and out over time. We derive a new update rule
for a crucial $\lambda$-network with theoretical convergence guarantees and
empirically demonstrate the advantages of our approach on several challenging,
real-world inspired problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17589">An Open Source Data Contamination Report for Large Language Models. (arXiv:2310.17589v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yucheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Guerin_F/0/1/0/all/0/1">Frank Guerin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1">Chenghua Lin</a></p>
<p>Data contamination in model evaluation has become increasingly prevalent with
the growing popularity of large language models. It allows models to "cheat"
via memorisation instead of displaying true capabilities. Therefore,
contamination analysis has become an crucial part of reliable model evaluation
to validate results. However, existing contamination analysis is usually
conducted internally by large language model developers and often lacks
transparency and completeness. This paper presents an extensive data
contamination report for over 15 popular large language models across six
popular multiple-choice QA benchmarks. We also introduce an open-source
pipeline that enables the community to perform contamination analysis on
customised data and models. Our experiments reveal varying contamination levels
ranging from 1\% to 45\% across benchmarks, with the contamination degree
increasing rapidly over time. Performance analysis of large language models
indicates that data contamination does not necessarily lead to increased model
metrics: while significant accuracy boosts of up to 14\% and 7\% are observed
on contaminated C-Eval and Hellaswag benchmarks, only a minimal increase is
noted on contaminated MMLU. We also find larger models seem able to gain more
advantages than smaller models on contaminated test sets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.18446">A Novel Skip Orthogonal List for Dynamic Optimal Transport Problem. (arXiv:2310.18446v5 [cs.DS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiaoyang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1">Hu Ding</a></p>
<p>Optimal transport is a fundamental topic that has attracted a great amount of
attention from the optimization community in the past decades. In this paper,
we consider an interesting discrete dynamic optimal transport problem: can we
efficiently update the optimal transport plan when the weights or the locations
of the data points change? This problem is naturally motivated by several
applications in machine learning. For example, we often need to compute the
optimal transport cost between two different data sets; if some changes happen
to a few data points, should we re-compute the high complexity cost function or
update the cost by some efficient dynamic data structure? We are aware that
several dynamic maximum flow algorithms have been proposed before, however, the
research on dynamic minimum cost flow problem is still quite limited, to the
best of our knowledge. We propose a novel 2D Skip Orthogonal List together with
some dynamic tree techniques. Although our algorithm is based on the
conventional simplex method, it can efficiently find the variable to pivot
within expected $O(1)$ time, and complete each pivoting operation within
expected $O(|V|)$ time where $V$ is the set of all supply and demand nodes.
Since dynamic modifications typically do not introduce significant changes, our
algorithm requires only a few simplex iterations in practice. So our algorithm
is more efficient than re-computing the optimal transport cost that needs at
least one traversal over all $|E| = O(|V|^2)$ variables, where $|E|$ denotes
the number of edges in the network. Our experiments demonstrate that our
algorithm significantly outperforms existing algorithms in the dynamic
scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.20025">GOPlan: Goal-conditioned Offline Reinforcement Learning by Planning with Learned Models. (arXiv:2310.20025v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Mianchu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1">Rui Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Hao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Montana_G/0/1/0/all/0/1">Giovanni Montana</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1">Meng Fang</a></p>
<p>Offline Goal-Conditioned RL (GCRL) offers a feasible paradigm for learning
general-purpose policies from diverse and multi-task offline datasets. Despite
notable recent progress, the predominant offline GCRL methods, mainly
model-free, face constraints in handling limited data and generalizing to
unseen goals. In this work, we propose Goal-conditioned Offline Planning
(GOPlan), a novel model-based framework that contains two key phases: (1)
pretraining a prior policy capable of capturing multi-modal action distribution
within the multi-goal dataset; (2) employing the reanalysis method with
planning to generate imagined trajectories for funetuning policies.
Specifically, we base the prior policy on an advantage-weighted conditioned
generative adversarial network, which facilitates distinct mode separation,
mitigating the pitfalls of out-of-distribution (OOD) actions. For further
policy optimization, the reanalysis method generates high-quality imaginary
data by planning with learned models for both intra-trajectory and
inter-trajectory goals. With thorough experimental evaluations, we demonstrate
that GOPlan achieves state-of-the-art performance on various offline multi-goal
navigation and manipulation tasks. Moreover, our results highlight the superior
ability of GOPlan to handle small data budgets and generalize to OOD goals.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01927">GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling. (arXiv:2311.01927v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Katsch_T/0/1/0/all/0/1">Tobias Katsch</a></p>
<p>Linear Recurrence has proven to be a powerful tool for modeling long
sequences efficiently. In this work, we show that existing models fail to take
full advantage of its potential. Motivated by this finding, we develop
GateLoop, a foundational sequence model that generalizes linear recurrent
models such as S4, S5, LRU and RetNet, by employing data-controlled state
transitions. Utilizing this theoretical advance, GateLoop empirically
outperforms existing models for auto-regressive language modeling. Our method
comes with a low-cost $O(l)$ recurrent mode and an efficient $O(l \log_{2} l)$
parallel mode making use of highly optimized associative scan implementations.
Furthermore, we derive an $O(l^2)$ surrogate attention mode, revealing
remarkable implications for Transformer and recently proposed architectures.
Specifically, we prove that our approach can be interpreted as providing
data-controlled relative-positional information to Attention. While many
existing models solely rely on data-controlled cumulative sums for context
aggregation, our findings suggest that incorporating data-controlled complex
cumulative products may be a crucial step towards more powerful sequence
models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07745">Simplifying Complex Observation Models in Continuous POMDP Planning with Probabilistic Guarantees and Practice. (arXiv:2311.07745v4 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lev_Yehudi_I/0/1/0/all/0/1">Idan Lev-Yehudi</a>, <a href="http://arxiv.org/find/cs/1/au:+Barenboim_M/0/1/0/all/0/1">Moran Barenboim</a>, <a href="http://arxiv.org/find/cs/1/au:+Indelman_V/0/1/0/all/0/1">Vadim Indelman</a></p>
<p>Solving partially observable Markov decision processes (POMDPs) with high
dimensional and continuous observations, such as camera images, is required for
many real life robotics and planning problems. Recent researches suggested
machine learned probabilistic models as observation models, but their use is
currently too computationally expensive for online deployment. We deal with the
question of what would be the implication of using simplified observation
models for planning, while retaining formal guarantees on the quality of the
solution. Our main contribution is a novel probabilistic bound based on a
statistical total variation distance of the simplified model. We show that it
bounds the theoretical POMDP value w.r.t. original model, from the empirical
planned value with the simplified model, by generalizing recent results of
particle-belief MDP concentration bounds. Our calculations can be separated
into offline and online parts, and we arrive at formal guarantees without
having to access the costly model at all during planning, which is also a novel
result. Finally, we demonstrate in simulation how to integrate the bound into
the routine of an existing continuous online POMDP solver.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09335">Investigating Hallucinations in Pruned Large Language Models for Abstractive Summarization. (arXiv:2311.09335v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chrysostomou_G/0/1/0/all/0/1">George Chrysostomou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zhixue Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Williams_M/0/1/0/all/0/1">Miles Williams</a>, <a href="http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1">Nikolaos Aletras</a></p>
<p>Despite the remarkable performance of generative large language models (LLMs)
on abstractive summarization, they face two significant challenges: their
considerable size and tendency to hallucinate. Hallucinations are concerning
because they erode reliability and raise safety issues. Pruning is a technique
that reduces model size by removing redundant weights, enabling more efficient
sparse inference. Pruned models yield downstream task performance comparable to
the original, making them ideal alternatives when operating on a limited
budget. However, the effect that pruning has upon hallucinations in abstractive
summarization with LLMs has yet to be explored. In this paper, we provide an
extensive empirical study across five summarization datasets, two
state-of-the-art pruning methods, and five instruction-tuned LLMs.
Surprisingly, we find that hallucinations from pruned LLMs are less prevalent
than the original models. Our analysis suggests that pruned models tend to
depend more on the source document for summary generation. This leads to a
higher lexical overlap between the generated summary and the source document,
which could be a reason for the reduction in hallucination risk.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15480">Automatic Time Signature Determination for New Scores Using Lyrics for Latent Rhythmic Structure. (arXiv:2311.15480v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liao_C/0/1/0/all/0/1">Callie C. Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_D/0/1/0/all/0/1">Duoduo Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Guessford_J/0/1/0/all/0/1">Jesse Guessford</a></p>
<p>There has recently been a sharp increase in interest in Artificial
Intelligence-Generated Content (AIGC). Despite this, musical components such as
time signatures have not been studied sufficiently to form an algorithmic
determination approach for new compositions, especially lyrical songs. This is
likely because of the neglect of musical details, which is critical for
constructing a robust framework. Specifically, time signatures establish the
fundamental rhythmic structure for almost all aspects of a song, including the
phrases and notes. In this paper, we propose a novel approach that only uses
lyrics as input to automatically generate a fitting time signature for lyrical
songs and uncover the latent rhythmic structure utilizing explainable machine
learning models. In particular, we devise multiple methods that are associated
with discovering lyrical patterns and creating new features that simultaneously
contain lyrical, rhythmic, and statistical information. In this approach, the
best of our experimental results reveal a 97.6% F1 score and a 0.996 Area Under
the Curve (AUC) of the Receiver Operating Characteristic (ROC) score. In
conclusion, our research directly generates time signatures from lyrics
automatically for new scores utilizing machine learning, which is an innovative
idea that approaches an understudied component of musicology and therefore
contributes significantly to the future of Artificial Intelligence (AI) music
generation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02493">Flexible Communication for Optimal Distributed Learning over Unpredictable Networks. (arXiv:2312.02493v2 [cs.DC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tyagi_S/0/1/0/all/0/1">Sahil Tyagi</a>, <a href="http://arxiv.org/find/cs/1/au:+Swany_M/0/1/0/all/0/1">Martin Swany</a></p>
<p>Gradient compression alleviates expensive communication in distributed deep
learning by sending fewer values and its corresponding indices, typically via
Allgather (AG). Training with high compression ratio (CR) achieves high
accuracy like DenseSGD, but has lower parallel scaling due to high
communication cost (i.e., parallel efficiency). Using lower CRs improves
parallel efficiency by lowering synchronization cost, but degrades model
accuracy as well (statistical efficiency). Further, speedup attained with
different models and CRs also varies with network latency, effective bandwidth
and collective op used for aggregation. In many cases, collectives like
Allreduce (AR) have lower cost than AG to exchange the same amount of data. In
this paper, we propose an AR-compatible Topk compressor that is
bandwidth-optimal and thus performs better than AG in certain network
configurations. We develop a flexible communication strategy that switches
between AG and AR based on which collective is optimal in the current settings,
and model the pareto-relationship between parallel and statistical efficiency
as a multi-objective optimization (MOO) problem to dynamically adjust CR and
accelerate training while still converging to high accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03687">MatterGen: a generative model for inorganic materials design. (arXiv:2312.03687v2 [cond-mat.mtrl-sci] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Zeni_C/0/1/0/all/0/1">Claudio Zeni</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Pinsler_R/0/1/0/all/0/1">Robert Pinsler</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Zugner_D/0/1/0/all/0/1">Daniel Z&#xfc;gner</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Fowler_A/0/1/0/all/0/1">Andrew Fowler</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Horton_M/0/1/0/all/0/1">Matthew Horton</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Fu_X/0/1/0/all/0/1">Xiang Fu</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Shysheya_S/0/1/0/all/0/1">Sasha Shysheya</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Crabbe_J/0/1/0/all/0/1">Jonathan Crabb&#xe9;</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Sun_L/0/1/0/all/0/1">Lixin Sun</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Smith_J/0/1/0/all/0/1">Jake Smith</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Nguyen_B/0/1/0/all/0/1">Bichlien Nguyen</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Schulz_H/0/1/0/all/0/1">Hannes Schulz</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Lewis_S/0/1/0/all/0/1">Sarah Lewis</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Huang_C/0/1/0/all/0/1">Chin-Wei Huang</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Lu_Z/0/1/0/all/0/1">Ziheng Lu</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Zhou_Y/0/1/0/all/0/1">Yichi Zhou</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Yang_H/0/1/0/all/0/1">Han Yang</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Hao_H/0/1/0/all/0/1">Hongxia Hao</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Li_J/0/1/0/all/0/1">Jielan Li</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Tomioka_R/0/1/0/all/0/1">Ryota Tomioka</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Xie_T/0/1/0/all/0/1">Tian Xie</a></p>
<p>The design of functional materials with desired properties is essential in
driving technological advances in areas like energy storage, catalysis, and
carbon capture. Generative models provide a new paradigm for materials design
by directly generating entirely novel materials given desired property
constraints. Despite recent progress, current generative models have low
success rate in proposing stable crystals, or can only satisfy a very limited
set of property constraints. Here, we present MatterGen, a model that generates
stable, diverse inorganic materials across the periodic table and can further
be fine-tuned to steer the generation towards a broad range of property
constraints. To enable this, we introduce a new diffusion-based generative
process that produces crystalline structures by gradually refining atom types,
coordinates, and the periodic lattice. We further introduce adapter modules to
enable fine-tuning towards any given property constraints with a labeled
dataset. Compared to prior generative models, structures produced by MatterGen
are more than twice as likely to be novel and stable, and more than 15 times
closer to the local energy minimum. After fine-tuning, MatterGen successfully
generates stable, novel materials with desired chemistry, symmetry, as well as
mechanical, electronic and magnetic properties. Finally, we demonstrate
multi-property materials design capabilities by proposing structures that have
both high magnetic density and a chemical composition with low supply-chain
risk. We believe that the quality of generated materials and the breadth of
MatterGen's capabilities represent a major advancement towards creating a
universal generative model for materials design.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03905">A Pseudo-Semantic Loss for Autoregressive Models with Logical Constraints. (arXiv:2312.03905v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ahmed_K/0/1/0/all/0/1">Kareem Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1">Kai-Wei Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Broeck_G/0/1/0/all/0/1">Guy Van den Broeck</a></p>
<p>Neuro-symbolic AI bridges the gap between purely symbolic and neural
approaches to learning. This often requires maximizing the likelihood of a
symbolic constraint w.r.t the neural network's output distribution. Such output
distributions are typically assumed to be fully-factorized. This limits the
applicability of neuro-symbolic learning to the more expressive autoregressive
distributions, e.g., transformers. Under such distributions, computing the
likelihood of even simple constraints is #P-hard. Instead of attempting to
enforce the constraint on the entire output distribution, we propose to do so
on a random, local approximation thereof. More precisely, we optimize the
likelihood of the constraint under a pseudolikelihood-based approximation
centered around a model sample. Our approximation is factorized, allowing the
reuse of solutions to sub-problems, a main tenet for efficiently computing
neuro-symbolic losses. Moreover, it is a local, high-fidelity approximation of
the likelihood, exhibiting low entropy and KL-divergence around the model
sample. We evaluate our approach on Sudoku and shortest-path prediction cast as
autoregressive generation, and observe that we greatly improve upon the base
model's ability to predict logically-consistent outputs. We also evaluate on
the task of detoxifying large language models. Using a simple constraint
disallowing a list of toxic words, we are able to steer the model's outputs
away from toxic generations, achieving SoTA detoxification compared to previous
approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04027">The sample complexity of multi-distribution learning. (arXiv:2312.04027v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1">Binghui Peng</a></p>
<p>Multi-distribution learning generalizes the classic PAC learning to handle
data coming from multiple distributions. Given a set of $k$ data distributions
and a hypothesis class of VC dimension $d$, the goal is to learn a hypothesis
that minimizes the maximum population loss over $k$ distributions, up to
$\epsilon$ additive error. In this paper, we settle the sample complexity of
multi-distribution learning by giving an algorithm of sample complexity
$\widetilde{O}((d+k)\epsilon^{-2}) \cdot (k/\epsilon)^{o(1)}$. This matches the
lower bound up to sub-polynomial factor and resolves the COLT 2023 open problem
of Awasthi, Haghtalab and Zhao [AHZ23].
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05928">AesFA: An Aesthetic Feature-Aware Arbitrary Neural Style Transfer. (arXiv:2312.05928v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kwon_J/0/1/0/all/0/1">Joonwoo Kwon</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Sooyoung Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yuewei Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1">Shinjae Yoo</a>, <a href="http://arxiv.org/find/cs/1/au:+Cha_J/0/1/0/all/0/1">Jiook Cha</a></p>
<p>Neural style transfer (NST) has evolved significantly in recent years. Yet,
despite its rapid progress and advancement, existing NST methods either
struggle to transfer aesthetic information from a style effectively or suffer
from high computational costs and inefficiencies in feature disentanglement due
to using pre-trained models. This work proposes a lightweight but effective
model, AesFA -- Aesthetic Feature-Aware NST. The primary idea is to decompose
the image via its frequencies to better disentangle aesthetic styles from the
reference image while training the entire model in an end-to-end manner to
exclude pre-trained models at inference completely. To improve the network's
ability to extract more distinct representations and further enhance the
stylization quality, this work introduces a new aesthetic feature: contrastive
loss. Extensive experiments and ablations show the approach not only
outperforms recent NST methods in terms of stylization quality, but it also
achieves faster inference. Codes are available at
https://github.com/Sooyyoungg/AesFA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06695">Evolving Reservoirs for Meta Reinforcement Learning. (arXiv:2312.06695v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Leger_C/0/1/0/all/0/1">Corentin L&#xe9;ger</a>, <a href="http://arxiv.org/find/cs/1/au:+Hamon_G/0/1/0/all/0/1">Gautier Hamon</a>, <a href="http://arxiv.org/find/cs/1/au:+Nisioti_E/0/1/0/all/0/1">Eleni Nisioti</a>, <a href="http://arxiv.org/find/cs/1/au:+Hinaut_X/0/1/0/all/0/1">Xavier Hinaut</a>, <a href="http://arxiv.org/find/cs/1/au:+Moulin_Frier_C/0/1/0/all/0/1">Cl&#xe9;ment Moulin-Frier</a></p>
<p>Animals often demonstrate a remarkable ability to adapt to their environments
during their lifetime. They do so partly due to the evolution of morphological
and neural structures. These structures capture features of environments shared
between generations to bias and speed up lifetime learning. In this work, we
propose a computational model for studying a mechanism that can enable such a
process. We adopt a computational framework based on meta reinforcement
learning as a model of the interplay between evolution and development. At the
evolutionary scale, we evolve reservoirs, a family of recurrent neural networks
that differ from conventional networks in that one optimizes not the synaptic
weights, but hyperparameters controlling macro-level properties of the
resulting network architecture. At the developmental scale, we employ these
evolved reservoirs to facilitate the learning of a behavioral policy through
Reinforcement Learning (RL). Within an RL agent, a reservoir encodes the
environment state before providing it to an action policy. We evaluate our
approach on several 2D and 3D simulated environments. Our results show that the
evolution of reservoirs can improve the learning of diverse challenging tasks.
We study in particular three hypotheses: the use of an architecture combining
reservoirs and reinforcement learning could enable (1) solving tasks with
partial observability, (2) generating oscillatory dynamics that facilitate the
learning of locomotion tasks, and (3) facilitating the generalization of
learned behaviors to new tasks unknown during the evolution phase.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08616">A Generalized Neural Diffusion Framework on Graphs. (arXiv:2312.08616v2 [cs.SI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yibo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hongrui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1">Chuan Shi</a></p>
<p>Recent studies reveal the connection between GNNs and the diffusion process,
which motivates many diffusion-based GNNs to be proposed. However, since these
two mechanisms are closely related, one fundamental question naturally arises:
Is there a general diffusion framework that can formally unify these GNNs? The
answer to this question can not only deepen our understanding of the learning
process of GNNs, but also may open a new door to design a broad new class of
GNNs. In this paper, we propose a general diffusion equation framework with the
fidelity term, which formally establishes the relationship between the
diffusion process with more GNNs. Meanwhile, with this framework, we identify
one characteristic of graph diffusion networks, i.e., the current neural
diffusion process only corresponds to the first-order diffusion equation.
However, by an experimental investigation, we show that the labels of
high-order neighbors actually exhibit monophily property, which induces the
similarity based on labels among high-order neighbors without requiring the
similarity among first-order neighbors. This discovery motives to design a new
high-order neighbor-aware diffusion equation, and derive a new type of graph
diffusion network (HiD-Net) based on the framework. With the high-order
diffusion equation, HiD-Net is more robust against attacks and works on both
homophily and heterophily graphs. We not only theoretically analyze the
relation between HiD-Net with high-order random walk, but also provide a
theoretical convergence guarantee. Extensive experimental results well
demonstrate the effectiveness of HiD-Net over state-of-the-art graph diffusion
networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10572">Improved Anonymous Multi-Agent Path Finding Algorithm. (arXiv:2312.10572v4 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ali_Z/0/1/0/all/0/1">Zain Alabedeen Ali</a>, <a href="http://arxiv.org/find/cs/1/au:+Yakovlev_K/0/1/0/all/0/1">Konstantin Yakovlev</a></p>
<p>We consider an Anonymous Multi-Agent Path-Finding (AMAPF) problem where the
set of agents is confined to a graph, a set of goal vertices is given and each
of these vertices has to be reached by some agent. The problem is to find an
assignment of the goals to the agents as well as the collision-free paths, and
we are interested in finding the solution with the optimal makespan. A
well-established approach to solve this problem is to reduce it to a special
type of a graph search problem, i.e. to the problem of finding a maximum flow
on an auxiliary graph induced by the input one. The size of the former graph
may be very large and the search on it may become a bottleneck. To this end, we
suggest a specific search algorithm that leverages the idea of exploring the
search space not through considering separate search states but rather bulks of
them simultaneously. That is, we implicitly compress, store and expand bulks of
the search states as single states, which results in high reduction in runtime
and memory. Empirically, the resultant AMAPF solver demonstrates superior
performance compared to the state-of-the-art competitor and is able to solve
all publicly available MAPF instances from the well-known MovingAI benchmark in
less than 30 seconds.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11456">Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-Constraint. (arXiv:2312.11456v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1">Wei Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1">Hanze Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1">Chenlu Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Ziqi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1">Han Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1">Heng Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1">Nan Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tong Zhang</a></p>
<p>This paper studies the theoretical framework of the alignment process of
generative models with Reinforcement Learning from Human Feedback (RLHF). We
consider a standard mathematical formulation, the reverse-KL regularized
contextual bandit for RLHF. Despite its widespread practical application, a
rigorous theoretical analysis of this formulation remains open. We investigate
its behavior in three distinct settings -- offline, online, and hybrid -- and
propose efficient algorithms with finite-sample theoretical guarantees.
</p>
<p>Moving towards practical applications, our framework, with a robust
approximation of the information-theoretical policy improvement oracle,
naturally gives rise to several novel RLHF algorithms. This includes an
iterative version of the Direct Preference Optimization (DPO) algorithm for
online settings, and a multi-step rejection sampling strategy for offline
scenarios. Our empirical evaluations on real-world alignment experiment of
large language model demonstrate that these proposed methods significantly
surpass existing strong baselines, such as DPO and Rejection Sampling
Optimization (RSO), showcasing the connections between solid theoretical
foundations and their powerful practical implementations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11714">Time-Transformer: Integrating Local and Global Features for Better Time Series Generation. (arXiv:2312.11714v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuansan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wijewickrema_S/0/1/0/all/0/1">Sudanthi Wijewickrema</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1">Ang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Bester_C/0/1/0/all/0/1">Christofer Bester</a>, <a href="http://arxiv.org/find/cs/1/au:+OLeary_S/0/1/0/all/0/1">Stephen O&#x27;Leary</a>, <a href="http://arxiv.org/find/cs/1/au:+Bailey_J/0/1/0/all/0/1">James Bailey</a></p>
<p>Generating time series data is a promising approach to address data
deficiency problems. However, it is also challenging due to the complex
temporal properties of time series data, including local correlations as well
as global dependencies. Most existing generative models have failed to
effectively learn both the local and global properties of time series data. To
address this open problem, we propose a novel time series generative model
named 'Time-Transformer AAE', which consists of an adversarial autoencoder
(AAE) and a newly designed architecture named 'Time-Transformer' within the
decoder. The Time-Transformer first simultaneously learns local and global
features in a layer-wise parallel design, combining the abilities of Temporal
Convolutional Networks and Transformer in extracting local features and global
dependencies respectively. Second, a bidirectional cross attention is proposed
to provide complementary guidance across the two branches and achieve proper
fusion between local and global features. Experimental results demonstrate that
our model can outperform existing state-of-the-art models in 5 out of 6
datasets, specifically on those with data containing both global and local
properties. Furthermore, we highlight our model's advantage on handling this
kind of data via an artificial dataset. Finally, we show our model's ability to
address a real-world problem: data augmentation to support learning with small
datasets and imbalanced datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16554">A Theoretical Analysis of Efficiency Constrained Utility-Privacy Bi-Objective Optimization in Federated Learning. (arXiv:2312.16554v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gu_H/0/1/0/all/0/1">Hanlin Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xinyuan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1">Gongxi Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1">Yuxing Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1">Yan Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1">Lixin Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1">Qiang Yang</a></p>
<p>Federated learning (FL) enables multiple clients to collaboratively learn a
shared model without sharing their individual data. Concerns about utility,
privacy, and training efficiency in FL have garnered significant research
attention. Differential privacy has emerged as a prevalent technique in FL,
safeguarding the privacy of individual user data while impacting utility and
training efficiency. Within Differential Privacy Federated Learning (DPFL),
previous studies have primarily focused on the utility-privacy trade-off,
neglecting training efficiency, which is crucial for timely completion.
Moreover, differential privacy achieves privacy by introducing controlled
randomness (noise) on selected clients in each communication round. Previous
work has mainly examined the impact of noise level ($\sigma$) and communication
rounds ($T$) on the privacy-utility dynamic, overlooking other influential
factors like the sample ratio ($q$, the proportion of selected clients). This
paper systematically formulates an efficiency-constrained utility-privacy
bi-objective optimization problem in DPFL, focusing on $\sigma$, $T$, and $q$.
We provide a comprehensive theoretical analysis, yielding analytical solutions
for the Pareto front. Extensive empirical experiments verify the validity and
efficacy of our analysis, offering valuable guidance for low-cost parameter
design in DPFL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01801">A quatum inspired neural network for geometric modeling. (arXiv:2401.01801v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1">Weitao Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shengchao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xuecang Zhang</a></p>
<p>By conceiving physical systems as 3D many-body point clouds, geometric graph
neural networks (GNNs), such as SE(3)/E(3) equivalent GNNs, have showcased
promising performance. In particular, their effective message-passing mechanics
make them adept at modeling molecules and crystalline materials. However,
current geometric GNNs only offer a mean-field approximation of the many-body
system, encapsulated within two-body message passing, thus falling short in
capturing intricate relationships within these geometric graphs. To address
this limitation, tensor networks, widely employed by computational physics to
handle manybody systems using high-order tensors, have been introduced.
Nevertheless, integrating these tensorized networks into the message-passing
framework of GNNs faces scalability and symmetry conservation (e.g.,
permutation and rotation) challenges. In response, we introduce an innovative
equivariant Matrix Product State (MPS)-based message-passing strategy, through
achieving an efficient implementation of the tensor contraction operation. Our
method effectively models complex many-body relationships, suppressing
mean-field approximations, and captures symmetries within geometric graphs.
Importantly, it seamlessly replaces the standard message-passing and
layer-aggregation modules intrinsic to geometric GNNs. We empirically validate
the superior accuracy of our approach on benchmark tasks, including predicting
classical Newton systems and quantum tensor Hamiltonian matrices. To our
knowledge, our approach represents the inaugural utilization of parameterized
geometric tensor networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01836">Neural Optimal Control: Concurrent System Identification and Control Learning with Neural ODE. (arXiv:2401.01836v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chi_C/0/1/0/all/0/1">Cheng Chi</a></p>
<p>Controlling continuous-time dynamical systems is generally a two step
process: first, identify or model the system dynamics with differential
equations, then, minimize the control objectives to achieve optimal control
function and optimal state trajectories. However, any inaccuracy in dynamics
modeling will lead to sub-optimality in the resulting control function. To
address this, we propose a neural ODE based method for controlling unknown
dynamical systems, denoted as Neural Optimal Control (NOC), which combines
dynamics identification and optimal control learning using a coupled neural
ODE. Through an intriguing interplay between the two neural networks in coupled
neural ODE structure, our model concurrently learns system dynamics as well as
optimal controls that guides towards target states. Our experiments demonstrate
the effectiveness of our model for learning optimal control of unknown
dynamical systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.03343">Rediscovering Ranganathan: A Prismatic View of His Life through the Knowledge Graph Spectrum. (arXiv:2401.03343v2 [cs.DL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dutta_B/0/1/0/all/0/1">B. Dutta</a>, <a href="http://arxiv.org/find/cs/1/au:+Arzoo_S/0/1/0/all/0/1">S. Arzoo</a></p>
<p>The present study puts forward a novel biographical knowledge graph (KG) on
Prof. S. R. Ranganathan, one of the pioneering figures in the Library and
Information Science (LIS) domain. It has been found that most of the relevant
facts about Ranganathan exist in a variety of resources (e.g., books, essays,
journal articles, websites, blogs, etc.), offering information in a fragmented
and piecemeal way. With this dedicated KG (henceforth known as RKG), we hope to
furnish a 360-degree view of his life and achievements. To the best of our
knowledge, such a dedicated representation is unparalleled in its scope and
coverage: using state-of-the-art technology for anyone to openly access,
use/re-use, and contribute. Inspired by Ranganathan's theories and ideas, the
KG was developed using a "facet-based methodology" at two levels: in the
identification of the vital biographical aspects and the development of the
ontological model. Finally, with this study, we call for a community-driven
effort to enhance the KG and pay homage to the Father of Library Science on the
hundredth anniversary of his revitalizing the LIS domain through his enduring
participation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06256">A Universal Knowledge Model and Cognitive Architecture for Prototyping AGI. (arXiv:2401.06256v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sukhobokov_A/0/1/0/all/0/1">Artem Sukhobokov</a>, <a href="http://arxiv.org/find/cs/1/au:+Belousov_E/0/1/0/all/0/1">Evgeny Belousov</a>, <a href="http://arxiv.org/find/cs/1/au:+Gromozdov_D/0/1/0/all/0/1">Danila Gromozdov</a>, <a href="http://arxiv.org/find/cs/1/au:+Zenger_A/0/1/0/all/0/1">Anna Zenger</a>, <a href="http://arxiv.org/find/cs/1/au:+Popov_I/0/1/0/all/0/1">Ilya Popov</a></p>
<p>The article identified 42 cognitive architectures for creating general
artificial intelligence (AGI) and proposed a set of interrelated functional
blocks that an agent approaching AGI in its capabilities should possess. Since
the required set of blocks is not found in any of the existing architectures,
the article proposes a new cognitive architecture for intelligent systems
approaching AGI in their capabilities. As one of the key solutions within the
framework of the architecture, a universal method of knowledge representation
is proposed, which allows combining various non-formalized, partially and fully
formalized methods of knowledge representation in a single knowledge base, such
as texts in natural languages, images, audio and video recordings, graphs,
algorithms, databases, neural networks, knowledge graphs, ontologies, frames,
essence-property-relation models, production systems, predicate calculus
models, conceptual models, and others. To combine and structure various
fragments of knowledge, archigraph models are used, constructed as a
development of annotated metagraphs. As components, the cognitive architecture
being developed includes machine consciousness, machine subconsciousness,
blocks of interaction with the external environment, a goal management block,
an emotional control system, a block of social interaction, a block of
reflection, an ethics block and a worldview block, a learning block, a
monitoring block, blocks of statement and solving problems, self-organization
and meta learning block.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.08552">Explaining Time Series via Contrastive and Locally Sparse Perturbations. (arXiv:2401.08552v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zichuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yingying Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tianchun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zefan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1">Dongsheng Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1">Mengnan Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1">Min Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chunlin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1">Lunting Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_Q/0/1/0/all/0/1">Qingsong Wen</a></p>
<p>Explaining multivariate time series is a compound challenge, as it requires
identifying important locations in the time series and matching complex
temporal patterns. Although previous saliency-based methods addressed the
challenges, their perturbation may not alleviate the distribution shift issue,
which is inevitable especially in heterogeneous samples. We present ContraLSP,
a locally sparse model that introduces counterfactual samples to build
uninformative perturbations but keeps distribution using contrastive learning.
Furthermore, we incorporate sample-specific sparse gates to generate more
binary-skewed and smooth masks, which easily integrate temporal trends and
select the salient features parsimoniously. Empirical studies on both synthetic
and real-world datasets show that ContraLSP outperforms state-of-the-art
models, demonstrating a substantial improvement in explanation quality for time
series data. The source code is available at
\url{https://github.com/zichuan-liu/ContraLSP}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.10282">BioDiffusion: A Versatile Diffusion Model for Biomedical Signal Synthesis. (arXiv:2401.10282v2 [eess.SP] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1">Xiaomin Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Sakevych_M/0/1/0/all/0/1">Mykhailo Sakevych</a>, <a href="http://arxiv.org/find/eess/1/au:+Atkinson_G/0/1/0/all/0/1">Gentry Atkinson</a>, <a href="http://arxiv.org/find/eess/1/au:+Metsis_V/0/1/0/all/0/1">Vangelis Metsis</a></p>
<p>Machine learning tasks involving biomedical signals frequently grapple with
issues such as limited data availability, imbalanced datasets, labeling
complexities, and the interference of measurement noise. These challenges often
hinder the optimal training of machine learning algorithms. Addressing these
concerns, we introduce BioDiffusion, a diffusion-based probabilistic model
optimized for the synthesis of multivariate biomedical signals. BioDiffusion
demonstrates excellence in producing high-fidelity, non-stationary,
multivariate signals for a range of tasks including unconditional,
label-conditional, and signal-conditional generation. Leveraging these
synthesized signals offers a notable solution to the aforementioned challenges.
Our research encompasses both qualitative and quantitative assessments of the
synthesized data quality, underscoring its capacity to bolster accuracy in
machine learning tasks tied to biomedical signals. Furthermore, when juxtaposed
with current leading time-series generative models, empirical evidence suggests
that BioDiffusion outperforms them in biomedical signal generation quality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.10711">Weakly Supervised Gaussian Contrastive Grounding with Large Multimodal Models for Video Question Answering. (arXiv:2401.10711v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haibo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1">Chenghang Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yixuan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1">Weifeng Ge</a></p>
<p>Video Question Answering (VideoQA) aims to answer natural language questions
based on the information observed in videos. Despite the recent success of
Large Multimodal Models (LMMs) in image-language understanding and reasoning,
they deal with VideoQA insufficiently by simply taking uniformly sampled frames
as visual inputs, which ignores question-relevant visual clues. Moreover, there
are no human annotations for question-critical timestamps in existing VideoQA
datasets. In light of this, we propose a novel weakly supervised framework to
enforce the LMMs to reason out the answers with question-critical moments as
visual inputs. Specifically, we fuse the question and answer pairs as event
descriptions to find multiple keyframes as target moments, which will be
pseudo-labels. With these pseudo-labels as additionally weak supervision, we
devise a lightweight Gaussian-based Contrastive Grounding (GCG) module. GCG
learns multiple Gaussian functions to characterize the temporal structure of
the video, and sample question-critical frames as positive moments to be the
visual inputs of LMMs. Extensive experiments on several VideoQA benchmarks
verify the effectiveness of our framework, and we achieve substantial
improvements compared to previous state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11113">SleepNet: Attention-Enhanced Robust Sleep Prediction using Dynamic Social Networks. (arXiv:2401.11113v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khalid_M/0/1/0/all/0/1">Maryam Khalid</a>, <a href="http://arxiv.org/find/cs/1/au:+Klerman_E/0/1/0/all/0/1">Elizabeth B. Klerman</a>, <a href="http://arxiv.org/find/cs/1/au:+Mchill_A/0/1/0/all/0/1">Andrew W. Mchill</a>, <a href="http://arxiv.org/find/cs/1/au:+Phillips_A/0/1/0/all/0/1">Andrew J. K. Phillips</a>, <a href="http://arxiv.org/find/cs/1/au:+Sano_A/0/1/0/all/0/1">Akane Sano</a></p>
<p>Sleep behavior significantly impacts health and acts as an indicator of
physical and mental well-being. Monitoring and predicting sleep behavior with
ubiquitous sensors may therefore assist in both sleep management and tracking
of related health conditions. While sleep behavior depends on, and is reflected
in the physiology of a person, it is also impacted by external factors such as
digital media usage, social network contagion, and the surrounding weather. In
this work, we propose SleepNet, a system that exploits social contagion in
sleep behavior through graph networks and integrates it with physiological and
phone data extracted from ubiquitous mobile and wearable devices for predicting
next-day sleep labels about sleep duration. Our architecture overcomes the
limitations of large-scale graphs containing connections irrelevant to sleep
behavior by devising an attention mechanism. The extensive experimental
evaluation highlights the improvement provided by incorporating social networks
in the model. Additionally, we conduct robustness analysis to demonstrate the
system's performance in real-life conditions. The outcomes affirm the stability
of SleepNet against perturbations in input data. Further analyses emphasize the
significance of network topology in prediction performance revealing that users
with higher eigenvalue centrality are more vulnerable to data perturbations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11156">Generalizing Speaker Verification for Spoof Awareness in the Embedding Space. (arXiv:2401.11156v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xuechen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sahidullah_M/0/1/0/all/0/1">Md Sahidullah</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kong Aik Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kinnunen_T/0/1/0/all/0/1">Tomi Kinnunen</a></p>
<p>It is now well-known that automatic speaker verification (ASV) systems can be
spoofed using various types of adversaries. The usual approach to counteract
ASV systems against such attacks is to develop a separate spoofing
countermeasure (CM) module to classify speech input either as a bonafide, or a
spoofed utterance. Nevertheless, such a design requires additional computation
and utilization efforts at the authentication stage. An alternative strategy
involves a single monolithic ASV system designed to handle both zero-effort
imposter (non-targets) and spoofing attacks. Such spoof-aware ASV systems have
the potential to provide stronger protections and more economic computations.
To this end, we propose to generalize the standalone ASV (G-SASV) against
spoofing attacks, where we leverage limited training data from CM to enhance a
simple backend in the embedding space, without the involvement of a separate CM
module during the test (authentication) phase. We propose a novel yet simple
backend classifier based on deep neural networks and conduct the study via
domain adaptation and multi-task integration of spoof embeddings at the
training stage. Experiments are conducted on the ASVspoof 2019 logical access
dataset, where we improve the performance of statistical ASV backends on the
joint (bonafide and spoofed) and spoofed conditions by a maximum of 36.2% and
49.8% in terms of equal error rates, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11257">Measuring Policy Distance for Multi-Agent Reinforcement Learning. (arXiv:2401.11257v2 [cs.MA] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1">Tianyi Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pu_Z/0/1/0/all/0/1">Zhiqiang Pu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ai_X/0/1/0/all/0/1">Xiaolin Ai</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_T/0/1/0/all/0/1">Tenghai Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1">Jianqiang Yi</a></p>
<p>Diversity plays a crucial role in improving the performance of multi-agent
reinforcement learning (MARL). Currently, many diversity-based methods have
been developed to overcome the drawbacks of excessive parameter sharing in
traditional MARL. However, there remains a lack of a general metric to quantify
policy differences among agents. Such a metric would not only facilitate the
evaluation of the diversity evolution in multi-agent systems, but also provide
guidance for the design of diversity-based MARL algorithms. In this paper, we
propose the multi-agent policy distance (MAPD), a general tool for measuring
policy differences in MARL. By learning the conditional representations of
agents' decisions, MAPD can computes the policy distance between any pair of
agents. Furthermore, we extend MAPD to a customizable version, which can
quantify differences among agent policies on specified aspects. Based on the
online deployment of MAPD, we design a multi-agent dynamic parameter sharing
(MADPS) algorithm as an example of the MAPD's applications. Extensive
experiments demonstrate that our method is effective in measuring differences
in agent policies and specific behavioral tendencies. Moreover, in comparison
to other methods of parameter sharing, MADPS exhibits superior performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11414">S$^3$M-Net: Joint Learning of Semantic Segmentation and Stereo Matching for Autonomous Driving. (arXiv:2401.11414v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhiyuan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yi Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chuang-Wei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1">Fisher Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Qijun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1">Rui Fan</a></p>
<p>Semantic segmentation and stereo matching are two essential components of 3D
environmental perception systems for autonomous driving. Nevertheless,
conventional approaches often address these two problems independently,
employing separate models for each task. This approach poses practical
limitations in real-world scenarios, particularly when computational resources
are scarce or real-time performance is imperative. Hence, in this article, we
introduce S$^3$M-Net, a novel joint learning framework developed to perform
semantic segmentation and stereo matching simultaneously. Specifically,
S$^3$M-Net shares the features extracted from RGB images between both tasks,
resulting in an improved overall scene understanding capability. This feature
sharing process is realized using a feature fusion adaption (FFA) module, which
effectively transforms the shared features into semantic space and subsequently
fuses them with the encoded disparity features. The entire joint learning
framework is trained by minimizing a novel semantic consistency-guided (SCG)
loss, which places emphasis on the structural consistency in both tasks.
Extensive experimental results conducted on the vKITTI2 and KITTI datasets
demonstrate the effectiveness of our proposed joint learning framework and its
superior performance compared to other state-of-the-art single-task networks.
Our project webpage is accessible at mias.group/S3M-Net.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11648">Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal Contrastive EHR Modelling with Hierarchical Regularisation. (arXiv:2401.11648v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Koo_H/0/1/0/all/0/1">Heejoon Koo</a></p>
<p>Predicting next visit diagnosis using Electronic Health Records (EHR) is an
essential task in healthcare, critical for devising proactive future plans for
both healthcare providers and patients. Nonetheless, many preceding studies
have not sufficiently addressed the heterogeneous and hierarchical
characteristics inherent in EHR data, inevitably leading to sub-optimal
performance. To this end, we propose NECHO, a novel medical code-centric
multimodal contrastive EHR learning framework with hierarchical regularisation.
First, we integrate multifaceted information encompassing medical codes,
demographics, and clinical notes using a tailored network design and a pair of
bimodal contrastive losses, all of which pivot around a medical codes
representation. We also regularise modality-specific encoders using a parental
level information in medical ontology to learn hierarchical structure of EHR
data. A series of experiments on MIMIC-III data demonstrates effectiveness of
our approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11723">Unraveling Attacks in Machine Learning-based IoT Ecosystems: A Survey and the Open Libraries Behind Them. (arXiv:2401.11723v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Boxi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1">Wei Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chris Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1">Kelvin Wong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yi Zhang</a></p>
<p>The advent of the Internet of Things (IoT) has brought forth an era of
unprecedented connectivity, with an estimated 80 billion smart devices expected
to be in operation by the end of 2025. These devices facilitate a multitude of
smart applications, enhancing the quality of life and efficiency across various
domains. Machine Learning (ML) serves as a crucial technology, not only for
analyzing IoT-generated data but also for diverse applications within the IoT
ecosystem. For instance, ML finds utility in IoT device recognition, anomaly
detection, and even in uncovering malicious activities. This paper embarks on a
comprehensive exploration of the security threats arising from ML's integration
into various facets of IoT, spanning various attack types including membership
inference, adversarial evasion, reconstruction, property inference, model
extraction, and poisoning attacks. Unlike previous studies, our work offers a
holistic perspective, categorizing threats based on criteria such as adversary
models, attack targets, and key security attributes (confidentiality,
availability, and integrity). We delve into the underlying techniques of ML
attacks in IoT environment, providing a critical evaluation of their mechanisms
and impacts. Furthermore, our research thoroughly assesses 65 libraries, both
author-contributed and third-party, evaluating their role in safeguarding model
and data privacy. We emphasize the availability and usability of these
libraries, aiming to arm the community with the necessary tools to bolster
their defenses against the evolving threat landscape. Through our comprehensive
review and analysis, this paper seeks to contribute to the ongoing discourse on
ML-based IoT security, offering valuable insights and practical solutions to
secure ML models and data in the rapidly expanding field of artificial
intelligence in IoT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11792">Safe and Generalized end-to-end Autonomous Driving System with Reinforcement Learning and Demonstrations. (arXiv:2401.11792v4 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1">Zuojin Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xiaoyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">YongQiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jianyu Chen</a></p>
<p>An intelligent driving system should be capable of dynamically formulating
appropriate driving strategies based on the current environment and vehicle
status, while ensuring the security and reliability of the system. However,
existing methods based on reinforcement learning and imitation learning suffer
from low safety, poor generalization, and inefficient sampling. Additionally,
they cannot accurately predict future driving trajectories, and the accurate
prediction of future driving trajectories is a precondition for making optimal
decisions. To solve these problems, in this paper, we introduce a Safe and
Generalized end-to-end Autonomous Driving System (SGADS) for complex and
various scenarios. Our SGADS incorporates variational inference with
normalizing flows, enabling the intelligent vehicle to accurately predict
future driving trajectories. Moreover, we propose the formulation of robust
safety constraints. Furthermore, we combine reinforcement learning with
demonstrations to augment search process of the agent. The experimental results
demonstrate that our SGADS can significantly improve safety performance,
exhibit strong generalization, and enhance the training efficiency of
intelligent vehicles in complex urban scenarios compared to existing methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11798">Knowledge Distillation on Spatial-Temporal Graph Convolutional Network for Traffic Prediction. (arXiv:2401.11798v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Izadi_M/0/1/0/all/0/1">Mohammad Izadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Safayani_M/0/1/0/all/0/1">Mehran Safayani</a>, <a href="http://arxiv.org/find/cs/1/au:+Mirzaei_A/0/1/0/all/0/1">Abdolreza Mirzaei</a></p>
<p>Efficient real-time traffic prediction is crucial for reducing transportation
time. To predict traffic conditions, we employ a spatio-temporal graph neural
network (ST-GNN) to model our real-time traffic data as temporal graphs.
Despite its capabilities, it often encounters challenges in delivering
efficient real-time predictions for real-world traffic data. Recognizing the
significance of timely prediction due to the dynamic nature of real-time data,
we employ knowledge distillation (KD) as a solution to enhance the execution
time of ST-GNNs for traffic prediction. In this paper, We introduce a cost
function designed to train a network with fewer parameters (the student) using
distilled data from a complex network (the teacher) while maintaining its
accuracy close to that of the teacher. We use knowledge distillation,
incorporating spatial-temporal correlations from the teacher network to enable
the student to learn the complex patterns perceived by the teacher. However, a
challenge arises in determining the student network architecture rather than
considering it inadvertently. To address this challenge, we propose an
algorithm that utilizes the cost function to calculate pruning scores,
addressing small network architecture search issues, and jointly fine-tunes the
network resulting from each pruning stage using KD. Ultimately, we evaluate our
proposed ideas on two real-world datasets, PeMSD7 and PeMSD8. The results
indicate that our method can maintain the student's accuracy close to that of
the teacher, even with the retention of only $3\%$ of network parameters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11864">Improving Small Language Models&#x27; Mathematical Reasoning via Equation-of-Thought Distillation. (arXiv:2401.11864v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xunyu Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jian Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1">Can Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Weiping Wang</a></p>
<p>This work addresses the challenge of democratizing advanced Large Language
Models (LLMs) by compressing their mathematical reasoning capabilities into
sub-billion parameter Small Language Models (SLMs) without compromising
performance. We introduce Equation-of-Thought Distillation (EoTD), a novel
technique that encapsulates the reasoning process into equation-based
representations to construct an EoTD dataset for fine-tuning SLMs.
Additionally, we propose the Ensemble Thoughts Distillation (ETD) framework to
enhance the reasoning performance of SLMs. This involves creating a reasoning
dataset with multiple thought processes, including Chain-of-Thought (CoT),
Program-of-Thought (PoT), and Equation-of-Thought (EoT), and using it for
fine-tuning. Our experimental findings demonstrate that EoTD significantly
boosts the reasoning abilities of SLMs, while ETD enables these models to
achieve state-of-the-art reasoning performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.12007">Tensor-view Topological Graph Neural Network. (arXiv:2401.12007v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wen_T/0/1/0/all/0/1">Tao Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1">Elynn Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuzhou Chen</a></p>
<p>Graph classification is an important learning task for graph-structured data.
Graph neural networks (GNNs) have recently gained growing attention in graph
learning and have shown significant improvements in many important graph
problems. Despite their state-of-the-art performances, existing GNNs only use
local information from a very limited neighborhood around each node, suffering
from loss of multi-modal information and overheads of excessive computation. To
address these issues, we propose a novel Tensor-view Topological Graph Neural
Network (TTG-NN), a class of simple yet effective topological deep learning
built upon persistent homology, graph convolution, and tensor operations. This
new method incorporates tensor learning to simultaneously capture Tensor-view
Topological (TT), as well as Tensor-view Graph (TG) structural information on
both local and global levels. Computationally, to fully exploit graph topology
and structure, we propose two flexible TT and TG representation learning
modules that disentangle feature tensor aggregation and transformation and
learn to preserve multi-modal structure with less computation. Theoretically,
we derive high probability bounds on both the out-of-sample and in-sample mean
squared approximation errors for our proposed Tensor Transformation Layer
(TTL). Real data experiments show that the proposed TTG-NN outperforms 20
state-of-the-art methods on various graph benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.12665">ClipSAM: CLIP and SAM Collaboration for Zero-Shot Anomaly Segmentation. (arXiv:2401.12665v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shengze Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1">Jianjian Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_P/0/1/0/all/0/1">Peng Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1">Yuhan Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_C/0/1/0/all/0/1">Chongjun Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tao Chen</a></p>
<p>Recently, foundational models such as CLIP and SAM have shown promising
performance for the task of Zero-Shot Anomaly Segmentation (ZSAS). However,
either CLIP-based or SAM-based ZSAS methods still suffer from non-negligible
key drawbacks: 1) CLIP primarily focuses on global feature alignment across
different inputs, leading to imprecise segmentation of local anomalous parts;
2) SAM tends to generate numerous redundant masks without proper prompt
constraints, resulting in complex post-processing requirements. In this work,
we innovatively propose a CLIP and SAM collaboration framework called ClipSAM
for ZSAS. The insight behind ClipSAM is to employ CLIP's semantic understanding
capability for anomaly localization and rough segmentation, which is further
used as the prompt constraints for SAM to refine the anomaly segmentation
results. In details, we introduce a crucial Unified Multi-scale Cross-modal
Interaction (UMCI) module for interacting language with visual features at
multiple scales of CLIP to reason anomaly positions. Then, we design a novel
Multi-level Mask Refinement (MMR) module, which utilizes the positional
information as multi-level prompts for SAM to acquire hierarchical levels of
masks and merges them. Extensive experiments validate the effectiveness of our
approach, achieving the optimal segmentation performance on the MVTec-AD and
VisA datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13034">Locality Sensitive Sparse Encoding for Learning World Models Online. (arXiv:2401.13034v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zichen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1">Chao Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1">Wee Sun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1">Min Lin</a></p>
<p>Acquiring an accurate world model online for model-based reinforcement
learning (MBRL) is challenging due to data nonstationarity, which typically
causes catastrophic forgetting for neural networks (NNs). From the online
learning perspective, a Follow-The-Leader (FTL) world model is desirable, which
optimally fits all previous experiences at each round. Unfortunately, NN-based
models need re-training on all accumulated data at every interaction step to
achieve FTL, which is computationally expensive for lifelong agents. In this
paper, we revisit models that can achieve FTL with incremental updates.
Specifically, our world model is a linear regression model supported by
nonlinear random features. The linear part ensures efficient FTL update while
the nonlinear random feature empowers the fitting of complex environments. To
best trade off model capacity and computation efficiency, we introduce a
locality sensitive sparse encoding, which allows us to conduct efficient sparse
updates even with very high dimensional nonlinear features. We validate the
representation power of our encoding and verify that it allows efficient online
learning under data covariate shift. We also show, in the Dyna MBRL setting,
that our world models learned online using a single pass of trajectory data
either surpass or match the performance of deep world models trained with
replay and other continual learning methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13098">Gravity-Informed Deep Learning Framework for Predicting Ship Traffic Flow and Invasion Risk of Non-Indigenous Species via Ballast Water Discharge. (arXiv:2401.13098v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1">Ruixin Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Spadon_G/0/1/0/all/0/1">Gabriel Spadon</a>, <a href="http://arxiv.org/find/cs/1/au:+Pelot_R/0/1/0/all/0/1">Ronald Pelot</a>, <a href="http://arxiv.org/find/cs/1/au:+Matwin_S/0/1/0/all/0/1">Stan Matwin</a>, <a href="http://arxiv.org/find/cs/1/au:+Soares_A/0/1/0/all/0/1">Amilcar Soares</a></p>
<p>Invasive species in water bodies pose a major threat to the environment and
biodiversity globally. Due to increased transportation and trade, non-native
species have been introduced to new environments, causing damage to ecosystems
and leading to economic losses in agriculture, forestry, and fisheries.
Therefore, there is a pressing need for risk assessment and management
techniques to mitigate the impact of these invasions. This study aims to
develop a new physics-inspired model to forecast maritime shipping traffic and
thus inform risk assessment of invasive species spread through global
transportation networks. Inspired by the gravity model for international
trades, our model considers various factors that influence the likelihood and
impact of vessel activities, such as shipping flux density, distance between
ports, trade flow, and centrality measures of transportation hubs.
Additionally, by analyzing the risk network of invasive species, we provide a
comprehensive framework for assessing the invasion threat level given a pair of
origin and destination. Accordingly, this paper introduces transformers to
gravity models to rebuild the short- and long-term dependencies that make the
risk analysis feasible. Thus, we introduce a physics-inspired framework that
achieves an 89% segmentation accuracy for existing and non-existing
trajectories and an 84.8% accuracy for the number of vessels flowing between
key port areas, representing more than 10% improvement over the traditional
deep-gravity model. Along these lines, this research contributes to a better
understanding of invasive species risk assessment. It allows policymakers,
conservationists, and stakeholders to prioritize management actions by
identifying high-risk invasion pathways. Besides, our model is versatile and
can include new data sources, making it suitable for assessing species invasion
risks in a changing global landscape.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13112">DISCOUNT: Distributional Counterfactual Explanation With Optimal Transport. (arXiv:2401.13112v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+You_L/0/1/0/all/0/1">Lei You</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1">Lele Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Nilsson_M/0/1/0/all/0/1">Mattias Nilsson</a></p>
<p>Counterfactual Explanations (CE) is the de facto method for providing insight
and interpretability in black-box decision-making models by identifying
alternative input instances that lead to different outcomes. This paper extends
the concept of CEs to a distributional context, broadening the scope from
individual data points to entire input and output distributions, named
Distributional Counterfactual Explanation (DCE). In DCE, our focus shifts to
analyzing the distributional properties of the factual and counterfactual,
drawing parallels to the classical approach of assessing individual instances
and their resulting decisions. We leverage Optimal Transport (OT) to frame a
chance-constrained optimization problem, aiming to derive a counterfactual
distribution that closely aligns with its factual counterpart, substantiated by
statistical confidence. Our proposed optimization method, DISCOUNT,
strategically balances this confidence across both input and output
distributions. This algorithm is accompanied by an analysis of its convergence
rate. The efficacy of our proposed method is substantiated through a series of
illustrative case studies, highlighting its potential in providing deep
insights into decision-making models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13275">Can AI Assistants Know What They Don&#x27;t Know?. (arXiv:2401.13275v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1">Qinyuan Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1">Tianxiang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiangyang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1">Zhangyue Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shimin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Linyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1">Zhengfu He</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Kai Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1">Xipeng Qiu</a></p>
<p>Recently, AI assistants based on large language models (LLMs) show surprising
performance in many tasks, such as dialogue, solving math problems, writing
code, and using tools. Although LLMs possess intensive world knowledge, they
still make factual errors when facing some knowledge intensive tasks, like
open-domain question answering. These untruthful responses from the AI
assistant may cause significant risks in practical applications. We believe
that an AI assistant's refusal to answer questions it does not know is a
crucial method for reducing hallucinations and making the assistant truthful.
Therefore, in this paper, we ask the question "Can AI assistants know what they
don't know and express them through natural language?" To answer this question,
we construct a model-specific "I don't know" (Idk) dataset for an assistant,
which contains its known and unknown questions, based on existing open-domain
question answering datasets. Then we align the assistant with its corresponding
Idk dataset and observe whether it can refuse to answer its unknown questions
after alignment. Experimental results show that after alignment with Idk
datasets, the assistant can refuse to answer most its unknown questions. For
questions they attempt to answer, the accuracy is significantly higher than
before the alignment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13324">Information That Matters: Exploring Information Needs of People Affected by Algorithmic Decisions. (arXiv:2401.13324v4 [cs.HC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schmude_T/0/1/0/all/0/1">Timoth&#xe9;e Schmude</a>, <a href="http://arxiv.org/find/cs/1/au:+Koesten_L/0/1/0/all/0/1">Laura Koesten</a>, <a href="http://arxiv.org/find/cs/1/au:+Moller_T/0/1/0/all/0/1">Torsten M&#xf6;ller</a>, <a href="http://arxiv.org/find/cs/1/au:+Tschiatschek_S/0/1/0/all/0/1">Sebastian Tschiatschek</a></p>
<p>Explanations of AI systems rarely address the information needs of people
affected by algorithmic decision-making (ADM). This gap between conveyed
information and information that matters to affected stakeholders can impede
understanding and adherence to regulatory frameworks such as the AI Act. To
address this gap, we present the "XAI Novice Question Bank": A catalog of
affected stakeholders' information needs in two ADM use cases (employment
prediction and health monitoring), covering the categories data, system
context, system usage, and system specifications. Information needs were
gathered in an interview study where participants received explanations in
response to their inquiries. Participants further reported their understanding
and decision confidence, showing that while confidence tended to increase after
receiving explanations, participants also met understanding challenges, such as
being unable to tell why their understanding felt incomplete. Explanations
further influenced participants' perceptions of the systems' risks and
benefits, which they confirmed or changed depending on the use case. When risks
were perceived as high, participants expressed particular interest in
explanations about intention, such as why and to what end a system was put in
place. With this work, we aim to support the inclusion of affected stakeholders
into explainability by contributing an overview of information and challenges
relevant to them when deciding on the adoption of ADM systems. We close by
summarizing our findings in a list of six key implications that inform the
design of future explanations for affected stakeholder audiences.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13802">Investigating the Efficacy of Large Language Models for Code Clone Detection. (arXiv:2401.13802v2 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khajezade_M/0/1/0/all/0/1">Mohamad Khajezade</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jie JW Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fard_F/0/1/0/all/0/1">Fatemeh Hendijani Fard</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_Perez_G/0/1/0/all/0/1">Gema Rodr&#xed;guez-P&#xe9;rez</a>, <a href="http://arxiv.org/find/cs/1/au:+Shehata_M/0/1/0/all/0/1">Mohamed Sami Shehata</a></p>
<p>Large Language Models (LLMs) have demonstrated remarkable success in various
natural language processing and software engineering tasks, such as code
generation. The LLMs are mainly utilized in the prompt-based zero/few-shot
paradigm to guide the model in accomplishing the task. GPT-based models are one
of the popular ones studied for tasks such as code comment generation or test
generation. These tasks are `generative' tasks. However, there is limited
research on the usage of LLMs for `non-generative' tasks such as classification
using the prompt-based paradigm. In this preliminary exploratory study, we
investigated the applicability of LLMs for Code Clone Detection (CCD), a
non-generative task. By building a mono-lingual and cross-lingual CCD dataset
derived from CodeNet, we first investigated two different prompts using ChatGPT
to detect Type-4 code clones in Java-Java and Java-Ruby pairs in a zero-shot
setting. We then conducted an analysis to understand the strengths and
weaknesses of ChatGPT in CCD. ChatGPT surpasses the baselines in cross-language
CCD attaining an F1-score of 0.877 and achieves comparable performance to fully
fine-tuned models for mono-lingual CCD, with an F1-score of 0.878. Also, the
prompt and the difficulty level of the problems has an impact on the
performance of ChatGPT. Finally we provide insights and future directions based
on our initial analysis
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13919">WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models. (arXiv:2401.13919v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1">Hongliang He</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1">Wenlin Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1">Kaixin Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1">Wenhao Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1">Yong Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1">Zhenzhong Lan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1">Dong Yu</a></p>
<p>The advancement of large language models (LLMs) leads to a new era marked by
the development of autonomous applications in the real world, which drives
innovation in the creation of advanced web-based agents. Existing web agents
typically only handle one input modality and are evaluated only in simplified
web simulators or static web snapshots, greatly limiting their applicability in
real-world scenarios. To bridge this gap, we introduce WebVoyager, an
innovative Large Multimodal Model (LMM) powered web agent that can complete
user instructions end-to-end by interacting with real-world websites. Moreover,
we propose a new evaluation protocol for web agents to address the challenges
of automatic evaluation of open-ended web agent tasks, leveraging the robust
multimodal comprehension capabilities of GPT-4V. We create a new benchmark by
gathering real-world tasks from 15 widely used websites to evaluate our agents.
We show that WebVoyager achieves a 55.7% task success rate, significantly
surpassing the performance of both GPT-4 (All Tools) and the WebVoyager
(text-only) setups, underscoring the exceptional capability of WebVoyager in
practical applications. We found that our proposed automatic evaluation
achieves 85.3% agreement with human judgment, paving the way for further
development of web agents in a real-world setting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14166">BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on Few-shot Inference via Debiased Domain Abstraction. (arXiv:2401.14166v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiangmeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_F/0/1/0/all/0/1">Fei Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1">Yifan Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiang_W/0/1/0/all/0/1">Wenwen Qiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1">Changwen Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1">Fuchun Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1">Hui Xiong</a></p>
<p>As a novel and effective fine-tuning paradigm based on large-scale
pre-trained language models (PLMs), prompt-tuning aims to reduce the gap
between downstream tasks and pre-training objectives. While prompt-tuning has
yielded continuous advancements in various tasks, such an approach still
remains a persistent defect: prompt-tuning methods fail to generalize to
specific few-shot patterns. From the perspective of distribution analyses, we
disclose that the intrinsic issues behind the phenomenon are the
over-multitudinous conceptual knowledge contained in PLMs and the abridged
knowledge for target downstream domains, which jointly result in that PLMs
mis-locate the knowledge distributions corresponding to the target domains in
the universal knowledge embedding space. To this end, we intuitively explore to
approximate the unabridged target domains of downstream tasks in a debiased
manner, and then abstract such domains to generate discriminative prompts,
thereby providing the de-ambiguous guidance for PLMs. Guided by such an
intuition, we propose a simple yet effective approach, namely BayesPrompt, to
learn prompts that contain the domain discriminative information against the
interference from domain-irrelevant knowledge. BayesPrompt primitively
leverages known distributions to approximate the debiased factual distributions
of target domains and further uniformly samples certain representative features
from the approximated distributions to generate the ultimate prompts for PLMs.
We provide theoretical insights with the connection to domain adaptation.
Empirically, our method achieves state-of-the-art performance on benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14215">Commonsense-augmented Memory Construction and Management in Long-term Conversations via Context-aware Persona Refinement. (arXiv:2401.14215v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">Hana Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Ong_K/0/1/0/all/0/1">Kai Tzu-iunn Ong</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Seoyeon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1">Dongha Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Yeo_J/0/1/0/all/0/1">Jinyoung Yeo</a></p>
<p>Memorizing and utilizing speakers' personas is a common practice for response
generation in long-term conversations. Yet, human-authored datasets often
provide uninformative persona sentences that hinder response quality. This
paper presents a novel framework that leverages commonsense-based persona
expansion to address such issues in long-term conversation. While prior work
focuses on not producing personas that contradict others, we focus on
transforming contradictory personas into sentences that contain rich speaker
information, by refining them based on their contextual backgrounds with
designed strategies. As the pioneer of persona expansion in multi-session
settings, our framework facilitates better response generation via human-like
persona refinement. The supplementary video of our work is available at
https://caffeine-15bbf.web.app/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14257">Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation. (arXiv:2401.14257v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Minglin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1">Weihao Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yukun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sheng_Z/0/1/0/all/0/1">Zhe Sheng</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yisheng He</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1">Zilong Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Bo_L/0/1/0/all/0/1">Liefeng Bo</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yulan Guo</a></p>
<p>Recently, text-to-3D approaches have achieved high-fidelity 3D content
generation using text description. However, the generated objects are
stochastic and lack fine-grained control. Sketches provide a cheap approach to
introduce such fine-grained control. Nevertheless, it is challenging to achieve
flexible control from these sketches due to their abstraction and ambiguity. In
this paper, we present a multi-view sketch-guided text-to-3D generation
framework (namely, Sketch2NeRF) to add sketch control to 3D generation.
Specifically, our method leverages pretrained 2D diffusion models (e.g., Stable
Diffusion and ControlNet) to supervise the optimization of a 3D scene
represented by a neural radiance field (NeRF). We propose a novel synchronized
generation and reconstruction method to effectively optimize the NeRF. In the
experiments, we collected two kinds of multi-view sketch datasets to evaluate
the proposed method. We demonstrate that our method can synthesize 3D
consistent contents with fine-grained sketch control while being high-fidelity
to text prompts. Extensive results show that our method achieves
state-of-the-art performance in terms of sketch similarity and text alignment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14403">Adaptive Mobile Manipulation for Articulated Objects In the Open World. (arXiv:2401.14403v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1">Haoyu Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Mendonca_R/0/1/0/all/0/1">Russell Mendonca</a>, <a href="http://arxiv.org/find/cs/1/au:+Shaw_K/0/1/0/all/0/1">Kenneth Shaw</a>, <a href="http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1">Deepak Pathak</a></p>
<p>Deploying robots in open-ended unstructured environments such as homes has
been a long-standing research problem. However, robots are often studied only
in closed-off lab settings, and prior mobile manipulation work is restricted to
pick-move-place, which is arguably just the tip of the iceberg in this area. In
this paper, we introduce Open-World Mobile Manipulation System, a full-stack
approach to tackle realistic articulated object operation, e.g. real-world
doors, cabinets, drawers, and refrigerators in open-ended unstructured
environments. The robot utilizes an adaptive learning framework to initially
learns from a small set of data through behavior cloning, followed by learning
from online practice on novel objects that fall outside the training
distribution. We also develop a low-cost mobile manipulation hardware platform
capable of safe and autonomous online adaptation in unstructured environments
with a cost of around 20,000 USD. In our experiments we utilize 20 articulate
objects across 4 buildings in the CMU campus. With less than an hour of online
learning for each object, the system is able to increase success rate from 50%
of BC pre-training to 95% using online adaptation. Video results at
https://open-world-mobilemanip.github.io/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14424">Discovering Mathematical Formulas from Data via GPT-guided Monte Carlo Tree Search. (arXiv:2401.14424v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yanjie Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Weijun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1">Lina Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1">Min Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jingyi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wenqiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hao_M/0/1/0/all/0/1">Meilan Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1">Shu Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1">Yusong Deng</a></p>
<p>Finding a concise and interpretable mathematical formula that accurately
describes the relationship between each variable and the predicted value in the
data is a crucial task in scientific research, as well as a significant
challenge in artificial intelligence. This problem is referred to as symbolic
regression, which is an NP-hard problem. Last year, a symbolic regression
method based on Monte Carlo Tree Search (MCTS) was proposed and sota was
obtained on multiple datasets. While this algorithm has shown considerable
improvement in recovering target expressions compared to previous methods, the
lack of guidance during the MCTS process severely hampers its search
efficiency. Recently, some algorithms have added a pre-trained policy network
to guide the search of MCTS, but the pre-trained policy network generalizes
poorly. To balance efficiency and generality, we propose SR-GPT combining ideas
from AlphaZero. SR-GPT is a new symbolic regression algorithm that combines
MCTS with a Generative Pre-Trained Transformer (GPT). By using GPT to guide the
MCTS process, the search efficiency of MCTS is significantly improved. Next, we
utilize the MCTS results to further refine the GPT, enhancing its capabilities
and providing more accurate guidance for the MCTS process. MCTS and GPT are
coupled together and optimize each other until the target expression is
successfully determined. We conducted extensive evaluations of SR-GPT using 222
expressions sourced from over 10 different symbolic regression datasets. The
experimental results demonstrate that SR-GPT outperforms existing
state-of-the-art algorithms in accurately recovering symbolic expressions both
with and without added noise.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14521">Towards Interpretable Physical-Conceptual Catchment-Scale Hydrological Modeling using the Mass-Conserving-Perceptron. (arXiv:2401.14521v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuan-Heng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1">Hoshin V. Gupta</a></p>
<p>We investigate the applicability of machine learning technologies to the
development of parsimonious, interpretable, catchment-scale hydrologic models
using directed-graph architectures based on the mass-conserving perceptron
(MCP) as the fundamental computational unit. Here, we focus on architectural
complexity (depth) at a single location, rather than universal applicability
(breadth) across large samples of catchments. The goal is to discover a minimal
representation (numbers of cell-states and flow paths) that represents the
dominant processes that can explain the input-state-output behaviors of a given
catchment, with particular emphasis given to simulating the full range (high,
medium, and low) of flow dynamics. We find that a HyMod-like architecture with
three cell-states and two major flow pathways achieves such a representation at
our study location, but that the additional incorporation of an input-bypass
mechanism significantly improves the timing and shape of the hydrograph, while
the inclusion of bi-directional groundwater mass exchanges significantly
enhances the simulation of baseflow. Overall, our results demonstrate the
importance of using multiple diagnostic metrics for model evaluation, while
highlighting the need for designing training metrics that are better suited to
extracting information across the full range of flow dynamics. Further, they
set the stage for interpretable regional-scale MCP-based hydrological modeling
(using large sample data) by using neural architecture search to determine
appropriate minimal representations for catchments in different hydroclimatic
regimes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14915">Charting the Future of AI in Project-Based Learning: A Co-Design Exploration with Students. (arXiv:2401.14915v2 [cs.HC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1">Chengbo Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1">Kangyu Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1">Bingcan Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Mogavi_R/0/1/0/all/0/1">Reza Hadi Mogavi</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1">Zhenhui Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1">Shuai Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xiaojuan Ma</a></p>
<p>The increasing use of Artificial Intelligence (AI) by students in learning
presents new challenges for assessing their learning outcomes in project-based
learning (PBL). This paper introduces a co-design study to explore the
potential of students' AI usage data as a novel material for PBL assessment. We
conducted workshops with 18 college students, encouraging them to speculate an
alternative world where they could freely employ AI in PBL while needing to
report this process to assess their skills and contributions. Our workshops
yielded various scenarios of students' use of AI in PBL and ways of analyzing
these uses grounded by students' vision of education goal transformation. We
also found students with different attitudes toward AI exhibited distinct
preferences in how to analyze and understand the use of AI. Based on these
findings, we discuss future research opportunities on student-AI interactions
and understanding AI-enhanced learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14279">ZS4C: Zero-Shot Synthesis of Compilable Code for Incomplete Code Snippets using ChatGPT. (arXiv:2401.14279v1 [cs.SE] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kabir_A/0/1/0/all/0/1">Azmain Kabir</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shaowei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yuan Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Tse-Hsun/0/1/0/all/0/1">Tse-Hsun</a> (Peter) <a href="http://arxiv.org/find/cs/1/au:+Chen/0/1/0/all/0/1">Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Asaduzzaman_M/0/1/0/all/0/1">Muhammad Asaduzzaman</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenbin Zhang</a></p>
<p>Technical question and answering (Q&amp;A) sites such as Stack Overflow have
become an important source for software developers to seek knowledge. However,
code snippets on Q&amp;A sites are usually uncompilable and semantically incomplete
for compilation due to unresolved types and missing dependent libraries, which
raises the obstacle for users to reuse or analyze Q&amp;A code snippets. Prior
approaches either are not designed for synthesizing compilable code or suffer
from a low compilation success rate. To address this problem, we propose ZS4C,
a lightweight approach to perform zero-shot synthesis of compilable code from
incomplete code snippets using Large Language Model (LLM). ZS4C operates in two
stages. In the first stage, ZS4C utilizes an LLM, i.e., ChatGPT, to identify
missing import statements for a given code snippet, leveraging our designed
task-specific prompt template. In the second stage, ZS4C fixes compilation
errors caused by incorrect import statements and syntax errors through
collaborative work between ChatGPT and a compiler. We thoroughly evaluated ZS4C
on a widely used benchmark called StatType-SO against the SOTA approach SnR.
Compared with SnR, ZS4C improves the compilation rate from 63% to 87.6%, with a
39.3% improvement. On average, ZS4C can infer more accurate import statements
than SnR, with an improvement of 6.6% in the F1.
</p>
</p>
</div>

    </div>
    </body>
    