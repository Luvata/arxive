<!DOCTYPE html>
<html>
<head>
<title>2023-12-28-cs-lg</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2312.14935">AS-XAI: Self-supervised Automatic Semantic Interpretation for CNN. (arXiv:2312.14935v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1">Changqi Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuntian Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dongxiao Zhang</a></p>
<p>Explainable artificial intelligence (XAI) aims to develop transparent
explanatory approaches for "black-box" deep learning models. However,it remains
difficult for existing methods to achieve the trade-off of the three key
criteria in interpretability, namely, reliability, causality, and usability,
which hinder their practical applications. In this paper, we propose a
self-supervised automatic semantic interpretable explainable artificial
intelligence (AS-XAI) framework, which utilizes transparent orthogonal
embedding semantic extraction spaces and row-centered principal component
analysis (PCA) for global semantic interpretation of model decisions in the
absence of human interference, without additional computational costs. In
addition, the invariance of filter feature high-rank decomposition is used to
evaluate model sensitivity to different semantic concepts. Extensive
experiments demonstrate that robust and orthogonal semantic spaces can be
automatically extracted by AS-XAI, providing more effective global
interpretability for convolutional neural networks (CNNs) and generating
human-comprehensible explanations. The proposed approach offers broad
fine-grained extensible practical applications, including shared semantic
interpretation under out-of-distribution (OOD) categories, auxiliary
explanations for species that are challenging to distinguish, and
classification explanations from various perspectives.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14936">PerCNet: Periodic Complete Representation for Crystal Graphs. (arXiv:2312.14936v1 [cond-mat.mtrl-sci])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Huang_J/0/1/0/all/0/1">Jiao Huang</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Xing_Q/0/1/0/all/0/1">Qianli Xing</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Ji_J/0/1/0/all/0/1">Jinglong Ji</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Yang_B/0/1/0/all/0/1">Bo Yang</a></p>
<p>Crystal material representation is the foundation of crystal material
research. Existing works consider crystal molecules as graph data with
different representation methods and leverage the advantages of techniques in
graph learning. A reasonable crystal representation method should capture the
local and global information. However, existing methods only consider the local
information of crystal molecules by modeling the bond distance and bond angle
of first-order neighbors of atoms, which leads to the issue that different
crystals will have the same representation. To solve this many-to-one issue, we
consider the global information by further considering dihedral angles, which
can guarantee that the proposed representation corresponds one-to-one with the
crystal material. We first propose a periodic complete representation and
calculation algorithm for infinite extended crystal materials. A theoretical
proof for the representation that satisfies the periodic completeness is
provided. Based on the proposed representation, we then propose a network for
predicting crystal material properties, PerCNet, with a specially designed
message passing mechanism. Extensive experiments are conducted on two
real-world material benchmark datasets. The PerCNet achieves the best
performance among baseline methods in terms of MAE. In addition, our results
demonstrate the importance of the periodic scheme and completeness for crystal
representation learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14939">Large-scale Graph Representation Learning of Dynamic Brain Connectome with Transformers. (arXiv:2312.14939v1 [q-bio.NC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Kim_B/0/1/0/all/0/1">Byung-Hoon Kim</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Choi_J/0/1/0/all/0/1">Jungwon Choi</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Yun_E/0/1/0/all/0/1">EungGu Yun</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Kim_K/0/1/0/all/0/1">Kyungsang Kim</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Li_X/0/1/0/all/0/1">Xiang Li</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Lee_J/0/1/0/all/0/1">Juho Lee</a></p>
<p>Graph Transformers have recently been successful in various graph
representation learning tasks, providing a number of advantages over
message-passing Graph Neural Networks. Utilizing Graph Transformers for
learning the representation of the brain functional connectivity network is
also gaining interest. However, studies to date have underlooked the temporal
dynamics of functional connectivity, which fluctuates over time. Here, we
propose a method for learning the representation of dynamic functional
connectivity with Graph Transformers. Specifically, we define the connectome
embedding, which holds the position, structure, and time information of the
functional connectivity graph, and use Transformers to learn its representation
across time. We perform experiments with over 50,000 resting-state fMRI samples
obtained from three datasets, which is the largest number of fMRI data used in
studies by far. The experimental results show that our proposed method
outperforms other competitive baselines in gender classification and age
regression tasks based on the functional connectivity extracted from the fMRI
data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14941">Multi-Criteria Client Selection and Scheduling with Fairness Guarantee for Federated Learning Service. (arXiv:2312.14941v1 [cs.DC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Meiying Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Huan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ebron_S/0/1/0/all/0/1">Sheldon Ebron</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1">Ruitao Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1">Kan Yang</a></p>
<p>Federated Learning (FL) enables multiple clients to train machine learning
models collaboratively without sharing the raw training data. However, for a
given FL task, how to select a group of appropriate clients fairly becomes a
challenging problem due to budget restrictions and client heterogeneity. In
this paper, we propose a multi-criteria client selection and scheduling scheme
with a fairness guarantee, comprising two stages: 1) preliminary client pool
selection, and 2) per-round client scheduling. Specifically, we first define a
client selection metric informed by several criteria, such as client resources,
data quality, and client behaviors. Then, we formulate the initial client pool
selection problem into an optimization problem that aims to maximize the
overall scores of selected clients within a given budget and propose a greedy
algorithm to solve it. To guarantee fairness, we further formulate the
per-round client scheduling problem and propose a heuristic algorithm to divide
the client pool into several subsets such that every client is selected at
least once while guaranteeing that the `integrated' dataset in a subset is
close to an independent and identical distribution (iid). Our experimental
results show that our scheme can improve the model quality especially when data
are non-iid.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14943">Flood Event Extraction from News Media to Support Satellite-Based Flood Insurance. (arXiv:2312.14943v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pabari_T/0/1/0/all/0/1">Tejit Pabari</a>, <a href="http://arxiv.org/find/cs/1/au:+Tellman_B/0/1/0/all/0/1">Beth Tellman</a>, <a href="http://arxiv.org/find/cs/1/au:+Karamanolakis_G/0/1/0/all/0/1">Giannis Karamanolakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Thomas_M/0/1/0/all/0/1">Mitchell Thomas</a>, <a href="http://arxiv.org/find/cs/1/au:+Mauerman_M/0/1/0/all/0/1">Max Mauerman</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_E/0/1/0/all/0/1">Eugene Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lall_U/0/1/0/all/0/1">Upmanu Lall</a>, <a href="http://arxiv.org/find/cs/1/au:+Tedesco_M/0/1/0/all/0/1">Marco Tedesco</a>, <a href="http://arxiv.org/find/cs/1/au:+Steckler_M/0/1/0/all/0/1">Michael S Steckler</a>, <a href="http://arxiv.org/find/cs/1/au:+Colosio_P/0/1/0/all/0/1">Paolo Colosio</a>, <a href="http://arxiv.org/find/cs/1/au:+Osgood_D/0/1/0/all/0/1">Daniel E Osgood</a>, <a href="http://arxiv.org/find/cs/1/au:+Braun_M/0/1/0/all/0/1">Melody Braun</a>, <a href="http://arxiv.org/find/cs/1/au:+Bruijn_J/0/1/0/all/0/1">Jens de Bruijn</a>, <a href="http://arxiv.org/find/cs/1/au:+Islam_S/0/1/0/all/0/1">Shammun Islam</a></p>
<p>Floods cause large losses to property, life, and livelihoods across the world
every year, hindering sustainable development. Safety nets to help absorb
financial shocks in disasters, such as insurance, are often unavailable in
regions of the world most vulnerable to floods, like Bangladesh. Index-based
insurance has emerged as an affordable solution, which considers weather data
or information from satellites to create a "flood index" that should correlate
with the damage insured. However, existing flood event databases are often
incomplete, and satellite sensors are not reliable under extreme weather
conditions (e.g., because of clouds), which limits the spatial and temporal
resolution of current approaches for index-based insurance.
</p>
<p>In this work, we explore a novel approach for supporting satellite-based
flood index insurance by extracting high-resolution spatio-temporal information
from news media. First, we publish a dataset consisting of 40,000 news articles
covering flood events in Bangladesh by 10 prominent news sources, and inundated
area estimates for each division in Bangladesh collected from a satellite radar
sensor. Second, we show that keyword-based models are not adequate for this
novel application, while context-based classifiers cover complex and implicit
flood related patterns. Third, we show that time series extracted from news
media have substantial correlation Spearman's rho$=0.70 with satellite
estimates of inundated area. Our work demonstrates that news media is a
promising source for improving the temporal resolution and expanding the
spatial coverage of the available flood damage data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14948">An Evolving Population Approach to Data-Stream Classification with Extreme Verification Latency. (arXiv:2312.14948v1 [cs.NE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fahy_C/0/1/0/all/0/1">Conor Fahy</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Shengxiang Yang</a></p>
<p>Recognising and reacting to change in non-stationary data-streams is a
challenging task. The majority of research in this area assumes that the true
class label of incoming points are available, either at each time step or
intermittently with some latency. In the worse case this latency approaches
infinity and we can assume that no labels are available beyond the initial
training set. When change is expected and no further training labels are
provided the challenge of maintaining a high classification accuracy is very
great. The challenge is to propagate the original training information through
several timesteps, possibly indefinitely, while adapting to underlying change
in the data-stream. In this paper we conduct an initial study into the
effectiveness of using an evolving, population-based approach as the mechanism
for adapting to change. An ensemble of one-class-classifiers is maintained for
each class. Each classifier is considered as an agent in the sub-population and
is subject to selection pressure to find interesting areas of the feature
space. This selection pressure forces the ensemble to adapt to the underlying
change in the data-stream.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14954">Neuromorphic Co-Design as a Game. (arXiv:2312.14954v1 [cs.NE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vineyard_C/0/1/0/all/0/1">Craig M. Vineyard</a>, <a href="http://arxiv.org/find/cs/1/au:+Severa_W/0/1/0/all/0/1">William M. Severa</a>, <a href="http://arxiv.org/find/cs/1/au:+Aimone_J/0/1/0/all/0/1">James B. Aimone</a></p>
<p>Co-design is a prominent topic presently in computing, speaking to the mutual
benefit of coordinating design choices of several layers in the technology
stack. For example, this may be designing algorithms which can most efficiently
take advantage of the acceleration properties of a given architecture, while
simultaneously designing the hardware to support the structural needs of a
class of computation. The implications of these design decisions are
influential enough to be deemed a lottery, enabling an idea to win out over
others irrespective of the individual merits. Coordination is a well studied
topic in the mathematics of game theory, where in many cases without a
coordination mechanism the outcome is sub-optimal. Here we consider what
insights game theoretic analysis can offer for computer architecture co-design.
In particular, we consider the interplay between algorithm and architecture
advances in the field of neuromorphic computing. Analyzing developments of
spiking neural network algorithms and neuromorphic hardware as a co-design game
we use the Stag Hunt model to illustrate challenges for spiking algorithms or
architectures to advance the field independently and advocate for a strategic
pursuit to advance neuromorphic computing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14958">Graph Neural Network-Based Bandwidth Allocation for Secure Wireless Communications. (arXiv:2312.14958v1 [cs.IT])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hao_X/0/1/0/all/0/1">Xin Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yeoh_P/0/1/0/all/0/1">Phee Lep Yeoh</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuhong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+She_C/0/1/0/all/0/1">Changyang She</a>, <a href="http://arxiv.org/find/cs/1/au:+Vucetic_B/0/1/0/all/0/1">Branka Vucetic</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yonghui Li</a></p>
<p>This paper designs a graph neural network (GNN) to improve bandwidth
allocations for multiple legitimate wireless users transmitting to a base
station in the presence of an eavesdropper. To improve the privacy and prevent
eavesdropping attacks, we propose a user scheduling algorithm to schedule users
satisfying an instantaneous minimum secrecy rate constraint. Based on this, we
optimize the bandwidth allocations with three algorithms namely iterative
search (IvS), GNN-based supervised learning (GNN-SL), and GNN-based
unsupervised learning (GNN-USL). We present a computational complexity analysis
which shows that GNN-SL and GNN-USL can be more efficient compared to IvS which
is limited by the bandwidth block size. Numerical simulation results highlight
that our proposed GNN-based resource allocations can achieve a comparable sum
secrecy rate compared to IvS with significantly lower computational complexity.
Furthermore, we observe that the GNN approach is more robust to uncertainties
in the eavesdropper's channel state information, especially compared with the
best channel allocation scheme.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14963">Optimizing Mario Adventures in a Constrained Environment. (arXiv:2312.14963v1 [cs.NE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1">Sanyam Jain</a></p>
<p>This project proposes and compares a new way to optimise Super Mario Bros.
(SMB) environment where the control is in hand of two approaches, namely,
Genetic Algorithm (MarioGA) and NeuroEvolution (MarioNE). Not only we learn
playing SMB using these techniques, but also optimise it with constrains of
collection of coins and finishing levels. Firstly, we formalise the SMB agent
to maximize the total value of collected coins (reward) and maximising the
total distance traveled (reward) in order to finish the level faster (time
penalty) for both the algorithms. Secondly, we study MarioGA and its evaluation
function (fitness criteria) including its representation methods, crossover
used, mutation operator formalism, selection method used, MarioGA loop, and few
other parameters. Thirdly, MarioNE is applied on SMB where a population of ANNs
with random weights is generated, and these networks control Marios actions in
the game. Fourth, SMB is further constrained to complete the task within the
specified time, rebirths (deaths) within the limit, and performs actions or
moves within the maximum allowed moves, while seeking to maximize the total
coin value collected. This ensures an efficient way of finishing SMB levels.
Finally, we provide a fivefold comparative analysis by plotting fitness plots,
ability to finish different levels of world 1, and domain adaptation (transfer
learning) of the trained models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14965">Unraveling the Temporal Dynamics of the Unet in Diffusion Models. (arXiv:2312.14965v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Prasad_V/0/1/0/all/0/1">Vidya Prasad</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Tian_C/0/1/0/all/0/1">Chen Zhu-Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Vilanova_A/0/1/0/all/0/1">Anna Vilanova</a>, <a href="http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1">Hanspeter Pfister</a>, <a href="http://arxiv.org/find/cs/1/au:+Pezzotti_N/0/1/0/all/0/1">Nicola Pezzotti</a>, <a href="http://arxiv.org/find/cs/1/au:+Strobelt_H/0/1/0/all/0/1">Hendrik Strobelt</a></p>
<p>Diffusion models have garnered significant attention since they can
effectively learn complex multivariate Gaussian distributions, resulting in
diverse, high-quality outcomes. They introduce Gaussian noise into training
data and reconstruct the original data iteratively. Central to this iterative
process is a single Unet, adapting across time steps to facilitate generation.
Recent work revealed the presence of composition and denoising phases in this
generation process, raising questions about the Unets' varying roles. Our study
dives into the dynamic behavior of Unets within denoising diffusion
probabilistic models (DDPM), focusing on (de)convolutional blocks and skip
connections across time steps. We propose an analytical method to
systematically assess the impact of time steps and core Unet components on the
final output. This method eliminates components to study causal relations and
investigate their influence on output changes. The main purpose is to
understand the temporal dynamics and identify potential shortcuts during
inference. Our findings provide valuable insights into the various generation
phases during inference and shed light on the Unets' usage patterns across
these phases. Leveraging these insights, we identify redundancies in GLIDE (an
improved DDPM) and improve inference time by ~27% with minimal degradation in
output quality. Our ultimate goal is to guide more informed optimization
strategies for inference and influence new model designs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14967">Multi-Armed Bandit Learning for Content Provisioning in Network of UAVs. (arXiv:2312.14967v1 [cs.NI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bhuyan_A/0/1/0/all/0/1">Amit Kumar Bhuyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Dutta_H/0/1/0/all/0/1">Hrishikesh Dutta</a>, <a href="http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1">Subir Biswas</a></p>
<p>This paper proposes an unmanned aerial vehicle (UAV) aided content management
system in communication-challenged disaster scenarios. Without cellular
infrastructure in such scenarios, community of stranded users can be provided
access to situation-critical contents using a hybrid network of static and
traveling UAVs. A set of relatively static anchor UAVs can download content
from central servers and provide content access to its local users. A set of
ferrying UAVs with wider mobility can provision content to users by shuffling
them across different anchor UAVs while visiting different communities of
users. The objective is to design a content dissemination system that
on-the-fly learns content caching policies for maximizing content availability
to the stranded users. This paper proposes a decentralized Top-k Multi-Armed
Bandit Learning model for UAV-caching decision-making that takes geo-temporal
differences in content popularity and heterogeneity in content demands into
consideration. The proposed paradigm is able to combine the expected reward
maximization attribute and a proposed multi-dimensional reward structure of
Top-k Multi-Armed Bandit, for caching decision at the UAVs. This study is done
for different user-specified tolerable access delay, heterogeneous popularity
distributions, and inter-community geographical characteristics. Functional
verification and performance evaluation of the proposed caching framework is
done for a wide range of network size, UAV distribution, and content
popularity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14968">Enhancing Edge Intelligence with Highly Discriminant LNT Features. (arXiv:2312.14968v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1">Xinyu Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Mishra_V/0/1/0/all/0/1">Vinod K. Mishra</a>, <a href="http://arxiv.org/find/eess/1/au:+Kuo_C/0/1/0/all/0/1">C.-C. Jay Kuo</a></p>
<p>AI algorithms at the edge demand smaller model sizes and lower computational
complexity. To achieve these objectives, we adopt a green learning (GL)
paradigm rather than the deep learning paradigm. GL has three modules: 1)
unsupervised representation learning, 2) supervised feature learning, and 3)
supervised decision learning. We focus on the second module in this work. In
particular, we derive new discriminant features from proper linear combinations
of input features, denoted by x, obtained in the first module. They are called
complementary and raw features, respectively. Along this line, we present a
novel supervised learning method to generate highly discriminant complementary
features based on the least-squares normal transform (LNT). LNT consists of two
steps. First, we convert a C-class classification problem to a binary
classification problem. The two classes are assigned with 0 and 1,
respectively. Next, we formulate a least-squares regression problem from the
N-dimensional (N-D) feature space to the 1-D output space, and solve the
least-squares normal equation to obtain one N-D normal vector, denoted by a1.
Since one normal vector is yielded by one binary split, we can obtain M normal
vectors with M splits. Then, Ax is called an LNT of x, where transform matrix A
in R^{M by N} by stacking aj^T, j=1, ..., M, and the LNT, Ax, can generate M
new features. The newly generated complementary features are shown to be more
discriminant than the raw features. Experiments show that the classification
performance can be improved by these new features.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14972">Scaling Down to Scale Up: A Cost-Benefit Analysis of Replacing OpenAI&#x27;s GPT-4 with Self-Hosted Open Source SLMs in Production. (arXiv:2312.14972v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Irugalbandara_C/0/1/0/all/0/1">Chandra Irugalbandara</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahendra_A/0/1/0/all/0/1">Ashish Mahendra</a>, <a href="http://arxiv.org/find/cs/1/au:+Daynauth_R/0/1/0/all/0/1">Roland Daynauth</a>, <a href="http://arxiv.org/find/cs/1/au:+Arachchige_T/0/1/0/all/0/1">Tharuka Kasthuri Arachchige</a>, <a href="http://arxiv.org/find/cs/1/au:+Flautner_K/0/1/0/all/0/1">Krisztian Flautner</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1">Lingjia Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1">Yiping Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mars_J/0/1/0/all/0/1">Jason Mars</a></p>
<p>Many companies rely on APIs of managed AI models such as OpenAI's GPT-4 to
create AI-enabled experiences in their products. Along with the benefits of
ease of use and shortened time to production, this reliance on proprietary APIs
has downsides in terms of model control, performance reliability, up-time
predictability, and cost. At the same time, there has been a flurry of open
source small language models (SLMs) that have been made available for
commercial use. However, their readiness to replace existing capabilities
remains unclear, and a systematic approach to test these models is not readily
available. In this paper, we present a systematic evaluation methodology for,
and characterization of, modern open source SLMs and their trade-offs when
replacing a proprietary LLM APIs for a real-world product feature. We have
designed SLaM, an automated analysis tool that enables the quantitative and
qualitative testing of product features utilizing arbitrary SLMs. Using SLaM,
we examine both the quality and the performance characteristics of modern SLMs
relative to an existing customer-facing OpenAI-based implementation. We find
that across 9 SLMs and 29 variants, we observe competitive quality-of-results
for our use case, significant performance consistency improvement, and a cost
reduction of 5x-29x when compared to OpenAI GPT-4.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14975">Unsupervised Random Quantum Networks for PDEs. (arXiv:2312.14975v1 [quant-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Dees_J/0/1/0/all/0/1">Josh Dees</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Jacquier_A/0/1/0/all/0/1">Antoine Jacquier</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Laizet_S/0/1/0/all/0/1">Sylvain Laizet</a></p>
<p>Classical Physics-informed neural networks (PINNs) approximate solutions to
PDEs with the help of deep neural networks trained to satisfy the differential
operator and the relevant boundary conditions. We revisit this idea in the
quantum computing realm, using parameterised random quantum circuits as trial
solutions. We further adapt recent PINN-based techniques to our quantum
setting, in particular Gaussian smoothing. Our analysis concentrates on the
Poisson, the Heat and the Hamilton-Jacobi-Bellman equations, which are
ubiquitous in most areas of science. On the theoretical side, we develop a
complexity analysis of this approach, and show numerically that random quantum
networks can outperform more traditional quantum networks as well as random
classical networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14977">Diffusion Models for Generative Artificial Intelligence: An Introduction for Applied Mathematicians. (arXiv:2312.14977v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Higham_C/0/1/0/all/0/1">Catherine F. Higham</a>, <a href="http://arxiv.org/find/cs/1/au:+Higham_D/0/1/0/all/0/1">Desmond J. Higham</a>, <a href="http://arxiv.org/find/cs/1/au:+Grindrod_P/0/1/0/all/0/1">Peter Grindrod</a></p>
<p>Generative artificial intelligence (AI) refers to algorithms that create
synthetic but realistic output. Diffusion models currently offer state of the
art performance in generative AI for images. They also form a key component in
more general tools, including text-to-image generators and large language
models. Diffusion models work by adding noise to the available training data
and then learning how to reverse the process. The reverse operation may then be
applied to new random data in order to produce new outputs. We provide a brief
introduction to diffusion models for applied mathematicians and statisticians.
Our key aims are (a) to present illustrative computational examples, (b) to
give a careful derivation of the underlying mathematical formulas involved, and
(c) to draw a connection with partial differential equation (PDE) diffusion
models. We provide code for the computational experiments. We hope that this
topic will be of interest to advanced undergraduate students and postgraduate
students. Portions of the material may also provide useful motivational
examples for those who teach courses in stochastic processes, inference,
machine learning, PDEs or scientific computing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14978">On Quantifying Sentiments of Financial News -- Are We Doing the Right Things?. (arXiv:2312.14978v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nath_G/0/1/0/all/0/1">Gourab Nath</a>, <a href="http://arxiv.org/find/cs/1/au:+Sood_A/0/1/0/all/0/1">Arav Sood</a>, <a href="http://arxiv.org/find/cs/1/au:+Khanna_A/0/1/0/all/0/1">Aanchal Khanna</a>, <a href="http://arxiv.org/find/cs/1/au:+Wilson_S/0/1/0/all/0/1">Savi Wilson</a>, <a href="http://arxiv.org/find/cs/1/au:+Manot_K/0/1/0/all/0/1">Karan Manot</a>, <a href="http://arxiv.org/find/cs/1/au:+Durbaka_S/0/1/0/all/0/1">Sree Kavya Durbaka</a></p>
<p>Typical investors start off the day by going through the daily news to get an
intuition about the performance of the market. The speculations based on the
tone of the news ultimately shape their responses towards the market. Today,
computers are being trained to compute the news sentiment so that it can be
used as a variable to predict stock market movements and returns. Some
researchers have even developed news-based market indices to forecast stock
market returns. Majority of the research in the field of news sentiment
analysis has focussed on using libraries like Vader, Loughran-McDonald (LM),
Harvard IV and Pattern. However, are the popular approaches for measuring
financial news sentiment really approaching the problem of sentiment analysis
correctly? Our experiments suggest that measuring sentiments using these
libraries, especially for financial news, fails to depict the true picture and
hence may not be very reliable. Therefore, the question remains: What is the
most effective and accurate approach to measure financial news sentiment? Our
paper explores these questions and attempts to answer them through SENTInews: a
one-of-its-kind financial news sentiment analyzer customized to the Indian
context
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14979">Stacked tensorial neural networks for reduced-order modeling of a parametric partial differential equation. (arXiv:2312.14979v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wagner_C/0/1/0/all/0/1">Caleb G. Wagner</a></p>
<p>Tensorial neural networks (TNNs) combine the successes of multilinear algebra
with those of deep learning to enable extremely efficient reduced-order models
of high-dimensional problems. Here, I describe a deep neural network
architecture that fuses multiple TNNs into a larger network, intended to solve
a broader class of problems than a single TNN. I evaluate this architecture,
referred to as a "stacked tensorial neural network" (STNN), on a parametric PDE
with three independent variables and three parameters. The three parameters
correspond to one PDE coefficient and two quantities describing the domain
geometry. The STNN provides an accurate reduced-order description of the
solution manifold over a wide range of parameters. There is also evidence of
meaningful generalization to parameter values outside its training data.
Finally, while the STNN architecture is relatively simple and problem agnostic,
it can be regularized to incorporate problem-specific features like symmetries
and physical modeling assumptions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14980">TPTNet: A Data-Driven Temperature Prediction Model Based on Turbulent Potential Temperature. (arXiv:2312.14980v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Jun Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1">Changhoon Lee</a></p>
<p>A data-driven model for predicting the surface temperature using neural
networks was proposed to alleviate the computational burden of numerical
weather prediction (NWP). Our model, named TPTNet uses only 2m temperature
measured at the weather stations of the South Korean Peninsula as input to
predict the local temperature at finite forecast hours. The turbulent
fluctuation component of the temperature was extracted from the station
measurements by separating the climatology component accounting for the yearly
and daily variations. The effect of station altitude was then compensated by
introducing a potential temperature. The resulting turbulent potential
temperature data at irregularly distributed stations were used as input for
predicting the turbulent potential temperature at forecast hours through three
trained networks based on convolutional neural network (CNN), Swin Transformer,
and a graphic neural network (GNN). The prediction performance of our network
was compared with that of persistence and NWP, confirming that our model
outperformed NWP for up to 12 forecast hours.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14987">Deformable Image Registration with Stochastically Regularized Biomechanical Equilibrium. (arXiv:2312.14987v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Alvarez_P/0/1/0/all/0/1">Pablo Alvarez</a> (MIMESIS), <a href="http://arxiv.org/find/eess/1/au:+Cotin_S/0/1/0/all/0/1">St&#xe9;phane Cotin</a> (MIMESIS)</p>
<p>Numerous regularization methods for deformable image registration aim at
enforcing smooth transformations, but are difficult to tune-in a priori and
lack a clear physical basis. Physically inspired strategies have emerged,
offering a sound theoretical basis, but still necessitating complex
discretization and resolution schemes. This study introduces a regularization
strategy that does not require discretization, making it compatible with
current registration frameworks, while retaining the benefits of physically
motivated regularization for medical image registration. The proposed method
performs favorably in both synthetic and real datasets, exhibiting an accuracy
comparable to current state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14990">Learning to Prompt Knowledge Transfer for Open-World Continual Learning. (arXiv:2312.14990v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yujie Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiangkun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1">Tianrui Li</a></p>
<p>This paper studies the problem of continual learning in an open-world
scenario, referred to as Open-world Continual Learning (OwCL). OwCL is
increasingly rising while it is highly challenging in two-fold: i) learning a
sequence of tasks without forgetting knowns in the past, and ii) identifying
unknowns (novel objects/classes) in the future. Existing OwCL methods suffer
from the adaptability of task-aware boundaries between knowns and unknowns, and
do not consider the mechanism of knowledge transfer. In this work, we propose
Pro-KT, a novel prompt-enhanced knowledge transfer model for OwCL. Pro-KT
includes two key components: (1) a prompt bank to encode and transfer both
task-generic and task-specific knowledge, and (2) a task-aware open-set
boundary to identify unknowns in the new tasks. Experimental results using two
real-world datasets demonstrate that the proposed Pro-KT outperforms the
state-of-the-art counterparts in both the detection of unknowns and the
classification of knowns markedly.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14996">Bridging AI and Clinical Practice: Integrating Automated Sleep Scoring Algorithm with Uncertainty-Guided Physician Review. (arXiv:2312.14996v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bechny_M/0/1/0/all/0/1">Michal Bechny</a> (1 and 2), <a href="http://arxiv.org/find/cs/1/au:+Monachino_G/0/1/0/all/0/1">Giuliana Monachino</a> (1 and 2), <a href="http://arxiv.org/find/cs/1/au:+Fiorillo_L/0/1/0/all/0/1">Luigi Fiorillo</a> (2), <a href="http://arxiv.org/find/cs/1/au:+Meer_J/0/1/0/all/0/1">Julia van der Meer</a> (3), <a href="http://arxiv.org/find/cs/1/au:+Schmidt_M/0/1/0/all/0/1">Markus H. Schmidt</a> (3 and 4), <a href="http://arxiv.org/find/cs/1/au:+Bassetti_C/0/1/0/all/0/1">Claudio L. A. Bassetti</a> (3), <a href="http://arxiv.org/find/cs/1/au:+Tzovara_A/0/1/0/all/0/1">Athina Tzovara</a> (1 and 5), <a href="http://arxiv.org/find/cs/1/au:+Faraci_F/0/1/0/all/0/1">Francesca D. Faraci</a> (2) ((1) Institute of Computer Science, University of Bern, Bern, Switzerland (2) Institute of Digital Technologies for Personalized Healthcare (MeDiTech), University of Applied Sciences and Arts of Southern Switzerland, Lugano, Switzerland (3) Department of Neurology, Inselspital, Bern University Hospital, University of Bern, Bern, Switzerland (4) Ohio Sleep Medicine Institute, Dublin, United States (5) Center for Experimental Neurology, Department of Neurology, Inselspital, Bern University Hospital, University of Bern, Bern, Switzerland)</p>
<p>Purpose: This study aims to enhance the clinical use of automated
sleep-scoring algorithms by incorporating an uncertainty estimation approach to
efficiently assist clinicians in the manual review of predicted hypnograms, a
necessity due to the notable inter-scorer variability inherent in
polysomnography (PSG) databases. Our efforts target the extent of review
required to achieve predefined agreement levels, examining both in-domain and
out-of-domain data, and considering subjects diagnoses. Patients and methods:
Total of 19578 PSGs from 13 open-access databases were used to train U-Sleep, a
state-of-the-art sleep-scoring algorithm. We leveraged a comprehensive clinical
database of additional 8832 PSGs, covering a full spectrum of ages and
sleep-disorders, to refine the U-Sleep, and to evaluate different
uncertainty-quantification approaches, including our novel confidence network.
The ID data consisted of PSGs scored by over 50 physicians, and the two OOD
sets comprised recordings each scored by a unique senior physician. Results:
U-Sleep demonstrated robust performance, with Cohen's kappa (K) at 76.2% on ID
and 73.8-78.8% on OOD data. The confidence network excelled at identifying
uncertain predictions, achieving AUROC scores of 85.7% on ID and 82.5-85.6% on
OOD data. Independently of sleep-disorder status, statistical evaluations
revealed significant differences in confidence scores between aligning vs
discording predictions, and significant correlations of confidence scores with
classification performance metrics. To achieve K of at least 90% with physician
intervention, examining less than 29.0% of uncertain epochs was required,
substantially reducing physicians workload, and facilitating near-perfect
agreement.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15001">Discovering modular solutions that generalize compositionally. (arXiv:2312.15001v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schug_S/0/1/0/all/0/1">Simon Schug</a>, <a href="http://arxiv.org/find/cs/1/au:+Kobayashi_S/0/1/0/all/0/1">Seijin Kobayashi</a>, <a href="http://arxiv.org/find/cs/1/au:+Akram_Y/0/1/0/all/0/1">Yassir Akram</a>, <a href="http://arxiv.org/find/cs/1/au:+Wolczyk_M/0/1/0/all/0/1">Maciej Wo&#x142;czyk</a>, <a href="http://arxiv.org/find/cs/1/au:+Proca_A/0/1/0/all/0/1">Alexandra Proca</a>, <a href="http://arxiv.org/find/cs/1/au:+Oswald_J/0/1/0/all/0/1">Johannes von Oswald</a>, <a href="http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1">Razvan Pascanu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sacramento_J/0/1/0/all/0/1">Jo&#xe3;o Sacramento</a>, <a href="http://arxiv.org/find/cs/1/au:+Steger_A/0/1/0/all/0/1">Angelika Steger</a></p>
<p>Many complex tasks and environments can be decomposed into simpler,
independent parts. Discovering such underlying compositional structure has the
potential to expedite adaptation and enable compositional generalization.
Despite progress, our most powerful systems struggle to compose flexibly. While
most of these systems are monolithic, modularity promises to allow capturing
the compositional nature of many tasks. However, it is unclear under which
circumstances modular systems discover this hidden compositional structure. To
shed light on this question, we study a teacher-student setting with a modular
teacher where we have full control over the composition of ground truth
modules. This allows us to relate the problem of compositional generalization
to that of identification of the underlying modules. We show theoretically that
identification up to linear transformation purely from demonstrations is
possible in hypernetworks without having to learn an exponential number of
module combinations. While our theory assumes the infinite data limit, in an
extensive empirical study we demonstrate how meta-learning from finite data can
discover modular solutions that generalize compositionally in modular but not
monolithic architectures. We further show that our insights translate outside
the teacher-student setting and demonstrate that in tasks with compositional
preferences and tasks with compositional goals hypernetworks can discover
modular policies that compositionally generalize.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15002">C2FAR: Coarse-to-Fine Autoregressive Networks for Precise Probabilistic Forecasting. (arXiv:2312.15002v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bergsma_S/0/1/0/all/0/1">Shane Bergsma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeyl_T/0/1/0/all/0/1">Timothy Zeyl</a>, <a href="http://arxiv.org/find/cs/1/au:+Anaraki_J/0/1/0/all/0/1">Javad Rahimipour Anaraki</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1">Lei Guo</a></p>
<p>We present coarse-to-fine autoregressive networks (C2FAR), a method for
modeling the probability distribution of univariate, numeric random variables.
C2FAR generates a hierarchical, coarse-to-fine discretization of a variable
autoregressively; progressively finer intervals of support are generated from a
sequence of binned distributions, where each distribution is conditioned on
previously-generated coarser intervals. Unlike prior (flat) binned
distributions, C2FAR can represent values with exponentially higher precision,
for only a linear increase in complexity. We use C2FAR for probabilistic
forecasting via a recurrent neural network, thus modeling time series
autoregressively in both space and time. C2FAR is the first method to
simultaneously handle discrete and continuous series of arbitrary scale and
distribution shape. This flexibility enables a variety of time series use
cases, including anomaly detection, interpolation, and compression. C2FAR
achieves improvements over the state-of-the-art on several benchmark
forecasting datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15006">Assessing the Impact of Prompting, Persona, and Chain of Thought Methods on ChatGPT&#x27;s Arithmetic Capabilities. (arXiv:2312.15006v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuhao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1">Chloe Wong</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Hanwen Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Aguenza_J/0/1/0/all/0/1">Juan Aguenza</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhujangari_S/0/1/0/all/0/1">Sai Bhujangari</a>, <a href="http://arxiv.org/find/cs/1/au:+Vu_B/0/1/0/all/0/1">Benthan Vu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_X/0/1/0/all/0/1">Xun Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Prasad_A/0/1/0/all/0/1">Amisha Prasad</a>, <a href="http://arxiv.org/find/cs/1/au:+Fluss_M/0/1/0/all/0/1">Manny Fluss</a>, <a href="http://arxiv.org/find/cs/1/au:+Phuong_E/0/1/0/all/0/1">Eric Phuong</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Minghao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1">James Davis</a></p>
<p>This study critically evaluates the mathematical proficiency of OpenAI's
language model, ChatGPT, by juxtaposing its default computational capabilities
against the efficiency of three prescriptive methods: strategic prompting,
persona implementation, and the Chain of Thought approach. The evaluation
harnessed the diverse and extensive problem sets from the MATH, GSM8K, and MMLU
data-sets, which encompassing a broad spectrum of mathematical conundrums and
levels of complexity. A sophisticated grading script was designed to determine
the efficacy of these interventions in enhancing the model's mathematical
precision. Contrary to expectations, our empirical analysis revealed that none
of the trialed methods substantially improved ChatGPT's baseline performance.
In some cases, these interventions inadvertently disrupted the model's response
generation. This investigation concluded that while the pursuit of innovative
strategies for augmenting language model performance remains crucial, the
specific methods examined within this study did not induce significant
improvements in ChatGPT's computational aptitude. These findings underscore the
importance of further comprehensive research and exploration of novel
techniques to enhance the precision and dependability of such models across
diverse domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15023">Federated Q-Learning: Linear Regret Speedup with Low Communication Cost. (arXiv:2312.15023v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Zhong Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1">Fengyu Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1">Lingzhou Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jing Yang</a></p>
<p>In this paper, we consider federated reinforcement learning for tabular
episodic Markov Decision Processes (MDP) where, under the coordination of a
central server, multiple agents collaboratively explore the environment and
learn an optimal policy without sharing their raw data. While linear speedup in
the number of agents has been achieved for some metrics, such as convergence
rate and sample complexity, in similar settings, it is unclear whether it is
possible to design a model-free algorithm to achieve linear regret speedup with
low communication cost. We propose two federated Q-Learning algorithms termed
as FedQ-Hoeffding and FedQ-Bernstein, respectively, and show that the
corresponding total regrets achieve a linear speedup compared with their
single-agent counterparts when the time horizon is sufficiently large, while
the communication cost scales logarithmically in the total number of time steps
$T$. Those results rely on an event-triggered synchronization mechanism between
the agents and the server, a novel step size selection when the server
aggregates the local estimates of the state-action values to form the global
estimates, and a set of new concentration inequalities to bound the sum of
non-martingale differences. This is the first work showing that linear regret
speedup and logarithmic communication cost can be achieved by model-free
algorithms in federated reinforcement learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15036">SODA: Protecting Proprietary Information in On-Device Machine Learning Models. (arXiv:2312.15036v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Atrey_A/0/1/0/all/0/1">Akanksha Atrey</a>, <a href="http://arxiv.org/find/cs/1/au:+Sinha_R/0/1/0/all/0/1">Ritwik Sinha</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitra_S/0/1/0/all/0/1">Saayan Mitra</a>, <a href="http://arxiv.org/find/cs/1/au:+Shenoy_P/0/1/0/all/0/1">Prashant Shenoy</a></p>
<p>The growth of low-end hardware has led to a proliferation of machine
learning-based services in edge applications. These applications gather
contextual information about users and provide some services, such as
personalized offers, through a machine learning (ML) model. A growing practice
has been to deploy such ML models on the user's device to reduce latency,
maintain user privacy, and minimize continuous reliance on a centralized
source. However, deploying ML models on the user's edge device can leak
proprietary information about the service provider. In this work, we
investigate on-device ML models that are used to provide mobile services and
demonstrate how simple attacks can leak proprietary information of the service
provider. We show that different adversaries can easily exploit such models to
maximize their profit and accomplish content theft. Motivated by the need to
thwart such attacks, we present an end-to-end framework, SODA, for deploying
and serving on edge devices while defending against adversarial usage. Our
results demonstrate that SODA can detect adversarial usage with 89% accuracy in
less than 50 queries with minimal impact on service performance, latency, and
storage.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15045">Probabilistic Modeling for Sequences of Sets in Continuous-Time. (arXiv:2312.15045v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1">Yuxin Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Boyd_A/0/1/0/all/0/1">Alex Boyd</a>, <a href="http://arxiv.org/find/cs/1/au:+Smyth_P/0/1/0/all/0/1">Padhraic Smyth</a></p>
<p>Neural marked temporal point processes have been a valuable addition to the
existing toolbox of statistical parametric models for continuous-time event
data. These models are useful for sequences where each event is associated with
a single item (a single type of event or a "mark") -- but such models are not
suited for the practical situation where each event is associated with a set of
items. In this work, we develop a general framework for modeling set-valued
data in continuous-time, compatible with any intensity-based recurrent neural
point process model. In addition, we develop inference methods that can use
such models to answer probabilistic queries such as "the probability of item
$A$ being observed before item $B$," conditioned on sequence history. Computing
exact answers for such queries is generally intractable for neural models due
to both the continuous-time nature of the problem setting and the
combinatorially-large space of potential outcomes for each event. To address
this, we develop a class of importance sampling methods for querying with
set-based sequences and demonstrate orders-of-magnitude improvements in
efficiency over direct sampling via systematic experiments with four real-world
datasets. We also illustrate how to use this framework to perform model
selection using likelihoods that do not involve one-step-ahead prediction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15046">Information-seeking polynomial NARX model-predictive control through expected free energy minimization. (arXiv:2312.15046v1 [eess.SY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Kouw_W/0/1/0/all/0/1">Wouter M. Kouw</a></p>
<p>We propose an adaptive model-predictive controller that balances driving the
system to a goal state and seeking system observations that are informative
with respect to the parameters of a nonlinear autoregressive exogenous model.
The controller's objective function is derived from an expected free energy
functional and contains information-theoretic terms expressing uncertainty over
model parameters and output predictions. Experiments illustrate how parameter
uncertainty affects the control objective and evaluate the proposed controller
for a pendulum swing-up task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15055">Deep Learning for Efficient GWAS Feature Selection. (arXiv:2312.15055v1 [q-bio.GN])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Li_K/0/1/0/all/0/1">Kexuan Li</a></p>
<p>Genome-Wide Association Studies (GWAS) face unique challenges in the era of
big genomics data, particularly when dealing with ultra-high-dimensional
datasets where the number of genetic features significantly exceeds the
available samples. This paper introduces an extension to the feature selection
methodology proposed by Mirzaei et al. (2020), specifically tailored to tackle
the intricacies associated with ultra-high-dimensional GWAS data. Our extended
approach enhances the original method by introducing a Frobenius norm penalty
into the student network, augmenting its capacity to adapt to scenarios
characterized by a multitude of features and limited samples. Operating
seamlessly in both supervised and unsupervised settings, our method employs two
key neural networks. The first leverages an autoencoder or supervised
autoencoder for dimension reduction, extracting salient features from the
ultra-high-dimensional genomic data. The second network, a regularized
feed-forward model with a single hidden layer, is designed for precise feature
selection. The introduction of the Frobenius norm penalty in the student
network significantly boosts the method's resilience to the challenges posed by
ultra-high-dimensional GWAS datasets. Experimental results showcase the
efficacy of our approach in feature selection for GWAS data. The method not
only handles the inherent complexities of ultra-high-dimensional settings but
also demonstrates superior adaptability to the nuanced structures present in
genomics data. The flexibility and versatility of our proposed methodology are
underscored by its successful performance across a spectrum of experiments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15058">The State of Documentation Practices of Third-party Machine Learning Models and Datasets. (arXiv:2312.15058v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oreamuno_E/0/1/0/all/0/1">Ernesto Lang Oreamuno</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_R/0/1/0/all/0/1">Rohan Faiyaz Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bangash_A/0/1/0/all/0/1">Abdul Ali Bangash</a>, <a href="http://arxiv.org/find/cs/1/au:+Stinson_C/0/1/0/all/0/1">Catherine Stinson</a>, <a href="http://arxiv.org/find/cs/1/au:+Adams_B/0/1/0/all/0/1">Bram Adams</a></p>
<p>Model stores offer third-party ML models and datasets for easy project
integration, minimizing coding efforts. One might hope to find detailed
specifications of these models and datasets in the documentation, leveraging
documentation standards such as model and dataset cards. In this study, we use
statistical analysis and hybrid card sorting to assess the state of the
practice of documenting model cards and dataset cards in one of the largest
model stores in use today--Hugging Face (HF). Our findings show that only
21,902 models (39.62\%) and 1,925 datasets (28.48\%) have documentation.
Furthermore, we observe inconsistency in ethics and transparency-related
documentation for ML models and datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15063">A universal approximation theorem for nonlinear resistive networks. (arXiv:2312.15063v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Scellier_B/0/1/0/all/0/1">Benjamin Scellier</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1">Siddhartha Mishra</a></p>
<p>Resistor networks have recently had a surge of interest as substrates for
energy-efficient self-learning machines. This work studies the computational
capabilities of these resistor networks. We show that electrical networks
composed of voltage sources, linear resistors, diodes and voltage-controlled
voltage sources (VCVS) can implement any continuous functions. To prove it, we
assume that the circuit elements are ideal and that the conductances of
variable resistors and the amplification factors of the VCVS's can take
arbitrary values -- arbitrarily small or arbitrarily large. The constructive
nature of our proof could also inform the design of such self-learning
electrical networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15064">Joint Self-Supervised and Supervised Contrastive Learning for Multimodal MRI Data: Towards Predicting Abnormal Neurodevelopment. (arXiv:2312.15064v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1">Zhiyuan Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1">Hailong Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Ralescu_A/0/1/0/all/0/1">Anca L. Ralescu</a>, <a href="http://arxiv.org/find/eess/1/au:+Dillman_J/0/1/0/all/0/1">Jonathan R. Dillman</a>, <a href="http://arxiv.org/find/eess/1/au:+Altaye_M/0/1/0/all/0/1">Mekibib Altaye</a>, <a href="http://arxiv.org/find/eess/1/au:+Cecil_K/0/1/0/all/0/1">Kim M. Cecil</a>, <a href="http://arxiv.org/find/eess/1/au:+Parikh_N/0/1/0/all/0/1">Nehal A. Parikh</a>, <a href="http://arxiv.org/find/eess/1/au:+He_L/0/1/0/all/0/1">Lili He</a></p>
<p>The integration of different imaging modalities, such as structural,
diffusion tensor, and functional magnetic resonance imaging, with deep learning
models has yielded promising outcomes in discerning phenotypic characteristics
and enhancing disease diagnosis. The development of such a technique hinges on
the efficient fusion of heterogeneous multimodal features, which initially
reside within distinct representation spaces. Naively fusing the multimodal
features does not adequately capture the complementary information and could
even produce redundancy. In this work, we present a novel joint self-supervised
and supervised contrastive learning method to learn the robust latent feature
representation from multimodal MRI data, allowing the projection of
heterogeneous features into a shared common space, and thereby amalgamating
both complementary and analogous information across various modalities and
among similar subjects. We performed a comparative analysis between our
proposed method and alternative deep multimodal learning approaches. Through
extensive experiments on two independent datasets, the results demonstrated
that our method is significantly superior to several other deep multimodal
learning methods in predicting abnormal neurodevelopment. Our method has the
capability to facilitate computer-aided diagnosis within clinical practice,
harnessing the power of multimodal data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15068">Refining GPT-3 Embeddings with a Siamese Structure for Technical Post Duplicate Detection. (arXiv:2312.15068v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xingfang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Heng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoshioka_N/0/1/0/all/0/1">Nobukazu Yoshioka</a>, <a href="http://arxiv.org/find/cs/1/au:+Washizaki_H/0/1/0/all/0/1">Hironori Washizaki</a>, <a href="http://arxiv.org/find/cs/1/au:+Khomh_F/0/1/0/all/0/1">Foutse Khomh</a></p>
<p>One goal of technical online communities is to help developers find the right
answer in one place. A single question can be asked in different ways with
different wordings, leading to the existence of duplicate posts on technical
forums. The question of how to discover and link duplicate posts has garnered
the attention of both developer communities and researchers. For example, Stack
Overflow adopts a voting-based mechanism to mark and close duplicate posts.
However, addressing these constantly emerging duplicate posts in a timely
manner continues to pose challenges. Therefore, various approaches have been
proposed to detect duplicate posts on technical forum posts automatically. The
existing methods suffer from limitations either due to their reliance on
handcrafted similarity metrics which can not sufficiently capture the semantics
of posts, or their lack of supervision to improve the performance.
Additionally, the efficiency of these methods is hindered by their dependence
on pair-wise feature generation, which can be impractical for large amount of
data. In this work, we attempt to employ and refine the GPT-3 embeddings for
the duplicate detection task. We assume that the GPT-3 embeddings can
accurately represent the semantics of the posts. In addition, by training a
Siamese-based network based on the GPT-3 embeddings, we obtain a latent
embedding that accurately captures the duplicate relation in technical forum
posts. Our experiment on a benchmark dataset confirms the effectiveness of our
approach and demonstrates superior performance compared to baseline methods.
When applied to the dataset we constructed with a recent Stack Overflow dump,
our approach attains a Top-1, Top-5, and Top-30 accuracy of 23.1%, 43.9%, and
68.9%, respectively. With a manual study, we confirm our approach's potential
of finding unlabelled duplicates on technical forums.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15081">Learning Rich Rankings. (arXiv:2312.15081v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Seshadri_A/0/1/0/all/0/1">Arjun Seshadri</a>, <a href="http://arxiv.org/find/cs/1/au:+Ragain_S/0/1/0/all/0/1">Stephen Ragain</a>, <a href="http://arxiv.org/find/cs/1/au:+Ugander_J/0/1/0/all/0/1">Johan Ugander</a></p>
<p>Although the foundations of ranking are well established, the ranking
literature has primarily been focused on simple, unimodal models, e.g. the
Mallows and Plackett-Luce models, that define distributions centered around a
single total ordering. Explicit mixture models have provided some tools for
modelling multimodal ranking data, though learning such models from data is
often difficult. In this work, we contribute a contextual repeated selection
(CRS) model that leverages recent advances in choice modeling to bring a
natural multimodality and richness to the rankings space. We provide rigorous
theoretical guarantees for maximum likelihood estimation under the model
through structure-dependent tail risk and expected risk bounds. As a
by-product, we also furnish the first tight bounds on the expected risk of
maximum likelihood estimators for the multinomial logit (MNL) choice model and
the Plackett-Luce (PL) ranking model, as well as the first tail risk bound on
the PL ranking model. The CRS model significantly outperforms existing methods
for modeling real world ranking data in a variety of settings, from racing to
rank choice voting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15086">HyperMix: Out-of-Distribution Detection and Classification in Few-Shot Settings. (arXiv:2312.15086v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mehta_N/0/1/0/all/0/1">Nikhil Mehta</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1">Kevin J Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jing Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_F/0/1/0/all/0/1">Fu-Jen Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1">Li Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Hassner_T/0/1/0/all/0/1">Tal Hassner</a></p>
<p>Out-of-distribution (OOD) detection is an important topic for real-world
machine learning systems, but settings with limited in-distribution samples
have been underexplored. Such few-shot OOD settings are challenging, as models
have scarce opportunities to learn the data distribution before being tasked
with identifying OOD samples. Indeed, we demonstrate that recent
state-of-the-art OOD methods fail to outperform simple baselines in the
few-shot setting. We thus propose a hypernetwork framework called HyperMix,
using Mixup on the generated classifier parameters, as well as a natural
out-of-episode outlier exposure technique that does not require an additional
outlier dataset. We conduct experiments on CIFAR-FS and MiniImageNet,
significantly outperforming other OOD methods in the few-shot regime.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15088">Adaptive Domain Inference Attack. (arXiv:2312.15088v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1">Yuechun Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Keke Chen</a></p>
<p>As deep neural networks are increasingly deployed in sensitive application
domains, such as healthcare and security, it's necessary to understand what
kind of sensitive information can be inferred from these models. Existing
model-targeted attacks all assume the attacker has known the application domain
or training data distribution, which plays an essential role in successful
attacks. Can removing the domain information from model APIs protect models
from these attacks? This paper studies this critical problem. Unfortunately,
even with minimal knowledge, i.e., accessing the model as an unnamed function
without leaking the meaning of input and output, the proposed adaptive domain
inference attack (ADI) can still successfully estimate relevant subsets of
training data. We show that the extracted relevant data can significantly
improve, for instance, the performance of model-inversion attacks.
Specifically, the ADI method utilizes a concept hierarchy built on top of a
large collection of available public and private datasets and a novel algorithm
to adaptively tune the likelihood of leaf concepts showing up in the unseen
training data. The ADI attack not only extracts partial training data at the
concept level, but also converges fast and requires much fewer target-model
accesses than another domain inference attack, GDI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15091">A Note on Stability in Asynchronous Stochastic Approximation without Communication Delays. (arXiv:2312.15091v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Huizhen Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1">Yi Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sutton_R/0/1/0/all/0/1">Richard S. Sutton</a></p>
<p>In this paper, we study asynchronous stochastic approximation algorithms
without communication delays. Our main contribution is a stability proof for
these algorithms that extends a method of Borkar and Meyn by accommodating more
general noise conditions. We also derive convergence results from this
stability result and discuss their application in important average-reward
reinforcement learning problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15097">Recourse under Model Multiplicity via Argumentative Ensembling. (arXiv:2312.15097v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1">Junqi Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Rago_A/0/1/0/all/0/1">Antonio Rago</a>, <a href="http://arxiv.org/find/cs/1/au:+Leofante_F/0/1/0/all/0/1">Francesco Leofante</a>, <a href="http://arxiv.org/find/cs/1/au:+Toni_F/0/1/0/all/0/1">Francesca Toni</a></p>
<p>Model Multiplicity (MM) arises when multiple, equally performing machine
learning models can be trained to solve the same prediction task. Recent
studies show that models obtained under MM may produce inconsistent predictions
for the same input. When this occurs, it becomes challenging to provide
counterfactual explanations (CEs), a common means for offering recourse
recommendations to individuals negatively affected by models' predictions. In
this paper, we formalise this problem, which we name recourse-aware ensembling,
and identify several desirable properties which methods for solving it should
satisfy. We show that existing ensembling methods, naturally extended in
different ways to provide CEs, fail to satisfy these properties. We then
introduce argumentative ensembling, deploying computational argumentation to
guarantee robustness of CEs to MM, while also accommodating customisable user
preferences. We show theoretically and experimentally that argumentative
ensembling satisfies properties which the existing methods lack, and that the
trade-offs are minimal wrt accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15099">Moderating New Waves of Online Hate with Chain-of-Thought Reasoning in Large Language Models. (arXiv:2312.15099v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vishwamitra_N/0/1/0/all/0/1">Nishant Vishwamitra</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1">Keyan Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Romit_F/0/1/0/all/0/1">Farhan Tajwar Romit</a>, <a href="http://arxiv.org/find/cs/1/au:+Ondracek_I/0/1/0/all/0/1">Isabelle Ondracek</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1">Long Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Ziming Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1">Hongxin Hu</a></p>
<p>Online hate is an escalating problem that negatively impacts the lives of
Internet users, and is also subject to rapid changes due to evolving events,
resulting in new waves of online hate that pose a critical threat. Detecting
and mitigating these new waves present two key challenges: it demands
reasoning-based complex decision-making to determine the presence of hateful
content, and the limited availability of training samples hinders updating the
detection model. To address this critical issue, we present a novel framework
called HATEGUARD for effectively moderating new waves of online hate. HATEGUARD
employs a reasoning-based approach that leverages the recently introduced
chain-of-thought (CoT) prompting technique, harnessing the capabilities of
large language models (LLMs). HATEGUARD further achieves prompt-based zero-shot
detection by automatically generating and updating detection prompts with new
derogatory terms and targets in new wave samples to effectively address new
waves of online hate. To demonstrate the effectiveness of our approach, we
compile a new dataset consisting of tweets related to three recently witnessed
new waves: the 2022 Russian invasion of Ukraine, the 2021 insurrection of the
US Capitol, and the COVID-19 pandemic. Our studies reveal crucial longitudinal
patterns in these new waves concerning the evolution of events and the pressing
need for techniques to rapidly update existing moderation tools to counteract
them. Comparative evaluations against state-of-the-art tools illustrate the
superiority of our framework, showcasing a substantial 22.22% to 83.33%
improvement in detecting the three new waves of online hate. Our work
highlights the severe threat posed by the emergence of new waves of online hate
and represents a paradigm shift in addressing this threat practically.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15101">Fix-Con: Automatic Fault Localization and Repair of Deep Learning Model Conversions. (arXiv:2312.15101v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Louloudakis_N/0/1/0/all/0/1">Nikolaos Louloudakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Gibson_P/0/1/0/all/0/1">Perry Gibson</a>, <a href="http://arxiv.org/find/cs/1/au:+Cano_J/0/1/0/all/0/1">Jos&#xe9; Cano</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajan_A/0/1/0/all/0/1">Ajitha Rajan</a></p>
<p>Converting deep learning models between frameworks is a common step to
maximize model compatibility across devices and leverage optimization features
that may be exclusively provided in one deep learning framework. However, this
conversion process may be riddled with bugs, making the converted models either
undeployable or problematic, considerably degrading their prediction
correctness.
</p>
<p>We propose an automated approach for fault localization and repair, Fix-Con,
during model conversion between deep learning frameworks. Fix-Con is capable of
detecting and fixing faults introduced in model input, parameters,
hyperparameters, and the model graph during conversion.
</p>
<p>Fix-Con uses a set of fault types mined from surveying conversion issues
raised to localize potential conversion faults in the converted target model,
and then repairs them appropriately, e.g. replacing the parameters of the
target model with those from the source model. This is done iteratively for
every image in the dataset with output label differences between the source
model and the converted target model until all differences are resolved. We
evaluate the effectiveness of Fix-Con in fixing model conversion bugs of three
widely used image recognition models converted across four different deep
learning frameworks. Overall, Fix-Con was able to either completely repair, or
significantly improve the performance of 14 out of the 15 erroneous conversion
cases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15103">Energy-based learning algorithms for analog computing: a comparative study. (arXiv:2312.15103v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Scellier_B/0/1/0/all/0/1">Benjamin Scellier</a>, <a href="http://arxiv.org/find/cs/1/au:+Ernoult_M/0/1/0/all/0/1">Maxence Ernoult</a>, <a href="http://arxiv.org/find/cs/1/au:+Kendall_J/0/1/0/all/0/1">Jack Kendall</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1">Suhas Kumar</a></p>
<p>Energy-based learning algorithms have recently gained a surge of interest due
to their compatibility with analog (post-digital) hardware. Existing algorithms
include contrastive learning (CL), equilibrium propagation (EP) and coupled
learning (CpL), all consisting in contrasting two states, and differing in the
type of perturbation used to obtain the second state from the first one.
However, these algorithms have never been explicitly compared on equal footing
with same models and datasets, making it difficult to assess their scalability
and decide which one to select in practice. In this work, we carry out a
comparison of seven learning algorithms, namely CL and different variants of EP
and CpL depending on the signs of the perturbations. Specifically, using these
learning algorithms, we train deep convolutional Hopfield networks (DCHNs) on
five vision tasks (MNIST, F-MNIST, SVHN, CIFAR-10 and CIFAR-100). We find that,
while all algorithms yield comparable performance on MNIST, important
differences in performance arise as the difficulty of the task increases. Our
key findings reveal that negative perturbations are better than positive ones,
and highlight the centered variant of EP (which uses two perturbations of
opposite sign) as the best-performing algorithm. We also endorse these findings
with theoretical arguments. Additionally, we establish new SOTA results with
DCHNs on all five datasets, both in performance and speed. In particular, our
DCHN simulations are 13.5 times faster with respect to Laborieux et al. (2021),
which we achieve thanks to the use of a novel energy minimisation algorithm
based on asynchronous updates, combined with reduced precision (16 bits).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15112">Less or More From Teacher: Exploiting Trilateral Geometry For Knowledge Distillation. (arXiv:2312.15112v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1">Chengming Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Haolun Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1">Chen Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1">Jun Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Boyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xue Liu</a></p>
<p>Knowledge distillation aims to train a compact student network using soft
supervision from a larger teacher network and hard supervision from ground
truths. However, determining an optimal knowledge fusion ratio that balances
these supervisory signals remains challenging. Prior methods generally resort
to a constant or heuristic-based fusion ratio, which often falls short of a
proper balance. In this study, we introduce a novel adaptive method for
learning a sample-wise knowledge fusion ratio, exploiting both the correctness
of teacher and student, as well as how well the student mimics the teacher on
each sample. Our method naturally leads to the intra-sample trilateral
geometric relations among the student prediction ($S$), teacher prediction
($T$), and ground truth ($G$). To counterbalance the impact of outliers, we
further extend to the inter-sample relations, incorporating the teacher's
global average prediction $\bar{T}$ for samples within the same class. A simple
neural network then learns the implicit mapping from the intra- and
inter-sample relations to an adaptive, sample-wise knowledge fusion ratio in a
bilevel-optimization manner. Our approach provides a simple, practical, and
adaptable solution for knowledge distillation that can be employed across
various architectures and model sizes. Extensive experiments demonstrate
consistent improvements over other loss re-weighting methods on image
classification, attack detection, and click-through rate prediction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15113">Understanding driver-pedestrian interactions to predict driver yielding: naturalistic open-source dataset collected in Minnesota. (arXiv:2312.15113v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1">Tianyi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Klavins_J/0/1/0/all/0/1">Joshua Klavins</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1">Te Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zafri_N/0/1/0/all/0/1">Niaz Mahmud Zafri</a>, <a href="http://arxiv.org/find/cs/1/au:+Stern_R/0/1/0/all/0/1">Raphael Stern</a></p>
<p>Many factors influence the yielding result of a driver-pedestrian
interaction, including traffic volume, vehicle speed, roadway characteristics,
etc. While individual aspects of these interactions have been explored,
comprehensive, naturalistic studies, particularly those considering the built
environment's influence on driver-yielding behavior, are lacking. To address
this gap, our study introduces an extensive open-source dataset, compiled from
video data at 18 unsignalized intersections across Minnesota. Documenting more
than 3000 interactions, this dataset provides a detailed view of
driver-pedestrian interactions and over 50 distinct contextual variables. The
data, which covers individual driver-pedestrian interactions and contextual
factors, is made publicly available at
https://github.com/tianyi17/pedestrian_yielding_data_MN.
</p>
<p>Using logistic regression, we developed a classification model that predicts
driver yielding based on the identified variables. Our analysis indicates that
vehicle speed, the presence of parking lots, proximity to parks or schools, and
the width of major road crossings significantly influence driver yielding at
unsignalized intersections. This study contributes to one of the most
comprehensive driver-pedestrian datasets in the US, offering valuable insights
for traffic safety improvements. By making this information available, our
study will support communities across Minnesota and the United States in their
ongoing efforts to improve road safety for pedestrians.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15122">Scaling Is All You Need: Training Strong Policies for Autonomous Driving with JAX-Accelerated Reinforcement Learning. (arXiv:2312.15122v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Harmel_M/0/1/0/all/0/1">Moritz Harmel</a>, <a href="http://arxiv.org/find/cs/1/au:+Paras_A/0/1/0/all/0/1">Anubhav Paras</a>, <a href="http://arxiv.org/find/cs/1/au:+Pasternak_A/0/1/0/all/0/1">Andreas Pasternak</a>, <a href="http://arxiv.org/find/cs/1/au:+Linscott_G/0/1/0/all/0/1">Gary Linscott</a></p>
<p>Reinforcement learning has been used to train policies that outperform even
the best human players in various games. However, a large amount of data is
needed to achieve good performance, which in turn requires building large-scale
frameworks and simulators. In this paper, we study how large-scale
reinforcement learning can be applied to autonomous driving, analyze how the
resulting policies perform as the experiment size is scaled, and what the most
important factors contributing to policy performance are. To do this, we first
introduce a hardware-accelerated autonomous driving simulator, which allows us
to efficiently collect experience from billions of agent steps. This simulator
is paired with a large-scale, multi-GPU reinforcement learning framework. We
demonstrate that simultaneous scaling of dataset size, model size, and agent
steps trained provides increasingly strong driving policies in regard to
collision, traffic rule violations, and progress. In particular, our best
policy reduces the failure rate by 57% while improving progress by 23% compared
to the current state-of-the-art machine learning policies for autonomous
driving.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15124">On fundamental aspects of quantum extreme learning machines. (arXiv:2312.15124v1 [quant-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Xiong_W/0/1/0/all/0/1">Weijie Xiong</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Facelli_G/0/1/0/all/0/1">Giorgio Facelli</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Sahebi_M/0/1/0/all/0/1">Mehrad Sahebi</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Agnel_O/0/1/0/all/0/1">Owen Agnel</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Chotibut_T/0/1/0/all/0/1">Thiparat Chotibut</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Thanasilp_S/0/1/0/all/0/1">Supanut Thanasilp</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Holmes_Z/0/1/0/all/0/1">Zo&#xeb; Holmes</a></p>
<p>Quantum Extreme Learning Machines (QELMs) have emerged as a promising
framework for quantum machine learning. Their appeal lies in the rich feature
map induced by the dynamics of a quantum substrate - the quantum reservoir -
and the efficient post-measurement training via linear regression. Here we
study the expressivity of QELMs by decomposing the prediction of QELMs into a
Fourier series. We show that the achievable Fourier frequencies are determined
by the data encoding scheme, while Fourier coefficients depend on both the
reservoir and the measurement. Notably, the expressivity of QELMs is
fundamentally limited by the number of Fourier frequencies and the number of
observables, while the complexity of the prediction hinges on the reservoir. As
a cautionary note on scalability, we identify four sources that can lead to the
exponential concentration of the observables as the system size grows
(randomness, hardware noise, entanglement, and global measurements) and show
how this can turn QELMs into useless input-agnostic oracles. Our analysis
elucidates the potential and fundamental limitations of QELMs, and lays the
groundwork for systematically exploring quantum reservoir systems for other
machine learning tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15127">Gradient Shaping for Multi-Constraint Safe Reinforcement Learning. (arXiv:2312.15127v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1">Yihang Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zuxin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cen_Z/0/1/0/all/0/1">Zhepeng Cen</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1">Peide Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tingnan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1">Wenhao Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1">Ding Zhao</a></p>
<p>Online safe reinforcement learning (RL) involves training a policy that
maximizes task efficiency while satisfying constraints via interacting with the
environments. In this paper, our focus lies in addressing the complex
challenges associated with solving multi-constraint (MC) safe RL problems. We
approach the safe RL problem from the perspective of Multi-Objective
Optimization (MOO) and propose a unified framework designed for MC safe RL
algorithms. This framework highlights the manipulation of gradients derived
from constraints. Leveraging insights from this framework and recognizing the
significance of \textit{redundant} and \textit{conflicting} constraint
conditions, we introduce the Gradient Shaping (GradS) method for general
Lagrangian-based safe RL algorithms to improve the training efficiency in terms
of both reward and constraint satisfaction. Our extensive experimentation
demonstrates the effectiveness of our proposed method in encouraging
exploration and learning a policy that improves both safety and reward
performance across various challenging MC safe RL tasks as well as good
scalability to the number of constraints.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15138">An FPGA-Based Accelerator for Graph Embedding using Sequential Training Algorithm. (arXiv:2312.15138v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sunaga_K/0/1/0/all/0/1">Kazuki Sunaga</a>, <a href="http://arxiv.org/find/cs/1/au:+Sugiura_K/0/1/0/all/0/1">Keisuke Sugiura</a>, <a href="http://arxiv.org/find/cs/1/au:+Matsutani_H/0/1/0/all/0/1">Hiroki Matsutani</a></p>
<p>A graph embedding is an emerging approach that can represent a graph
structure with a fixed-length low-dimensional vector. node2vec is a well-known
algorithm to obtain such a graph embedding by sampling neighboring nodes on a
given graph with a random walk technique. However, the original node2vec
algorithm typically relies on a batch training of graph structures; thus, it is
not suited for applications in which the graph structure changes after the
deployment. In this paper, we focus on node2vec applications for IoT (Internet
of Things) environments. To handle the changes of graph structures after the
IoT devices have been deployed in edge environments, in this paper we propose
to combine an online sequential training algorithm with node2vec. The proposed
sequentially-trainable model is implemented on a resource-limited FPGA
(Field-Programmable Gate Array) device to demonstrate the benefits of our
approach. The proposed FPGA implementation achieves up to 205.25 times speedup
compared to the original model on CPU. Evaluation results using dynamic graphs
show that although the original model decreases the accuracy, the proposed
sequential model can obtain better graph embedding that can increase the
accuracy even when the graph structure is changed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15141">Improving the Performance of Echo State Networks Through Feedback. (arXiv:2312.15141v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ehlers_P/0/1/0/all/0/1">Peter J. Ehlers</a>, <a href="http://arxiv.org/find/cs/1/au:+Nurdin_H/0/1/0/all/0/1">Hendra I. Nurdin</a>, <a href="http://arxiv.org/find/cs/1/au:+Soh_D/0/1/0/all/0/1">Daniel Soh</a></p>
<p>Reservoir computing, using nonlinear dynamical systems, offers a
cost-effective alternative to neural networks for complex tasks involving
processing of sequential data, time series modeling, and system identification.
Echo state networks (ESNs), a type of reservoir computer, mirror neural
networks but simplify training. They apply fixed, random linear transformations
to the internal state, followed by nonlinear changes. This process, guided by
input signals and linear regression, adapts the system to match target
characteristics, reducing computational demands. A potential drawback of ESNs
is that the fixed reservoir may not offer the complexity needed for specific
problems. While directly altering (training) the internal ESN would reintroduce
the computational burden, an indirect modification can be achieved by
redirecting some output as input. This feedback can influence the internal
reservoir state, yielding ESNs with enhanced complexity suitable for broader
challenges. In this paper, we demonstrate that by feeding some component of the
reservoir state back into the network through the input, we can drastically
improve upon the performance of a given ESN. We rigorously prove that, for any
given ESN, feedback will almost always improve the accuracy of the output. For
a set of three tasks, each representing different problem classes, we find that
with feedback the average error measures are reduced by $30\%-60\%$.
Remarkably, feedback provides at least an equivalent performance boost to
doubling the initial number of computational nodes, a computationally expensive
and technologically challenging alternative. These results demonstrate the
broad applicability and substantial usefulness of this feedback scheme.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2008.07007">Interpretable Representations in Explainable AI: From Theory to Practice. (arXiv:2008.07007v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sokol_K/0/1/0/all/0/1">Kacper Sokol</a>, <a href="http://arxiv.org/find/cs/1/au:+Flach_P/0/1/0/all/0/1">Peter Flach</a></p>
<p>Interpretable representations are the backbone of many explainers that target
black-box predictive systems based on artificial intelligence and machine
learning algorithms. They translate the low-level data representation necessary
for good predictive performance into high-level human-intelligible concepts
used to convey the explanatory insights. Notably, the explanation type and its
cognitive complexity are directly controlled by the interpretable
representation, tweaking which allows to target a particular audience and use
case. However, many explainers built upon interpretable representations
overlook their merit and fall back on default solutions that often carry
implicit assumptions, thereby degrading the explanatory power and reliability
of such techniques. To address this problem, we study properties of
interpretable representations that encode presence and absence of
human-comprehensible concepts. We demonstrate how they are operationalised for
tabular, image and text data; discuss their assumptions, strengths and
weaknesses; identify their core building blocks; and scrutinise their
configuration and parameterisation. In particular, this in-depth analysis
allows us to pinpoint their explanatory properties, desiderata and scope for
(malicious) manipulation in the context of tabular data where a linear model is
used to quantify the influence of interpretable concepts on a black-box
prediction. Our findings lead to a range of recommendations for designing
trustworthy interpretable representations; specifically, the benefits of
class-aware (supervised) discretisation of tabular data, e.g., with decision
trees, and sensitivity of image interpretable representations to segmentation
granularity and occlusion colour.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2012.11816">Molecular CT: Unifying Geometry and Representation Learning for Molecules at Different Scales. (arXiv:2012.11816v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1">Yao-Kun Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yaqiang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yi Isaac Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yi Qin Gao</a></p>
<p>Deep learning is changing many areas in molecular physics, and it has shown
great potential to deliver new solutions to challenging molecular modeling
problems. Along with this trend arises the increasing demand of expressive and
versatile neural network architectures which are compatible with molecular
systems. A new deep neural network architecture, Molecular Configuration
Transformer (Molecular CT), is introduced for this purpose. Molecular CT is
composed of a relation-aware encoder module and a computationally universal
geometry learning unit, thus able to account for the relational constraints
between particles meanwhile scalable to different particle numbers and
invariant with respect to the trans-rotational transforms. The computational
efficiency and universality make Molecular CT versatile for a variety of
molecular learning scenarios and especially appealing for transferable
representation learning across different molecular systems. As examples, we
show that Molecular CT enables representational learning for molecular systems
at different scales, and achieves comparable or improved results on common
benchmarks using a more light-weighted structure compared to baseline models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2109.12679">Be More Active! Understanding the Differences between Mean and Sampled Representations of Variational Autoencoders. (arXiv:2109.12679v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bonheme_L/0/1/0/all/0/1">Lisa Bonheme</a>, <a href="http://arxiv.org/find/cs/1/au:+Grzes_M/0/1/0/all/0/1">Marek Grzes</a></p>
<p>The ability of Variational Autoencoders to learn disentangled representations
has made them appealing for practical applications. However, their mean
representations, which are generally used for downstream tasks, have recently
been shown to be more correlated than their sampled counterpart, on which
disentanglement is usually measured. In this paper, we refine this observation
through the lens of selective posterior collapse, which states that only a
subset of the learned representations, the active variables, is encoding useful
information while the rest (the passive variables) is discarded. We first
extend the existing definition to multiple data examples and show that active
variables are equally disentangled in mean and sampled representations. Based
on this extension and the pre-trained models from disentanglement lib, we then
isolate the passive variables and show that they are responsible for the
discrepancies between mean and sampled representations. Specifically, passive
variables exhibit high correlation scores with other variables in mean
representations while being fully uncorrelated in sampled ones. We thus
conclude that despite what their higher correlation might suggest, mean
representations are still good candidates for downstream tasks applications.
However, it may be beneficial to remove their passive variables, especially
when used with models sensitive to correlated features.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2111.04941">Solving PDE-constrained Control Problems Using Operator Learning. (arXiv:2111.04941v3 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Hwang_R/0/1/0/all/0/1">Rakhoon Hwang</a>, <a href="http://arxiv.org/find/math/1/au:+Lee_J/0/1/0/all/0/1">Jae Yong Lee</a>, <a href="http://arxiv.org/find/math/1/au:+Shin_J/0/1/0/all/0/1">Jin Young Shin</a>, <a href="http://arxiv.org/find/math/1/au:+Hwang_H/0/1/0/all/0/1">Hyung Ju Hwang</a></p>
<p>The modeling and control of complex physical systems are essential in
real-world problems. We propose a novel framework that is generally applicable
to solving PDE-constrained optimal control problems by introducing surrogate
models for PDE solution operators with special regularizers. The procedure of
the proposed framework is divided into two phases: solution operator learning
for PDE constraints (Phase 1) and searching for optimal control (Phase 2). Once
the surrogate model is trained in Phase 1, the optimal control can be inferred
in Phase 2 without intensive computations. Our framework can be applied to both
data-driven and data-free cases. We demonstrate the successful application of
our method to various optimal control problems for different control variables
with diverse PDE constraints from the Poisson equation to Burgers' equation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2111.05486">Uncoupled Bandit Learning towards Rationalizability: Benchmarks, Barriers, and Algorithms. (arXiv:2111.05486v3 [cs.GT] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jibang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Haifeng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_F/0/1/0/all/0/1">Fan Yao</a></p>
<p>Under the uncoupled learning setup, the last-iterate convergence guarantee
towards Nash equilibrium is shown to be impossible in many games. This work
studies the last-iterate convergence guarantee in general games toward
rationalizability, a key solution concept in epistemic game theory that relaxes
the stringent belief assumptions in both Nash and correlated equilibrium. This
learning task naturally generalizes best arm identification problems, due to
the intrinsic connections between rationalizable action profiles and the
elimination of iteratively dominated actions. Despite a seemingly simple task,
our first main result is a surprisingly negative one; that is, a large and
natural class of no regret algorithms, including the entire family of Dual
Averaging algorithms, provably take exponentially many rounds to reach
rationalizability. Moreover, algorithms with the stronger no swap regret also
suffer similar exponential inefficiency. To overcome these barriers, we develop
a new algorithm that adjusts Exp3 with Diminishing Historical rewards (termed
Exp3-DH); Exp3-DH gradually forgets history at carefully tailored rates. We
prove that when all agents run Exp3-DH (a.k.a., self-play in multi-agent
learning), all iteratively dominated actions can be eliminated within
polynomially many rounds. Our experimental results further demonstrate the
efficiency of Exp3-DH, and that state-of-the-art bandit algorithms, even those
developed specifically for learning in games, fail to reach rationalizability
efficiently.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.07434">Exploring the Limits of Natural Language Inference Based Setup for Few-Shot Intent Detection. (arXiv:2112.07434v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1">Ayush Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Malik_V/0/1/0/all/0/1">Vijit Malik</a>, <a href="http://arxiv.org/find/cs/1/au:+Vepa_J/0/1/0/all/0/1">Jithendra Vepa</a></p>
<p>Intent Detection is one of the core tasks of dialog systems. Few-shot Intent
Detection is challenging due to limited number of annotated utterances for
novel classes. Generalized Few-shot intent detection is more realistic but
challenging setup which aims to discriminate the joint label space of both
novel intents which have few examples each and existing intents consisting of
enough labeled data. Large label spaces and fewer number of shots increase the
complexity of the task. In this work, we employ a simple and effective method
based on Natural Language Inference that leverages the semantics in the
class-label names to learn and predict the novel classes. Our method achieves
state-of-the-art results on 1-shot and 5-shot intent detection task with gains
ranging from 2-8\% points in F1 score on four benchmark datasets. Our method
also outperforms existing approaches on a more practical setting of generalized
few-shot intent detection with gains up to 20% F1 score. We show that the
suggested approach performs well across single and multi domain datasets with
the number of class labels from as few as 7 to as high as 150.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2201.05759">FairIF: Boosting Fairness in Deep Learning via Influence Functions with Validation Set Sensitive Attributes. (arXiv:2201.05759v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haonan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Ziwei Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Jingrui He</a></p>
<p>Most fair machine learning methods either highly rely on the sensitive
information of the training samples or require a large modification on the
target models, which hinders their practical application. To address this
issue, we propose a two-stage training algorithm named FAIRIF. It minimizes the
loss over the reweighted data set (second stage) where the sample weights are
computed to balance the model performance across different demographic groups
(first stage). FAIRIF can be applied on a wide range of models trained by
stochastic gradient descent without changing the model, while only requiring
group annotations on a small validation set to compute sample weights.
Theoretically, we show that, in the classification setting, three notions of
disparity among different groups can be mitigated by training with the weights.
Experiments on synthetic data sets demonstrate that FAIRIF yields models with
better fairness-utility trade-offs against various types of bias; and on
real-world data sets, we show the effectiveness and scalability of FAIRIF.
Moreover, as evidenced by the experiments with pretrained models, FAIRIF is
able to alleviate the unfairness issue of pretrained models without hurting
their performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.00526">Multi-Objective Latent Space Optimization of Generative Molecular Design Models. (arXiv:2203.00526v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abeer_A/0/1/0/all/0/1">A N M Nafiz Abeer</a>, <a href="http://arxiv.org/find/cs/1/au:+Urban_N/0/1/0/all/0/1">Nathan Urban</a>, <a href="http://arxiv.org/find/cs/1/au:+Weil_M/0/1/0/all/0/1">M Ryan Weil</a>, <a href="http://arxiv.org/find/cs/1/au:+Alexander_F/0/1/0/all/0/1">Francis J. Alexander</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_B/0/1/0/all/0/1">Byung-Jun Yoon</a></p>
<p>Molecular design based on generative models, such as variational autoencoders
(VAEs), has become increasingly popular in recent years due to its efficiency
for exploring high-dimensional molecular space to identify molecules with
desired properties. While the efficacy of the initial model strongly depends on
the training data, the sampling efficiency of the model for suggesting novel
molecules with enhanced properties can be further enhanced via latent space
optimization. In this paper, we propose a multi-objective latent space
optimization (LSO) method that can significantly enhance the performance of
generative molecular design (GMD). The proposed method adopts an iterative
weighted retraining approach, where the respective weights of the molecules in
the training data are determined by their Pareto efficiency. We demonstrate
that our multi-objective GMD LSO method can significantly improve the
performance of GMD for jointly optimizing multiple molecular properties.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.01077">Addressing Gap between Training Data and Deployed Environment by On-Device Learning. (arXiv:2203.01077v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sunaga_K/0/1/0/all/0/1">Kazuki Sunaga</a>, <a href="http://arxiv.org/find/cs/1/au:+Kondo_M/0/1/0/all/0/1">Masaaki Kondo</a>, <a href="http://arxiv.org/find/cs/1/au:+Matsutani_H/0/1/0/all/0/1">Hiroki Matsutani</a></p>
<p>The accuracy of tinyML applications is often affected by various
environmental factors, such as noises, location/calibration of sensors, and
time-related changes. This article introduces a neural network based on-device
learning (ODL) approach to address this issue by retraining in deployed
environments. Our approach relies on semi-supervised sequential training of
multiple neural networks tailored for low-end edge devices. This article
introduces its algorithm and implementation on wireless sensor nodes consisting
of a Raspberry Pi Pico and low-power wireless module. Experiments using
vibration patterns of rotating machines demonstrate that retraining by ODL
improves anomaly detection accuracy compared with a prediction-only deep neural
network in a noisy environment. The results also show that the ODL approach can
save communication cost and energy consumption for battery-powered Internet of
Things devices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.01515">Understanding Deep Learning via Decision Boundary. (arXiv:2206.01515v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1">Shiye Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1">Fengxiang He</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1">Yancheng Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1">Dacheng Tao</a></p>
<p>This paper discovers that the neural network with lower decision boundary
(DB) variability has better generalizability. Two new notions, algorithm DB
variability and $(\epsilon, \eta)$-data DB variability, are proposed to measure
the decision boundary variability from the algorithm and data perspectives.
Extensive experiments show significant negative correlations between the
decision boundary variability and the generalizability. From the theoretical
view, two lower bounds based on algorithm DB variability are proposed and do
not explicitly depend on the sample size. We also prove an upper bound of order
$\mathcal{O}\left(\frac{1}{\sqrt{m}}+\epsilon+\eta\log\frac{1}{\eta}\right)$
based on data DB variability. The bound is convenient to estimate without the
requirement of labels, and does not explicitly depend on the network size which
is usually prohibitively large in deep learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.02659">Robust Fine-Tuning of Deep Neural Networks with Hessian-based Generalization Guarantees. (arXiv:2206.02659v6 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ju_H/0/1/0/all/0/1">Haotian Ju</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dongyue Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongyang R. Zhang</a></p>
<p>We consider fine-tuning a pretrained deep neural network on a target task. We
study the generalization properties of fine-tuning to understand the problem of
overfitting, which has often been observed (e.g., when the target dataset is
small or when the training labels are noisy). Existing generalization measures
for deep networks depend on notions such as distance from the initialization
(i.e., the pretrained network) of the fine-tuned model and noise stability
properties of deep networks. This paper identifies a Hessian-based distance
measure through PAC-Bayesian analysis, which is shown to correlate well with
observed generalization gaps of fine-tuned models. Theoretically, we prove
Hessian distance-based generalization bounds for fine-tuned models. We also
describe an extended study of fine-tuning against label noise, where
overfitting remains a critical problem. We present an algorithm and a
generalization error guarantee for this algorithm under a class conditional
independent noise model. Empirically, we observe that the Hessian-based
distance measure can match the scale of the observed generalization gap of
fine-tuned models in practice. We also test our algorithm on several image
classification tasks with noisy training labels, showing gains over prior
methods and decreases in the Hessian distance measure of the fine-tuned model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.06339">Learning robust marking policies for adaptive mesh refinement. (arXiv:2207.06339v2 [math.NA] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Gillette_A/0/1/0/all/0/1">Andrew Gillette</a>, <a href="http://arxiv.org/find/math/1/au:+Keith_B/0/1/0/all/0/1">Brendan Keith</a>, <a href="http://arxiv.org/find/math/1/au:+Petrides_S/0/1/0/all/0/1">Socratis Petrides</a></p>
<p>In this work, we revisit the marking decisions made in the standard adaptive
finite element method (AFEM). Experience shows that a na\"{i}ve marking policy
leads to inefficient use of computational resources for adaptive mesh
refinement (AMR). Consequently, using AFEM in practice often involves ad-hoc or
time-consuming offline parameter tuning to set appropriate parameters for the
marking subroutine. To address these practical concerns, we recast AMR as a
Markov decision process in which refinement parameters can be selected
on-the-fly at run time, without the need for pre-tuning by expert users. In
this new paradigm, the refinement parameters are also chosen adaptively via a
marking policy that can be optimized using methods from reinforcement learning.
We use the Poisson equation to demonstrate our techniques on $h$- and
$hp$-refinement benchmark problems, and our experiments suggest that superior
marking policies remain undiscovered for many classical AFEM applications.
Furthermore, an unexpected observation from this work is that marking policies
trained on one family of PDEs are sometimes robust enough to perform well on
problems far outside the training family. For illustration, we show that a
simple $hp$-refinement policy trained on 2D domains with only a single
re-entrant corner can be deployed on far more complicated 2D domains, and even
3D domains, without significant performance loss. For reproduction and broader
adoption, we accompany this work with an open-source implementation of our
methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.08143">Can large language models reason about medical questions?. (arXiv:2207.08143v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lievin_V/0/1/0/all/0/1">Valentin Li&#xe9;vin</a>, <a href="http://arxiv.org/find/cs/1/au:+Hother_C/0/1/0/all/0/1">Christoffer Egeberg Hother</a>, <a href="http://arxiv.org/find/cs/1/au:+Motzfeldt_A/0/1/0/all/0/1">Andreas Geert Motzfeldt</a>, <a href="http://arxiv.org/find/cs/1/au:+Winther_O/0/1/0/all/0/1">Ole Winther</a></p>
<p>Although large language models (LLMs) often produce impressive outputs, it
remains unclear how they perform in real-world scenarios requiring strong
reasoning skills and expert domain knowledge. We set out to investigate whether
close- and open-source models (GPT-3.5, LLama-2, etc.) can be applied to answer
and reason about difficult real-world-based questions. We focus on three
popular medical benchmarks (MedQA-USMLE, MedMCQA, and PubMedQA) and multiple
prompting scenarios: Chain-of-Thought (CoT, think step-by-step), few-shot and
retrieval augmentation. Based on an expert annotation of the generated CoTs, we
found that InstructGPT can often read, reason and recall expert knowledge.
Last, by leveraging advances in prompt engineering (few-shot and ensemble
methods), we demonstrated that GPT-3.5 not only yields calibrated predictive
distributions, but also reaches the passing score on three datasets:
MedQA-USMLE 60.2%, MedMCQA 62.7% and PubMedQA 78.2%. Open-source models are
closing the gap: Llama-2 70B also passed the MedQA-USMLE with 62.5% accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.14539">Pre-training General Trajectory Embeddings with Maximum Multi-view Entropy Coding. (arXiv:2207.14539v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yan Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_H/0/1/0/all/0/1">Huaiyu Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1">Shengnan Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1">Jilin Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jensen_C/0/1/0/all/0/1">Christian S. Jensen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Youfang Lin</a></p>
<p>Spatio-temporal trajectories provide valuable information about movement and
travel behavior, enabling various downstream tasks that in turn power
real-world applications. Learning trajectory embeddings can improve task
performance but may incur high computational costs and face limited training
data availability. Pre-training learns generic embeddings by means of specially
constructed pretext tasks that enable learning from unlabeled data. Existing
pre-training methods face (i) difficulties in learning general embeddings due
to biases towards certain downstream tasks incurred by the pretext tasks, (ii)
limitations in capturing both travel semantics and spatio-temporal
correlations, and (iii) the complexity of long, irregularly sampled
trajectories.
</p>
<p>To tackle these challenges, we propose Maximum Multi-view Trajectory Entropy
Coding (MMTEC) for learning general and comprehensive trajectory embeddings. We
introduce a pretext task that reduces biases in pre-trained trajectory
embeddings, yielding embeddings that are useful for a wide variety of
downstream tasks. We also propose an attention-based discrete encoder and a
NeuralCDE-based continuous encoder that extract and represent travel behavior
and continuous spatio-temporal correlations from trajectories in embeddings,
respectively. Extensive experiments on two real-world datasets and three
downstream tasks offer insight into the design properties of our proposal and
indicate that it is capable of outperforming existing trajectory embedding
methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.14653">Ensemble forecasts in reproducing kernel Hilbert space family. (arXiv:2207.14653v3 [math-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math-ph/1/au:+Dufee_B/0/1/0/all/0/1">Benjamin Duf&#xe9;e</a>, <a href="http://arxiv.org/find/math-ph/1/au:+Hug_B/0/1/0/all/0/1">B&#xe9;renger Hug</a>, <a href="http://arxiv.org/find/math-ph/1/au:+Memin_E/0/1/0/all/0/1">Etienne M&#xe9;min</a>, <a href="http://arxiv.org/find/math-ph/1/au:+Tissot_G/0/1/0/all/0/1">Gilles Tissot</a></p>
<p>A methodological framework for ensemble-based estimation and simulation of
high dimensional dynamical systems such as the oceanic or atmospheric flows is
proposed. To that end, the dynamical system is embedded in a family of
reproducing kernel Hilbert spaces (RKHS) with kernel functions driven by the
dynamics. In the RKHS family, the Koopman and Perron-Frobenius operators are
unitary and uniformly continuous. This property warrants they can be expressed
in exponential series of diagonalizable bounded evolution operators defined
from their infinitesimal generators. Access to Lyapunov exponents and to exact
ensemble based expressions of the tangent linear dynamics are directly
available as well. The RKHS family enables us the devise of strikingly simple
ensemble data assimilation methods for trajectory reconstructions in terms of
constant-in-time linear combinations of trajectory samples. Such an
embarrassingly simple strategy is made possible through a fully justified
superposition principle ensuing from several fundamental theorems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.07316">MENLI: Robust Evaluation Metrics from Natural Language Inference. (arXiv:2208.07316v5 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yanran Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1">Steffen Eger</a></p>
<p>Recently proposed BERT-based evaluation metrics for text generation perform
well on standard benchmarks but are vulnerable to adversarial attacks, e.g.,
relating to information correctness. We argue that this stems (in part) from
the fact that they are models of semantic similarity. In contrast, we develop
evaluation metrics based on Natural Language Inference (NLI), which we deem a
more appropriate modeling. We design a preference-based adversarial attack
framework and show that our NLI based metrics are much more robust to the
attacks than the recent BERT-based metrics. On standard benchmarks, our NLI
based metrics outperform existing summarization metrics, but perform below SOTA
MT metrics. However, when combining existing metrics with our NLI metrics, we
obtain both higher adversarial robustness (15%-30%) and higher quality metrics
as measured on standard benchmarks (+5% to 30%).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.02442">SimCLF: A Simple Contrastive Learning Framework for Function-level Binary Embeddings. (arXiv:2209.02442v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+RuiJin_S/0/1/0/all/0/1">Sun RuiJin</a>, <a href="http://arxiv.org/find/cs/1/au:+Shize_G/0/1/0/all/0/1">Guo Shize</a>, <a href="http://arxiv.org/find/cs/1/au:+Jinhong_G/0/1/0/all/0/1">Guo Jinhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1">Li Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Dazhi_Z/0/1/0/all/0/1">Zhan Dazhi</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_S/0/1/0/all/0/1">Sun Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhisong_P/0/1/0/all/0/1">Pan Zhisong</a></p>
<p>Function-level binary code similarity detection is a crucial aspect of
cybersecurity. It enables the detection of bugs and patent infringements in
released software and plays a pivotal role in preventing supply chain attacks.
A practical embedding learning framework relies on the robustness of the
assembly code representation and the accuracy of function-pair annotation,
which is traditionally accomplished using supervised learning-based frameworks.
However, annotating different function pairs with accurate labels poses
considerable challenges. These supervised learning methods can be easily
overtrained and suffer from representation robustness problems. To address
these challenges, we propose SimCLF: A Simple Contrastive Learning Framework
for Function-level Binary Embeddings. We take an unsupervised learning approach
and formulate binary code similarity detection as instance discrimination.
SimCLF directly operates on disassembled binary functions and could be
implemented with any encoder. It does not require manually annotated
information but only augmented data. Augmented data is generated using compiler
optimization options and code obfuscation techniques. The experimental results
demonstrate that SimCLF surpasses the state-of-the-art in accuracy and has a
significant advantage in few-shot settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.02535">Analyzing Transformers in Embedding Space. (arXiv:2209.02535v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dar_G/0/1/0/all/0/1">Guy Dar</a>, <a href="http://arxiv.org/find/cs/1/au:+Geva_M/0/1/0/all/0/1">Mor Geva</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1">Ankit Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1">Jonathan Berant</a></p>
<p>Understanding Transformer-based models has attracted significant attention,
as they lie at the heart of recent technological advances across machine
learning. While most interpretability methods rely on running models over
inputs, recent work has shown that a zero-pass approach, where parameters are
interpreted directly without a forward/backward pass is feasible for some
Transformer parameters, and for two-layer attention networks. In this work, we
present a theoretical analysis where all parameters of a trained Transformer
are interpreted by projecting them into the embedding space, that is, the space
of vocabulary items they operate on. We derive a simple theoretical framework
to support our arguments and provide ample evidence for its validity. First, an
empirical analysis showing that parameters of both pretrained and fine-tuned
models can be interpreted in embedding space. Second, we present two
applications of our framework: (a) aligning the parameters of different models
that share a vocabulary, and (b) constructing a classifier without training by
``translating'' the parameters of a fine-tuned classifier to parameters of a
different model that was only pretrained. Overall, our findings open the door
to interpretation methods that, at least in part, abstract away from model
specifics and operate in the embedding space only.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.02215">On the Statistical Complexity of Estimation and Testing under Privacy Constraints. (arXiv:2210.02215v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lalanne_C/0/1/0/all/0/1">Cl&#xe9;ment Lalanne</a> (DANTE, OCKHAM), <a href="http://arxiv.org/find/cs/1/au:+Garivier_A/0/1/0/all/0/1">Aur&#xe9;lien Garivier</a> (UMPA-ENSL), <a href="http://arxiv.org/find/cs/1/au:+Gribonval_R/0/1/0/all/0/1">R&#xe9;mi Gribonval</a> (DANTE, OCKHAM)</p>
<p>The challenge of producing accurate statistics while respecting the privacy
of the individuals in a sample is an important area of research. We study
minimax lower bounds for classes of differentially private estimators. In
particular, we show how to characterize the power of a statistical test under
differential privacy in a plug-and-play fashion by solving an appropriate
transport problem. With specific coupling constructions, this observation
allows us to derive Le Cam-type and Fano-type inequalities not only for regular
definitions of differential privacy but also for those based on Renyi
divergence. We then proceed to illustrate our results on three simple, fully
worked out examples. In particular, we show that the problem class has a huge
importance on the provable degradation of utility due to privacy. In certain
scenarios, we show that maintaining privacy results in a noticeable reduction
in performance only when the level of privacy protection is very high.
Conversely, for other problems, even a modest level of privacy protection can
lead to a significant decrease in performance. Finally, we demonstrate that the
DP-SGLD algorithm, a private convex solver, can be employed for maximum
likelihood estimation with a high degree of confidence, as it provides
near-optimal results with respect to both the size of the sample and the level
of privacy protection. This algorithm is applicable to a broad range of
parametric estimation procedures, including exponential families.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.08415">Stability of Accuracy for the Training of DNNs Via the Uniform Doubling Condition. (arXiv:2210.08415v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shmalo_Y/0/1/0/all/0/1">Yitzchak Shmalo</a></p>
<p>We study the stability of accuracy during the training of deep neural
networks (DNNs). In this context, the training of a DNN is performed via the
minimization of a cross-entropy loss function, and the performance metric is
accuracy (the proportion of objects that are classified correctly). While
training results in a decrease of loss, the accuracy does not necessarily
increase during the process and may sometimes even decrease. The goal of
achieving stability of accuracy is to ensure that if accuracy is high at some
initial time, it remains high throughout training.
</p>
<p>A recent result by Berlyand, Jabin, and Safsten introduces a doubling
condition on the training data, which ensures the stability of accuracy during
training for DNNs using the absolute value activation function. For training
data in $\mathbb{R}^n$, this doubling condition is formulated using slabs in
$\mathbb{R}^n$ and depends on the choice of the slabs. The goal of this paper
is twofold. First, to make the doubling condition uniform, that is, independent
of the choice of slabs. This leads to sufficient conditions for stability in
terms of training data only. In other words, for a training set $T$ that
satisfies the uniform doubling condition, there exists a family of DNNs such
that a DNN from this family with high accuracy on the training set at some
training time $t_0$ will have high accuracy for all time $t&gt;t_0$. Moreover,
establishing uniformity is necessary for the numerical implementation of the
doubling condition.
</p>
<p>The second goal is to extend the original stability results from the absolute
value activation function to a broader class of piecewise linear activation
functions with finitely many critical points, such as the popular Leaky ReLU.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.12723">A Faithful Deep Sensitivity Estimation for Accelerated Magnetic Resonance Imaging. (arXiv:2210.12723v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1">Zi Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Fang_H/0/1/0/all/0/1">Haoming Fang</a>, <a href="http://arxiv.org/find/eess/1/au:+Qian_C/0/1/0/all/0/1">Chen Qian</a>, <a href="http://arxiv.org/find/eess/1/au:+Shi_B/0/1/0/all/0/1">Boxuan Shi</a>, <a href="http://arxiv.org/find/eess/1/au:+Bao_L/0/1/0/all/0/1">Lijun Bao</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhu_L/0/1/0/all/0/1">Liuhong Zhu</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_J/0/1/0/all/0/1">Jianjun Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Wei_W/0/1/0/all/0/1">Wenping Wei</a>, <a href="http://arxiv.org/find/eess/1/au:+Lin_J/0/1/0/all/0/1">Jianzhong Lin</a>, <a href="http://arxiv.org/find/eess/1/au:+Guo_D/0/1/0/all/0/1">Di Guo</a>, <a href="http://arxiv.org/find/eess/1/au:+Qu_X/0/1/0/all/0/1">Xiaobo Qu</a></p>
<p>Magnetic resonance imaging (MRI) is an essential diagnostic tool that suffers
from prolonged scan time. To alleviate this limitation, advanced fast MRI
technology attracts extensive research interests. Recent deep learning has
shown its great potential in improving image quality and reconstruction speed.
Faithful coil sensitivity estimation is vital for MRI reconstruction. However,
most deep learning methods still rely on pre-estimated sensitivity maps and
ignore their inaccuracy, resulting in the significant quality degradation of
reconstructed images. In this work, we propose a Joint Deep Sensitivity
estimation and Image reconstruction network, called JDSI. During the image
artifacts removal, it gradually provides more faithful sensitivity maps with
high-frequency information, leading to improved image reconstructions. To
understand the behavior of the network, the mutual promotion of sensitivity
estimation and image reconstruction is revealed through the visualization of
network intermediate results. Results on in vivo datasets and radiologist
reader study demonstrate that, for both calibration-based and calibrationless
reconstruction, the proposed JDSI achieves the state-of-the-art performance
visually and quantitatively, especially when the acceleration factor is high.
Additionally, JDSI owns nice robustness to patients and autocalibration
signals.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.04218">Clustered Federated Learning based on Nonconvex Pairwise Fusion. (arXiv:2211.04218v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1">Xue Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Ziyi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yifan Sun</a></p>
<p>This study investigates clustered federated learning (FL), one of the
formulations of FL with non-i.i.d. data, where the devices are partitioned into
clusters and each cluster optimally fits its data with a localized model. We
propose a clustered FL framework that incorporates a nonconvex penalty to
pairwise differences of parameters. Without a priori knowledge of the set of
devices in each cluster and the number of clusters, this framework can
autonomously estimate cluster structures. To implement the proposed framework,
we introduce a novel clustered FL method called Fusion Penalized Federated
Clustering (FPFC). Building upon the standard alternating direction method of
multipliers (ADMM), FPFC can perform partial updates at each communication
round and allows parallel computation with variable workload. These strategies
significantly reduce the communication cost while ensuring privacy, making it
practical for FL. We also propose a new warmup strategy for hyperparameter
tuning in FL settings and explore the asynchronous variant of FPFC (asyncFPFC).
Theoretical analysis provides convergence guarantees for FPFC with general
losses and establishes the statistical convergence rate under a linear model
with squared loss. Extensive experiments have demonstrated the superiority of
FPFC compared to current methods, including robustness and generalization
capability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.07206">Scalable PAC-Bayesian Meta-Learning via the PAC-Optimal Hyper-Posterior: From Theory to Practice. (arXiv:2211.07206v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Rothfuss_J/0/1/0/all/0/1">Jonas Rothfuss</a>, <a href="http://arxiv.org/find/stat/1/au:+Josifoski_M/0/1/0/all/0/1">Martin Josifoski</a>, <a href="http://arxiv.org/find/stat/1/au:+Fortuin_V/0/1/0/all/0/1">Vincent Fortuin</a>, <a href="http://arxiv.org/find/stat/1/au:+Krause_A/0/1/0/all/0/1">Andreas Krause</a></p>
<p>Meta-Learning aims to speed up the learning process on new tasks by acquiring
useful inductive biases from datasets of related learning tasks. While, in
practice, the number of related tasks available is often small, most of the
existing approaches assume an abundance of tasks; making them unrealistic and
prone to overfitting. A central question in the meta-learning literature is how
to regularize to ensure generalization to unseen tasks. In this work, we
provide a theoretical analysis using the PAC-Bayesian theory and present a
generalization bound for meta-learning, which was first derived by Rothfuss et
al. (2021a). Crucially, the bound allows us to derive the closed form of the
optimal hyper-posterior, referred to as PACOH, which leads to the best
performance guarantees. We provide a theoretical analysis and empirical case
study under which conditions and to what extent these guarantees for
meta-learning improve upon PAC-Bayesian per-task learning bounds. The
closed-form PACOH inspires a practical meta-learning approach that avoids the
reliance on bi-level optimization, giving rise to a stochastic optimization
problem that is amenable to standard variational methods that scale well. Our
experiments show that, when instantiating the PACOH with Gaussian processes and
Bayesian Neural Networks models, the resulting methods are more scalable, and
yield state-of-the-art performance, both in terms of predictive accuracy and
the quality of uncertainty estimates.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.01792">Classification by sparse additive models. (arXiv:2212.01792v3 [math.ST] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Abramovich_F/0/1/0/all/0/1">Felix Abramovich</a></p>
<p>We consider (nonparametric) sparse additive models (SpAM) for classification.
The design of a SpAM classifier is based on minimizing the logistic loss with a
sparse group Lasso/Slope-type penalties on the coefficients of univariate
additive components' expansions in orthonormal series (e.g., Fourier or
wavelets). The resulting classifier is inherently adaptive to the unknown
sparsity and smoothness. We show that under certain sparse group restricted
eigenvalue condition it is nearly-minimax (up to log-factors) simultaneously
across the entire range of analytic, Sobolev and Besov classes. The performance
of the proposed classifier is illustrated on a simulated and a real-data
examples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.02758">Tackling Data Heterogeneity in Federated Learning with Class Prototypes. (arXiv:2212.02758v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1">Yutong Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zeyuan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Junnan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Heinecke_S/0/1/0/all/0/1">Shelby Heinecke</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Lichao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">Ran Xu</a></p>
<p>Data heterogeneity across clients in federated learning (FL) settings is a
widely acknowledged challenge. In response, personalized federated learning
(PFL) emerged as a framework to curate local models for clients' tasks. In PFL,
a common strategy is to develop local and global models jointly - the global
model (for generalization) informs the local models, and the local models (for
personalization) are aggregated to update the global model. A key observation
is that if we can improve the generalization ability of local models, then we
can improve the generalization of global models, which in turn builds better
personalized models. In this work, we consider class imbalance, an overlooked
type of data heterogeneity, in the classification setting. We propose FedNH, a
novel method that improves the local models' performance for both
personalization and generalization by combining the uniformity and semantics of
class prototypes. FedNH initially distributes class prototypes uniformly in the
latent space and smoothly infuses the class semantics into class prototypes. We
show that imposing uniformity helps to combat prototype collapse while infusing
class semantics improves local models. Extensive experiments were conducted on
popular classification datasets under the cross-device setting. Our results
demonstrate the effectiveness and stability of our method over recent works.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.05259">Online Real-time Learning of Dynamical Systems from Noisy Streaming Data: A Koopman Operator Approach. (arXiv:2212.05259v2 [math.DS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Sinha_S/0/1/0/all/0/1">S. Sinha</a>, <a href="http://arxiv.org/find/math/1/au:+Nandanoori_S/0/1/0/all/0/1">Sai P. Nandanoori</a>, <a href="http://arxiv.org/find/math/1/au:+Barajas_Solano_D/0/1/0/all/0/1">David Barajas-Solano</a></p>
<p>Recent advancements in sensing and communication facilitate obtaining
high-frequency real-time data from various physical systems like power
networks, climate systems, biological networks, etc. However, since the data
are recorded by physical sensors, it is natural that the obtained data is
corrupted by measurement noise. In this paper, we present a novel algorithm for
online real-time learning of dynamical systems from noisy time-series data,
which employs the Robust Koopman operator framework to mitigate the effect of
measurement noise. The proposed algorithm has three main advantages: a) it
allows for online real-time monitoring of a dynamical system; b) it obtains a
linear representation of the underlying dynamical system, thus enabling the
user to use linear systems theory for analysis and control of the system; c) it
is computationally fast and less intensive than the popular Extended Dynamic
Mode Decomposition (EDMD) algorithm. We illustrate the efficiency of the
proposed algorithm by applying it to identify the Van der Pol oscillator, the
IEEE 68 bus system, and a ring network of Van der Pol oscillators.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.08965">Physics-informed Neural Networks with Periodic Activation Functions for Solute Transport in Heterogeneous Porous Media. (arXiv:2212.08965v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Faroughi_S/0/1/0/all/0/1">Salah A Faroughi</a>, <a href="http://arxiv.org/find/cs/1/au:+Soltanmohammad_R/0/1/0/all/0/1">Ramin Soltanmohammad</a>, <a href="http://arxiv.org/find/cs/1/au:+Datta_P/0/1/0/all/0/1">Pingki Datta</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahjour_S/0/1/0/all/0/1">Seyed Kourosh Mahjour</a>, <a href="http://arxiv.org/find/cs/1/au:+Faroughi_S/0/1/0/all/0/1">Shirko Faroughi</a></p>
<p>Simulating solute transport in heterogeneous porous media poses computational
challenges due to the high-resolution meshing required for traditional solvers.
To overcome these challenges, this study explores a mesh-free method based on
deep learning to accelerate solute transport simulation. We employ
Physics-informed Neural Networks (PiNN) with a periodic activation function to
solve solute transport problems in both homogeneous and heterogeneous porous
media governed by the advection-dispersion equation. Unlike traditional neural
networks that rely on large training datasets, PiNNs use strong-form
mathematical models to constrain the network in the training phase and
simultaneously solve for multiple dependent or independent field variables,
such as pressure and solute concentration fields. To demonstrate the
effectiveness of using PiNNs with a periodic activation function to resolve
solute transport in porous media, we construct PiNNs using two activation
functions, sin and tanh, for seven case studies, including 1D and 2D scenarios.
The accuracy of the PiNNs' predictions is then evaluated using absolute point
error and mean square error metrics and compared to the ground truth solutions
obtained analytically or numerically. Our results demonstrate that the PiNN
with sin activation function, compared to tanh activation function, is up to
two orders of magnitude more accurate and up to two times faster to train,
especially in heterogeneous porous media. Moreover, PiNN's simultaneous
predictions of pressure and concentration fields can reduce computational
expenses in terms of inference time by three orders of magnitude compared to
FEM simulations for two-dimensional cases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.01829">t-SMILES: A Scalable Fragment-based Molecular Representation Framework for De Novo Molecule Generation. (arXiv:2301.01829v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Juan-Ni Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yue Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1">Li-Juan Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Hai-Long Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1">Ru-Qin Yu</a></p>
<p>Effective representation of molecules is a crucial factor affecting the
performance of artificial intelligence models. This study introduces a
flexible, fragment-based, multiscale molecular representation framework called
t-SMILES (tree-based SMILES) with three code algorithms: TSSA (t-SMILES with
Shared Atom), TSDY (t-SMILES with Dummy Atom) and TSID (t-SMILES with ID). It
describes molecules using SMILES-type strings obtained by performing a
breadth-first search on a full binary tree formed from a fragmented molecular
graph. Systematic evaluations using JTVAE, BRICS, MMPA, and Scaffold show the
feasibility to construct a multilingual molecular description system, where
various descriptions complement each other, enhancing the overall performance.
Additionally, it exhibits impressive performance on low-resource datasets,
whether the model is original, data augmented, or pre-training fine-tuned. It
significantly outperforms classical SMILES, DeepSMILES, SELFIES and baseline
models in goal-directed tasks. Furthermore, it surpasses start-of-the-art
fragment, graph and SMILES based approaches on ChEMBL, Zinc, and QM9.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.05603">A Comprehensive Survey of Dataset Distillation. (arXiv:2301.05603v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1">Shiye Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1">Dacheng Tao</a></p>
<p>Deep learning technology has developed unprecedentedly in the last decade and
has become the primary choice in many application domains. This progress is
mainly attributed to a systematic collaboration in which rapidly growing
computing resources encourage advanced algorithms to deal with massive data.
However, it has gradually become challenging to handle the unlimited growth of
data with limited computing power. To this end, diverse approaches are proposed
to improve data processing efficiency. Dataset distillation, a dataset
reduction method, addresses this problem by synthesizing a small typical
dataset from substantial data and has attracted much attention from the deep
learning community. Existing dataset distillation methods can be taxonomized
into meta-learning and data matching frameworks according to whether they
explicitly mimic the performance of target data. Although dataset distillation
has shown surprising performance in compressing datasets, there are still
several limitations such as distilling high-resolution data or data with
complex label spaces. This paper provides a holistic understanding of dataset
distillation from multiple aspects, including distillation frameworks and
algorithms, factorized dataset distillation, performance comparison, and
applications. Finally, we discuss challenges and promising directions to
further promote future studies on dataset distillation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.10737">Distributed Control of Partial Differential Equations Using Convolutional Reinforcement Learning. (arXiv:2301.10737v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peitz_S/0/1/0/all/0/1">Sebastian Peitz</a>, <a href="http://arxiv.org/find/cs/1/au:+Stenner_J/0/1/0/all/0/1">Jan Stenner</a>, <a href="http://arxiv.org/find/cs/1/au:+Chidananda_V/0/1/0/all/0/1">Vikas Chidananda</a>, <a href="http://arxiv.org/find/cs/1/au:+Wallscheid_O/0/1/0/all/0/1">Oliver Wallscheid</a>, <a href="http://arxiv.org/find/cs/1/au:+Brunton_S/0/1/0/all/0/1">Steven L. Brunton</a>, <a href="http://arxiv.org/find/cs/1/au:+Taira_K/0/1/0/all/0/1">Kunihiko Taira</a></p>
<p>We present a convolutional framework which significantly reduces the
complexity and thus, the computational effort for distributed reinforcement
learning control of dynamical systems governed by partial differential
equations (PDEs). Exploiting translational invariances, the high-dimensional
distributed control problem can be transformed into a multi-agent control
problem with many identical, uncoupled agents. Furthermore, using the fact that
information is transported with finite velocity in many cases, the dimension of
the agents' environment can be drastically reduced using a convolution
operation over the state space of the PDE. In this setting, the complexity can
be flexibly adjusted via the kernel width or by using a stride greater than
one. Moreover, scaling from smaller to larger systems -- or the transfer
between different domains -- becomes a straightforward task requiring little
effort. We demonstrate the performance of the proposed framework using several
PDE examples with increasing complexity, where stabilization is achieved by
training a low-dimensional deep deterministic policy gradient agent using
minimal computing resources.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.10923">Trust Region-Based Safe Distributional Reinforcement Learning for Multiple Constraints. (arXiv:2301.10923v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1">Dohyeong Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kyungjae Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1">Songhwai Oh</a></p>
<p>In safety-critical robotic tasks, potential failures must be reduced, and
multiple constraints must be met, such as avoiding collisions, limiting energy
consumption, and maintaining balance. Thus, applying safe reinforcement
learning (RL) in such robotic tasks requires to handle multiple constraints and
use risk-averse constraints rather than risk-neutral constraints. To this end,
we propose a trust region-based safe RL algorithm for multiple constraints
called a safe distributional actor-critic (SDAC). Our main contributions are as
follows: 1) introducing a gradient integration method to manage infeasibility
issues in multi-constrained problems, ensuring theoretical convergence, and 2)
developing a TD($\lambda$) target distribution to estimate risk-averse
constraints with low biases. We evaluate SDAC through extensive experiments
involving multi- and single-constrained robotic tasks. While maintaining high
scores, SDAC shows 1.93 times fewer steps to satisfy all constraints in
multi-constrained tasks and 1.78 times fewer constraint violations in
single-constrained tasks compared to safe RL baselines. Code is available at:
https://github.com/rllab-snu/Safe-Distributional-Actor-Critic.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.11923">Information loss from dimensionality reduction in 5D-Gaussian spectral data. (arXiv:2301.11923v2 [physics.data-an] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Schelle_A/0/1/0/all/0/1">A. Schelle</a>, <a href="http://arxiv.org/find/physics/1/au:+Luling_H/0/1/0/all/0/1">H. L&#xfc;ling</a></p>
<p>Understanding the loss of information in spectral analytics is a crucial
first step towards finding root causes for failures and uncertainties using
spectral data in artificial intelligence models built from modern complex data
science applications. Here, we show from an elementary Shannon entropy model
analysis with quantum statistics of Gaussian distributed spectral data, that
the relative loss of information from dimensionality reduction due to the
projection of an initial five-dimensional dataset onto two-dimensional diagrams
is less than one percent in the parameter range of small data sets with sample
sizes on the order of few hundred data samples. From our analysis, we also
conclude that the density and expectation value of the entropy probability
distribution increases with the sample number and sample size using artificial
data models derived from random sampling Monte Carlo simulation methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.01477">A Reduction-based Framework for Sequential Decision Making with Delayed Feedback. (arXiv:2302.01477v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yunchang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1">Han Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1">Tianhao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Liwei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1">Simon S. Du</a></p>
<p>We study stochastic delayed feedback in general multi-agent sequential
decision making, which includes bandits, single-agent Markov decision processes
(MDPs), and Markov games (MGs). We propose a novel reduction-based framework,
which turns any multi-batched algorithm for sequential decision making with
instantaneous feedback into a sample-efficient algorithm that can handle
stochastic delays in sequential decision making. By plugging different
multi-batched algorithms into our framework, we provide several examples
demonstrating that our framework not only matches or improves existing results
for bandits, tabular MDPs, and tabular MGs, but also provides the first line of
studies on delays in sequential decision making with function approximation. In
summary, we provide a complete set of sharp results for multi-agent sequential
decision making with delayed feedback.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.01633">Convergence Analysis of Sequential Split Learning on Heterogeneous Data. (arXiv:2302.01633v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yipeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_X/0/1/0/all/0/1">Xinchen Lyu</a></p>
<p>Federated Learning (FL) and Split Learning (SL) are two popular paradigms of
distributed machine learning. By offloading the computation-intensive portions
to the server, SL is promising for deep model training on resource-constrained
devices, yet still lacking of rigorous convergence analysis. In this paper, we
derive the convergence guarantees of Sequential SL (SSL, the vanilla case of SL
that conducts the model training in sequence) for strongly/general/non-convex
objectives on heterogeneous data. Notably, the derived guarantees suggest that
SSL is better than Federated Averaging (FedAvg, the most popular algorithm in
FL) on heterogeneous data. We validate the counterintuitive analysis result
empirically on extremely heterogeneous data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.04178">DynGFN: Towards Bayesian Inference of Gene Regulatory Networks with GFlowNets. (arXiv:2302.04178v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Atanackovic_L/0/1/0/all/0/1">Lazar Atanackovic</a>, <a href="http://arxiv.org/find/cs/1/au:+Tong_A/0/1/0/all/0/1">Alexander Tong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_L/0/1/0/all/0/1">Leo J. Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1">Yoshua Bengio</a>, <a href="http://arxiv.org/find/cs/1/au:+Hartford_J/0/1/0/all/0/1">Jason Hartford</a></p>
<p>One of the grand challenges of cell biology is inferring the gene regulatory
network (GRN) which describes interactions between genes and their products
that control gene expression and cellular function. We can treat this as a
causal discovery problem but with two non-standard challenges: (1) regulatory
networks are inherently cyclic so we should not model a GRN as a directed
acyclic graph (DAG), and (2) observations have significant measurement noise,
so for typical sample sizes there will always be a large equivalence class of
graphs that are likely given the data, and we want methods that capture this
uncertainty. Existing methods either focus on challenge (1), identifying cyclic
structure from dynamics, or on challenge (2) learning complex Bayesian
posteriors over DAGs, but not both. In this paper we leverage the fact that it
is possible to estimate the "velocity" of gene expression with RNA velocity
techniques to develop an approach that addresses both challenges. Because we
have access to velocity information, we can treat the Bayesian structure
learning problem as a problem of sparse identification of a dynamical system,
capturing cyclic feedback loops through time. Since our objective is to model
uncertainty over discrete structures, we leverage Generative Flow Networks
(GFlowNets) to estimate the posterior distribution over the combinatorial space
of possible sparse dependencies. Our results indicate that our method learns
posteriors that better encapsulate the distributions of cyclic structures
compared to counterpart state-of-the-art Bayesian structure learning
approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.04452">An Information-Theoretic Analysis of Nonstationary Bandit Learning. (arXiv:2302.04452v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1">Seungki Min</a>, <a href="http://arxiv.org/find/cs/1/au:+Russo_D/0/1/0/all/0/1">Daniel Russo</a></p>
<p>In nonstationary bandit learning problems, the decision-maker must
continually gather information and adapt their action selection as the latent
state of the environment evolves. In each time period, some latent optimal
action maximizes expected reward under the environment state. We view the
optimal action sequence as a stochastic process, and take an
information-theoretic approach to analyze attainable performance. We bound
limiting per-period regret in terms of the entropy rate of the optimal action
process. The bound applies to a wide array of problems studied in the
literature and reflects the problem's information structure through its
information-ratio.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.06943">Private Statistical Estimation of Many Quantiles. (arXiv:2302.06943v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Lalanne_C/0/1/0/all/0/1">Cl&#xe9;ment Lalanne</a> (ENS de Lyon, DANTE, OCKHAM), <a href="http://arxiv.org/find/stat/1/au:+Garivier_A/0/1/0/all/0/1">Aur&#xe9;lien Garivier</a> (UMPA-ENSL, MC2), <a href="http://arxiv.org/find/stat/1/au:+Gribonval_R/0/1/0/all/0/1">R&#xe9;mi Gribonval</a> (DANTE, OCKHAM)</p>
<p>This work studies the estimation of many statistical quantiles under
differential privacy. More precisely, given a distribution and access to i.i.d.
samples from it, we study the estimation of the inverse of its cumulative
distribution function (the quantile function) at specific points. For instance,
this task is of key importance in private data generation. We present two
different approaches. The first one consists in privately estimating the
empirical quantiles of the samples and using this result as an estimator of the
quantiles of the distribution. In particular, we study the statistical
properties of the recently published algorithm introduced by Kaplan et al. 2022
that privately estimates the quantiles recursively. The second approach is to
use techniques of density estimation in order to uniformly estimate the
quantile function on an interval. In particular, we show that there is a
tradeoff between the two methods. When we want to estimate many quantiles, it
is better to estimate the density rather than estimating the quantile function
at specific points.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.07409">Quantum Learning Theory Beyond Batch Binary Classification. (arXiv:2302.07409v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mohan_P/0/1/0/all/0/1">Preetham Mohan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tewari_A/0/1/0/all/0/1">Ambuj Tewari</a></p>
<p>Arunachalam and de Wolf (2018) showed that the sample complexity of quantum
batch learning of boolean functions, in the realizable and agnostic settings,
has the same form and order as the corresponding classical sample complexities.
In this paper, we extend this, ostensibly surprising, message to batch
multiclass learning, online boolean learning, and online multiclass learning.
For our online learning results, we first consider an adaptive adversary
variant of the classical model of Dawid and Tewari (2022). Then, we introduce
the first (to the best of our knowledge) model of online learning with quantum
examples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.08005">Slapo: A Schedule Language for Progressive Optimization of Large Deep Learning Model Training. (arXiv:2302.08005v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hongzheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1">Cody Hao Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1">Shuai Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhiru Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yida Wang</a></p>
<p>Recent years have seen an increase in the development of large deep learning
(DL) models, which makes training efficiency crucial. Common practice is
struggling with the trade-off between usability and performance. On one hand,
DL frameworks such as PyTorch use dynamic graphs to facilitate model developers
at a price of sub-optimal model training performance. On the other hand,
practitioners propose various approaches to improving the training efficiency
by sacrificing some of the flexibility, ranging from making the graph static
for more thorough optimization (e.g., XLA) to customizing optimization towards
large-scale distributed training (e.g., DeepSpeed and Megatron-LM). In this
paper, we aim to address the tension between usability and training efficiency
through separation of concerns. Inspired by DL compilers that decouple the
platform-specific optimizations of a tensor-level operator from its arithmetic
definition, this paper proposes a schedule language, Slapo, to decouple model
execution from definition. Specifically, Slapo works on a PyTorch model and
uses a set of schedule primitives to convert the model for common model
training optimizations such as high-performance kernels, effective 3D
parallelism, and efficient activation checkpointing. Compared to existing
optimization solutions, Slapo progressively optimizes the model "as-needed"
through high-level primitives, and thus preserving programmability and
debuggability for users to a large extent. Our evaluation results show that by
scheduling the existing hand-crafted optimizations in a systematic way using
Slapo, we are able to improve training throughput by up to 2.92x on a single
machine with 8 NVIDIA V100 GPUs, and by up to 1.41x on multiple machines with
up to 64 GPUs, when compared to the out-of-the-box performance of DeepSpeed and
Megatron-LM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.00859">FuNVol: A Multi-Asset Implied Volatility Market Simulator using Functional Principal Components and Neural SDEs. (arXiv:2303.00859v4 [q-fin.CP] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+Choudhary_V/0/1/0/all/0/1">Vedant Choudhary</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Jaimungal_S/0/1/0/all/0/1">Sebastian Jaimungal</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Bergeron_M/0/1/0/all/0/1">Maxime Bergeron</a></p>
<p>We introduce a new approach for generating sequences of implied volatility
(IV) surfaces across multiple assets that is faithful to historical prices. We
do so using a combination of functional data analysis and neural stochastic
differential equations (SDEs) combined with a probability integral transform
penalty to reduce model misspecification. We demonstrate that learning the
joint dynamics of IV surfaces and prices produces market scenarios that are
consistent with historical features and lie within the sub-manifold of surfaces
that are essentially free of static arbitrage. Finally, we demonstrate that
delta hedging using the simulated surfaces generates profit and loss (P&amp;L)
distributions that are consistent with realised P&amp;Ls.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.14496">Learning with Explanation Constraints. (arXiv:2303.14496v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pukdee_R/0/1/0/all/0/1">Rattana Pukdee</a>, <a href="http://arxiv.org/find/cs/1/au:+Sam_D/0/1/0/all/0/1">Dylan Sam</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1">J. Zico Kolter</a>, <a href="http://arxiv.org/find/cs/1/au:+Balcan_M/0/1/0/all/0/1">Maria-Florina Balcan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ravikumar_P/0/1/0/all/0/1">Pradeep Ravikumar</a></p>
<p>As larger deep learning models are hard to interpret, there has been a recent
focus on generating explanations of these black-box models. In contrast, we may
have apriori explanations of how models should behave. In this paper, we
formalize this notion as learning from explanation constraints and provide a
learning theoretic framework to analyze how such explanations can improve the
learning of our models. One may naturally ask, "When would these explanations
be helpful?" Our first key contribution addresses this question via a class of
models that satisfies these explanation constraints in expectation over new
data. We provide a characterization of the benefits of these models (in terms
of the reduction of their Rademacher complexities) for a canonical class of
explanations given by gradient information in the settings of both linear
models and two layer neural networks. In addition, we provide an algorithmic
solution for our framework, via a variational approximation that achieves
better performance and satisfies these constraints more frequently, when
compared to simpler augmented Lagrangian methods to incorporate these
explanations. We demonstrate the benefits of our approach over a large array of
synthetic and real-world experiments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.15216">Robust Risk-Aware Option Hedging. (arXiv:2303.15216v3 [q-fin.CP] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+Wu_D/0/1/0/all/0/1">David Wu</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Jaimungal_S/0/1/0/all/0/1">Sebastian Jaimungal</a></p>
<p>The objectives of option hedging/trading extend beyond mere protection
against downside risks, with a desire to seek gains also driving agent's
strategies. In this study, we showcase the potential of robust risk-aware
reinforcement learning (RL) in mitigating the risks associated with
path-dependent financial derivatives. We accomplish this by leveraging a policy
gradient approach that optimises robust risk-aware performance criteria. We
specifically apply this methodology to the hedging of barrier options, and
highlight how the optimal hedging strategy undergoes distortions as the agent
moves from being risk-averse to risk-seeking. As well as how the agent
robustifies their strategy. We further investigate the performance of the hedge
when the data generating process (DGP) varies from the training DGP, and
demonstrate that the robust strategies outperform the non-robust ones.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.01046">Deep Manifold Learning for Reading Comprehension and Logical Reasoning Tasks with Polytuplet Loss. (arXiv:2304.01046v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Jeffrey Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_I/0/1/0/all/0/1">Ivan Rodriguez</a></p>
<p>The current trend in developing machine learning models for reading
comprehension and logical reasoning tasks is focused on improving the models'
abilities to understand and utilize logical rules. This work focuses on
providing a novel loss function and accompanying model architecture that has
more interpretable components than some other models by representing a common
strategy employed by humans when given reading comprehension and logical
reasoning tasks. Our strategy involves emphasizing relative accuracy over
absolute accuracy and can theoretically produce the correct answer with
incomplete knowledge. We examine the effectiveness of this strategy to solve
reading comprehension and logical reasoning questions. The models were
evaluated on the ReClor dataset, a challenging reading comprehension and
logical reasoning benchmark. We propose the polytuplet loss function, which
forces prioritization of learning the relative correctness of answer choices
over learning the true accuracy of each choice. Our results indicate that
models employing polytuplet loss outperform existing baseline models, though
further research is required to quantify the benefits it may present.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.02711">Structured prompt interrogation and recursive extraction of semantics (SPIRES): A method for populating knowledge bases using zero-shot learning. (arXiv:2304.02711v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Caufield_J/0/1/0/all/0/1">J. Harry Caufield</a>, <a href="http://arxiv.org/find/cs/1/au:+Hegde_H/0/1/0/all/0/1">Harshad Hegde</a>, <a href="http://arxiv.org/find/cs/1/au:+Emonet_V/0/1/0/all/0/1">Vincent Emonet</a>, <a href="http://arxiv.org/find/cs/1/au:+Harris_N/0/1/0/all/0/1">Nomi L. Harris</a>, <a href="http://arxiv.org/find/cs/1/au:+Joachimiak_M/0/1/0/all/0/1">Marcin P. Joachimiak</a>, <a href="http://arxiv.org/find/cs/1/au:+Matentzoglu_N/0/1/0/all/0/1">Nicolas Matentzoglu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">HyeongSik Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Moxon_S/0/1/0/all/0/1">Sierra A.T. Moxon</a>, <a href="http://arxiv.org/find/cs/1/au:+Reese_J/0/1/0/all/0/1">Justin T. Reese</a>, <a href="http://arxiv.org/find/cs/1/au:+Haendel_M/0/1/0/all/0/1">Melissa A. Haendel</a>, <a href="http://arxiv.org/find/cs/1/au:+Robinson_P/0/1/0/all/0/1">Peter N. Robinson</a>, <a href="http://arxiv.org/find/cs/1/au:+Mungall_C/0/1/0/all/0/1">Christopher J. Mungall</a></p>
<p>Creating knowledge bases and ontologies is a time consuming task that relies
on a manual curation. AI/NLP approaches can assist expert curators in
populating these knowledge bases, but current approaches rely on extensive
training data, and are not able to populate arbitrary complex nested knowledge
schemas.
</p>
<p>Here we present Structured Prompt Interrogation and Recursive Extraction of
Semantics (SPIRES), a Knowledge Extraction approach that relies on the ability
of Large Language Models (LLMs) to perform zero-shot learning (ZSL) and
general-purpose query answering from flexible prompts and return information
conforming to a specified schema. Given a detailed, user-defined knowledge
schema and an input text, SPIRES recursively performs prompt interrogation
against GPT-3+ to obtain a set of responses matching the provided schema.
SPIRES uses existing ontologies and vocabularies to provide identifiers for all
matched elements.
</p>
<p>We present examples of use of SPIRES in different domains, including
extraction of food recipes, multi-species cellular signaling pathways, disease
treatments, multi-step drug mechanisms, and chemical to disease causation
graphs. Current SPIRES accuracy is comparable to the mid-range of existing
Relation Extraction (RE) methods, but has the advantage of easy customization,
flexibility, and, crucially, the ability to perform new tasks in the absence of
any training data. This method supports a general strategy of leveraging the
language interpreting capabilities of LLMs to assemble knowledge bases,
assisting manual knowledge curation and acquisition while supporting validation
with publicly-available databases and ontologies external to the LLM.
</p>
<p>SPIRES is available as part of the open source OntoGPT package:
https://github.com/ monarch-initiative/ontogpt.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.14660">Segment Anything Model for Medical Images?. (arXiv:2304.14660v6 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1">Yuhao Huang</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1">Xin Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_L/0/1/0/all/0/1">Lian Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_H/0/1/0/all/0/1">Han Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Chang_A/0/1/0/all/0/1">Ao Chang</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1">Xinrui Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_R/0/1/0/all/0/1">Rusi Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Yu_J/0/1/0/all/0/1">Junxuan Yu</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1">Jiongquan Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1">Chaoyu Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1">Sijing Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Chi_H/0/1/0/all/0/1">Haozhe Chi</a>, <a href="http://arxiv.org/find/eess/1/au:+Hu_X/0/1/0/all/0/1">Xindi Hu</a>, <a href="http://arxiv.org/find/eess/1/au:+Yue_K/0/1/0/all/0/1">Kejuan Yue</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1">Lei Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Grau_V/0/1/0/all/0/1">Vicente Grau</a>, <a href="http://arxiv.org/find/eess/1/au:+Fan_D/0/1/0/all/0/1">Deng-Ping Fan</a>, <a href="http://arxiv.org/find/eess/1/au:+Dong_F/0/1/0/all/0/1">Fajin Dong</a>, <a href="http://arxiv.org/find/eess/1/au:+Ni_D/0/1/0/all/0/1">Dong Ni</a></p>
<p>The Segment Anything Model (SAM) is the first foundation model for general
image segmentation. It has achieved impressive results on various natural image
segmentation tasks. However, medical image segmentation (MIS) is more
challenging because of the complex modalities, fine anatomical structures,
uncertain and complex object boundaries, and wide-range object scales. To fully
validate SAM's performance on medical data, we collected and sorted 53
open-source datasets and built a large medical segmentation dataset with 18
modalities, 84 objects, 125 object-modality paired targets, 1050K 2D images,
and 6033K masks. We comprehensively analyzed different models and strategies on
the so-called COSMOS 1050K dataset. Our findings mainly include the following:
1) SAM showed remarkable performance in some specific objects but was unstable,
imperfect, or even totally failed in other situations. 2) SAM with the large
ViT-H showed better overall performance than that with the small ViT-B. 3) SAM
performed better with manual hints, especially box, than the Everything mode.
4) SAM could help human annotation with high labeling quality and less time. 5)
SAM was sensitive to the randomness in the center point and tight box prompts,
and may suffer from a serious performance drop. 6) SAM performed better than
interactive methods with one or a few points, but will be outpaced as the
number of points increases. 7) SAM's performance correlated to different
factors, including boundary complexity, intensity differences, etc. 8)
Finetuning the SAM on specific medical tasks could improve its average DICE
performance by 4.39% and 6.68% for ViT-B and ViT-H, respectively. We hope that
this comprehensive report can help researchers explore the potential of SAM
applications in MIS, and guide how to appropriately use and develop SAM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.03148">CAMEL: Co-Designing AI Models and Embedded DRAMs for Efficient On-Device Learning. (arXiv:2305.03148v3 [cs.AR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Sai Qian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tambe_T/0/1/0/all/0/1">Thierry Tambe</a>, <a href="http://arxiv.org/find/cs/1/au:+Cuevas_N/0/1/0/all/0/1">Nestor Cuevas</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_G/0/1/0/all/0/1">Gu-Yeon Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Brooks_D/0/1/0/all/0/1">David Brooks</a></p>
<p>On-device learning allows AI models to adapt to user data, thereby enhancing
service quality on edge platforms. However, training AI on resource-limited
devices poses significant challenges due to the demanding computing workload
and the substantial memory consumption and data access required by deep neural
networks (DNNs). To address these issues, we propose utilizing embedded dynamic
random-access memory (eDRAM) as the primary storage medium for transient
training data. In comparison to static random-access memory (SRAM), eDRAM
provides higher storage density and lower leakage power, resulting in reduced
access cost and power leakage. Nevertheless, to maintain the integrity of the
stored data, periodic power-hungry refresh operations could potentially degrade
system performance.
</p>
<p>To minimize the occurrence of expensive eDRAM refresh operations, it is
beneficial to shorten the lifetime of stored data during the training process.
To achieve this, we adopt the principles of algorithm and hardware co-design,
introducing a family of reversible DNN architectures that effectively decrease
data lifetime and storage costs throughout training. Additionally, we present a
highly efficient on-device training engine named \textit{CAMEL}, which
leverages eDRAM as the primary on-chip memory. This engine enables efficient
on-device training with significantly reduced memory usage and off-chip DRAM
traffic while maintaining superior training accuracy. We evaluate our CAMEL
system on multiple DNNs with different datasets, demonstrating a $2.5\times$
speedup of the training process and $2.8\times$ training energy savings than
the other baseline hardware platforms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.05276">Causal Discovery from Subsampled Time Series with Proxy Variables. (arXiv:2305.05276v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Mingzhou Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xinwei Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1">Lingjing Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yizhou Wang</a></p>
<p>Inferring causal structures from time series data is the central interest of
many scientific inquiries. A major barrier to such inference is the problem of
subsampling, i.e., the frequency of measurement is much lower than that of
causal influence. To overcome this problem, numerous methods have been
proposed, yet either was limited to the linear case or failed to achieve
identifiability. In this paper, we propose a constraint-based algorithm that
can identify the entire causal structure from subsampled time series, without
any parametric constraint. Our observation is that the challenge of subsampling
arises mainly from hidden variables at the unobserved time steps. Meanwhile,
every hidden variable has an observed proxy, which is essentially itself at
some observable time in the future, benefiting from the temporal structure.
Based on these, we can leverage the proxies to remove the bias induced by the
hidden variables and hence achieve identifiability. Following this intuition,
we propose a proxy-based causal discovery algorithm. Our algorithm is
nonparametric and can achieve full causal identification. Theoretical
advantages are reflected in synthetic and real-world experiments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.06547">Neural Lyapunov Control for Discrete-Time Systems. (arXiv:2305.06547v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Junlin Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Clark_A/0/1/0/all/0/1">Andrew Clark</a>, <a href="http://arxiv.org/find/cs/1/au:+Kantaros_Y/0/1/0/all/0/1">Yiannis Kantaros</a>, <a href="http://arxiv.org/find/cs/1/au:+Vorobeychik_Y/0/1/0/all/0/1">Yevgeniy Vorobeychik</a></p>
<p>While ensuring stability for linear systems is well understood, it remains a
major challenge for nonlinear systems. A general approach in such cases is to
compute a combination of a Lyapunov function and an associated control policy.
However, finding Lyapunov functions for general nonlinear systems is a
challenging task. To address this challenge, several methods have been proposed
that represent Lyapunov functions using neural networks. However, such
approaches either focus on continuous-time systems, or highly restricted
classes of nonlinear dynamics. We propose the first approach for learning
neural Lyapunov control in a broad class of discrete-time systems. Three key
ingredients enable us to effectively learn provably stable control policies.
The first is a novel mixed-integer linear programming approach for verifying
the discrete-time Lyapunov stability conditions, leveraging the particular
structure of these conditions. The second is a novel approach for computing
verified sublevel sets. The third is a heuristic gradient-based method for
quickly finding counterexamples to significantly speed up Lyapunov function
learning. Our experiments on four standard benchmarks demonstrate that our
approach significantly outperforms state-of-the-art baselines. For example, on
the path tracking benchmark, we outperform recent neural Lyapunov control
baselines by an order of magnitude in both running time and the size of the
region of attraction, and on two of the four benchmarks (cartpole and PVTOL),
ours is the first automated approach to return a provably stable controller.
Our code is available at: https://github.com/jlwu002/nlc_discrete.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.06743">Implicitly normalized forecaster with clipping for linear and non-linear heavy-tailed multi-armed bandits. (arXiv:2305.06743v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dorn_Y/0/1/0/all/0/1">Yuriy Dorn</a>, <a href="http://arxiv.org/find/cs/1/au:+Kornilov_N/0/1/0/all/0/1">Nikita Kornilov</a>, <a href="http://arxiv.org/find/cs/1/au:+Kutuzov_N/0/1/0/all/0/1">Nikolay Kutuzov</a>, <a href="http://arxiv.org/find/cs/1/au:+Nazin_A/0/1/0/all/0/1">Alexander Nazin</a>, <a href="http://arxiv.org/find/cs/1/au:+Gorbunov_E/0/1/0/all/0/1">Eduard Gorbunov</a>, <a href="http://arxiv.org/find/cs/1/au:+Gasnikov_A/0/1/0/all/0/1">Alexander Gasnikov</a></p>
<p>The Implicitly Normalized Forecaster (INF) algorithm is considered to be an
optimal solution for adversarial multi-armed bandit (MAB) problems. However,
most of the existing complexity results for INF rely on restrictive
assumptions, such as bounded rewards. Recently, a related algorithm was
proposed that works for both adversarial and stochastic heavy-tailed MAB
settings. However, this algorithm fails to fully exploit the available data.
</p>
<p>In this paper, we propose a new version of INF called the Implicitly
Normalized Forecaster with clipping (INF-clip) for MAB problems with
heavy-tailed reward distributions. We establish convergence results under mild
assumptions on the rewards distribution and demonstrate that INF-clip is
optimal for linear heavy-tailed stochastic MAB problems and works well for
non-linear ones. Furthermore, we show that INF-clip outperforms the
best-of-both-worlds algorithm in cases where it is difficult to distinguish
between different arms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.09956">The Adversarial Consistency of Surrogate Risks for Binary Classification. (arXiv:2305.09956v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Frank_N/0/1/0/all/0/1">Natalie Frank</a>, <a href="http://arxiv.org/find/cs/1/au:+Niles_Weed_J/0/1/0/all/0/1">Jonathan Niles-Weed</a></p>
<p>We study the consistency of surrogate risks for robust binary classification.
It is common to learn robust classifiers by adversarial training, which seeks
to minimize the expected $0$-$1$ loss when each example can be maliciously
corrupted within a small ball. We give a simple and complete characterization
of the set of surrogate loss functions that are \emph{consistent}, i.e., that
can replace the $0$-$1$ loss without affecting the minimizing sequences of the
original adversarial risk, for any data distribution. We also prove a
quantitative version of adversarial consistency for the $\rho$-margin loss. Our
results reveal that the class of adversarially consistent surrogates is
substantially smaller than in the standard setting, where many common
surrogates are known to be consistent.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13245">GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints. (arXiv:2305.13245v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1">Joshua Ainslie</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_Thorp_J/0/1/0/all/0/1">James Lee-Thorp</a>, <a href="http://arxiv.org/find/cs/1/au:+Jong_M/0/1/0/all/0/1">Michiel de Jong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zemlyanskiy_Y/0/1/0/all/0/1">Yury Zemlyanskiy</a>, <a href="http://arxiv.org/find/cs/1/au:+Lebron_F/0/1/0/all/0/1">Federico Lebr&#xf3;n</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanghai_S/0/1/0/all/0/1">Sumit Sanghai</a></p>
<p>Multi-query attention (MQA), which only uses a single key-value head,
drastically speeds up decoder inference. However, MQA can lead to quality
degradation, and moreover it may not be desirable to train a separate model
just for faster inference. We (1) propose a recipe for uptraining existing
multi-head language model checkpoints into models with MQA using 5% of original
pre-training compute, and (2) introduce grouped-query attention (GQA), a
generalization of multi-query attention which uses an intermediate (more than
one, less than number of query heads) number of key-value heads. We show that
uptrained GQA achieves quality close to multi-head attention with comparable
speed to MQA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14901">Chain-of-Questions Training with Latent Answers for Robust Multistep Question Answering. (arXiv:2305.14901v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1">Wang Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1">Jesse Thomason</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1">Robin Jia</a></p>
<p>We train a language model (LM) to robustly answer multistep questions by
generating and answering sub-questions. We propose Chain-of-Questions, a
framework that trains a model to generate sub-questions and sub-answers one at
a time by leveraging human annotated question decomposition meaning
representation (QDMR). The key technical challenge is that QDMR only contains
sub-questions but not answers to those sub-questions, so we treat sub-answers
as latent variables and optimize them using a novel dynamic mixture of Hard-EM
and MAPO. Chain-of-Questions greatly outperforms strong neuro-symbolic methods
by 9.0 F1 on DROP contrast set, and outperforms GPT-3.5 by 24.3 F1 on HOTPOTQA
adversarial set, thus demonstrating the effectiveness and robustness of our
framework.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14910">From Shortcuts to Triggers: Backdoor Defense with Denoised PoE. (arXiv:2305.14910v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Fei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Chaowei Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Muhao Chen</a></p>
<p>Language models are often at risk of diverse backdoor attacks, especially
data poisoning. Thus, it is important to investigate defense solutions for
addressing them. Existing backdoor defense methods mainly focus on backdoor
attacks with explicit triggers, leaving a universal defense against various
backdoor attacks with diverse triggers largely unexplored. In this paper, we
propose an end-to-end ensemble-based backdoor defense framework, DPoE (Denoised
Product-of-Experts), which is inspired by the shortcut nature of backdoor
attacks, to defend various backdoor attacks. DPoE consists of two models: a
shallow model that captures the backdoor shortcuts and a main model that is
prevented from learning the backdoor shortcuts. To address the label flip
caused by backdoor attackers, DPoE incorporates a denoising design. Experiments
on SST-2 dataset show that DPoE significantly improves the defense performance
against various types of backdoor triggers including word-level,
sentence-level, and syntactic triggers. Furthermore, DPoE is also effective
under a more challenging but practical setting that mixes multiple types of
trigger.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14943">Learning Rate Free Sampling in Constrained Domains. (arXiv:2305.14943v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Sharrock_L/0/1/0/all/0/1">Louis Sharrock</a>, <a href="http://arxiv.org/find/stat/1/au:+Mackey_L/0/1/0/all/0/1">Lester Mackey</a>, <a href="http://arxiv.org/find/stat/1/au:+Nemeth_C/0/1/0/all/0/1">Christopher Nemeth</a></p>
<p>We introduce a suite of new particle-based algorithms for sampling in
constrained domains which are entirely learning rate free. Our approach
leverages coin betting ideas from convex optimisation, and the viewpoint of
constrained sampling as a mirrored optimisation problem on the space of
probability measures. Based on this viewpoint, we also introduce a unifying
framework for several existing constrained sampling algorithms, including
mirrored Langevin dynamics and mirrored Stein variational gradient descent. We
demonstrate the performance of our algorithms on a range of numerical examples,
including sampling from targets on the simplex, sampling with fairness
constraints, and constrained sampling problems in post-selection inference. Our
results indicate that our algorithms achieve competitive performance with
existing constrained sampling methods, without the need to tune any
hyperparameters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15408">Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective. (arXiv:2305.15408v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_G/0/1/0/all/0/1">Guhao Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Bohang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1">Yuntian Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1">Haotian Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1">Di He</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Liwei Wang</a></p>
<p>Recent studies have discovered that Chain-of-Thought prompting (CoT) can
dramatically improve the performance of Large Language Models (LLMs),
particularly when dealing with complex tasks involving mathematics or
reasoning. Despite the enormous empirical success, the underlying mechanisms
behind CoT and how it unlocks the potential of LLMs remain elusive. In this
paper, we take a first step towards theoretically answering these questions.
Specifically, we examine the expressivity of LLMs with CoT in solving
fundamental mathematical and decision-making problems. By using circuit
complexity theory, we first give impossibility results showing that
bounded-depth Transformers are unable to directly produce correct answers for
basic arithmetic/equation tasks unless the model size grows super-polynomially
with respect to the input length. In contrast, we then prove by construction
that autoregressive Transformers of constant size suffice to solve both tasks
by generating CoT derivations using a commonly used math language format.
Moreover, we show LLMs with CoT can handle a general class of decision-making
problems known as Dynamic Programming, thus justifying its power in tackling
complex real-world tasks. Finally, an extensive set of experiments show that,
while Transformers always fail to directly predict the answers, they can
consistently learn to generate correct solutions step-by-step given sufficient
CoT demonstrations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.18341">Coarse-Tuning Models of Code with Reinforcement Learning Feedback. (arXiv:2305.18341v2 [cs.PL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1">Abhinav Jain</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Adiole_C/0/1/0/all/0/1">Chima Adiole</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1">Swarat Chaudhuri</a> (2), <a href="http://arxiv.org/find/cs/1/au:+Reps_T/0/1/0/all/0/1">Thomas Reps</a> (3), <a href="http://arxiv.org/find/cs/1/au:+Jermaine_C/0/1/0/all/0/1">Chris Jermaine</a> (1) ((1) Rice University, (2) UT Austin, (3) University of Wisconsin)</p>
<p>Large Language Models (LLMs) pre-trained on code have recently emerged as the
dominant approach to program synthesis. However, these models are trained using
next-token prediction, which ignores the syntax and semantics of code. We
propose RLCF, that further trains a pre-trained LLM via reinforcement learning,
using feedback from a grounding function that scores the quality of the code.
The grounding function uses (i) compiler-derived feedback on whether the code
it generates passes a set of correctness checks; and (ii) feedback from a
different LLM that compares the generated code to a reference code. RLCF is
model- and language-agnostic. We empirically evaluate it on the MBJP and MathQA
tasks for Java. Our experiments show that RLCF raises the odds that an
LLM-generated program compiles, is executable, and produces the right output on
tests, often allowing LLMs to match the performance of 2x-8x larger LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19082">Embedding Inequalities for Barron-type Spaces. (arXiv:2305.19082v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Wu_L/0/1/0/all/0/1">Lei Wu</a></p>
<p>One of the fundamental problems in deep learning theory is understanding the
approximation and generalization properties of two-layer neural networks in
high dimensions. In order to tackle this issue, researchers have introduced the
Barron space $\mathcal{B}_s(\Omega)$ and the spectral Barron space
$\mathcal{F}_s(\Omega)$, where the index $s$ characterizes the smoothness of
functions within these spaces and $\Omega\subset\mathbb{R}^d$ represents the
input domain. However, it is still not clear what is the relationship between
the two types of Barron spaces. In this paper, we establish continuous
embeddings between these spaces as implied by the following inequality: for any
$\delta\in (0,1), s\in \mathbb{N}^{+}$ and $f: \Omega \mapsto\mathbb{R}$, it
holds that \[
\delta\gamma^{\delta-s}_{\Omega}\|f\|_{\mathcal{F}_{s-\delta}(\Omega)}\lesssim_s
\|f\|_{\mathcal{B}_s(\Omega)}\lesssim_s \|f\|_{\mathcal{F}_{s+1}(\Omega)}, \]
where $\gamma_{\Omega}=\sup_{\|v\|_2=1,x\in\Omega}|v^Tx|$ and notably, the
hidden constants depend solely on the value of $s$. Furthermore, we provide
examples to demonstrate that the lower bound is tight.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.00959">Dynamic Algorithms for Matroid Submodular Maximization. (arXiv:2306.00959v2 [cs.DS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Banihashem_K/0/1/0/all/0/1">Kiarash Banihashem</a>, <a href="http://arxiv.org/find/cs/1/au:+Biabani_L/0/1/0/all/0/1">Leyla Biabani</a>, <a href="http://arxiv.org/find/cs/1/au:+Goudarzi_S/0/1/0/all/0/1">Samira Goudarzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hajiaghayi_M/0/1/0/all/0/1">MohammadTaghi Hajiaghayi</a>, <a href="http://arxiv.org/find/cs/1/au:+Jabbarzade_P/0/1/0/all/0/1">Peyman Jabbarzade</a>, <a href="http://arxiv.org/find/cs/1/au:+Monemizadeh_M/0/1/0/all/0/1">Morteza Monemizadeh</a></p>
<p>Submodular maximization under matroid and cardinality constraints are
classical problems with a wide range of applications in machine learning,
auction theory, and combinatorial optimization. In this paper, we consider
these problems in the dynamic setting, where (1) we have oracle access to a
monotone submodular function $f: 2^{V} \rightarrow \mathbb{R}^+$ and (2) we are
given a sequence $\mathcal{S}$ of insertions and deletions of elements of an
underlying ground set $V$.
</p>
<p>We develop the first fully dynamic $(4+\epsilon)$-approximation algorithm for
the submodular maximization problem under the matroid constraint using an
expected worst-case $O(k\log(k)\log^3{(k/\epsilon)})$ query complexity where $0
&lt; \epsilon \le 1$. This resolves an open problem of Chen and Peng (STOC'22) and
Lattanzi et al. (NeurIPS'20).
</p>
<p>As a byproduct, for the submodular maximization under the cardinality
constraint $k$, we propose a parameterized (by the cardinality constraint $k$)
dynamic algorithm that maintains a $(2+\epsilon)$-approximate solution of the
sequence $\mathcal{S}$ at any time $t$ using an expected worst-case query
complexity $O(k\epsilon^{-1}\log^2(k))$. This is the first dynamic algorithm
for the problem that has a query complexity independent of the size of ground
set $V$.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.01997">UADB: Unsupervised Anomaly Detection Booster. (arXiv:2306.01997v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1">Hangting Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhining Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1">Xinyi Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_W/0/1/0/all/0/1">Wei Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1">Shun Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Gui_X/0/1/0/all/0/1">Xiaofan Gui</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Huishuai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1">Yi Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1">Jiang Bian</a></p>
<p>Unsupervised Anomaly Detection (UAD) is a key data mining problem owing to
its wide real-world applications. Due to the complete absence of supervision
signals, UAD methods rely on implicit assumptions about anomalous patterns
(e.g., scattered/sparsely/densely clustered) to detect anomalies. However,
real-world data are complex and vary significantly across different domains. No
single assumption can describe such complexity and be valid in all scenarios.
This is also confirmed by recent research that shows no UAD method is
omnipotent. Based on above observations, instead of searching for a magic
universal winner assumption, we seek to design a general UAD Booster (UADB)
that empowers any UAD models with adaptability to different data. This is a
challenging task given the heterogeneous model structures and assumptions
adopted by existing UAD methods. To achieve this, we dive deep into the UAD
problem and find that compared to normal data, anomalies (i) lack clear
structure/pattern in feature space, thus (ii) harder to learn by model without
a suitable assumption, and finally, leads to (iii) high variance between
different learners. In light of these findings, we propose to (i) distill the
knowledge of the source UAD model to an imitation learner (booster) that holds
no data assumption, then (ii) exploit the variance between them to perform
automatic correction, and thus (iii) improve the booster over the original UAD
model. We use a neural network as the booster for its strong expressive power
as a universal approximator and ability to perform flexible post-hoc tuning.
Note that UADB is a model-agnostic framework that can enhance heterogeneous UAD
models in a unified way. Extensive experiments on over 80 tabular datasets
demonstrate the effectiveness of UADB.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.03831">GEO-Bench: Toward Foundation Models for Earth Monitoring. (arXiv:2306.03831v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lacoste_A/0/1/0/all/0/1">Alexandre Lacoste</a>, <a href="http://arxiv.org/find/cs/1/au:+Lehmann_N/0/1/0/all/0/1">Nils Lehmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_P/0/1/0/all/0/1">Pau Rodriguez</a>, <a href="http://arxiv.org/find/cs/1/au:+Sherwin_E/0/1/0/all/0/1">Evan David Sherwin</a>, <a href="http://arxiv.org/find/cs/1/au:+Kerner_H/0/1/0/all/0/1">Hannah Kerner</a>, <a href="http://arxiv.org/find/cs/1/au:+Lutjens_B/0/1/0/all/0/1">Bj&#xf6;rn L&#xfc;tjens</a>, <a href="http://arxiv.org/find/cs/1/au:+Irvin_J/0/1/0/all/0/1">Jeremy Andrew Irvin</a>, <a href="http://arxiv.org/find/cs/1/au:+Dao_D/0/1/0/all/0/1">David Dao</a>, <a href="http://arxiv.org/find/cs/1/au:+Alemohammad_H/0/1/0/all/0/1">Hamed Alemohammad</a>, <a href="http://arxiv.org/find/cs/1/au:+Drouin_A/0/1/0/all/0/1">Alexandre Drouin</a>, <a href="http://arxiv.org/find/cs/1/au:+Gunturkun_M/0/1/0/all/0/1">Mehmet Gunturkun</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1">Gabriel Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Vazquez_D/0/1/0/all/0/1">David Vazquez</a>, <a href="http://arxiv.org/find/cs/1/au:+Newman_D/0/1/0/all/0/1">Dava Newman</a>, <a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1">Yoshua Bengio</a>, <a href="http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1">Stefano Ermon</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiao Xiang Zhu</a></p>
<p>Recent progress in self-supervision has shown that pre-training large neural
networks on vast amounts of unsupervised data can lead to substantial increases
in generalization to downstream tasks. Such models, recently coined foundation
models, have been transformational to the field of natural language processing.
Variants have also been proposed for image data, but their applicability to
remote sensing tasks is limited. To stimulate the development of foundation
models for Earth monitoring, we propose a benchmark comprised of six
classification and six segmentation tasks, which were carefully curated and
adapted to be both relevant to the field and well-suited for model evaluation.
We accompany this benchmark with a robust methodology for evaluating models and
reporting aggregated results to enable a reliable assessment of progress.
Finally, we report results for 20 baselines to gain information about the
performance of existing models. We believe that this benchmark will be a driver
of progress across a variety of Earth monitoring tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.06157">Fault Localization for Buggy Deep Learning Framework Conversions in Image Recognition. (arXiv:2306.06157v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Louloudakis_N/0/1/0/all/0/1">Nikolaos Louloudakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Gibson_P/0/1/0/all/0/1">Perry Gibson</a>, <a href="http://arxiv.org/find/cs/1/au:+Cano_J/0/1/0/all/0/1">Jos&#xe9; Cano</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajan_A/0/1/0/all/0/1">Ajitha Rajan</a></p>
<p>When deploying Deep Neural Networks (DNNs), developers often convert models
from one deep learning framework to another (e.g., TensorFlow to PyTorch).
However, this process is error-prone and can impact target model accuracy. To
identify the extent of such impact, we perform and briefly present a
differential analysis against three DNNs widely used for image recognition
(MobileNetV2, ResNet101, and InceptionV3) converted across four well-known deep
learning frameworks (PyTorch, Keras, TensorFlow (TF), and TFLite), which
revealed numerous model crashes and output label discrepancies of up to 72%. To
mitigate such errors, we present a novel approach towards fault localization
and repair of buggy deep learning framework conversions, focusing on
pre-trained image recognition models. Our technique consists of four stages of
analysis: 1) conversion tools, 2) model parameters, 3) model hyperparameters,
and 4) graph representation. In addition, we propose various strategies towards
fault repair of the faults detected. We implement our technique on top of the
Apache TVM deep learning compiler, and we test it by conducting a preliminary
fault localization analysis for the conversion of InceptionV3 from TF to
TFLite. Our approach detected a fault in a common DNN converter tool, which
introduced precision errors in weights, reducing model accuracy. After our
fault localization, we repaired the issue, reducing our conversion error to
zero.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.06208">DeltaNN: Assessing the Impact of Computational Environment Parameters on the Performance of Image Recognition Models. (arXiv:2306.06208v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Louloudakis_N/0/1/0/all/0/1">Nikolaos Louloudakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Gibson_P/0/1/0/all/0/1">Perry Gibson</a>, <a href="http://arxiv.org/find/cs/1/au:+Cano_J/0/1/0/all/0/1">Jos&#xe9; Cano</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajan_A/0/1/0/all/0/1">Ajitha Rajan</a></p>
<p>Image recognition tasks typically use deep learning and require enormous
processing power, thus relying on hardware accelerators like GPUs and TPUs for
fast, timely processing. Failure in real-time image recognition tasks can occur
due to sub-optimal mapping on hardware accelerators during model deployment,
which may lead to timing uncertainty and erroneous behavior. Mapping on
hardware accelerators is done using multiple software components like deep
learning frameworks, compilers, and device libraries, that we refer to as the
computational environment. Owing to the increased use of image recognition
tasks in safety-critical applications like autonomous driving and medical
imaging, it is imperative to assess their robustness to changes in the
computational environment, as the impact of parameters like deep learning
frameworks, compiler optimizations, and hardware devices on model performance
and correctness is not yet well understood.
</p>
<p>In this paper we present a differential testing framework, DeltaNN, that
allows us to assess the impact of different computational environment
parameters on the performance of image recognition models during deployment,
post training. DeltaNN generates different implementations of a given image
recognition model for variations in environment parameters, namely, deep
learning frameworks, compiler optimizations and hardware devices and analyzes
differences in model performance as a result. Using DeltaNN, we conduct an
empirical study of robustness analysis of three popular image recognition
models using the ImageNet dataset. We report the impact in terms of
misclassifications and inference time differences across different settings. In
total, we observed up to 72% output label differences across deep learning
frameworks, and up to 81% unexpected performance degradation in terms of
inference time, when applying compiler optimizations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.10280">OpenGSL: A Comprehensive Benchmark for Graph Structure Learning. (arXiv:2306.10280v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhiyao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1">Sheng Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_B/0/1/0/all/0/1">Bochao Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xuanyi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiawei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_Q/0/1/0/all/0/1">Qiaoyu Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zha_D/0/1/0/all/0/1">Daochen Zha</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yan Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Can Wang</a></p>
<p>Graph Neural Networks (GNNs) have emerged as the de facto standard for
representation learning on graphs, owing to their ability to effectively
integrate graph topology and node attributes. However, the inherent suboptimal
nature of node connections, resulting from the complex and contingent formation
process of graphs, presents significant challenges in modeling them
effectively. To tackle this issue, Graph Structure Learning (GSL), a family of
data-centric learning approaches, has garnered substantial attention in recent
years. The core concept behind GSL is to jointly optimize the graph structure
and the corresponding GNN models. Despite the proposal of numerous GSL methods,
the progress in this field remains unclear due to inconsistent experimental
protocols, including variations in datasets, data processing techniques, and
splitting strategies. In this paper, we introduce OpenGSL, the first
comprehensive benchmark for GSL, aimed at addressing this gap. OpenGSL enables
a fair comparison among state-of-the-art GSL methods by evaluating them across
various popular datasets using uniform data processing and splitting
strategies. Through extensive experiments, we observe that existing GSL methods
do not consistently outperform vanilla GNN counterparts. We also find that
there is no significant correlation between the homophily of the learned
structure and task performance, challenging the common belief. Moreover, we
observe that the learned graph structure demonstrates a strong generalization
ability across different GNN models, despite the high computational and space
consumption. We hope that our open-sourced library will facilitate rapid and
equitable evaluation and inspire further innovative research in this field. The
code of the benchmark can be found in https://github.com/OpenGSL/OpenGSL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.10982">Differentially Private Over-the-Air Federated Learning Over MIMO Fading Channels. (arXiv:2306.10982v3 [cs.IT] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1">Jia Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Ying-Jun Angela Zhang</a></p>
<p>Federated learning (FL) enables edge devices to collaboratively train machine
learning models, with model communication replacing direct data uploading.
While over-the-air model aggregation improves communication efficiency,
uploading models to an edge server over wireless networks can pose privacy
risks. Differential privacy (DP) is a widely used quantitative technique to
measure statistical data privacy in FL. Previous research has focused on
over-the-air FL with a single-antenna server, leveraging communication noise to
enhance user-level DP. This approach achieves the so-called "free DP" by
controlling transmit power rather than introducing additional DP-preserving
mechanisms at devices, such as adding artificial noise. In this paper, we study
differentially private over-the-air FL over a multiple-input multiple-output
(MIMO) fading channel. We show that FL model communication with a
multiple-antenna server amplifies privacy leakage as the multiple-antenna
server employs separate receive combining for model aggregation and information
inference. Consequently, relying solely on communication noise, as done in the
multiple-input single-output system, cannot meet high privacy requirements, and
a device-side privacy-preserving mechanism is necessary for optimal DP design.
We analyze the learning convergence and privacy loss of the studied FL system
and propose a transceiver design algorithm based on alternating optimization.
Numerical results demonstrate that the proposed method achieves a better
privacy-learning trade-off compared to prior work.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.12383">Sample Complexity for Quadratic Bandits: Hessian Dependent Bounds and Optimal Algorithms. (arXiv:2306.12383v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1">Qian Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yining Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1">Baihe Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_Q/0/1/0/all/0/1">Qi Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jason D. Lee</a></p>
<p>In stochastic zeroth-order optimization, a problem of practical relevance is
understanding how to fully exploit the local geometry of the underlying
objective function. We consider a fundamental setting in which the objective
function is quadratic, and provide the first tight characterization of the
optimal Hessian-dependent sample complexity. Our contribution is twofold.
First, from an information-theoretic point of view, we prove tight lower bounds
on Hessian-dependent complexities by introducing a concept called energy
allocation, which captures the interaction between the searching algorithm and
the geometry of objective functions. A matching upper bound is obtained by
solving the optimal energy spectrum. Then, algorithmically, we show the
existence of a Hessian-independent algorithm that universally achieves the
asymptotic optimal sample complexities for all Hessian instances. The optimal
sample complexities achieved by our algorithm remain valid for heavy-tailed
noise distributions, which are enabled by a truncation method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.12574">An efficient and straightforward online quantization method for a data stream through remove-birth updating. (arXiv:2306.12574v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fujita_K/0/1/0/all/0/1">Kazuhisa Fujita</a></p>
<p>The growth of network-connected devices has led to an exponential increase in
data generation, creating significant challenges for efficient data analysis.
This data is generated continuously, creating a dynamic flow known as a data
stream. The characteristics of a data stream may change dynamically, and this
change is known as concept drift. Consequently, a method for handling data
streams must efficiently reduce their volume while dynamically adapting to
these changing characteristics. This paper proposes a simple online vector
quantization method for concept drift. The proposed method identifies and
replaces units with low win probability through remove-birth updating, thus
achieving a rapid adaptation to concept drift. Furthermore, the results of this
study show that the proposed method can generate minimal dead units even in the
presence of concept drift. This study also suggests that some metrics
calculated from the proposed method will be helpful for drift detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.02588">TransformerG2G: Adaptive time-stepping for learning temporal graph embeddings using transformers. (arXiv:2307.02588v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Varghese_A/0/1/0/all/0/1">Alan John Varghese</a>, <a href="http://arxiv.org/find/cs/1/au:+Bora_A/0/1/0/all/0/1">Aniruddha Bora</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1">Mengjia Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Karniadakis_G/0/1/0/all/0/1">George Em Karniadakis</a></p>
<p>Dynamic graph embedding has emerged as a very effective technique for
addressing diverse temporal graph analytic tasks (i.e., link prediction, node
classification, recommender systems, anomaly detection, and graph generation)
in various applications. Such temporal graphs exhibit heterogeneous transient
dynamics, varying time intervals, and highly evolving node features throughout
their evolution. Hence, incorporating long-range dependencies from the
historical graph context plays a crucial role in accurately learning their
temporal dynamics. In this paper, we develop a graph embedding model with
uncertainty quantification, TransformerG2G, by exploiting the advanced
transformer encoder to first learn intermediate node representations from its
current state ($t$) and previous context (over timestamps [$t-1, t-l$], $l$ is
the length of context). Moreover, we employ two projection layers to generate
lower-dimensional multivariate Gaussian distributions as each node's latent
embedding at timestamp $t$. We consider diverse benchmarks with varying levels
of ``novelty" as measured by the TEA (Temporal Edge Appearance) plots. Our
experiments demonstrate that the proposed TransformerG2G model outperforms
conventional multi-step methods and our prior work (DynG2G) in terms of both
link prediction accuracy and computational efficiency, especially for high
degree of novelty. Furthermore, the learned time-dependent attention weights
across multiple graph snapshots reveal the development of an automatic adaptive
time stepping enabled by the transformer. Importantly, by examining the
attention weights, we can uncover temporal dependencies, identify influential
elements, and gain insights into the complex interactions within the graph
structure. For example, we identified a strong correlation between attention
weights and node degree at the various stages of the graph topology evolution.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.02779">Large Language Models Empowered Autonomous Edge AI for Connected Intelligence. (arXiv:2307.02779v3 [cs.IT] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yifei Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1">Jiawei Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xinjie Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zehong Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1">Hao Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dongsheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Letaief_K/0/1/0/all/0/1">Khaled B. Letaief</a></p>
<p>The evolution of wireless networks gravitates towards connected intelligence,
a concept that envisions seamless interconnectivity among humans, objects, and
intelligence in a hyper-connected cyber-physical world. Edge artificial
intelligence (Edge AI) is a promising solution to achieve connected
intelligence by delivering high-quality, low-latency, and privacy-preserving AI
services at the network edge. This article presents a vision of autonomous edge
AI systems that automatically organize, adapt, and optimize themselves to meet
users' diverse requirements, leveraging the power of large language models
(LLMs), i.e., Generative Pretrained Transformer (GPT). By exploiting the
powerful abilities of GPT in language understanding, planning, and code
generation, as well as incorporating classic wisdom such as task-oriented
communication and edge federated learning, we present a versatile framework
that efficiently coordinates edge AI models to cater to users' personal demands
while automatically generating code to train new models in a privacy-preserving
manner. Experimental results demonstrate the system's remarkable ability to
accurately comprehend user demands, efficiently execute AI models with minimal
cost, and effectively create high-performance AI models at edge servers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.02813">CPDG: A Contrastive Pre-Training Method for Dynamic Graph Neural Networks. (arXiv:2307.02813v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bei_Y/0/1/0/all/0/1">Yuanchen Bei</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1">Sheng Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chi_H/0/1/0/all/0/1">Huixuan Chi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haishuai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Mengdi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Bu_J/0/1/0/all/0/1">Jiajun Bu</a></p>
<p>Dynamic graph data mining has gained popularity in recent years due to the
rich information contained in dynamic graphs and their widespread use in the
real world. Despite the advances in dynamic graph neural networks (DGNNs), the
rich information and diverse downstream tasks have posed significant
difficulties for the practical application of DGNNs in industrial scenarios. To
this end, in this paper, we propose to address them by pre-training and present
the Contrastive Pre-Training Method for Dynamic Graph Neural Networks (CPDG).
CPDG tackles the challenges of pre-training for DGNNs, including generalization
capability and long-short term modeling capability, through a flexible
structural-temporal subgraph sampler along with structural-temporal contrastive
pre-training schemes. Extensive experiments conducted on both large-scale
research and industrial dynamic graph datasets show that CPDG outperforms
existing methods in dynamic graph pre-training for various downstream tasks
under three transfer settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05385">Learned Kernels for Interpretable and Efficient Medical Time Series Processing. (arXiv:2307.05385v2 [eess.SP] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Chen_S/0/1/0/all/0/1">Sully F. Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Guo_Z/0/1/0/all/0/1">Zhicheng Guo</a>, <a href="http://arxiv.org/find/eess/1/au:+Ding_C/0/1/0/all/0/1">Cheng Ding</a>, <a href="http://arxiv.org/find/eess/1/au:+Hu_X/0/1/0/all/0/1">Xiao Hu</a>, <a href="http://arxiv.org/find/eess/1/au:+Rudin_C/0/1/0/all/0/1">Cynthia Rudin</a></p>
<p>Background: Signal processing methods are the foundation for clinical
interpretation across a wide variety of medical applications. The advent of
deep learning allowed for an explosion of new models that offered unprecedented
performance but at a cost: deep learning models are often compute-intensive and
lack interpretability.
</p>
<p>Methods: We propose a sparse, interpretable architecture for medical time
series processing. The method learns a set of lightweight flexible kernels to
construct a single-layer neural network, providing a new efficient, robust, and
interpretable approach. We introduce novel parameter reduction techniques to
further reduce the size of our network. We demonstrate the power of our
architecture on the important task of photoplethysmography artifact detection,
where our approach has performance similar to the state-of-the-art deep neural
networks with several orders of magnitude fewer parameters, allowing for the
integration of deep neural network level performance into extremely low-power
wearable devices.
</p>
<p>Results: Our interpretable method achieves greater than 99\% of the
performance of the state-of-the-art methods on the artifact detection task, and
even outperforms the state-of-the-art on a challenging out-of-distribution test
set, while using dramatically fewer parameters (2\% of the parameters of
Segade, and about half of the parameters of Tiny-PPG).
</p>
<p>Conclusions: Learned kernels are competitive with deep neural networks for
medical time series processing with dramatically fewer parameters. Our method
is particularly suited for real-time applications and low-power devices, and it
maintains interpretability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07516">Voting-based Multimodal Automatic Deception Detection. (arXiv:2307.07516v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Touma_L/0/1/0/all/0/1">Lana Touma</a>, <a href="http://arxiv.org/find/cs/1/au:+Horani_M/0/1/0/all/0/1">Mohammad Al Horani</a>, <a href="http://arxiv.org/find/cs/1/au:+Tailouni_M/0/1/0/all/0/1">Manar Tailouni</a>, <a href="http://arxiv.org/find/cs/1/au:+Dahabiah_A/0/1/0/all/0/1">Anas Dahabiah</a>, <a href="http://arxiv.org/find/cs/1/au:+Jallad_K/0/1/0/all/0/1">Khloud Al Jallad</a></p>
<p>Automatic Deception Detection has been a hot research topic for a long time,
using machine learning and deep learning to automatically detect deception,
brings new light to this old field. In this paper, we proposed a voting-based
method for automatic deception detection from videos using audio, visual and
lexical features. Experiments were done on two datasets, the Real-life trial
dataset by Michigan University and the Miami University deception detection
dataset. Video samples were split into frames of images, audio, and
manuscripts. Our Voting-based Multimodal proposed solution consists of three
models. The first model is CNN for detecting deception from images, the second
model is Support Vector Machine (SVM) on Mel spectrograms for detecting
deception from audio and the third model is Word2Vec on Support Vector Machine
(SVM) for detecting deception from manuscripts. Our proposed solution
outperforms state of the art. Best results achieved on images, audio and text
were 97%, 96%, 92% respectively on Real-Life Trial Dataset, and 97%, 82%, 73%
on video, audio and text respectively on Miami University Deception Detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.12480">Learning Resource Allocation Policy: Vertex-GNN or Edge-GNN?. (arXiv:2307.12480v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1">Yao Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jia Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Chenyang Yang</a></p>
<p>Graph neural networks (GNNs) update the hidden representations of vertices
(called Vertex-GNNs) or hidden representations of edges (called Edge-GNNs) by
processing and pooling the information of neighboring vertices and edges and
combining to exploit topology information. When learning resource allocation
policies, GNNs cannot perform well if their expressive power is weak, i.e., if
they cannot differentiate all input features such as channel matrices. In this
paper, we analyze the expressive power of the Vertex-GNNs and Edge-GNNs for
learning three representative wireless policies: link scheduling, power
control, and precoding policies. We find that the expressive power of the GNNs
depends on the linearity and output dimensions of the processing and
combination functions. When linear processors are used, the Vertex-GNNs cannot
differentiate all channel matrices due to the loss of channel information,
while the Edge-GNNs can. When learning the precoding policy, even the
Vertex-GNNs with non-linear processors may not be with strong expressive
ability due to the dimension compression. We proceed to provide necessary
conditions for the GNNs to well learn the precoding policy. Simulation results
validate the analyses and show that the Edge-GNNs can achieve the same
performance as the Vertex-GNNs with much lower training and inference time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14439">Fixed Integral Neural Networks. (arXiv:2307.14439v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kortvelesy_R/0/1/0/all/0/1">Ryan Kortvelesy</a></p>
<p>It is often useful to perform integration over learned functions represented
by neural networks. However, this integration is usually performed numerically,
as analytical integration over learned functions (especially neural networks)
is generally viewed as intractable. In this work, we present a method for
representing the analytical integral of a learned function $f$. This allows the
exact integral of a neural network to be computed, and enables constrained
neural networks to be parametrised by applying constraints directly to the
integral. Crucially, we also introduce a method to constrain $f$ to be
positive, a necessary condition for many applications (e.g. probability
distributions, distance metrics, etc). Finally, we introduce several
applications where our fixed-integral neural network (FINN) can be utilised.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03321">AFN: Adaptive Fusion Normalization via an Encoder-Decoder Framework. (arXiv:2308.03321v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zikai Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huanran Chen</a></p>
<p>The success of deep learning is inseparable from normalization layers.
Researchers have proposed various normalization functions, and each of them has
both advantages and disadvantages. In response, efforts have been made to
design a unified normalization function that combines all normalization
procedures and mitigates their weaknesses. We also proposed a new normalization
function called Adaptive Fusion Normalization. Through experiments, we
demonstrate AFN outperforms the previous normalization techniques in domain
generalization and image classification tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.11730">Knowledge Graph Prompting for Multi-Document Question Answering. (arXiv:2308.11730v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lipka_N/0/1/0/all/0/1">Nedim Lipka</a>, <a href="http://arxiv.org/find/cs/1/au:+Rossi_R/0/1/0/all/0/1">Ryan A. Rossi</a>, <a href="http://arxiv.org/find/cs/1/au:+Siu_A/0/1/0/all/0/1">Alexa Siu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Ruiyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Derr_T/0/1/0/all/0/1">Tyler Derr</a></p>
<p>The `pre-train, prompt, predict' paradigm of large language models (LLMs) has
achieved remarkable success in open-domain question answering (OD-QA). However,
few works explore this paradigm in the scenario of multi-document question
answering (MD-QA), a task demanding a thorough understanding of the logical
associations among the contents and structures of different documents. To fill
this crucial gap, we propose a Knowledge Graph Prompting (KGP) method to
formulate the right context in prompting LLMs for MD-QA, which consists of a
graph construction module and a graph traversal module. For graph construction,
we create a knowledge graph (KG) over multiple documents with nodes symbolizing
passages or document structures (e.g., pages/tables), and edges denoting the
semantic/lexical similarity between passages or intra-document structural
relations. For graph traversal, we design an LLM-based graph traversal agent
that navigates across nodes and gathers supporting passages assisting LLMs in
MD-QA. The constructed graph serves as the global ruler that regulates the
transitional space among passages and reduces retrieval latency. Concurrently,
the graph traversal agent acts as a local navigator that gathers pertinent
context to progressively approach the question and guarantee retrieval quality.
Extensive experiments underscore the efficacy of KGP for MD-QA, signifying the
potential of leveraging graphs in enhancing the prompt design for LLMs. Our
code: https://github.com/YuWVandy/KG-LLM-MDQA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12110">Constrained Stein Variational Trajectory Optimization. (arXiv:2308.12110v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Power_T/0/1/0/all/0/1">Thomas Power</a>, <a href="http://arxiv.org/find/cs/1/au:+Berenson_D/0/1/0/all/0/1">Dmitry Berenson</a></p>
<p>We present Constrained Stein Variational Trajectory Optimization (CSVTO), an
algorithm for performing trajectory optimization with constraints on a set of
trajectories in parallel. We frame constrained trajectory optimization as a
novel form of constrained functional minimization over trajectory
distributions, which avoids treating the constraints as a penalty in the
objective and allows us to generate diverse sets of constraint-satisfying
trajectories. Our method uses Stein Variational Gradient Descent (SVGD) to find
a set of particles that approximates a distribution over low-cost trajectories
while obeying constraints. CSVTO is applicable to problems with arbitrary
equality and inequality constraints and includes a novel particle resampling
step to escape local minima. By explicitly generating diverse sets of
trajectories, CSVTO is better able to avoid poor local minima and is more
robust to initialization. We demonstrate that CSVTO outperforms baselines in
challenging highly-constrained tasks, such as a 7DoF wrench manipulation task,
where CSVTO succeeds in 20/20 trials vs 13/20 for the closest baseline. Our
results demonstrate that generating diverse constraint-satisfying trajectories
improves robustness to disturbances and initialization over baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12161">Data-driven decision-focused surrogate modeling. (arXiv:2308.12161v2 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Gupta_R/0/1/0/all/0/1">Rishabh Gupta</a>, <a href="http://arxiv.org/find/math/1/au:+Zhang_Q/0/1/0/all/0/1">Qi Zhang</a></p>
<p>We introduce the concept of decision-focused surrogate modeling for solving
computationally challenging nonlinear optimization problems in real-time
settings. The proposed data-driven framework seeks to learn a simpler, e.g.
convex, surrogate optimization model that is trained to minimize the decision
prediction error, which is defined as the difference between the optimal
solutions of the original and the surrogate optimization models. The learning
problem, formulated as a bilevel program, can be viewed as a data-driven
inverse optimization problem to which we apply a decomposition-based solution
algorithm from previous work. We validate our framework through numerical
experiments involving the optimization of common nonlinear chemical processes
such as chemical reactors, heat exchanger networks, and material blending
systems. We also present a detailed comparison of decision-focused surrogate
modeling with standard data-driven surrogate modeling methods and demonstrate
that our approach is significantly more data-efficient while producing simple
surrogate models with high decision prediction accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.13150">Enhancing Breast Cancer Histopathology Image Classification Using Dual-Activated Lightweight Attention ResNet50 Model. (arXiv:2308.13150v5 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1">Suxing Liu</a></p>
<p>Despite the remarkable results of deep learning in breast cancer
histopathology image classification, challenges such as data imbalance and
interpretability still exist and require cross-domain knowledge and
collaboration among medical experts. This study proposes a breast cancer
classification method using a dual-activated lightweight attention ResNet50
model, effectively addressing data imbalance and interpretability challenges.
The model fuses a pre-trained deep ResNet50 and a lightweight attention
mechanism to accomplish classification by embedding an attention module in
layer 4 of ResNet50 and adding two fully connected layers. The fully connected
network design employs LeakyReLU and ReLU activation functions.
</p>
<p>The model outperforms SEResNet50, DensNet121, VGG16, VGG16Inception, ViT,
Swin- Transformer, Dinov2_Vitb14, and ResNet50 models regarding precision,
accuracy, recall, F1 score, and GMean, especially in the application
performance on the BreakHis dataset. In particular, the model demonstrates
significant robustness and broad applicability when dealing with the unbalanced
breast cancer dataset. The model has been evaluated on histopathology images at
magnification factors of 40X, 100X, 200X, and 400X, achieving accuracies of
98.5%, 98.7%, 97.9%, and 94.3%, respectively. The study comprehensively
assessed the model's performance. In the later stages of training, the
validated losses and accuracies change minimally, showing that the model avoids
overfitting and exhibits good generalization ability. This model exhibited the
fastest convergence in all laboratory experiments, even though its parameters
are not the smallest. This highlights the model's efficacy as a lightweight
attention framework, showcasing its efficiency in achieving rapid convergence
without compromising performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.14089">MedAlign: A Clinician-Generated Dataset for Instruction Following with Electronic Medical Records. (arXiv:2308.14089v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fleming_S/0/1/0/all/0/1">Scott L. Fleming</a>, <a href="http://arxiv.org/find/cs/1/au:+Lozano_A/0/1/0/all/0/1">Alejandro Lozano</a>, <a href="http://arxiv.org/find/cs/1/au:+Haberkorn_W/0/1/0/all/0/1">William J. Haberkorn</a>, <a href="http://arxiv.org/find/cs/1/au:+Jindal_J/0/1/0/all/0/1">Jenelle A. Jindal</a>, <a href="http://arxiv.org/find/cs/1/au:+Reis_E/0/1/0/all/0/1">Eduardo P. Reis</a>, <a href="http://arxiv.org/find/cs/1/au:+Thapa_R/0/1/0/all/0/1">Rahul Thapa</a>, <a href="http://arxiv.org/find/cs/1/au:+Blankemeier_L/0/1/0/all/0/1">Louis Blankemeier</a>, <a href="http://arxiv.org/find/cs/1/au:+Genkins_J/0/1/0/all/0/1">Julian Z. Genkins</a>, <a href="http://arxiv.org/find/cs/1/au:+Steinberg_E/0/1/0/all/0/1">Ethan Steinberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Nayak_A/0/1/0/all/0/1">Ashwin Nayak</a>, <a href="http://arxiv.org/find/cs/1/au:+Patel_B/0/1/0/all/0/1">Birju S. Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Chiang_C/0/1/0/all/0/1">Chia-Chun Chiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Callahan_A/0/1/0/all/0/1">Alison Callahan</a>, <a href="http://arxiv.org/find/cs/1/au:+Huo_Z/0/1/0/all/0/1">Zepeng Huo</a>, <a href="http://arxiv.org/find/cs/1/au:+Gatidis_S/0/1/0/all/0/1">Sergios Gatidis</a>, <a href="http://arxiv.org/find/cs/1/au:+Adams_S/0/1/0/all/0/1">Scott J. Adams</a>, <a href="http://arxiv.org/find/cs/1/au:+Fayanju_O/0/1/0/all/0/1">Oluseyi Fayanju</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1">Shreya J. Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Savage_T/0/1/0/all/0/1">Thomas Savage</a>, <a href="http://arxiv.org/find/cs/1/au:+Goh_E/0/1/0/all/0/1">Ethan Goh</a>, <a href="http://arxiv.org/find/cs/1/au:+Chaudhari_A/0/1/0/all/0/1">Akshay S. Chaudhari</a>, <a href="http://arxiv.org/find/cs/1/au:+Aghaeepour_N/0/1/0/all/0/1">Nima Aghaeepour</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharp_C/0/1/0/all/0/1">Christopher Sharp</a>, <a href="http://arxiv.org/find/cs/1/au:+Pfeffer_M/0/1/0/all/0/1">Michael A. Pfeffer</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1">Percy Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jonathan H. Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Morse_K/0/1/0/all/0/1">Keith E. Morse</a>, <a href="http://arxiv.org/find/cs/1/au:+Brunskill_E/0/1/0/all/0/1">Emma P. Brunskill</a>, <a href="http://arxiv.org/find/cs/1/au:+Fries_J/0/1/0/all/0/1">Jason A. Fries</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1">Nigam H. Shah</a></p>
<p>The ability of large language models (LLMs) to follow natural language
instructions with human-level fluency suggests many opportunities in healthcare
to reduce administrative burden and improve quality of care. However,
evaluating LLMs on realistic text generation tasks for healthcare remains
challenging. Existing question answering datasets for electronic health record
(EHR) data fail to capture the complexity of information needs and
documentation burdens experienced by clinicians. To address these challenges,
we introduce MedAlign, a benchmark dataset of 983 natural language instructions
for EHR data. MedAlign is curated by 15 clinicians (7 specialities), includes
clinician-written reference responses for 303 instructions, and provides 276
longitudinal EHRs for grounding instruction-response pairs. We used MedAlign to
evaluate 6 general domain LLMs, having clinicians rank the accuracy and quality
of each LLM response. We found high error rates, ranging from 35% (GPT-4) to
68% (MPT-7B-Instruct), and an 8.3% drop in accuracy moving from 32k to 2k
context lengths for GPT-4. Finally, we report correlations between clinician
rankings and automated natural language generation metrics as a way to rank
LLMs without human review. We make MedAlign available under a research data use
agreement to enable LLM evaluations on tasks aligned with clinician needs and
preferences.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.14602">Recent Progress in Energy Management of Connected Hybrid Electric Vehicles Using Reinforcement Learning. (arXiv:2308.14602v2 [eess.SY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Hua_M/0/1/0/all/0/1">Min Hua</a>, <a href="http://arxiv.org/find/eess/1/au:+Shuai_B/0/1/0/all/0/1">Bin Shuai</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_Q/0/1/0/all/0/1">Quan Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1">Jinhai Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+He_Y/0/1/0/all/0/1">Yinglong He</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_H/0/1/0/all/0/1">Hongming Xu</a></p>
<p>The growing adoption of hybrid electric vehicles (HEVs) presents a
transformative opportunity for revolutionizing transportation energy systems.
The shift towards electrifying transportation aims to curb environmental
concerns related to fossil fuel consumption. This necessitates efficient energy
management systems (EMS) to optimize energy efficiency. The evolution of EMS
from HEVs to connected hybrid electric vehicles (CHEVs) represent a pivotal
shift. For HEVs, EMS now confronts the intricate energy cooperation
requirements of CHEVs, necessitating advanced algorithms for route
optimization, charging coordination, and load distribution. Challenges persist
in both domains, including optimal energy utilization for HEVs, and cooperative
eco-driving control (CED) for CHEVs across diverse vehicle types. Reinforcement
learning (RL) stands out as a promising tool for addressing these challenges.
Specifically, within the realm of CHEVs, the application of multi-agent
reinforcement learning (MARL) emerges as a powerful approach for effectively
tackling the intricacies of CED control. Despite extensive research, few
reviews span from individual vehicles to multi-vehicle scenarios. This review
bridges the gap, highlighting challenges, advancements, and potential
contributions of RL-based solutions for future sustainable transportation
systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.01966">AdaPlus: Integrating Nesterov Momentum and Precise Stepsize Adjustment on AdamW Basis. (arXiv:2309.01966v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guan_L/0/1/0/all/0/1">Lei Guan</a></p>
<p>This paper proposes an efficient optimizer called AdaPlus which integrates
Nesterov momentum and precise stepsize adjustment on AdamW basis. AdaPlus
combines the advantages of AdamW, Nadam, and AdaBelief and, in particular, does
not introduce any extra hyper-parameters. We perform extensive experimental
evaluations on three machine learning tasks to validate the effectiveness of
AdaPlus. The experiment results validate that AdaPlus (i) among all the
evaluated adaptive methods, performs most comparable with (even slightly better
than) SGD with momentum on image classification tasks and (ii) outperforms
other state-of-the-art optimizers on language modeling tasks and illustrates
pretty high stability when training GANs. The experiment code of AdaPlus will
be accessible at: https://github.com/guanleics/AdaPlus.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07867">Beta Diffusion. (arXiv:2309.07867v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1">Mingyuan Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tianqi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhendong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1">Huangjie Zheng</a></p>
<p>We introduce beta diffusion, a novel generative modeling method that
integrates demasking and denoising to generate data within bounded ranges.
Using scaled and shifted beta distributions, beta diffusion utilizes
multiplicative transitions over time to create both forward and reverse
diffusion processes, maintaining beta distributions in both the forward
marginals and the reverse conditionals, given the data at any point in time.
Unlike traditional diffusion-based generative models relying on additive
Gaussian noise and reweighted evidence lower bounds (ELBOs), beta diffusion is
multiplicative and optimized with KL-divergence upper bounds (KLUBs) derived
from the convexity of the KL divergence. We demonstrate that the proposed KLUBs
are more effective for optimizing beta diffusion compared to negative ELBOs,
which can also be derived as the KLUBs of the same KL divergence with its two
arguments swapped. The loss function of beta diffusion, expressed in terms of
Bregman divergence, further supports the efficacy of KLUBs for optimization.
Experimental results on both synthetic data and natural images demonstrate the
unique capabilities of beta diffusion in generative modeling of range-bounded
data and validate the effectiveness of KLUBs in optimizing diffusion models,
thereby making them valuable additions to the family of diffusion-based
generative models and the optimization techniques used to train them.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.08201">Sparsity-Aware Distributed Learning for Gaussian Processes with Linear Multiple Kernel. (arXiv:2309.08201v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Suwandi_R/0/1/0/all/0/1">Richard Cornelius Suwandi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zhidi Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_F/0/1/0/all/0/1">Feng Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhiguo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Theodoridis_S/0/1/0/all/0/1">Sergios Theodoridis</a></p>
<p>Gaussian processes (GPs) stand as crucial tools in machine learning and
signal processing, with their effectiveness hinging on kernel design and
hyper-parameter optimization. This paper presents a novel GP linear multiple
kernel (LMK) and a generic sparsity-aware distributed learning framework to
optimize the hyper-parameters. The newly proposed grid spectral mixture (GSM)
kernel is tailored for multi-dimensional data, effectively reducing the number
of hyper-parameters while maintaining good approximation capabilities. We
further demonstrate that the associated hyper-parameter optimization of this
kernel yields sparse solutions. To exploit the inherent sparsity property of
the solutions, we introduce the Sparse LInear Multiple Kernel Learning
(SLIM-KL) framework. The framework incorporates a quantized alternating
direction method of multipliers (ADMM) scheme for collaborative learning among
multiple agents, where the local optimization problem is solved using a
distributed successive convex approximation (DSCA) algorithm. SLIM-KL
effectively manages large-scale hyper-parameter optimization for the proposed
kernel, simultaneously ensuring data privacy and minimizing communication
costs. Theoretical analysis establishes convergence guarantees for the learning
framework, while experiments on diverse datasets demonstrate the superior
prediction performance and efficiency of our proposed methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.08420">FedDCSR: Federated Cross-domain Sequential Recommendation via Disentangled Representation Learning. (arXiv:2309.08420v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1">Dongyi Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1">Jiyuan Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1">Qing Liao</a></p>
<p>Cross-domain Sequential Recommendation (CSR) which leverages user sequence
data from multiple domains has received extensive attention in recent years.
However, the existing CSR methods require sharing origin user data across
domains, which violates the General Data Protection Regulation (GDPR). Thus, it
is necessary to combine federated learning (FL) and CSR to fully utilize
knowledge from different domains while preserving data privacy. Nonetheless,
the sequence feature heterogeneity across different domains significantly
impacts the overall performance of FL. In this paper, we propose FedDCSR, a
novel federated cross-domain sequential recommendation framework via
disentangled representation learning. Specifically, to address the sequence
feature heterogeneity across domains, we introduce an approach called
inter-intra domain sequence representation disentanglement (SRD) to disentangle
the user sequence features into domain-shared and domain-exclusive features. In
addition, we design an intra domain contrastive infomax (CIM) strategy to learn
richer domain-exclusive features of users by performing data augmentation on
user sequences. Extensive experiments on three real-world scenarios demonstrate
that FedDCSR achieves significant improvements over existing baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12334">Deep Knowledge Tracing is an implicit dynamic multidimensional item response theory model. (arXiv:2309.12334v3 [cs.CY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vie_J/0/1/0/all/0/1">Jill-J&#xea;nn Vie</a> (SODA), <a href="http://arxiv.org/find/cs/1/au:+Kashima_H/0/1/0/all/0/1">Hisashi Kashima</a></p>
<p>Knowledge tracing consists in predicting the performance of some students on
new questions given their performance on previous questions, and can be a prior
step to optimizing assessment and learning. Deep knowledge tracing (DKT) is a
competitive model for knowledge tracing relying on recurrent neural networks,
even if some simpler models may match its performance. However, little is known
about why DKT works so well. In this paper, we frame deep knowledge tracing as
a encoderdecoder architecture. This viewpoint not only allows us to propose
better models in terms of performance, simplicity or expressivity but also
opens up promising avenues for future research directions. In particular, we
show on several small and large datasets that a simpler decoder, with possibly
fewer parameters than the one used by DKT, can predict student performance
better.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.14316">Physics of Language Models: Part 3.1, Knowledge Storage and Extraction. (arXiv:2309.14316v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Allen_Zhu_Z/0/1/0/all/0/1">Zeyuan Allen-Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuanzhi Li</a></p>
<p>Large language models (LLMs) can store a vast amount of world knowledge,
often extractable via question-answering (e.g., "What is Abraham Lincoln's
birthday?"). However, do they answer such questions based on exposure to
similar questions during training (i.e., cheating), or by genuinely learning to
extract knowledge from sources like Wikipedia?
</p>
<p>In this paper, we investigate this issue using a controlled biography
dataset. We find a strong correlation between the model's ability to extract
knowledge and various diversity measures of the training data.
$\textbf{Essentially}$, for knowledge to be reliably extracted, it must be
sufficiently augmented (e.g., through paraphrasing, sentence shuffling)
$\textit{during pretraining}$. Without such augmentation, knowledge may be
memorized but not extractable, leading to 0% accuracy, regardless of subsequent
instruction fine-tuning.
</p>
<p>To understand why this occurs, we employ (nearly) linear probing to
demonstrate a strong connection between the observed correlation and how the
model internally encodes knowledge -- whether it is linearly encoded in the
hidden embeddings of entity names or distributed across other token embeddings
in the training text.
</p>
<p>This paper provides $\textbf{several key recommendations for LLM pretraining
in the industry}$: (1) rewrite the pretraining data -- using small, auxiliary
models -- to provide knowledge augmentation, and (2) incorporate more
instruction-finetuning data into the pretraining stage before it becomes too
late.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.14970">Recurrent Hypernetworks are Surprisingly Strong in Meta-RL. (arXiv:2309.14970v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Beck_J/0/1/0/all/0/1">Jacob Beck</a>, <a href="http://arxiv.org/find/cs/1/au:+Vuorio_R/0/1/0/all/0/1">Risto Vuorio</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1">Zheng Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1">Shimon Whiteson</a></p>
<p>Deep reinforcement learning (RL) is notoriously impractical to deploy due to
sample inefficiency. Meta-RL directly addresses this sample inefficiency by
learning to perform few-shot learning when a distribution of related tasks is
available for meta-training. While many specialized meta-RL methods have been
proposed, recent work suggests that end-to-end learning in conjunction with an
off-the-shelf sequential model, such as a recurrent network, is a surprisingly
strong baseline. However, such claims have been controversial due to limited
supporting evidence, particularly in the face of prior work establishing
precisely the opposite. In this paper, we conduct an empirical investigation.
While we likewise find that a recurrent network can achieve strong performance,
we demonstrate that the use of hypernetworks is crucial to maximizing their
potential. Surprisingly, when combined with hypernetworks, the recurrent
baselines that are far simpler than existing specialized methods actually
achieve the strongest performance of all methods evaluated. We provide code at
https://github.com/jacooba/hyper.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00806">Bayesian Design Principles for Frequentist Sequential Learning. (arXiv:2310.00806v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yunbei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeevi_A/0/1/0/all/0/1">Assaf Zeevi</a></p>
<p>We develop a general theory to optimize the frequentist regret for sequential
learning problems, where efficient bandit and reinforcement learning algorithms
can be derived from unified Bayesian principles. We propose a novel
optimization approach to generate "algorithmic beliefs" at each round, and use
Bayesian posteriors to make decisions. The optimization objective to create
"algorithmic beliefs," which we term "Algorithmic Information Ratio,"
represents an intrinsic complexity measure that effectively characterizes the
frequentist regret of any algorithm. To the best of our knowledge, this is the
first systematical approach to make Bayesian-type algorithms prior-free and
applicable to adversarial settings, in a generic and optimal manner. Moreover,
the algorithms are simple and often efficient to implement. As a major
application, we present a novel algorithm for multi-armed bandits that achieves
the "best-of-all-worlds" empirical performance in the stochastic, adversarial,
and non-stationary environments. And we illustrate how these principles can be
used in linear bandits, bandit convex optimization, and reinforcement learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01037">SeisT: A foundational deep learning model for earthquake monitoring tasks. (arXiv:2310.01037v3 [physics.geo-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Li_S/0/1/0/all/0/1">Sen Li</a>, <a href="http://arxiv.org/find/physics/1/au:+Yang_X/0/1/0/all/0/1">Xu Yang</a>, <a href="http://arxiv.org/find/physics/1/au:+Cao_A/0/1/0/all/0/1">Anye Cao</a>, <a href="http://arxiv.org/find/physics/1/au:+Wang_C/0/1/0/all/0/1">Changbin Wang</a>, <a href="http://arxiv.org/find/physics/1/au:+Liu_Y/0/1/0/all/0/1">Yaoqi Liu</a>, <a href="http://arxiv.org/find/physics/1/au:+Liu_Y/0/1/0/all/0/1">Yapeng Liu</a>, <a href="http://arxiv.org/find/physics/1/au:+Niu_Q/0/1/0/all/0/1">Qiang Niu</a></p>
<p>Seismograms, the fundamental seismic records, have revolutionized earthquake
research and monitoring. Recent advancements in deep learning have further
enhanced seismic signal processing, leading to even more precise and effective
earthquake monitoring capabilities. This paper introduces a foundational deep
learning model, the Seismogram Transformer (SeisT), designed for a variety of
earthquake monitoring tasks. SeisT combines multiple modules tailored to
different tasks and exhibits impressive out-of-distribution generalization
performance, outperforming or matching state-of-the-art models in tasks like
earthquake detection, seismic phase picking, first-motion polarity
classification, magnitude estimation, back-azimuth estimation, and epicentral
distance estimation. The performance scores on the tasks are 0.96, 0.96, 0.68,
0.95, 0.86, 0.55, and 0.81, respectively. The most significant improvements, in
comparison to existing models, are observed in phase-P picking, phase-S
picking, and magnitude estimation, with gains of 1.7%, 9.5%, and 8.0%,
respectively. Our study, through rigorous experiments and evaluations, suggests
that SeisT has the potential to contribute to the advancement of seismic signal
processing and earthquake research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01569">Iterative Option Discovery for Planning, by Planning. (arXiv:2310.01569v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Young_K/0/1/0/all/0/1">Kenny Young</a>, <a href="http://arxiv.org/find/cs/1/au:+Sutton_R/0/1/0/all/0/1">Richard S. Sutton</a></p>
<p>Discovering useful temporal abstractions, in the form of options, is widely
thought to be key to applying reinforcement learning and planning to
increasingly complex domains. Building on the empirical success of the Expert
Iteration approach to policy learning used in AlphaZero, we propose Option
Iteration, an analogous approach to option discovery. Rather than learning a
single strong policy that is trained to match the search results everywhere,
Option Iteration learns a set of option policies trained such that for each
state encountered, at least one policy in the set matches the search results
for some horizon into the future. Intuitively, this may be significantly easier
as it allows the algorithm to hedge its bets compared to learning a single
globally strong policy, which may have complex dependencies on the details of
the current state. Having learned such a set of locally strong policies, we can
use them to guide the search algorithm resulting in a virtuous cycle where
better options lead to better search results which allows for training of
better options. We demonstrate experimentally that planning using options
learned with Option Iteration leads to a significant benefit in challenging
planning environments compared to an analogous planning algorithm operating in
the space of primitive actions and learning a single rollout policy with Expert
Iteration.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03165">Enhancing Accuracy in Deep Learning Using Random Matrix Theory. (arXiv:2310.03165v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Berlyand_L/0/1/0/all/0/1">Leonid Berlyand</a>, <a href="http://arxiv.org/find/cs/1/au:+Sandier_E/0/1/0/all/0/1">Etienne Sandier</a>, <a href="http://arxiv.org/find/cs/1/au:+Shmalo_Y/0/1/0/all/0/1">Yitzchak Shmalo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lei Zhang</a></p>
<p>We explore the applications of random matrix theory (RMT) in the training of
deep neural networks (DNNs), focusing on layer pruning that is reducing the
number of DNN parameters (weights). Our numerical results show that this
pruning leads to a drastic reduction of parameters while not reducing the
accuracy of DNNs and CNNs. Moreover, pruning the fully connected DNNs actually
increases the accuracy and decreases the variance for random initializations.
Our numerics indicate that this enhancement in accuracy is due to the
simplification of the loss landscape. We next provide rigorous mathematical
underpinning of these numerical results by proving the RMT-based Pruning
Theorem. Our results offer valuable insights into the practical application of
RMT for the creation of more efficient and accurate deep-learning models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.06150">Latent Diffusion Model for DNA Sequence Generation. (arXiv:2310.06150v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zehui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1">Yuhao Ni</a>, <a href="http://arxiv.org/find/cs/1/au:+Huygelen_T/0/1/0/all/0/1">Tim August B. Huygelen</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1">Akashaditya Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1">Guoxuan Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Stan_G/0/1/0/all/0/1">Guy-Bart Stan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yiren Zhao</a></p>
<p>The harnessing of machine learning, especially deep generative models, has
opened up promising avenues in the field of synthetic DNA sequence generation.
Whilst Generative Adversarial Networks (GANs) have gained traction for this
application, they often face issues such as limited sample diversity and mode
collapse. On the other hand, Diffusion Models are a promising new class of
generative models that are not burdened with these problems, enabling them to
reach the state-of-the-art in domains such as image generation. In light of
this, we propose a novel latent diffusion model, DiscDiff, tailored for
discrete DNA sequence generation. By simply embedding discrete DNA sequences
into a continuous latent space using an autoencoder, we are able to leverage
the powerful generative abilities of continuous diffusion models for the
generation of discrete data. Additionally, we introduce Fr\'echet
Reconstruction Distance (FReD) as a new metric to measure the sample quality of
DNA sequence generations. Our DiscDiff model demonstrates an ability to
generate synthetic DNA sequences that align closely with real DNA in terms of
Motif Distribution, Latent Embedding Distribution (FReD), and Chromatin
Profiles. Additionally, we contribute a comprehensive cross-species dataset of
150K unique promoter-gene sequences from 15 species, enriching resources for
future generative modelling in genomics. We will make our code public upon
publication.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08475">Can We Edit Multimodal Large Language Models?. (arXiv:2310.08475v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1">Siyuan Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_B/0/1/0/all/0/1">Bozhong Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qingbin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yongheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huajun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Ningyu Zhang</a></p>
<p>In this paper, we focus on editing Multimodal Large Language Models (MLLMs).
Compared to editing single-modal LLMs, multimodal model editing is more
challenging, which demands a higher level of scrutiny and careful consideration
in the editing process. To facilitate research in this area, we construct a new
benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite
of innovative metrics for evaluation. We conduct comprehensive experiments
involving various model editing baselines and analyze the impact of editing
different components for multimodal LLMs. Empirically, we notice that previous
baselines can implement editing multimodal LLMs to some extent, but the effect
is still barely satisfactory, indicating the potential difficulty of this task.
We hope that our work can provide the NLP community with insights. Code and
dataset are available in https://github.com/zjunlp/EasyEdit.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.11971">Improving Generalization of Alignment with Human Preferences through Group Invariant Learning. (arXiv:2310.11971v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1">Rui Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1">Wei Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1">Yuan Hua</a>, <a href="http://arxiv.org/find/cs/1/au:+Lai_W/0/1/0/all/0/1">Wenbin Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+Dou_S/0/1/0/all/0/1">Shihan Dou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yuhao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1">Zhiheng Xi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Haoran Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1">Tao Gui</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xuanjing Huang</a></p>
<p>The success of AI assistants based on language models (LLMs) hinges crucially
on Reinforcement Learning from Human Feedback (RLHF), which enables the
generation of responses more aligned with human preferences. As universal AI
assistants, there's a growing expectation for them to perform consistently
across various domains. However, previous work shows that Reinforcement
Learning (RL) often exploits shortcuts to attain high rewards and overlooks
challenging samples. This focus on quick reward gains undermines both the
stability in training and the model's ability to generalize to new, unseen
data. In this work, we propose a novel approach that can learn a consistent
policy via RL across various data groups or domains. Given the challenges
associated with acquiring group annotations, our method automatically
classifies data into different groups, deliberately maximizing performance
variance. Then, we optimize the policy to perform well on challenging groups.
Lastly, leveraging the established groups, our approach adaptively adjusts the
exploration space, allocating more learning capacity to more challenging data
and preventing the model from over-optimizing on simpler data. Experimental
results indicate that our approach significantly enhances training stability
and model generalization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16407">Information-Theoretic Generalization Analysis for Topology-aware Heterogeneous Federated Edge Learning over Noisy Channels. (arXiv:2310.16407v3 [cs.IT] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zheshun Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zenglin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Hongfang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jie Liu</a></p>
<p>With the rapid growth of edge intelligence, the deployment of federated
learning (FL) over wireless networks has garnered increasing attention, which
is called Federated Edge Learning (FEEL). In FEEL, both mobile devices
transmitting model parameters over noisy channels and collecting data in
diverse environments pose challenges to the generalization of trained models.
Moreover, devices can engage in decentralized FL via Device-to-Device
communication while the communication topology of connected devices also
impacts the generalization of models. Most recent theoretical studies overlook
the incorporation of all these effects into FEEL when developing generalization
analyses. In contrast, our work presents an information-theoretic
generalization analysis for topology-aware FEEL in the presence of data
heterogeneity and noisy channels. Additionally, we propose a novel
regularization method called Federated Global Mutual Information Reduction
(FedGMIR) to enhance the performance of models based on our analysis. Numerical
results validate our theoretical findings and provide evidence for the
effectiveness of the proposed method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16979">Unsupervised Domain Adaptation for Semantic Segmentation with Pseudo Label Self-Refinement. (arXiv:2310.16979v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xingchen Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Mithun_N/0/1/0/all/0/1">Niluthpol Chowdhury Mithun</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajvanshi_A/0/1/0/all/0/1">Abhinav Rajvanshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Chiu_H/0/1/0/all/0/1">Han-Pang Chiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Samarasekera_S/0/1/0/all/0/1">Supun Samarasekera</a></p>
<p>Deep learning-based solutions for semantic segmentation suffer from
significant performance degradation when tested on data with different
characteristics than what was used during the training. Adapting the models
using annotated data from the new domain is not always practical. Unsupervised
Domain Adaptation (UDA) approaches are crucial in deploying these models in the
actual operating conditions. Recent state-of-the-art (SOTA) UDA methods employ
a teacher-student self-training approach, where a teacher model is used to
generate pseudo-labels for the new data which in turn guide the training
process of the student model. Though this approach has seen a lot of success,
it suffers from the issue of noisy pseudo-labels being propagated in the
training process. To address this issue, we propose an auxiliary pseudo-label
refinement network (PRN) for online refining of the pseudo labels and also
localizing the pixels whose predicted labels are likely to be noisy. Being able
to improve the quality of pseudo labels and select highly reliable ones, PRN
helps self-training of segmentation models to be robust against pseudo label
noise propagation during different stages of adaptation. We evaluate our
approach on benchmark datasets with three different domain shifts, and our
approach consistently performs significantly better than the previous
state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01248">Multimodal and Force-Matched Imitation Learning with a See-Through Visuotactile Sensor. (arXiv:2311.01248v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ablett_T/0/1/0/all/0/1">Trevor Ablett</a>, <a href="http://arxiv.org/find/cs/1/au:+Limoyo_O/0/1/0/all/0/1">Oliver Limoyo</a>, <a href="http://arxiv.org/find/cs/1/au:+Sigal_A/0/1/0/all/0/1">Adam Sigal</a>, <a href="http://arxiv.org/find/cs/1/au:+Jilani_A/0/1/0/all/0/1">Affan Jilani</a>, <a href="http://arxiv.org/find/cs/1/au:+Kelly_J/0/1/0/all/0/1">Jonathan Kelly</a>, <a href="http://arxiv.org/find/cs/1/au:+Siddiqi_K/0/1/0/all/0/1">Kaleem Siddiqi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hogan_F/0/1/0/all/0/1">Francois Hogan</a>, <a href="http://arxiv.org/find/cs/1/au:+Dudek_G/0/1/0/all/0/1">Gregory Dudek</a></p>
<p>Kinesthetic Teaching is a popular approach to collecting expert robotic
demonstrations of contact-rich tasks for imitation learning (IL), but it
typically only measures motion, ignoring the force placed on the environment by
the robot. Furthermore, contact-rich tasks require accurate sensing of both
reaching and touching, which can be difficult to provide with conventional
sensing modalities. We address these challenges with a See-Through-your-Skin
(STS) visuotactile sensor, using the sensor both (i) as a measurement tool to
improve kinesthetic teaching, and (ii) as a policy input in contact-rich door
manipulation tasks. An STS sensor can be switched between visual and tactile
modes by leveraging a semi-transparent surface and controllable lighting,
allowing for both pre-contact visual sensing and during-contact tactile sensing
with a single sensor. First, we propose tactile force matching, a methodology
that enables a robot to match forces read during kinesthetic teaching using
tactile signals. Second, we develop a policy that controls STS mode switching,
allowing a policy to learn the appropriate moment to switch an STS from its
visual to its tactile mode. Finally, we study multiple observation
configurations to compare and contrast the value of visual and tactile data
from an STS with visual data from a wrist-mounted eye-in-hand camera. With over
3,000 test episodes from real-world manipulation experiments, we find that the
inclusion of force matching raises average policy success rates by 62.5%, STS
mode switching by 30.3%, and STS data as a policy input by 42.5%. Our results
highlight the utility of see-through tactile sensing for IL, both for data
collection to allow force matching, and for policy execution to allow accurate
task feedback.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01796">Learning to Augment Distributions for Out-of-Distribution Detection. (arXiv:2311.01796v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qizhou Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1">Zhen Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yonggang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1">Feng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yixuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1">Bo Han</a></p>
<p>Open-world classification systems should discern out-of-distribution (OOD)
data whose labels deviate from those of in-distribution (ID) cases, motivating
recent studies in OOD detection. Advanced works, despite their promising
progress, may still fail in the open world, owing to the lack of knowledge
about unseen OOD data in advance. Although one can access auxiliary OOD data
(distinct from unseen ones) for model training, it remains to analyze how such
auxiliary data will work in the open world. To this end, we delve into such a
problem from a learning theory perspective, finding that the distribution
discrepancy between the auxiliary and the unseen real OOD data is the key to
affecting the open-world detection performance. Accordingly, we propose
Distributional-Augmented OOD Learning (DAL), alleviating the OOD distribution
discrepancy by crafting an OOD distribution set that contains all distributions
in a Wasserstein ball centered on the auxiliary OOD distribution. We justify
that the predictor trained over the worst OOD data in the ball can shrink the
OOD distribution discrepancy, thus improving the open-world detection
performance given only the auxiliary OOD data. We conduct extensive evaluations
across representative OOD detection setups, demonstrating the superiority of
our DAL over its advanced counterparts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04945">Auto deep learning for bioacoustic signals. (arXiv:2311.04945v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tosato_G/0/1/0/all/0/1">Giulio Tosato</a>, <a href="http://arxiv.org/find/cs/1/au:+Shehata_A/0/1/0/all/0/1">Abdelrahman Shehata</a>, <a href="http://arxiv.org/find/cs/1/au:+Janssen_J/0/1/0/all/0/1">Joshua Janssen</a>, <a href="http://arxiv.org/find/cs/1/au:+Kamp_K/0/1/0/all/0/1">Kees Kamp</a>, <a href="http://arxiv.org/find/cs/1/au:+Jati_P/0/1/0/all/0/1">Pramatya Jati</a>, <a href="http://arxiv.org/find/cs/1/au:+Stowell_D/0/1/0/all/0/1">Dan Stowell</a></p>
<p>This study investigates the potential of automated deep learning to enhance
the accuracy and efficiency of multi-class classification of bird
vocalizations, compared against traditional manually-designed deep learning
models. Using the Western Mediterranean Wetland Birds dataset, we investigated
the use of AutoKeras, an automated machine learning framework, to automate
neural architecture search and hyperparameter tuning. Comparative analysis
validates our hypothesis that the AutoKeras-derived model consistently
outperforms traditional models like MobileNet, ResNet50 and VGG16. Our approach
and findings underscore the transformative potential of automated deep learning
for advancing bioacoustics research and models. In fact, the automated
techniques eliminate the need for manual feature engineering and model design
while improving performance. This study illuminates best practices in sampling,
evaluation and reporting to enhance reproducibility in this nascent field. All
the code used is available at https:
//github.com/giuliotosato/AutoKeras-bioacustic
</p>
<p>Keywords: AutoKeras; automated deep learning; audio classification; Wetlands
Bird dataset; comparative analysis; bioacoustics; validation dataset;
multi-class classification; spectrograms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.11056">Choose Your Simulator Wisely: A Review on Open-source Simulators for Autonomous Driving. (arXiv:2311.11056v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yueyuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1">Wei Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Songan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_W/0/1/0/all/0/1">Weihao Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Q/0/1/0/all/0/1">Qiyuan Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chunxiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Ming Yang</a></p>
<p>Simulators play a crucial role in autonomous driving, offering significant
time, cost, and labor savings. Over the past few years, the number of
simulators for autonomous driving has grown substantially. However, there is a
growing concern about the validity of algorithms developed and evaluated in
simulators, indicating a need for a thorough analysis of the development status
of the simulators.
</p>
<p>To bridge the gap in research, this paper analyzes the evolution of
simulators and explains how the functionalities and utilities have developed.
Then, the existing simulators are categorized based on their task
applicability, providing researchers with a taxonomy to swiftly assess a
simulator's suitability for specific tasks. Recommendations for select
simulators are presented, considering factors such as accessibility,
maintenance status, and quality. Recognizing potential hazards in simulators
that could impact the confidence of simulation experiments, the paper dedicates
substantial effort to identifying and justifying critical issues in actively
maintained open-source simulators. Moreover, the paper reviews potential
solutions to address these issues, serving as a guide for enhancing the
credibility of simulators.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.11167">Benchmarking Machine Learning Models for Quantum Error Correction. (arXiv:2311.11167v2 [quant-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Fu_T/0/1/0/all/0/1">Tim Fu</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Zhao_Y/0/1/0/all/0/1">Yue Zhao</a></p>
<p>Quantum Error Correction (QEC) is one of the fundamental problems in quantum
computer systems, which aims to detect and correct errors in the data qubits
within quantum computers. Due to the presence of unreliable data qubits in
existing quantum computers, implementing quantum error correction is a critical
step when establishing a stable quantum computer system. Recently, machine
learning (ML)-based approaches have been proposed to address this challenge.
However, they lack a thorough understanding of quantum error correction. To
bridge this research gap, we provide a new perspective to understand machine
learning-based QEC in this paper. We find that syndromes in the ancilla qubits
result from errors on connected data qubits, and distant ancilla qubits can
provide auxiliary information to rule out some incorrect predictions for the
data qubits. Therefore, to detect errors in data qubits, we must consider the
information present in the long-range ancilla qubits. To the best of our
knowledge, machine learning is less explored in the dependency relationship of
QEC. To fill the blank, we curate a machine learning benchmark to assess the
capacity to capture long-range dependencies for quantum error correction. To
provide a comprehensive evaluation, we evaluate seven state-of-the-art deep
learning algorithms spanning diverse neural network architectures, such as
convolutional neural networks, graph neural networks, and graph transformers.
Our exhaustive experiments reveal an enlightening trend: By enlarging the
receptive field to exploit information from distant ancilla qubits, the
accuracy of QEC significantly improves. For instance, U-Net can improve CNN by
a margin of about 50%. Finally, we provide a comprehensive analysis that could
inspire future research in this field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.11575">Testing multivariate normality by testing independence. (arXiv:2311.11575v2 [stat.ME] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Daniusis_P/0/1/0/all/0/1">Povilas Daniu&#x161;is</a></p>
<p>We propose a simple multivariate normality test based on Kac-Bernstein's
characterization, which can be conducted by utilising existing statistical
independence tests for sums and differences of data samples. We also perform
its empirical investigation, which reveals that for high-dimensional data, the
proposed approach may be more efficient than the alternative ones. The
accompanying code repository is provided at \url{https://shorturl.at/rtuy5}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12875">Nav-Q: Quantum Deep Reinforcement Learning for Collision-Free Navigation of Self-Driving Cars. (arXiv:2311.12875v2 [quant-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Sinha_A/0/1/0/all/0/1">Akash Sinha</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Macaluso_A/0/1/0/all/0/1">Antonio Macaluso</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Klusch_M/0/1/0/all/0/1">Matthias Klusch</a></p>
<p>The task of collision-free navigation (CFN) of self-driving cars is an
NP-hard problem usually tackled using Deep Reinforcement Learning (DRL). While
DRL methods have proven to be effective, their implementation requires
substantial computing resources and extended training periods to develop a
robust agent. On the other hand, quantum reinforcement learning has recently
demonstrated faster convergence and improved stability in simple,
non-real-world environments.
</p>
<p>In this work, we propose Nav-Q, the first quantum-supported DRL algorithm for
CFN of self-driving cars, that leverages quantum computation for improving the
training performance without the requirement for onboard quantum hardware.
Nav-Q is based on the actor-critic approach, where the critic is implemented
using a hybrid quantum-classical algorithm suitable for near-term quantum
devices. We assess the performance of Nav-Q using the CARLA driving simulator,
a de facto standard benchmark for evaluating state-of-the-art DRL methods. Our
empirical evaluations showcase that Nav-Q surpasses its classical counterpart
in terms of training stability and, in certain instances, with respect to the
convergence rate. Furthermore, we assess Nav-Q in relation to effective
dimension, unveiling that the incorporation of a quantum component results in a
model with greater descriptive power compared to classical baselines. Finally,
we evaluate the performance of Nav-Q using noisy quantum simulation, observing
that the quantum noise deteriorates the training performances but enhances the
exploratory tendencies of the agent during training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17842">Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning. (arXiv:2311.17842v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yingdong Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1">Fanqi Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_L/0/1/0/all/0/1">Li Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yang Gao</a></p>
<p>In this study, we are interested in imbuing robots with the capability of
physically-grounded task planning. Recent advancements have shown that large
language models (LLMs) possess extensive knowledge useful in robotic tasks,
especially in reasoning and planning. However, LLMs are constrained by their
lack of world grounding and dependence on external affordance models to
perceive environmental information, which cannot jointly reason with LLMs. We
argue that a task planner should be an inherently grounded, unified multimodal
system. To this end, we introduce Robotic Vision-Language Planning (ViLa), a
novel approach for long-horizon robotic planning that leverages vision-language
models (VLMs) to generate a sequence of actionable steps. ViLa directly
integrates perceptual data into its reasoning and planning process, enabling a
profound understanding of commonsense knowledge in the visual world, including
spatial layouts and object attributes. It also supports flexible multimodal
goal specification and naturally incorporates visual feedback. Our extensive
evaluation, conducted in both real-robot and simulated environments,
demonstrates ViLa's superiority over existing LLM-based planners, highlighting
its effectiveness in a wide array of open-world manipulation tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00645">Hashmarks: Privacy-Preserving Benchmarks for High-Stakes AI Evaluation. (arXiv:2312.00645v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bricman_P/0/1/0/all/0/1">Paul Bricman</a></p>
<p>There is a growing need to gain insight into language model capabilities that
relate to sensitive topics, such as bioterrorism or cyberwarfare. However,
traditional open source benchmarks are not fit for the task, due to the
associated practice of publishing the correct answers in human-readable form.
At the same time, enforcing mandatory closed-quarters evaluations might stifle
development and erode trust. In this context, we propose hashmarking, a
protocol for evaluating language models in the open without having to disclose
the correct answers. In its simplest form, a hashmark is a benchmark whose
reference solutions have been cryptographically hashed prior to publication.
Following an overview of the proposed evaluation protocol, we go on to assess
its resilience against traditional attack vectors (e.g. rainbow table attacks),
as well as against failure modes unique to increasingly capable generative
models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01678">Jellyfish: A Large Language Model for Data Preprocessing. (arXiv:2312.01678v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haochen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1">Yuyang Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Chuan Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Oyamada_M/0/1/0/all/0/1">Masafumi Oyamada</a></p>
<p>In this paper, we present Jellyfish, an open-source LLM as a universal task
solver for DP. Built on the Llama 2 13B model, Jellyfish is instruction-tuned
with the datasets of several typical DP tasks including error detection, data
imputation, schema matching, and entity matching, and delivers generalizability
to other tasks. Remarkably, Jellyfish can operate on a local, single, and
low-priced GPU with its 13 billion parameters, ensuring data security and
enabling further tuning. Its proficiency in understanding natural language
allows users to manually craft instructions for DP tasks. Unlike many existing
methods that heavily rely on prior knowledge, Jellyfish acquires domain
knowledge during its tuning process and integrates optional knowledge injection
during inference. A distinctive feature of Jellyfish is its interpreter, which
elucidates its output decisions. To construct Jellyfish, we develop a series of
pre-tuning and DP-tuning techniques. Jellyfish is equipped with an instance
serializer, which automatically translates raw data into model prompts, and a
knowledge injector, which optionally introduces task- and dataset-specific
knowledge to enhance DP performance. Our evaluation of Jellyfish, using a range
of real datasets, shows its competitiveness compared to state-of-the-art
methods and its strong generalizability to unseen tasks. Jellyfish's
performance rivals that of GPT series models, and its interpreter offers
enhanced reasoning capabilities compared to GPT-3.5. Furthermore, our
evaluation highlights the effectiveness of the techniques employed in
constructing Jellyfish. Our model is available at Hugging Face:
https://huggingface.co/NECOUDBFM/Jellyfish .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02646">SAMSGL: Series-Aligned Multi-Scale Graph Learning for Spatio-Temporal Forecasting. (arXiv:2312.02646v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1">Xiaobei Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1">Luolin Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1">Yang Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kurths_J/0/1/0/all/0/1">Jurgen Kurths</a></p>
<p>Spatio-temporal forecasting in various domains, like traffic prediction and
weather forecasting, is a challenging endeavor, primarily due to the
difficulties in modeling propagation dynamics and capturing high-dimensional
interactions among nodes. Despite the significant strides made by graph-based
networks in spatio-temporal forecasting, there remain two pivotal factors
closely related to forecasting performance that need further consideration:
time delays in propagation dynamics and multi-scale high-dimensional
interactions. In this work, we present a Series-Aligned Multi-Scale Graph
Learning (SAMSGL) framework, aiming to enhance forecasting performance. In
order to handle time delays in spatial interactions, we propose a
series-aligned graph convolution layer to facilitate the aggregation of
non-delayed graph signals, thereby mitigating the influence of time delays for
the improvement in accuracy. To understand global and local spatio-temporal
interactions, we develop a spatio-temporal architecture via multi-scale graph
learning, which encompasses two essential components: multi-scale graph
structure learning and graph-fully connected (Graph-FC) blocks. The multi-scale
graph structure learning includes a global graph structure to learn both
delayed and non-delayed node embeddings, as well as a local one to learn node
variations influenced by neighboring factors. The Graph-FC blocks
synergistically fuse spatial and temporal information to boost prediction
accuracy. To evaluate the performance of SAMSGL, we conduct experiments on
meteorological and traffic forecasting datasets, which demonstrate its
effectiveness and superiority.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03212">Constrained Bayesian Optimization Under Partial Observations: Balanced Improvements and Provable Convergence. (arXiv:2312.03212v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shengbo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Ke Li</a></p>
<p>The partially observable constrained optimization problems (POCOPs) impede
data-driven optimization techniques since an infeasible solution of POCOPs can
provide little information about the objective as well as the constraints. We
endeavor to design an efficient and provable method for expensive POCOPs under
the framework of constrained Bayesian optimization. Our method consists of two
key components. Firstly, we present an improved design of the acquisition
functions that introduces balanced exploration during optimization. We
rigorously study the convergence properties of this design to demonstrate its
effectiveness. Secondly, we propose a Gaussian process embedding different
likelihoods as the surrogate model for a partially observable constraint. This
model leads to a more accurate representation of the feasible regions compared
to traditional classification-based models. Our proposed method is empirically
studied on both synthetic and real-world problems. The results demonstrate the
competitiveness of our method for solving POCOPs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03325">FAGC:Feature Augmentation on Geodesic Curve in the Pre-Shape Space. (arXiv:2312.03325v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1">Yuexing Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_G/0/1/0/all/0/1">Guanxin Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bing Wang</a></p>
<p>Deep learning has yielded remarkable outcomes in various domains. However,
the challenge of requiring large-scale labeled samples still persists in deep
learning. Thus, data augmentation has been introduced as a critical strategy to
train deep learning models. However, data augmentation suffers from information
loss and poor performance in small sample environments. To overcome these
drawbacks, we propose a feature augmentation method based on shape space
theory, i.e., feature augmentation on Geodesic curve, called FAGC in
brevity.First, we extract features from the image with the neural network
model. Then, the multiple image features are projected into a pre-shape space
as features. In the pre-shape space, a Geodesic curve is built to fit the
features. Finally, the many generated features on the Geodesic curve are used
to train the various machine learning models. The FAGC module can be seamlessly
integrated with most machine learning methods. And the proposed method is
simple, effective and insensitive for the small sample datasets.Several
examples demonstrate that the FAGC method can greatly improve the performance
of the data preprocessing model in a small sample environment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05332">Bridging the Gaps: Learning Verifiable Model-Free Quadratic Programming Controllers Inspired by Model Predictive Control. (arXiv:2312.05332v2 [eess.SY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Lu_Y/0/1/0/all/0/1">Yiwen Lu</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1">Zishuo Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1">Yihan Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_N/0/1/0/all/0/1">Na Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Mo_Y/0/1/0/all/0/1">Yilin Mo</a></p>
<p>In this paper, we introduce a new class of parameterized controllers, drawing
inspiration from Model Predictive Control (MPC). The controller resembles a
Quadratic Programming (QP) solver of a linear MPC problem, with the parameters
of the controller being trained via Deep Reinforcement Learning (DRL) rather
than derived from system models. This approach addresses the limitations of
common controllers with Multi-Layer Perceptron (MLP) or other general neural
network architecture used in DRL, in terms of verifiability and performance
guarantees, and the learned controllers possess verifiable properties like
persistent feasibility and asymptotic stability akin to MPC. On the other hand,
numerical examples illustrate that the proposed controller empirically matches
MPC and MLP controllers in terms of control performance and has superior
robustness against modeling uncertainty and noises. Furthermore, the proposed
controller is significantly more computationally efficient compared to MPC and
requires fewer parameters to learn than MLP controllers. Real-world experiments
on vehicle drift maneuvering task demonstrate the potential of these
controllers for robotics and other demanding control tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05910">Ensemble Kalman Filtering-Aided Variational Inference for Gaussian Process State-Space Models. (arXiv:2312.05910v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zhidi Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yiyong Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_F/0/1/0/all/0/1">Feng Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Thiery_A/0/1/0/all/0/1">Alexandre Hoang Thi&#xe9;ry</a></p>
<p>Gaussian process state-space models (GPSSMs) are a flexible and principled
approach for modeling dynamical systems. However, existing variational learning
and inference methods for GPSSMs often necessitate optimizing a substantial
number of variational distribution parameters, leading to inadequate
performance and efficiency. To overcome this issue, we propose incorporating
the ensemble Kalman filter (EnKF), a well-established model-based filtering
technique, into the variational inference framework to approximate the
posterior distribution of latent states. This utilization of EnKF can
effectively exploit the dependencies between latent states and GP dynamics,
while eliminating the need for parameterizing the variational distribution,
thereby significantly reducing the number of variational parameters. Moreover,
we show that our proposed algorithm allows straightforward evaluation of an
approximated evidence lower bound (ELBO) in variational inference via simply
summating multiple terms with readily available closed-form solutions.
Leveraging automatic differentiation tools, we hence can maximize the ELBO and
train the GPSSM efficiently. We also extend the proposed algorithm to an online
setting and provide detailed algorithmic analyses and insights. Extensive
evaluation on diverse real and synthetic datasets demonstrates the superiority
of our EnKF-aided variational inference algorithms in terms of learning and
inference performance compared to existing methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06177">Randomized Physics-Informed Machine Learning for Uncertainty Quantification in High-Dimensional Inverse Problems. (arXiv:2312.06177v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zong_Y/0/1/0/all/0/1">Yifei Zong</a>, <a href="http://arxiv.org/find/cs/1/au:+Barajas_Solano_D/0/1/0/all/0/1">David Barajas-Solano</a>, <a href="http://arxiv.org/find/cs/1/au:+Tartakovsky_A/0/1/0/all/0/1">Alexandre M. Tartakovsky</a></p>
<p>We propose a physics-informed machine learning method for uncertainty
quantification in high-dimensional inverse problems. In this method, the states
and parameters of partial differential equations (PDEs) are approximated with
truncated conditional Karhunen-Lo\`eve expansions (CKLEs), which, by
construction, match the measurements of the respective variables. The maximum a
posteriori (MAP) solution of the inverse problem is formulated as a
minimization problem over CKLE coefficients where the loss function is the sum
of the norm of PDE residuals and the $\ell_2$ regularization term. This MAP
formulation is known as the physics-informed CKLE (PICKLE) method. Uncertainty
in the inverse solution is quantified in terms of the posterior distribution of
CKLE coefficients, and we sample the posterior by solving a randomized PICKLE
minimization problem, formulated by adding zero-mean Gaussian perturbations in
the PICKLE loss function. We call the proposed approach the randomized PICKLE
(rPICKLE) method.
</p>
<p>For linear and low-dimensional nonlinear problems (15 CKLE parameters), we
show analytically and through comparison with Hamiltonian Monte Carlo (HMC)
that the rPICKLE posterior converges to the true posterior given by the Bayes
rule. For high-dimensional non-linear problems with 2000 CKLE parameters, we
numerically demonstrate that rPICKLE posteriors are highly informative--they
provide mean estimates with an accuracy comparable to the estimates given by
the MAP solution and the confidence interval that mostly covers the reference
solution. We are not able to obtain the HMC posterior to validate rPICKLE's
convergence to the true posterior due to the HMC's prohibitive computational
cost for the considered high-dimensional problems. Our results demonstrate the
advantages of rPICKLE over HMC for approximately sampling high-dimensional
posterior distributions subject to physics constraints.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06199">Towards Transferable Adversarial Attacks with Centralized Perturbation. (arXiv:2312.06199v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Shangbo Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1">Yu-an Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yajie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1">Ruinan Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1">Wencong Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuanzhang Li</a></p>
<p>Adversarial transferability enables black-box attacks on unknown victim deep
neural networks (DNNs), rendering attacks viable in real-world scenarios.
Current transferable attacks create adversarial perturbation over the entire
image, resulting in excessive noise that overfit the source model.
Concentrating perturbation to dominant image regions that are model-agnostic is
crucial to improving adversarial efficacy. However, limiting perturbation to
local regions in the spatial domain proves inadequate in augmenting
transferability. To this end, we propose a transferable adversarial attack with
fine-grained perturbation optimization in the frequency domain, creating
centralized perturbation. We devise a systematic pipeline to dynamically
constrain perturbation optimization to dominant frequency coefficients. The
constraint is optimized in parallel at each iteration, ensuring the directional
alignment of perturbation optimization with model prediction. Our approach
allows us to centralize perturbation towards sample-specific important
frequency features, which are shared by DNNs, effectively mitigating source
model overfitting. Experiments demonstrate that by dynamically centralizing
perturbation on dominating frequency coefficients, crafted adversarial examples
exhibit stronger transferability, and allowing them to bypass various defenses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06353">Federated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes. (arXiv:2312.06353v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1">Zhen Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Daoyuan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_B/0/1/0/all/0/1">Bingchen Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1">Bolin Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yaliang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1">Shuiguang Deng</a></p>
<p>Pre-trained large language models (LLMs) require fine-tuning to improve their
responsiveness to natural language instructions. Federated learning (FL) offers
a way to perform fine-tuning using the abundant data on end devices without
compromising data privacy. Most existing federated fine-tuning methods for LLMs
rely on parameter-efficient fine-tuning techniques, which may not reach the
performance heights possible with full-parameter tuning. However, the
communication overhead associated with full-parameter tuning is prohibitively
high for both servers and clients. This work introduces FedKSeed, a novel
approach that employs zeroth-order optimization (ZOO) with a set of random
seeds. It enables federated full-parameter tuning of billion-sized LLMs
directly on devices. Our method significantly reduces transmission requirements
between the server and clients to just a few scalar gradients and random seeds,
amounting to only a few thousand bytes. Building on this, we develop a strategy
to assess the significance of ZOO perturbations for FL, allowing for
probability-differentiated seed sampling. This prioritizes perturbations that
have a greater impact on model accuracy. Experiments across six scenarios with
different LLMs, datasets and data partitions demonstrate that our approach
outperforms existing federated LLM fine-tuning methods in terms of both
communication efficiency and new task generalization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06625">Decoding Mean Field Games from Population and Environment Observations By Gaussian Processes. (arXiv:2312.06625v2 [cs.GT] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jinyan Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Mou_C/0/1/0/all/0/1">Chenchen Mou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xianjin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1">Chao Zhou</a></p>
<p>This paper presents a Gaussian Process (GP) framework, a non-parametric
technique widely acknowledged for regression and classification tasks, to
address inverse problems in mean field games (MFGs). By leveraging GPs, we aim
to recover agents' strategic actions and the environment's configurations from
partial and noisy observations of the population of agents and the setup of the
environment. Our method is a probabilistic tool to infer the behaviors of
agents in MFGs from data in scenarios where the comprehensive dataset is either
inaccessible or contaminated by noises.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06635">Gated Linear Attention Transformers with Hardware-Efficient Training. (arXiv:2312.06635v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Songlin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bailin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yikang Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1">Rameswar Panda</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Yoon Kim</a></p>
<p>Transformers with linear attention allow for efficient parallel training but
can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden
states, thus enjoying linear (with respect to output length) inference
complexity. Recent works such as RetNet (Sun et al., 2023) and TransNormerLLM
(Qin et al., 2023a) observe that adding a global decay term to the additive RNN
update rule greatly improves performance, sometimes outperforming standard
Transformers with softmax attention when trained at scale. In this work we show
that adding a data-dependent gating mechanism further improves performance. We
derive a parallel form of this gated linear attention layer that enables
efficient training. However, a straightforward, numerically stable
implementation of this parallel form requires generalized matrix
multiplications in log-space for numerical stability, and thus cannot take
advantage of tensor cores on modern GPUs which are optimized for standard
matrix multiplications. We develop a hardware-efficient version of the parallel
form that can still make use of tensor cores through block-parallel
computations over sequence chunks. Experiments on moderate-scale language
modeling (340M-parameter models trained on 15B tokens, 1.3B-parameter models
trained on 100B tokens) show that gated linear attention (GLA) Transformers
perform competitively against a strong LLaMA-architecture Transformer baseline
(Touvron et al., 2023) as well as Mamba (Gu &amp; Dao, 2023), a recently introduced
state-space model with a data-dependent state transition mechanism. For
training speed, our Triton-based implementation performs comparably to
CUDA-optimized FlashAttention-2 (Dao, 2023) under the regular 2048 training
length setting, while outperforming FlashAttention-2 when training on longer
sequences beyond 4096.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07331">Coupled Confusion Correction: Learning from Crowds with Sparse Annotations. (arXiv:2312.07331v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hansong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shikun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1">Dan Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1">Chenggang Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1">Shiming Ge</a></p>
<p>As the size of the datasets getting larger, accurately annotating such
datasets is becoming more impractical due to the expensiveness on both time and
economy. Therefore, crowd-sourcing has been widely adopted to alleviate the
cost of collecting labels, which also inevitably introduces label noise and
eventually degrades the performance of the model. To learn from crowd-sourcing
annotations, modeling the expertise of each annotator is a common but
challenging paradigm, because the annotations collected by crowd-sourcing are
usually highly-sparse. To alleviate this problem, we propose Coupled Confusion
Correction (CCC), where two models are simultaneously trained to correct the
confusion matrices learned by each other. Via bi-level optimization, the
confusion matrices learned by one model can be corrected by the distilled data
from the other. Moreover, we cluster the ``annotator groups'' who share similar
expertise so that their confusion matrices could be corrected together. In this
way, the expertise of the annotators, especially of those who provide seldom
labels, could be better captured. Remarkably, we point out that the annotation
sparsity not only means the average number of labels is low, but also there are
always some annotators who provide very few labels, which is neglected by
previous works when constructing synthetic crowd-sourcing annotations. Based on
that, we propose to use Beta distribution to control the generation of the
crowd-sourcing labels so that the synthetic annotations could be more
consistent with the real-world ones. Extensive experiments are conducted on two
types of synthetic datasets and three real-world datasets, the results of which
demonstrate that CCC significantly outperforms state-of-the-art approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07991">Accelerating the Global Aggregation of Local Explanations. (arXiv:2312.07991v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mor_A/0/1/0/all/0/1">Alon Mor</a>, <a href="http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1">Yonatan Belinkov</a>, <a href="http://arxiv.org/find/cs/1/au:+Kimelfeld_B/0/1/0/all/0/1">Benny Kimelfeld</a></p>
<p>Local explanation methods highlight the input tokens that have a considerable
impact on the outcome of classifying the document at hand. For example, the
Anchor algorithm applies a statistical analysis of the sensitivity of the
classifier to changes in the token. Aggregating local explanations over a
dataset provides a global explanation of the model. Such aggregation aims to
detect words with the most impact, giving valuable insights about the model,
like what it has learned in training and which adversarial examples expose its
weaknesses. However, standard aggregation methods bear a high computational
cost: a na\"ive implementation applies a costly algorithm to each token of each
document, and hence, it is infeasible for a simple user running in the scope of
a short analysis session. % We devise techniques for accelerating the global
aggregation of the Anchor algorithm. Specifically, our goal is to compute a set
of top-$k$ words with the highest global impact according to different
aggregation functions. Some of our techniques are lossless and some are lossy.
We show that for a very mild loss of quality, we are able to accelerate the
computation by up to 30$\times$, reducing the computation from hours to
minutes. We also devise and study a probabilistic model that accounts for noise
in the Anchor algorithm and diminishes the bias toward words that are frequent
yet low in impact.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08550">Harmonics of Learning: Universal Fourier Features Emerge in Invariant Networks. (arXiv:2312.08550v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Marchetti_G/0/1/0/all/0/1">Giovanni Luca Marchetti</a>, <a href="http://arxiv.org/find/cs/1/au:+Hillar_C/0/1/0/all/0/1">Christopher Hillar</a>, <a href="http://arxiv.org/find/cs/1/au:+Kragic_D/0/1/0/all/0/1">Danica Kragic</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanborn_S/0/1/0/all/0/1">Sophia Sanborn</a></p>
<p>In this work, we formally prove that, under certain conditions, if a neural
network is invariant to a finite group then its weights recover the Fourier
transform on that group. This provides a mathematical explanation for the
emergence of Fourier features -- a ubiquitous phenomenon in both biological and
artificial learning systems. The results hold even for non-commutative groups,
in which case the Fourier transform encodes all the irreducible unitary group
representations. Our findings have consequences for the problem of symmetry
discovery. Specifically, we demonstrate that the algebraic structure of an
unknown group can be recovered from the weights of a network that is at least
approximately invariant within certain bounds. Overall, this work contributes
to a foundation for an algebraic learning theory of invariant neural network
representations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08585">Unraveling Key Factors of Knowledge Distillation. (arXiv:2312.08585v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1">Jingxuan Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Linzhuang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1">Xu Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1">Bihui Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1">Ruifeng Guo</a></p>
<p>Knowledge distillation, a technique for model compression and performance
enhancement, has gained significant traction in Neural Machine Translation
(NMT). However, existing research primarily focuses on empirical applications,
and there is a lack of comprehensive understanding of how student model
capacity, data complexity, and decoding strategies collectively influence
distillation effectiveness. Addressing this gap, our study conducts an in-depth
investigation into these factors, particularly focusing on their interplay in
word-level and sequence-level distillation within NMT. Through extensive
experimentation across datasets like IWSLT13 En$\rightarrow$Fr, IWSLT14
En$\rightarrow$De, and others, we empirically validate hypotheses related to
the impact of these factors on knowledge distillation. Our research not only
elucidates the significant influence of model capacity, data complexity, and
decoding strategies on distillation effectiveness but also introduces a novel,
optimized distillation approach. This approach, when applied to the IWSLT14
de$\rightarrow$en translation task, achieves state-of-the-art performance,
demonstrating its practical efficacy in advancing the field of NMT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08749">Mitigating Label Bias in Machine Learning: Fairness through Confident Learning. (arXiv:2312.08749v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yixuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Boyu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1">Zenan Ling</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1">Feng Zhou</a></p>
<p>Discrimination can occur when the underlying unbiased labels are overwritten
by an agent with potential bias, resulting in biased datasets that unfairly
harm specific groups and cause classifiers to inherit these biases. In this
paper, we demonstrate that despite only having access to the biased labels, it
is possible to eliminate bias by filtering the fairest instances within the
framework of confident learning. In the context of confident learning, low
self-confidence usually indicates potential label errors; however, this is not
always the case. Instances, particularly those from underrepresented groups,
might exhibit low confidence scores for reasons other than labeling errors. To
address this limitation, our approach employs truncation of the confidence
score and extends the confidence interval of the probabilistic threshold.
Additionally, we incorporate with co-teaching paradigm for providing a more
robust and reliable selection of fair instances and effectively mitigating the
adverse effects of biased labels. Through extensive experimentation and
evaluation of various datasets, we demonstrate the efficacy of our approach in
promoting fairness and reducing the impact of label bias in machine learning
models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.09257">Brain-Inspired Machine Intelligence: A Survey of Neurobiologically-Plausible Credit Assignment. (arXiv:2312.09257v2 [cs.NE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ororbia_A/0/1/0/all/0/1">Alexander G. Ororbia</a></p>
<p>In this survey, we examine algorithms for conducting credit assignment in
artificial neural networks that are inspired or motivated by neurobiology.
These processes are unified under one possible taxonomy, which is constructed
based on how a learning algorithm answers a central question underpinning the
mechanisms of synaptic plasticity in complex adaptive neuronal systems: where
do the signals that drive the learning in individual elements of a network come
from and how are they produced? In this unified treatment, we organize the
ever-growing set of brain-inspired learning schemes into six general families
and consider these in the context of backpropagation of errors and its known
criticisms. The results of this review are meant to encourage future
developments in neuro-mimetic systems and their constituent learning processes,
wherein lies an important opportunity to build a strong bridge between machine
learning, computational neuroscience, and cognitive science.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.09821">Fragility, Robustness and Antifragility in Deep Learning. (arXiv:2312.09821v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pravin_C/0/1/0/all/0/1">Chandresh Pravin</a>, <a href="http://arxiv.org/find/cs/1/au:+Martino_I/0/1/0/all/0/1">Ivan Martino</a>, <a href="http://arxiv.org/find/cs/1/au:+Nicosia_G/0/1/0/all/0/1">Giuseppe Nicosia</a>, <a href="http://arxiv.org/find/cs/1/au:+Ojha_V/0/1/0/all/0/1">Varun Ojha</a></p>
<p>We propose a systematic analysis of deep neural networks (DNNs) based on a
signal processing technique for network parameter removal, in the form of
synaptic filters that identifies the fragility, robustness and antifragility
characteristics of DNN parameters. Our proposed analysis investigates if the
DNN performance is impacted negatively, invariantly, or positively on both
clean and adversarially perturbed test datasets when the DNN undergoes synaptic
filtering. We define three \textit{filtering scores} for quantifying the
fragility, robustness and antifragility characteristics of DNN parameters based
on the performances for (i) clean dataset, (ii) adversarial dataset, and (iii)
the difference in performances of clean and adversarial datasets. We validate
the proposed systematic analysis on ResNet-18, ResNet-50, SqueezeNet-v1.1 and
ShuffleNet V2 x1.0 network architectures for MNIST, CIFAR10 and Tiny ImageNet
datasets. The filtering scores, for a given network architecture, identify
network parameters that are invariant in characteristics across different
datasets over learning epochs. Vice-versa, for a given dataset, the filtering
scores identify the parameters that are invariant in characteristics across
different network architectures. We show that our synaptic filtering method
improves the test accuracy of ResNet and ShuffleNet models on adversarial
datasets when only the robust and antifragile parameters are selectively
retrained at any given epoch, thus demonstrating applications of the proposed
strategy in improving model robustness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10104">ICD-LM: Configuring Vision-Language In-Context Demonstrations by Language Modeling. (arXiv:2312.10104v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1">Yingzhe Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1">Haoxuan Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Shuo Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1">Yucheng Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hanwang Zhang</a></p>
<p>This paper studies how to configure powerful In-Context Demonstration (ICD)
sequences for a Large Vision-Language Model (LVLM) to solve Vision-Language
tasks through In-Context Learning (ICL). After observing that configuring an
ICD sequence is a mirror process of composing a sentence, i.e., just as a
sentence can be composed word by word via a Language Model, an ICD sequence can
also be configured one by one. Consequently, we introduce an ICD Language Model
(ICD-LM) specifically designed to generate effective ICD sequences. This
involves creating a dataset of hand-crafted ICD sequences for various query
samples and using it to train the ICD-LM. Our approach, diverging from
traditional methods in NLP that select and order ICDs separately, enables to
simultaneously learn how to select and order ICDs, enhancing the effect of the
sequences. Moreover, during data construction, we use the LVLM intended for ICL
implementation to validate the strength of each ICD sequence, resulting in a
model-specific dataset and the ICD-LM trained by this dataset is also
model-specific. We validate our methodology through experiments in Visual
Question Answering and Image Captioning, confirming the viability of using a
Language Model for ICD configuration. Our comprehensive ablation studies
further explore the impact of various dataset construction and ICD-LM
development settings on the outcomes. The code is given in
https://github.com/ForJadeForest/ICD-LM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10385">Imitate the Good and Avoid the Bad: An Incremental Approach to Safe Reinforcement Learning. (arXiv:2312.10385v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hoang_H/0/1/0/all/0/1">Huy Hoang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mai_T/0/1/0/all/0/1">Tien Mai</a>, <a href="http://arxiv.org/find/cs/1/au:+Varakantham_P/0/1/0/all/0/1">Pradeep Varakantham</a></p>
<p>A popular framework for enforcing safe actions in Reinforcement Learning (RL)
is Constrained RL, where trajectory based constraints on expected cost (or
other cost measures) are employed to enforce safety and more importantly these
constraints are enforced while maximizing expected reward. Most recent
approaches for solving Constrained RL convert the trajectory based cost
constraint into a surrogate problem that can be solved using minor
modifications to RL methods. A key drawback with such approaches is an over or
underestimation of the cost constraint at each state. Therefore, we provide an
approach that does not modify the trajectory based cost constraint and instead
imitates ``good'' trajectories and avoids ``bad'' trajectories generated from
incrementally improving policies. We employ an oracle that utilizes a reward
threshold (which is varied with learning) and the overall cost constraint to
label trajectories as ``good'' or ``bad''. A key advantage of our approach is
that we are able to work from any starting policy or set of trajectories and
improve on it. In an exhaustive set of experiments, we demonstrate that our
approach is able to outperform top benchmark approaches for solving Constrained
RL problems, with respect to expected cost, CVaR cost, or even unknown cost
constraints.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10407">DeepArt: A Benchmark to Advance Fidelity Research in AI-Generated Content. (arXiv:2312.10407v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wentao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xuanyao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tianyang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1">Swalpa Kumar Roy</a></p>
<p>This paper explores the image synthesis capabilities of GPT-4, a leading
multi-modal large language model. We establish a benchmark for evaluating the
fidelity of texture features in images generated by GPT-4, comprising manually
painted pictures and their AI-generated counterparts. The contributions of this
study are threefold: First, we provide an in-depth analysis of the fidelity of
image synthesis features based on GPT-4, marking the first such study on this
state-of-the-art model. Second, the quantitative and qualitative experiments
fully reveals the limitations of the GPT-4 model in image synthesis. Third, we
have compiled a unique benchmark of manual drawings and corresponding
GPT-4-generated images, introducing a new task to advance fidelity research in
AI-generated content (AIGC). The dataset is available at:
\url{https://github.com/rickwang28574/DeepArt}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10587">E2E-AT: A Unified Framework for Tackling Uncertainty in Task-aware End-to-end Learning. (arXiv:2312.10587v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Wangkun Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jianhong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Teng_F/0/1/0/all/0/1">Fei Teng</a></p>
<p>Successful machine learning involves a complete pipeline of data, model, and
downstream applications. Instead of treating them separately, there has been a
prominent increase of attention within the constrained optimization (CO) and
machine learning (ML) communities towards combining prediction and optimization
models. The so-called end-to-end (E2E) learning captures the task-based
objective for which they will be used for decision making. Although a large
variety of E2E algorithms have been presented, it has not been fully
investigated how to systematically address uncertainties involved in such
models. Most of the existing work considers the uncertainties of ML in the
input space and improves robustness through adversarial training. We extend
this idea to E2E learning and prove that there is a robustness certification
procedure by solving augmented integer programming. Furthermore, we show that
neglecting the uncertainty of COs during training causes a new trigger for
generalization errors. To include all these components, we propose a unified
framework that covers the uncertainties emerging in both the input feature
space of the ML models and the COs. The framework is described as a robust
optimization problem and is practically solved via end-to-end adversarial
training (E2E-AT). Finally, the performance of E2E-AT is evaluated by a
real-world end-to-end power system operation problem, including load
forecasting and sequential scheduling tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10943">Model Stealing Attack against Graph Classification with Authenticity, Uncertainty and Diversity. (arXiv:2312.10943v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Zhihao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1">Chenwang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1">Rui Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lian_D/0/1/0/all/0/1">Defu Lian</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1">Enhong Chen</a></p>
<p>Recent research demonstrates that GNNs are vulnerable to the model stealing
attack, a nefarious endeavor geared towards duplicating the target model via
query permissions. However, they mainly focus on node classification tasks,
neglecting the potential threats entailed within the domain of graph
classification tasks. Furthermore, their practicality is questionable due to
unreasonable assumptions, specifically concerning the large data requirements
and extensive model knowledge. To this end, we advocate following strict
settings with limited real data and hard-label awareness to generate synthetic
data, thereby facilitating the stealing of the target model. Specifically,
following important data generation principles, we introduce three model
stealing attacks to adapt to different actual scenarios: MSA-AU is inspired by
active learning and emphasizes the uncertainty to enhance query value of
generated samples; MSA-AD introduces diversity based on Mixup augmentation
strategy to alleviate the query inefficiency issue caused by over-similar
samples generated by MSA-AU; MSA-AUD combines the above two strategies to
seamlessly integrate the authenticity, uncertainty, and diversity of the
generated samples. Finally, extensive experiments consistently demonstrate the
superiority of the proposed methods in terms of concealment, query efficiency,
and stealing performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11562">A Survey of Reasoning with Foundation Models. (arXiv:2312.11562v4 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jiankai Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1">Chuanyang Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1">Enze Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhengying Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_R/0/1/0/all/0/1">Ruihang Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1">Jianing Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jiaqi Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1">Mingyu Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hongyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Geng_M/0/1/0/all/0/1">Mengzhe Geng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yue Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenhai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Junsong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1">Zhangyue Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1">Xiaozhe Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1">Jie Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Junxian He</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1">Wu Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xihui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1">Hao Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1">Yu Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Ming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1">Pheng Ann Heng</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1">Jifeng Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1">Ping Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jingdong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1">Ji-Rong Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1">Xipeng Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yike Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1">Hui Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhenguo Li</a></p>
<p>Reasoning, a crucial ability for complex problem-solving, plays a pivotal
role in various real-world settings such as negotiation, medical diagnosis, and
criminal investigation. It serves as a fundamental methodology in the field of
Artificial General Intelligence (AGI). With the ongoing development of
foundation models, there is a growing interest in exploring their abilities in
reasoning tasks. In this paper, we introduce seminal foundation models proposed
or adaptable for reasoning, highlighting the latest advancements in various
reasoning tasks, methods, and benchmarks. We then delve into the potential
future directions behind the emergence of reasoning abilities within foundation
models. We also discuss the relevance of multimodal learning, autonomous
agents, and super alignment in the context of reasoning. By discussing these
future research directions, we hope to inspire researchers in their exploration
of this field, stimulate further advancements in reasoning with foundation
models, and contribute to the development of AGI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11571">Model Stealing Attack against Recommender System. (arXiv:2312.11571v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Zhihao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1">Rui Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1">Chenwang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lian_D/0/1/0/all/0/1">Defu Lian</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1">Enhong Chen</a></p>
<p>Recent studies have demonstrated the vulnerability of recommender systems to
data privacy attacks. However, research on the threat to model privacy in
recommender systems, such as model stealing attacks, is still in its infancy.
Some adversarial attacks have achieved model stealing attacks against
recommender systems, to some extent, by collecting abundant training data of
the target model (target data) or making a mass of queries. In this paper, we
constrain the volume of available target data and queries and utilize auxiliary
data, which shares the item set with the target data, to promote model stealing
attacks. Although the target model treats target and auxiliary data
differently, their similar behavior patterns allow them to be fused using an
attention mechanism to assist attacks. Besides, we design stealing functions to
effectively extract the recommendation list obtained by querying the target
model. Experimental results show that the proposed methods are applicable to
most recommender systems and various scenarios and exhibit excellent attack
performance on multiple datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11583">AI-Based Energy Transportation Safety: Pipeline Radial Threat Estimation Using Intelligent Sensing System. (arXiv:2312.11583v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1">Chengyuan Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yiyuan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1">Kaixiang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haifeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1">Qinmin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">C. L. Philip Chen</a></p>
<p>The application of artificial intelligence technology has greatly enhanced
and fortified the safety of energy pipelines, particularly in safeguarding
against external threats. The predominant methods involve the integration of
intelligent sensors to detect external vibration, enabling the identification
of event types and locations, thereby replacing manual detection methods.
However, practical implementation has exposed a limitation in current methods -
their constrained ability to accurately discern the spatial dimensions of
external signals, which complicates the authentication of threat events. Our
research endeavors to overcome the above issues by harnessing deep learning
techniques to achieve a more fine-grained recognition and localization process.
This refinement is crucial in effectively identifying genuine threats to
pipelines, thus enhancing the safety of energy transportation. This paper
proposes a radial threat estimation method for energy pipelines based on
distributed optical fiber sensing technology. Specifically, we introduce a
continuous multi-view and multi-domain feature fusion methodology to extract
comprehensive signal features and construct a threat estimation and recognition
network. The utilization of collected acoustic signal data is optimized, and
the underlying principle is elucidated. Moreover, we incorporate the concept of
transfer learning through a pre-trained model, enhancing both recognition
accuracy and training efficiency. Empirical evidence gathered from real-world
scenarios underscores the efficacy of our method, notably in its substantial
reduction of false alarms and remarkable gains in recognition accuracy. More
generally, our method exhibits versatility and can be extrapolated to a broader
spectrum of recognition tasks and scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12223">Self-Supervised Detection of Perfect and Partial Input-Dependent Symmetries. (arXiv:2312.12223v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Urbano_A/0/1/0/all/0/1">Alonso Urbano</a>, <a href="http://arxiv.org/find/cs/1/au:+Romero_D/0/1/0/all/0/1">David W. Romero</a></p>
<p>Group equivariance ensures consistent responses to group transformations of
the input, leading to more robust models and enhanced generalization
capabilities. However, this property can lead to overly constrained models if
the symmetries considered in the group differ from those observed in data.
While common methods address this by determining the appropriate level of
symmetry at the dataset level, they are limited to supervised settings and
ignore scenarios in which multiple levels of symmetry co-exist in the same
dataset. For instance, pictures of cars and planes exhibit different levels of
rotation, yet both are included in the CIFAR-10 dataset. In this paper, we
propose a method able to detect the level of symmetry of each input without the
need for labels. To this end, we derive a sufficient and necessary condition to
learn the distribution of symmetries in the data. Using the learned
distribution, we generate pseudo-labels that allow us to learn the levels of
symmetry of each input in a self-supervised manner. We validate the
effectiveness of our approach on synthetic datasets with different per-class
levels of symmetries e.g. MNISTMultiple, in which digits are uniformly rotated
within a class-dependent interval. We demonstrate that our method can be used
for practical applications such as the generation of standardized datasets in
which the symmetries are not present, as well as the detection of
out-of-distribution symmetries during inference. By doing so, both the
generalization and robustness of non-equivariant models can be improved. Our
code is publicly available at https://github.com/aurban0/ssl-sym.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12564">Leading the Pack: N-player Opponent Shaping. (arXiv:2312.12564v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Souly_A/0/1/0/all/0/1">Alexandra Souly</a>, <a href="http://arxiv.org/find/cs/1/au:+Willi_T/0/1/0/all/0/1">Timon Willi</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1">Akbir Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kirk_R/0/1/0/all/0/1">Robert Kirk</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1">Chris Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Grefenstette_E/0/1/0/all/0/1">Edward Grefenstette</a>, <a href="http://arxiv.org/find/cs/1/au:+Rocktaschel_T/0/1/0/all/0/1">Tim Rockt&#xe4;schel</a></p>
<p>Reinforcement learning solutions have great success in the 2-player general
sum setting. In this setting, the paradigm of Opponent Shaping (OS), in which
agents account for the learning of their co-players, has led to agents which
are able to avoid collectively bad outcomes, whilst also maximizing their
reward. These methods have currently been limited to 2-player game. However,
the real world involves interactions with many more agents, with interactions
on both local and global scales. In this paper, we extend Opponent Shaping (OS)
methods to environments involving multiple co-players and multiple shaping
agents. We evaluate on over 4 different environments, varying the number of
players from 3 to 5, and demonstrate that model-based OS methods converge to
equilibrium with better global welfare than naive learning. However, we find
that when playing with a large number of co-players, OS methods' relative
performance reduces, suggesting that in the limit OS methods may not perform
well. Finally, we explore scenarios where more than one OS method is present,
noticing that within games requiring a majority of cooperating agents, OS
methods converge to outcomes with poor global welfare.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12715">Learning Performance Maximizing Ensembles with Explainability Guarantees. (arXiv:2312.12715v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Pisztora_V/0/1/0/all/0/1">Vincent Pisztora</a>, <a href="http://arxiv.org/find/stat/1/au:+Li_J/0/1/0/all/0/1">Jia Li</a></p>
<p>In this paper we propose a method for the optimal allocation of observations
between an intrinsically explainable glass box model and a black box model. An
optimal allocation being defined as one which, for any given explainability
level (i.e. the proportion of observations for which the explainable model is
the prediction function), maximizes the performance of the ensemble on the
underlying task, and maximizes performance of the explainable model on the
observations allocated to it, subject to the maximal ensemble performance
condition. The proposed method is shown to produce such explainability optimal
allocations on a benchmark suite of tabular datasets across a variety of
explainable and black box model types. These learned allocations are found to
consistently maintain ensemble performance at very high explainability levels
(explaining $74\%$ of observations on average), and in some cases even
outperforming both the component explainable and black box models while
improving explainability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13555">CR-SAM: Curvature Regularized Sharpness-Aware Minimization. (arXiv:2312.13555v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1">Tao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1">Tie Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wunsch_D/0/1/0/all/0/1">Donald C. Wunsch</a></p>
<p>The capacity to generalize to future unseen data stands as one of the utmost
crucial attributes of deep neural networks. Sharpness-Aware Minimization (SAM)
aims to enhance the generalizability by minimizing worst-case loss using
one-step gradient ascent as an approximation. However, as training progresses,
the non-linearity of the loss landscape increases, rendering one-step gradient
ascent less effective. On the other hand, multi-step gradient ascent will incur
higher training cost. In this paper, we introduce a normalized Hessian trace to
accurately measure the curvature of loss landscape on {\em both} training and
test sets. In particular, to counter excessive non-linearity of loss landscape,
we propose Curvature Regularized SAM (CR-SAM), integrating the normalized
Hessian trace as a SAM regularizer. Additionally, we present an efficient way
to compute the trace via finite differences with parallelism. Our theoretical
analysis based on PAC-Bayes bounds establishes the regularizer's efficacy in
reducing generalization error. Empirical evaluation on CIFAR and ImageNet
datasets shows that CR-SAM consistently enhances classification performance for
ResNet and Vision Transformer (ViT) models across various datasets. Our code is
available at https://github.com/TrustAIoT/CR-SAM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13772">On Task Performance and Model Calibration with Supervised and Self-Ensembled In-Context Learning. (arXiv:2312.13772v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chengzu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Han Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1">Goran Glava&#x161;</a>, <a href="http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1">Anna Korhonen</a>, <a href="http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1">Ivan Vuli&#x107;</a></p>
<p>Following the standard supervised fine-tuning (SFT) paradigm, in-context
learning (ICL) has become an efficient approach propelled by the recent
advancements in large language models (LLMs), yielding promising performance
across various tasks in few-shot data setups. However, both paradigms are prone
to suffer from the critical problem of overconfidence (i.e., miscalibration),
especially in such limited data setups. In this work, we deliver an in-depth
analysis of the behavior across different choices of learning methods from the
perspective of both performance and calibration, as well as their interplay.
Through extensive controlled experiments, we find that simultaneous gains for
both task performance and calibration are difficult to achieve, and the problem
of miscalibration exists across all learning methods in low-resource scenarios.
To address this challenging trade-off between performance and calibration, we
then investigate the potential of self-ensembling techniques applied at
different modeling stages (e.g., variations of in-context examples or
variations in prompts or different ensembling strategies). We justify the
feasibility of self-ensembling on SFT in addition to ICL, to make the
predictions more calibrated and have comparable or even better performance. Our
work sheds light on which learning paradigm to choose and how to enhance both
task performance and calibration of LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13923">Fed-CO2: Cooperation of Online and Offline Models for Severe Data Heterogeneity in Federated Learning. (arXiv:2312.13923v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1">Zhongyi Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Ye Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1">Wei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jingya Wang</a></p>
<p>Federated Learning (FL) has emerged as a promising distributed learning
paradigm that enables multiple clients to learn a global model collaboratively
without sharing their private data. However, the effectiveness of FL is highly
dependent on the quality of the data that is being used for training. In
particular, data heterogeneity issues, such as label distribution skew and
feature skew, can significantly impact the performance of FL. Previous studies
in FL have primarily focused on addressing label distribution skew data
heterogeneity, while only a few recent works have made initial progress in
tackling feature skew issues. Notably, these two forms of data heterogeneity
have been studied separately and have not been well explored within a unified
FL framework. To address this gap, we propose Fed-CO$_{2}$, a universal FL
framework that handles both label distribution skew and feature skew within a
\textbf{C}ooperation mechanism between the \textbf{O}nline and \textbf{O}ffline
models. Specifically, the online model learns general knowledge that is shared
among all clients, while the offline model is trained locally to learn the
specialized knowledge of each individual client. To further enhance model
cooperation in the presence of feature shifts, we design an intra-client
knowledge transfer mechanism that reinforces mutual learning between the online
and offline models, and an inter-client knowledge transfer mechanism to
increase the models' domain generalization ability. Extensive experiments show
that our Fed-CO$_{2}$ outperforms a wide range of existing personalized
federated learning algorithms in terms of handling label distribution skew and
feature skew, both individually and collectively. The empirical results are
supported by our convergence analyses in a simplified setting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13933">Structured Probabilistic Coding. (arXiv:2312.13933v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1">Dou Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1">Lingwei Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yaxin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1">Wei Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1">Songlin Hu</a></p>
<p>This paper presents a new supervised representation learning framework,
namely structured probabilistic coding (SPC), to learn compact and informative
representations from input related to the target task. SPC is an encoder-only
probabilistic coding technology with a structured regularization from the
target label space. It can enhance the generalization ability of pre-trained
language models for better language understanding. Specifically, our
probabilistic coding technology simultaneously performs information encoding
and task prediction in one module to more fully utilize the effective
information from input data. It uses variational inference in the output space
to reduce randomness and uncertainty. Besides, to better control the
probability distribution in the latent space, a structured regularization is
proposed to promote class-level uniformity in the latent space. With the
regularization term, SPC can preserve the Gaussian distribution structure of
latent code as well as better cover the hidden space with class uniformly.
Experimental results on 12 natural language understanding tasks demonstrate
that our SPC effectively improves the performance of pre-trained language
models for classification and regression. Extensive experiments show that SPC
can enhance the generalization capability, robustness to label noise, and
clustering quality of output representations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14222">Hierarchical Topology Isomorphism Expertise Embedded Graph Contrastive Learning. (arXiv:2312.14222v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiangmeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1">Yifan Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1">Hang Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiang_W/0/1/0/all/0/1">Wenwen Qiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1">Changwen Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1">Fuchun Sun</a></p>
<p>Graph contrastive learning (GCL) aims to align the positive features while
differentiating the negative features in the latent space by minimizing a
pair-wise contrastive loss. As the embodiment of an outstanding discriminative
unsupervised graph representation learning approach, GCL achieves impressive
successes in various graph benchmarks. However, such an approach falls short of
recognizing the topology isomorphism of graphs, resulting in that graphs with
relatively homogeneous node features cannot be sufficiently discriminated. By
revisiting classic graph topology recognition works, we disclose that the
corresponding expertise intuitively complements GCL methods. To this end, we
propose a novel hierarchical topology isomorphism expertise embedded graph
contrastive learning, which introduces knowledge distillations to empower GCL
models to learn the hierarchical topology isomorphism expertise, including the
graph-tier and subgraph-tier. On top of this, the proposed method holds the
feature of plug-and-play, and we empirically demonstrate that the proposed
method is universal to multiple state-of-the-art GCL models. The solid
theoretical analyses are further provided to prove that compared with
conventional GCL methods, our method acquires the tighter upper bound of Bayes
classification error. We conduct extensive experiments on real-world benchmarks
to exhibit the performance superiority of our method over candidate GCL
methods, e.g., for the real-world graph representation learning experiments,
the proposed method beats the state-of-the-art method by 0.23% on unsupervised
representation learning setting, 0.43% on transfer learning setting. Our code
is available at https://github.com/jyf123/HTML.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14507">Unsupervised Harmonic Parameter Estimation Using Differentiable DSP and Spectral Optimal Transport. (arXiv:2312.14507v2 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Torres_B/0/1/0/all/0/1">Bernardo Torres</a> (S2A, IDS, LTCI), <a href="http://arxiv.org/find/cs/1/au:+Peeters_G/0/1/0/all/0/1">Geoffroy Peeters</a> (S2A, IDS, LTCI), <a href="http://arxiv.org/find/cs/1/au:+Richard_G/0/1/0/all/0/1">Ga&#xeb;l Richard</a> (S2A, IDS, LTCI)</p>
<p>In neural audio signal processing, pitch conditioning has been used to
enhance the performance of synthesizers. However, jointly training pitch
estimators and synthesizers is a challenge when using standard audio-to-audio
reconstruction loss, leading to reliance on external pitch trackers. To address
this issue, we propose using a spectral loss function inspired by optimal
transportation theory that minimizes the displacement of spectral energy. We
validate this approach through an unsupervised autoencoding task that fits a
harmonic template to harmonic signals. We jointly estimate the fundamental
frequency and amplitudes of harmonics using a lightweight encoder and
reconstruct the signals using a differentiable harmonic synthesizer. The
proposed approach offers a promising direction for improving unsupervised
parameter estimation in neural audio applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14769">Large Language Model (LLM) Bias Index -- LLMBI. (arXiv:2312.14769v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oketunji_A/0/1/0/all/0/1">Abiodun Finbarrs Oketunji</a>, <a href="http://arxiv.org/find/cs/1/au:+Anas_M/0/1/0/all/0/1">Muhammad Anas</a>, <a href="http://arxiv.org/find/cs/1/au:+Saina_D/0/1/0/all/0/1">Deepthi Saina</a></p>
<p>The Large Language Model Bias Index (LLMBI) is a pioneering approach designed
to quantify and address biases inherent in large language models (LLMs), such
as GPT-4. We recognise the increasing prevalence and impact of LLMs across
diverse sectors. This research introduces a novel metric, LLMBI, to
systematically measure and mitigate biases potentially skewing model responses.
We formulated LLMBI using a composite scoring system incorporating multiple
dimensions of bias, including but not limited to age, gender, and racial
biases.
</p>
<p>To operationalise this metric, we engaged in a multi-step process involving
collecting and annotating LLM responses, applying sophisticated Natural
Language Processing (NLP) techniques for bias detection, and computing the
LLMBI score through a specially crafted mathematical formula. The formula
integrates weighted averages of various bias dimensions, a penalty for dataset
diversity deficiencies, and a correction for sentiment biases. Our empirical
analysis, conducted using responses from OpenAI's API, employs advanced
sentiment analysis as a representative method for bias detection.
</p>
<p>The research reveals LLMs, whilst demonstrating impressive capabilities in
text generation, exhibit varying degrees of bias across different dimensions.
LLMBI provides a quantifiable measure to compare biases across models and over
time, offering a vital tool for systems engineers, researchers and regulators
in enhancing the fairness and reliability of LLMs. It highlights the potential
of LLMs in mimicking unbiased human-like responses. Additionally, it
underscores the necessity of continuously monitoring and recalibrating such
models to align with evolving societal norms and ethical standards.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14847">Large Scale Training of Graph Neural Networks for Optimal Markov-Chain Partitioning Using the Kemeny Constant. (arXiv:2312.14847v2 [physics.bio-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Martino_S/0/1/0/all/0/1">Sam Alexander Martino</a>, <a href="http://arxiv.org/find/physics/1/au:+Morado_J/0/1/0/all/0/1">Jo&#xe3;o Morado</a>, <a href="http://arxiv.org/find/physics/1/au:+Li_C/0/1/0/all/0/1">Chenghao Li</a>, <a href="http://arxiv.org/find/physics/1/au:+Lu_Z/0/1/0/all/0/1">Zhenghao Lu</a>, <a href="http://arxiv.org/find/physics/1/au:+Rosta_E/0/1/0/all/0/1">Edina Rosta</a></p>
<p>Traditional clustering algorithms often struggle to capture the complex
relationships within graphs and generalise to arbitrary clustering criteria.
The emergence of graph neural networks (GNNs) as a powerful framework for
learning representations of graph data provides new approaches to solving the
problem. Previous work has shown GNNs to be capable of proposing partitionings
using a variety of criteria, however, these approaches have not yet been
extended to work on Markov chains or kinetic networks. These arise frequently
in the study of molecular systems and are of particular interest to the
biochemical modelling community. In this work, we propose several GNN-based
architectures to tackle the graph partitioning problem for Markov Chains
described as kinetic networks. This approach aims to minimize how much a
proposed partitioning changes the Kemeny constant. We propose using an
encoder-decoder architecture and show how simple GraphSAGE-based GNNs with
linear layers can outperform much larger and more expressive attention-based
models in this context. As a proof of concept, we first demonstrate the
method's ability to cluster randomly connected graphs. We also use a linear
chain architecture corresponding to a 1D free energy profile as our kinetic
network. Subsequently, we demonstrate the effectiveness of our method through
experiments on a data set derived from molecular dynamics. We compare the
performance of our method to other partitioning techniques such as PCCA+. We
explore the importance of feature and hyperparameter selection and propose a
general strategy for large-scale parallel training of GNNs for discovering
optimal graph partitionings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14890">NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes. (arXiv:2312.14890v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1">Lizhou Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1">Wenyue Hua</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lingyao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1">Haoyang Ling</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yongfeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hemphill_L/0/1/0/all/0/1">Libby Hemphill</a></p>
<p>Complex reasoning ability is one of the most important features of current
LLMs, which has also been leveraged to play an integral role in complex
decision-making tasks. Therefore, the investigation into the reasoning
capabilities of Large Language Models (LLMs) is critical: numerous benchmarks
have been established to assess the reasoning abilities of LLMs. However,
current benchmarks are inadequate in offering a rigorous evaluation of the full
extent of reasoning abilities that LLMs are capable of achieving. They are also
prone to the risk of overfitting, as these benchmarks, being publicly
accessible and static, allow models to potentially tailor their responses to
specific benchmark metrics, thereby inflating their performance. Addressing
these limitations, our research introduces a new benchmark, named NPHardEval.
This benchmark is designed to evaluate the reasoning abilities of LLMs across a
broad spectrum of 900 algorithmic questions, extending up to the NP-Hard
complexity class. These questions are meticulously chosen to represent a wide
range of complexity class below the NP-hard complexity class, offering a
rigorous measure of the reasoning ability of LLMs. Through this study, we shed
light on the current state of reasoning in LLMs, providing an objective and
rigorous perspective through the comparison of LLMs' performance across complex
classes. Moreover, this benchmark is designed with a dynamic update mechanism,
where the datapoints are refreshed on a monthly basis. Such regular updates
play a crucial role in mitigating the risk of LLMs overfitting to the
benchmark, promoting a more accurate and reliable assessment of their reasoning
capabilities. The benchmark dataset and code of NPHardEval are available at
https://github.com/casmlab/NPHardEval.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14919">Lift-Attend-Splat: Bird&#x27;s-eye-view camera-lidar fusion using transformers. (arXiv:2312.14919v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gunn_J/0/1/0/all/0/1">James Gunn</a>, <a href="http://arxiv.org/find/cs/1/au:+Lenyk_Z/0/1/0/all/0/1">Zygmunt Lenyk</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1">Anuj Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Donati_A/0/1/0/all/0/1">Andrea Donati</a>, <a href="http://arxiv.org/find/cs/1/au:+Buburuzan_A/0/1/0/all/0/1">Alexandru Buburuzan</a>, <a href="http://arxiv.org/find/cs/1/au:+Redford_J/0/1/0/all/0/1">John Redford</a>, <a href="http://arxiv.org/find/cs/1/au:+Mueller_R/0/1/0/all/0/1">Romain Mueller</a></p>
<p>Combining complementary sensor modalities is crucial to providing robust
perception for safety-critical robotics applications such as autonomous driving
(AD). Recent state-of-the-art camera-lidar fusion methods for AD rely on
monocular depth estimation which is a notoriously difficult task compared to
using depth information from the lidar directly. Here, we find that this
approach does not leverage depth as expected and show that naively improving
depth estimation does not lead to improvements in object detection performance
and that, strikingly, removing depth estimation altogether does not degrade
object detection performance. This suggests that relying on monocular depth
could be an unnecessary architectural bottleneck during camera-lidar fusion. In
this work, we introduce a novel fusion method that bypasses monocular depth
estimation altogether and instead selects and fuses camera and lidar features
in a bird's-eye-view grid using a simple attention mechanism. We show that our
model can modulate its use of camera features based on the availability of
lidar features and that it yields better 3D object detection on the nuScenes
dataset than baselines relying on monocular depth estimation.
</p>
</p>
</div>

    </div>
    </body>
    