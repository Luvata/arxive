<!DOCTYPE html>
<html>
<head>
<title>2023-09-23-cs-cv</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2309.11510">When is a Foundation Model a Foundation Model. (arXiv:2309.11510v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alfasly_S/0/1/0/all/0/1">Saghir Alfasly</a>, <a href="http://arxiv.org/find/cs/1/au:+Nejat_P/0/1/0/all/0/1">Peyman Nejat</a>, <a href="http://arxiv.org/find/cs/1/au:+Hemati_S/0/1/0/all/0/1">Sobhan Hemati</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_J/0/1/0/all/0/1">Jibran Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lahr_I/0/1/0/all/0/1">Isaiah Lahr</a>, <a href="http://arxiv.org/find/cs/1/au:+Alsaafin_A/0/1/0/all/0/1">Areej Alsaafin</a>, <a href="http://arxiv.org/find/cs/1/au:+Shafique_A/0/1/0/all/0/1">Abubakr Shafique</a>, <a href="http://arxiv.org/find/cs/1/au:+Comfere_N/0/1/0/all/0/1">Nneka Comfere</a>, <a href="http://arxiv.org/find/cs/1/au:+Murphree_D/0/1/0/all/0/1">Dennis Murphree</a>, <a href="http://arxiv.org/find/cs/1/au:+Meroueh_C/0/1/0/all/0/1">Chady Meroueh</a>, <a href="http://arxiv.org/find/cs/1/au:+Yasir_S/0/1/0/all/0/1">Saba Yasir</a>, <a href="http://arxiv.org/find/cs/1/au:+Mangold_A/0/1/0/all/0/1">Aaron Mangold</a>, <a href="http://arxiv.org/find/cs/1/au:+Boardman_L/0/1/0/all/0/1">Lisa Boardman</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_V/0/1/0/all/0/1">Vijay Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Garcia_J/0/1/0/all/0/1">Joaquin J. Garcia</a>, <a href="http://arxiv.org/find/cs/1/au:+Tizhoosh_H/0/1/0/all/0/1">H.R. Tizhoosh</a></p>
<p>Recently, several studies have reported on the fine-tuning of foundation
models for image-text modeling in the field of medicine, utilizing images from
online data sources such as Twitter and PubMed. Foundation models are large,
deep artificial neural networks capable of learning the context of a specific
domain through training on exceptionally extensive datasets. Through
validation, we have observed that the representations generated by such models
exhibit inferior performance in retrieval tasks within digital pathology when
compared to those generated by significantly smaller, conventional deep
networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11523">RMT: Retentive Networks Meet Vision Transformers. (arXiv:2309.11523v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1">Qihang Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Huaibo Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Mingrui Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hongmin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1">Ran He</a></p>
<p>Transformer first appears in the field of natural language processing and is
later migrated to the computer vision domain, where it demonstrates excellent
performance in vision tasks. However, recently, Retentive Network (RetNet) has
emerged as an architecture with the potential to replace Transformer,
attracting widespread attention in the NLP community. Therefore, we raise the
question of whether transferring RetNet's idea to vision can also bring
outstanding performance to vision tasks. To address this, we combine RetNet and
Transformer to propose RMT. Inspired by RetNet, RMT introduces explicit decay
into the vision backbone, bringing prior knowledge related to spatial distances
to the vision model. This distance-related spatial prior allows for explicit
control of the range of tokens that each token can attend to. Additionally, to
reduce the computational cost of global modeling, we decompose this modeling
process along the two coordinate axes of the image. Abundant experiments have
demonstrated that our RMT exhibits exceptional performance across various
computer vision tasks. For example, RMT achieves 84.1% Top1-acc on ImageNet-1k
using merely 4.5G FLOPs. To the best of our knowledge, among all models, RMT
achieves the highest Top1-acc when models are of similar size and trained with
the same strategy. Moreover, RMT significantly outperforms existing vision
backbones in downstream tasks such as object detection, instance segmentation,
and semantic segmentation. Our work is still in progress.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11525">Light Field Diffusion for Single-View Novel View Synthesis. (arXiv:2309.11525v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1">Yifeng Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1">Haoyu Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1">Shanlin Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1">Kun Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xiaohui Xie</a></p>
<p>Single-view novel view synthesis, the task of generating images from new
viewpoints based on a single reference image, is an important but challenging
task in computer vision. Recently, Denoising Diffusion Probabilistic Model
(DDPM) has become popular in this area due to its strong ability to generate
high-fidelity images. However, current diffusion-based methods directly rely on
camera pose matrices as viewing conditions, globally and implicitly introducing
3D constraints. These methods may suffer from inconsistency among generated
images from different perspectives, especially in regions with intricate
textures and structures. In this work, we present Light Field Diffusion (LFD),
a conditional diffusion-based model for single-view novel view synthesis.
Unlike previous methods that employ camera pose matrices, LFD transforms the
camera view information into light field encoding and combines it with the
reference image. This design introduces local pixel-wise constraints within the
diffusion models, thereby encouraging better multi-view consistency.
Experiments on several datasets show that our LFD can efficiently generate
high-fidelity images and maintain better 3D consistency even in intricate
regions. Our method can generate images with higher quality than NeRF-based
models, and we obtain sample quality similar to other diffusion-based models
but with only one-third of the model size.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11531">EPTQ: Enhanced Post-Training Quantization via Label-Free Hessian. (arXiv:2309.11531v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gordon_O/0/1/0/all/0/1">Ofir Gordon</a>, <a href="http://arxiv.org/find/cs/1/au:+Habi_H/0/1/0/all/0/1">Hai Victor Habi</a>, <a href="http://arxiv.org/find/cs/1/au:+Netzer_A/0/1/0/all/0/1">Arnon Netzer</a></p>
<p>Quantization of deep neural networks (DNN) has become a key element in the
efforts of embedding such networks on end-user devices. However, current
quantization methods usually suffer from costly accuracy degradation. In this
paper, we propose a new method for Enhanced Post Training Quantization named
EPTQ. The method is based on knowledge distillation with an adaptive weighting
of layers. In addition, we introduce a new label-free technique for
approximating the Hessian trace of the task loss, named Label-Free Hessian.
This technique removes the requirement of a labeled dataset for computing the
Hessian. The adaptive knowledge distillation uses the Label-Free Hessian
technique to give greater attention to the sensitive parts of the model while
performing the optimization. Empirically, by employing EPTQ we achieve
state-of-the-art results on a wide variety of models, tasks, and datasets,
including ImageNet classification, COCO object detection, and Pascal-VOC for
semantic segmentation. We demonstrate the performance and compatibility of EPTQ
on an extended set of architectures, including CNNs, Transformers, hybrid, and
MLP-only models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11569">Revisiting Kernel Temporal Segmentation as an Adaptive Tokenizer for Long-form Video Understanding. (arXiv:2309.11569v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Afham_M/0/1/0/all/0/1">Mohamed Afham</a>, <a href="http://arxiv.org/find/cs/1/au:+Shukla_S/0/1/0/all/0/1">Satya Narayan Shukla</a>, <a href="http://arxiv.org/find/cs/1/au:+Poursaeed_O/0/1/0/all/0/1">Omid Poursaeed</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Pengchuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1">Ashish Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1">Sernam Lim</a></p>
<p>While most modern video understanding models operate on short-range clips,
real-world videos are often several minutes long with semantically consistent
segments of variable length. A common approach to process long videos is
applying a short-form video model over uniformly sampled clips of fixed
temporal length and aggregating the outputs. This approach neglects the
underlying nature of long videos since fixed-length clips are often redundant
or uninformative. In this paper, we aim to provide a generic and adaptive
sampling approach for long-form videos in lieu of the de facto uniform
sampling. Viewing videos as semantically consistent segments, we formulate a
task-agnostic, unsupervised, and scalable approach based on Kernel Temporal
Segmentation (KTS) for sampling and tokenizing long videos. We evaluate our
method on long-form video understanding tasks such as video classification and
temporal action localization, showing consistent gains over existing approaches
and achieving state-of-the-art performance on long-form video modeling.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11575">Distilling Adversarial Prompts from Safety Benchmarks: Report for the Adversarial Nibbler Challenge. (arXiv:2309.11575v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Brack_M/0/1/0/all/0/1">Manuel Brack</a>, <a href="http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1">Patrick Schramowski</a>, <a href="http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1">Kristian Kersting</a></p>
<p>Text-conditioned image generation models have recently achieved astonishing
image quality and alignment results. Consequently, they are employed in a
fast-growing number of applications. Since they are highly data-driven, relying
on billion-sized datasets randomly scraped from the web, they also produce
unsafe content. As a contribution to the Adversarial Nibbler challenge, we
distill a large set of over 1,000 potential adversarial inputs from existing
safety benchmarks. Our analysis of the gathered prompts and corresponding
images demonstrates the fragility of input filters and provides further
insights into systematic safety issues in current generative image models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11591">Continuous Levels of Detail for Light Field Networks. (arXiv:2309.11591v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">David Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_B/0/1/0/all/0/1">Brandon Y. Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Varshney_A/0/1/0/all/0/1">Amitabh Varshney</a></p>
<p>Recently, several approaches have emerged for generating neural
representations with multiple levels of detail (LODs). LODs can improve the
rendering by using lower resolutions and smaller model sizes when appropriate.
However, existing methods generally focus on a few discrete LODs which suffer
from aliasing and flicker artifacts as details are changed and limit their
granularity for adapting to resource limitations. In this paper, we propose a
method to encode light field networks with continuous LODs, allowing for finely
tuned adaptations to rendering conditions. Our training procedure uses
summed-area table filtering allowing efficient and continuous filtering at
various LODs. Furthermore, we use saliency-based importance sampling which
enables our light field networks to distribute their capacity, particularly
limited at lower LODs, towards representing the details viewers are most likely
to focus on. Incorporating continuous LODs into neural representations enables
progressive streaming of neural representations, decreasing the latency and
resource utilization for rendering.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11593">Sentence Attention Blocks for Answer Grounding. (arXiv:2309.11593v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khoshsirat_S/0/1/0/all/0/1">Seyedalireza Khoshsirat</a>, <a href="http://arxiv.org/find/cs/1/au:+Kambhamettu_C/0/1/0/all/0/1">Chandra Kambhamettu</a></p>
<p>Answer grounding is the task of locating relevant visual evidence for the
Visual Question Answering task. While a wide variety of attention methods have
been introduced for this task, they suffer from the following three problems:
designs that do not allow the usage of pre-trained networks and do not benefit
from large data pre-training, custom designs that are not based on
well-grounded previous designs, therefore limiting the learning power of the
network, or complicated designs that make it challenging to re-implement or
improve them. In this paper, we propose a novel architectural block, which we
term Sentence Attention Block, to solve these problems. The proposed block
re-calibrates channel-wise image feature-maps by explicitly modeling
inter-dependencies between the image feature-maps and sentence embedding. We
visually demonstrate how this block filters out irrelevant feature-maps
channels based on sentence embedding. We start our design with a well-known
attention method, and by making minor modifications, we improve the results to
achieve state-of-the-art accuracy. The flexibility of our method makes it easy
to use different pre-trained backbone networks, and its simplicity makes it
easy to understand and be re-implemented. We demonstrate the effectiveness of
our method on the TextVQA-X, VQS, VQA-X, and VizWiz-VQA-Grounding datasets. We
perform multiple ablation studies to show the effectiveness of our design
choices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11610">Hand Gesture Recognition with Two Stage Approach Using Transfer Learning and Deep Ensemble Learning. (arXiv:2309.11610v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Savas_S/0/1/0/all/0/1">Serkan Sava&#x15f;</a>, <a href="http://arxiv.org/find/cs/1/au:+Erguzen_A/0/1/0/all/0/1">Atilla Erg&#xfc;zen</a></p>
<p>Human-Computer Interaction (HCI) has been the subject of research for many
years, and recent studies have focused on improving its performance through
various techniques. In the past decade, deep learning studies have shown high
performance in various research areas, leading researchers to explore their
application to HCI. Convolutional neural networks can be used to recognize hand
gestures from images using deep architectures. In this study, we evaluated
pre-trained high-performance deep architectures on the HG14 dataset, which
consists of 14 different hand gesture classes. Among 22 different models,
versions of the VGGNet and MobileNet models attained the highest accuracy
rates. Specifically, the VGG16 and VGG19 models achieved accuracy rates of
94.64% and 94.36%, respectively, while the MobileNet and MobileNetV2 models
achieved accuracy rates of 96.79% and 94.43%, respectively. We performed hand
gesture recognition on the dataset using an ensemble learning technique, which
combined the four most successful models. By utilizing these models as base
learners and applying the Dirichlet ensemble technique, we achieved an accuracy
rate of 98.88%. These results demonstrate the effectiveness of the deep
ensemble learning technique for HCI and its potential applications in areas
such as augmented reality, virtual reality, and game technologies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11627">GenLayNeRF: Generalizable Layered Representations with 3D Model Alignment for Multi-Human View Synthesis. (arXiv:2309.11627v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abdelkareem_Y/0/1/0/all/0/1">Youssef Abdelkareem</a>, <a href="http://arxiv.org/find/cs/1/au:+Shehata_S/0/1/0/all/0/1">Shady Shehata</a>, <a href="http://arxiv.org/find/cs/1/au:+Karray_F/0/1/0/all/0/1">Fakhri Karray</a></p>
<p>Novel view synthesis (NVS) of multi-human scenes imposes challenges due to
the complex inter-human occlusions. Layered representations handle the
complexities by dividing the scene into multi-layered radiance fields, however,
they are mainly constrained to per-scene optimization making them inefficient.
Generalizable human view synthesis methods combine the pre-fitted 3D human
meshes with image features to reach generalization, yet they are mainly
designed to operate on single-human scenes. Another drawback is the reliance on
multi-step optimization techniques for parametric pre-fitting of the 3D body
models that suffer from misalignment with the images in sparse view settings
causing hallucinations in synthesized views. In this work, we propose,
GenLayNeRF, a generalizable layered scene representation for free-viewpoint
rendering of multiple human subjects which requires no per-scene optimization
and very sparse views as input. We divide the scene into multi-human layers
anchored by the 3D body meshes. We then ensure pixel-level alignment of the
body models with the input views through a novel end-to-end trainable module
that carries out iterative parametric correction coupled with multi-view
feature fusion to produce aligned 3D models. For NVS, we extract point-wise
image-aligned and human-anchored features which are correlated and fused using
self-attention and cross-attention modules. We augment low-level RGB values
into the features with an attention-based RGB fusion module. To evaluate our
approach, we construct two multi-human view synthesis datasets; DeepMultiSyn
and ZJU-MultiHuman. The results indicate that our proposed approach outperforms
generalizable and non-human per-scene NeRF methods while performing at par with
layered per-scene methods without test time optimization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11641">Attentive VQ-VAE. (arXiv:2309.11641v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rivera_M/0/1/0/all/0/1">Mariano Rivera</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoyos_A/0/1/0/all/0/1">Angello Hoyos</a></p>
<p>We present a novel approach to enhance the capabilities of VQVAE models
through the integration of an Attentive Residual Encoder (AREN) and a Residual
Pixel Attention layer. The objective of our research is to improve the
performance of VQVAE while maintaining practical parameter levels. The AREN
encoder is designed to operate effectively at multiple levels, accommodating
diverse architectural complexities. The key innovation is the integration of an
inter-pixel auto-attention mechanism into the AREN encoder. This approach
allows us to efficiently capture and utilize contextual information across
latent vectors. Additionally, our models uses additional encoding levels to
further enhance the model's representational power. Our attention layer employs
a minimal parameter approach, ensuring that latent vectors are modified only
when pertinent information from other pixels is available. Experimental results
demonstrate that our proposed modifications lead to significant improvements in
data representation and generation, making VQVAEs even more suitable for a wide
range of applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11648">Orbital AI-based Autonomous Refuelling Solution. (arXiv:2309.11648v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rondao_D/0/1/0/all/0/1">Duarte Rondao</a>, <a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1">Lei He</a>, <a href="http://arxiv.org/find/cs/1/au:+Aouf_N/0/1/0/all/0/1">Nabil Aouf</a></p>
<p>Cameras are rapidly becoming the choice for on-board sensors towards space
rendezvous due to their small form factor and inexpensive power, mass, and
volume costs. When it comes to docking, however, they typically serve a
secondary role, whereas the main work is done by active sensors such as lidar.
This paper documents the development of a proposed AI-based (artificial
intelligence) navigation algorithm intending to mature the use of on-board
visible wavelength cameras as a main sensor for docking and on-orbit servicing
(OOS), reducing the dependency on lidar and greatly reducing costs.
Specifically, the use of AI enables the expansion of the relative navigation
solution towards multiple classes of scenarios, e.g., in terms of targets or
illumination conditions, which would otherwise have to be crafted on a
case-by-case manner using classical image processing methods. Multiple
convolutional neural network (CNN) backbone architectures are benchmarked on
synthetically generated data of docking manoeuvres with the International Space
Station (ISS), achieving position and attitude estimates close to 1%
range-normalised and 1 deg, respectively. The integration of the solution with
a physical prototype of the refuelling mechanism is validated in laboratory
using a robotic arm to simulate a berthing procedure.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11661">Neural Image Compression Using Masked Sparse Visual Representation. (arXiv:2309.11661v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1">Wei Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yue Chen</a></p>
<p>We study neural image compression based on the Sparse Visual Representation
(SVR), where images are embedded into a discrete latent space spanned by
learned visual codebooks. By sharing codebooks with the decoder, the encoder
transfers integer codeword indices that are efficient and cross-platform
robust, and the decoder retrieves the embedded latent feature using the indices
for reconstruction. Previous SVR-based compression lacks effective mechanism
for rate-distortion tradeoffs, where one can only pursue either high
reconstruction quality or low transmission bitrate. We propose a Masked
Adaptive Codebook learning (M-AdaCode) method that applies masks to the latent
feature subspace to balance bitrate and reconstruction quality. A set of
semantic-class-dependent basis codebooks are learned, which are weighted
combined to generate a rich latent feature for high-quality reconstruction. The
combining weights are adaptively derived from each input image, providing
fidelity information with additional transmission costs. By masking out
unimportant weights in the encoder and recovering them in the decoder, we can
trade off reconstruction quality for transmission bits, and the masking rate
controls the balance between bitrate and distortion. Experiments over the
standard JPEG-AI dataset demonstrate the effectiveness of our M-AdaCode
approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11667">Understanding Pose and Appearance Disentanglement in 3D Human Pose Estimation. (arXiv:2309.11667v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nakka_K/0/1/0/all/0/1">Krishna Kanth Nakka</a>, <a href="http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1">Mathieu Salzmann</a></p>
<p>As 3D human pose estimation can now be achieved with very high accuracy in
the supervised learning scenario, tackling the case where 3D pose annotations
are not available has received increasing attention. In particular, several
methods have proposed to learn image representations in a self-supervised
fashion so as to disentangle the appearance information from the pose one. The
methods then only need a small amount of supervised data to train a pose
regressor using the pose-related latent vector as input, as it should be free
of appearance information. In this paper, we carry out in-depth analysis to
understand to what degree the state-of-the-art disentangled representation
learning methods truly separate the appearance information from the pose one.
First, we study disentanglement from the perspective of the self-supervised
network, via diverse image synthesis experiments. Second, we investigate
disentanglement with respect to the 3D pose regressor following an adversarial
attack perspective. Specifically, we design an adversarial strategy focusing on
generating natural appearance changes of the subject, and against which we
could expect a disentangled network to be robust. Altogether, our analyses show
that disentanglement in the three state-of-the-art disentangled representation
learning frameworks if far from complete, and that their pose codes contain
significant appearance information. We believe that our approach provides a
valuable testbed to evaluate the degree of disentanglement of pose from
appearance in self-supervised 3D human pose estimation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11705">Meta OOD Learning for Continuously Adaptive OOD Detection. (arXiv:2309.11705v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xinheng Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Jie Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1">Zhen Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Guangquan Zhang</a></p>
<p>Out-of-distribution (OOD) detection is crucial to modern deep learning
applications by identifying and alerting about the OOD samples that should not
be tested or used for making predictions. Current OOD detection methods have
made significant progress when in-distribution (ID) and OOD samples are drawn
from static distributions. However, this can be unrealistic when applied to
real-world systems which often undergo continuous variations and shifts in ID
and OOD distributions over time. Therefore, for an effective application in
real-world systems, the development of OOD detection methods that can adapt to
these dynamic and evolving distributions is essential. In this paper, we
propose a novel and more realistic setting called continuously adaptive
out-of-distribution (CAOOD) detection which targets on developing an OOD
detection model that enables dynamic and quick adaptation to a new arriving
distribution, with insufficient ID samples during deployment time. To address
CAOOD, we develop meta OOD learning (MOL) by designing a learning-to-adapt
diagram such that a good initialized OOD detection model is learned during the
training process. In the testing process, MOL ensures OOD detection performance
over shifting distributions by quickly adapting to new distributions with a few
adaptations. Extensive experiments on several OOD benchmarks endorse the
effectiveness of our method in preserving both ID classification accuracy and
OOD detection performance on continuously shifting distributions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11707">Efficient Long-Short Temporal Attention Network for Unsupervised Video Object Segmentation. (arXiv:2309.11707v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Ping Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1">Li Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1">Huaxin Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1">Binbin Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xianghua Xu</a></p>
<p>Unsupervised Video Object Segmentation (VOS) aims at identifying the contours
of primary foreground objects in videos without any prior knowledge. However,
previous methods do not fully use spatial-temporal context and fail to tackle
this challenging task in real-time. This motivates us to develop an efficient
Long-Short Temporal Attention network (termed LSTA) for unsupervised VOS task
from a holistic view. Specifically, LSTA consists of two dominant modules,
i.e., Long Temporal Memory and Short Temporal Attention. The former captures
the long-term global pixel relations of the past frames and the current frame,
which models constantly present objects by encoding appearance pattern.
Meanwhile, the latter reveals the short-term local pixel relations of one
nearby frame and the current frame, which models moving objects by encoding
motion pattern. To speedup the inference, the efficient projection and the
locality-based sliding window are adopted to achieve nearly linear time
complexity for the two light modules, respectively. Extensive empirical studies
on several benchmarks have demonstrated promising performances of the proposed
method with high efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11710">ContextRef: Evaluating Referenceless Metrics For Image Description Generation. (arXiv:2309.11710v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kreiss_E/0/1/0/all/0/1">Elisa Kreiss</a>, <a href="http://arxiv.org/find/cs/1/au:+Zelikman_E/0/1/0/all/0/1">Eric Zelikman</a>, <a href="http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1">Christopher Potts</a>, <a href="http://arxiv.org/find/cs/1/au:+Haber_N/0/1/0/all/0/1">Nick Haber</a></p>
<p>Referenceless metrics (e.g., CLIPScore) use pretrained vision--language
models to assess image descriptions directly without costly ground-truth
reference texts. Such methods can facilitate rapid progress, but only if they
truly align with human preference judgments. In this paper, we introduce
ContextRef, a benchmark for assessing referenceless metrics for such alignment.
ContextRef has two components: human ratings along a variety of established
quality dimensions, and ten diverse robustness checks designed to uncover
fundamental weaknesses. A crucial aspect of ContextRef is that images and
descriptions are presented in context, reflecting prior work showing that
context is important for description quality. Using ContextRef, we assess a
variety of pretrained models, scoring functions, and techniques for
incorporating context. None of the methods is successful with ContextRef, but
we show that careful fine-tuning yields substantial improvements. ContextRef
remains a challenging benchmark though, in large part due to the challenge of
context dependence.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11711">MoDA: Leveraging Motion Priors from Videos for Advancing Unsupervised Domain Adaptation in Semantic Segmentation. (arXiv:2309.11711v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1">Fei Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1">Xu Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Seokju Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1">Sungeui Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1">In So Kweon</a></p>
<p>Unsupervised domain adaptation (UDA) is an effective approach to handle the
lack of annotations in the target domain for the semantic segmentation task. In
this work, we consider a more practical UDA setting where the target domain
contains sequential frames of the unlabeled videos which are easy to collect in
practice. A recent study suggests self-supervised learning of the object motion
from unlabeled videos with geometric constraints. We design a motion-guided
domain adaptive semantic segmentation framework (MoDA), that utilizes
self-supervised object motion to learn effective representations in the target
domain. MoDA differs from previous methods that use temporal consistency
regularization for the target domain frames. Instead, MoDA deals separately
with the domain alignment on the foreground and background categories using
different strategies. Specifically, MoDA contains foreground object discovery
and foreground semantic mining to align the foreground domain gaps by taking
the instance-level guidance from the object motion. Additionally, MoDA includes
background adversarial training which contains a background category-specific
discriminator to handle the background domain gaps. Experimental results on
multiple benchmarks highlight the effectiveness of MoDA against existing
approaches in the domain adaptive image segmentation and domain adaptive video
segmentation. Moreover, MoDA is versatile and can be used in conjunction with
existing state-of-the-art approaches to further improve performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11715">Deshadow-Anything: When Segment Anything Model Meets Zero-shot shadow removal. (arXiv:2309.11715v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiao Feng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_T/0/1/0/all/0/1">Tian Yi Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1">Jia Wei Yao</a></p>
<p>Segment Anything (SAM), an advanced universal image segmentation model
trained on an expansive visual dataset, has set a new benchmark in image
segmentation and computer vision. However, it faced challenges when it came to
distinguishing between shadows and their backgrounds. To address this, we
developed Deshadow-Anything, considering the generalization of large-scale
datasets, and we performed Fine-tuning on large-scale datasets to achieve image
shadow removal. The diffusion model can diffuse along the edges and textures of
an image, helping to remove shadows while preserving the details of the image.
Furthermore, we design Multi-Self-Attention Guidance (MSAG) and adaptive input
perturbation (DDPM-AIP) to accelerate the iterative training speed of
diffusion. Experiments on shadow removal tasks demonstrate that these methods
can effectively improve image restoration performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11718">CPR-Coach: Recognizing Composite Error Actions based on Single-class Training. (arXiv:2309.11718v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shunli Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1">Qing Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuaibing Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1">Dingkang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1">Liuzhen Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xiao Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuang_H/0/1/0/all/0/1">Haopeng Kuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Peixuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_P/0/1/0/all/0/1">Peng Zhai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lihua Zhang</a></p>
<p>The fine-grained medical action analysis task has received considerable
attention from pattern recognition communities recently, but it faces the
problems of data and algorithm shortage. Cardiopulmonary Resuscitation (CPR) is
an essential skill in emergency treatment. Currently, the assessment of CPR
skills mainly depends on dummies and trainers, leading to high training costs
and low efficiency. For the first time, this paper constructs a vision-based
system to complete error action recognition and skill assessment in CPR.
Specifically, we define 13 types of single-error actions and 74 types of
composite error actions during external cardiac compression and then develop a
video dataset named CPR-Coach. By taking the CPR-Coach as a benchmark, this
paper thoroughly investigates and compares the performance of existing action
recognition models based on different data modalities. To solve the unavoidable
Single-class Training &amp; Multi-class Testing problem, we propose a
humancognition-inspired framework named ImagineNet to improve the model's
multierror recognition performance under restricted supervision. Extensive
experiments verify the effectiveness of the framework. We hope this work could
advance research toward fine-grained medical action analysis and skill
assessment. The CPR-Coach dataset and the code of ImagineNet are publicly
available on Github.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11745">PIE: Simulating Disease Progression via Progressive Image Editing. (arXiv:2309.11745v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Liang_K/0/1/0/all/0/1">Kaizhao Liang</a>, <a href="http://arxiv.org/find/eess/1/au:+Cao_X/0/1/0/all/0/1">Xu Cao</a>, <a href="http://arxiv.org/find/eess/1/au:+Liao_K/0/1/0/all/0/1">Kuei-Da Liao</a>, <a href="http://arxiv.org/find/eess/1/au:+Gao_T/0/1/0/all/0/1">Tianren Gao</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1">Zhengyu Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Nama_T/0/1/0/all/0/1">Tejas Nama</a></p>
<p>Disease progression simulation is a crucial area of research that has
significant implications for clinical diagnosis, prognosis, and treatment. One
major challenge in this field is the lack of continuous medical imaging
monitoring of individual patients over time. To address this issue, we develop
a novel framework termed Progressive Image Editing (PIE) that enables
controlled manipulation of disease-related image features, facilitating precise
and realistic disease progression simulation. Specifically, we leverage recent
advancements in text-to-image generative models to simulate disease progression
accurately and personalize it for each patient. We theoretically analyze the
iterative refining process in our framework as a gradient descent with an
exponentially decayed learning rate. To validate our framework, we conduct
experiments in three medical imaging domains. Our results demonstrate the
superiority of PIE over existing methods such as Stable Diffusion Walk and
Style-Based Manifold Extrapolation based on CLIP score (Realism) and Disease
Classification Confidence (Alignment). Our user study collected feedback from
35 veteran physicians to assess the generated progressions. Remarkably, 76.2%
of the feedback agrees with the fidelity of the generated progressions. To our
best knowledge, PIE is the first of its kind to generate disease progression
images meeting real-world standards. It is a promising tool for medical
research and clinical practice, potentially allowing healthcare providers to
model disease trajectories over time, predict future treatment responses, and
improve patient outcomes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11751">How Robust is Google&#x27;s Bard to Adversarial Image Attacks?. (arXiv:2309.11751v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1">Yinpeng Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huanran Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiawei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1">Zhengwei Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xiao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yichi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yu Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1">Hang Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jun Zhu</a></p>
<p>Multimodal Large Language Models (MLLMs) that integrate text and other
modalities (especially vision) have achieved unprecedented performance in
various multimodal tasks. However, due to the unsolved adversarial robustness
problem of vision models, MLLMs can have more severe safety and security risks
by introducing the vision inputs. In this work, we study the adversarial
robustness of Google's Bard, a competitive chatbot to ChatGPT that released its
multimodal capability recently, to better understand the vulnerabilities of
commercial MLLMs. By attacking white-box surrogate vision encoders or MLLMs,
the generated adversarial examples can mislead Bard to output wrong image
descriptions with a 22% success rate based solely on the transferability. We
show that the adversarial examples can also attack other MLLMs, e.g., a 26%
attack success rate against Bing Chat and a 86% attack success rate against
ERNIE bot. Moreover, we identify two defense mechanisms of Bard, including face
detection and toxicity detection of images. We design corresponding attacks to
evade these defenses, demonstrating that the current defenses of Bard are also
vulnerable. We hope this work can deepen our understanding on the robustness of
MLLMs and facilitate future research on defenses. Our code is available at
https://github.com/thu-ml/Attack-Bard.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11754">A Vision-Centric Approach for Static Map Element Annotation. (arXiv:2309.11754v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiaxin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shiyuan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1">Haoran Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Mei_R/0/1/0/all/0/1">Ruohong Mei</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Cong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sui_W/0/1/0/all/0/1">Wei Sui</a></p>
<p>The recent development of online static map element (a.k.a. HD Map)
construction algorithms has raised a vast demand for data with ground truth
annotations. However, available public datasets currently cannot provide
high-quality training data regarding consistency and accuracy. To this end, we
present CAMA: a vision-centric approach for Consistent and Accurate Map
Annotation. Without LiDAR inputs, our proposed framework can still generate
high-quality 3D annotations of static map elements. Specifically, the
annotation can achieve high reprojection accuracy across all surrounding
cameras and is spatial-temporal consistent across the whole sequence. We apply
our proposed framework to the popular nuScenes dataset to provide efficient and
highly accurate annotations. Compared with the original nuScenes static map
element, models trained with annotations from CAMA achieve lower reprojection
errors (e.g., 4.73 vs. 8.03 pixels).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11755">2DDATA: 2D Detection Annotations Transmittable Aggregation for Semantic Segmentation on Point Cloud. (arXiv:2309.11755v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1">Guan-Cheng Lee</a></p>
<p>Recently, multi-modality models have been introduced because of the
complementary information from different sensors such as LiDAR and cameras. It
requires paired data along with precise calibrations for all modalities, the
complicated calibration among modalities hugely increases the cost of
collecting such high-quality datasets, and hinder it from being applied to
practical scenarios. Inherit from the previous works, we not only fuse the
information from multi-modality without above issues, and also exhaust the
information in the RGB modality. We introduced the 2D Detection Annotations
Transmittable Aggregation(\textbf{2DDATA}), designing a data-specific branch,
called \textbf{Local Object Branch}, which aims to deal with points in a
certain bounding box, because of its easiness of acquiring 2D bounding box
annotations. We demonstrate that our simple design can transmit bounding box
prior information to the 3D encoder model, proving the feasibility of large
multi-modality models fused with modality-specific data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11758">SAM-OCTA: A Fine-Tuning Strategy for Applying Foundation Model to OCTA Image Segmentation Tasks. (arXiv:2309.11758v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chengliang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xinrun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ning_H/0/1/0/all/0/1">Haojian Ning</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shiying Li</a></p>
<p>In the analysis of optical coherence tomography angiography (OCTA) images,
the operation of segmenting specific targets is necessary. Existing methods
typically train on supervised datasets with limited samples (approximately a
few hundred), which can lead to overfitting. To address this, the low-rank
adaptation technique is adopted for foundation model fine-tuning and proposed
corresponding prompt point generation strategies to process various
segmentation tasks on OCTA datasets. This method is named SAM-OCTA and has been
experimented on the publicly available OCTA-500 dataset. While achieving
state-of-the-art performance metrics, this method accomplishes local vessel
segmentation as well as effective artery-vein segmentation, which was not
well-solved in previous works. The code is available at:
https://github.com/ShellRedia/SAM-OCTA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11766">Dictionary Attack on IMU-based Gait Authentication. (arXiv:2309.11766v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1">Rajesh Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Isik_C/0/1/0/all/0/1">Can Isik</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohan_C/0/1/0/all/0/1">Chilukuri K. Mohan</a></p>
<p>We present a novel adversarial model for authentication systems that use gait
patterns recorded by the inertial measurement unit (IMU) built into
smartphones. The attack idea is inspired by and named after the concept of a
dictionary attack on knowledge (PIN or password) based authentication systems.
In particular, this work investigates whether it is possible to build a
dictionary of IMUGait patterns and use it to launch an attack or find an
imitator who can actively reproduce IMUGait patterns that match the target's
IMUGait pattern. Nine physically and demographically diverse individuals walked
at various levels of four predefined controllable and adaptable gait factors
(speed, step length, step width, and thigh-lift), producing 178 unique IMUGait
patterns. Each pattern attacked a wide variety of user authentication models.
The deeper analysis of error rates (before and after the attack) challenges the
belief that authentication systems based on IMUGait patterns are the most
difficult to spoof; further research is needed on adversarial models and
associated countermeasures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11767">Fast Satellite Tensorial Radiance Field for Multi-date Satellite Imagery of Large Size. (arXiv:2309.11767v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tongtong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuanxiang Li</a></p>
<p>Existing NeRF models for satellite images suffer from slow speeds, mandatory
solar information as input, and limitations in handling large satellite images.
In response, we present SatensoRF, which significantly accelerates the entire
process while employing fewer parameters for satellite imagery of large size.
Besides, we observed that the prevalent assumption of Lambertian surfaces in
neural radiance fields falls short for vegetative and aquatic elements. In
contrast to the traditional hierarchical MLP-based scene representation, we
have chosen a multiscale tensor decomposition approach for color, volume
density, and auxiliary variables to model the lightfield with specular color.
Additionally, to rectify inconsistencies in multi-date imagery, we incorporate
total variation loss to restore the density tensor field and treat the problem
as a denosing task.To validate our approach, we conducted assessments of
SatensoRF using subsets from the spacenet multi-view dataset, which includes
both multi-date and single-date multi-view RGB images. Our results clearly
demonstrate that SatensoRF surpasses the state-of-the-art Sat-NeRF series in
terms of novel view synthesis performance. Significantly, SatensoRF requires
fewer parameters for training, resulting in faster training and inference
speeds and reduced computational demands.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11773">A Real-Time Multi-Task Learning System for Joint Detection of Face, Facial Landmark and Head Pose. (arXiv:2309.11773v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1">Qingtian Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Liming Zhang</a></p>
<p>Extreme head postures pose a common challenge across a spectrum of facial
analysis tasks, including face detection, facial landmark detection (FLD), and
head pose estimation (HPE). These tasks are interdependent, where accurate FLD
relies on robust face detection, and HPE is intricately associated with these
key points. This paper focuses on the integration of these tasks, particularly
when addressing the complexities posed by large-angle face poses. The primary
contribution of this study is the proposal of a real-time multi-task detection
system capable of simultaneously performing joint detection of faces, facial
landmarks, and head poses. This system builds upon the widely adopted YOLOv8
detection framework. It extends the original object detection head by
incorporating additional landmark regression head, enabling efficient
localization of crucial facial landmarks. Furthermore, we conduct optimizations
and enhancements on various modules within the original YOLOv8 framework. To
validate the effectiveness and real-time performance of our proposed model, we
conduct extensive experiments on 300W-LP and AFLW2000-3D datasets. The results
obtained verify the capability of our model to tackle large-angle face pose
challenges while delivering real-time performance across these interconnected
tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11782">DimCL: Dimensional Contrastive Learning For Improving Self-Supervised Learning. (arXiv:2309.11782v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Thanh Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1">Trung Pham</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chaoning Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luu_T/0/1/0/all/0/1">Tung Luu</a>, <a href="http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1">Thang Vu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoo_C/0/1/0/all/0/1">Chang D. Yoo</a></p>
<p>Self-supervised learning (SSL) has gained remarkable success, for which
contrastive learning (CL) plays a key role. However, the recent development of
new non-CL frameworks has achieved comparable or better performance with high
improvement potential, prompting researchers to enhance these frameworks
further. Assimilating CL into non-CL frameworks has been thought to be
beneficial, but empirical evidence indicates no visible improvements. In view
of that, this paper proposes a strategy of performing CL along the dimensional
direction instead of along the batch direction as done in conventional
contrastive learning, named Dimensional Contrastive Learning (DimCL). DimCL
aims to enhance the feature diversity, and it can serve as a regularizer to
prior SSL frameworks. DimCL has been found to be effective, and the
hardness-aware property is identified as a critical reason for its success.
Extensive experimental results reveal that assimilating DimCL into SSL
frameworks leads to performance improvement by a non-trivial margin on various
datasets and backbone architectures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11804">FGFusion: Fine-Grained Lidar-Camera Fusion for 3D Object Detection. (arXiv:2309.11804v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1">Zixuan Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Han Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1">Ningzhong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Huiyu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1">Jiaquan Shen</a></p>
<p>Lidars and cameras are critical sensors that provide complementary
information for 3D detection in autonomous driving. While most prevalent
methods progressively downscale the 3D point clouds and camera images and then
fuse the high-level features, the downscaled features inevitably lose low-level
detailed information. In this paper, we propose Fine-Grained Lidar-Camera
Fusion (FGFusion) that make full use of multi-scale features of image and point
cloud and fuse them in a fine-grained way. First, we design a dual pathway
hierarchy structure to extract both high-level semantic and low-level detailed
features of the image. Second, an auxiliary network is introduced to guide
point cloud features to better learn the fine-grained spatial information.
Finally, we propose multi-scale fusion (MSF) to fuse the last N feature maps of
image and point cloud. Extensive experiments on two popular autonomous driving
benchmarks, i.e. KITTI and Waymo, demonstrate the effectiveness of our method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11820">Automatic Endoscopic Ultrasound Station Recognition with Limited Data. (arXiv:2309.11820v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ramesh_A/0/1/0/all/0/1">Abhijit Ramesh</a>, <a href="http://arxiv.org/find/eess/1/au:+Nandanan_A/0/1/0/all/0/1">Anantha Nandanan</a>, <a href="http://arxiv.org/find/eess/1/au:+Nandanan_A/0/1/0/all/0/1">Anantha Nandanan</a>, <a href="http://arxiv.org/find/eess/1/au:+MD_P/0/1/0/all/0/1">Priya Nair MD</a>, <a href="http://arxiv.org/find/eess/1/au:+Gressel_G/0/1/0/all/0/1">Gilad Gressel</a></p>
<p>Pancreatic cancer is a lethal form of cancer that significantly contributes
to cancer-related deaths worldwide. Early detection is essential to improve
patient prognosis and survival rates. Despite advances in medical imaging
techniques, pancreatic cancer remains a challenging disease to detect.
Endoscopic ultrasound (EUS) is the most effective diagnostic tool for detecting
pancreatic cancer. However, it requires expert interpretation of complex
ultrasound images to complete a reliable patient scan. To obtain complete
imaging of the pancreas, practitioners must learn to guide the endoscope into
multiple "EUS stations" (anatomical locations), which provide different views
of the pancreas. This is a difficult skill to learn, involving over 225
proctored procedures with the support of an experienced doctor. We build an
AI-assisted tool that utilizes deep learning techniques to identify these
stations of the stomach in real time during EUS procedures. This
computer-assisted diagnostic (CAD) will help train doctors more efficiently.
Historically, the challenge faced in developing such a tool has been the amount
of retrospective labeling required by trained clinicians. To solve this, we
developed an open-source user-friendly labeling web app that streamlines the
process of annotating stations during the EUS procedure with minimal effort
from the clinicians. Our research shows that employing only 43 procedures with
no hyperparameter fine-tuning obtained a balanced accuracy of 90%, comparable
to the current state of the art. In addition, we employ Grad-CAM, a
visualization technology that provides clinicians with interpretable and
explainable visualizations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11839">MoPA: Multi-Modal Prior Aided Domain Adaptation for 3D Semantic Segmentation. (arXiv:2309.11839v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1">Haozhi Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yuecong Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jianfei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1">Pengyu Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1">Shenghai Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1">Lihua Xie</a></p>
<p>Multi-modal unsupervised domain adaptation (MM-UDA) for 3D semantic
segmentation is a practical solution to embed semantic understanding in
autonomous systems without expensive point-wise annotations. While previous
MM-UDA methods can achieve overall improvement, they suffer from significant
class-imbalanced performance, restricting their adoption in real applications.
This imbalanced performance is mainly caused by: 1) self-training with
imbalanced data and 2) the lack of pixel-wise 2D supervision signals. In this
work, we propose Multi-modal Prior Aided (MoPA) domain adaptation to improve
the performance of rare objects. Specifically, we develop Valid Ground-based
Insertion (VGI) to rectify the imbalance supervision signals by inserting prior
rare objects collected from the wild while avoiding introducing artificial
artifacts that lead to trivial solutions. Meanwhile, our SAM consistency loss
leverages the 2D prior semantic masks from SAM as pixel-wise supervision
signals to encourage consistent predictions for each object in the semantic
mask. The knowledge learned from modal-specific prior is then shared across
modalities to achieve better rare object segmentation. Extensive experiments
show that our method achieves state-of-the-art performance on the challenging
MM-UDA benchmark. Code will be available at https://github.com/AronCao49/MoPA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11847">MEFLUT: Unsupervised 1D Lookup Tables for Multi-exposure Image Fusion. (arXiv:2309.11847v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1">Ting Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xinpeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1">Ru Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1">Haoqiang Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shuaicheng Liu</a></p>
<p>In this paper, we introduce a new approach for high-quality multi-exposure
image fusion (MEF). We show that the fusion weights of an exposure can be
encoded into a 1D lookup table (LUT), which takes pixel intensity value as
input and produces fusion weight as output. We learn one 1D LUT for each
exposure, then all the pixels from different exposures can query 1D LUT of that
exposure independently for high-quality and efficient fusion. Specifically, to
learn these 1D LUTs, we involve attention mechanism in various dimensions
including frame, channel and spatial ones into the MEF task so as to bring us
significant quality improvement over the state-of-the-art (SOTA). In addition,
we collect a new MEF dataset consisting of 960 samples, 155 of which are
manually tuned by professionals as ground-truth for evaluation. Our network is
trained by this dataset in an unsupervised manner. Extensive experiments are
conducted to demonstrate the effectiveness of all the newly proposed
components, and results show that our approach outperforms the SOTA in our and
another representative dataset SICE, both qualitatively and quantitatively.
Moreover, our 1D LUT approach takes less than 4ms to run a 4K image on a PC
GPU. Given its high quality, efficiency and robustness, our method has been
shipped into millions of Android mobiles across multiple brands world-wide.
Code is available at: https://github.com/Hedlen/MEFLUT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11851">DEYOv3: DETR with YOLO for Real-time Object Detection. (arXiv:2309.11851v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ouyang_H/0/1/0/all/0/1">Haodong Ouyang</a></p>
<p>Recently, end-to-end object detectors have gained significant attention from
the research community due to their outstanding performance. However, DETR
typically relies on supervised pretraining of the backbone on ImageNet, which
limits the practical application of DETR and the design of the backbone,
affecting the model's potential generalization ability. In this paper, we
propose a new training method called step-by-step training. Specifically, in
the first stage, the one-to-many pre-trained YOLO detector is used to
initialize the end-to-end detector. In the second stage, the backbone and
encoder are consistent with the DETR-like model, but only the detector needs to
be trained from scratch. Due to this training method, the object detector does
not need the additional dataset (ImageNet) to train the backbone, which makes
the design of the backbone more flexible and dramatically reduces the training
cost of the detector, which is helpful for the practical application of the
object detector. At the same time, compared with the DETR-like model, the
step-by-step training method can achieve higher accuracy than the traditional
training method of the DETR-like model. With the aid of this novel training
method, we propose a brand-new end-to-end real-time object detection model
called DEYOv3. DEYOv3-N achieves 41.1% on COCO val2017 and 270 FPS on T4 GPU,
while DEYOv3-L achieves 51.3% AP and 102 FPS. Without the use of additional
training data, DEYOv3 surpasses all existing real-time object detectors in
terms of both speed and accuracy. It is worth noting that for models of N, S,
and M scales, the training on the COCO dataset can be completed using a single
24GB RTX3090 GPU.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11857">TCOVIS: Temporally Consistent Online Video Instance Segmentation. (arXiv:2309.11857v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Junlong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1">Bingyao Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1">Yongming Rao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jie Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Jiwen Lu</a></p>
<p>In recent years, significant progress has been made in video instance
segmentation (VIS), with many offline and online methods achieving
state-of-the-art performance. While offline methods have the advantage of
producing temporally consistent predictions, they are not suitable for
real-time scenarios. Conversely, online methods are more practical, but
maintaining temporal consistency remains a challenging task. In this paper, we
propose a novel online method for video instance segmentation, called TCOVIS,
which fully exploits the temporal information in a video clip. The core of our
method consists of a global instance assignment strategy and a spatio-temporal
enhancement module, which improve the temporal consistency of the features from
two aspects. Specifically, we perform global optimal matching between the
predictions and ground truth across the whole video clip, and supervise the
model with the global optimal objective. We also capture the spatial feature
and aggregate it with the semantic feature between frames, thus realizing the
spatio-temporal enhancement. We evaluate our method on four widely adopted VIS
benchmarks, namely YouTube-VIS 2019/2021/2022 and OVIS, and achieve
state-of-the-art performance on all benchmarks without bells-and-whistles. For
instance, on YouTube-VIS 2021, TCOVIS achieves 49.5 AP and 61.3 AP with
ResNet-50 and Swin-L backbones, respectively. Code is available at
https://github.com/jun-long-li/TCOVIS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11858">OSNet &amp; MNetO: Two Types of General Reconstruction Architectures for Linear Computed Tomography in Multi-Scenarios. (arXiv:2309.11858v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhisheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1">Zihan Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1">Fenglin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yixing Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Haijun Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1">Junning Cui</a></p>
<p>Recently, linear computed tomography (LCT) systems have actively attracted
attention. To weaken projection truncation and image the region of interest
(ROI) for LCT, the backprojection filtration (BPF) algorithm is an effective
solution. However, in BPF for LCT, it is difficult to achieve stable interior
reconstruction, and for differentiated backprojection (DBP) images of LCT,
multiple rotation-finite inversion of Hilbert transform (Hilbert
filtering)-inverse rotation operations will blur the image. To satisfy multiple
reconstruction scenarios for LCT, including interior ROI, complete object, and
exterior region beyond field-of-view (FOV), and avoid the rotation operations
of Hilbert filtering, we propose two types of reconstruction architectures. The
first overlays multiple DBP images to obtain a complete DBP image, then uses a
network to learn the overlying Hilbert filtering function, referred to as the
Overlay-Single Network (OSNet). The second uses multiple networks to train
different directional Hilbert filtering models for DBP images of multiple
linear scannings, respectively, and then overlays the reconstructed results,
i.e., Multiple Networks Overlaying (MNetO). In two architectures, we introduce
a Swin Transformer (ST) block to the generator of pix2pixGAN to extract both
local and global features from DBP images at the same time. We investigate two
architectures from different networks, FOV sizes, pixel sizes, number of
projections, geometric magnification, and processing time. Experimental results
show that two architectures can both recover images. OSNet outperforms BPF in
various scenarios. For the different networks, ST-pix2pixGAN is superior to
pix2pixGAN and CycleGAN. MNetO exhibits a few artifacts due to the differences
among the multiple models, but any one of its models is suitable for imaging
the exterior edge in a certain direction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11876">Multi-level Asymmetric Contrastive Learning for Medical Image Segmentation Pre-training. (arXiv:2309.11876v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1">Shuang Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1">Lei Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xinliang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1">Zifeng Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Qian Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1">Lujia Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiayi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yanye Lu</a></p>
<p>Contrastive learning, which is a powerful technique for learning image-level
representations from unlabeled data, leads a promising direction to dealing
with the dilemma between large-scale pre-training and limited labeled data.
However, most existing contrastive learning strategies are designed mainly for
downstream tasks of natural images, therefore they are sub-optimal and even
worse than learning from scratch when directly applied to medical images whose
downstream tasks are usually segmentation. In this work, we propose a novel
asymmetric contrastive learning framework named JCL for medical image
segmentation with self-supervised pre-training. Specifically, (1) A novel
asymmetric contrastive learning strategy is proposed to pre-train both encoder
and decoder simultaneously in one-stage to provide better initialization for
segmentation models. (2) A multi-level contrastive loss is designed to take the
correspondence among feature-level, image-level and pixel-level projections,
respectively into account to make sure multi-level representations can be
learned by the encoder and decoder during pre-training. (3) Experiments on
multiple medical image datasets indicate our JCL framework outperforms existing
SOTA contrastive learning strategies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11881">Using Saliency and Cropping to Improve Video Memorability. (arXiv:2309.11881v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mudgal_V/0/1/0/all/0/1">Vaibhav Mudgal</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qingyang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sweeney_L/0/1/0/all/0/1">Lorin Sweeney</a>, <a href="http://arxiv.org/find/cs/1/au:+Smeaton_A/0/1/0/all/0/1">Alan F. Smeaton</a></p>
<p>Video memorability is a measure of how likely a particular video is to be
remembered by a viewer when that viewer has no emotional connection with the
video content. It is an important characteristic as videos that are more
memorable are more likely to be shared, viewed, and discussed. This paper
presents results of a series of experiments where we improved the memorability
of a video by selectively cropping frames based on image saliency. We present
results of a basic fixed cropping as well as the results from dynamic cropping
where both the size of the crop and the position of the crop within the frame,
move as the video is played and saliency is tracked. Our results indicate that
especially for videos of low initial memorability, the memorability score can
be improved.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11883">On-the-Fly SfM: What you capture is What you get. (arXiv:2309.11883v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhan_Z/0/1/0/all/0/1">Zongqian Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_R/0/1/0/all/0/1">Rui Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yifei Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yibo Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xin Wang</a></p>
<p>Over the last decades, ample achievements have been made on Structure from
motion (SfM). However, the vast majority of them basically work in an offline
manner, i.e., images are firstly captured and then fed together into a SfM
pipeline for obtaining poses and sparse point cloud. In this work, on the
contrary, we present an on-the-fly SfM: running online SfM while image
capturing, the newly taken On-the-Fly image is online estimated with the
corresponding pose and points, i.e., what you capture is what you get.
Specifically, our approach firstly employs a vocabulary tree that is
unsupervised trained using learning-based global features for fast image
retrieval of newly fly-in image. Then, a robust feature matching mechanism with
least squares (LSM) is presented to improve image registration performance.
Finally, via investigating the influence of newly fly-in image's connected
neighboring images, an efficient hierarchical weighted local bundle adjustment
(BA) is used for optimization. Extensive experimental results demonstrate that
on-the-fly SfM can meet the goal of robustly registering the images while
capturing in an online way.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11891">Heart Rate Detection Using an Event Camera. (arXiv:2309.11891v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Jagtap_A/0/1/0/all/0/1">Aniket Jagtap</a>, <a href="http://arxiv.org/find/eess/1/au:+Saripalli_R/0/1/0/all/0/1">RamaKrishna Venkatesh Saripalli</a>, <a href="http://arxiv.org/find/eess/1/au:+Lemley_J/0/1/0/all/0/1">Joe Lemley</a>, <a href="http://arxiv.org/find/eess/1/au:+Shariff_W/0/1/0/all/0/1">Waseem Shariff</a>, <a href="http://arxiv.org/find/eess/1/au:+Smeaton_A/0/1/0/all/0/1">Alan F. Smeaton</a></p>
<p>Event cameras, also known as neuromorphic cameras, are an emerging technology
that offer advantages over traditional shutter and frame-based cameras,
including high temporal resolution, low power consumption, and selective data
acquisition. In this study, we propose to harnesses the capabilities of
event-based cameras to capture subtle changes in the surface of the skin caused
by the pulsatile flow of blood in the wrist region. We investigate whether an
event camera could be used for continuous noninvasive monitoring of heart rate
(HR). Event camera video data from 25 participants, comprising varying age
groups and skin colours, was collected and analysed. Ground-truth HR
measurements obtained using conventional methods were used to evaluate of the
accuracy of automatic detection of HR from event camera data. Our experimental
results and comparison to the performance of other non-contact HR measurement
methods demonstrate the feasibility of using event cameras for pulse detection.
We also acknowledge the challenges and limitations of our method, such as
light-induced flickering and the sub-conscious but naturally-occurring tremors
of an individual during data capture.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11899">Unlocking the Heart Using Adaptive Locked Agnostic Networks. (arXiv:2309.11899v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Majchrowska_S/0/1/0/all/0/1">Sylwia Majchrowska</a>, <a href="http://arxiv.org/find/cs/1/au:+Hildeman_A/0/1/0/all/0/1">Anders Hildeman</a>, <a href="http://arxiv.org/find/cs/1/au:+Teare_P/0/1/0/all/0/1">Philip Teare</a>, <a href="http://arxiv.org/find/cs/1/au:+Diethe_T/0/1/0/all/0/1">Tom Diethe</a></p>
<p>Supervised training of deep learning models for medical imaging applications
requires a significant amount of labeled data. This is posing a challenge as
the images are required to be annotated by medical professionals. To address
this limitation, we introduce the Adaptive Locked Agnostic Network (ALAN), a
concept involving self-supervised visual feature extraction using a large
backbone model to produce anatomically robust semantic self-segmentation. In
the ALAN methodology, this self-supervised training occurs only once on a large
and diverse dataset. Due to the intuitive interpretability of the segmentation,
downstream models tailored for specific tasks can be easily designed using
white-box models with few parameters. This, in turn, opens up the possibility
of communicating the inner workings of a model with domain experts and
introducing prior knowledge into it. It also means that the downstream models
become less data-hungry compared to fully supervised approaches. These
characteristics make ALAN particularly well-suited for resource-scarce
scenarios, such as costly clinical trials and rare diseases. In this paper, we
apply the ALAN approach to three publicly available echocardiography datasets:
EchoNet-Dynamic, CAMUS, and TMED-2. Our findings demonstrate that the
self-supervised backbone model robustly identifies anatomical subregions of the
heart in an apical four-chamber view. Building upon this, we design two
downstream models, one for segmenting a target anatomical region, and a second
for echocardiogram view classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11913">Spatial-Temporal Transformer based Video Compression Framework. (arXiv:2309.11913v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Gao_Y/0/1/0/all/0/1">Yanbo Gao</a>, <a href="http://arxiv.org/find/eess/1/au:+Huang_W/0/1/0/all/0/1">Wenjia Huang</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1">Shuai Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Yuan_H/0/1/0/all/0/1">Hui Yuan</a>, <a href="http://arxiv.org/find/eess/1/au:+Ye_M/0/1/0/all/0/1">Mao Ye</a>, <a href="http://arxiv.org/find/eess/1/au:+Ma_S/0/1/0/all/0/1">Siwei Ma</a></p>
<p>Learned video compression (LVC) has witnessed remarkable advancements in
recent years. Similar as the traditional video coding, LVC inherits motion
estimation/compensation, residual coding and other modules, all of which are
implemented with neural networks (NNs). However, within the framework of NNs
and its training mechanism using gradient backpropagation, most existing works
often struggle to consistently generate stable motion information, which is in
the form of geometric features, from the input color features. Moreover, the
modules such as the inter-prediction and residual coding are independent from
each other, making it inefficient to fully reduce the spatial-temporal
redundancy. To address the above problems, in this paper, we propose a novel
Spatial-Temporal Transformer based Video Compression (STT-VC) framework. It
contains a Relaxed Deformable Transformer (RDT) with Uformer based offsets
estimation for motion estimation and compensation, a Multi-Granularity
Prediction (MGP) module based on multi-reference frames for prediction
refinement, and a Spatial Feature Distribution prior based Transformer (SFD-T)
for efficient temporal-spatial joint residual compression. Specifically, RDT is
developed to stably estimate the motion information between frames by
thoroughly investigating the relationship between the similarity based
geometric motion feature extraction and self-attention. MGP is designed to fuse
the multi-reference frame information by effectively exploring the
coarse-grained prediction feature generated with the coded motion information.
SFD-T is to compress the residual information by jointly exploring the spatial
feature distributions in both residual and temporal prediction to further
reduce the spatial-temporal redundancy. Experimental results demonstrate that
our method achieves the best result with 13.5% BD-Rate saving over VTM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11923">TextCLIP: Text-Guided Face Image Generation And Manipulation Without Adversarial Training. (arXiv:2309.11923v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+You_X/0/1/0/all/0/1">Xiaozhou You</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jian Zhang</a></p>
<p>Text-guided image generation aimed to generate desired images conditioned on
given texts, while text-guided image manipulation refers to semantically edit
parts of a given image based on specified texts. For these two similar tasks,
the key point is to ensure image fidelity as well as semantic consistency. Many
previous approaches require complex multi-stage generation and adversarial
training, while struggling to provide a unified framework for both tasks. In
this work, we propose TextCLIP, a unified framework for text-guided image
generation and manipulation without adversarial training. The proposed method
accepts input from images or random noise corresponding to these two different
tasks, and under the condition of the specific texts, a carefully designed
mapping network that exploits the powerful generative capabilities of StyleGAN
and the text image representation capabilities of Contrastive Language-Image
Pre-training (CLIP) generates images of up to $1024\times1024$ resolution that
can currently be generated. Extensive experiments on the Multi-modal CelebA-HQ
dataset have demonstrated that our proposed method outperforms existing
state-of-the-art methods, both on text-guided generation tasks and manipulation
tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11928">Video Scene Location Recognition with Neural Networks. (arXiv:2309.11928v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Korel_L/0/1/0/all/0/1">Luk&#xe1;&#x161; Korel</a>, <a href="http://arxiv.org/find/cs/1/au:+Pulc_P/0/1/0/all/0/1">Petr Pulc</a>, <a href="http://arxiv.org/find/cs/1/au:+Tumpach_J/0/1/0/all/0/1">Ji&#x159;&#xed; Tumpach</a>, <a href="http://arxiv.org/find/cs/1/au:+Holena_M/0/1/0/all/0/1">Martin Hole&#x148;a</a></p>
<p>This paper provides an insight into the possibility of scene recognition from
a video sequence with a small set of repeated shooting locations (such as in
television series) using artificial neural networks. The basic idea of the
presented approach is to select a set of frames from each scene, transform them
by a pre-trained singleimage pre-processing convolutional network, and classify
the scene location with subsequent layers of the neural network. The considered
networks have been tested and compared on a dataset obtained from The Big Bang
Theory television series. We have investigated different neural network layers
to combine individual frames, particularly AveragePooling, MaxPooling, Product,
Flatten, LSTM, and Bidirectional LSTM layers. We have observed that only some
of the approaches are suitable for the task at hand.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11930">Bridging the Gap: Learning Pace Synchronization for Open-World Semi-Supervised Learning. (arXiv:2309.11930v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ye_B/0/1/0/all/0/1">Bo Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Gan_K/0/1/0/all/0/1">Kai Gan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1">Tong Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Min-Ling Zhang</a></p>
<p>In open-world semi-supervised learning, a machine learning model is tasked
with uncovering novel categories from unlabeled data while maintaining
performance on seen categories from labeled data. The central challenge is the
substantial learning gap between seen and novel categories, as the model learns
the former faster due to accurate supervisory information. To address this, we
introduce 1) an adaptive margin loss based on estimated class distribution,
which encourages a large negative margin for samples in seen classes, to
synchronize learning paces, and 2) pseudo-label contrastive clustering, which
pulls together samples which are likely from the same class in the output
space, to enhance novel class discovery. Our extensive evaluations on multiple
datasets demonstrate that existing models still hinder novel class learning,
whereas our approach strikingly balances both seen and novel classes, achieving
a remarkable 3% average accuracy increase on the ImageNet dataset compared to
the prior state-of-the-art. Additionally, we find that fine-tuning the
self-supervised pre-trained backbone significantly boosts performance over the
default in prior literature. After our paper is accepted, we will release the
code.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11933">Fully Transformer-Equipped Architecture for End-to-End Referring Video Object Segmentation. (arXiv:2309.11933v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Ping Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1">Li Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xianghua Xu</a></p>
<p>Referring Video Object Segmentation (RVOS) requires segmenting the object in
video referred by a natural language query. Existing methods mainly rely on
sophisticated pipelines to tackle such cross-modal task, and do not explicitly
model the object-level spatial context which plays an important role in
locating the referred object. Therefore, we propose an end-to-end RVOS
framework completely built upon transformers, termed \textit{Fully
Transformer-Equipped Architecture} (FTEA), which treats the RVOS task as a mask
sequence learning problem and regards all the objects in video as candidate
objects. Given a video clip with a text query, the visual-textual features are
yielded by encoder, while the corresponding pixel-level and word-level features
are aligned in terms of semantic similarity. To capture the object-level
spatial context, we have developed the Stacked Transformer, which individually
characterizes the visual appearance of each candidate object, whose feature map
is decoded to the binary mask sequence in order directly. Finally, the model
finds the best matching between mask sequence and text query. In addition, to
diversify the generated masks for candidate objects, we impose a diversity loss
on the model for capturing more accurate mask of the referred object. Empirical
studies have shown the superiority of the proposed method on three benchmarks,
e.g., FETA achieves 45.1% and 38.7% in terms of mAP on A2D Sentences (3782
videos) and J-HMDB Sentences (928 videos), respectively; it achieves 56.6% in
terms of $\mathcal{J\&amp;F}$ on Ref-YouTube-VOS (3975 videos and 7451 objects).
Particularly, compared to the best candidate method, it has a gain of 2.1% and
3.2% in terms of P$@$0.5 on the former two, respectively, while it has a gain
of 2.9% in terms of $\mathcal{J}$ on the latter one.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11955">A Study of Forward-Forward Algorithm for Self-Supervised Learning. (arXiv:2309.11955v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Brenig_J/0/1/0/all/0/1">Jonas Brenig</a>, <a href="http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1">Radu Timofte</a></p>
<p>Self-supervised representation learning has seen remarkable progress in the
last few years, with some of the recent methods being able to learn useful
image representations without labels. These methods are trained using
backpropagation, the de facto standard. Recently, Geoffrey Hinton proposed the
forward-forward algorithm as an alternative training method. It utilizes two
forward passes and a separate loss function for each layer to train the network
without backpropagation.
</p>
<p>In this study, for the first time, we study the performance of
forward-forward vs. backpropagation for self-supervised representation learning
and provide insights into the learned representation spaces. Our benchmark
employs four standard datasets, namely MNIST, F-MNIST, SVHN and CIFAR-10, and
three commonly used self-supervised representation learning techniques, namely
rotation, flip and jigsaw.
</p>
<p>Our main finding is that while the forward-forward algorithm performs
comparably to backpropagation during (self-)supervised training, the transfer
performance is significantly lagging behind in all the studied settings. This
may be caused by a combination of factors, including having a loss function for
each layer and the way the supervised training is realized in the
forward-forward paradigm. In comparison to backpropagation, the forward-forward
algorithm focuses more on the boundaries and drops part of the information
unnecessary for making decisions which harms the representation learning goal.
Further investigation and research are necessary to stabilize the
forward-forward strategy for self-supervised learning, to work beyond the
datasets and configurations demonstrated by Geoffrey Hinton.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11962">Ego3DPose: Capturing 3D Cues from Binocular Egocentric Views. (arXiv:2309.11962v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kang_T/0/1/0/all/0/1">Taeho Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kyungjin Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jinrui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1">Youngki Lee</a></p>
<p>We present Ego3DPose, a highly accurate binocular egocentric 3D pose
reconstruction system. The binocular egocentric setup offers practicality and
usefulness in various applications, however, it remains largely under-explored.
It has been suffering from low pose estimation accuracy due to viewing
distortion, severe self-occlusion, and limited field-of-view of the joints in
egocentric 2D images. Here, we notice that two important 3D cues, stereo
correspondences, and perspective, contained in the egocentric binocular input
are neglected. Current methods heavily rely on 2D image features, implicitly
learning 3D information, which introduces biases towards commonly observed
motions and leads to low overall accuracy. We observe that they not only fail
in challenging occlusion cases but also in estimating visible joint positions.
To address these challenges, we propose two novel approaches. First, we design
a two-path network architecture with a path that estimates pose per limb
independently with its binocular heatmaps. Without full-body information
provided, it alleviates bias toward trained full-body distribution. Second, we
leverage the egocentric view of body limbs, which exhibits strong perspective
variance (e.g., a significantly large-size hand when it is close to the
camera). We propose a new perspective-aware representation using trigonometry,
enabling the network to estimate the 3D orientation of limbs. Finally, we
develop an end-to-end pose reconstruction network that synergizes both
techniques. Our comprehensive evaluations demonstrate that Ego3DPose
outperforms state-of-the-art models by a pose estimation error (i.e., MPJPE)
reduction of 23.1% in the UnrealEgo dataset. Our qualitative results highlight
the superiority of our approach across a range of scenarios and challenges.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11966">NeuralLabeling: A versatile toolset for labeling vision datasets using Neural Radiance Fields. (arXiv:2309.11966v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Erich_F/0/1/0/all/0/1">Floris Erich</a>, <a href="http://arxiv.org/find/cs/1/au:+Chiba_N/0/1/0/all/0/1">Naoya Chiba</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoshiyasu_Y/0/1/0/all/0/1">Yusuke Yoshiyasu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ando_N/0/1/0/all/0/1">Noriaki Ando</a>, <a href="http://arxiv.org/find/cs/1/au:+Hanai_R/0/1/0/all/0/1">Ryo Hanai</a>, <a href="http://arxiv.org/find/cs/1/au:+Domae_Y/0/1/0/all/0/1">Yukiyasu Domae</a></p>
<p>We present NeuralLabeling, a labeling approach and toolset for annotating a
scene using either bounding boxes or meshes and generating segmentation masks,
affordance maps, 2D bounding boxes, 3D bounding boxes, 6DOF object poses, depth
maps and object meshes. NeuralLabeling uses Neural Radiance Fields (NeRF) as
renderer, allowing labeling to be performed using 3D spatial tools while
incorporating geometric clues such as occlusions, relying only on images
captured from multiple viewpoints as input. To demonstrate the applicability of
NeuralLabeling to a practical problem in robotics, we added ground truth depth
maps to 30000 frames of transparent object RGB and noisy depth maps of glasses
placed in a dishwasher captured using an RGBD sensor, yielding the
Dishwasher30k dataset. We show that training a simple deep neural network with
supervision using the annotated depth maps yields a higher reconstruction
performance than training with the previously applied weakly supervised
approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11986">ZS6D: Zero-shot 6D Object Pose Estimation using Vision Transformers. (arXiv:2309.11986v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ausserlechner_P/0/1/0/all/0/1">Philipp Ausserlechner</a>, <a href="http://arxiv.org/find/cs/1/au:+Haberger_D/0/1/0/all/0/1">David Haberger</a>, <a href="http://arxiv.org/find/cs/1/au:+Thalhammer_S/0/1/0/all/0/1">Stefan Thalhammer</a>, <a href="http://arxiv.org/find/cs/1/au:+Weibel_J/0/1/0/all/0/1">Jean-Baptiste Weibel</a>, <a href="http://arxiv.org/find/cs/1/au:+Vincze_M/0/1/0/all/0/1">Markus Vincze</a></p>
<p>As robotic systems increasingly encounter complex and unconstrained
real-world scenarios, there is a demand to recognize diverse objects. The
state-of-the-art 6D object pose estimation methods rely on object-specific
training and therefore do not generalize to unseen objects. Recent novel object
pose estimation methods are solving this issue using task-specific fine-tuned
CNNs for deep template matching. This adaptation for pose estimation still
requires expensive data rendering and training procedures. MegaPose for example
is trained on a dataset consisting of two million images showing 20,000
different objects to reach such generalization capabilities. To overcome this
shortcoming we introduce ZS6D, for zero-shot novel object 6D pose estimation.
Visual descriptors, extracted using pre-trained Vision Transformers (ViT), are
used for matching rendered templates against query images of objects and for
establishing local correspondences. These local correspondences enable deriving
geometric correspondences and are used for estimating the object's 6D pose with
RANSAC-based PnP. This approach showcases that the image descriptors extracted
by pre-trained ViTs are well-suited to achieve a notable improvement over two
state-of-the-art novel object 6D pose estimation methods, without the need for
task-specific fine-tuning. Experiments are performed on LMO, YCBV, and TLESS.
In comparison to one of the two methods we improve the Average Recall on all
three datasets and compared to the second method we improve on two datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11989">Crop Row Switching for Vision-Based Navigation: A Comprehensive Approach for Efficient Crop Field Navigation. (arXiv:2309.11989v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Silva_R/0/1/0/all/0/1">Rajitha de Silva</a>, <a href="http://arxiv.org/find/cs/1/au:+Cielniak_G/0/1/0/all/0/1">Grzegorz Cielniak</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Junfeng Gao</a></p>
<p>Vision-based mobile robot navigation systems in arable fields are mostly
limited to in-row navigation. The process of switching from one crop row to the
next in such systems is often aided by GNSS sensors or multiple camera setups.
This paper presents a novel vision-based crop row-switching algorithm that
enables a mobile robot to navigate an entire field of arable crops using a
single front-mounted camera. The proposed row-switching manoeuvre uses deep
learning-based RGB image segmentation and depth data to detect the end of the
crop row, and re-entry point to the next crop row which would be used in a
multi-state row switching pipeline. Each state of this pipeline use visual
feedback or wheel odometry of the robot to successfully navigate towards the
next crop row. The proposed crop row navigation pipeline was tested in a real
sugar beet field containing crop rows with discontinuities, varying light
levels, shadows and irregular headland surfaces. The robot could successfully
exit from one crop row and re-enter the next crop row using the proposed
pipeline with absolute median errors averaging at 19.25 cm and 6.77{\deg} for
linear and rotational steps of the proposed manoeuvre.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11993">Neural Stochastic Screened Poisson Reconstruction. (arXiv:2309.11993v1 [cs.GR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sellan_S/0/1/0/all/0/1">Silvia Sell&#xe1;n</a>, <a href="http://arxiv.org/find/cs/1/au:+Jacobson_A/0/1/0/all/0/1">Alec Jacobson</a></p>
<p>Reconstructing a surface from a point cloud is an underdetermined problem. We
use a neural network to study and quantify this reconstruction uncertainty
under a Poisson smoothness prior. Our algorithm addresses the main limitations
of existing work and can be fully integrated into the 3D scanning pipeline,
from obtaining an initial reconstruction to deciding on the next best sensor
position and updating the reconstruction upon capturing more data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11995">Identification of pneumonia on chest x-ray images through machine learning. (arXiv:2309.11995v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Roeder_E/0/1/0/all/0/1">Eduardo Augusto Roeder</a></p>
<p>Pneumonia is the leading infectious cause of infant death in the world. When
identified early, it is possible to alter the prognosis of the patient, one
could use imaging exams to help in the diagnostic confirmation. Performing and
interpreting the exams as soon as possible is vital for a good treatment, with
the most common exam for this pathology being chest X-ray. The objective of
this study was to develop a software that identify the presence or absence of
pneumonia in chest radiographs. The software was developed as a computational
model based on machine learning using transfer learning technique. For the
training process, images were collected from a database available online with
children's chest X-rays images taken at a hospital in China. After training,
the model was then exposed to new images, achieving relevant results on
identifying such pathology, reaching 98% sensitivity and 97.3% specificity for
the sample used for testing. It can be concluded that it is possible to develop
a software that identifies pneumonia in chest X-ray images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12009">Elevating Skeleton-Based Action Recognition with Efficient Multi-Modality Self-Supervision. (arXiv:2309.12009v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1">Yiping Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1">Kunyu Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Roitberg_A/0/1/0/all/0/1">Alina Roitberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiaming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1">Junwei Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1">Ruiping Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yufan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1">Kailun Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1">Rainer Stiefelhagen</a></p>
<p>Self-supervised representation learning for human action recognition has
developed rapidly in recent years. Most of the existing works are based on
skeleton data while using a multi-modality setup. These works overlooked the
differences in performance among modalities, which led to the propagation of
erroneous knowledge between modalities while only three fundamental modalities,
i.e., joints, bones, and motions are used, hence no additional modalities are
explored.
</p>
<p>In this work, we first propose an Implicit Knowledge Exchange Module (IKEM)
which alleviates the propagation of erroneous knowledge between low-performance
modalities. Then, we further propose three new modalities to enrich the
complementary information between modalities. Finally, to maintain efficiency
when introducing new modalities, we propose a novel teacher-student framework
to distill the knowledge from the secondary modalities into the mandatory
modalities considering the relationship constrained by anchors, positives, and
negatives, named relational cross-modality knowledge distillation. The
experimental results demonstrate the effectiveness of our approach, unlocking
the efficient use of skeleton-based multi-modality data. Source code will be
made publicly available at https://github.com/desehuileng0o0/IKEM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12010">Convolution and Attention Mixer for Synthetic Aperture Radar Image Change Detection. (arXiv:2309.12010v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1">Haopeng Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Lin_Z/0/1/0/all/0/1">Zijing Lin</a>, <a href="http://arxiv.org/find/eess/1/au:+Gao_F/0/1/0/all/0/1">Feng Gao</a>, <a href="http://arxiv.org/find/eess/1/au:+Dong_J/0/1/0/all/0/1">Junyu Dong</a>, <a href="http://arxiv.org/find/eess/1/au:+Du_Q/0/1/0/all/0/1">Qian Du</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1">Heng-Chao Li</a></p>
<p>Synthetic aperture radar (SAR) image change detection is a critical task and
has received increasing attentions in the remote sensing community. However,
existing SAR change detection methods are mainly based on convolutional neural
networks (CNNs), with limited consideration of global attention mechanism. In
this letter, we explore Transformer-like architecture for SAR change detection
to incorporate global attention. To this end, we propose a convolution and
attention mixer (CAMixer). First, to compensate the inductive bias for
Transformer, we combine self-attention with shift convolution in a parallel
way. The parallel design effectively captures the global semantic information
via the self-attention and performs local feature extraction through shift
convolution simultaneously. Second, we adopt a gating mechanism in the
feed-forward network to enhance the non-linear feature transformation. The
gating mechanism is formulated as the element-wise multiplication of two
parallel linear layers. Important features can be highlighted, leading to
high-quality representations against speckle noise. Extensive experiments
conducted on three SAR datasets verify the superior performance of the proposed
CAMixer. The source codes will be publicly available at
https://github.com/summitgao/CAMixer .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12022">Demystifying Visual Features of Movie Posters for Multi-Label Genre Identification. (arXiv:2309.12022v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nareti_U/0/1/0/all/0/1">Utsav Kumar Nareti</a>, <a href="http://arxiv.org/find/cs/1/au:+Adak_C/0/1/0/all/0/1">Chandranath Adak</a>, <a href="http://arxiv.org/find/cs/1/au:+Chattopadhyay_S/0/1/0/all/0/1">Soumi Chattopadhyay</a></p>
<p>In the film industry, movie posters have been an essential part of
advertising and marketing for many decades, and continue to play a vital role
even today in the form of digital posters through online, social media and OTT
platforms. Typically, movie posters can effectively promote and communicate the
essence of a film, such as its genre, visual style/ tone, vibe and storyline
cue/ theme, which are essential to attract potential viewers. Identifying the
genres of a movie often has significant practical applications in recommending
the film to target audiences. Previous studies on movie genre identification
are limited to subtitles, plot synopses, and movie scenes that are mostly
accessible after the movie release. Posters usually contain pre-release
implicit information to generate mass interest. In this paper, we work for
automated multi-label genre identification only from movie poster images,
without any aid of additional textual/meta-data information about movies, which
is one of the earliest attempts of its kind. Here, we present a deep
transformer network with a probabilistic module to identify the movie genres
exclusively from the poster. For experimental analysis, we procured 13882
number of posters of 13 genres from the Internet Movie Database (IMDb), where
our model performances were encouraging and even outperformed some major
contemporary architectures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12027">Precision in Building Extraction: Comparing Shallow and Deep Models using LiDAR Data. (arXiv:2309.12027v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sulaiman_M/0/1/0/all/0/1">Muhammad Sulaiman</a>, <a href="http://arxiv.org/find/cs/1/au:+Farmanbar_M/0/1/0/all/0/1">Mina Farmanbar</a>, <a href="http://arxiv.org/find/cs/1/au:+Belbachir_A/0/1/0/all/0/1">Ahmed Nabil Belbachir</a>, <a href="http://arxiv.org/find/cs/1/au:+Rong_C/0/1/0/all/0/1">Chunming Rong</a></p>
<p>Building segmentation is essential in infrastructure development, population
management, and geological observations. This article targets shallow models
due to their interpretable nature to assess the presence of LiDAR data for
supervised segmentation. The benchmark data used in this article are published
in NORA MapAI competition for deep learning model. Shallow models are compared
with deep learning models based on Intersection over Union (IoU) and Boundary
Intersection over Union (BIoU). In the proposed work, boundary masks from the
original mask are generated to improve the BIoU score, which relates to
building shapes' borderline. The influence of LiDAR data is tested by training
the model with only aerial images in task 1 and a combination of aerial and
LiDAR data in task 2 and then compared. shallow models outperform deep learning
models in IoU by 8% using aerial images (task 1) only and 2% in combined aerial
images and LiDAR data (task 2). In contrast, deep learning models show better
performance on BIoU scores. Boundary masks improve BIoU scores by 4% in both
tasks. Light Gradient-Boosting Machine (LightGBM) performs better than RF and
Extreme Gradient Boosting (XGBoost).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12029">Unveiling the Hidden Realm: Self-supervised Skeleton-based Action Recognition in Occluded Environments. (arXiv:2309.12029v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yifei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1">Kunyu Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Roitberg_A/0/1/0/all/0/1">Alina Roitberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Schneider_D/0/1/0/all/0/1">David Schneider</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiaming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1">Junwei Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1">Ruiping Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yufan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1">Kailun Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1">Rainer Stiefelhagen</a></p>
<p>To integrate action recognition methods into autonomous robotic systems, it
is crucial to consider adverse situations involving target occlusions. Such a
scenario, despite its practical relevance, is rarely addressed in existing
self-supervised skeleton-based action recognition methods. To empower robots
with the capacity to address occlusion, we propose a simple and effective
method. We first pre-train using occluded skeleton sequences, then use k-means
clustering (KMeans) on sequence embeddings to group semantically similar
samples. Next, we employ K-nearest-neighbor (KNN) to fill in missing skeleton
data based on the closest sample neighbors. Imputing incomplete skeleton
sequences to create relatively complete sequences as input provides significant
benefits to existing skeleton-based self-supervised models. Meanwhile, building
on the state-of-the-art Partial Spatio-Temporal Learning (PSTL), we introduce
an Occluded Partial Spatio-Temporal Learning (OPSTL) framework. This
enhancement utilizes Adaptive Spatial Masking (ASM) for better use of
high-quality, intact skeletons. The effectiveness of our imputation methods is
verified on the challenging occluded versions of the NTURGB+D 60 and NTURGB+D
120. The source code will be made publicly available at
https://github.com/cyfml/OPSTL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12033">Face Identity-Aware Disentanglement in StyleGAN. (arXiv:2309.12033v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Suwala_A/0/1/0/all/0/1">Adrian Suwa&#x142;a</a>, <a href="http://arxiv.org/find/cs/1/au:+Wojcik_B/0/1/0/all/0/1">Bartosz W&#xf3;jcik</a>, <a href="http://arxiv.org/find/cs/1/au:+Proszewska_M/0/1/0/all/0/1">Magdalena Proszewska</a>, <a href="http://arxiv.org/find/cs/1/au:+Tabor_J/0/1/0/all/0/1">Jacek Tabor</a>, <a href="http://arxiv.org/find/cs/1/au:+Spurek_P/0/1/0/all/0/1">Przemys&#x142;aw Spurek</a>, <a href="http://arxiv.org/find/cs/1/au:+Smieja_M/0/1/0/all/0/1">Marek &#x15a;mieja</a></p>
<p>Conditional GANs are frequently used for manipulating the attributes of face
images, such as expression, hairstyle, pose, or age. Even though the
state-of-the-art models successfully modify the requested attributes, they
simultaneously modify other important characteristics of the image, such as a
person's identity. In this paper, we focus on solving this problem by
introducing PluGeN4Faces, a plugin to StyleGAN, which explicitly disentangles
face attributes from a person's identity. Our key idea is to perform training
on images retrieved from movie frames, where a given person appears in various
poses and with different attributes. By applying a type of contrastive loss, we
encourage the model to group images of the same person in similar regions of
latent space. Our experiments demonstrate that the modifications of face
attributes performed by PluGeN4Faces are significantly less invasive on the
remaining characteristics of the image than in the existing state-of-the-art
models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12035">BASE: Probably a Better Approach to Multi-Object Tracking. (arXiv:2309.12035v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Larsen_M/0/1/0/all/0/1">Martin Vonheim Larsen</a>, <a href="http://arxiv.org/find/cs/1/au:+Rolfsjord_S/0/1/0/all/0/1">Sigmund Rolfsjord</a>, <a href="http://arxiv.org/find/cs/1/au:+Gusland_D/0/1/0/all/0/1">Daniel Gusland</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahlberg_J/0/1/0/all/0/1">J&#xf6;rgen Ahlberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Mathiassen_K/0/1/0/all/0/1">Kim Mathiassen</a></p>
<p>The field of visual object tracking is dominated by methods that combine
simple tracking algorithms and ad hoc schemes. Probabilistic tracking
algorithms, which are leading in other fields, are surprisingly absent from the
leaderboards. We found that accounting for distance in target kinematics,
exploiting detector confidence and modelling non-uniform clutter
characteristics is critical for a probabilistic tracker to work in visual
tracking. Previous probabilistic methods fail to address most or all these
aspects, which we believe is why they fall so far behind current
state-of-the-art (SOTA) methods (there are no probabilistic trackers in the
MOT17 top 100). To rekindle progress among probabilistic approaches, we propose
a set of pragmatic models addressing these challenges, and demonstrate how they
can be incorporated into a probabilistic framework. We present BASE (Bayesian
Approximation Single-hypothesis Estimator), a simple, performant and easily
extendible visual tracker, achieving state-of-the-art (SOTA) on MOT17 and
MOT20, without using Re-Id. Code will be made available at
https://github.com/ffi-no
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12042">Beyond Image Borders: Learning Feature Extrapolation for Unbounded Image Composition. (arXiv:2309.12042v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiaoyu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Ming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Junyi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shuai Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaotao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_L/0/1/0/all/0/1">Lei Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1">Wangmeng Zuo</a></p>
<p>For improving image composition and aesthetic quality, most existing methods
modulate the captured images by striking out redundant content near the image
borders. However, such image cropping methods are limited in the range of image
views. Some methods have been suggested to extrapolate the images and predict
cropping boxes from the extrapolated image. Nonetheless, the synthesized
extrapolated regions may be included in the cropped image, making the image
composition result not real and potentially with degraded image quality. In
this paper, we circumvent this issue by presenting a joint framework for both
unbounded recommendation of camera view and image composition (i.e., UNIC). In
this way, the cropped image is a sub-image of the image acquired by the
predicted camera view, and thus can be guaranteed to be real and consistent in
image quality. Specifically, our framework takes the current camera preview
frame as input and provides a recommendation for view adjustment, which
contains operations unlimited by the image borders, such as zooming in or out
and camera movement. To improve the prediction accuracy of view adjustment
prediction, we further extend the field of view by feature extrapolation. After
one or several times of view adjustments, our method converges and results in
both a camera view and a bounding box showing the image composition
recommendation. Extensive experiments are conducted on the datasets constructed
upon existing image cropping datasets, showing the effectiveness of our UNIC in
unbounded recommendation of camera view and image composition. The source code,
dataset, and pretrained models is available at
https://github.com/liuxiaoyu1104/UNIC.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12047">Self-Calibrating, Fully Differentiable NLOS Inverse Rendering. (arXiv:2309.12047v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Choi_K/0/1/0/all/0/1">Kiseok Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_I/0/1/0/all/0/1">Inchul Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1">Dongyoung Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Marco_J/0/1/0/all/0/1">Julio Marco</a>, <a href="http://arxiv.org/find/cs/1/au:+Gutierrez_D/0/1/0/all/0/1">Diego Gutierrez</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1">Min H. Kim</a></p>
<p>Existing time-resolved non-line-of-sight (NLOS) imaging methods reconstruct
hidden scenes by inverting the optical paths of indirect illumination measured
at visible relay surfaces. These methods are prone to reconstruction artifacts
due to inversion ambiguities and capture noise, which are typically mitigated
through the manual selection of filtering functions and parameters. We
introduce a fully-differentiable end-to-end NLOS inverse rendering pipeline
that self-calibrates the imaging parameters during the reconstruction of hidden
scenes, using as input only the measured illumination while working both in the
time and frequency domains. Our pipeline extracts a geometric representation of
the hidden scene from NLOS volumetric intensities and estimates the
time-resolved illumination at the relay wall produced by such geometric
information using differentiable transient rendering. We then use gradient
descent to optimize imaging parameters by minimizing the error between our
simulated time-resolved illumination and the measured illumination. Our
end-to-end differentiable pipeline couples diffraction-based volumetric NLOS
reconstruction with path-space light transport and a simple ray marching
technique to extract detailed, dense sets of surface points and normals of
hidden scenes. We demonstrate the robustness of our method to consistently
reconstruct geometry and albedo, even under significant noise levels.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12067">Survey of Action Recognition, Spotting and Spatio-Temporal Localization in Soccer -- Current Trends and Research Perspectives. (arXiv:2309.12067v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Seweryn_K/0/1/0/all/0/1">Karolina Seweryn</a>, <a href="http://arxiv.org/find/cs/1/au:+Wroblewska_A/0/1/0/all/0/1">Anna Wr&#xf3;blewska</a>, <a href="http://arxiv.org/find/cs/1/au:+Lukasik_S/0/1/0/all/0/1">Szymon &#x141;ukasik</a></p>
<p>Action scene understanding in soccer is a challenging task due to the complex
and dynamic nature of the game, as well as the interactions between players.
This article provides a comprehensive overview of this task divided into action
recognition, spotting, and spatio-temporal action localization, with a
particular emphasis on the modalities used and multimodal methods. We explore
the publicly available data sources and metrics used to evaluate models'
performance. The article reviews recent state-of-the-art methods that leverage
deep learning techniques and traditional methods. We focus on multimodal
methods, which integrate information from multiple sources, such as video and
audio data, and also those that represent one source in various ways. The
advantages and limitations of methods are discussed, along with their potential
for improving the accuracy and robustness of models. Finally, the article
highlights some of the open research questions and future directions in the
field of soccer action recognition, including the potential for multimodal
methods to advance this field. Overall, this survey provides a valuable
resource for researchers interested in the field of action scene understanding
in soccer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12090">Multi-Task Cooperative Learning via Searching for Flat Minima. (arXiv:2309.12090v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1">Fuping Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Le Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Mo_Y/0/1/0/all/0/1">Yuanhan Mo</a>, <a href="http://arxiv.org/find/cs/1/au:+Nichols_T/0/1/0/all/0/1">Thomas Nichols</a>, <a href="http://arxiv.org/find/cs/1/au:+Papiez_B/0/1/0/all/0/1">Bartlomiej W. Papiez</a></p>
<p>Multi-task learning (MTL) has shown great potential in medical image
analysis, improving the generalizability of the learned features and the
performance in individual tasks. However, most of the work on MTL focuses on
either architecture design or gradient manipulation, while in both scenarios,
features are learned in a competitive manner. In this work, we propose to
formulate MTL as a multi/bi-level optimization problem, and therefore force
features to learn from each task in a cooperative approach. Specifically, we
update the sub-model for each task alternatively taking advantage of the
learned sub-models of the other tasks. To alleviate the negative transfer
problem during the optimization, we search for flat minima for the current
objective function with regard to features from other tasks. To demonstrate the
effectiveness of the proposed approach, we validate our method on three
publicly available datasets. The proposed method shows the advantage of
cooperative learning, and yields promising results when compared with the
state-of-the-art MTL approaches. The code will be available online.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12095">Bayesian sparsification for deep neural networks with Bayesian model reduction. (arXiv:2309.12095v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Markovic_D/0/1/0/all/0/1">Dimitrije Markovi&#x107;</a>, <a href="http://arxiv.org/find/stat/1/au:+Friston_K/0/1/0/all/0/1">Karl J. Friston</a>, <a href="http://arxiv.org/find/stat/1/au:+Kiebel_S/0/1/0/all/0/1">Stefan J. Kiebel</a></p>
<p>Deep learning's immense capabilities are often constrained by the complexity
of its models, leading to an increasing demand for effective sparsification
techniques. Bayesian sparsification for deep learning emerges as a crucial
approach, facilitating the design of models that are both computationally
efficient and competitive in terms of performance across various deep learning
applications. The state-of-the-art -- in Bayesian sparsification of deep neural
networks -- combines structural shrinkage priors on model weights with an
approximate inference scheme based on black-box stochastic variational
inference. However, model inversion of the full generative model is
exceptionally computationally demanding, especially when compared to standard
deep learning of point estimates. In this context, we advocate for the use of
Bayesian model reduction (BMR) as a more efficient alternative for pruning of
model weights. As a generalization of the Savage-Dickey ratio, BMR allows a
post-hoc elimination of redundant model weights based on the posterior
estimates under a straightforward (non-hierarchical) generative model. Our
comparative study highlights the computational efficiency and the pruning rate
of the BMR method relative to the established stochastic variational inference
(SVI) scheme, when applied to the full hierarchical generative model. We
illustrate the potential of BMR to prune model parameters across various deep
learning architectures, from classical networks like LeNet to modern frameworks
such as Vision Transformers and MLP-Mixers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12106">FourierLoss: Shape-Aware Loss Function with Fourier Descriptors. (arXiv:2309.12106v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Erden_M/0/1/0/all/0/1">Mehmet Bahadir Erden</a>, <a href="http://arxiv.org/find/cs/1/au:+Cansiz_S/0/1/0/all/0/1">Selahattin Cansiz</a>, <a href="http://arxiv.org/find/cs/1/au:+Caki_O/0/1/0/all/0/1">Onur Caki</a>, <a href="http://arxiv.org/find/cs/1/au:+Khattak_H/0/1/0/all/0/1">Haya Khattak</a>, <a href="http://arxiv.org/find/cs/1/au:+Etiz_D/0/1/0/all/0/1">Durmus Etiz</a>, <a href="http://arxiv.org/find/cs/1/au:+Yakar_M/0/1/0/all/0/1">Melek Cosar Yakar</a>, <a href="http://arxiv.org/find/cs/1/au:+Duruer_K/0/1/0/all/0/1">Kerem Duruer</a>, <a href="http://arxiv.org/find/cs/1/au:+Barut_B/0/1/0/all/0/1">Berke Barut</a>, <a href="http://arxiv.org/find/cs/1/au:+Gunduz_Demir_C/0/1/0/all/0/1">Cigdem Gunduz-Demir</a></p>
<p>Encoder-decoder networks become a popular choice for various medical image
segmentation tasks. When they are trained with a standard loss function, these
networks are not explicitly enforced to preserve the shape integrity of an
object in an image. However, this ability of the network is important to obtain
more accurate results, especially when there is a low-contrast difference
between the object and its surroundings. In response to this issue, this work
introduces a new shape-aware loss function, which we name FourierLoss. This
loss function relies on quantifying the shape dissimilarity between the ground
truth and the predicted segmentation maps through the Fourier descriptors
calculated on their objects, and penalizing this dissimilarity in network
training. Different than the previous studies, FourierLoss offers an adaptive
loss function with trainable hyperparameters that control the importance of the
level of the shape details that the network is enforced to learn in the
training process. This control is achieved by the proposed adaptive loss update
mechanism, which end-to-end learns the hyperparameters simultaneously with the
network weights by backpropagation. As a result of using this mechanism, the
network can dynamically change its attention from learning the general outline
of an object to learning the details of its contour points, or vice versa, in
different training epochs. Working on 2879 computed tomography images of 93
subjects, our experiments revealed that the proposed adaptive shape-aware loss
function led to statistically significantly better results for liver
segmentation, compared to its counterparts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12110">Exploiting CLIP-based Multi-modal Approach for Artwork Classification and Retrieval. (arXiv:2309.12110v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Baldrati_A/0/1/0/all/0/1">Alberto Baldrati</a>, <a href="http://arxiv.org/find/cs/1/au:+Bertini_M/0/1/0/all/0/1">Marco Bertini</a>, <a href="http://arxiv.org/find/cs/1/au:+Uricchio_T/0/1/0/all/0/1">Tiberio Uricchio</a>, <a href="http://arxiv.org/find/cs/1/au:+Bimbo_A/0/1/0/all/0/1">Alberto Del Bimbo</a></p>
<p>Given the recent advances in multimodal image pretraining where visual models
trained with semantically dense textual supervision tend to have better
generalization capabilities than those trained using categorical attributes or
through unsupervised techniques, in this work we investigate how recent CLIP
model can be applied in several tasks in artwork domain. We perform exhaustive
experiments on the NoisyArt dataset which is a dataset of artwork images
crawled from public resources on the web. On such dataset CLIP achieves
impressive results on (zero-shot) classification and promising results in both
artwork-to-artwork and description-to-artwork domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12114">AutoPET Challenge 2023: Sliding Window-based Optimization of U-Net. (arXiv:2309.12114v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Hadlich_M/0/1/0/all/0/1">Matthias Hadlich</a>, <a href="http://arxiv.org/find/eess/1/au:+Marinov_Z/0/1/0/all/0/1">Zdravko Marinov</a>, <a href="http://arxiv.org/find/eess/1/au:+Stiefelhagen_R/0/1/0/all/0/1">Rainer Stiefelhagen</a></p>
<p>Tumor segmentation in medical imaging is crucial and relies on precise
delineation. Fluorodeoxyglucose Positron-Emission Tomography (FDG-PET) is
widely used in clinical practice to detect metabolically active tumors.
However, FDG-PET scans may misinterpret irregular glucose consumption in
healthy or benign tissues as cancer. Combining PET with Computed Tomography
(CT) can enhance tumor segmentation by integrating metabolic and anatomic
information. FDG-PET/CT scans are pivotal for cancer staging and reassessment,
utilizing radiolabeled fluorodeoxyglucose to highlight metabolically active
regions. Accurately distinguishing tumor-specific uptake from physiological
uptake in normal tissues is a challenging aspect of precise tumor segmentation.
The AutoPET challenge addresses this by providing a dataset of 1014 FDG-PET/CT
studies, encouraging advancements in accurate tumor segmentation and analysis
within the FDG-PET/CT domain. Code:
https://github.com/matt3o/AutoPET2-Submission/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12118">Vulnerability of 3D Face Recognition Systems to Morphing Attacks. (arXiv:2309.12118v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vardam_S/0/1/0/all/0/1">Sanjeet Vardam</a>, <a href="http://arxiv.org/find/cs/1/au:+Spreeuwers_L/0/1/0/all/0/1">Luuk Spreeuwers</a></p>
<p>In recent years face recognition systems have been brought to the mainstream
due to development in hardware and software. Consistent efforts are being made
to make them better and more secure. This has also brought developments in 3D
face recognition systems at a rapid pace. These 3DFR systems are expected to
overcome certain vulnerabilities of 2DFR systems. One such problem that the
domain of 2DFR systems face is face image morphing. A substantial amount of
research is being done for generation of high quality face morphs along with
detection of attacks from these morphs. Comparatively the understanding of
vulnerability of 3DFR systems against 3D face morphs is less. But at the same
time an expectation is set from 3DFR systems to be more robust against such
attacks. This paper attempts to research and gain more information on this
matter. The paper describes a couple of methods that can be used to generate 3D
face morphs. The face morphs that are generated using this method are then
compared to the contributing faces to obtain similarity scores. The highest
MMPMR is obtained around 40% with RMMR of 41.76% when 3DFRS are attacked with
look-a-like morphs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12140">Unsupervised Domain Adaptation for Self-Driving from Past Traversal Features. (arXiv:2309.12140v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Travis Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_K/0/1/0/all/0/1">Katie Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Phoo_C/0/1/0/all/0/1">Cheng Perng Phoo</a>, <a href="http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1">Yurong You</a>, <a href="http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1">Wei-Lun Chao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hariharan_B/0/1/0/all/0/1">Bharath Hariharan</a>, <a href="http://arxiv.org/find/cs/1/au:+Campbell_M/0/1/0/all/0/1">Mark Campbell</a>, <a href="http://arxiv.org/find/cs/1/au:+Weinberger_K/0/1/0/all/0/1">Kilian Q. Weinberger</a></p>
<p>The rapid development of 3D object detection systems for self-driving cars
has significantly improved accuracy. However, these systems struggle to
generalize across diverse driving environments, which can lead to
safety-critical failures in detecting traffic participants. To address this, we
propose a method that utilizes unlabeled repeated traversals of multiple
locations to adapt object detectors to new driving environments. By
incorporating statistics computed from repeated LiDAR scans, we guide the
adaptation process effectively. Our approach enhances LiDAR-based detection
models using spatial quantized historical features and introduces a lightweight
regression head to leverage the statistics for feature regularization.
Additionally, we leverage the statistics for a novel self-training process to
stabilize the training. The framework is detector model-agnostic and
experiments on real-world datasets demonstrate significant improvements,
achieving up to a 20-point performance gain, especially in detecting
pedestrians and distant objects. Code is available at
https://github.com/zhangtravis/Hist-DA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12159">Information Forensics and Security: A quarter-century-long journey. (arXiv:2309.12159v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Barni_M/0/1/0/all/0/1">Mauro Barni</a>, <a href="http://arxiv.org/find/cs/1/au:+Campisi_P/0/1/0/all/0/1">Patrizio Campisi</a>, <a href="http://arxiv.org/find/cs/1/au:+Delp_E/0/1/0/all/0/1">Edward J. Delp</a>, <a href="http://arxiv.org/find/cs/1/au:+Doerr_G/0/1/0/all/0/1">Gwenael Do&#xeb;rr</a>, <a href="http://arxiv.org/find/cs/1/au:+Fridrich_J/0/1/0/all/0/1">Jessica Fridrich</a>, <a href="http://arxiv.org/find/cs/1/au:+Memon_N/0/1/0/all/0/1">Nasir Memon</a>, <a href="http://arxiv.org/find/cs/1/au:+Perez_Gonzalez_F/0/1/0/all/0/1">Fernando P&#xe9;rez-Gonz&#xe1;lez</a>, <a href="http://arxiv.org/find/cs/1/au:+Rocha_A/0/1/0/all/0/1">Anderson Rocha</a>, <a href="http://arxiv.org/find/cs/1/au:+Verdoliva_L/0/1/0/all/0/1">Luisa Verdoliva</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1">Min Wu</a></p>
<p>Information Forensics and Security (IFS) is an active R&amp;D area whose goal is
to ensure that people use devices, data, and intellectual properties for
authorized purposes and to facilitate the gathering of solid evidence to hold
perpetrators accountable. For over a quarter century since the 1990s, the IFS
research area has grown tremendously to address the societal needs of the
digital information era. The IEEE Signal Processing Society (SPS) has emerged
as an important hub and leader in this area, and the article below celebrates
some landmark technical contributions. In particular, we highlight the major
technological advances on some selected focus areas in the field developed in
the last 25 years from the research community and present future trends.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12172">SANPO: A Scene Understanding, Accessibility, Navigation, Pathfinding, Obstacle Avoidance Dataset. (arXiv:2309.12172v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Waghmare_S/0/1/0/all/0/1">Sagar M. Waghmare</a>, <a href="http://arxiv.org/find/cs/1/au:+Wilber_K/0/1/0/all/0/1">Kimberly Wilber</a>, <a href="http://arxiv.org/find/cs/1/au:+Hawkey_D/0/1/0/all/0/1">Dave Hawkey</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xuan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wilson_M/0/1/0/all/0/1">Matthew Wilson</a>, <a href="http://arxiv.org/find/cs/1/au:+Debats_S/0/1/0/all/0/1">Stephanie Debats</a>, <a href="http://arxiv.org/find/cs/1/au:+Nuengsigkapian_C/0/1/0/all/0/1">Cattalyya Nuengsigkapian</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1">Astuti Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Pandikow_L/0/1/0/all/0/1">Lars Pandikow</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Huisheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Adam_H/0/1/0/all/0/1">Hartwig Adam</a>, <a href="http://arxiv.org/find/cs/1/au:+Sirotenko_M/0/1/0/all/0/1">Mikhail Sirotenko</a></p>
<p>We introduce SANPO, a large-scale egocentric video dataset focused on dense
prediction in outdoor environments. It contains stereo video sessions collected
across diverse outdoor environments, as well as rendered synthetic video
sessions. (Synthetic data was provided by Parallel Domain.) All sessions have
(dense) depth and odometry labels. All synthetic sessions and a subset of real
sessions have temporally consistent dense panoptic segmentation labels. To our
knowledge, this is the first human egocentric video dataset with both large
scale dense panoptic segmentation and depth annotations. In addition to the
dataset we also provide zero-shot baselines and SANPO benchmarks for future
research. We hope that the challenging nature of SANPO will help advance the
state-of-the-art in video segmentation, depth estimation, multi-task visual
modeling, and synthetic-to-real domain adaptation, while enabling human
navigation systems.
</p>
<p>SANPO is available here:
https://google-research-datasets.github.io/sanpo_dataset/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12179">Autoregressive Sign Language Production: A Gloss-Free Approach with Discrete Representations. (arXiv:2309.12179v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hwang_E/0/1/0/all/0/1">Eui Jun Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Huije Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Jong C. Park</a></p>
<p>Gloss-free Sign Language Production (SLP) offers a direct translation of
spoken language sentences into sign language, bypassing the need for gloss
intermediaries. This paper presents the Sign language Vector Quantization
Network, a novel approach to SLP that leverages Vector Quantization to derive
discrete representations from sign pose sequences. Our method, rooted in both
manual and non-manual elements of signing, supports advanced decoding methods
and integrates latent-level alignment for enhanced linguistic coherence.
Through comprehensive evaluations, we demonstrate superior performance of our
method over prior SLP methods and highlight the reliability of Back-Translation
and Fr\'echet Gesture Distance as evaluation metrics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12183">ORTexME: Occlusion-Robust Human Shape and Pose via Temporal Average Texture and Mesh Encoding. (arXiv:2309.12183v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1">Yu Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_R/0/1/0/all/0/1">Robby T. Tan</a></p>
<p>In 3D human shape and pose estimation from a monocular video, models trained
with limited labeled data cannot generalize well to videos with occlusion,
which is common in the wild videos. The recent human neural rendering
approaches focusing on novel view synthesis initialized by the off-the-shelf
human shape and pose methods have the potential to correct the initial human
shape. However, the existing methods have some drawbacks such as, erroneous in
handling occlusion, sensitive to inaccurate human segmentation, and ineffective
loss computation due to the non-regularized opacity field. To address these
problems, we introduce ORTexME, an occlusion-robust temporal method that
utilizes temporal information from the input video to better regularize the
occluded body parts. While our ORTexME is based on NeRF, to determine the
reliable regions for the NeRF ray sampling, we utilize our novel average
texture learning approach to learn the average appearance of a person, and to
infer a mask based on the average texture. In addition, to guide the
opacity-field updates in NeRF to suppress blur and noise, we propose the use of
human body mesh. The quantitative evaluation demonstrates that our method
achieves significant improvement on the challenging multi-person 3DPW dataset,
where our method achieves 1.8 P-MPJPE error reduction. The SOTA rendering-based
methods fail and enlarge the error up to 5.6 on the same dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12188">SG-Bot: Object Rearrangement via Coarse-to-Fine Robotic Imagination on Scene Graphs. (arXiv:2309.12188v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1">Guangyao Zhai</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1">Xiaoni Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1">Dianye Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Di_Y/0/1/0/all/0/1">Yan Di</a>, <a href="http://arxiv.org/find/cs/1/au:+Manhardt_F/0/1/0/all/0/1">Fabian Manhardt</a>, <a href="http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1">Federico Tombari</a>, <a href="http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1">Nassir Navab</a>, <a href="http://arxiv.org/find/cs/1/au:+Busam_B/0/1/0/all/0/1">Benjamin Busam</a></p>
<p>Object rearrangement is pivotal in robotic-environment interactions,
representing a significant capability in embodied AI. In this paper, we present
SG-Bot, a novel rearrangement framework that utilizes a coarse-to-fine scheme
with a scene graph as the scene representation. Unlike previous methods that
rely on either known goal priors or zero-shot large models, SG-Bot exemplifies
lightweight, real-time, and user-controllable characteristics, seamlessly
blending the consideration of commonsense knowledge with automatic generation
capabilities. SG-Bot employs a three-fold procedure--observation, imagination,
and execution--to adeptly address the task. Initially, objects are discerned
and extracted from a cluttered scene during the observation. These objects are
first coarsely organized and depicted within a scene graph, guided by either
commonsense or user-defined criteria. Then, this scene graph subsequently
informs a generative model, which forms a fine-grained goal scene considering
the shape information from the initial scene and object semantics. Finally, for
execution, the initial and envisioned goal scenes are matched to formulate
robotic action policies. Experimental results demonstrate that SG-Bot
outperforms competitors by a large margin.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12193">Brain Tumor Detection Using Deep Learning Approaches. (arXiv:2309.12193v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Misu_R/0/1/0/all/0/1">Razia Sultana Misu</a></p>
<p>Brain tumors are collections of abnormal cells that can develop into masses
or clusters. Because they have the potential to infiltrate other tissues, they
pose a risk to the patient. The main imaging technique used, MRI, may be able
to identify a brain tumor with accuracy. The fast development of Deep Learning
methods for use in computer vision applications has been facilitated by a vast
amount of training data and improvements in model construction that offer
better approximations in a supervised setting. The need for these approaches
has been the main driver of this expansion. Deep learning methods have shown
promise in improving the precision of brain tumor detection and classification
using magnetic resonance imaging (MRI). The study on the use of deep learning
techniques, especially ResNet50, for brain tumor identification is presented in
this abstract. As a result, this study investigates the possibility of
automating the detection procedure using deep learning techniques. In this
study, I utilized five transfer learning models which are VGG16, VGG19,
DenseNet121, ResNet50 and YOLO V4 where ResNet50 provide the best or highest
accuracy 99.54%. The goal of the study is to guide researchers and medical
professionals toward powerful brain tumor detecting systems by employing deep
learning approaches by way of this evaluation and analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12214">Can We Reliably Improve the Robustness to Image Acquisition of Remote Sensing of PV Systems?. (arXiv:2309.12214v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kasmi_G/0/1/0/all/0/1">Gabriel Kasmi</a>, <a href="http://arxiv.org/find/cs/1/au:+Dubus_L/0/1/0/all/0/1">Laurent Dubus</a>, <a href="http://arxiv.org/find/cs/1/au:+Saint_Drenan_Y/0/1/0/all/0/1">Yves-Marie Saint-Drenan</a>, <a href="http://arxiv.org/find/cs/1/au:+Blanc_P/0/1/0/all/0/1">Philippe Blanc</a></p>
<p>Photovoltaic (PV) energy is crucial for the decarbonization of energy
systems. Due to the lack of centralized data, remote sensing of rooftop PV
installations is the best option to monitor the evolution of the rooftop PV
installed fleet at a regional scale. However, current techniques lack
reliability and are notably sensitive to shifts in the acquisition conditions.
To overcome this, we leverage the wavelet scale attribution method (WCAM),
which decomposes a model's prediction in the space-scale domain. The WCAM
enables us to assess on which scales the representation of a PV model rests and
provides insights to derive methods that improve the robustness to acquisition
conditions, thus increasing trust in deep learning systems to encourage their
use for the safe integration of clean energy in electric systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12245">Adaptive Input-image Normalization for Solving Mode Collapse Problem in GAN-based X-ray Images. (arXiv:2309.12245v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Saad_M/0/1/0/all/0/1">Muhammad Muneeb Saad</a>, <a href="http://arxiv.org/find/eess/1/au:+Rehmani_M/0/1/0/all/0/1">Mubashir Husain Rehmani</a>, <a href="http://arxiv.org/find/eess/1/au:+OReilly_R/0/1/0/all/0/1">Ruairi O&#x27;Reilly</a></p>
<p>Biomedical image datasets can be imbalanced due to the rarity of targeted
diseases. Generative Adversarial Networks play a key role in addressing this
imbalance by enabling the generation of synthetic images to augment datasets.
It is important to generate synthetic images that incorporate a diverse range
of features to accurately represent the distribution of features present in the
training imagery. Furthermore, the absence of diverse features in synthetic
images can degrade the performance of machine learning classifiers. The mode
collapse problem impacts Generative Adversarial Networks' capacity to generate
diversified images. Mode collapse comes in two varieties: intra-class and
inter-class. In this paper, both varieties of the mode collapse problem are
investigated, and their subsequent impact on the diversity of synthetic X-ray
images is evaluated. This work contributes an empirical demonstration of the
benefits of integrating the adaptive input-image normalization with the Deep
Convolutional GAN and Auxiliary Classifier GAN to alleviate the mode collapse
problems. Synthetically generated images are utilized for data augmentation and
training a Vision Transformer model. The classification performance of the
model is evaluated using accuracy, recall, and precision scores. Results
demonstrate that the DCGAN and the ACGAN with adaptive input-image
normalization outperform the DCGAN and ACGAN with un-normalized X-ray images as
evidenced by the superior diversity scores and classification scores.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12295">Learning to Drive Anywhere. (arXiv:2309.12295v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1">Ruizhao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1">Peng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ohn_Bar_E/0/1/0/all/0/1">Eshed Ohn-Bar</a>, <a href="http://arxiv.org/find/cs/1/au:+Saligrama_V/0/1/0/all/0/1">Venkatesh Saligrama</a></p>
<p>Human drivers can seamlessly adapt their driving decisions across
geographical locations with diverse conditions and rules of the road, e.g.,
left vs. right-hand traffic. In contrast, existing models for autonomous
driving have been thus far only deployed within restricted operational domains,
i.e., without accounting for varying driving behaviors across locations or
model scalability. In this work, we propose AnyD, a single geographically-aware
conditional imitation learning (CIL) model that can efficiently learn from
heterogeneous and globally distributed data with dynamic environmental,
traffic, and social characteristics. Our key insight is to introduce a
high-capacity geo-location-based channel attention mechanism that effectively
adapts to local nuances while also flexibly modeling similarities among regions
in a data-driven manner. By optimizing a contrastive imitation objective, our
proposed approach can efficiently scale across inherently imbalanced data
distributions and location-dependent events. We demonstrate the benefits of our
AnyD agent across multiple datasets, cities, and scalable deployment paradigms,
i.e., centralized, semi-supervised, and distributed agent training.
Specifically, AnyD outperforms CIL baselines by over 14% in open-loop
evaluation and 30% in closed-loop testing on CARLA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12300">See to Touch: Learning Tactile Dexterity through Visual Incentives. (arXiv:2309.12300v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guzey_I/0/1/0/all/0/1">Irmak Guzey</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1">Yinlong Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Evans_B/0/1/0/all/0/1">Ben Evans</a>, <a href="http://arxiv.org/find/cs/1/au:+Chintala_S/0/1/0/all/0/1">Soumith Chintala</a>, <a href="http://arxiv.org/find/cs/1/au:+Pinto_L/0/1/0/all/0/1">Lerrel Pinto</a></p>
<p>Equipping multi-fingered robots with tactile sensing is crucial for achieving
the precise, contact-rich, and dexterous manipulation that humans excel at.
However, relying solely on tactile sensing fails to provide adequate cues for
reasoning about objects' spatial configurations, limiting the ability to
correct errors and adapt to changing situations. In this paper, we present
Tactile Adaptation from Visual Incentives (TAVI), a new framework that enhances
tactile-based dexterity by optimizing dexterous policies using vision-based
rewards. First, we use a contrastive-based objective to learn visual
representations. Next, we construct a reward function using these visual
representations through optimal-transport based matching on one human
demonstration. Finally, we use online reinforcement learning on our robot to
optimize tactile-based policies that maximize the visual reward. On six
challenging tasks, such as peg pick-and-place, unstacking bowls, and flipping
slender objects, TAVI achieves a success rate of 73% using our four-fingered
Allegro robot hand. The increase in performance is 108% higher than policies
using tactile and vision-based rewards and 135% higher than policies without
tactile observational input. Robot videos are best viewed on our project
website: https://see-to-touch.github.io/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12301">Environment-biased Feature Ranking for Novelty Detection Robustness. (arXiv:2309.12301v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Smeu_S/0/1/0/all/0/1">Stefan Smeu</a>, <a href="http://arxiv.org/find/cs/1/au:+Burceanu_E/0/1/0/all/0/1">Elena Burceanu</a>, <a href="http://arxiv.org/find/cs/1/au:+Haller_E/0/1/0/all/0/1">Emanuela Haller</a>, <a href="http://arxiv.org/find/cs/1/au:+Nicolicioiu_A/0/1/0/all/0/1">Andrei Liviu Nicolicioiu</a></p>
<p>We tackle the problem of robust novelty detection, where we aim to detect
novelties in terms of semantic content while being invariant to changes in
other, irrelevant factors. Specifically, we operate in a setup with multiple
environments, where we determine the set of features that are associated more
with the environments, rather than to the content relevant for the task. Thus,
we propose a method that starts with a pretrained embedding and a multi-env
setup and manages to rank the features based on their environment-focus. First,
we compute a per-feature score based on the feature distribution variance
between envs. Next, we show that by dropping the highly scored ones, we manage
to remove spurious correlations and improve the overall performance by up to
6%, both in covariance and sub-population shift cases, both for a real and a
synthetic benchmark, that we introduce for this task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12302">Text-Guided Vector Graphics Customization. (arXiv:2309.12302v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Peiying Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_N/0/1/0/all/0/1">Nanxuan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1">Jing Liao</a></p>
<p>Vector graphics are widely used in digital art and valued by designers for
their scalability and layer-wise topological properties. However, the creation
and editing of vector graphics necessitate creativity and design expertise,
leading to a time-consuming process. In this paper, we propose a novel pipeline
that generates high-quality customized vector graphics based on textual prompts
while preserving the properties and layer-wise information of a given exemplar
SVG. Our method harnesses the capabilities of large pre-trained text-to-image
models. By fine-tuning the cross-attention layers of the model, we generate
customized raster images guided by textual prompts. To initialize the SVG, we
introduce a semantic-based path alignment method that preserves and transforms
crucial paths from the exemplar SVG. Additionally, we optimize path parameters
using both image-level and vector-level losses, ensuring smooth shape
deformation while aligning with the customized raster image. We extensively
evaluate our method using multiple metrics from vector-level, image-level, and
text-level perspectives. The evaluation results demonstrate the effectiveness
of our pipeline in generating diverse customizations of vector graphics with
exceptional quality. The project page is
https://intchous.github.io/SVGCustomization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12303">PanoVOS:Bridging Non-panoramic and Panoramic Views with Transformer for Video Segmentation. (arXiv:2309.12303v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1">Shilin Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiaohao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1">Lingyi Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wenchao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenqiang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wei Zhang</a></p>
<p>Panoramic videos contain richer spatial information and have attracted
tremendous amounts of attention due to their exceptional experience in some
fields such as autonomous driving and virtual reality. However, existing
datasets for video segmentation only focus on conventional planar images. To
address the challenge, in this paper, we present a panoramic video dataset,
PanoVOS. The dataset provides 150 videos with high video resolutions and
diverse motions. To quantify the domain gap between 2D planar videos and
panoramic videos, we evaluate 15 off-the-shelf video object segmentation (VOS)
models on PanoVOS. Through error analysis, we found that all of them fail to
tackle pixel-level content discontinues of panoramic videos. Thus, we present a
Panoramic Space Consistency Transformer (PSCFormer), which can effectively
utilize the semantic boundary information of the previous frame for pixel-level
matching with the current frame. Extensive experiments demonstrate that
compared with the previous SOTA models, our PSCFormer network exhibits a great
advantage in terms of segmentation results under the panoramic setting. Our
dataset poses new challenges in panoramic VOS and we hope that our PanoVOS can
advance the development of panoramic segmentation/tracking.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12304">SlowFast Network for Continuous Sign Language Recognition. (arXiv:2309.12304v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ahn_J/0/1/0/all/0/1">Junseok Ahn</a>, <a href="http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1">Youngjoon Jang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1">Joon Son Chung</a></p>
<p>The objective of this work is the effective extraction of spatial and dynamic
features for Continuous Sign Language Recognition (CSLR). To accomplish this,
we utilise a two-pathway SlowFast network, where each pathway operates at
distinct temporal resolutions to separately capture spatial (hand shapes,
facial expressions) and dynamic (movements) information. In addition, we
introduce two distinct feature fusion methods, carefully designed for the
characteristics of CSLR: (1) Bi-directional Feature Fusion (BFF), which
facilitates the transfer of dynamic semantics into spatial semantics and vice
versa; and (2) Pathway Feature Enhancement (PFE), which enriches dynamic and
spatial representations through auxiliary subnetworks, while avoiding the need
for extra inference time. As a result, our model further strengthens spatial
and dynamic representations in parallel. We demonstrate that the proposed
framework outperforms the current state-of-the-art performance on popular CSLR
datasets, including PHOENIX14, PHOENIX14-T, and CSL-Daily.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12306">TalkNCE: Improving Active Speaker Detection with Talk-Aware Contrastive Learning. (arXiv:2309.12306v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jung_C/0/1/0/all/0/1">Chaeyoung Jung</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Suyeon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Nam_K/0/1/0/all/0/1">Kihyun Nam</a>, <a href="http://arxiv.org/find/cs/1/au:+Rho_K/0/1/0/all/0/1">Kyeongha Rho</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">You Jin Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1">Youngjoon Jang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1">Joon Son Chung</a></p>
<p>The goal of this work is Active Speaker Detection (ASD), a task to determine
whether a person is speaking or not in a series of video frames. Previous works
have dealt with the task by exploring network architectures while learning
effective representations has been less explored. In this work, we propose
TalkNCE, a novel talk-aware contrastive loss. The loss is only applied to part
of the full segments where a person on the screen is actually speaking. This
encourages the model to learn effective representations through the natural
correspondence of speech and facial movements. Our loss can be jointly
optimized with the existing objectives for training ASD models without the need
for additional supervision or training data. The experiments demonstrate that
our loss can be easily integrated into the existing ASD frameworks, improving
their performance. Our method achieves state-of-the-art performances on
AVA-ActiveSpeaker and ASW datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12311">LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent. (arXiv:2309.12311v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jianing Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xuweiyi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1">Shengyi Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Madaan_N/0/1/0/all/0/1">Nikhil Madaan</a>, <a href="http://arxiv.org/find/cs/1/au:+Iyengar_M/0/1/0/all/0/1">Madhavan Iyengar</a>, <a href="http://arxiv.org/find/cs/1/au:+Fouhey_D/0/1/0/all/0/1">David F. Fouhey</a>, <a href="http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1">Joyce Chai</a></p>
<p>3D visual grounding is a critical skill for household robots, enabling them
to navigate, manipulate objects, and answer questions based on their
environment. While existing approaches often rely on extensive labeled data or
exhibit limitations in handling complex language queries, we propose
LLM-Grounder, a novel zero-shot, open-vocabulary, Large Language Model
(LLM)-based 3D visual grounding pipeline. LLM-Grounder utilizes an LLM to
decompose complex natural language queries into semantic constituents and
employs a visual grounding tool, such as OpenScene or LERF, to identify objects
in a 3D scene. The LLM then evaluates the spatial and commonsense relations
among the proposed objects to make a final grounding decision. Our method does
not require any labeled training data and can generalize to novel 3D scenes and
arbitrary text queries. We evaluate LLM-Grounder on the ScanRefer benchmark and
demonstrate state-of-the-art zero-shot grounding accuracy. Our findings
indicate that LLMs significantly improve the grounding capability, especially
for complex language queries, making LLM-Grounder an effective approach for 3D
vision-language tasks in robotics. Videos and interactive demos can be found on
the project website https://chat-with-nerf.github.io/ .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12312">ForceSight: Text-Guided Mobile Manipulation with Visual-Force Goals. (arXiv:2309.12312v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Collins_J/0/1/0/all/0/1">Jeremy A. Collins</a>, <a href="http://arxiv.org/find/cs/1/au:+Houff_C/0/1/0/all/0/1">Cody Houff</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1">You Liang Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kemp_C/0/1/0/all/0/1">Charles C. Kemp</a></p>
<p>We present ForceSight, a system for text-guided mobile manipulation that
predicts visual-force goals using a deep neural network. Given a single RGBD
image combined with a text prompt, ForceSight determines a target end-effector
pose in the camera frame (kinematic goal) and the associated forces (force
goal). Together, these two components form a visual-force goal. Prior work has
demonstrated that deep models outputting human-interpretable kinematic goals
can enable dexterous manipulation by real robots. Forces are critical to
manipulation, yet have typically been relegated to lower-level execution in
these systems. When deployed on a mobile manipulator equipped with an
eye-in-hand RGBD camera, ForceSight performed tasks such as precision grasps,
drawer opening, and object handovers with an 81% success rate in unseen
environments with object instances that differed significantly from the
training data. In a separate experiment, relying exclusively on visual servoing
and ignoring force goals dropped the success rate from 90% to 45%,
demonstrating that force goals can significantly enhance performance. The
appendix, videos, code, and trained models are available at
https://force-sight.github.io/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12314">TinyCLIP: CLIP Distillation via Affinity Mimicking and Weight Inheritance. (arXiv:2309.12314v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1">Kan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1">Houwen Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhenghong Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1">Bin Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Mengchen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1">Lu Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xuan_H/0/1/0/all/0/1">Hong Xuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Valenzuela_M/0/1/0/all/0/1">Michael Valenzuela</a>, Xi (Stephen) <a href="http://arxiv.org/find/cs/1/au:+Chen/0/1/0/all/0/1">Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinggang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chao_H/0/1/0/all/0/1">Hongyang Chao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1">Han Hu</a></p>
<p>In this paper, we propose a novel cross-modal distillation method, called
TinyCLIP, for large-scale language-image pre-trained models. The method
introduces two core techniques: affinity mimicking and weight inheritance.
Affinity mimicking explores the interaction between modalities during
distillation, enabling student models to mimic teachers' behavior of learning
cross-modal feature alignment in a visual-linguistic affinity space. Weight
inheritance transmits the pre-trained weights from the teacher models to their
student counterparts to improve distillation efficiency. Moreover, we extend
the method into a multi-stage progressive distillation to mitigate the loss of
informative weights during extreme compression. Comprehensive experiments
demonstrate the efficacy of TinyCLIP, showing that it can reduce the size of
the pre-trained CLIP ViT-B/32 by 50%, while maintaining comparable zero-shot
performance. While aiming for comparable performance, distillation with weight
inheritance can speed up the training by 1.4 - 7.8 $\times$ compared to
training from scratch. Moreover, our TinyCLIP ViT-8M/16, trained on YFCC-15M,
achieves an impressive zero-shot top-1 accuracy of 41.1% on ImageNet,
surpassing the original CLIP ViT-B/16 by 3.5% while utilizing only 8.9%
parameters. Finally, we demonstrate the good transferability of TinyCLIP in
various downstream tasks. Code and models will be open-sourced at
https://aka.ms/tinyclip.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12315">Active Stereo Without Pattern Projector. (arXiv:2309.12315v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bartolomei_L/0/1/0/all/0/1">Luca Bartolomei</a>, <a href="http://arxiv.org/find/cs/1/au:+Poggi_M/0/1/0/all/0/1">Matteo Poggi</a>, <a href="http://arxiv.org/find/cs/1/au:+Tosi_F/0/1/0/all/0/1">Fabio Tosi</a>, <a href="http://arxiv.org/find/cs/1/au:+Conti_A/0/1/0/all/0/1">Andrea Conti</a>, <a href="http://arxiv.org/find/cs/1/au:+Mattoccia_S/0/1/0/all/0/1">Stefano Mattoccia</a></p>
<p>This paper proposes a novel framework integrating the principles of active
stereo in standard passive camera systems without a physical pattern projector.
We virtually project a pattern over the left and right images according to the
sparse measurements obtained from a depth sensor. Any such devices can be
seamlessly plugged into our framework, allowing for the deployment of a virtual
active stereo setup in any possible environment, overcoming the limitation of
pattern projectors, such as limited working range or environmental conditions.
Experiments on indoor/outdoor datasets, featuring both long and close-range,
support the seamless effectiveness of our approach, boosting the accuracy of
both stereo algorithms and deep networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2108.05015">VisEvent: Reliable Object Tracking via Collaboration of Frame and Event Flows. (arXiv:2108.05015v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jianing Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1">Lin Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhipeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhe Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yaowei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yonghong Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1">Feng Wu</a></p>
<p>Different from visible cameras which record intensity images frame by frame,
the biologically inspired event camera produces a stream of asynchronous and
sparse events with much lower latency. In practice, visible cameras can better
perceive texture details and slow motion, while event cameras can be free from
motion blurs and have a larger dynamic range which enables them to work well
under fast motion and low illumination. Therefore, the two sensors can
cooperate with each other to achieve more reliable object tracking. In this
work, we propose a large-scale Visible-Event benchmark (termed VisEvent) due to
the lack of a realistic and scaled dataset for this task. Our dataset consists
of 820 video pairs captured under low illumination, high speed, and background
clutter scenarios, and it is divided into a training and a testing subset, each
of which contains 500 and 320 videos, respectively. Based on VisEvent, we
transform the event flows into event images and construct more than 30 baseline
methods by extending current single-modality trackers into dual-modality
versions. More importantly, we further build a simple but effective tracking
algorithm by proposing a cross-modality transformer, to achieve more effective
feature fusion between visible and event data. Extensive experiments on the
proposed VisEvent dataset, FE108, COESOT, and two simulated datasets (i.e.,
OTB-DVS and VOT-DVS), validated the effectiveness of our model. The dataset and
source code have been released on:
\url{https://github.com/wangxiao5791509/VisEvent_SOT_Benchmark}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.14985">THE Benchmark: Transferable Representation Learning for Monocular Height Estimation. (arXiv:2112.14985v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1">Zhitong Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1">Wei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1">Jingtao Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiao Xiang Zhu</a></p>
<p>Generating 3D city models rapidly is crucial for many applications. Monocular
height estimation is one of the most efficient and timely ways to obtain
large-scale geometric information. However, existing works focus primarily on
training and testing models using unbiased datasets, which does not align well
with real-world applications. Therefore, we propose a new benchmark dataset to
study the transferability of height estimation models in a cross-dataset
setting. To this end, we first design and construct a large-scale benchmark
dataset for cross-dataset transfer learning on the height estimation task. This
benchmark dataset includes a newly proposed large-scale synthetic dataset, a
newly collected real-world dataset, and four existing datasets from different
cities. Next, a new experimental protocol, few-shot cross-dataset transfer, is
designed. Furthermore, in this paper, we propose a scale-deformable convolution
module to enhance the window-based Transformer for handling the scale-variation
problem in the height estimation task. Experimental results have demonstrated
the effectiveness of the proposed methods in the traditional and cross-dataset
transfer settings. The datasets and codes are publicly available at
https://mediatum.ub.tum.<a href="/abs/de/1662763">de/1662763</a> and https://thebenchmarkh.github.io/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.00164">Distilled Low Rank Neural Radiance Field with Quantization for Light Field Compression. (arXiv:2208.00164v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1">Jinglei Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Guillemot_C/0/1/0/all/0/1">Christine Guillemot</a></p>
<p>We propose in this paper a Quantized Distilled Low-Rank Neural Radiance Field
(QDLR-NeRF) representation for the task of light field compression. While
existing compression methods encode the set of light field sub-aperture images,
our proposed method learns an implicit scene representation in the form of a
Neural Radiance Field (NeRF), which also enables view synthesis. To reduce its
size, the model is first learned under a Low-Rank (LR) constraint using a
Tensor Train (TT) decomposition within an Alternating Direction Method of
Multipliers (ADMM) optimization framework. To further reduce the model's size,
the components of the tensor train decomposition need to be quantized. However,
simultaneously considering the optimization of the NeRF model with both the
low-rank constraint and rate-constrained weight quantization is challenging. To
address this difficulty, we introduce a network distillation operation that
separates the low-rank approximation and the weight quantization during network
training. The information from the initial LR-constrained NeRF (LR-NeRF) is
distilled into a model of much smaller dimension (DLR-NeRF) based on the TT
decomposition of the LR-NeRF. We then learn an optimized global codebook to
quantize all TT components, producing the final QDLR-NeRF. Experimental results
show that our proposed method yields better compression efficiency compared to
state-of-the-art methods, and it additionally has the advantage of allowing the
synthesis of any light field view with high quality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.03392">Federated Learning for Medical Applications: A Taxonomy, Current Trends, Challenges, and Future Research Directions. (arXiv:2208.03392v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rauniyar_A/0/1/0/all/0/1">Ashish Rauniyar</a>, <a href="http://arxiv.org/find/cs/1/au:+Hagos_D/0/1/0/all/0/1">Desta Haileselassie Hagos</a>, <a href="http://arxiv.org/find/cs/1/au:+Jha_D/0/1/0/all/0/1">Debesh Jha</a>, <a href="http://arxiv.org/find/cs/1/au:+Haakegaard_J/0/1/0/all/0/1">Jan Erik H&#xe5;keg&#xe5;rd</a>, <a href="http://arxiv.org/find/cs/1/au:+Bagci_U/0/1/0/all/0/1">Ulas Bagci</a>, <a href="http://arxiv.org/find/cs/1/au:+Rawat_D/0/1/0/all/0/1">Danda B. Rawat</a>, <a href="http://arxiv.org/find/cs/1/au:+Vlassov_V/0/1/0/all/0/1">Vladimir Vlassov</a></p>
<p>With the advent of the IoT, AI and ML/DL algorithms, the landscape of
data-driven medical applications has emerged as a promising avenue for
designing robust and scalable diagnostic and prognostic models from medical
data. Consequently, the realm of data-driven medical applications has garnered
significant attention spanning academia and industry, ushering in marked
enhancements in healthcare delivery quality. Despite these strides, the
adoption of AI-driven medical applications remains hindered by formidable
challenges, including the arduous task of meeting security, privacy, and
quality of service (QoS) standards. Recent developments in federated learning
have made it possible to train complex machine-learned models in a distributed
manner and has become an active research domain, particularly processing the
medical data at the edge of the network in a decentralized way to preserve
privacy and address security concerns. To this end, this survey paper
highlights the current and future of FL technology in medical applications
where data sharing is a significant burden. We delve into the contemporary
research trends and their outcomes, unravelling the intricacies of designing
reliable and scalable FL models. Our survey outlines the foundational
statistical predicaments of FL, confronts device-related obstacles, delves into
security challenges, and navigates the intricate terrain of privacy concerns,
all while spotlighting its transformative potential within the medical domain.
A primary focus of our study rests on medical applications, where we underscore
the weighty burden of global cancer and illuminate the potency of FL in
engendering computer-aided diagnosis tools that address this challenge with
heightened efficacy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.05070">mmBody Benchmark: 3D Body Reconstruction Dataset and Analysis for Millimeter Wave Radar. (arXiv:2209.05070v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1">Anjun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiangyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1">Shaohao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yanxu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiming Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1">Qi Ye</a></p>
<p>Millimeter Wave (mmWave) Radar is gaining popularity as it can work in
adverse environments like smoke, rain, snow, poor lighting, etc. Prior work has
explored the possibility of reconstructing 3D skeletons or meshes from the
noisy and sparse mmWave Radar signals. However, it is unclear how accurately we
can reconstruct the 3D body from the mmWave signals across scenes and how it
performs compared with cameras, which are important aspects needed to be
considered when either using mmWave radars alone or combining them with
cameras. To answer these questions, an automatic 3D body annotation system is
first designed and built up with multiple sensors to collect a large-scale
dataset. The dataset consists of synchronized and calibrated mmWave radar point
clouds and RGB(D) images in different scenes and skeleton/mesh annotations for
humans in the scenes. With this dataset, we train state-of-the-art methods with
inputs from different sensors and test them in various scenarios. The results
demonstrate that 1) despite the noise and sparsity of the generated point
clouds, the mmWave radar can achieve better reconstruction accuracy than the
RGB camera but worse than the depth camera; 2) the reconstruction from the
mmWave radar is affected by adverse weather conditions moderately while the
RGB(D) camera is severely affected. Further, analysis of the dataset and the
results shadow insights on improving the reconstruction from the mmWave radar
and the combination of signals from different sensors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.00577">Fine-tuned Generative Adversarial Network-based Model for Medical Image Super-Resolution. (arXiv:2211.00577v7 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Aghelan_A/0/1/0/all/0/1">Alireza Aghelan</a>, <a href="http://arxiv.org/find/eess/1/au:+Rouhani_M/0/1/0/all/0/1">Modjtaba Rouhani</a></p>
<p>In the field of medical image analysis, there is a substantial need for
high-resolution (HR) images to improve diagnostic accuracy. However, It is a
challenging task to obtain HR medical images, as it requires advanced
instruments and significant time. Deep learning-based super-resolution methods
can help to improve the resolution and perceptual quality of low-resolution
(LR) medical images. Recently, Generative Adversarial Network (GAN) based
methods have shown remarkable performance among deep learning-based
super-resolution methods. Real-Enhanced Super-Resolution Generative Adversarial
Network (Real-ESRGAN) is a practical model for recovering HR images from
real-world LR images. In our proposed approach, we use transfer learning
technique and fine-tune the pre-trained Real-ESRGAN model using medical image
datasets. This technique helps in improving the performance of the model. The
focus of this paper is on enhancing the resolution and perceptual quality of
chest X-ray and retinal images. We use the Tuberculosis chest X-ray (Shenzhen)
dataset and the STARE dataset of retinal images for fine-tuning the model. The
proposed model achieves superior perceptual quality compared to the Real-ESRGAN
model, effectively preserving fine details and generating images with more
realistic textures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.06681">Cross-domain Self-supervised Framework for Photoacoustic Computed Tomography Image Reconstruction. (arXiv:2301.06681v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Lan_H/0/1/0/all/0/1">Hengrong Lan</a>, <a href="http://arxiv.org/find/eess/1/au:+Huang_L/0/1/0/all/0/1">Lijie Huang</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1">Zhiqiang Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Lv_J/0/1/0/all/0/1">Jing Lv</a>, <a href="http://arxiv.org/find/eess/1/au:+Luo_J/0/1/0/all/0/1">Jianwen Luo</a></p>
<p>Accurate image reconstruction is crucial for photoacoustic (PA) computed
tomography (PACT). Recently, deep learning has been used to reconstruct the PA
image with a supervised scheme, which requires high-quality images as ground
truth labels. In practice, there are inevitable trade-offs between cost and
performance since the use of more channels is an expensive strategy to access
more measurements. Here, we propose a cross-domain unsupervised reconstruction
(CDUR) strategy with a pure transformer model, which overcomes the lack of
ground truth labels from limited PA measurements. The proposed approach
exploits the equivariance of PACT to achieve high performance with a smaller
number of channels. We implement a self-supervised reconstruction in a
model-based form. Meanwhile, we also leverage the self-supervision to enforce
the measurement and image consistency on three partitions of measured PA data,
by randomly masking different channels. We find that dynamically masking a high
proportion of the channels, e.g., 80%, yields nontrivial self-supervisors in
both image and signal domains, which decrease the multiplicity of the pseudo
solution to efficiently reconstruct the image from fewer PA measurements with
minimum error of the image. Experimental results on in-vivo PACT dataset of
mice demonstrate the potential of our unsupervised framework. In addition, our
method shows a high performance (0.83 structural similarity index (SSIM) in the
extreme sparse case with 13 channels), which is close to that of supervised
scheme (0.77 SSIM with 16 channels). On top of all the advantages, our method
may be deployed on different trainable models in an end-to-end manner.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.10670">Towards Arbitrary Text-driven Image Manipulation via Space Alignment. (arXiv:2301.10670v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1">Yunpeng Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1">Zihan Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1">Chao Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Weichen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1">Guowei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1">Chun Yuan</a></p>
<p>The recent GAN inversion methods have been able to successfully invert the
real image input to the corresponding editable latent code in StyleGAN. By
combining with the language-vision model (CLIP), some text-driven image
manipulation methods are proposed. However, these methods require extra costs
to perform optimization for a certain image or a new attribute editing mode. To
achieve a more efficient editing method, we propose a new Text-driven image
Manipulation framework via Space Alignment (TMSA). The Space Alignment module
aims to align the same semantic regions in CLIP and StyleGAN spaces. Then, the
text input can be directly accessed into the StyleGAN space and be used to find
the semantic shift according to the text description. The framework can support
arbitrary image editing mode without additional cost. Our work provides the
user with an interface to control the attributes of a given image according to
text input and get the result in real time. Ex tensive experiments demonstrate
our superior performance over prior works.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.02350">Aggregation of Disentanglement: Reconsidering Domain Variations in Domain Generalization. (arXiv:2302.02350v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Daoan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Mingkai Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chenming Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1">Lingyun Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jianguo Zhang</a></p>
<p>Domain Generalization (DG) is a fundamental challenge for machine learning
models, which aims to improve model generalization on various domains. Previous
methods focus on generating domain invariant features from various source
domains. However, we argue that the domain variantions also contain useful
information, ie, classification-aware information, for downstream tasks, which
has been largely ignored. Different from learning domain invariant features
from source domains, we decouple the input images into Domain Expert Features
and noise. The proposed domain expert features lie in a learned latent space
where the images in each domain can be classified independently, enabling the
implicit use of classification-aware domain variations. Based on the analysis,
we proposed a novel paradigm called Domain Disentanglement Network (DDN) to
disentangle the domain expert features from the source domain images and
aggregate the source domain expert features for representing the target test
domain. We also propound a new contrastive learning method to guide the domain
expert features to form a more balanced and separable feature space.
Experiments on the widely-used benchmarks of PACS, VLCS, OfficeHome, DomainNet,
and TerraIncognita demonstrate the competitive performance of our method
compared to the recently proposed alternatives.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.09187">Video Action Recognition Collaborative Learning with Dynamics via PSO-ConvNet Transformer. (arXiv:2302.09187v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Phong_N/0/1/0/all/0/1">Nguyen Huu Phong</a>, <a href="http://arxiv.org/find/cs/1/au:+Ribeiro_B/0/1/0/all/0/1">Bernardete Ribeiro</a></p>
<p>Recognizing human actions in video sequences, known as Human Action
Recognition (HAR), is a challenging task in pattern recognition. While
Convolutional Neural Networks (ConvNets) have shown remarkable success in image
recognition, they are not always directly applicable to HAR, as temporal
features are critical for accurate classification. In this paper, we propose a
novel dynamic PSO-ConvNet model for learning actions in videos, building on our
recent work in image recognition. Our approach leverages a framework where the
weight vector of each neural network represents the position of a particle in
phase space, and particles share their current weight vectors and gradient
estimates of the Loss function. To extend our approach to video, we integrate
ConvNets with state-of-the-art temporal methods such as Transformer and
Recurrent Neural Networks. Our experimental results on the UCF-101 dataset
demonstrate substantial improvements of up to 9% in accuracy, which confirms
the effectiveness of our proposed method. In addition, we conducted experiments
on larger and more variety of datasets including Kinetics-400 and HMDB-51 and
obtained preference for Collaborative Learning in comparison with
Non-Collaborative Learning (Individual Learning). Overall, our dynamic
PSO-ConvNet model provides a promising direction for improving HAR by better
capturing the spatio-temporal dynamics of human actions in videos. The code is
available at
https://github.com/leonlha/Video-Action-Recognition-Collaborative-Learning-with-Dynamics-via-PSO-ConvNet-Transformer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.14595">MateRobot: Material Recognition in Wearable Robotics for People with Visual Impairments. (arXiv:2302.14595v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1">Junwei Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiaming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1">Kailun Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1">Kunyu Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1">Rainer Stiefelhagen</a></p>
<p>People with Visual Impairments (PVI) typically recognize objects through
haptic perception. Knowing objects and materials before touching is desired by
the target users but under-explored in the field of human-centered robotics. To
fill this gap, in this work, a wearable vision-based robotic system, MateRobot,
is established for PVI to recognize materials and object categories beforehand.
To address the computational constraints of mobile platforms, we propose a
lightweight yet accurate model MateViT to perform pixel-wise semantic
segmentation, simultaneously recognizing both objects and materials. Our
methods achieve respective 40.2% and 51.1% of mIoU on COCOStuff-10K and DMS
datasets, surpassing the previous method with +5.7% and +7.0% gains. Moreover,
on the field test with participants, our wearable system reaches a score of 28
in the NASA-Task Load Index, indicating low cognitive demands and ease of use.
Our MateRobot demonstrates the feasibility of recognizing material property
through visual cues and offers a promising step towards improving the
functionality of wearable robots for PVI. The source code has been made
publicly available at
https://junweizheng93.github.io/publications/MATERobot/MATERobot.html.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.07820">Adaptive Rotated Convolution for Rotated Object Detection. (arXiv:2303.07820v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pu_Y/0/1/0/all/0/1">Yifan Pu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yiru Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_Z/0/1/0/all/0/1">Zhuofan Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1">Yizeng Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yulin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gan_W/0/1/0/all/0/1">Weihao Gan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zidong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1">Shiji Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1">Gao Huang</a></p>
<p>Rotated object detection aims to identify and locate objects in images with
arbitrary orientation. In this scenario, the oriented directions of objects
vary considerably across different images, while multiple orientations of
objects exist within an image. This intrinsic characteristic makes it
challenging for standard backbone networks to extract high-quality features of
these arbitrarily orientated objects. In this paper, we present Adaptive
Rotated Convolution (ARC) module to handle the aforementioned challenges. In
our ARC module, the convolution kernels rotate adaptively to extract object
features with varying orientations in different images, and an efficient
conditional computation mechanism is introduced to accommodate the large
orientation variations of objects within an image. The two designs work
seamlessly in rotated object detection problem. Moreover, ARC can conveniently
serve as a plug-and-play module in various vision backbones to boost their
representation ability to detect oriented objects accurately. Experiments on
commonly used benchmarks (DOTA and HRSC2016) demonstrate that equipped with our
proposed ARC module in the backbone network, the performance of multiple
popular oriented object detectors is significantly improved (\eg +3.03\% mAP on
Rotated RetinaNet and +4.16\% on CFA). Combined with the highly competitive
method Oriented R-CNN, the proposed approach achieves state-of-the-art
performance on the DOTA dataset with 81.77\% mAP. Code is available at
\url{https://github.com/LeapLabTHU/ARC}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.10310">Domain-knowledge Inspired Pseudo Supervision (DIPS) for Unsupervised Image-to-Image Translation Models to Support Cross-Domain Classification. (arXiv:2303.10310v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Al_Hindawi_F/0/1/0/all/0/1">Firas Al-Hindawi</a>, <a href="http://arxiv.org/find/cs/1/au:+Siddiquee_M/0/1/0/all/0/1">Md Mahfuzur Rahman Siddiquee</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1">Teresa Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1">Han Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Ying Sun</a></p>
<p>The ability to classify images is dependent on having access to large labeled
datasets and testing on data from the same domain that the model can train on.
Classification becomes more challenging when dealing with new data from a
different domain, where gathering and especially labeling a larger image
dataset for retraining a classification model requires a labor-intensive human
effort. Cross-domain classification frameworks were developed to handle this
data domain shift problem by utilizing unsupervised image-to-image translation
models to translate an input image from the unlabeled domain to the labeled
domain. The problem with these unsupervised models lies in their unsupervised
nature. For lack of annotations, it is not possible to use the traditional
supervised metrics to evaluate these translation models to pick the best-saved
checkpoint model. This paper introduces a new method called Domain-knowledge
Inspired Pseudo Supervision (DIPS) which utilizes domain-informed Gaussian
Mixture Models to generate pseudo annotations to enable the use of traditional
supervised metrics. This method was designed specifically to support
cross-domain classification applications contrary to other typically used
metrics such as the FID which were designed to evaluate the model in terms of
the quality of the generated image from a human-eye perspective. DIPS proves
its effectiveness by outperforming various GAN evaluation metrics, including
FID, when selecting the optimal saved checkpoint model. It is also evaluated
against truly supervised metrics. Furthermore, DIPS showcases its robustness
and interpretability by demonstrating a strong correlation with truly
supervised metrics, highlighting its superiority over existing state-of-the-art
alternatives. The code and data to replicate the results can be found on the
official Github repository: https://github.com/Hindawi91/DIPS
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.13199">First Session Adaptation: A Strong Replay-Free Baseline for Class-Incremental Learning. (arXiv:2303.13199v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Panos_A/0/1/0/all/0/1">Aristeidis Panos</a>, <a href="http://arxiv.org/find/cs/1/au:+Kobe_Y/0/1/0/all/0/1">Yuriko Kobe</a>, <a href="http://arxiv.org/find/cs/1/au:+Reino_D/0/1/0/all/0/1">Daniel Olmeda Reino</a>, <a href="http://arxiv.org/find/cs/1/au:+Aljundi_R/0/1/0/all/0/1">Rahaf Aljundi</a>, <a href="http://arxiv.org/find/cs/1/au:+Turner_R/0/1/0/all/0/1">Richard E. Turner</a></p>
<p>In Class-Incremental Learning (CIL) an image classification system is exposed
to new classes in each learning session and must be updated incrementally.
Methods approaching this problem have updated both the classification head and
the feature extractor body at each session of CIL. In this work, we develop a
baseline method, First Session Adaptation (FSA), that sheds light on the
efficacy of existing CIL approaches and allows us to assess the relative
performance contributions from head and body adaption. FSA adapts a pre-trained
neural network body only on the first learning session and fixes it thereafter;
a head based on linear discriminant analysis (LDA), is then placed on top of
the adapted body, allowing exact updates through CIL. FSA is replay-free
i.e.~it does not memorize examples from previous sessions of continual
learning. To empirically motivate FSA, we first consider a diverse selection of
22 image-classification datasets, evaluating different heads and body
adaptation techniques in high/low-shot offline settings. We find that the LDA
head performs well and supports CIL out-of-the-box. We also find that
Featurewise Layer Modulation (FiLM) adapters are highly effective in the
few-shot setting, and full-body adaption in the high-shot setting. Second, we
empirically investigate various CIL settings including high-shot CIL and
few-shot CIL, including settings that have previously been used in the
literature. We show that FSA significantly improves over the state-of-the-art
in 15 of the 16 settings considered. FSA with FiLM adapters is especially
performant in the few-shot setting. These results indicate that current
approaches to continuous body adaptation are not working as expected. Finally,
we propose a measure that can be applied to a set of unlabelled inputs which is
predictive of the benefits of body adaptation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.13867">Few Shot Medical Image Segmentation with Cross Attention Transformer. (arXiv:2303.13867v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yi Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yufan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1">Kwang-Ting Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hao Chen</a></p>
<p>Medical image segmentation has made significant progress in recent years.
Deep learning-based methods are recognized as data-hungry techniques, requiring
large amounts of data with manual annotations. However, manual annotation is
expensive in the field of medical image analysis, which requires
domain-specific expertise. To address this challenge, few-shot learning has the
potential to learn new classes from only a few examples. In this work, we
propose a novel framework for few-shot medical image segmentation, termed
CAT-Net, based on cross masked attention Transformer. Our proposed network
mines the correlations between the support image and query image, limiting them
to focus only on useful foreground information and boosting the representation
capacity of both the support prototype and query features. We further design an
iterative refinement framework that refines the query image segmentation
iteratively and promotes the support feature in turn. We validated the proposed
method on three public datasets: Abd-CT, Abd-MRI, and Card-MRI. Experimental
results demonstrate the superior performance of our method compared to
state-of-the-art methods and the effectiveness of each component. Code:
https://github.com/hust-linyi/CAT-Net.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.14772">$\Delta$-Patching: A Framework for Rapid Adaptation of Pre-trained Convolutional Networks without Base Performance Loss. (arXiv:2303.14772v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Devaguptapu_C/0/1/0/all/0/1">Chaitanya Devaguptapu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sinha_S/0/1/0/all/0/1">Samarth Sinha</a>, <a href="http://arxiv.org/find/cs/1/au:+Joseph_K/0/1/0/all/0/1">K J Joseph</a>, <a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1">Vineeth N Balasubramanian</a>, <a href="http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1">Animesh Garg</a></p>
<p>Models pre-trained on large-scale datasets are often fine-tuned to support
newer tasks and datasets that arrive over time. This process necessitates
storing copies of the model over time for each task that the pre-trained model
is fine-tuned to. Building on top of recent model patching work, we propose
$\Delta$-Patching for fine-tuning neural network models in an efficient manner,
without the need to store model copies. We propose a simple and lightweight
method called $\Delta$-Networks to achieve this objective. Our comprehensive
experiments across setting and architecture variants show that
$\Delta$-Networks outperform earlier model patching work while only requiring a
fraction of parameters to be trained. We also show that this approach can be
used for other problem settings such as transfer learning and zero-shot domain
adaptation, as well as other tasks such as detection and segmentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.09423">ASM: Adaptive Skinning Model for High-Quality 3D Face Modeling. (arXiv:2304.09423v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1">Kai Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_H/0/1/0/all/0/1">Hong Shang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_T/0/1/0/all/0/1">Tianyang Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xinghan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jingkai Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1">Zhongqian Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1">Wei Yang</a></p>
<p>The research fields of parametric face model and 3D face reconstruction have
been extensively studied. However, a critical question remains unanswered: how
to tailor the face model for specific reconstruction settings. We argue that
reconstruction with multi-view uncalibrated images demands a new model with
stronger capacity. Our study shifts attention from data-dependent 3D Morphable
Models (3DMM) to an understudied human-designed skinning model. We propose
Adaptive Skinning Model (ASM), which redefines the skinning model with more
compact and fully tunable parameters. With extensive experiments, we
demonstrate that ASM achieves significantly improved capacity than 3DMM, with
the additional advantage of model size and easy implementation for new
topology. We achieve state-of-the-art performance with ASM for multi-view
reconstruction on the Florence MICC Coop benchmark. Our quantitative analysis
demonstrates the importance of a high-capacity model for fully exploiting
abundant information from multi-view input in reconstruction. Furthermore, our
model with physical-semantic parameters can be directly utilized for real-world
applications, such as in-game avatar creation. As a result, our work opens up
new research direction for parametric face model and facilitates future
research on multi-view reconstruction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.13147">Self-Supervised Multi-Object Tracking For Autonomous Driving From Consistency Across Timescales. (arXiv:2304.13147v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lang_C/0/1/0/all/0/1">Christopher Lang</a>, <a href="http://arxiv.org/find/cs/1/au:+Braun_A/0/1/0/all/0/1">Alexander Braun</a>, <a href="http://arxiv.org/find/cs/1/au:+Schillingmann_L/0/1/0/all/0/1">Lars Schillingmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1">Abhinav Valada</a></p>
<p>Self-supervised multi-object trackers have tremendous potential as they
enable learning from raw domain-specific data. However, their re-identification
accuracy still falls short compared to their supervised counterparts. We
hypothesize that this drawback results from formulating self-supervised
objectives that are limited to single frames or frame pairs. Such formulations
do not capture sufficient visual appearance variations to facilitate learning
consistent re-identification features for autonomous driving when the frame
rate is low or object dynamics are high. In this work, we propose a training
objective that enables self-supervised learning of re-identification features
from multiple sequential frames by enforcing consistent association scores
across short and long timescales. We perform extensive evaluations
demonstrating that re-identification features trained from longer sequences
significantly reduce ID switches on standard autonomous driving datasets
compared to existing self-supervised learning methods, which are limited to
training on frame pairs. Using our proposed SubCo loss function, we set the new
state-of-the-art among self-supervised methods and even perform on par with
fully supervised learning methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.10424">ZeroFlow: Fast, Zero Label, Scalable Scene Flow via Distillation. (arXiv:2305.10424v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vedder_K/0/1/0/all/0/1">Kyle Vedder</a>, <a href="http://arxiv.org/find/cs/1/au:+Peri_N/0/1/0/all/0/1">Neehar Peri</a>, <a href="http://arxiv.org/find/cs/1/au:+Chodosh_N/0/1/0/all/0/1">Nathaniel Chodosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Khatri_I/0/1/0/all/0/1">Ishan Khatri</a>, <a href="http://arxiv.org/find/cs/1/au:+Eaton_E/0/1/0/all/0/1">Eric Eaton</a>, <a href="http://arxiv.org/find/cs/1/au:+Jayaraman_D/0/1/0/all/0/1">Dinesh Jayaraman</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1">Deva Ramanan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hays_J/0/1/0/all/0/1">James Hays</a></p>
<p>Scene flow estimation is the task of describing the 3D motion field between
temporally successive point clouds. State-of-the-art methods use strong priors
and test-time optimization techniques, but require on the order of tens of
seconds to process large-scale point clouds, making them unusable as computer
vision primitives for real-time applications such as open world object
detection. Feed forward methods are considerably faster, running on the order
of tens to hundreds of milliseconds for large-scale point clouds, but require
expensive human supervision. To address both limitations, we propose Scene Flow
via Distillation, a simple, scalable distillation framework that uses a
label-free optimization method to produce pseudo-labels to supervise a feed
forward model. Our instantiation of this framework, ZeroFlow, achieves
state-of-the-art performance on the Argoverse 2 Self-Supervised Scene Flow
Challenge while using zero human labels by simply training on large-scale,
diverse unlabeled data. At test-time, ZeroFlow is over 1000$\times$ faster than
label-free state-of-the-art optimization-based methods on large-scale point
clouds and over 1000$\times$ cheaper to train on unlabeled data compared to the
cost of human annotation of that data. To facilitate further research, we will
release our code, trained model weights, and high quality pseudo-labels for the
Argoverse 2 and Waymo Open datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19478">Permutation-Aware Action Segmentation via Unsupervised Frame-to-Segment Alignment. (arXiv:2305.19478v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1">Quoc-Huy Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Mehmood_A/0/1/0/all/0/1">Ahmed Mehmood</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_M/0/1/0/all/0/1">Muhammad Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Naufil_M/0/1/0/all/0/1">Muhammad Naufil</a>, <a href="http://arxiv.org/find/cs/1/au:+Zafar_A/0/1/0/all/0/1">Anas Zafar</a>, <a href="http://arxiv.org/find/cs/1/au:+Konin_A/0/1/0/all/0/1">Andrey Konin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zia_M/0/1/0/all/0/1">M. Zeeshan Zia</a></p>
<p>This paper presents an unsupervised transformer-based framework for temporal
activity segmentation which leverages not only frame-level cues but also
segment-level cues. This is in contrast with previous methods which often rely
on frame-level information only. Our approach begins with a frame-level
prediction module which estimates framewise action classes via a transformer
encoder. The frame-level prediction module is trained in an unsupervised manner
via temporal optimal transport. To exploit segment-level information, we
utilize a segment-level prediction module and a frame-to-segment alignment
module. The former includes a transformer decoder for estimating video
transcripts, while the latter matches frame-level features with segment-level
features, yielding permutation-aware segmentation results. Moreover, inspired
by temporal optimal transport, we introduce simple-yet-effective pseudo labels
for unsupervised training of the above modules. Our experiments on four public
datasets, i.e., 50 Salads, YouTube Instructions, Breakfast, and Desktop
Assembly show that our approach achieves comparable or better performance than
previous methods in unsupervised activity segmentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19480">Learning by Aligning 2D Skeleton Sequences in Time. (arXiv:2305.19480v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1">Quoc-Huy Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_M/0/1/0/all/0/1">Muhammad Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_M/0/1/0/all/0/1">M. Hassan Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Popattia_M/0/1/0/all/0/1">Murad Popattia</a>, <a href="http://arxiv.org/find/cs/1/au:+Konin_A/0/1/0/all/0/1">Andrey Konin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zia_M/0/1/0/all/0/1">M. Zeeshan Zia</a></p>
<p>This paper presents a self-supervised temporal video alignment framework
which is useful for several fine-grained human activity understanding
applications. In contrast with the state-of-the-art method of CASA, where
sequences of 3D skeleton coordinates are taken directly as input, our key idea
is to use sequences of 2D skeleton heatmaps as input. Unlike CASA which
performs self-attention in the temporal domain only, we feed 2D skeleton
heatmaps to a video transformer which performs self-attention both in the
spatial and temporal domains for extracting effective spatiotemporal and
contextual features. In addition, we introduce simple heatmap augmentation
techniques based on 2D skeletons for self-supervised learning. Despite the lack
of 3D information, our approach achieves not only higher accuracy but also
better robustness against missing and noisy keypoints than CASA. Furthermore,
extensive evaluations on three public datasets, i.e., Penn Action, IKEA ASM,
and H2O, demonstrate that our approach outperforms previous methods in
different fine-grained human activity understanding tasks. Finally, fusing 2D
skeleton heatmaps with RGB videos yields the state-of-the-art on all metrics
and datasets. To the best of our knowledge, our work is the first to utilize 2D
skeleton heatmap inputs and the first to explore multi-modality fusion for
temporal video alignment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09715">Semantic-Aware Dual Contrastive Learning for Multi-label Image Classification. (arXiv:2307.09715v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1">Leilei Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1">Dengdi Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Haifeng Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1">Bin Luo</a></p>
<p>Extracting image semantics effectively and assigning corresponding labels to
multiple objects or attributes for natural images is challenging due to the
complex scene contents and confusing label dependencies. Recent works have
focused on modeling label relationships with graph and understanding object
regions using class activation maps (CAM). However, these methods ignore the
complex intra- and inter-category relationships among specific semantic
features, and CAM is prone to generate noisy information. To this end, we
propose a novel semantic-aware dual contrastive learning framework that
incorporates sample-to-sample contrastive learning (SSCL) as well as
prototype-to-sample contrastive learning (PSCL). Specifically, we leverage
semantic-aware representation learning to extract category-related local
discriminative features and construct category prototypes. Then based on SSCL,
label-level visual representations of the same category are aggregated
together, and features belonging to distinct categories are separated.
Meanwhile, we construct a novel PSCL module to narrow the distance between
positive samples and category prototypes and push negative samples away from
the corresponding category prototypes. Finally, the discriminative label-level
features related to the image content are accurately captured by the joint
training of the above three parts. Experiments on five challenging large-scale
public datasets demonstrate that our proposed method is effective and
outperforms the state-of-the-art methods. Code and supplementary materials are
released on https://github.com/yu-gi-oh-leilei/SADCL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.15506">Improving Image Quality of Sparse-view Lung Cancer CT Images with a Convolutional Neural Network. (arXiv:2307.15506v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ries_A/0/1/0/all/0/1">Annika Ries</a>, <a href="http://arxiv.org/find/cs/1/au:+Dorosti_T/0/1/0/all/0/1">Tina Dorosti</a>, <a href="http://arxiv.org/find/cs/1/au:+Thalhammer_J/0/1/0/all/0/1">Johannes Thalhammer</a>, <a href="http://arxiv.org/find/cs/1/au:+Sasse_D/0/1/0/all/0/1">Daniel Sasse</a>, <a href="http://arxiv.org/find/cs/1/au:+Sauter_A/0/1/0/all/0/1">Andreas Sauter</a>, <a href="http://arxiv.org/find/cs/1/au:+Meurer_F/0/1/0/all/0/1">Felix Meurer</a>, <a href="http://arxiv.org/find/cs/1/au:+Benne_A/0/1/0/all/0/1">Ashley Benne</a>, <a href="http://arxiv.org/find/cs/1/au:+Lasser_T/0/1/0/all/0/1">Tobias Lasser</a>, <a href="http://arxiv.org/find/cs/1/au:+Pfeiffer_F/0/1/0/all/0/1">Franz Pfeiffer</a>, <a href="http://arxiv.org/find/cs/1/au:+Schaff_F/0/1/0/all/0/1">Florian Schaff</a>, <a href="http://arxiv.org/find/cs/1/au:+Pfeiffer_D/0/1/0/all/0/1">Daniela Pfeiffer</a></p>
<p>Purpose: To improve the image quality of sparse-view computed tomography (CT)
images with a U-Net for lung cancer detection and to determine the best
trade-off between number of views, image quality, and diagnostic confidence.
</p>
<p>Methods: CT images from 41 subjects (34 with lung cancer, seven healthy) were
retrospectively selected (01.2016-12.2018) and forward projected onto 2048-view
sinograms. Six corresponding sparse-view CT data subsets at varying levels of
undersampling were reconstructed from sinograms using filtered backprojection
with 16, 32, 64, 128, 256, and 512 views, respectively. A dual-frame U-Net was
trained and evaluated for each subsampling level on 8,658 images from 22
diseased subjects. A representative image per scan was selected from 19
subjects (12 diseased, seven healthy) for a single-blinded reader study. The
selected slices, for all levels of subsampling, with and without
post-processing by the U-Net model, were presented to three readers. Image
quality and diagnostic confidence were ranked using pre-defined scales.
Subjective nodule segmentation was evaluated utilizing sensitivity (Se) and
Dice Similarity Coefficient (DSC) with 95% confidence intervals (CI).
</p>
<p>Results: The 64-projection sparse-view images resulted in Se = 0.89 and DSC =
0.81 [0.75,0.86] while their counterparts, post-processed with the U-Net, had
improved metrics (Se = 0.94, DSC = 0.85 [0.82,0.87]). Fewer views lead to
insufficient quality for diagnostic purposes. For increased views, no
substantial discrepancies were noted between the sparse-view and post-processed
images.
</p>
<p>Conclusion: Projection views can be reduced from 2048 to 64 while maintaining
image quality and the confidence of the radiologists on a satisfactory level.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05721">Deformable Mixer Transformer with Gating for Multi-Task Learning of Dense Prediction. (arXiv:2308.05721v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yangyang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yibo Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1">Bernard Ghanem</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lefei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bo_D/0/1/0/all/0/1">Du Bo</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1">Dacheng Tao</a></p>
<p>CNNs and Transformers have their own advantages and both have been widely
used for dense prediction in multi-task learning (MTL). Most of the current
studies on MTL solely rely on CNN or Transformer. In this work, we present a
novel MTL model by combining both merits of deformable CNN and query-based
Transformer with shared gating for multi-task learning of dense prediction.
This combination may offer a simple and efficient solution owing to its
powerful and flexible task-specific learning and advantages of lower cost, less
complexity and smaller parameters than the traditional MTL methods. We
introduce deformable mixer Transformer with gating (DeMTG), a simple and
effective encoder-decoder architecture up-to-date that incorporates the
convolution and attention mechanism in a unified network for MTL. It is
exquisitely designed to use advantages of each block, and provide deformable
and comprehensive features for all tasks from local and global perspective.
First, the deformable mixer encoder contains two types of operators: the
channel-aware mixing operator leveraged to allow communication among different
channels, and the spatial-aware deformable operator with deformable convolution
applied to efficiently sample more informative spatial locations. Second, the
task-aware gating transformer decoder is used to perform the task-specific
predictions, in which task interaction block integrated with self-attention is
applied to capture task interaction features, and the task query block
integrated with gating attention is leveraged to select corresponding
task-specific features. Further, the experiment results demonstrate that the
proposed DeMTG uses fewer GFLOPs and significantly outperforms current
Transformer-based and CNN-based competitive models on a variety of metrics on
three dense prediction datasets. Our code and models are available at
https://github.com/yangyangxu0/DeMTG.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.07929">Fast Adaptation with Bradley-Terry Preference Models in Text-To-Image Classification and Generation. (arXiv:2308.07929v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gallego_V/0/1/0/all/0/1">Victor Gallego</a></p>
<p>Recently, large multimodal models, such as CLIP and Stable Diffusion have
experimented tremendous successes in both foundations and applications.
However, as these models increase in parameter size and computational
requirements, it becomes more challenging for users to personalize them for
specific tasks or preferences. In this work, we address the problem of adapting
the previous models towards sets of particular human preferences, aligning the
retrieved or generated images with the preferences of the user. We leverage the
Bradley-Terry preference model to develop a fast adaptation method that
efficiently fine-tunes the original model, with few examples and with minimal
computing resources. Extensive evidence of the capabilities of this framework
is provided through experiments in different domains related to multimodal text
and image understanding, including preference prediction as a reward model, and
generation tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.10658">Learning Clothing and Pose Invariant 3D Shape Representation for Long-Term Person Re-Identification. (arXiv:2308.10658v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1">Feng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1">Minchul Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_Z/0/1/0/all/0/1">ZiAng Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1">Anil Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiaoming Liu</a></p>
<p>Long-Term Person Re-Identification (LT-ReID) has become increasingly crucial
in computer vision and biometrics. In this work, we aim to extend LT-ReID
beyond pedestrian recognition to include a wider range of real-world human
activities while still accounting for cloth-changing scenarios over large time
gaps. This setting poses additional challenges due to the geometric
misalignment and appearance ambiguity caused by the diversity of human pose and
clothing. To address these challenges, we propose a new approach 3DInvarReID
for (i) disentangling identity from non-identity components (pose, clothing
shape, and texture) of 3D clothed humans, and (ii) reconstructing accurate 3D
clothed body shapes and learning discriminative features of naked body shapes
for person ReID in a joint manner. To better evaluate our study of LT-ReID, we
collect a real-world dataset called CCDA, which contains a wide variety of
human activities and clothing changes. Experimentally, we show the superior
performance of our approach for person ReID.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.13266">Integrating Boxes and Masks: A Multi-Object Framework for Unified Visual Tracking and Segmentation. (arXiv:2308.13266v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yuanyou Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zongxin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yi Yang</a></p>
<p>Tracking any given object(s) spatially and temporally is a common purpose in
Visual Object Tracking (VOT) and Video Object Segmentation (VOS). Joint
tracking and segmentation have been attempted in some studies but they often
lack full compatibility of both box and mask in initialization and prediction,
and mainly focus on single-object scenarios. To address these limitations, this
paper proposes a Multi-object Mask-box Integrated framework for unified
Tracking and Segmentation, dubbed MITS. Firstly, the unified identification
module is proposed to support both box and mask reference for initialization,
where detailed object information is inferred from boxes or directly retained
from masks. Additionally, a novel pinpoint box predictor is proposed for
accurate multi-object box prediction, facilitating target-oriented
representation learning. All target objects are processed simultaneously from
encoding to propagation and decoding, as a unified pipeline for VOT and VOS.
Experimental results show MITS achieves state-of-the-art performance on both
VOT and VOS benchmarks. Notably, MITS surpasses the best prior VOT competitor
by around 6% on the GOT-10k test set, and significantly improves the
performance of box initialization on VOS benchmarks. The code is available at
https://github.com/yoxu515/MITS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.03508">Dynamic Frame Interpolation in Wavelet Domain. (arXiv:2309.03508v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1">Lingtong Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1">Boyuan Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1">Donghao Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1">Wenqing Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1">Ying Tai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chengjie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jie Yang</a></p>
<p>Video frame interpolation is an important low-level vision task, which can
increase frame rate for more fluent visual experience. Existing methods have
achieved great success by employing advanced motion models and synthesis
networks. However, the spatial redundancy when synthesizing the target frame
has not been fully explored, that can result in lots of inefficient
computation. On the other hand, the computation compression degree in frame
interpolation is highly dependent on both texture distribution and scene
motion, which demands to understand the spatial-temporal information of each
input frame pair for a better compression degree selection. In this work, we
propose a novel two-stage frame interpolation framework termed WaveletVFI to
address above problems. It first estimates intermediate optical flow with a
lightweight motion perception network, and then a wavelet synthesis network
uses flow aligned context features to predict multi-scale wavelet coefficients
with sparse convolution for efficient target frame reconstruction, where the
sparse valid masks that control computation in each scale are determined by a
crucial threshold ratio. Instead of setting a fixed value like previous
methods, we find that embedding a classifier in the motion perception network
to learn a dynamic threshold for each sample can achieve more computation
reduction with almost no loss of accuracy. On the common high resolution and
animation frame interpolation benchmarks, proposed WaveletVFI can reduce
computation up to 40% while maintaining similar accuracy, making it perform
more efficiently against other state-of-the-arts. Code is available at
https://github.com/ltkong218/WaveletVFI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.08957">ExBluRF: Efficient Radiance Fields for Extreme Motion Blurred Images. (arXiv:2309.08957v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1">Dongwoo Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1">Jeongtaek Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Rim_J/0/1/0/all/0/1">Jaesung Rim</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1">Sunghyun Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kyoung Mu Lee</a></p>
<p>We present ExBluRF, a novel view synthesis method for extreme motion blurred
images based on efficient radiance fields optimization. Our approach consists
of two main components: 6-DOF camera trajectory-based motion blur formulation
and voxel-based radiance fields. From extremely blurred images, we optimize the
sharp radiance fields by jointly estimating the camera trajectories that
generate the blurry images. In training, multiple rays along the camera
trajectory are accumulated to reconstruct single blurry color, which is
equivalent to the physical motion blur operation. We minimize the
photo-consistency loss on blurred image space and obtain the sharp radiance
fields with camera trajectories that explain the blur of all images. The joint
optimization on the blurred image space demands painfully increasing
computation and resources proportional to the blur size. Our method solves this
problem by replacing the MLP-based framework to low-dimensional 6-DOF camera
poses and voxel-based radiance fields. Compared with the existing works, our
approach restores much sharper 3D scenes from challenging motion blurred views
with the order of 10 times less training time and GPU memory consumption.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.09553">Causal-Story: Local Causal Attention Utilizing Parameter-Efficient Tuning For Visual Story Synthesis. (arXiv:2309.09553v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_T/0/1/0/all/0/1">Tianyi Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1">Jiuxin Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaofeng Zhang</a></p>
<p>The excellent text-to-image synthesis capability of diffusion models has
driven progress in synthesizing coherent visual stories. The current
state-of-the-art method combines the features of historical captions,
historical frames, and the current captions as conditions for generating the
current frame. However, this method treats each historical frame and caption as
the same contribution. It connects them in order with equal weights, ignoring
that not all historical conditions are associated with the generation of the
current frame. To address this issue, we propose Causal-Story. This model
incorporates a local causal attention mechanism that considers the causal
relationship between previous captions, frames, and current captions. By
assigning weights based on this relationship, Causal-Story generates the
current frame, thereby improving the global consistency of story generation. We
evaluated our model on the PororoSV and FlintstonesSV datasets and obtained
state-of-the-art FID scores, and the generated frames also demonstrate better
storytelling in visuals.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.10314">Dive Deeper into Rectifying Homography for Stereo Camera Online Self-Calibration. (arXiv:2309.10314v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Hongbo Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yikang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Qijun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1">Rui Fan</a></p>
<p>Accurate estimation of stereo camera extrinsic parameters is the key to
guarantee the performance of stereo matching algorithms. In prior arts, the
online self-calibration of stereo cameras has commonly been formulated as a
specialized visual odometry problem, without taking into account the principles
of stereo rectification. In this paper, we first delve deeply into the concept
of rectifying homography, which serves as the cornerstone for the development
of our novel stereo camera online self-calibration algorithm, for cases where
only a single pair of images is available. Furthermore, we introduce a simple
yet effective solution for global optimum extrinsic parameter estimation in the
presence of stereo video sequences. Additionally, we emphasize the
impracticality of using three Euler angles and three components in the
translation vectors for performance quantification. Instead, we introduce four
new evaluation metrics to quantify the robustness and accuracy of extrinsic
parameter estimation, applicable to both single-pair and multi-pair cases.
Extensive experiments conducted across indoor and outdoor environments using
various experimental setups validate the effectiveness of our proposed
algorithm. The comprehensive evaluation results demonstrate its superior
performance in comparison to the baseline algorithm. Our source code, demo
video, and supplement are publicly available at mias.group/StereoCalibrator.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11009">Controllable Dynamic Appearance for Neural 3D Portraits. (arXiv:2309.11009v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Athar_S/0/1/0/all/0/1">ShahRukh Athar</a>, <a href="http://arxiv.org/find/cs/1/au:+Shu_Z/0/1/0/all/0/1">Zhixin Shu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zexiang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Luan_F/0/1/0/all/0/1">Fujun Luan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bi_S/0/1/0/all/0/1">Sai Bi</a>, <a href="http://arxiv.org/find/cs/1/au:+Sunkavalli_K/0/1/0/all/0/1">Kalyan Sunkavalli</a>, <a href="http://arxiv.org/find/cs/1/au:+Samaras_D/0/1/0/all/0/1">Dimitris Samaras</a></p>
<p>Recent advances in Neural Radiance Fields (NeRFs) have made it possible to
reconstruct and reanimate dynamic portrait scenes with control over head-pose,
facial expressions and viewing direction. However, training such models assumes
photometric consistency over the deformed region e.g. the face must be evenly
lit as it deforms with changing head-pose and facial expression. Such
photometric consistency across frames of a video is hard to maintain, even in
studio environments, thus making the created reanimatable neural portraits
prone to artifacts during reanimation. In this work, we propose CoDyNeRF, a
system that enables the creation of fully controllable 3D portraits in
real-world capture conditions. CoDyNeRF learns to approximate illumination
dependent effects via a dynamic appearance model in the canonical space that is
conditioned on predicted surface normals and the facial expressions and
head-pose deformations. The surface normals prediction is guided using 3DMM
normals that act as a coarse prior for the normals of the human head, where
direct prediction of normals is hard due to rigid and non-rigid deformations
induced by head-pose and facial expression changes. Using only a
smartphone-captured short video of a subject for training, we demonstrate the
effectiveness of our method on free view synthesis of a portrait scene with
explicit head pose and expression controls, and realistic lighting effects. The
project page can be found here:
<a href="http://shahrukhathar.github.io/2023/08/22/CoDyNeRF.html">this http URL</a>
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11119">BroadBEV: Collaborative LiDAR-camera Fusion for Broad-sighted Bird&#x27;s Eye View Map Construction. (arXiv:2309.11119v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1">Minsu Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1">Giseop Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_K/0/1/0/all/0/1">Kyong Hwan Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1">Sunwook Choi</a></p>
<p>A recent sensor fusion in a Bird's Eye View (BEV) space has shown its utility
in various tasks such as 3D detection, map segmentation, etc. However, the
approach struggles with inaccurate camera BEV estimation, and a perception of
distant areas due to the sparsity of LiDAR points. In this paper, we propose a
broad BEV fusion (BroadBEV) that addresses the problems with a spatial
synchronization approach of cross-modality. Our strategy aims to enhance camera
BEV estimation for a broad-sighted perception while simultaneously improving
the completion of LiDAR's sparsity in the entire BEV space. Toward that end, we
devise Point-scattering that scatters LiDAR BEV distribution to camera depth
distribution. The method boosts the learning of depth estimation of the camera
branch and induces accurate location of dense camera features in BEV space. For
an effective BEV fusion between the spatially synchronized features, we suggest
ColFusion that applies self-attention weights of LiDAR and camera BEV features
to each other. Our extensive experiments demonstrate that BroadBEV provides a
broad-sighted BEV perception with remarkable performance gains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11139">More complex encoder is not all you need. (arXiv:2309.11139v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Yang_W/0/1/0/all/0/1">Weibin Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_L/0/1/0/all/0/1">Longwei Xu</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_P/0/1/0/all/0/1">Pengwei Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Geng_D/0/1/0/all/0/1">Dehua Geng</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1">Yusong Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_M/0/1/0/all/0/1">Mingyuan Xu</a>, <a href="http://arxiv.org/find/eess/1/au:+Dong_Z/0/1/0/all/0/1">Zhiqi Dong</a></p>
<p>U-Net and its variants have been widely used in medical image segmentation.
However, most current U-Net variants confine their improvement strategies to
building more complex encoder, while leaving the decoder unchanged or adopting
a simple symmetric structure. These approaches overlook the true functionality
of the decoder: receiving low-resolution feature maps from the encoder and
restoring feature map resolution and lost information through upsampling. As a
result, the decoder, especially its upsampling component, plays a crucial role
in enhancing segmentation outcomes. However, in 3D medical image segmentation,
the commonly used transposed convolution can result in visual artifacts. This
issue stems from the absence of direct relationship between adjacent pixels in
the output feature map. Furthermore, plain encoder has already possessed
sufficient feature extraction capability because downsampling operation leads
to the gradual expansion of the receptive field, but the loss of information
during downsampling process is unignorable. To address the gap in relevant
research, we extend our focus beyond the encoder and introduce neU-Net (i.e.,
not complex encoder U-Net), which incorporates a novel Sub-pixel Convolution
for upsampling to construct a powerful decoder. Additionally, we introduce
multi-scale wavelet inputs module on the encoder side to provide additional
information. Our model design achieves excellent results, surpassing other
state-of-the-art methods on both the Synapse and ACDC datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11148">Online Calibration of a Single-Track Ground Vehicle Dynamics Model by Tight Fusion with Visual-Inertial Odometry. (arXiv:2309.11148v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Haolong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Stueckler_J/0/1/0/all/0/1">Joerg Stueckler</a></p>
<p>Wheeled mobile robots need the ability to estimate their motion and the
effect of their control actions for navigation planning. In this paper, we
present ST-VIO, a novel approach which tightly fuses a single-track dynamics
model for wheeled ground vehicles with visual inertial odometry. Our method
calibrates and adapts the dynamics model online and facilitates accurate
forward prediction conditioned on future control inputs. The single-track
dynamics model approximates wheeled vehicle motion under specific control
inputs on flat ground using ordinary differential equations. We use a
singularity-free and differentiable variant of the single-track model to enable
seamless integration as dynamics factor into VIO and to optimize the model
parameters online together with the VIO state variables. We validate our method
with real-world data in both indoor and outdoor environments with different
terrain types and wheels. In our experiments, we demonstrate that our ST-VIO
can not only adapt to the change of the environments and achieve accurate
prediction under new control inputs, but even improves the tracking accuracy.
Supplementary video: https://youtu.be/BuGY1L1FRa4.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11157">Learning Deformable 3D Graph Similarity to Track Plant Cells in Unregistered Time Lapse Images. (arXiv:2309.11157v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1">Md Shazid Islam</a>, <a href="http://arxiv.org/find/cs/1/au:+Dutta_A/0/1/0/all/0/1">Arindam Dutta</a>, <a href="http://arxiv.org/find/cs/1/au:+Ta_C/0/1/0/all/0/1">Calvin-Khang Ta</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_K/0/1/0/all/0/1">Kevin Rodriguez</a>, <a href="http://arxiv.org/find/cs/1/au:+Michael_C/0/1/0/all/0/1">Christian Michael</a>, <a href="http://arxiv.org/find/cs/1/au:+Alber_M/0/1/0/all/0/1">Mark Alber</a>, <a href="http://arxiv.org/find/cs/1/au:+Reddy_G/0/1/0/all/0/1">G. Venugopala Reddy</a>, <a href="http://arxiv.org/find/cs/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1">Amit K. Roy-Chowdhury</a></p>
<p>Tracking of plant cells in images obtained by microscope is a challenging
problem due to biological phenomena such as large number of cells, non-uniform
growth of different layers of the tightly packed plant cells and cell division.
Moreover, images in deeper layers of the tissue being noisy and unavoidable
systemic errors inherent in the imaging process further complicates the
problem. In this paper, we propose a novel learning-based method that exploits
the tightly packed three-dimensional cell structure of plant cells to create a
three-dimensional graph in order to perform accurate cell tracking. We further
propose novel algorithms for cell division detection and effective
three-dimensional registration, which improve upon the state-of-the-art
algorithms. We demonstrate the efficacy of our algorithm in terms of tracking
accuracy and inference-time on a benchmark dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11331">Gold-YOLO: Efficient Object Detector via Gather-and-Distribute Mechanism. (arXiv:2309.11331v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chengcheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1">Wei He</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1">Ying Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jianyuan Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chuanjian Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1">Kai Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yunhe Wang</a></p>
<p>In the past years, YOLO-series models have emerged as the leading approaches
in the area of real-time object detection. Many studies pushed up the baseline
to a higher level by modifying the architecture, augmenting data and designing
new losses. However, we find previous models still suffer from information
fusion problem, although Feature Pyramid Network (FPN) and Path Aggregation
Network (PANet) have alleviated this. Therefore, this study provides an
advanced Gatherand-Distribute mechanism (GD) mechanism, which is realized with
convolution and self-attention operations. This new designed model named as
Gold-YOLO, which boosts the multi-scale feature fusion capabilities and
achieves an ideal balance between latency and accuracy across all model scales.
Additionally, we implement MAE-style pretraining in the YOLO-series for the
first time, allowing YOLOseries models could be to benefit from unsupervised
pretraining. Gold-YOLO-N attains an outstanding 39.9% AP on the COCO val2017
datasets and 1030 FPS on a T4 GPU, which outperforms the previous SOTA model
YOLOv6-3.0-N with similar FPS by +2.4%. The PyTorch code is available at
https://github.com/huawei-noah/Efficient-Computing/tree/master/Detection/Gold-YOLO,
and the MindSpore code is available at
https://gitee.com/mindspore/models/tree/master/research/cv/Gold_YOLO.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11354">Self-supervised learning unveils change in urban housing from street-level images. (arXiv:2309.11354v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stalder_S/0/1/0/all/0/1">Steven Stalder</a>, <a href="http://arxiv.org/find/cs/1/au:+Volpi_M/0/1/0/all/0/1">Michele Volpi</a>, <a href="http://arxiv.org/find/cs/1/au:+Buttner_N/0/1/0/all/0/1">Nicolas B&#xfc;ttner</a>, <a href="http://arxiv.org/find/cs/1/au:+Law_S/0/1/0/all/0/1">Stephen Law</a>, <a href="http://arxiv.org/find/cs/1/au:+Harttgen_K/0/1/0/all/0/1">Kenneth Harttgen</a>, <a href="http://arxiv.org/find/cs/1/au:+Suel_E/0/1/0/all/0/1">Esra Suel</a></p>
<p>Cities around the world face a critical shortage of affordable and decent
housing. Despite its critical importance for policy, our ability to effectively
monitor and track progress in urban housing is limited. Deep learning-based
computer vision methods applied to street-level images have been successful in
the measurement of socioeconomic and environmental inequalities but did not
fully utilize temporal images to track urban change as time-varying labels are
often unavailable. We used self-supervised methods to measure change in London
using 15 million street images taken between 2008 and 2021. Our novel
adaptation of Barlow Twins, Street2Vec, embeds urban structure while being
invariant to seasonal and daily changes without manual annotations. It
outperformed generic embeddings, successfully identified point-level change in
London's housing supply from street-level images, and distinguished between
major and minor change. This capability can provide timely information for
urban planning and policy decisions toward more liveable, equitable, and
sustainable cities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.00216">Cross-scale Multi-instance Learning for Pathological Image Diagnosis. (arXiv:2304.00216v2 [eess.IV] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Deng_R/0/1/0/all/0/1">Ruining Deng</a>, <a href="http://arxiv.org/find/eess/1/au:+Cui_C/0/1/0/all/0/1">Can Cui</a>, <a href="http://arxiv.org/find/eess/1/au:+Remedios_L/0/1/0/all/0/1">Lucas W. Remedios</a>, <a href="http://arxiv.org/find/eess/1/au:+Bao_S/0/1/0/all/0/1">Shunxing Bao</a>, <a href="http://arxiv.org/find/eess/1/au:+Womick_R/0/1/0/all/0/1">R. Michael Womick</a>, <a href="http://arxiv.org/find/eess/1/au:+Chiron_S/0/1/0/all/0/1">Sophie Chiron</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1">Jia Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Roland_J/0/1/0/all/0/1">Joseph T. Roland</a>, <a href="http://arxiv.org/find/eess/1/au:+Lau_K/0/1/0/all/0/1">Ken S. Lau</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1">Qi Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Wilson_K/0/1/0/all/0/1">Keith T. Wilson</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1">Yaohong Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Coburn_L/0/1/0/all/0/1">Lori A. Coburn</a>, <a href="http://arxiv.org/find/eess/1/au:+Landman_B/0/1/0/all/0/1">Bennett A. Landman</a>, <a href="http://arxiv.org/find/eess/1/au:+Huo_Y/0/1/0/all/0/1">Yuankai Huo</a></p>
<p>Analyzing high resolution whole slide images (WSIs) with regard to
information across multiple scales poses a significant challenge in digital
pathology. Multi-instance learning (MIL) is a common solution for working with
high resolution images by classifying bags of objects (i.e. sets of smaller
image patches). However, such processing is typically performed at a single
scale (e.g., 20x magnification) of WSIs, disregarding the vital inter-scale
information that is key to diagnoses by human pathologists. In this study, we
propose a novel cross-scale MIL algorithm to explicitly aggregate inter-scale
relationships into a single MIL network for pathological image diagnosis. The
contribution of this paper is three-fold: (1) A novel cross-scale MIL (CS-MIL)
algorithm that integrates the multi-scale information and the inter-scale
relationships is proposed; (2) A toy dataset with scale-specific morphological
features is created and released to examine and visualize differential
cross-scale attention; (3) Superior performance on both in-house and public
datasets is demonstrated by our simple cross-scale MIL strategy. The official
implementation is publicly available at https://github.com/hrlblab/CS-MIL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11132">Contrastive Pseudo Learning for Open-World DeepFake Attribution. (arXiv:2309.11132v1 [cs.CV] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1">Zhimin Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1">Taiping Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1">Bangjie Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_R/0/1/0/all/0/1">Ran Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1">Shouhong Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1">Lizhuang Ma</a></p>
<p>The challenge in sourcing attribution for forgery faces has gained widespread
attention due to the rapid development of generative techniques. While many
recent works have taken essential steps on GAN-generated faces, more
threatening attacks related to identity swapping or expression transferring are
still overlooked. And the forgery traces hidden in unknown attacks from the
open-world unlabeled faces still remain under-explored. To push the related
frontier research, we introduce a new benchmark called Open-World DeepFake
Attribution (OW-DFA), which aims to evaluate attribution performance against
various types of fake faces under open-world scenarios. Meanwhile, we propose a
novel framework named Contrastive Pseudo Learning (CPL) for the OW-DFA task
through 1) introducing a Global-Local Voting module to guide the feature
alignment of forged faces with different manipulated regions, 2) designing a
Confidence-based Soft Pseudo-label strategy to mitigate the pseudo-noise caused
by similar methods in unlabeled set. In addition, we extend the CPL framework
with a multi-stage paradigm that leverages pre-train technique and iterative
learning to further enhance traceability performance. Extensive experiments
verify the superiority of our proposed method on the OW-DFA and also
demonstrate the interpretability of deepfake attribution task and its impact on
improving the security of deepfake detection area.
</p>
</p>
</div>

    </div>
    </body>
    