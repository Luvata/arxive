<!DOCTYPE html>
<html>
<head>
<title>2024-01-01-cs-cv</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2312.16197">INFAMOUS-NeRF: ImproviNg FAce MOdeling Using Semantically-Aligned Hypernetworks with Neural Radiance Fields. (arXiv:2312.16197v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hou_A/0/1/0/all/0/1">Andrew Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1">Feng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1">Zhiyuan Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarkis_M/0/1/0/all/0/1">Michel Sarkis</a>, <a href="http://arxiv.org/find/cs/1/au:+Bi_N/0/1/0/all/0/1">Ning Bi</a>, <a href="http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1">Yiying Tong</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiaoming Liu</a></p>
<p>We propose INFAMOUS-NeRF, an implicit morphable face model that introduces
hypernetworks to NeRF to improve the representation power in the presence of
many training subjects. At the same time, INFAMOUS-NeRF resolves the classic
hypernetwork tradeoff of representation power and editability by learning
semantically-aligned latent spaces despite the subject-specific models, all
without requiring a large pretrained model. INFAMOUS-NeRF further introduces a
novel constraint to improve NeRF rendering along the face boundary. Our
constraint can leverage photometric surface rendering and multi-view
supervision to guide surface color prediction and improve rendering near the
surface. Finally, we introduce a novel, loss-guided adaptive sampling method
for more effective NeRF training by reducing the sampling redundancy. We show
quantitatively and qualitatively that our method achieves higher representation
power than prior face modeling methods in both controlled and in-the-wild
settings. Code and models will be released upon publication.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16202">Time Travelling Pixels: Bitemporal Features Integration with Foundation Model for Remote Sensing Image Change Detection. (arXiv:2312.16202v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Keyan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chengyang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wenyuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zili Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haotian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1">Zhengxia Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1">Zhenwei Shi</a></p>
<p>Change detection, a prominent research area in remote sensing, is pivotal in
observing and analyzing surface transformations. Despite significant
advancements achieved through deep learning-based methods, executing
high-precision change detection in spatio-temporally complex remote sensing
scenarios still presents a substantial challenge. The recent emergence of
foundation models, with their powerful universality and generalization
capabilities, offers potential solutions. However, bridging the gap of data and
tasks remains a significant obstacle. In this paper, we introduce Time
Travelling Pixels (TTP), a novel approach that integrates the latent knowledge
of the SAM foundation model into change detection. This method effectively
addresses the domain shift in general knowledge transfer and the challenge of
expressing homogeneous and heterogeneous characteristics of multi-temporal
images. The state-of-the-art results obtained on the LEVIR-CD underscore the
efficacy of the TTP. The Code is available at \url{https://kychen.me/TTP}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16204">Iterative Prompt Relabeling for diffusion model with RLDF. (arXiv:2312.16204v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ge_J/0/1/0/all/0/1">Jiaxin Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xinyan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tianjun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shanghang Zhang</a></p>
<p>Diffusion models have shown impressive performance in many domains, including
image generation, time series prediction, and reinforcement learning. The
algorithm demonstrates superior performance over the traditional GAN and
transformer based methods. However, the model's capability to follow natural
language instructions (e.g., spatial relationships between objects, generating
complex scenes) is still unsatisfactory. This has been an important research
area to enhance such capability. Prior works adopt reinforcement learning to
adjust the behavior of the diffusion models. However, RL methods not only
require careful reward design and complex hyperparameter tuning, but also fails
to incorporate rich natural language feedback. In this work, we propose
iterative prompt relabeling (IP-RLDF), a novel algorithm that aligns images to
text through iterative image sampling and prompt relabeling. IP-RLDF first
samples a batch of images conditioned on the text, then relabels the text
prompts of unmatched text-image pairs with classifier feedback. We conduct
thorough experiments on three different models, including SDv2, GLIGEN, and
SDXL, testing their capability to generate images following instructions. With
IP-RLDF, we improved up to 15.22% (absolute improvement) on the challenging
spatial relation VISOR benchmark, demonstrating superior performance compared
to previous RL methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16215">SUNDIAL: 3D Satellite Understanding through Direct, Ambient, and Complex Lighting Decomposition. (arXiv:2312.16215v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Behari_N/0/1/0/all/0/1">Nikhil Behari</a>, <a href="http://arxiv.org/find/cs/1/au:+Dave_A/0/1/0/all/0/1">Akshat Dave</a>, <a href="http://arxiv.org/find/cs/1/au:+Tiwary_K/0/1/0/all/0/1">Kushagra Tiwary</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1">William Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Raskar_R/0/1/0/all/0/1">Ramesh Raskar</a></p>
<p>3D modeling from satellite imagery is essential in areas of environmental
science, urban planning, agriculture, and disaster response. However,
traditional 3D modeling techniques face unique challenges in the remote sensing
context, including limited multi-view baselines over extensive regions, varying
direct, ambient, and complex illumination conditions, and time-varying scene
changes across captures. In this work, we introduce SUNDIAL, a comprehensive
approach to 3D reconstruction of satellite imagery using neural radiance
fields. We jointly learn satellite scene geometry, illumination components, and
sun direction in this single-model approach, and propose a secondary shadow ray
casting technique to 1) improve scene geometry using oblique sun angles to
render shadows, 2) enable physically-based disentanglement of scene albedo and
illumination, and 3) determine the components of illumination from direct,
ambient (sky), and complex sources. To achieve this, we incorporate lighting
cues and geometric priors from remote sensing literature in a neural rendering
approach, modeling physical properties of satellite scenes such as shadows,
scattered sky illumination, and complex illumination and shading of vegetation
and water. We evaluate the performance of SUNDIAL against existing NeRF-based
techniques for satellite scene modeling and demonstrate improved scene and
lighting disentanglement, novel view and lighting rendering, and geometry and
sun direction estimation on challenging scenes with small baselines, sparse
inputs, and variable illumination.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16217">ManipLLM: Embodied Multimodal Large Language Model for Object-Centric Robotic Manipulation. (arXiv:2312.16217v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaoqi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Mingxu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Geng_Y/0/1/0/all/0/1">Yiran Geng</a>, <a href="http://arxiv.org/find/cs/1/au:+Geng_H/0/1/0/all/0/1">Haoran Geng</a>, <a href="http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1">Yuxing Long</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yan Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Renrui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiaming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1">Hao Dong</a></p>
<p>Robot manipulation relies on accurately predicting contact points and
end-effector directions to ensure successful operation. However, learning-based
robot manipulation, trained on a limited category within a simulator, often
struggles to achieve generalizability, especially when confronted with
extensive categories. Therefore, we introduce an innovative approach for robot
manipulation that leverages the robust reasoning capabilities of Multimodal
Large Language Models (MLLMs) to enhance the stability and generalization of
manipulation. By fine-tuning the injected adapters, we preserve the inherent
common sense and reasoning ability of the MLLMs while equipping them with the
ability for manipulation. The fundamental insight lies in the introduced
fine-tuning paradigm, encompassing object category understanding, affordance
prior reasoning, and object-centric pose prediction to stimulate the reasoning
ability of MLLM in manipulation. During inference, our approach utilizes an RGB
image and text prompt to predict the end effector's pose in chain of thoughts.
After the initial contact is established, an active impedance adaptation policy
is introduced to plan the upcoming waypoints in a closed-loop manner. Moreover,
in real world, we design a test-time adaptation (TTA) strategy for manipulation
to enable the model better adapt to the current real-world scene configuration.
Experiments in simulator and real-world show the promising performance of
ManipLLM. More details and demonstrations can be found at
https://sites.google.com/view/manipllm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16218">Hyper-VolTran: Fast and Generalizable One-Shot Image to 3D Object Structure via HyperNetworks. (arXiv:2312.16218v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Simon_C/0/1/0/all/0/1">Christian Simon</a>, <a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1">Sen He</a>, <a href="http://arxiv.org/find/cs/1/au:+Perez_Rua_J/0/1/0/all/0/1">Juan-Manuel Perez-Rua</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1">Frost Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Benhalloum_A/0/1/0/all/0/1">Amine Benhalloum</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1">Tao Xiang</a></p>
<p>Solving image-to-3D from a single view is an ill-posed problem, and current
neural reconstruction methods addressing it through diffusion models still rely
on scene-specific optimization, constraining their generalization capability.
To overcome the limitations of existing approaches regarding generalization and
consistency, we introduce a novel neural rendering technique. Our approach
employs the signed distance function as the surface representation and
incorporates generalizable priors through geometry-encoding volumes and
HyperNetworks. Specifically, our method builds neural encoding volumes from
generated multi-view inputs. We adjust the weights of the SDF network
conditioned on an input image at test-time to allow model adaptation to novel
scenes in a feed-forward manner via HyperNetworks. To mitigate artifacts
derived from the synthesized views, we propose the use of a volume transformer
module to improve the aggregation of image features instead of processing each
viewpoint separately. Through our proposed method, dubbed as Hyper-VolTran, we
avoid the bottleneck of scene-specific optimization and maintain consistency
across the images generated from multiple viewpoints. Our experiments show the
advantages of our proposed approach with consistent results and rapid
generation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16220">AI Mirage: The Impostor Bias and the Deepfake Detection Challenge in the Era of Artificial Illusions. (arXiv:2312.16220v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Casu_M/0/1/0/all/0/1">Mirko Casu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guarnera_L/0/1/0/all/0/1">Luca Guarnera</a>, <a href="http://arxiv.org/find/cs/1/au:+Caponnetto_P/0/1/0/all/0/1">Pasquale Caponnetto</a>, <a href="http://arxiv.org/find/cs/1/au:+Battiato_S/0/1/0/all/0/1">Sebastiano Battiato</a></p>
<p>This paper provides a comprehensive analysis of cognitive biases in forensics
and digital forensics, examining their implications for decision-making
processes in these fields. It explores the various types of cognitive biases
that may arise during forensic investigations and digital forensic analyses,
such as confirmation bias, expectation bias, overconfidence in errors,
contextual bias, and attributional biases. It also evaluates existing methods
and techniques used to mitigate cognitive biases in these contexts, assessing
the effectiveness of interventions aimed at reducing biases and improving
decision-making outcomes. Additionally, this paper introduces a new cognitive
bias, called "impostor bias", that may affect the use of generative Artificial
Intelligence (AI) tools in forensics and digital forensics. The impostor bias
is the tendency to doubt the authenticity or validity of the output generated
by AI tools, such as deepfakes, in the form of audio, images, and videos. This
bias may lead to erroneous judgments or false accusations, undermining the
reliability and credibility of forensic evidence. The paper discusses the
potential causes and consequences of the impostor bias, and suggests some
strategies to prevent or counteract it. By addressing these topics, this paper
seeks to offer valuable insights into understanding cognitive biases in
forensic practices and provide recommendations for future research and
practical applications to enhance the objectivity and validity of forensic
investigations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16221">TEMP3D: Temporally Continuous 3D Human Pose Estimation Under Occlusions. (arXiv:2312.16221v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lal_R/0/1/0/all/0/1">Rohit Lal</a>, <a href="http://arxiv.org/find/cs/1/au:+Garg_Y/0/1/0/all/0/1">Yash Garg</a>, <a href="http://arxiv.org/find/cs/1/au:+Dutta_A/0/1/0/all/0/1">Arindam Dutta</a>, <a href="http://arxiv.org/find/cs/1/au:+Ta_C/0/1/0/all/0/1">Calvin-Khang Ta</a>, <a href="http://arxiv.org/find/cs/1/au:+Raychaudhuri_D/0/1/0/all/0/1">Dripta S. Raychaudhuri</a>, <a href="http://arxiv.org/find/cs/1/au:+Asif_M/0/1/0/all/0/1">M. Salman Asif</a>, <a href="http://arxiv.org/find/cs/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1">Amit K. Roy-Chowdhury</a></p>
<p>Existing 3D human pose estimation methods perform remarkably well in both
monocular and multi-view settings. However, their efficacy diminishes
significantly in the presence of heavy occlusions, which limits their practical
utility. For video sequences, temporal continuity can help infer accurate
poses, especially in heavily occluded frames. In this paper, we aim to leverage
this potential of temporal continuity through human motion priors, coupled with
large-scale pre-training on 3D poses and self-supervised learning, to enhance
3D pose estimation in a given video sequence. This leads to a temporally
continuous 3D pose estimate on unlabelled in-the-wild videos, which may contain
occlusions, while exclusively relying on pre-trained 3D pose models. We propose
an unsupervised method named TEMP3D that aligns a motion prior model on a given
in-the-wild video using existing SOTA single image-based 3D pose estimation
methods to give temporally continuous output under occlusions. To evaluate our
method, we test it on the Occluded Human3.6M dataset, our custom-built dataset
which contains significantly large (up to 100%) human body occlusions
incorporated into the Human3.6M dataset. We achieve SOTA results on Occluded
Human3.6M and the OcMotion dataset while maintaining competitive performance on
non-occluded data. URL: https://sites.google.com/ucr.edu/temp3d
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16222">Segment Any Events via Weighted Adaptation of Pivotal Tokens. (arXiv:2312.16222v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhiwen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Zhiyu Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yifan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1">Junhui Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_G/0/1/0/all/0/1">Guangming Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jinjian Wu</a></p>
<p>In this paper, we delve into the nuanced challenge of tailoring the Segment
Anything Models (SAMs) for integration with event data, with the overarching
objective of attaining robust and universal object segmentation within the
event-centric domain. One pivotal issue at the heart of this endeavor is the
precise alignment and calibration of embeddings derived from event-centric data
such that they harmoniously coincide with those originating from RGB imagery.
Capitalizing on the vast repositories of datasets with paired events and RGB
images, our proposition is to harness and extrapolate the profound knowledge
encapsulated within the pre-trained SAM framework. As a cornerstone to
achieving this, we introduce a multi-scale feature distillation methodology.
This methodology rigorously optimizes the alignment of token embeddings
originating from event data with their RGB image counterparts, thereby
preserving and enhancing the robustness of the overall architecture.
Considering the distinct significance that token embeddings from intermediate
layers hold for higher-level embeddings, our strategy is centered on accurately
calibrating the pivotal token embeddings. This targeted calibration is aimed at
effectively managing the discrepancies in high-level embeddings originating
from both the event and image domains. Extensive experiments on different
datasets demonstrate the effectiveness of the proposed distillation method.
Code in <a href="http://github.com/happychenpipi/EventSAM.">this http URL</a>
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16226">Advancing Person Re-Identification: Tensor-based Feature Fusion and Multilinear Subspace Learning. (arXiv:2312.16226v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gharbi_A/0/1/0/all/0/1">Akram Abderraouf Gharbi</a>, <a href="http://arxiv.org/find/cs/1/au:+Chouchane_A/0/1/0/all/0/1">Ammar Chouchane</a>, <a href="http://arxiv.org/find/cs/1/au:+Ouamane_A/0/1/0/all/0/1">Abdelmalik Ouamane</a></p>
<p>Person re-identification (PRe-ID) is a computer vision issue, that has been a
fertile research area in the last few years. It aims to identify persons across
different non-overlapping camera views. In this paper, We propose a novel
PRe-ID system that combines tensor feature representation and multilinear
subspace learning. Our method exploits the power of pre-trained Convolutional
Neural Networks (CNNs) as a strong deep feature extractor, along with two
complementary descriptors, Local Maximal Occurrence (LOMO) and Gaussian Of
Gaussian (GOG). Then, Tensor-based Cross-View Quadratic Discriminant Analysis
(TXQDA) is used to learn a discriminative subspace that enhances the
separability between different individuals. Mahalanobis distance is used to
match and similarity computation between query and gallery samples. Finally, we
evaluate our approach by conducting experiments on three datasets VIPeR, GRID,
and PRID450s.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16240">Merging Vision Transformers from Different Tasks and Domains. (arXiv:2312.16240v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ye_P/0/1/0/all/0/1">Peng Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1">Chenyu Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_M/0/1/0/all/0/1">Mingzhu Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yongqi Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuning Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1">Wanli Ouyang</a></p>
<p>This work targets to merge various Vision Transformers (ViTs) trained on
different tasks (i.e., datasets with different object categories) or domains
(i.e., datasets with the same categories but different environments) into one
unified model, yielding still good performance on each task or domain. Previous
model merging works focus on either CNNs or NLP models, leaving the ViTs
merging research untouched. To fill this gap, we first explore and find that
existing model merging methods cannot well handle the merging of the whole ViT
models and still have improvement space. To enable the merging of the whole
ViT, we propose a simple-but-effective gating network that can both merge all
kinds of layers (e.g., Embedding, Norm, Attention, and MLP) and select the
suitable classifier. Specifically, the gating network is trained by unlabeled
datasets from all the tasks (domains), and predicts the probability of which
task (domain) the input belongs to for merging the models during inference. To
further boost the performance of the merged model, especially when the
difficulty of merging tasks increases, we design a novel metric of model weight
similarity, and utilize it to realize controllable and combined weight merging.
Comprehensive experiments on kinds of newly established benchmarks, validate
the superiority of the proposed ViT merging framework for different tasks and
domains. Our method can even merge beyond 10 ViT models from different vision
tasks with a negligible effect on the performance of each task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16244">Modality-missing RGBT Tracking via Invertible Prompt Learning and A High-quality Data Simulation Method. (arXiv:2312.16244v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_A/0/1/0/all/0/1">Andong Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_j/0/1/0/all/0/1">jiacong Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chenglong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jin Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1">Bin Luo</a></p>
<p>Current RGBT tracking researches mainly focus on the modality-complete
scenarios, overlooking the modality-missing challenge in real-world scenes. In
this work, we comprehensively investigate the impact of modality-missing
challenge in RGBT tracking and propose a novel invertible prompt learning
approach, which integrates the content-preserving prompts into a well-trained
tracking model to adapt to various modality-missing scenarios, for
modality-missing RGBT tracking. In particular, given one modality-missing
scenario, we propose to utilize the available modality to generate the prompt
of the missing modality to adapt to RGBT tracking model. However, the
cross-modality gap between available and missing modalities usually causes
semantic distortion and information loss in prompt generation. To handle this
issue, we propose the invertible prompt learning scheme by incorporating the
full reconstruction of the input available modality from the prompt in prompt
generation model. Considering that there lacks a modality-missing RGBT tracking
dataset and many modality-missing scenarios are difficult to capture, we design
a high-quality data simulation method based on hierarchical combination schemes
to generate real-world modality-missing data. Extensive experiments on three
modality-missing datasets show that our method achieves significant performance
improvements compared with state-of-the-art methods. We will release the code
and simulation dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16245">iKUN: Speak to Trackers without Retraining. (arXiv:2312.16245v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1">Yunhao Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_C/0/1/0/all/0/1">Cheng Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zhicheng Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_F/0/1/0/all/0/1">Fei Su</a></p>
<p>Referring multi-object tracking (RMOT) aims to track multiple objects based
on input textual descriptions. Previous works realize it by simply integrating
an extra textual module into the multi-object tracker. However, they typically
need to retrain the entire framework and have difficulties in optimization. In
this work, we propose an insertable Knowledge Unification Network, termed iKUN,
to enable communication with off-the-shelf trackers in a plug-and-play manner.
Concretely, a knowledge unification module (KUM) is designed to adaptively
extract visual features based on textual guidance. Meanwhile, to improve the
localization accuracy, we present a neural version of Kalman filter (NKF) to
dynamically adjust process noise and observation noise based on the current
motion status. Moreover, to address the problem of open-set long-tail
distribution of textual descriptions, a test-time similarity calibration method
is proposed to refine the confidence score with pseudo frequency. Extensive
experiments on Refer-KITTI dataset verify the effectiveness of our framework.
Finally, to speed up the development of RMOT, we also contribute a more
challenging dataset, Refer-Dance, by extending public DanceTrack dataset with
motion and dressing descriptions. The code and dataset will be released in
https://github.com/dyhBUPT/iKUN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16246">Nighttime Person Re-Identification via Collaborative Enhancement Network with Multi-domain Learning. (arXiv:2312.16246v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_A/0/1/0/all/0/1">Andong Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zha_T/0/1/0/all/0/1">Tianrui Zha</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chenglong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jin Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaofeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1">Bin Luo</a></p>
<p>Prevalent nighttime ReID methods typically combine relighting networks and
ReID networks in a sequential manner, which not only restricts the ReID
performance by the quality of relighting images, but also neglects the
effective collaborative modeling between image relighting and person ReID
tasks. To handle these problems, we propose a novel Collaborative Enhancement
Network called CENet, which performs the multilevel feature interactions in a
parallel framework, for nighttime person ReID. In particular, CENet is a
parallel Transformer network, in which the designed parallel structure can
avoid the impact of the quality of relighting images on ReID performance. To
perform effective collaborative modeling between image relighting and person
ReID tasks, we integrate the multilevel feature interactions in CENet.
Specifically, we share the Transformer encoder to build the low-level feature
interaction, and then perform the feature distillation to transfer the
high-level features from image relighting to ReID. In addition, the sizes of
existing real-world nighttime person ReID datasets are small, and large-scale
synthetic ones exhibit substantial domain gaps with real-world data. To
leverage both small-scale real-world and large-scale synthetic training data,
we develop a multi-domain learning algorithm, which alternately utilizes both
kinds of data to reduce the inter-domain difference in the training of CENet.
Extensive experiments on two real nighttime datasets, \textit{Night600} and
\textit{RGBNT201$_{rgb}$}, and a synthetic nighttime ReID dataset are conducted
to validate the effectiveness of CENet. We will release the code and synthetic
dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16247">Toward Accurate and Temporally Consistent Video Restoration from Raw Data. (arXiv:2312.16247v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1">Shi Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1">Jianqi Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhengqiang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lei Zhang</a></p>
<p>Denoising and demosaicking are two fundamental steps in reconstructing a
clean full-color video from raw data, while performing video denoising and
demosaicking jointly, namely VJDD, could lead to better video restoration
performance than performing them separately. In addition to restoration
accuracy, another key challenge to VJDD lies in the temporal consistency of
consecutive frames. This issue exacerbates when perceptual regularization terms
are introduced to enhance video perceptual quality. To address these
challenges, we present a new VJDD framework by consistent and accurate latent
space propagation, which leverages the estimation of previous frames as prior
knowledge to ensure consistent recovery of the current frame. A data temporal
consistency (DTC) loss and a relational perception consistency (RPC) loss are
accordingly designed. Compared with the commonly used flow-based losses, the
proposed losses can circumvent the error accumulation problem caused by
inaccurate flow estimation and effectively handle intensity changes in videos,
improving much the temporal consistency of output videos while preserving
texture details. Extensive experiments demonstrate the leading VJDD performance
of our method in term of restoration accuracy, perceptual quality and temporal
consistency. Codes and dataset are available at
\url{https://github.com/GuoShi28/VJDD}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16250">A Comprehensive Study of Object Tracking in Low-Light Environments. (arXiv:2312.16250v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yi_A/0/1/0/all/0/1">Anqi Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Anantrasirichai_N/0/1/0/all/0/1">Nantheera Anantrasirichai</a></p>
<p>Accurate object tracking in low-light environments is crucial, particularly
in surveillance and ethology applications. However, achieving this is
significantly challenging due to the poor quality of captured sequences.
Factors such as noise, color imbalance, and low contrast contribute to these
challenges. This paper presents a comprehensive study examining the impact of
these distortions on automatic object trackers. Additionally, we propose a
solution to enhance tracking performance by integrating denoising and low-light
enhancement methods into the transformer-based object tracking system.
Experimental results show that the proposed tracker, trained with low-light
synthetic datasets, outperforms both the vanilla MixFormer and Siam R-CNN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16251">MetaScript: Few-Shot Handwritten Chinese Content Generation via Generative Adversarial Networks. (arXiv:2312.16251v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1">Xiangyuan Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kailing Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bu_J/0/1/0/all/0/1">Jiazi Bu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qirui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhiyuan Zhang</a></p>
<p>In this work, we propose MetaScript, a novel Chinese content generation
system designed to address the diminishing presence of personal handwriting
styles in the digital representation of Chinese characters. Our approach
harnesses the power of few-shot learning to generate Chinese characters that
not only retain the individual's unique handwriting style but also maintain the
efficiency of digital typing. Trained on a diverse dataset of handwritten
styles, MetaScript is adept at producing high-quality stylistic imitations from
minimal style references and standard fonts. Our work demonstrates a practical
solution to the challenges of digital typography in preserving the personal
touch in written communication, particularly in the context of Chinese script.
Notably, our system has demonstrated superior performance in various
evaluations, including recognition accuracy, inception score, and Frechet
inception distance. At the same time, the training conditions of our model are
easy to meet and facilitate generalization to real applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16256">DL3DV-10K: A Large-Scale Scene Dataset for Deep Learning-based 3D Vision. (arXiv:2312.16256v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ling_L/0/1/0/all/0/1">Lu Ling</a>, <a href="http://arxiv.org/find/cs/1/au:+Sheng_Y/0/1/0/all/0/1">Yichen Sheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1">Zhi Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Wentian Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xin_C/0/1/0/all/0/1">Cheng Xin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_K/0/1/0/all/0/1">Kun Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1">Lantao Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1">Qianyu Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Zixun Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yawen Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xuanmao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xingpeng Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Ashok_R/0/1/0/all/0/1">Rohan Ashok</a>, <a href="http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1">Aniruddha Mukherjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1">Hao Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1">Xiangrui Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1">Gang Hua</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tianti Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Benes_B/0/1/0/all/0/1">Bedrich Benes</a>, <a href="http://arxiv.org/find/cs/1/au:+Bera_A/0/1/0/all/0/1">Aniket Bera</a></p>
<p>We have witnessed significant progress in deep learning-based 3D vision,
ranging from neural radiance field (NeRF) based 3D representation learning to
applications in novel view synthesis (NVS). However, existing scene-level
datasets for deep learning-based 3D vision, limited to either synthetic
environments or a narrow selection of real-world scenes, are quite
insufficient. This insufficiency not only hinders a comprehensive benchmark of
existing methods but also caps what could be explored in deep learning-based 3D
analysis. To address this critical gap, we present DL3DV-10K, a large-scale
scene dataset, featuring 51.2 million frames from 10,510 videos captured from
65 types of point-of-interest (POI) locations, covering both bounded and
unbounded scenes, with different levels of reflection, transparency, and
lighting. We conducted a comprehensive benchmark of recent NVS methods on
DL3DV-10K, which revealed valuable insights for future research in NVS. In
addition, we have obtained encouraging results in a pilot study to learn
generalizable NeRF from DL3DV-10K, which manifests the necessity of a
large-scale scene-level dataset to forge a path toward a foundation model for
learning 3D representation. Our DL3DV-10K dataset, benchmark results, and
models will be publicly accessible at https://dl3dv-10k.github.io/DL3DV-10K/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16264">SPnet: Estimating Garment Sewing Patterns from a Single Image. (arXiv:2312.16264v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1">Seungchan Lim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Sumin Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Sung-Hee Lee</a></p>
<p>This paper presents a novel method for reconstructing 3D garment models from
a single image of a posed user. Previous studies that have primarily focused on
accurately reconstructing garment geometries to match the input garment image
may often result in unnatural-looking garments when deformed for new poses. To
overcome this limitation, our approach takes a different approach by inferring
the fundamental shape of the garment through sewing patterns from a single
image, rather than directly reconstructing 3D garments. Our method consists of
two stages. Firstly, given a single image of a posed user, it predicts the
garment image worn on a T-pose, representing the baseline form of the garment.
Then, it estimates the sewing pattern parameters based on the T-pose garment
image. By simulating the stitching and draping of the sewing pattern using
physics simulation, we can generate 3D garments that can adaptively deform to
arbitrary poses. The effectiveness of our method is validated through ablation
studies on the major components and a comparison with other approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16268">360 Layout Estimation via Orthogonal Planes Disentanglement and Multi-view Geometric Consistency Perception. (arXiv:2312.16268v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1">Zhijie Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1">Chunyu Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Junsong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1">Lang Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_K/0/1/0/all/0/1">Kang Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yao Zhao</a></p>
<p>Existing panoramic layout estimation solutions tend to recover room
boundaries from a vertically compressed sequence, yielding imprecise results as
the compression process often muddles the semantics between various planes.
Besides, these data-driven approaches impose an urgent demand for massive data
annotations, which are laborious and time-consuming. For the first problem, we
propose an orthogonal plane disentanglement network (termed DOPNet) to
distinguish ambiguous semantics. DOPNet consists of three modules that are
integrated to deliver distortion-free, semantics-clean, and detail-sharp
disentangled representations, which benefit the subsequent layout recovery. For
the second problem, we present an unsupervised adaptation technique tailored
for horizon-depth and ratio representations. Concretely, we introduce an
optimization strategy for decision-level layout analysis and a 1D cost volume
construction method for feature-level multi-view aggregation, both of which are
designed to fully exploit the geometric consistency across multiple
perspectives. The optimizer provides a reliable set of pseudo-labels for
network training, while the 1D cost volume enriches each view with
comprehensive scene information derived from other perspectives. Extensive
experiments demonstrate that our solution outperforms other SoTA models on both
monocular layout estimation and multi-view layout estimation tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16272">SSR-Encoder: Encoding Selective Subject Representation for Subject-Driven Generation. (arXiv:2312.16272v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuxuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiaming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Yiren Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Rui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1">Hao Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Jinpeng Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Huaxia Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1">Xu Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yao Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1">Han Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Jing_Z/0/1/0/all/0/1">Zhongliang Jing</a></p>
<p>Recent advancements in subject-driven image generation have led to zero-shot
generation, yet precise selection and focus on crucial subject representations
remain challenging. Addressing this, we introduce the SSR-Encoder, a novel
architecture designed for selectively capturing any subject from single or
multiple reference images. It responds to various query modalities including
text and masks, without necessitating test-time fine-tuning. The SSR-Encoder
combines a Token-to-Patch Aligner that aligns query inputs with image patches
and a Detail-Preserving Subject Encoder for extracting and preserving fine
features of the subjects, thereby generating subject embeddings. These
embeddings, used in conjunction with original text embeddings, condition the
generation process. Characterized by its model generalizability and efficiency,
the SSR-Encoder adapts to a range of custom models and control modules.
Enhanced by the Embedding Consistency Regularization Loss for improved
training, our extensive experiments demonstrate its effectiveness in versatile
and high-quality image generation, indicating its broad applicability. Project
page: https://ssr-encoder.github.io
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16274">Towards Flexible, Scalable, and Adaptive Multi-Modal Conditioned Face Synthesis. (arXiv:2312.16274v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1">Jingjing Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Cheng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Haoyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1">Xinran Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chongyi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1">Lei Zhu</a></p>
<p>Recent progress in multi-modal conditioned face synthesis has enabled the
creation of visually striking and accurately aligned facial images. Yet,
current methods still face issues with scalability, limited flexibility, and a
one-size-fits-all approach to control strength, not accounting for the
differing levels of conditional entropy, a measure of unpredictability in data
given some condition, across modalities. To address these challenges, we
introduce a novel uni-modal training approach with modal surrogates, coupled
with an entropy-aware modal-adaptive modulation, to support flexible, scalable,
and scalable multi-modal conditioned face synthesis network. Our uni-modal
training with modal surrogate that only leverage uni-modal data, use modal
surrogate to decorate condition with modal-specific characteristic and serve as
linker for inter-modal collaboration , fully learns each modality control in
face synthesis process as well as inter-modal collaboration. The entropy-aware
modal-adaptive modulation finely adjust diffusion noise according to
modal-specific characteristics and given conditions, enabling well-informed
step along denoising trajectory and ultimately leading to synthesis results of
high fidelity and quality. Our framework improves multi-modal face synthesis
under various conditions, surpassing current methods in image quality and
fidelity, as demonstrated by our thorough experimental results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16279">Cloud-Device Collaborative Learning for Multimodal Large Language Models. (arXiv:2312.16279v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guanqun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiaming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chenxuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1">Junpeng Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1">Xinyu Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kevin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chong_M/0/1/0/all/0/1">Maurice Chong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Ray Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yijiang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shanghang Zhang</a></p>
<p>The burgeoning field of Multimodal Large Language Models (MLLMs) has
exhibited remarkable performance in diverse tasks such as captioning,
commonsense reasoning, and visual scene understanding. However, the deployment
of these large-scale MLLMs on client devices is hindered by their extensive
model parameters, leading to a notable decline in generalization capabilities
when these models are compressed for device deployment. Addressing this
challenge, we introduce a Cloud-Device Collaborative Continual Adaptation
framework, designed to enhance the performance of compressed, device-deployed
MLLMs by leveraging the robust capabilities of cloud-based, larger-scale MLLMs.
Our framework is structured into three key components: a device-to-cloud uplink
for efficient data transmission, cloud-based knowledge adaptation, and an
optimized cloud-to-device downlink for model deployment. In the uplink phase,
we employ an Uncertainty-guided Token Sampling (UTS) strategy to effectively
filter out-of-distribution tokens, thereby reducing transmission costs and
improving training efficiency. On the cloud side, we propose Adapter-based
Knowledge Distillation (AKD) method to transfer refined knowledge from
large-scale to compressed, pocket-size MLLMs. Furthermore, we propose a Dynamic
Weight update Compression (DWC) strategy for the downlink, which adaptively
selects and quantizes updated weight parameters, enhancing transmission
efficiency and reducing the representational disparity between cloud and device
models. Extensive experiments on several multimodal benchmarks demonstrate the
superiority of our proposed framework over prior Knowledge Distillation and
device-cloud collaboration methods. Notably, we also validate the feasibility
of our approach to real-world experiments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16331">Early and Accurate Detection of Tomato Leaf Diseases Using TomFormer. (arXiv:2312.16331v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Khan_A/0/1/0/all/0/1">Asim Khan</a>, <a href="http://arxiv.org/find/eess/1/au:+Nawaz_U/0/1/0/all/0/1">Umair Nawaz</a>, <a href="http://arxiv.org/find/eess/1/au:+Kshetrimayum_L/0/1/0/all/0/1">Lochan Kshetrimayum</a>, <a href="http://arxiv.org/find/eess/1/au:+Seneviratne_L/0/1/0/all/0/1">Lakmal Seneviratne</a>, <a href="http://arxiv.org/find/eess/1/au:+Hussain_I/0/1/0/all/0/1">Irfan Hussain</a></p>
<p>Tomato leaf diseases pose a significant challenge for tomato farmers,
resulting in substantial reductions in crop productivity. The timely and
precise identification of tomato leaf diseases is crucial for successfully
implementing disease management strategies. This paper introduces a
transformer-based model called TomFormer for the purpose of tomato leaf disease
detection. The paper's primary contributions include the following: Firstly, we
present a novel approach for detecting tomato leaf diseases by employing a
fusion model that combines a visual transformer and a convolutional neural
network. Secondly, we aim to apply our proposed methodology to the Hello
Stretch robot to achieve real-time diagnosis of tomato leaf diseases. Thirdly,
we assessed our method by comparing it to models like YOLOS, DETR, ViT, and
Swin, demonstrating its ability to achieve state-of-the-art outcomes. For the
purpose of the experiment, we used three datasets of tomato leaf diseases,
namely KUTomaDATA, PlantDoc, and PlanVillage, where KUTomaDATA is being
collected from a greenhouse in Abu Dhabi, UAE. Finally, we present a
comprehensive analysis of the performance of our model and thoroughly discuss
the limitations inherent in our approach. TomFormer performed well on the
KUTomaDATA, PlantDoc, and PlantVillage datasets, with mean average accuracy
(mAP) scores of 87%, 81%, and 83%, respectively. The comparative results in
terms of mAP demonstrate that our method exhibits robustness, accuracy,
efficiency, and scalability. Furthermore, it can be readily adapted to new
datasets. We are confident that our work holds the potential to significantly
influence the tomato industry by effectively mitigating crop losses and
enhancing crop yields.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16338">State-of-the-Art in Nudity Classification: A Comparative Analysis. (arXiv:2312.16338v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Akyon_F/0/1/0/all/0/1">Fatih Cagatay Akyon</a>, <a href="http://arxiv.org/find/cs/1/au:+Temizel_A/0/1/0/all/0/1">Alptekin Temizel</a></p>
<p>This paper presents a comparative analysis of existing nudity classification
techniques for classifying images based on the presence of nudity, with a focus
on their application in content moderation. The evaluation focuses on CNN-based
models, vision transformer, and popular open-source safety checkers from Stable
Diffusion and Large-scale Artificial Intelligence Open Network (LAION). The
study identifies the limitations of current evaluation datasets and highlights
the need for more diverse and challenging datasets. The paper discusses the
potential implications of these findings for developing more accurate and
effective image classification systems on online platforms. Overall, the study
emphasizes the importance of continually improving image classification models
to ensure the safety and well-being of platform users. The project page,
including the demonstrations and results is publicly available at
https://github.com/fcakyon/content-moderation-deep-learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16339">Universal Pyramid Adversarial Training for Improved ViT Performance. (arXiv:2312.16339v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chiang_P/0/1/0/all/0/1">Ping-yeh Chiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yipin Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Poursaeed_O/0/1/0/all/0/1">Omid Poursaeed</a>, <a href="http://arxiv.org/find/cs/1/au:+Shukla_S/0/1/0/all/0/1">Satya Narayan Shukla</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1">Ashish Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1">Tom Goldstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1">Ser-Nam Lim</a></p>
<p>Recently, Pyramid Adversarial training (Herrmann et al., 2022) has been shown
to be very effective for improving clean accuracy and distribution-shift
robustness of vision transformers. However, due to the iterative nature of
adversarial training, the technique is up to 7 times more expensive than
standard training. To make the method more efficient, we propose Universal
Pyramid Adversarial training, where we learn a single pyramid adversarial
pattern shared across the whole dataset instead of the sample-wise patterns.
With our proposed technique, we decrease the computational cost of Pyramid
Adversarial training by up to 70% while retaining the majority of its benefit
on clean performance and distribution-shift robustness. In addition, to the
best of our knowledge, we are also the first to find that universal adversarial
training can be leveraged to improve clean model performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16388">Gaussian Mixture Proposals with Pull-Push Learning Scheme to Capture Diverse Events for Weakly Supervised Temporal Video Grounding. (arXiv:2312.16388v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Sunoh Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1">Jungchan Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Joonsang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1">YoungJoon Yoo</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1">Jin Young Choi</a></p>
<p>In the weakly supervised temporal video grounding study, previous methods use
predetermined single Gaussian proposals which lack the ability to express
diverse events described by the sentence query. To enhance the expression
ability of a proposal, we propose a Gaussian mixture proposal (GMP) that can
depict arbitrary shapes by learning importance, centroid, and range of every
Gaussian in the mixture. In learning GMP, each Gaussian is not trained in a
feature space but is implemented over a temporal location. Thus the
conventional feature-based learning for Gaussian mixture model is not valid for
our case. In our special setting, to learn moderately coupled Gaussian mixture
capturing diverse events, we newly propose a pull-push learning scheme using
pulling and pushing losses, each of which plays an opposite role to the other.
The effects of components in our scheme are verified in-depth with extensive
ablation studies and the overall scheme achieves state-of-the-art performance.
Our code is available at https://github.com/sunoh-kim/pps.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16392">Adaptive Depth Networks with Skippable Sub-Paths. (arXiv:2312.16392v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1">Woochul Kang</a></p>
<p>Systematic adaptation of network depths at runtime can be an effective way to
control inference latency and meet the resource condition of various devices.
However, previous depth adaptive networks do not provide general principles and
a formal explanation on why and which layers can be skipped, and, hence, their
approaches are hard to be generalized and require long and complex training
steps. In this paper, we present an architectural pattern and training method
for adaptive depth networks that can provide flexible accuracy-efficiency
trade-offs in a single network. In our approach, every residual stage is
divided into 2 consecutive sub-paths with different properties. While the first
sub-path is mandatory for hierarchical feature learning, the other is optimized
to incur minimal performance degradation even if it is skipped. Unlike previous
adaptive networks, our approach does not iteratively self-distill a fixed set
of sub-networks, resulting in significantly shorter training time. However,
once deployed on devices, it can instantly construct sub-networks of varying
depths to provide various accuracy-efficiency trade-offs in a single model. We
provide a formal rationale for why the proposed architectural pattern and
training method can reduce overall prediction errors while minimizing the
impact of skipping selected sub-paths. We also demonstrate the generality and
effectiveness of our approach with various residual networks, both from
convolutional neural networks and vision transformers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16401">Natural Adversarial Patch Generation Method Based on Latent Diffusion Model. (arXiv:2312.16401v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xianyi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1">Fazhan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1">Dong Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1">Kai Yan</a></p>
<p>Recently, some research show that deep neural networks are vulnerable to the
adversarial attacks, the well-trainned samples or patches could be used to
trick the neural network detector or human visual perception. However, these
adversarial patches, with their conspicuous and unusual patterns, lack
camouflage and can easily raise suspicion in the real world. To solve this
problem, this paper proposed a novel adversarial patch method called the Latent
Diffusion Patch (LDP), in which, a pretrained encoder is first designed to
compress the natural images into a feature space with key characteristics. Then
trains the diffusion model using the above feature space. Finally, explore the
latent space of the pretrained diffusion model using the image denoising
technology. It polishes the patches and images through the powerful natural
abilities of diffusion models, making them more acceptable to the human visual
system. Experimental results, both digital and physical worlds, show that LDPs
achieve a visual subjectivity score of 87.3%, while still maintaining effective
attack capabilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16409">Dynamic Sub-graph Distillation for Robust Semi-supervised Continual Learning. (arXiv:2312.16409v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1">Yan Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1">Pengfei Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1">Qinghua Hu</a></p>
<p>Continual learning (CL) has shown promising results and comparable
performance to learning at once in a fully supervised manner. However, CL
strategies typically require a large number of labeled samples, making their
real-life deployment challenging. In this work, we focus on semi-supervised
continual learning (SSCL), where the model progressively learns from partially
labeled data with unknown categories. We provide a comprehensive analysis of
SSCL and demonstrate that unreliable distributions of unlabeled data lead to
unstable training and refinement of the progressing stages. This problem
severely impacts the performance of SSCL. To address the limitations, we
propose a novel approach called Dynamic Sub-Graph Distillation (DSGD) for
semi-supervised continual learning, which leverages both semantic and
structural information to achieve more stable knowledge distillation on
unlabeled data and exhibit robustness against distribution bias. Firstly, we
formalize a general model of structural distillation and design a dynamic graph
construction for the continual learning progress. Next, we define a structure
distillation vector and design a dynamic sub-graph distillation algorithm,
which enables end-to-end training and adaptability to scale up tasks. The
entire proposed method is adaptable to various CL methods and supervision
settings. Finally, experiments conducted on three datasets CIFAR10, CIFAR100,
and ImageNet-100, with varying supervision ratios, demonstrate the
effectiveness of our proposed approach in mitigating the catastrophic
forgetting problem in semi-supervised continual learning scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16410">Segment Change Model (SCM) for Unsupervised Change detection in VHR Remote Sensing Images: a Case Study of Buildings. (arXiv:2312.16410v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1">Xiaoliang Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1">Guanzhou Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiaqi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaodong Zhang</a></p>
<p>The field of Remote Sensing (RS) widely employs Change Detection (CD) on
very-high-resolution (VHR) images. A majority of extant deep-learning-based
methods hinge on annotated samples to complete the CD process. Recently, the
emergence of Vision Foundation Model (VFM) enables zero-shot predictions in
particular vision tasks. In this work, we propose an unsupervised CD method
named Segment Change Model (SCM), built upon the Segment Anything Model (SAM)
and Contrastive Language-Image Pre-training (CLIP). Our method recalibrates
features extracted at different scales and integrates them in a top-down manner
to enhance discriminative change edges. We further design an innovative
Piecewise Semantic Attention (PSA) scheme, which can offer semantic
representation without training, thereby minimize pseudo change phenomenon.
Through conducting experiments on two public datasets, the proposed SCM
increases the mIoU from 46.09% to 53.67% on the LEVIR-CD dataset, and from
47.56% to 52.14% on the WHU-CD dataset. Our codes are available at
https://github.com/StephenApX/UCD-SCM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16414">Bellman Optimal Step-size Straightening of Flow-Matching Models. (arXiv:2312.16414v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1">Bao Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1">Binh Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1">Viet Anh Nguyen</a></p>
<p>Flow matching is a powerful framework for generating high-quality samples in
various applications, especially image synthesis. However, the intensive
computational demands of these models, especially during the fine-tuning
process and sampling processes, pose significant challenges for low-resource
scenarios. This paper introduces Bellman Optimal Step-size Straightening (BOSS)
technique for distilling flow-matching generative models: it aims specifically
for a few step efficient image sampling while adhering to a computational
budget constraint. First, this technique involves a dynamic programming
algorithm that optimizes the step sizes of the pretrained network. Then, it
refines the velocity network to match the optimal step sizes, aiming to
straighten the generation paths. Extensive experimental evaluations across
image generation tasks demonstrate the efficacy of BOSS in terms of both
resource utilization and image quality. Our results reveal that BOSS achieves
substantial gains in efficiency while maintaining competitive sample quality,
effectively bridging the gap between low-resource constraints and the demanding
requirements of flow-matching generative models. Our paper also fortifies the
responsible development of artificial intelligence, offering a more sustainable
generative model that reduces computational costs and environmental footprints.
Our code can be found at https://anonymous.4open.science/r/DRL-8E88.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16425">In-Hand 3D Object Reconstruction from a Monocular RGB Video. (arXiv:2312.16425v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1">Shijian Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1">Qi Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1">Rengan Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1">Yuchi Huo</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiming Chen</a></p>
<p>Our work aims to reconstruct a 3D object that is held and rotated by a hand
in front of a static RGB camera. Previous methods that use implicit neural
representations to recover the geometry of a generic hand-held object from
multi-view images achieved compelling results in the visible part of the
object. However, these methods falter in accurately capturing the shape within
the hand-object contact region due to occlusion. In this paper, we propose a
novel method that deals with surface reconstruction under occlusion by
incorporating priors of 2D occlusion elucidation and physical contact
constraints. For the former, we introduce an object amodal completion network
to infer the 2D complete mask of objects under occlusion. To ensure the
accuracy and view consistency of the predicted 2D amodal masks, we devise a
joint optimization method for both amodal mask refinement and 3D
reconstruction. For the latter, we impose penetration and attraction
constraints on the local geometry in contact regions. We evaluate our approach
on HO3D and HOD datasets and demonstrate that it outperforms the
state-of-the-art methods in terms of reconstruction surface quality, with an
improvement of $52\%$ on HO3D and $20\%$ on HOD. Project webpage:
https://east-j.github.io/ihor.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16451">Domain Generalization with Vital Phase Augmentation. (arXiv:2312.16451v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1">Ingyun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1">Wooju Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Myung_H/0/1/0/all/0/1">Hyun Myung</a></p>
<p>Deep neural networks have shown remarkable performance in image
classification. However, their performance significantly deteriorates with
corrupted input data. Domain generalization methods have been proposed to train
robust models against out-of-distribution data. Data augmentation in the
frequency domain is one of such approaches that enable a model to learn phase
features to establish domain-invariant representations. This approach changes
the amplitudes of the input data while preserving the phases. However, using
fixed phases leads to susceptibility to phase fluctuations because amplitudes
and phase fluctuations commonly occur in out-of-distribution. In this study, to
address this problem, we introduce an approach using finite variation of the
phases of input data rather than maintaining fixed phases. Based on the
assumption that the degree of domain-invariant features varies for each phase,
we propose a method to distinguish phases based on this degree. In addition, we
propose a method called vital phase augmentation (VIPAug) that applies the
variation to the phases differently according to the degree of domain-invariant
features of given phases. The model depends more on the vital phases that
contain more domain-invariant features for attaining robustness to amplitude
and phase fluctuations. We present experimental evaluations of our proposed
approach, which exhibited improved performance for both clean and corrupted
data. VIPAug achieved SOTA performance on the benchmark CIFAR-10 and CIFAR-100
datasets, as well as near-SOTA performance on the ImageNet-100 and ImageNet
datasets. Our code is available at https://github.com/excitedkid/vipaug.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16455">Learn From Orientation Prior for Radiograph Super-Resolution: Orientation Operator Transformer. (arXiv:2312.16455v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1">Yongsong Huang</a>, <a href="http://arxiv.org/find/eess/1/au:+Miyazaki_T/0/1/0/all/0/1">Tomo Miyazaki</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1">Xiaofeng Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Jiang_K/0/1/0/all/0/1">Kaiyuan Jiang</a>, <a href="http://arxiv.org/find/eess/1/au:+Tang_Z/0/1/0/all/0/1">Zhengmi Tang</a>, <a href="http://arxiv.org/find/eess/1/au:+Omachi_S/0/1/0/all/0/1">Shinichiro Omachi</a></p>
<p>Background and objective: High-resolution radiographic images play a pivotal
role in the early diagnosis and treatment of skeletal muscle-related diseases.
It is promising to enhance image quality by introducing single-image
super-resolution (SISR) model into the radiology image field. However, the
conventional image pipeline, which can learn a mixed mapping between SR and
denoising from the color space and inter-pixel patterns, poses a particular
challenge for radiographic images with limited pattern features. To address
this issue, this paper introduces a novel approach: Orientation Operator
Transformer - $O^{2}$former. Methods: We incorporate an orientation operator in
the encoder to enhance sensitivity to denoising mapping and to integrate
orientation prior. Furthermore, we propose a multi-scale feature fusion
strategy to amalgamate features captured by different receptive fields with the
directional prior, thereby providing a more effective latent representation for
the decoder. Based on these innovative components, we propose a
transformer-based SISR model, i.e., $O^{2}$former, specifically designed for
radiographic images. Results: The experimental results demonstrate that our
method achieves the best or second-best performance in the objective metrics
compared with the competitors at $\times 4$ upsampling factor. For qualitative,
more objective details are observed to be recovered. Conclusions: In this
study, we propose a novel framework called $O^{2}$former for radiological image
super-resolution tasks, which improves the reconstruction model's performance
by introducing an orientation operator and multi-scale feature fusion strategy.
Our approach is promising to further promote the radiographic image enhancement
field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16457">City-on-Web: Real-time Neural Rendering of Large-scale Scenes on the Web. (arXiv:2312.16457v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1">Kaiwen Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Juyong Zhang</a></p>
<p>NeRF has significantly advanced 3D scene reconstruction, capturing intricate
details across various environments. Existing methods have successfully
leveraged radiance field baking to facilitate real-time rendering of small
scenes. However, when applied to large-scale scenes, these techniques encounter
significant challenges, struggling to provide a seamless real-time experience
due to limited resources in computation, memory, and bandwidth. In this paper,
we propose City-on-Web, which represents the whole scene by partitioning it
into manageable blocks, each with its own Level-of-Detail, ensuring high
fidelity, efficient memory management and fast rendering. Meanwhile, we
carefully design the training and inference process such that the final
rendering result on web is consistent with training. Thanks to our novel
representation and carefully designed training/inference process, we are the
first to achieve real-time rendering of large-scale scenes in
resource-constrained environments. Extensive experimental results demonstrate
that our method facilitates real-time rendering of large-scale scenes on a web
platform, achieving 32FPS at 1080P resolution with an RTX 3060 GPU, while
simultaneously achieving a quality that closely rivals that of state-of-the-art
methods. Project page: https://ustc3dv.github.io/City-on-Web/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16470">ReSynthDetect: A Fundus Anomaly Detection Network with Reconstruction and Synthetic Features. (arXiv:2312.16470v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Niu_J/0/1/0/all/0/1">Jingqi Niu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1">Qinji Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1">Shiwen Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zilong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dang_K/0/1/0/all/0/1">Kang Dang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1">Xiaowei Ding</a></p>
<p>Detecting anomalies in fundus images through unsupervised methods is a
challenging task due to the similarity between normal and abnormal tissues, as
well as their indistinct boundaries. The current methods have limitations in
accurately detecting subtle anomalies while avoiding false positives. To
address these challenges, we propose the ReSynthDetect network which utilizes a
reconstruction network for modeling normal images, and an anomaly generator
that produces synthetic anomalies consistent with the appearance of fundus
images. By combining the features of consistent anomaly generation and image
reconstruction, our method is suited for detecting fundus abnormalities. The
proposed approach has been extensively tested on benchmark datasets such as
EyeQ and IDRiD, demonstrating state-of-the-art performance in both image-level
and pixel-level anomaly detection. Our experiments indicate a substantial 9%
improvement in AUROC on EyeQ and a significant 17.1% improvement in AUPR on
IDRiD.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16471">A Survey on Super Resolution for video Enhancement Using GAN. (arXiv:2312.16471v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Maity_A/0/1/0/all/0/1">Ankush Maity</a>, <a href="http://arxiv.org/find/eess/1/au:+Pious_R/0/1/0/all/0/1">Roshan Pious</a>, <a href="http://arxiv.org/find/eess/1/au:+Lenka_S/0/1/0/all/0/1">Sourabh Kumar Lenka</a>, <a href="http://arxiv.org/find/eess/1/au:+Choudhary_V/0/1/0/all/0/1">Vishal Choudhary</a>, <a href="http://arxiv.org/find/eess/1/au:+Lokande_P/0/1/0/all/0/1">Prof.Sharyau Lokande</a></p>
<p>This compilation of various research paper highlights provides a
comprehensive overview of recent developments in super-resolution image and
video using deep learning algorithms such as Generative Adversarial Networks.
The studies covered in these summaries provide fresh techniques to addressing
the issues of improving image and video quality, such as recursive learning for
video super-resolution, novel loss functions, frame-rate enhancement, and
attention model integration. These approaches are frequently evaluated using
criteria such as PSNR, SSIM, and perceptual indices. These advancements, which
aim to increase the visual clarity and quality of low-resolution video, have
tremendous potential in a variety of sectors ranging from surveillance
technology to medical imaging. In addition, this collection delves into the
wider field of Generative Adversarial Networks, exploring their principles,
training approaches, and applications across a broad range of domains, while
also emphasizing the challenges and opportunities for future research in this
rapidly advancing and changing field of artificial intelligence.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16476">SVGDreamer: Text Guided SVG Generation with Diffusion Model. (arXiv:2312.16476v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xing_X/0/1/0/all/0/1">Ximing Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Haitao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chuang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1">Dong Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1">Qian Yu</a></p>
<p>Recently, text-guided scalable vector graphics (SVGs) synthesis has shown
promise in domains such as iconography and sketch. However, existing
text-to-SVG generation methods lack editability and struggle with visual
quality and result diversity. To address these limitations, we propose a novel
text-guided vector graphics synthesis method called SVGDreamer. SVGDreamer
incorporates a semantic-driven image vectorization (SIVE) process that enables
the decomposition of synthesis into foreground objects and background, thereby
enhancing editability. Specifically, the SIVE process introduce attention-based
primitive control and an attention-mask loss function for effective control and
manipulation of individual elements. Additionally, we propose a Vectorized
Particle-based Score Distillation (VPSD) approach to tackle the challenges of
color over-saturation, vector primitives over-smoothing, and limited result
diversity in existing text-to-SVG generation methods. Furthermore, on the basis
of VPSD, we introduce Reward Feedback Learning (ReFL) to accelerate VPSD
convergence and improve aesthetic appeal. Extensive experiments have been
conducted to validate the effectiveness of SVGDreamer, demonstrating its
superiority over baseline methods in terms of editability, visual quality, and
diversity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16477">Group Multi-View Transformer for 3D Shape Analysis with Spatial Encoding. (arXiv:2312.16477v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1">Lixiang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_Q/0/1/0/all/0/1">Qingzhe Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_R/0/1/0/all/0/1">Richang Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Wei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1">Enhong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1">Xin Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1">Yuanyan Tang</a></p>
<p>In recent years, the results of view-based 3D shape recognition methods have
saturated, and models with excellent performance cannot be deployed on
memory-limited devices due to their huge size of parameters. To address this
problem, we introduce a compression method based on knowledge distillation for
this field, which largely reduces the number of parameters while preserving
model performance as much as possible. Specifically, to enhance the
capabilities of smaller models, we design a high-performing large model called
Group Multi-view Vision Transformer (GMViT). In GMViT, the view-level ViT first
establishes relationships between view-level features. Additionally, to capture
deeper features, we employ the grouping module to enhance view-level features
into group-level features. Finally, the group-level ViT aggregates group-level
features into complete, well-formed 3D shape descriptors. Notably, in both
ViTs, we introduce spatial encoding of camera coordinates as innovative
position embeddings. Furthermore, we propose two compressed versions based on
GMViT, namely GMViT-simple and GMViT-mini. To enhance the training
effectiveness of the small models, we introduce a knowledge distillation method
throughout the GMViT process, where the key outputs of each GMViT component
serve as distillation targets. Extensive experiments demonstrate the efficacy
of the proposed method. The large model GMViT achieves excellent 3D
classification and retrieval results on the benchmark datasets ModelNet,
ShapeNetCore55, and MCB. The smaller models, GMViT-simple and GMViT-mini,
reduce the parameter size by 8 and 17.6 times, respectively, and improve shape
recognition speed by 1.5 times on average, while preserving at least 90% of the
classification and retrieval performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16486">PanGu-Draw: Advancing Resource-Efficient Text-to-Image Synthesis with Time-Decoupled Training and Reusable Coop-Diffusion. (arXiv:2312.16486v2 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1">Guansong Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yuanfan Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Jianhua Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Niu_M/0/1/0/all/0/1">Minzhe Niu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1">Yihan Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Songcen Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zeyi Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1">Zhao Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hang Xu</a></p>
<p>Current large-scale diffusion models represent a giant leap forward in
conditional image synthesis, capable of interpreting diverse cues like text,
human poses, and edges. However, their reliance on substantial computational
resources and extensive data collection remains a bottleneck. On the other
hand, the integration of existing diffusion models, each specialized for
different controls and operating in unique latent spaces, poses a challenge due
to incompatible image resolutions and latent space embedding structures,
hindering their joint use. Addressing these constraints, we present
"PanGu-Draw", a novel latent diffusion model designed for resource-efficient
text-to-image synthesis that adeptly accommodates multiple control signals. We
first propose a resource-efficient Time-Decoupling Training Strategy, which
splits the monolithic text-to-image model into structure and texture
generators. Each generator is trained using a regimen that maximizes data
utilization and computational efficiency, cutting data preparation by 48% and
reducing training resources by 51%. Secondly, we introduce "Coop-Diffusion", an
algorithm that enables the cooperative use of various pre-trained diffusion
models with different latent spaces and predefined resolutions within a unified
denoising process. This allows for multi-control image synthesis at arbitrary
resolutions without the necessity for additional data or retraining. Empirical
validations of Pangu-Draw show its exceptional prowess in text-to-image and
multi-control image generation, suggesting a promising direction for future
model training efficiencies and generation versatility. The largest 5B T2I
PanGu-Draw model is released on the Ascend platform. Project page:
$\href{https://pangu-draw.github.io}{this~https~URL}$
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16498">A Non-Uniform Low-Light Image Enhancement Method with Multi-Scale Attention Transformer and Luminance Consistency Loss. (arXiv:2312.16498v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1">Xiao Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xin Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Baofeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_F/0/1/0/all/0/1">Feng Zhai</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1">Yu Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1">Zhihang Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Jiansheng Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Chun Xiao</a></p>
<p>Low-light image enhancement aims to improve the perception of images
collected in dim environments and provide high-quality data support for image
recognition tasks. When dealing with photos captured under non-uniform
illumination, existing methods cannot adaptively extract the differentiated
luminance information, which will easily cause over-exposure and
under-exposure. From the perspective of unsupervised learning, we propose a
multi-scale attention Transformer named MSATr, which sufficiently extracts
local and global features for light balance to improve the visual quality.
Specifically, we present a multi-scale window division scheme, which uses
exponential sequences to adjust the window size of each layer. Within
different-sized windows, the self-attention computation can be refined,
ensuring the pixel-level feature processing capability of the model. For
feature interaction across windows, a global transformer branch is constructed
to provide comprehensive brightness perception and alleviate exposure problems.
Furthermore, we propose a loop training strategy, using the diverse images
generated by weighted mixing and a luminance consistency loss to improve the
model's generalization ability effectively. Extensive experiments on several
benchmark datasets quantitatively and qualitatively prove that our MSATr is
superior to state-of-the-art low-light image enhancement methods, and the
enhanced images have more natural brightness and outstanding details. The code
is released at https://github.com/fang001021/MSATr.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16499">Camera calibration for the surround-view system: a benchmark and dataset. (arXiv:2312.16499v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1">L Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1">C Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">S Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">S Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Y Zhao</a></p>
<p>Surround-view system (SVS) is widely used in the Advanced Driver Assistance
System (ADAS). SVS uses four fisheye lenses to monitor real-time scenes around
the vehicle. However, accurate intrinsic and extrinsic parameter estimation is
required for the proper functioning of the system. At present, the intrinsic
calibration can be pipeline by utilizing checkerboard algorithm, while
extrinsic calibration is still immature. Therefore, we proposed a specific
calibration pipeline to estimate extrinsic parameters robustly. This scheme
takes a driving sequence of four cameras as input. It firstly utilizes lane
line to roughly estimate each camera pose. Considering the environmental
condition differences in each camera, we separately select strategies from two
methods to accurately estimate the extrinsic parameters. To achieve accurate
estimates for both front and rear camera, we proposed a method that mutually
iterating line detection and pose estimation. As for bilateral camera, we
iteratively adjust the camera pose and position by minimizing texture and edge
error between ground projections of adjacent cameras. After estimating the
extrinsic parameters, the surround-view image can be synthesized by
homography-based transformation. The proposed pipeline can robustly estimate
the four SVS camera extrinsic parameters in real driving environments. In
addition, to evaluate the proposed scheme, we build a surround-view fisheye
dataset, which contains 40 videos with 32,000 frames, acquired from different
real traffic scenarios. All the frames in each video are manually labeled with
lane annotation, with its GT extrinsic parameters. Moreover, this surround-view
dataset could be used by other researchers to evaluate their performance. The
dataset will be available soon.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16516">ConstScene: Dataset and Model for Advancing Robust Semantic Segmentation in Construction Environments. (arXiv:2312.16516v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Salimi_M/0/1/0/all/0/1">Maghsood Salimi</a>, <a href="http://arxiv.org/find/cs/1/au:+Loni_M/0/1/0/all/0/1">Mohammad Loni</a>, <a href="http://arxiv.org/find/cs/1/au:+Afshar_S/0/1/0/all/0/1">Sara Afshar</a>, <a href="http://arxiv.org/find/cs/1/au:+Sirjani_M/0/1/0/all/0/1">Marjan Sirjani</a>, <a href="http://arxiv.org/find/cs/1/au:+Cicchetti_A/0/1/0/all/0/1">Antonio Cicchetti</a></p>
<p>The increasing demand for autonomous machines in construction environments
necessitates the development of robust object detection algorithms that can
perform effectively across various weather and environmental conditions. This
paper introduces a new semantic segmentation dataset specifically tailored for
construction sites, taking into account the diverse challenges posed by adverse
weather and environmental conditions. The dataset is designed to enhance the
training and evaluation of object detection models, fostering their
adaptability and reliability in real-world construction applications. Our
dataset comprises annotated images captured under a wide range of different
weather conditions, including but not limited to sunny days, rainy periods,
foggy atmospheres, and low-light situations. Additionally, environmental
factors such as the existence of dirt/mud on the camera lens are integrated
into the dataset through actual captures and synthetic generation to simulate
the complex conditions prevalent in construction sites. We also generate
synthetic images of the annotations including precise semantic segmentation
masks for various objects commonly found in construction environments, such as
wheel loader machines, personnel, cars, and structural elements. To demonstrate
the dataset's utility, we evaluate state-of-the-art object detection algorithms
on our proposed benchmark. The results highlight the dataset's success in
adversarial training models across diverse conditions, showcasing its efficacy
compared to existing datasets that lack such environmental variability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16519">Image Restoration by Denoising Diffusion Models with Iteratively Preconditioned Guidance. (arXiv:2312.16519v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Garber_T/0/1/0/all/0/1">Tomer Garber</a>, <a href="http://arxiv.org/find/eess/1/au:+Tirer_T/0/1/0/all/0/1">Tom Tirer</a></p>
<p>Training deep neural networks has become a common approach for addressing
image restoration problems. An alternative for training a "task-specific"
network for each observation model is to use pretrained deep denoisers for
imposing only the signal's prior within iterative algorithms, without
additional training. Recently, a sampling-based variant of this approach has
become popular with the rise of diffusion/score-based generative models. Using
denoisers for general purpose restoration requires guiding the iterations to
ensure agreement of the signal with the observations. In low-noise settings,
guidance that is based on back-projection (BP) has been shown to be a promising
strategy (used recently also under the names "pseudoinverse" or
"range/null-space" guidance). However, the presence of noise in the
observations hinders the gains from this approach. In this paper, we propose a
novel guidance technique, based on preconditioning that allows traversing from
BP-based guidance to least squares based guidance along the restoration scheme.
The proposed approach is robust to noise while still having much simpler
implementation than alternative methods (e.g., it does not require SVD or a
large number of iterations). We use it within both an optimization scheme and a
sampling-based scheme, and demonstrate its advantages over existing methods for
image deblurring and super-resolution.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16551">Blind Image Quality Assessment: A Brief Survey. (arXiv:2312.16551v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Miaohui Wang</a></p>
<p>Blind Image Quality Assessment (BIQA) is essential for automatically
evaluating the perceptual quality of visual signals without access to the
references. In this survey, we provide a comprehensive analysis and discussion
of recent developments in the field of BIQA. We have covered various aspects,
including hand-crafted BIQAs that focus on distortion-specific and
general-purpose methods, as well as deep-learned BIQAs that employ supervised
and unsupervised learning techniques. Additionally, we have explored multimodal
quality assessment methods that consider interactions between visual and audio
modalities, as well as visual and text modalities. Finally, we have offered
insights into representative BIQA databases, including both synthetic and
authentic distortions. We believe this survey provides valuable understandings
into the latest developments and emerging trends for the visual quality
community.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16571">GRSDet: Learning to Generate Local Reverse Samples for Few-shot Object Detection. (arXiv:2312.16571v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1">Hefei Mei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1">Taijin Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1">Shiyuan Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1">Heqian Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lanxiao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Minjian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1">Fanman Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hongliang Li</a></p>
<p>Few-shot object detection (FSOD) aims to achieve object detection only using
a few novel class training data. Most of the existing methods usually adopt a
transfer-learning strategy to construct the novel class distribution by
transferring the base class knowledge. However, this direct way easily results
in confusion between the novel class and other similar categories in the
decision space. To address the problem, we propose generating local reverse
samples (LRSamples) in Prototype Reference Frames to adaptively adjust the
center position and boundary range of the novel class distribution to learn
more discriminative novel class samples for FSOD. Firstly, we propose a Center
Calibration Variance Augmentation (CCVA) module, which contains the selection
rule of LRSamples, the generator of LRSamples, and augmentation on the
calibrated distribution centers. Specifically, we design an intra-class feature
converter (IFC) as the generator of CCVA to learn the selecting rule. By
transferring the knowledge of IFC from the base training to fine-tuning, the
IFC generates plentiful novel samples to calibrate the novel class
distribution. Moreover, we propose a Feature Density Boundary Optimization
(FDBO) module to adaptively adjust the importance of samples depending on their
distance from the decision boundary. It can emphasize the importance of the
high-density area of the similar class (closer decision boundary area) and
reduce the weight of the low-density area of the similar class (farther
decision boundary area), thus optimizing a clearer decision boundary for each
category. We conduct extensive experiments to demonstrate the effectiveness of
our proposed method. Our method achieves consistent improvement on the Pascal
VOC and MS COCO datasets based on DeFRCN and MFDC baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16578">Multi-modality Affinity Inference for Weakly Supervised 3D Semantic Segmentation. (arXiv:2312.16578v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiawei Li Qingyuan Xu Jing Zhang Tianyi Zhang Qian Yu Lu Sheng Dong Xu</a></p>
<p>3D point cloud semantic segmentation has a wide range of applications.
Recently, weakly supervised point cloud segmentation methods have been
proposed, aiming to alleviate the expensive and laborious manual annotation
process by leveraging scene-level labels. However, these methods have not
effectively exploited the rich geometric information (such as shape and scale)
and appearance information (such as color and texture) present in RGB-D scans.
Furthermore, current approaches fail to fully leverage the point affinity that
can be inferred from the feature extraction network, which is crucial for
learning from weak scene-level labels. Additionally, previous work overlooks
the detrimental effects of the long-tailed distribution of point cloud data in
weakly supervised 3D semantic segmentation. To this end, this paper proposes a
simple yet effective scene-level weakly supervised point cloud segmentation
method with a newly introduced multi-modality point affinity inference module.
The point affinity proposed in this paper is characterized by features from
multiple modalities (e.g., point cloud and RGB), and is further refined by
normalizing the classifier weights to alleviate the detrimental effects of
long-tailed distribution without the need of the prior of category
distribution. Extensive experiments on the ScanNet and S3DIS benchmarks verify
the effectiveness of our proposed method, which outperforms the
state-of-the-art by ~4% to ~6% mIoU. Codes are released at
https://github.com/Sunny599/AAAI24-3DWSSG-MMA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16580">VLCounter: Text-aware VIsual Representation for Zero-Shot Object Counting. (arXiv:2312.16580v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1">Seunggu Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Moon_W/0/1/0/all/0/1">WonJun Moon</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1">Euiyeon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Heo_J/0/1/0/all/0/1">Jae-Pil Heo</a></p>
<p>Zero-Shot Object Counting (ZSOC) aims to count referred instances of
arbitrary classes in a query image without human-annotated exemplars. To deal
with ZSOC, preceding studies proposed a two-stage pipeline: discovering
exemplars and counting. However, there remains a challenge of vulnerability to
error propagation of the sequentially designed two-stage process. In this work,
an one-stage baseline, Visual-Language Baseline (VLBase), exploring the
implicit association of the semantic-patch embeddings of CLIP is proposed.
Subsequently, the extension of VLBase to Visual-language Counter (VLCounter) is
achieved by incorporating three modules devised to tailor VLBase for object
counting. First, Semantic-conditioned Prompt Tuning (SPT) is introduced within
the image encoder to acquire target-highlighted representations. Second,
Learnable Affine Transformation (LAT) is employed to translate the
semantic-patch similarity map to be appropriate for the counting task. Lastly,
the layer-wisely encoded features are transferred to the decoder through
Segment-aware Skip Connection (SaSC) to keep the generalization capability for
unseen classes. Through extensive experiments on FSC147, CARPK, and PUCPR+, the
benefits of the end-to-end framework, VLCounter, are demonstrated.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16582">Learnable Chamfer Distance for Point Cloud Reconstruction. (arXiv:2312.16582v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1">Tianxin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qingyao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xiangrui Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yong Liu</a></p>
<p>As point clouds are 3D signals with permutation invariance, most existing
works train their reconstruction networks by measuring shape differences with
the average point-to-point distance between point clouds matched with
predefined rules. However, the static matching rules may deviate from actual
shape differences. Although some works propose dynamically-updated learnable
structures to replace matching rules, they need more iterations to converge
well. In this work, we propose a simple but effective reconstruction loss,
named Learnable Chamfer Distance (LCD) by dynamically paying attention to
matching distances with different weight distributions controlled with a group
of learnable networks. By training with adversarial strategy, LCD learns to
search defects in reconstructed results and overcomes the weaknesses of static
matching rules, while the performances at low iterations can also be guaranteed
by the basic matching algorithm. Experiments on multiple reconstruction
networks confirm that LCD can help achieve better reconstruction performances
and extract more representative representations with faster convergence and
comparable training efficiency. The source codes are provided in
https://github.com/Tianxinhuang/LCDNet.git.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16602">Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey. (arXiv:2312.16602v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jiaxing Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jingyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1">Kai Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1">Han Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1">Shijian Lu</a></p>
<p>Traditional computer vision generally solves each single task independently
by a dedicated model with the task instruction implicitly designed in the model
architecture, arising two limitations: (1) it leads to task-specific models,
which require multiple models for different tasks and restrict the potential
synergies from diverse tasks; (2) it leads to a pre-defined and fixed model
interface that has limited interactivity and adaptability in following user'
task instructions. To address them, Visual Instruction Tuning (VIT) has been
intensively studied recently, which finetunes a large vision model with
language as task instructions, aiming to learn from a wide range of vision
tasks described by language instructions a general-purpose multimodal model
that can follow arbitrary instructions and thus solve arbitrary tasks specified
by the user. This work aims to provide a systematic review of visual
instruction tuning, covering (1) the background that presents computer vision
task paradigms and the development of VIT; (2) the foundations of VIT that
introduce commonly used network architectures, visual instruction tuning
frameworks and objectives, and evaluation setups and tasks; (3) the commonly
used datasets in visual instruction tuning and evaluation; (4) the review of
existing VIT methods that categorizes them with a taxonomy according to both
the studied vision task and the method design and highlights the major
contributions, strengths, and shortcomings of them; (5) the comparison and
discussion of VIT methods over various instruction-following benchmarks; (6)
several challenges, open directions and possible future works in visual
instruction tuning research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16607">A Polarization and Radiomics Feature Fusion Network for the Classification of Hepatocellular Carcinoma and Intrahepatic Cholangiocarcinoma. (arXiv:2312.16607v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Dong_J/0/1/0/all/0/1">Jia Dong</a>, <a href="http://arxiv.org/find/eess/1/au:+Yao_Y/0/1/0/all/0/1">Yao Yao</a>, <a href="http://arxiv.org/find/eess/1/au:+Lin_L/0/1/0/all/0/1">Liyan Lin</a>, <a href="http://arxiv.org/find/eess/1/au:+Dong_Y/0/1/0/all/0/1">Yang Dong</a>, <a href="http://arxiv.org/find/eess/1/au:+Wan_J/0/1/0/all/0/1">Jiachen Wan</a>, <a href="http://arxiv.org/find/eess/1/au:+Peng_R/0/1/0/all/0/1">Ran Peng</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1">Chao Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Ma_H/0/1/0/all/0/1">Hui Ma</a></p>
<p>Classifying hepatocellular carcinoma (HCC) and intrahepatic
cholangiocarcinoma (ICC) is a critical step in treatment selection and
prognosis evaluation for patients with liver diseases. Traditional
histopathological diagnosis poses challenges in this context. In this study, we
introduce a novel polarization and radiomics feature fusion network, which
combines polarization features obtained from Mueller matrix images of liver
pathological samples with radiomics features derived from corresponding
pathological images to classify HCC and ICC. Our fusion network integrates a
two-tier fusion approach, comprising early feature-level fusion and late
classification-level fusion. By harnessing the strengths of polarization
imaging techniques and image feature-based machine learning, our proposed
fusion network significantly enhances classification accuracy. Notably, even at
reduced imaging resolutions, the fusion network maintains robust performance
due to the additional information provided by polarization features, which may
not align with human visual perception. Our experimental results underscore the
potential of this fusion network as a powerful tool for computer-aided
diagnosis of HCC and ICC, showcasing the benefits and prospects of integrating
polarization imaging techniques into the current image-intensive digital
pathological diagnosis. We aim to contribute this innovative approach to
top-tier journals, offering fresh insights and valuable tools in the fields of
medical imaging and cancer diagnosis. By introducing polarization imaging into
liver cancer classification, we demonstrate its interdisciplinary potential in
addressing challenges in medical image analysis, promising advancements in
medical imaging and cancer diagnosis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16610">Efficient Deweather Mixture-of-Experts with Uncertainty-aware Feature-wise Linear Modulation. (arXiv:2312.16610v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Rongyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1">Yulin Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiaming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Huanrui Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1">Zhen Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Gudovskiy_D/0/1/0/all/0/1">Denis Gudovskiy</a>, <a href="http://arxiv.org/find/cs/1/au:+Okuno_T/0/1/0/all/0/1">Tomoyuki Okuno</a>, <a href="http://arxiv.org/find/cs/1/au:+Nakata_Y/0/1/0/all/0/1">Yohei Nakata</a>, <a href="http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1">Kurt Keutzer</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1">Yuan Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shanghang Zhang</a></p>
<p>The Mixture-of-Experts (MoE) approach has demonstrated outstanding
scalability in multi-task learning including low-level upstream tasks such as
concurrent removal of multiple adverse weather effects. However, the
conventional MoE architecture with parallel Feed Forward Network (FFN) experts
leads to significant parameter and computational overheads that hinder its
efficient deployment. In addition, the naive MoE linear router is suboptimal in
assigning task-specific features to multiple experts which limits its further
scalability. In this work, we propose an efficient MoE architecture with weight
sharing across the experts. Inspired by the idea of linear feature modulation
(FM), our architecture implicitly instantiates multiple experts via learnable
activation modulations on a single shared expert block. The proposed Feature
Modulated Expert (FME) serves as a building block for the novel
Mixture-of-Feature-Modulation-Experts (MoFME) architecture, which can scale up
the number of experts with low overhead. We further propose an
Uncertainty-aware Router (UaR) to assign task-specific features to different FM
modules with well-calibrated weights. This enables MoFME to effectively learn
diverse expert functions for multiple tasks. The conducted experiments on the
multi-deweather task show that our MoFME outperforms the baselines in the image
restoration quality by 0.1-0.2 dB and achieves SOTA-compatible performance
while saving more than 72% of parameters and 39% inference time over the
conventional MoE counterpart. Experiments on the downstream segmentation and
classification tasks further demonstrate the generalizability of MoFME to real
open-world applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16611">Learning from small data sets: Patch-based regularizers in inverse problems for image reconstruction. (arXiv:2312.16611v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Piening_M/0/1/0/all/0/1">Moritz Piening</a>, <a href="http://arxiv.org/find/cs/1/au:+Altekruger_F/0/1/0/all/0/1">Fabian Altekr&#xfc;ger</a>, <a href="http://arxiv.org/find/cs/1/au:+Hertrich_J/0/1/0/all/0/1">Johannes Hertrich</a>, <a href="http://arxiv.org/find/cs/1/au:+Hagemann_P/0/1/0/all/0/1">Paul Hagemann</a>, <a href="http://arxiv.org/find/cs/1/au:+Walther_A/0/1/0/all/0/1">Andrea Walther</a>, <a href="http://arxiv.org/find/cs/1/au:+Steidl_G/0/1/0/all/0/1">Gabriele Steidl</a></p>
<p>The solution of inverse problems is of fundamental interest in medical and
astronomical imaging, geophysics as well as engineering and life sciences.
Recent advances were made by using methods from machine learning, in particular
deep neural networks. Most of these methods require a huge amount of (paired)
data and computer capacity to train the networks, which often may not be
available. Our paper addresses the issue of learning from small data sets by
taking patches of very few images into account. We focus on the combination of
model-based and data-driven methods by approximating just the image prior, also
known as regularizer in the variational model. We review two methodically
different approaches, namely optimizing the maximum log-likelihood of the patch
distribution, and penalizing Wasserstein-like discrepancies of whole empirical
patch distributions. From the point of view of Bayesian inverse problems, we
show how we can achieve uncertainty quantification by approximating the
posterior using Langevin Monte Carlo methods. We demonstrate the power of the
methods in computed tomography, image super-resolution, and inpainting. Indeed,
the approach provides also high-quality results in zero-shot super-resolution,
where only a low-resolution image is available. The paper is accompanied by a
GitHub repository containing implementations of all methods as well as data
examples so that the reader can get their own insight into the performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16626">Sorting of Smartphone Components for Recycling Through Convolutional Neural Networks. (arXiv:2312.16626v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Becker_A/0/1/0/all/0/1">&#xc1;lvaro G. Becker</a>, <a href="http://arxiv.org/find/cs/1/au:+Cenci_M/0/1/0/all/0/1">Marcelo P. Cenci</a>, <a href="http://arxiv.org/find/cs/1/au:+Silveira_T/0/1/0/all/0/1">Thiago L. T. da Silveira</a>, <a href="http://arxiv.org/find/cs/1/au:+Veit_H/0/1/0/all/0/1">Hugo M. Veit</a></p>
<p>The recycling of waste electrical and electronic equipment is an essential
tool in allowing for a circular economy, presenting the potential for
significant environmental and economic gain. However, traditional material
separation techniques, based on physical and chemical processes, require
substantial investment and do not apply to all cases. In this work, we
investigate using an image classification neural network as a potential means
to control an automated material separation process in treating smartphone
waste, acting as a more efficient, less costly, and more widely applicable
alternative to existing tools. We produced a dataset with 1,127 images of
pyrolyzed smartphone components, which was then used to train and assess a
VGG-16 image classification model. The model achieved 83.33% accuracy, lending
credence to the viability of using such a neural network in material
separation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16648">LIP-Loc: LiDAR Image Pretraining for Cross-Modal Localization. (arXiv:2312.16648v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Puligilla_S/0/1/0/all/0/1">Sai Shubodh Puligilla</a>, <a href="http://arxiv.org/find/cs/1/au:+Omama_M/0/1/0/all/0/1">Mohammad Omama</a>, <a href="http://arxiv.org/find/cs/1/au:+Zaidi_H/0/1/0/all/0/1">Husain Zaidi</a>, <a href="http://arxiv.org/find/cs/1/au:+Parihar_U/0/1/0/all/0/1">Udit Singh Parihar</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishna_M/0/1/0/all/0/1">Madhava Krishna</a></p>
<p>Global visual localization in LiDAR-maps, crucial for autonomous driving
applications, remains largely unexplored due to the challenging issue of
bridging the cross-modal heterogeneity gap. Popular multi-modal learning
approach Contrastive Language-Image Pre-Training (CLIP) has popularized
contrastive symmetric loss using batch construction technique by applying it to
multi-modal domains of text and image. We apply this approach to the domains of
2D image and 3D LiDAR points on the task of cross-modal localization. Our
method is explained as follows: A batch of N (image, LiDAR) pairs is
constructed so as to predict what is the right match between N X N possible
pairings across the batch by jointly training an image encoder and LiDAR
encoder to learn a multi-modal embedding space. In this way, the cosine
similarity between N positive pairings is maximized, whereas that between the
remaining negative pairings is minimized. Finally, over the obtained similarity
scores, a symmetric cross-entropy loss is optimized. To the best of our
knowledge, this is the first work to apply batched loss approach to a
cross-modal setting of image &amp; LiDAR data and also to show Zero-shot transfer
in a visual localization setting. We conduct extensive analyses on standard
autonomous driving datasets such as KITTI and KITTI-360 datasets. Our method
outperforms state-of-the-art recall@1 accuracy on the KITTI-360 dataset by
22.4%, using only perspective images, in contrast to the state-of-the-art
approach, which utilizes the more informative fisheye images. Additionally,
this superior performance is achieved without resorting to complex
architectures. Moreover, we demonstrate the zero-shot capabilities of our model
and we beat SOTA by 8% without even training on it. Furthermore, we establish
the first benchmark for cross-modal localization on the KITTI dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16649">Forgery-aware Adaptive Transformer for Generalizable Synthetic Image Detection. (arXiv:2312.16649v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Huan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1">Zichang Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1">Chuangchuang Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1">Yunchao Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yao Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jingdong Wang</a></p>
<p>In this paper, we study the problem of generalizable synthetic image
detection, aiming to detect forgery images from diverse generative methods,
e.g., GANs and diffusion models. Cutting-edge solutions start to explore the
benefits of pre-trained models, and mainly follow the fixed paradigm of solely
training an attached classifier, e.g., combining frozen CLIP-ViT with a
learnable linear layer in UniFD. However, our analysis shows that such a fixed
paradigm is prone to yield detectors with insufficient learning regarding
forgery representations. We attribute the key challenge to the lack of forgery
adaptation, and present a novel forgery-aware adaptive transformer approach,
namely FatFormer. Based on the pre-trained vision-language spaces of CLIP,
FatFormer introduces two core designs for the adaption to build generalized
forgery representations. First, motivated by the fact that both image and
frequency analysis are essential for synthetic image detection, we develop a
forgery-aware adapter to adapt image features to discern and integrate local
forgery traces within image and frequency domains. Second, we find that
considering the contrastive objectives between adapted image features and text
prompt embeddings, a previously overlooked aspect, results in a nontrivial
generalization improvement. Accordingly, we introduce language-guided alignment
to supervise the forgery adaptation with image and text prompts in FatFormer.
Experiments show that, by coupling these two designs, our approach tuned on
4-class ProGAN data attains a remarkable detection performance, achieving an
average of 98% accuracy to unseen GANs, and surprisingly generalizes to unseen
diffusion models with 95% accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16693">I2V-Adapter: A General Image-to-Video Adapter for Video Diffusion Models. (arXiv:2312.16693v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1">Xun Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1">Mingwu Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1">Liang Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yuan Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1">Yufan Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1">Chongyang Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1">Weiming Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1">Zhengjun Zha</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Haibin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_P/0/1/0/all/0/1">Pengfei Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Di Zhang</a></p>
<p>In the rapidly evolving domain of digital content generation, the focus has
shifted from text-to-image (T2I) models to more advanced video diffusion
models, notably text-to-video (T2V) and image-to-video (I2V). This paper
addresses the intricate challenge posed by I2V: converting static images into
dynamic, lifelike video sequences while preserving the original image fidelity.
Traditional methods typically involve integrating entire images into diffusion
processes or using pretrained encoders for cross attention. However, these
approaches often necessitate altering the fundamental weights of T2I models,
thereby restricting their reusability. We introduce a novel solution, namely
I2V-Adapter, designed to overcome such limitations. Our approach preserves the
structural integrity of T2I models and their inherent motion modules. The
I2V-Adapter operates by processing noised video frames in parallel with the
input image, utilizing a lightweight adapter module. This module acts as a
bridge, efficiently linking the input to the model's self-attention mechanism,
thus maintaining spatial details without requiring structural changes to the
T2I model. Moreover, I2V-Adapter requires only a fraction of the parameters of
conventional models and ensures compatibility with existing community-driven
T2I models and controlling tools. Our experimental results demonstrate
I2V-Adapter's capability to produce high-quality video outputs. This
performance, coupled with its versatility and reduced need for trainable
parameters, represents a substantial advancement in the field of AI-driven
video generation, particularly for creative applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16717">Landslide Detection and Segmentation Using Remote Sensing Images and Deep Neural Network. (arXiv:2312.16717v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Le_C/0/1/0/all/0/1">Cam Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Pham_L/0/1/0/all/0/1">Lam Pham</a>, <a href="http://arxiv.org/find/cs/1/au:+Lampert_J/0/1/0/all/0/1">Jasmin Lampert</a>, <a href="http://arxiv.org/find/cs/1/au:+Schlogl_M/0/1/0/all/0/1">Matthias Schl&#xf6;gl</a>, <a href="http://arxiv.org/find/cs/1/au:+Schindler_A/0/1/0/all/0/1">Alexander Schindler</a></p>
<p>Knowledge about historic landslide event occurrence is important for
supporting disaster risk reduction strategies. Building upon findings from 2022
Landslide4Sense Competition, we propose a deep neural network based system for
landslide detection and segmentation from multisource remote sensing image
input. We use a U-Net trained with Cross Entropy loss as baseline model. We
then improve the U-Net baseline model by leveraging a wide range of deep
learning techniques. In particular, we conduct feature engineering by
generating new band data from the original bands, which helps to enhance the
quality of remote sensing image input. Regarding the network architecture, we
replace traditional convolutional layers in the U-Net baseline by a
residual-convolutional layer. We also propose an attention layer which
leverages the multi-head attention scheme. Additionally, we generate multiple
output masks with three different resolutions, which creates an ensemble of
three outputs in the inference process to enhance the performance. Finally, we
propose a combined loss function which leverages Focal loss and IoU loss to
train the network. Our experiments on the development set of the
Landslide4Sense challenge achieve an F1 score and an mIoU score of 84.07 and
76.07, respectively. Our best model setup outperforms the challenge baseline
and the proposed U-Net baseline, improving the F1 score/mIoU score by 6.8/7.4
and 10.5/8.8, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16720">Prompt Expansion for Adaptive Text-to-Image Generation. (arXiv:2312.16720v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Datta_S/0/1/0/all/0/1">Siddhartha Datta</a>, <a href="http://arxiv.org/find/cs/1/au:+Ku_A/0/1/0/all/0/1">Alexander Ku</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramachandran_D/0/1/0/all/0/1">Deepak Ramachandran</a>, <a href="http://arxiv.org/find/cs/1/au:+Anderson_P/0/1/0/all/0/1">Peter Anderson</a></p>
<p>Text-to-image generation models are powerful but difficult to use. Users
craft specific prompts to get better images, though the images can be
repetitive. This paper proposes a Prompt Expansion framework that helps users
generate high-quality, diverse images with less effort. The Prompt Expansion
model takes a text query as input and outputs a set of expanded text prompts
that are optimized such that when passed to a text-to-image model, generates a
wider variety of appealing images. We conduct a human evaluation study that
shows that images generated through Prompt Expansion are more aesthetically
pleasing and diverse than those generated by baseline methods. Overall, this
paper presents a novel and effective approach to improving the text-to-image
generation experience.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16724">A pipeline for multiple orange detection and tracking with 3-D fruit relocalization and neural-net based yield regression in commercial citrus orchards. (arXiv:2312.16724v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Santos_T/0/1/0/all/0/1">Thiago T. Santos</a>, <a href="http://arxiv.org/find/cs/1/au:+Souza_K/0/1/0/all/0/1">Kleber X. S. de Souza</a>, <a href="http://arxiv.org/find/cs/1/au:+Neto_J/0/1/0/all/0/1">Jo&#xe3;o Camargo Neto</a>, <a href="http://arxiv.org/find/cs/1/au:+Koenigkan_L/0/1/0/all/0/1">Luciano V. Koenigkan</a>, <a href="http://arxiv.org/find/cs/1/au:+Moreira_A/0/1/0/all/0/1">Al&#xe9;cio S. Moreira</a>, <a href="http://arxiv.org/find/cs/1/au:+Ternes_S/0/1/0/all/0/1">S&#xf4;nia Ternes</a></p>
<p>Traditionally, sweet orange crop forecasting has involved manually counting
fruits from numerous trees, which is a labor-intensive process. Automatic
systems for fruit counting, based on proximal imaging, computer vision, and
machine learning, have been considered a promising alternative or complement to
manual counting. These systems require data association components that prevent
multiple counting of the same fruit observed in different images. However,
there is a lack of work evaluating the accuracy of multiple fruit counting,
especially considering (i) occluded and re-entering green fruits on leafy
trees, and (ii) counting ground-truth data measured in the crop field. We
propose a non-invasive alternative that utilizes fruit counting from videos,
implemented as a pipeline. Firstly, we employ CNNs for the detection of visible
fruits. Inter-frame association techniques are then applied to track the fruits
across frames. To handle occluded and re-appeared fruit, we introduce a
relocalization component that employs 3-D estimation of fruit locations.
Finally, a neural network regressor is utilized to estimate the total number of
fruit, integrating image-based fruit counting with other tree data such as crop
variety and tree size. The results demonstrate that the performance of our
approach is closely tied to the quality of the field-collected videos. By
ensuring that at least 30% of the fruit is accurately detected, tracked, and
counted, our yield regressor achieves an impressive coefficient of
determination of 0.85. To the best of our knowledge, this study represents one
of the few endeavors in fruit estimation that incorporates manual fruit
counting as a reference point for evaluation. We also introduce annotated
datasets for multiple orange tracking (MOrangeT) and detection (OranDet),
publicly available to foster the development of novel methods for image-based
fruit counting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16731">Disentangled Continual Learning: Separating Memory Edits from Model Updates. (arXiv:2312.16731v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dziadzio_S/0/1/0/all/0/1">Sebastian Dziadzio</a>, <a href="http://arxiv.org/find/cs/1/au:+Yildiz_C/0/1/0/all/0/1">&#xc7;a&#x11f;atay Y&#x131;ld&#x131;z</a>, <a href="http://arxiv.org/find/cs/1/au:+Ven_G/0/1/0/all/0/1">Gido M. van de Ven</a>, <a href="http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1">Tomasz Trzci&#x144;ski</a>, <a href="http://arxiv.org/find/cs/1/au:+Tuytelaars_T/0/1/0/all/0/1">Tinne Tuytelaars</a>, <a href="http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1">Matthias Bethge</a></p>
<p>The ability of machine learning systems to learn continually is hindered by
catastrophic forgetting, the tendency of neural networks to overwrite existing
knowledge when learning a new task. Existing continual learning methods
alleviate this problem through regularisation, parameter isolation, or
rehearsal, and are typically evaluated on benchmarks consisting of a handful of
tasks. We propose a novel conceptual approach to continual classification that
aims to disentangle class-specific information that needs to be memorised from
the class-agnostic knowledge that encapsulates generalization. We store the
former in a buffer that can be easily pruned or updated when new categories
arrive, while the latter is represented with a neural network that generalizes
across tasks. We show that the class-agnostic network does not suffer from
catastrophic forgetting and by leveraging it to perform classification, we
improve accuracy on past tasks over time. In addition, our approach supports
open-set classification and one-shot generalization. To test our conceptual
framework, we introduce Infinite dSprites, a tool for creating continual
classification and disentanglement benchmarks of arbitrary length with full
control over generative factors. We show that over a sufficiently long time
horizon all major types of continual learning methods break down, while our
approach enables continual learning over hundreds of tasks with explicit
control over memorization and forgetting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16737">HMP: Hand Motion Priors for Pose and Shape Estimation from Video. (arXiv:2312.16737v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Duran_E/0/1/0/all/0/1">Enes Duran</a>, <a href="http://arxiv.org/find/cs/1/au:+Kocabas_M/0/1/0/all/0/1">Muhammed Kocabas</a>, <a href="http://arxiv.org/find/cs/1/au:+Choutas_V/0/1/0/all/0/1">Vasileios Choutas</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1">Zicong Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1">Michael J. Black</a></p>
<p>Understanding how humans interact with the world necessitates accurate 3D
hand pose estimation, a task complicated by the hand's high degree of
articulation, frequent occlusions, self-occlusions, and rapid motions. While
most existing methods rely on single-image inputs, videos have useful cues to
address aforementioned issues. However, existing video-based 3D hand datasets
are insufficient for training feedforward models to generalize to in-the-wild
scenarios. On the other hand, we have access to large human motion capture
datasets which also include hand motions, e.g. AMASS. Therefore, we develop a
generative motion prior specific for hands, trained on the AMASS dataset which
features diverse and high-quality hand motions. This motion prior is then
employed for video-based 3D hand motion estimation following a latent
optimization approach. Our integration of a robust motion prior significantly
enhances performance, especially in occluded scenarios. It produces stable,
temporally consistent results that surpass conventional single-frame methods.
We demonstrate our method's efficacy via qualitative and quantitative
evaluations on the HO3D and DexYCB datasets, with special emphasis on an
occlusion-focused subset of HO3D. Code is available at
https://hmp.is.tue.mpg.de
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16771">Scale-Aware Crowd Count Network with Annotation Error Correction. (arXiv:2312.16771v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hsieh_Y/0/1/0/all/0/1">Yi-Kuan Hsieh</a>, <a href="http://arxiv.org/find/cs/1/au:+Hsieh_J/0/1/0/all/0/1">Jun-Wei Hsieh</a>, <a href="http://arxiv.org/find/cs/1/au:+Tseng_Y/0/1/0/all/0/1">Yu-Chee Tseng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1">Ming-Ching Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xin_L/0/1/0/all/0/1">Li Xin</a></p>
<p>Traditional crowd counting networks suffer from information loss when feature
maps are downsized through pooling layers, leading to inaccuracies in counting
crowds at a distance. Existing methods often assume correct annotations during
training, disregarding the impact of noisy annotations, especially in crowded
scenes. Furthermore, the use of a fixed Gaussian kernel fails to account for
the varying pixel distribution with respect to the camera distance. To overcome
these challenges, we propose a Scale-Aware Crowd Counting Network (SACC-Net)
that introduces a ``scale-aware'' architecture with error-correcting
capabilities of noisy annotations. For the first time, we {\bf simultaneously}
model labeling errors (mean) and scale variations (variance) by
spatially-varying Gaussian distributions to produce fine-grained heat maps for
crowd counting. Furthermore, the proposed adaptive Gaussian kernel variance
enables the model to learn dynamically with a low-rank approximation, leading
to improved convergence efficiency with comparable accuracy. The performance of
SACC-Net is extensively evaluated on four public datasets: UCF-QNRF, UCF CC 50,
NWPU, and ShanghaiTech A-B. Experimental results demonstrate that SACC-Net
outperforms all state-of-the-art methods, validating its effectiveness in
achieving superior crowd counting accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16772">Unsupversied feature correlation model to predict breast abnormal variation maps in longitudinal mammograms. (arXiv:2312.16772v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Bai_J/0/1/0/all/0/1">Jun Bai</a>, <a href="http://arxiv.org/find/eess/1/au:+Jin_A/0/1/0/all/0/1">Annie Jin</a>, <a href="http://arxiv.org/find/eess/1/au:+Adams_M/0/1/0/all/0/1">Madison Adams</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_C/0/1/0/all/0/1">Clifford Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Nabavi_S/0/1/0/all/0/1">Sheida Nabavi</a></p>
<p>Breast cancer continues to be a significant cause of mortality among women
globally. Timely identification and precise diagnosis of breast abnormalities
are critical for enhancing patient prognosis. In this study, we focus on
improving the early detection and accurate diagnosis of breast abnormalities,
which is crucial for improving patient outcomes and reducing the mortality rate
of breast cancer. To address the limitations of traditional screening methods,
a novel unsupervised feature correlation network was developed to predict maps
indicating breast abnormal variations using longitudinal 2D mammograms. The
proposed model utilizes the reconstruction process of current year and prior
year mammograms to extract tissue from different areas and analyze the
differences between them to identify abnormal variations that may indicate the
presence of cancer. The model is equipped with a feature correlation module, an
attention suppression gate, and a breast abnormality detection module that work
together to improve the accuracy of the prediction. The proposed model not only
provides breast abnormal variation maps, but also distinguishes between normal
and cancer mammograms, making it more advanced compared to the state-of the-art
baseline models. The results of the study show that the proposed model
outperforms the baseline models in terms of Accuracy, Sensitivity, Specificity,
Dice score, and cancer detection rate.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2003.04117">The Utility of Feature Reuse: Transfer Learning in Data-Starved Regimes. (arXiv:2003.04117v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shadman_R/0/1/0/all/0/1">Rashik Shadman</a>, <a href="http://arxiv.org/find/cs/1/au:+Murshed_M/0/1/0/all/0/1">M.G. Sarwar Murshed</a>, <a href="http://arxiv.org/find/cs/1/au:+Verenich_E/0/1/0/all/0/1">Edward Verenich</a>, <a href="http://arxiv.org/find/cs/1/au:+Velasquez_A/0/1/0/all/0/1">Alvaro Velasquez</a>, <a href="http://arxiv.org/find/cs/1/au:+Hussain_F/0/1/0/all/0/1">Faraz Hussain</a></p>
<p>The use of transfer learning with deep neural networks has increasingly
become widespread for deploying well-tested computer vision systems to newer
domains, especially those with limited datasets. We describe a transfer
learning use case for a domain with a data-starved regime, having fewer than
100 labeled target samples. We evaluate the effectiveness of convolutional
feature extraction and fine-tuning of overparameterized models with respect to
the size of target training data, as well as their generalization performance
on data with covariate shift, or out-of-distribution (OOD) data. Our
experiments demonstrate that both overparameterization and feature reuse
contribute to the successful application of transfer learning in training image
classifiers in data-starved regimes. We provide visual explanations to support
our findings and conclude that transfer learning enhances the performance of
CNN architectures in data-starved regimes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2201.07366">TriCoLo: Trimodal Contrastive Loss for Text to Shape Retrieval. (arXiv:2201.07366v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ruan_Y/0/1/0/all/0/1">Yue Ruan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Han-Hung Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yiming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Ke Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_A/0/1/0/all/0/1">Angel X. Chang</a></p>
<p>Text-to-shape retrieval is an increasingly relevant problem with the growth
of 3D shape data. Recent work on contrastive losses for learning joint
embeddings over multimodal data has been successful at tasks such as retrieval
and classification. Thus far, work on joint representation learning for 3D
shapes and text has focused on improving embeddings through modeling of complex
attention between representations, or multi-task learning. We propose a
trimodal learning scheme over text, multi-view images and 3D shape voxels, and
show that with large batch contrastive learning we achieve good performance on
text-to-shape retrieval without complex attention mechanisms or losses. Our
experiments serve as a foundation for follow-up work on building trimodal
embeddings for text-image-shape.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2204.13809">Automated player identification and indexing using two-stage deep learning network. (arXiv:2204.13809v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hongshan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Aderon_C/0/1/0/all/0/1">Colin Aderon</a>, <a href="http://arxiv.org/find/cs/1/au:+Wagon_N/0/1/0/all/0/1">Noah Wagon</a>, <a href="http://arxiv.org/find/cs/1/au:+Bamba_A/0/1/0/all/0/1">Abdul Latif Bamba</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xueshen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Huapu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+MacCall_S/0/1/0/all/0/1">Steven MacCall</a>, <a href="http://arxiv.org/find/cs/1/au:+Gan_Y/0/1/0/all/0/1">Yu Gan</a></p>
<p>American football games attract significant worldwide attention every year.
Identifying players from videos in each play is also essential for the indexing
of player participation. Processing football game video presents great
challenges such as crowded settings, distorted objects, and imbalanced data for
identifying players, especially jersey numbers. In this work, we propose a deep
learning-based player tracking system to automatically track players and index
their participation per play in American football games. It is a two-stage
network design to highlight areas of interest and identify jersey number
information with high accuracy. First, we utilize an object detection network,
a detection transformer, to tackle the player detection problem in a crowded
context. Second, we identify players using jersey number recognition with a
secondary convolutional neural network, then synchronize it with a game clock
subsystem. Finally, the system outputs a complete log in a database for play
indexing. We demonstrate the effectiveness and reliability of player tracking
system by analyzing the qualitative and quantitative results on football
videos. The proposed system shows great potential for implementation in and
analysis of football broadcast video.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.15677">Augmentation-Aware Self-Supervision for Data-Efficient GAN Training. (arXiv:2205.15677v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1">Liang Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1">Qi Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1">Yige Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1">Songtao Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1">Chongyang Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1">Siyuan Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_P/0/1/0/all/0/1">Pengfei Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhongyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1">Huawei Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1">Xueqi Cheng</a></p>
<p>Training generative adversarial networks (GANs) with limited data is
challenging because the discriminator is prone to overfitting. Previously
proposed differentiable augmentation demonstrates improved data efficiency of
training GANs. However, the augmentation implicitly introduces undesired
invariance to augmentation for the discriminator since it ignores the change of
semantics in the label space caused by data transformation, which may limit the
representation learning ability of the discriminator and ultimately affect the
generative modeling performance of the generator. To mitigate the negative
impact of invariance while inheriting the benefits of data augmentation, we
propose a novel augmentation-aware self-supervised discriminator that predicts
the augmentation parameter of the augmented data. Particularly, the prediction
targets of real data and generated data are required to be distinguished since
they are different during training. We further encourage the generator to
adversarially learn from the self-supervised discriminator by generating
augmentation-predictable real and not fake data. This formulation connects the
learning objective of the generator and the arithmetic $-$ harmonic mean
divergence under certain assumptions. We compare our method with
state-of-the-art (SOTA) methods using the class-conditional BigGAN and
unconditional StyleGAN2 architectures on data-limited CIFAR-10, CIFAR-100,
FFHQ, LSUN-Cat, and five low-shot datasets. Experimental results demonstrate
significant improvements of our method over SOTA methods in training
data-efficient GANs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.05148">Weakly-supervised segmentation using inherently-explainable classification models and their application to brain tumour classification. (arXiv:2206.05148v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Chatterjee_S/0/1/0/all/0/1">Soumick Chatterjee</a>, <a href="http://arxiv.org/find/eess/1/au:+Yassin_H/0/1/0/all/0/1">Hadya Yassin</a>, <a href="http://arxiv.org/find/eess/1/au:+Dubost_F/0/1/0/all/0/1">Florian Dubost</a>, <a href="http://arxiv.org/find/eess/1/au:+Nurnberger_A/0/1/0/all/0/1">Andreas N&#xfc;rnberger</a>, <a href="http://arxiv.org/find/eess/1/au:+Speck_O/0/1/0/all/0/1">Oliver Speck</a></p>
<p>Deep learning models have shown their potential for several applications.
However, most of the models are opaque and difficult to trust due to their
complex reasoning - commonly known as the black-box problem. Some fields, such
as medicine, require a high degree of transparency to accept and adopt such
technologies. Consequently, creating explainable/interpretable models or
applying post-hoc methods on classifiers to build trust in deep learning models
are required. Moreover, deep learning methods can be used for segmentation
tasks, which typically require hard-to-obtain, time-consuming
manually-annotated segmentation labels for training. This paper introduces
three inherently-explainable classifiers to tackle both of these problems as
one. The localisation heatmaps provided by the networks -- representing the
models' focus areas and being used in classification decision-making -- can be
directly interpreted, without requiring any post-hoc methods to derive
information for model explanation. The models are trained by using the input
image and only the classification labels as ground-truth in a supervised
fashion - without using any information about the location of the region of
interest (i.e. the segmentation labels), making the segmentation training of
the models weakly-supervised through classification labels. The final
segmentation is obtained by thresholding these heatmaps. The models were
employed for the task of multi-class brain tumour classification using two
different datasets, resulting in the best F1-score of 0.93 for the supervised
classification task while securing a median Dice score of 0.67$\pm$0.08 for the
weakly-supervised segmentation task. Furthermore, the obtained accuracy on a
subset of tumour-only images outperformed the state-of-the-art glioma tumour
grading binary classifiers with the best model achieving 98.7\% accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.08263">Scalable SoftGroup for 3D Instance Segmentation on Point Clouds. (arXiv:2209.08263v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1">Thang Vu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1">Kookhoi Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Luu_T/0/1/0/all/0/1">Tung M. Luu</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Thanh Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Junyeong Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoo_C/0/1/0/all/0/1">Chang D. Yoo</a></p>
<p>This paper considers a network referred to as SoftGroup for accurate and
scalable 3D instance segmentation. Existing state-of-the-art methods produce
hard semantic predictions followed by grouping instance segmentation results.
Unfortunately, errors stemming from hard decisions propagate into the grouping,
resulting in poor overlap between predicted instances and ground truth and
substantial false positives. To address the abovementioned problems, SoftGroup
allows each point to be associated with multiple classes to mitigate the
uncertainty stemming from semantic prediction. It also suppresses false
positive instances by learning to categorize them as background. Regarding
scalability, the existing fast methods require computational time on the order
of tens of seconds on large-scale scenes, which is unsatisfactory and far from
applicable for real-time. Our finding is that the $k$-Nearest Neighbor ($k$-NN)
module, which serves as the prerequisite of grouping, introduces a
computational bottleneck. SoftGroup is extended to resolve this computational
bottleneck, referred to as SoftGroup++. The proposed SoftGroup++ reduces time
complexity with octree $k$-NN and reduces search space with class-aware pyramid
scaling and late devoxelization. Experimental results on various indoor and
outdoor datasets demonstrate the efficacy and generality of the proposed
SoftGroup and SoftGroup++. Their performances surpass the best-performing
baseline by a large margin (6\% $\sim$ 16\%) in terms of AP$_{50}$. On datasets
with large-scale scenes, SoftGroup++ achieves a 6$\times$ speed boost on
average compared to SoftGroup. Furthermore, SoftGroup can be extended to
perform object detection and panoptic segmentation with nontrivial improvements
over existing methods. The source code and trained models are available at
\url{https://github.com/thangvubk/SoftGroup}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.08303">Improving Radiology Summarization with Radiograph and Anatomy Prompts. (arXiv:2210.08303v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1">Jinpeng Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhihong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1">Xiang Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1">Tsung-Hui Chang</a></p>
<p>The impression is crucial for the referring physicians to grasp key
information since it is concluded from the findings and reasoning of
radiologists. To alleviate the workload of radiologists and reduce repetitive
human labor in impression writing, many researchers have focused on automatic
impression generation. However, recent works on this task mainly summarize the
corresponding findings and pay less attention to the radiology images. In
clinical, radiographs can provide more detailed valuable observations to
enhance radiologists' impression writing, especially for complicated cases.
Besides, each sentence in findings usually focuses on single anatomy, so they
only need to be matched to corresponding anatomical regions instead of the
whole image, which is beneficial for textual and visual features alignment.
Therefore, we propose a novel anatomy-enhanced multimodal model to promote
impression generation. In detail, we first construct a set of rules to extract
anatomies and put these prompts into each sentence to highlight anatomy
characteristics. Then, two separate encoders are applied to extract features
from the radiograph and findings. Afterward, we utilize a contrastive learning
module to align these two representations at the overall level and use a
co-attention to fuse them at the sentence level with the help of
anatomy-enhanced sentence representation. Finally, the decoder takes the fused
information as the input to generate impressions. The experimental results on
two benchmark datasets confirm the effectiveness of the proposed method, which
achieves state-of-the-art results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.00812">One-shot domain adaptation in video-based assessment of surgical skills. (arXiv:2301.00812v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yanik_E/0/1/0/all/0/1">Erim Yanik</a>, <a href="http://arxiv.org/find/cs/1/au:+Schwaitzberg_S/0/1/0/all/0/1">Steven Schwaitzberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1">Gene Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Intes_X/0/1/0/all/0/1">Xavier Intes</a>, <a href="http://arxiv.org/find/cs/1/au:+De_S/0/1/0/all/0/1">Suvranu De</a></p>
<p>Deep Learning (DL) has achieved automatic and objective assessment of
surgical skills. However, the applicability of DL models is often hampered by
their substantial data requirements and confinement to specific training
domains. This prevents them from transitioning to new tasks with scarce data.
Therefore, domain adaptation emerges as a critical element for the practical
implementation of DL in real-world scenarios. Herein, we introduce A-VBANet, a
novel meta-learning model capable of delivering domain-agnostic surgical skill
classification via one-shot learning. A-VBANet has been rigorously developed
and tested on five diverse laparoscopic and robotic surgical simulators.
Furthermore, we extend its validation to operating room (OR) videos of
laparoscopic cholecystectomy. Our model successfully adapts with accuracies up
to 99.5% in one-shot and 99.9% in few-shot settings for simulated tasks and
89.7% for laparoscopic cholecystectomy. This research marks the first instance
of a domain-agnostic methodology for surgical skill assessment, paving the way
for more precise and accessible training evaluation across diverse high-stakes
environments such as real-life surgery where data is scarce.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.00209">Towards Large Certified Radius in Randomized Smoothing using Quasiconcave Optimization. (arXiv:2302.00209v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kung_B/0/1/0/all/0/1">Bo-Han Kung</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shang-Tse Chen</a></p>
<p>Randomized smoothing is currently the state-of-the-art method that provides
certified robustness for deep neural networks. However, due to its excessively
conservative nature, this method of incomplete verification often cannot
achieve an adequate certified radius on real-world datasets. One way to obtain
a larger certified radius is to use an input-specific algorithm instead of
using a fixed Gaussian filter for all data points. Several methods based on
this idea have been proposed, but they either suffer from high computational
costs or gain marginal improvement in certified radius. In this work, we show
that by exploiting the quasiconvex problem structure, we can find the optimal
certified radii for most data points with slight computational overhead. This
observation leads to an efficient and effective input-specific randomized
smoothing algorithm. We conduct extensive experiments and empirical analysis on
CIFAR-10 and ImageNet. The results show that the proposed method significantly
enhances the certified radii with low computational overhead.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.03791">How to Trust Your Diffusion Model: A Convex Optimization Approach to Conformal Risk Control. (arXiv:2302.03791v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Teneggi_J/0/1/0/all/0/1">Jacopo Teneggi</a>, <a href="http://arxiv.org/find/stat/1/au:+Tivnan_M/0/1/0/all/0/1">Matthew Tivnan</a>, <a href="http://arxiv.org/find/stat/1/au:+Stayman_J/0/1/0/all/0/1">J. Webster Stayman</a>, <a href="http://arxiv.org/find/stat/1/au:+Sulam_J/0/1/0/all/0/1">Jeremias Sulam</a></p>
<p>Score-based generative modeling, informally referred to as diffusion models,
continue to grow in popularity across several important domains and tasks.
While they provide high-quality and diverse samples from empirical
distributions, important questions remain on the reliability and
trustworthiness of these sampling procedures for their responsible use in
critical scenarios. Conformal prediction is a modern tool to construct
finite-sample, distribution-free uncertainty guarantees for any black-box
predictor. In this work, we focus on image-to-image regression tasks and we
present a generalization of the Risk-Controlling Prediction Sets (RCPS)
procedure, that we term $K$-RCPS, which allows to $(i)$ provide entrywise
calibrated intervals for future samples of any diffusion model, and $(ii)$
control a certain notion of risk with respect to a ground truth image with
minimal mean interval length. Differently from existing conformal risk control
procedures, ours relies on a novel convex optimization approach that allows for
multidimensional risk control while provably minimizing the mean interval
length. We illustrate our approach on two real-world image denoising problems:
on natural images of faces as well as on computed tomography (CT) scans of the
abdomen, demonstrating state of the art performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.06504">Preconditioned Score-based Generative Models. (arXiv:2302.06504v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1">Hengyuan Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Li Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiatian Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1">Jianfeng Feng</a></p>
<p>Score-based generative models (SGMs) have recently emerged as a promising
class of generative models. However, a fundamental limitation is that their
sampling process is slow due to a need for many (\eg, $2000$) iterations of
sequential computations. An intuitive acceleration method is to reduce the
sampling iterations which however causes severe performance degradation. We
assault this problem to the ill-conditioned issues of the Langevin dynamics and
reverse diffusion in the sampling process. Under this insight, we propose a
model-agnostic {\bf\em preconditioned diffusion sampling} (PDS) method that
leverages matrix preconditioning to alleviate the aforementioned problem. PDS
alters the sampling process of a vanilla SGM at marginal extra computation
cost, and without model retraining. Theoretically, we prove that PDS preserves
the output distribution of the SGM, no risk of inducing systematical bias to
the original sampling process. We further theoretically reveal a relation
between the parameter of PDS and the sampling iterations,easing the parameter
estimation under varying sampling iterations. Extensive experiments on various
image datasets with a variety of resolutions and diversity validate that our
PDS consistently accelerates off-the-shelf SGMs whilst maintaining the
synthesis quality. In particular, PDS can accelerate by up to $29\times$ on
more challenging high resolution (1024$\times$1024) image generation. Compared
with the latest generative models (\eg, CLD-SGM, DDIM, and Analytic-DDIM), PDS
can achieve the best sampling quality on CIFAR-10 at a FID score of 1.99. Our
code is made publicly available to foster any further research
https://github.com/fudan-zvg/PDS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.13390">MDF-Net for abnormality detection by fusing X-rays with clinical data. (arXiv:2302.13390v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Hsieh_C/0/1/0/all/0/1">Chihcheng Hsieh</a>, <a href="http://arxiv.org/find/eess/1/au:+Nobre_I/0/1/0/all/0/1">Isabel Blanco Nobre</a>, <a href="http://arxiv.org/find/eess/1/au:+Sousa_S/0/1/0/all/0/1">Sandra Costa Sousa</a>, <a href="http://arxiv.org/find/eess/1/au:+Ouyang_C/0/1/0/all/0/1">Chun Ouyang</a>, <a href="http://arxiv.org/find/eess/1/au:+Brereton_M/0/1/0/all/0/1">Margot Brereton</a>, <a href="http://arxiv.org/find/eess/1/au:+Nascimento_J/0/1/0/all/0/1">Jacinto C. Nascimento</a>, <a href="http://arxiv.org/find/eess/1/au:+Jorge_J/0/1/0/all/0/1">Joaquim Jorge</a>, <a href="http://arxiv.org/find/eess/1/au:+Moreira_C/0/1/0/all/0/1">Catarina Moreira</a></p>
<p>This study investigates the effects of including patients' clinical
information on the performance of deep learning (DL) classifiers for disease
location in chest X-ray images. Although current classifiers achieve high
performance using chest X-ray images alone, our interviews with radiologists
indicate that clinical data is highly informative and essential for
interpreting images and making proper diagnoses.
</p>
<p>In this work, we propose a novel architecture consisting of two fusion
methods that enable the model to simultaneously process patients' clinical data
(structured data) and chest X-rays (image data). Since these data modalities
are in different dimensional spaces, we propose a spatial arrangement strategy,
spatialization, to facilitate the multimodal learning process in a Mask R-CNN
model. We performed an extensive experimental evaluation using MIMIC-Eye, a
dataset comprising modalities: MIMIC-CXR (chest X-ray images), MIMIC IV-ED
(patients' clinical data), and REFLACX (annotations of disease locations in
chest X-rays).
</p>
<p>Results show that incorporating patients' clinical data in a DL model
together with the proposed fusion methods improves the disease localization in
chest X-rays by 12\% in terms of Average Precision compared to a standard Mask
R-CNN using only chest X-rays. Further ablation studies also emphasize the
importance of multimodal DL architectures and the incorporation of patients'
clinical data in disease localization. The architecture proposed in this work
is publicly available to promote the scientific reproducibility of our study
(https://github.com/ChihchengHsieh/multimodal-abnormalities-detection)
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.02392">Audio-Visual Quality Assessment for User Generated Content: Database and Method. (arXiv:2303.02392v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Cao_Y/0/1/0/all/0/1">Yuqin Cao</a>, <a href="http://arxiv.org/find/eess/1/au:+Min_X/0/1/0/all/0/1">Xiongkuo Min</a>, <a href="http://arxiv.org/find/eess/1/au:+Sun_W/0/1/0/all/0/1">Wei Sun</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1">Xiaoping Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhai_G/0/1/0/all/0/1">Guangtao Zhai</a></p>
<p>With the explosive increase of User Generated Content (UGC), UGC video
quality assessment (VQA) becomes more and more important for improving users'
Quality of Experience (QoE). However, most existing UGC VQA studies only focus
on the visual distortions of videos, ignoring that the user's QoE also depends
on the accompanying audio signals. In this paper, we conduct the first study to
address the problem of UGC audio and video quality assessment (AVQA).
Specifically, we construct the first UGC AVQA database named the SJTU-UAV
database, which includes 520 in-the-wild UGC audio and video (A/V) sequences,
and conduct a user study to obtain the mean opinion scores of the A/V
sequences. The content of the SJTU-UAV database is then analyzed from both the
audio and video aspects to show the database characteristics. We also design a
family of AVQA models, which fuse the popular VQA methods and audio features
via support vector regressor (SVR). We validate the effectiveness of the
proposed models on the three databases. The experimental results show that with
the help of audio signals, the VQA models can evaluate the perceptual quality
more accurately. The database will be released to facilitate further research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.02862">EvHandPose: Event-based 3D Hand Pose Estimation with Sparse Supervision. (arXiv:2303.02862v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1">Jianping Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiahe Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Baowen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1">Xiaoming Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1">Boxin Shi</a></p>
<p>Event camera shows great potential in 3D hand pose estimation, especially
addressing the challenges of fast motion and high dynamic range in a low-power
way. However, due to the asynchronous differential imaging mechanism, it is
challenging to design event representation to encode hand motion information
especially when the hands are not moving (causing motion ambiguity), and it is
infeasible to fully annotate the temporally dense event stream. In this paper,
we propose EvHandPose with novel hand flow representations in Event-to-Pose
module for accurate hand pose estimation and alleviating the motion ambiguity
issue. To solve the problem under sparse annotation, we design contrast
maximization and hand-edge constraints in Pose-to-IWE (Image with Warped
Events) module and formulate EvHandPose in a weakly-supervision framework. We
further build EvRealHands, the first large-scale real-world event-based hand
pose dataset on several challenging scenes to bridge the real-synthetic domain
gap. Experiments on EvRealHands demonstrate that EvHandPose outperforms
previous event-based methods under all evaluation scenes, achieves accurate and
stable hand pose estimation with high temporal resolution in fast motion and
strong light scenes compared with RGB-based methods, generalizes well to
outdoor scenes and another type of event camera, and shows the potential for
the hand gesture recognition task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.04027">NeBLa: Neural Beer-Lambert for 3D Reconstruction of Oral Structures from Panoramic Radiographs. (arXiv:2304.04027v5 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Park_S/0/1/0/all/0/1">Sihwa Park</a>, <a href="http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1">Seongjun Kim</a>, <a href="http://arxiv.org/find/eess/1/au:+Kwon_D/0/1/0/all/0/1">Doeyoung Kwon</a>, <a href="http://arxiv.org/find/eess/1/au:+Jang_Y/0/1/0/all/0/1">Yohan Jang</a>, <a href="http://arxiv.org/find/eess/1/au:+Song_I/0/1/0/all/0/1">In-Seok Song</a>, <a href="http://arxiv.org/find/eess/1/au:+Baek_S/0/1/0/all/0/1">Seungjun Baek</a></p>
<p>Panoramic radiography (Panoramic X-ray, PX) is a widely used imaging modality
for dental examination. However, PX only provides a flattened 2D image, lacking
in a 3D view of the oral structure. In this paper, we propose NeBLa (Neural
Beer-Lambert) to estimate 3D oral structures from real-world PX. NeBLa tackles
full 3D reconstruction for varying subjects (patients) where each
reconstruction is based only on a single panoramic image. We create an
intermediate representation called simulated PX (SimPX) from 3D Cone-beam
computed tomography (CBCT) data based on the Beer-Lambert law of X-ray
rendering and rotational principles of PX imaging. SimPX aims at not only
truthfully simulating PX, but also facilitates the reverting process back to 3D
data. We propose a novel neural model based on ray tracing which exploits both
global and local input features to convert SimPX to 3D output. At inference, a
real PX image is translated to a SimPX-style image with semantic
regularization, and the translated image is processed by generation module to
produce high-quality outputs. Experiments show that NeBLa outperforms prior
state-of-the-art in reconstruction tasks both quantitatively and qualitatively.
Unlike prior methods, NeBLa does not require any prior information such as the
shape of dental arches, nor the matched PX-CBCT dataset for training, which is
difficult to obtain in clinical practice. Our code is available at
https://github.com/sihwa-park/nebla.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.05977">ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation. (arXiv:2304.05977v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jiazheng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yuchen Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1">Yuxuan Tong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qinkai Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1">Ming Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jie Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1">Yuxiao Dong</a></p>
<p>We present a comprehensive solution to learn and improve text-to-image models
from human preference feedback. To begin with, we build ImageReward -- the
first general-purpose text-to-image human preference reward model -- to
effectively encode human preferences. Its training is based on our systematic
annotation pipeline including rating and ranking, which collects 137k expert
comparisons to date. In human evaluation, ImageReward outperforms existing
scoring models and metrics, making it a promising automatic metric for
evaluating text-to-image synthesis. On top of it, we propose Reward Feedback
Learning (ReFL), a direct tuning algorithm to optimize diffusion models against
a scorer. Both automatic and human evaluation support ReFL's advantages over
compared methods. All code and datasets are provided at
\url{https://github.com/THUDM/ImageReward}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.08818">Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models. (arXiv:2304.08818v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Blattmann_A/0/1/0/all/0/1">Andreas Blattmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Rombach_R/0/1/0/all/0/1">Robin Rombach</a>, <a href="http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1">Huan Ling</a>, <a href="http://arxiv.org/find/cs/1/au:+Dockhorn_T/0/1/0/all/0/1">Tim Dockhorn</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Seung Wook Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1">Sanja Fidler</a>, <a href="http://arxiv.org/find/cs/1/au:+Kreis_K/0/1/0/all/0/1">Karsten Kreis</a></p>
<p>Latent Diffusion Models (LDMs) enable high-quality image synthesis while
avoiding excessive compute demands by training a diffusion model in a
compressed lower-dimensional latent space. Here, we apply the LDM paradigm to
high-resolution video generation, a particularly resource-intensive task. We
first pre-train an LDM on images only; then, we turn the image generator into a
video generator by introducing a temporal dimension to the latent space
diffusion model and fine-tuning on encoded image sequences, i.e., videos.
Similarly, we temporally align diffusion model upsamplers, turning them into
temporally consistent video super resolution models. We focus on two relevant
real-world applications: Simulation of in-the-wild driving data and creative
content creation with text-to-video modeling. In particular, we validate our
Video LDM on real driving videos of resolution 512 x 1024, achieving
state-of-the-art performance. Furthermore, our approach can easily leverage
off-the-shelf pre-trained image LDMs, as we only need to train a temporal
alignment model in that case. Doing so, we turn the publicly available,
state-of-the-art text-to-image LDM Stable Diffusion into an efficient and
expressive text-to-video model with resolution up to 1280 x 2048. We show that
the temporal layers trained in this way generalize to different fine-tuned
text-to-image LDMs. Utilizing this property, we show the first results for
personalized text-to-video generation, opening exciting directions for future
content creation. Project page:
https://research.nvidia.com/labs/toronto-ai/VideoLDM/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.14530">Generating images of rare concepts using pre-trained diffusion models. (arXiv:2304.14530v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Samuel_D/0/1/0/all/0/1">Dvir Samuel</a>, <a href="http://arxiv.org/find/cs/1/au:+Ben_Ari_R/0/1/0/all/0/1">Rami Ben-Ari</a>, <a href="http://arxiv.org/find/cs/1/au:+Raviv_S/0/1/0/all/0/1">Simon Raviv</a>, <a href="http://arxiv.org/find/cs/1/au:+Darshan_N/0/1/0/all/0/1">Nir Darshan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1">Gal Chechik</a></p>
<p>Text-to-image diffusion models can synthesize high-quality images, but they
have various limitations. Here we highlight a common failure mode of these
models, namely, generating uncommon concepts and structured concepts like hand
palms. We show that their limitation is partly due to the long-tail nature of
their training data: web-crawled data sets are strongly unbalanced, causing
models to under-represent concepts from the tail of the distribution. We
characterize the effect of unbalanced training data on text-to-image models and
offer a remedy. We show that rare concepts can be correctly generated by
carefully selecting suitable generation seeds in the noise space, using a small
reference set of images, a technique that we call SeedSelect. SeedSelect does
not require retraining or finetuning the diffusion model. We assess the
faithfulness, quality and diversity of SeedSelect in creating rare objects and
generating complex formations like hand images, and find it consistently
achieves superior performance. We further show the advantage of SeedSelect in
semantic data augmentation. Generating semantically appropriate images can
successfully improve performance in few-shot recognition benchmarks, for
classes from the head and from the tail of the training data of diffusion
models
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15581">Unsupervised Semantic Correspondence Using Stable Diffusion. (arXiv:2305.15581v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hedlin_E/0/1/0/all/0/1">Eric Hedlin</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_G/0/1/0/all/0/1">Gopal Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahajan_S/0/1/0/all/0/1">Shweta Mahajan</a>, <a href="http://arxiv.org/find/cs/1/au:+Isack_H/0/1/0/all/0/1">Hossam Isack</a>, <a href="http://arxiv.org/find/cs/1/au:+Kar_A/0/1/0/all/0/1">Abhishek Kar</a>, <a href="http://arxiv.org/find/cs/1/au:+Tagliasacchi_A/0/1/0/all/0/1">Andrea Tagliasacchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1">Kwang Moo Yi</a></p>
<p>Text-to-image diffusion models are now capable of generating images that are
often indistinguishable from real images. To generate such images, these models
must understand the semantics of the objects they are asked to generate. In
this work we show that, without any training, one can leverage this semantic
knowledge within diffusion models to find semantic correspondences - locations
in multiple images that have the same semantic meaning. Specifically, given an
image, we optimize the prompt embeddings of these models for maximum attention
on the regions of interest. These optimized embeddings capture semantic
information about the location, which can then be transferred to another image.
By doing so we obtain results on par with the strongly supervised state of the
art on the PF-Willow dataset and significantly outperform (20.9% relative for
the SPair-71k dataset) any existing weakly or unsupervised method on PF-Willow,
CUB-200 and SPair-71k datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.17214">Contrast, Attend and Diffuse to Decode High-Resolution Images from Brain Activities. (arXiv:2305.17214v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jingyuan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Mingxiao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zijiao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yunhao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shaonan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1">Marie-Francine Moens</a></p>
<p>Decoding visual stimuli from neural responses recorded by functional Magnetic
Resonance Imaging (fMRI) presents an intriguing intersection between cognitive
neuroscience and machine learning, promising advancements in understanding
human visual perception and building non-invasive brain-machine interfaces.
However, the task is challenging due to the noisy nature of fMRI signals and
the intricate pattern of brain visual representations. To mitigate these
challenges, we introduce a two-phase fMRI representation learning framework.
The first phase pre-trains an fMRI feature learner with a proposed
Double-contrastive Mask Auto-encoder to learn denoised representations. The
second phase tunes the feature learner to attend to neural activation patterns
most informative for visual reconstruction with guidance from an image
auto-encoder. The optimized fMRI feature learner then conditions a latent
diffusion model to reconstruct image stimuli from brain activities.
Experimental results demonstrate our model's superiority in generating
high-resolution and semantically accurate images, substantially exceeding
previous state-of-the-art methods by 39.34% in the 50-way-top-1 semantic
classification accuracy. Our research invites further exploration of the
decoding task's potential and contributes to the development of non-invasive
brain-machine interfaces.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.04047">CAVEN: An Embodied Conversational Agent for Efficient Audio-Visual Navigation in Noisy Environments. (arXiv:2306.04047v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiulong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1">Sudipta Paul</a>, <a href="http://arxiv.org/find/cs/1/au:+Chatterjee_M/0/1/0/all/0/1">Moitreya Chatterjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Cherian_A/0/1/0/all/0/1">Anoop Cherian</a></p>
<p>Audio-visual navigation of an agent towards locating an audio goal is a
challenging task especially when the audio is sporadic or the environment is
noisy. In this paper, we present CAVEN, a Conversation-based Audio-Visual
Embodied Navigation framework in which the agent may interact with a
human/oracle for solving the task of navigating to an audio goal. Specifically,
CAVEN is modeled as a budget-aware partially observable semi-Markov decision
process that implicitly learns the uncertainty in the audio-based navigation
policy to decide when and how the agent may interact with the oracle. Our CAVEN
agent can engage in fully-bidirectional natural language conversations by
producing relevant questions and interpret free-form, potentially noisy
responses from the oracle based on the audio-visual context. To enable such a
capability, CAVEN is equipped with: (i) a trajectory forecasting network that
is grounded in audio-visual cues to produce a potential trajectory to the
estimated goal, and (ii) a natural language based question generation and
reasoning network to pose an interactive question to the oracle or interpret
the oracle's response to produce navigation instructions. To train the
interactive modules, we present a large scale dataset: AVN-Instruct, based on
the Landmark-RxR dataset. To substantiate the usefulness of conversations, we
present experiments on the benchmark audio-goal task using the SoundSpaces
simulator under various noisy settings. Our results reveal that our
fully-conversational approach leads to nearly an order-of-magnitude improvement
in success rate, especially in localizing new sound sources and against methods
that only use uni-directional interaction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.04607">GeoDiffusion: Text-Prompted Geometric Control for Object Detection Data Generation. (arXiv:2306.04607v6 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Kai Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1">Enze Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhe Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yibo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1">Lanqing Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhenguo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yeung_D/0/1/0/all/0/1">Dit-Yan Yeung</a></p>
<p>Diffusion models have attracted significant attention due to the remarkable
ability to create content and generate data for tasks like image
classification. However, the usage of diffusion models to generate the
high-quality object detection data remains an underexplored area, where not
only image-level perceptual quality but also geometric conditions such as
bounding boxes and camera views are essential. Previous studies have utilized
either copy-paste synthesis or layout-to-image (L2I) generation with
specifically designed modules to encode the semantic layouts. In this paper, we
propose the GeoDiffusion, a simple framework that can flexibly translate
various geometric conditions into text prompts and empower pre-trained
text-to-image (T2I) diffusion models for high-quality detection data
generation. Unlike previous L2I methods, our GeoDiffusion is able to encode not
only the bounding boxes but also extra geometric conditions such as camera
views in self-driving scenes. Extensive experiments demonstrate GeoDiffusion
outperforms previous L2I methods while maintaining 4x training time faster. To
the best of our knowledge, this is the first work to adopt diffusion models for
layout-to-image generation with geometric conditions and demonstrate that
L2I-generated images can be beneficial for improving the performance of object
detectors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.04990">Multi-Architecture Multi-Expert Diffusion Models. (arXiv:2306.04990v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1">Yunsung Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jin-Young Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Go_H/0/1/0/all/0/1">Hyojun Go</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeong_M/0/1/0/all/0/1">Myeongho Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1">Shinhyeok Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1">Seungtaek Choi</a></p>
<p>In this paper, we address the performance degradation of efficient diffusion
models by introducing Multi-architecturE Multi-Expert diffusion models (MEME).
We identify the need for tailored operations at different time-steps in
diffusion processes and leverage this insight to create compact yet
high-performing models. MEME assigns distinct architectures to different
time-step intervals, balancing convolution and self-attention operations based
on observed frequency characteristics. We also introduce a soft interval
assignment strategy for comprehensive training. Empirically, MEME operates 3.3
times faster than baselines while improving image generation quality (FID
scores) by 0.62 (FFHQ) and 0.37 (CelebA). Though we validate the effectiveness
of assigning more optimal architecture per time-step, where efficient models
outperform the larger models, we argue that MEME opens a new design choice for
diffusion models that can be easily applied in other scenarios, such as large
multi-expert models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.05246">A Task-driven Network for Mesh Classification and Semantic Part Segmentation. (arXiv:2306.05246v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1">Qiujie Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1">Xiaoran Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">Rui Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zixiong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shuangmin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xin_S/0/1/0/all/0/1">Shiqing Xin</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_C/0/1/0/all/0/1">Changhe Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenping Wang</a></p>
<p>With the rapid development of geometric deep learning techniques, many
mesh-based convolutional operators have been proposed to bridge irregular mesh
structures and popular backbone networks. In this paper, we show that while
convolutions are helpful, a simple architecture based exclusively on
multi-layer perceptrons (MLPs) is competent enough to deal with mesh
classification and semantic segmentation. Our new network architecture, named
Mesh-MLP, takes mesh vertices equipped with the heat kernel signature (HKS) and
dihedral angles as the input, replaces the convolution module of a ResNet with
Multi-layer Perceptron (MLP), and utilizes layer normalization (LN) to perform
the normalization of the layers. The all-MLP architecture operates in an
end-to-end fashion and does not include a pooling module. Extensive
experimental results on the mesh classification/segmentation tasks validate the
effectiveness of the all-MLP architecture.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.08659">Explore In-Context Learning for 3D Point Cloud Understanding. (arXiv:2306.08659v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1">Zhongbin Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiangtai Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xia Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Buhmann_J/0/1/0/all/0/1">Joachim M. Buhmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1">Chen Change Loy</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Mengyuan Liu</a></p>
<p>With the rise of large-scale models trained on broad data, in-context
learning has become a new learning paradigm that has demonstrated significant
potential in natural language processing and computer vision tasks. Meanwhile,
in-context learning is still largely unexplored in the 3D point cloud domain.
Although masked modeling has been successfully applied for in-context learning
in 2D vision, directly extending it to 3D point clouds remains a formidable
challenge. In the case of point clouds, the tokens themselves are the point
cloud positions (coordinates) that are masked during inference. Moreover,
position embedding in previous works may inadvertently introduce information
leakage. To address these challenges, we introduce a novel framework, named
Point-In-Context, designed especially for in-context learning in 3D point
clouds, where both inputs and outputs are modeled as coordinates for each task.
Additionally, we propose the Joint Sampling module, carefully designed to work
in tandem with the general point sampling operator, effectively resolving the
aforementioned technical issues. We conduct extensive experiments to validate
the versatility and adaptability of our proposed methods in handling a wide
range of tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.08832">Contrasting Intra-Modal and Ranking Cross-Modal Hard Negatives to Enhance Visio-Linguistic Compositional Understanding. (arXiv:2306.08832v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Le Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Awal_R/0/1/0/all/0/1">Rabiul Awal</a>, <a href="http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1">Aishwarya Agrawal</a></p>
<p>Vision-Language Models (VLMs), such as CLIP, exhibit strong image-text
comprehension abilities, facilitating advances in several downstream tasks such
as zero-shot image classification, image-text retrieval, and text-to-image
generation. However, the compositional reasoning abilities of existing VLMs
remains subpar. The root of this limitation lies in the inadequate alignment
between the images and captions in the pretraining datasets. Additionally, the
current contrastive learning objective fails to focus on fine-grained grounding
components like relations, actions, and attributes, resulting in "bag-of-words"
representations. We introduce a simple and effective method to improve
compositional reasoning in VLMs. Our method better leverages available datasets
by refining and expanding the standard image-text contrastive learning
framework. Our approach does not require specific annotations and does not
incur extra parameters. When integrated with CLIP, our technique yields notable
improvement over state-of-the-art baselines across five vision-language
compositional benchmarks. We open-source our code at
https://github.com/lezhang7/Enhance-FineGrained.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.11203">AVOIDDS: Aircraft Vision-based Intruder Detection Dataset and Simulator. (arXiv:2306.11203v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Smyers_E/0/1/0/all/0/1">Elysia Q. Smyers</a>, <a href="http://arxiv.org/find/cs/1/au:+Katz_S/0/1/0/all/0/1">Sydney M. Katz</a>, <a href="http://arxiv.org/find/cs/1/au:+Corso_A/0/1/0/all/0/1">Anthony L. Corso</a>, <a href="http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1">Mykel J. Kochenderfer</a></p>
<p>Designing robust machine learning systems remains an open problem, and there
is a need for benchmark problems that cover both environmental changes and
evaluation on a downstream task. In this work, we introduce AVOIDDS, a
realistic object detection benchmark for the vision-based aircraft
detect-and-avoid problem. We provide a labeled dataset consisting of 72,000
photorealistic images of intruder aircraft with various lighting conditions,
weather conditions, relative geometries, and geographic locations. We also
provide an interface that evaluates trained models on slices of this dataset to
identify changes in performance with respect to changing environmental
conditions. Finally, we implement a fully-integrated, closed-loop simulator of
the vision-based detect-and-avoid problem to evaluate trained models with
respect to the downstream collision avoidance task. This benchmark will enable
further research in the design of robust machine learning systems for use in
safety-critical applications. The AVOIDDS dataset and code are publicly
available at https://purl.stanford.edu/hj293cv5980 and
https://github.com/sisl/VisionBasedAircraftDAA respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.17778">Look, Remember and Reason: Grounded reasoning in videos with language models. (arXiv:2306.17778v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bhattacharyya_A/0/1/0/all/0/1">Apratim Bhattacharyya</a>, <a href="http://arxiv.org/find/cs/1/au:+Panchal_S/0/1/0/all/0/1">Sunny Panchal</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1">Mingu Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Pourreza_R/0/1/0/all/0/1">Reza Pourreza</a>, <a href="http://arxiv.org/find/cs/1/au:+Madan_P/0/1/0/all/0/1">Pulkit Madan</a>, <a href="http://arxiv.org/find/cs/1/au:+Memisevic_R/0/1/0/all/0/1">Roland Memisevic</a></p>
<p>Multi-modal language models (LM) have recently shown promising performance in
high-level reasoning tasks on videos. However, existing methods still fall
short in tasks like causal or compositional spatiotemporal reasoning over
actions, in which model predictions need to be grounded in fine-grained
low-level details, such as object motions and object interactions. In this
work, we propose training an LM end-to-end on low-level surrogate tasks,
including object detection, re-identification, and tracking, to endow the model
with the required low-level visual capabilities. We show that a two-stream
video encoder with spatiotemporal attention is effective at capturing the
required static and motion-based cues in the video. By leveraging the LM's
ability to perform the low-level surrogate tasks, we can cast reasoning in
videos as the three-step process of Look, Remember, Reason wherein visual
information is extracted using low-level visual skills step-by-step and then
integrated to arrive at a final answer. We demonstrate the effectiveness of our
framework on diverse visual reasoning tasks from the ACRE, CATER, and
Something-Else datasets. Our approach is trainable end-to-end and surpasses
state-of-the-art task-specific methods across these tasks by a large margin.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04087">SVIT: Scaling up Visual Instruction Tuning. (arXiv:2307.04087v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1">Bo Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1">Boya Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1">Muyang He</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1">Tiejun Huang</a></p>
<p>Thanks to the emerging of foundation models, the large language and vision
models are integrated to acquire the multimodal ability of visual captioning,
question answering, etc. Although existing multimodal models present impressive
performance of visual understanding and reasoning, their limits are still
largely under-explored due to the scarcity of high-quality instruction tuning
data. To push the limits of multimodal capability, we Scale up Visual
Instruction Tuning (SVIT) by constructing a dataset of 4.2 million visual
instruction tuning data including 1.6M conversation question-answer (QA) pairs,
1.6M complex reasoning QA pairs, 1.0M referring QA pairs and 106K detailed
image descriptions. Besides the volume, the proposed dataset is also featured
by the high quality and rich diversity, which is generated by prompting GPT-4
with the abundant manual annotations of images. We also propose a new data
recipe to select subset with better diversity and balance, which evokes model's
superior capabilities. Extensive experiments verify that SVIT-v1.5, trained on
the proposed dataset, outperforms state-of-the-art Multimodal Large Language
Models on popular benchmarks. The data and code are publicly available at
https://github.com/BAAI-DCAI/Visual-Instruction-Tuning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09259">Adaptive Topological Feature via Persistent Homology: Filtration Learning for Point Clouds. (arXiv:2307.09259v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nishikawa_N/0/1/0/all/0/1">Naoki Nishikawa</a>, <a href="http://arxiv.org/find/cs/1/au:+Ike_Y/0/1/0/all/0/1">Yuichi Ike</a>, <a href="http://arxiv.org/find/cs/1/au:+Yamanishi_K/0/1/0/all/0/1">Kenji Yamanishi</a></p>
<p>Machine learning for point clouds has been attracting much attention, with
many applications in various fields, such as shape recognition and material
science. For enhancing the accuracy of such machine learning methods, it is
often effective to incorporate global topological features, which are typically
extracted by persistent homology. In the calculation of persistent homology for
a point cloud, we choose a filtration for the point cloud, an increasing
sequence of spaces. Since the performance of machine learning methods combined
with persistent homology is highly affected by the choice of a filtration, we
need to tune it depending on data and tasks. In this paper, we propose a
framework that learns a filtration adaptively with the use of neural networks.
In order to make the resulting persistent homology isometry-invariant, we
develop a neural network architecture with such invariance. Additionally, we
show a theoretical result on a finite-dimensional approximation of filtration
functions, which justifies the proposed network architecture. Experimental
results demonstrated the efficacy of our framework in several classification
tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10422">PreDiff: Precipitation Nowcasting with Latent Diffusion Models. (arXiv:2307.10422v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1">Zhihan Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1">Xingjian Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1">Boran Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1">Xiaoyong Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Maddix_D/0/1/0/all/0/1">Danielle Maddix</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yi Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Mu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuyang Wang</a></p>
<p>Earth system forecasting has traditionally relied on complex physical models
that are computationally expensive and require significant domain expertise. In
the past decade, the unprecedented increase in spatiotemporal Earth observation
data has enabled data-driven forecasting models using deep learning techniques.
These models have shown promise for diverse Earth system forecasting tasks but
either struggle with handling uncertainty or neglect domain-specific prior
knowledge, resulting in averaging possible futures to blurred forecasts or
generating physically implausible predictions. To address these limitations, we
propose a two-stage pipeline for probabilistic spatiotemporal forecasting: 1)
We develop PreDiff, a conditional latent diffusion model capable of
probabilistic forecasts. 2) We incorporate an explicit knowledge alignment
mechanism to align forecasts with domain-specific physical constraints. This is
achieved by estimating the deviation from imposed constraints at each denoising
step and adjusting the transition distribution accordingly. We conduct
empirical studies on two datasets: N-body MNIST, a synthetic dataset with
chaotic behavior, and SEVIR, a real-world precipitation nowcasting dataset.
Specifically, we impose the law of conservation of energy in N-body MNIST and
anticipated precipitation intensity in SEVIR. Experiments demonstrate the
effectiveness of PreDiff in handling uncertainty, incorporating domain-specific
prior knowledge, and generating forecasts that exhibit high operational
utility.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.10253">StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data. (arXiv:2308.10253v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yanda Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1">Gang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhibin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_B/0/1/0/all/0/1">Bin Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1">Guosheng Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1">Chunhua Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Ling Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1">Yunchao Wei</a></p>
<p>The remarkable multimodal capabilities demonstrated by OpenAI's GPT-4 have
sparked significant interest in the development of multimodal Large Language
Models (LLMs). A primary research objective of such models is to align visual
and textual modalities effectively while comprehending human instructions.
Current methodologies often rely on annotations derived from benchmark datasets
to construct image-dialogue datasets for training purposes, akin to instruction
tuning in LLMs. However, these datasets often exhibit domain bias, potentially
constraining the generative capabilities of the models. In an effort to
mitigate these limitations, we propose a novel data collection methodology that
synchronously synthesizes images and dialogues for visual instruction tuning.
This approach harnesses the power of generative models, marrying the abilities
of ChatGPT and text-to-image generative models to yield a diverse and
controllable dataset with varied image content. Additionally, datasets can be
arbitrarily scaled. This not only provides greater flexibility compared to
existing methodologies but also significantly enhances several model
capabilities. Our research includes comprehensive experiments conducted on
various datasets. The results emphasize substantial enhancements in more than
ten commonly assessed capabilities. Additionally, our model achieves
state-of-the-art results across multiple widely recognized multimodal
benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.15366">AnomalyGPT: Detecting Industrial Anomalies Using Large Vision-Language Models. (arXiv:2308.15366v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gu_Z/0/1/0/all/0/1">Zhaopeng Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1">Bingke Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1">Guibo Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yingying Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1">Ming Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jinqiao Wang</a></p>
<p>Large Vision-Language Models (LVLMs) such as MiniGPT-4 and LLaVA have
demonstrated the capability of understanding images and achieved remarkable
performance in various visual tasks. Despite their strong abilities in
recognizing common objects due to extensive training datasets, they lack
specific domain knowledge and have a weaker understanding of localized details
within objects, which hinders their effectiveness in the Industrial Anomaly
Detection (IAD) task. On the other hand, most existing IAD methods only provide
anomaly scores and necessitate the manual setting of thresholds to distinguish
between normal and abnormal samples, which restricts their practical
implementation. In this paper, we explore the utilization of LVLM to address
the IAD problem and propose AnomalyGPT, a novel IAD approach based on LVLM. We
generate training data by simulating anomalous images and producing
corresponding textual descriptions for each image. We also employ an image
decoder to provide fine-grained semantic and design a prompt learner to
fine-tune the LVLM using prompt embeddings. Our AnomalyGPT eliminates the need
for manual threshold adjustments, thus directly assesses the presence and
locations of anomalies. Additionally, AnomalyGPT supports multi-turn dialogues
and exhibits impressive few-shot in-context learning capabilities. With only
one normal shot, AnomalyGPT achieves the state-of-the-art performance with an
accuracy of 86.1%, an image-level AUC of 94.1%, and a pixel-level AUC of 95.3%
on the MVTec-AD dataset. Code is available at
https://github.com/CASIA-IVA-Lab/AnomalyGPT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.15478">An Adaptive Tangent Feature Perspective of Neural Networks. (arXiv:2308.15478v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+LeJeune_D/0/1/0/all/0/1">Daniel LeJeune</a>, <a href="http://arxiv.org/find/cs/1/au:+Alemohammad_S/0/1/0/all/0/1">Sina Alemohammad</a></p>
<p>In order to better understand feature learning in neural networks, we propose
a framework for understanding linear models in tangent feature space where the
features are allowed to be transformed during training. We consider linear
transformations of features, resulting in a joint optimization over parameters
and transformations with a bilinear interpolation constraint. We show that this
optimization problem has an equivalent linearly constrained optimization with
structured regularization that encourages approximately low rank solutions.
Specializing to neural network structure, we gain insights into how the
features and thus the kernel function change, providing additional nuance to
the phenomenon of kernel alignment when the target function is poorly
represented using tangent features. In addition to verifying our theoretical
observations in real neural networks on a simple regression problem, we
empirically show that an adaptive feature implementation of tangent feature
classification has an order of magnitude lower sample complexity than the fixed
tangent feature model on MNIST and CIFAR-10.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.04672">SSHNN: Semi-Supervised Hybrid NAS Network for Echocardiographic Image Segmentation. (arXiv:2309.04672v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Chen_R/0/1/0/all/0/1">Renqi Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Luo_J/0/1/0/all/0/1">Jingjing Luo</a>, <a href="http://arxiv.org/find/eess/1/au:+Nian_F/0/1/0/all/0/1">Fan Nian</a>, <a href="http://arxiv.org/find/eess/1/au:+Cen_Y/0/1/0/all/0/1">Yuhui Cen</a>, <a href="http://arxiv.org/find/eess/1/au:+Peng_Y/0/1/0/all/0/1">Yiheng Peng</a>, <a href="http://arxiv.org/find/eess/1/au:+Yu_Z/0/1/0/all/0/1">Zekuan Yu</a></p>
<p>Accurate medical image segmentation especially for echocardiographic images
with unmissable noise requires elaborate network design. Compared with manual
design, Neural Architecture Search (NAS) realizes better segmentation results
due to larger search space and automatic optimization, but most of the existing
methods are weak in layer-wise feature aggregation and adopt a ``strong
encoder, weak decoder" structure, insufficient to handle global relationships
and local details. To resolve these issues, we propose a novel semi-supervised
hybrid NAS network for accurate medical image segmentation termed SSHNN. In
SSHNN, we creatively use convolution operation in layer-wise feature fusion
instead of normalized scalars to avoid losing details, making NAS a stronger
encoder. Moreover, Transformers are introduced for the compensation of global
context and U-shaped decoder is designed to efficiently connect global context
with local features. Specifically, we implement a semi-supervised algorithm
Mean-Teacher to overcome the limited volume problem of labeled medical image
dataset. Extensive experiments on CAMUS echocardiography dataset demonstrate
that SSHNN outperforms state-of-the-art approaches and realizes accurate
segmentation. Code will be made publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11820">Automatic Endoscopic Ultrasound Station Recognition with Limited Data. (arXiv:2309.11820v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ramesh_A/0/1/0/all/0/1">Abhijit Ramesh</a>, <a href="http://arxiv.org/find/eess/1/au:+Nandanan_A/0/1/0/all/0/1">Anantha Nandanan</a>, <a href="http://arxiv.org/find/eess/1/au:+Boggavarapu_N/0/1/0/all/0/1">Nikhil Boggavarapu</a>, <a href="http://arxiv.org/find/eess/1/au:+MD_P/0/1/0/all/0/1">Priya Nair MD</a>, <a href="http://arxiv.org/find/eess/1/au:+Gressel_G/0/1/0/all/0/1">Gilad Gressel</a></p>
<p>Pancreatic cancer is a lethal form of cancer that significantly contributes
to cancer-related deaths worldwide. Early detection is essential to improve
patient prognosis and survival rates. Despite advances in medical imaging
techniques, pancreatic cancer remains a challenging disease to detect.
Endoscopic ultrasound (EUS) is the most effective diagnostic tool for detecting
pancreatic cancer. However, it requires expert interpretation of complex
ultrasound images to complete a reliable patient scan. To obtain complete
imaging of the pancreas, practitioners must learn to guide the endoscope into
multiple "EUS stations" (anatomical locations), which provide different views
of the pancreas. This is a difficult skill to learn, involving over 225
proctored procedures with the support of an experienced doctor. We build an
AI-assisted tool that utilizes deep learning techniques to identify these
stations of the stomach in real time during EUS procedures. This
computer-assisted diagnostic (CAD) will help train doctors more efficiently.
Historically, the challenge faced in developing such a tool has been the amount
of retrospective labeling required by trained clinicians. To solve this, we
developed an open-source user-friendly labeling web app that streamlines the
process of annotating stations during the EUS procedure with minimal effort
from the clinicians. Our research shows that employing only 43 procedures with
no hyperparameter fine-tuning obtained a balanced accuracy of 89%, comparable
to the current state of the art. In addition, we employ Grad-CAM, a
visualization technology that provides clinicians with interpretable and
explainable visualizations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.15556">Learning Dense Flow Field for Highly-accurate Cross-view Camera Localization. (arXiv:2309.15556v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1">Zhenbo Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Ze_X/0/1/0/all/0/1">Xianghui Ze</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Jianfeng Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yujiao Shi</a></p>
<p>This paper addresses the problem of estimating the 3-DoF camera pose for a
ground-level image with respect to a satellite image that encompasses the local
surroundings. We propose a novel end-to-end approach that leverages the
learning of dense pixel-wise flow fields in pairs of ground and satellite
images to calculate the camera pose. Our approach differs from existing methods
by constructing the feature metric at the pixel level, enabling full-image
supervision for learning distinctive geometric configurations and visual
appearances across views. Specifically, our method employs two distinct
convolution networks for ground and satellite feature extraction. Then, we
project the ground feature map to the bird's eye view (BEV) using a fixed
camera height assumption to achieve preliminary geometric alignment. To further
establish content association between the BEV and satellite features, we
introduce a residual convolution block to refine the projected BEV feature.
Optical flow estimation is performed on the refined BEV feature map and the
satellite feature map using flow decoder networks based on RAFT. After
obtaining dense flow correspondences, we apply the least square method to
filter matching inliers and regress the ground camera pose. Extensive
experiments demonstrate significant improvements compared to state-of-the-art
methods. Notably, our approach reduces the median localization error by 89%,
19%, 80% and 35% on the KITTI, Ford multi-AV, VIGOR and Oxford RobotCar
datasets, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03149">Attributing Learned Concepts in Neural Networks to Training Data. (arXiv:2310.03149v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Konz_N/0/1/0/all/0/1">Nicholas Konz</a>, <a href="http://arxiv.org/find/cs/1/au:+Godfrey_C/0/1/0/all/0/1">Charles Godfrey</a>, <a href="http://arxiv.org/find/cs/1/au:+Shapiro_M/0/1/0/all/0/1">Madelyn Shapiro</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_J/0/1/0/all/0/1">Jonathan Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kvinge_H/0/1/0/all/0/1">Henry Kvinge</a>, <a href="http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1">Davis Brown</a></p>
<p>By now there is substantial evidence that deep learning models learn certain
human-interpretable features as part of their internal representations of data.
As having the right (or wrong) concepts is critical to trustworthy machine
learning systems, it is natural to ask which inputs from the model's original
training set were most important for learning a concept at a given layer. To
answer this, we combine data attribution methods with methods for probing the
concepts learned by a model. Training network and probe ensembles for two
concept datasets on a range of network layers, we use the recently developed
TRAK method for large-scale data attribution. We find some evidence for
convergence, where removing the 10,000 top attributing images for a concept and
retraining the model does not change the location of the concept in the network
nor the probing sparsity of the concept. This suggests that rather than being
highly dependent on a few specific examples, the features that inform the
development of a concept are spread in a more diffuse manner across its
exemplars, implying robustness in concept formation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03704">Pose-Free Generalizable Rendering Transformer. (arXiv:2310.03704v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1">Zhiwen Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_P/0/1/0/all/0/1">Panwang Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Peihao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yifan Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1">Hanwen Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1">Dejia Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Zehao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dilin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhangyang Wang</a></p>
<p>In the field of novel-view synthesis, the necessity of knowing camera poses
(e.g., via Structure from Motion) before rendering has been a common practice.
However, the consistent acquisition of accurate camera poses remains elusive,
and errors in pose extraction can adversely impact the view synthesis process.
To address this challenge, we introduce PF-GRT, a new Pose-Free framework for
Generalizable Rendering Transformer, eliminating the need for pre-computed
camera poses and instead leveraging feature-matching learned directly from
data. PF-GRT is parameterized using a local relative coordinate system, where
one of the source images is set as the origin. An OmniView Transformer is
designed for fusing multi-view cues under the pose-free setting, where
unposed-view fusion and origin-centric aggregation are performed. The 3D point
feature along target ray is sampled by projecting onto the selected origin
plane. The final pixel intensities are modulated and decoded using another
Transformer. PF-GRT demonstrates an impressive ability to generalize to new
scenes that were not encountered during the training phase, without the need of
pre-computing camera poses. Our experiments with zero-shot rendering on the
LLFF, RealEstate-10k, Shiny, and Blender datasets reveal that it produces
superior quality in generating photo-realistic images. Moreover, it
demonstrates robustness against noise in test camera poses. Code is available
at https://zhiwenfan.github.io/PF-GRT/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04780">IPMix: Label-Preserving Data Augmentation Method for Training Robust Classifiers. (arXiv:2310.04780v6 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zhenglin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bao_X/0/1/0/all/0/1">Xianan Bao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Na Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qingqi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_X/0/1/0/all/0/1">Xiaomei Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1">Biao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xi Yang</a></p>
<p>Data augmentation has been proven effective for training high-accuracy
convolutional neural network classifiers by preventing overfitting. However,
building deep neural networks in real-world scenarios requires not only high
accuracy on clean data but also robustness when data distributions shift. While
prior methods have proposed that there is a trade-off between accuracy and
robustness, we propose IPMix, a simple data augmentation approach to improve
robustness without hurting clean accuracy. IPMix integrates three levels of
data augmentation (image-level, patch-level, and pixel-level) into a coherent
and label-preserving technique to increase the diversity of training data with
limited computational overhead. To further improve the robustness, IPMix
introduces structural complexity at different levels to generate more diverse
images and adopts the random mixing method for multi-scale information fusion.
Experiments demonstrate that IPMix outperforms state-of-the-art corruption
robustness on CIFAR-C and ImageNet-C. In addition, we show that IPMix also
significantly improves the other safety measures, including robustness to
adversarial perturbations, calibration, prediction consistency, and anomaly
detection, achieving state-of-the-art or comparable results on several
benchmarks, including ImageNet-R, ImageNet-A, and ImageNet-O.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04829">How To Effectively Train An Ensemble Of Faster R-CNN Object Detectors To Quantify Uncertainty. (arXiv:2310.04829v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Akola_D/0/1/0/all/0/1">Denis Mbey Akola</a>, <a href="http://arxiv.org/find/cs/1/au:+Franchi_G/0/1/0/all/0/1">Gianni Franchi</a></p>
<p>This paper presents a new approach for training two-stage object detection
ensemble models, more specifically, Faster R-CNN models to estimate
uncertainty. We propose training one Region Proposal Network(RPN) and multiple
Fast R-CNN prediction heads is all you need to build a robust deep ensemble
network for estimating uncertainty in object detection. We present this
approach and provide experiments to show that this approach is much faster than
the naive method of fully training all $n$ models in an ensemble. We also
estimate the uncertainty by measuring this ensemble model's Expected
Calibration Error (ECE). We then further compare the performance of this model
with that of Gaussian YOLOv3, a variant of YOLOv3 that models uncertainty using
predicted bounding box coordinates. The source code is released at
\url{https://github.com/Akola-Mbey-Denis/EfficientEnsemble}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08475">Can We Edit Multimodal Large Language Models?. (arXiv:2310.08475v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1">Siyuan Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_B/0/1/0/all/0/1">Bozhong Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qingbin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yongheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huajun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Ningyu Zhang</a></p>
<p>In this paper, we focus on editing Multimodal Large Language Models (MLLMs).
Compared to editing single-modal LLMs, multimodal model editing is more
challenging, which demands a higher level of scrutiny and careful consideration
in the editing process. To facilitate research in this area, we construct a new
benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite
of innovative metrics for evaluation. We conduct comprehensive experiments
involving various model editing baselines and analyze the impact of editing
different components for multimodal LLMs. Empirically, we notice that previous
baselines can implement editing multimodal LLMs to some extent, but the effect
is still barely satisfactory, indicating the potential difficulty of this task.
We hope that our work can provide the NLP community with insights. Code and
dataset are available in https://github.com/zjunlp/EasyEdit.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.09828">Top-K Pooling with Patch Contrastive Learning for Weakly-Supervised Semantic Segmentation. (arXiv:2310.09828v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1">Wangyu Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_T/0/1/0/all/0/1">Tianhong Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xiaowei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1">Fei Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1">Jimin Xiao</a></p>
<p>Weakly Supervised Semantic Segmentation (WSSS) using only image-level labels
has gained significant attention due to cost-effectiveness. Recently, Vision
Transformer (ViT) based methods without class activation map (CAM) have shown
greater capability in generating reliable pseudo labels than previous methods
using CAM. However, the current ViT-based methods utilize max pooling to select
the patch with the highest prediction score to map the patch-level
classification to the image-level one, which may affect the quality of pseudo
labels due to the inaccurate classification of the patches. In this paper, we
introduce a novel ViT-based WSSS method named top-K pooling with patch
contrastive learning (TKP-PCL), which employs a top-K pooling layer to
alleviate the limitations of previous max pooling selection. A patch
contrastive error (PCE) is also proposed to enhance the patch embeddings to
further improve the final results. The experimental results show that our
approach is very efficient and outperforms other state-of-the-art WSSS methods
on the PASCAL VOC 2012 dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.11077">United We Stand: Using Epoch-wise Agreement of Ensembles to Combat Overfit. (arXiv:2310.11077v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stern_U/0/1/0/all/0/1">Uri Stern</a>, <a href="http://arxiv.org/find/cs/1/au:+Shwartz_D/0/1/0/all/0/1">Daniel Shwartz</a>, <a href="http://arxiv.org/find/cs/1/au:+Weinshall_D/0/1/0/all/0/1">Daphna Weinshall</a></p>
<p>Deep neural networks have become the method of choice for solving many
classification tasks, largely because they can fit very complex functions
defined over raw data. The downside of such powerful learners is the danger of
overfit. In this paper, we introduce a novel ensemble classifier for deep
networks that effectively overcomes overfitting by combining models generated
at specific intermediate epochs during training. Our method allows for the
incorporation of useful knowledge obtained by the models during the overfitting
phase without deterioration of the general performance, which is usually missed
when early stopping is used. To motivate this approach, we begin with the
theoretical analysis of a regression model, whose prediction -- that the
variance among classifiers increases when overfit occurs -- is demonstrated
empirically in deep networks in common use. Guided by these results, we
construct a new ensemble-based prediction method, where the prediction is
determined by the class that attains the most consensual prediction throughout
the training epochs. Using multiple image and text classification datasets, we
show that when regular ensembles suffer from overfit, our method eliminates the
harmful reduction in generalization due to overfit, and often even surpasses
the performance obtained by early stopping. Our method is easy to implement and
can be integrated with any training scheme and architecture, without additional
prior knowledge beyond the training set. It is thus a practical and useful tool
to overcome overfit. Code is available at
https://github.com/uristern123/United-We-Stand-Using-Epoch-wise-Agreement-of-Ensembles-to-Combat-Overfit.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.15081">E4S: Fine-grained Face Swapping via Editing With Regional GAN Inversion. (arXiv:2310.15081v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Maomao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1">Ge Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Cairong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhian Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1">Yongwei Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jue Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1">Dong Xu</a></p>
<p>This paper proposes a novel approach to face swapping from the perspective of
fine-grained facial editing, dubbed "editing for swapping" (E4S). The
traditional face swapping methods rely on global feature extraction and often
fail to preserve the source identity. In contrast, our framework proposes a
Regional GAN Inversion (RGI) method, which allows the explicit disentanglement
of shape and texture. Specifically, our E4S performs face swapping in the
latent space of a pretrained StyleGAN, where a multi-scale mask-guided encoder
is applied to project the texture of each facial component into regional style
codes and a mask-guided injection module then manipulates feature maps with the
style codes. Based on this disentanglement, face swapping can be simplified as
style and mask swapping. Besides, since reconstructing the source face in the
target image may lead to disharmony lighting, we propose to train a re-coloring
network to make the swapped face maintain the lighting condition on the target
face. Further, to deal with the potential mismatch area during mask exchange,
we designed a face inpainting network as post-processing. The extensive
comparisons with state-of-the-art methods demonstrate that our E4S outperforms
existing methods in preserving texture, shape, and lighting. Our implementation
is available at https://github.com/e4s2023/E4S2023.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16221">Hierarchical Randomized Smoothing. (arXiv:2310.16221v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Scholten_Y/0/1/0/all/0/1">Yan Scholten</a>, <a href="http://arxiv.org/find/cs/1/au:+Schuchardt_J/0/1/0/all/0/1">Jan Schuchardt</a>, <a href="http://arxiv.org/find/cs/1/au:+Bojchevski_A/0/1/0/all/0/1">Aleksandar Bojchevski</a>, <a href="http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1">Stephan G&#xfc;nnemann</a></p>
<p>Real-world data is complex and often consists of objects that can be
decomposed into multiple entities (e.g. images into pixels, graphs into
interconnected nodes). Randomized smoothing is a powerful framework for making
models provably robust against small changes to their inputs - by guaranteeing
robustness of the majority vote when randomly adding noise before
classification. Yet, certifying robustness on such complex data via randomized
smoothing is challenging when adversaries do not arbitrarily perturb entire
objects (e.g. images) but only a subset of their entities (e.g. pixels). As a
solution, we introduce hierarchical randomized smoothing: We partially smooth
objects by adding random noise only on a randomly selected subset of their
entities. By adding noise in a more targeted manner than existing methods we
obtain stronger robustness guarantees while maintaining high accuracy. We
initialize hierarchical smoothing using different noising distributions,
yielding novel robustness certificates for discrete and continuous domains. We
experimentally demonstrate the importance of hierarchical smoothing in image
and node classification, where it yields superior robustness-accuracy
trade-offs. Overall, hierarchical smoothing is an important contribution
towards models that are both - certifiably robust to perturbations and
accurate.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.20704">Limited Data, Unlimited Potential: A Study on ViTs Augmented by Masked Autoencoders. (arXiv:2310.20704v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1">Srijan Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_T/0/1/0/all/0/1">Tanmay Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Reilly_D/0/1/0/all/0/1">Dominick Reilly</a>, <a href="http://arxiv.org/find/cs/1/au:+Balaji_P/0/1/0/all/0/1">Pranav Balaji</a>, <a href="http://arxiv.org/find/cs/1/au:+Karmakar_S/0/1/0/all/0/1">Soumyajit Karmakar</a>, <a href="http://arxiv.org/find/cs/1/au:+Marjit_S/0/1/0/all/0/1">Shyam Marjit</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1">Abhijit Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Ryoo_M/0/1/0/all/0/1">Michael S. Ryoo</a></p>
<p>Vision Transformers (ViTs) have become ubiquitous in computer vision. Despite
their success, ViTs lack inductive biases, which can make it difficult to train
them with limited data. To address this challenge, prior studies suggest
training ViTs with self-supervised learning (SSL) and fine-tuning sequentially.
However, we observe that jointly optimizing ViTs for the primary task and a
Self-Supervised Auxiliary Task (SSAT) is surprisingly beneficial when the
amount of training data is limited. We explore the appropriate SSL tasks that
can be optimized alongside the primary task, the training schemes for these
tasks, and the data scale at which they can be most effective. Our findings
reveal that SSAT is a powerful technique that enables ViTs to leverage the
unique characteristics of both the self-supervised and primary tasks, achieving
better performance than typical ViTs pre-training with SSL and fine-tuning
sequentially. Our experiments, conducted on 10 datasets, demonstrate that SSAT
significantly improves ViT performance while reducing carbon footprint. We also
confirm the effectiveness of SSAT in the video domain for deepfake detection,
showcasing its generalizability. Our code is available at
https://github.com/dominickrei/Limited-data-vits.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05348">u-LLaVA: Unifying Multi-Modal Tasks via Large Language Model. (arXiv:2311.05348v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jinjin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1">Liwu Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yuzhe Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1">Yanchun Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yi-Jie Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yaqian Li</a></p>
<p>Recent advances such as LLaVA and Mini-GPT4 have successfully integrated
visual information into LLMs, yielding inspiring outcomes and giving rise to a
new generation of multi-modal LLMs, or MLLMs. Nevertheless, these methods
struggle with hallucinations and the mutual interference between tasks. To
tackle these problems, we propose an efficient and accurate approach to adapt
to downstream tasks by utilizing LLM as a bridge to connect multiple expert
models, namely u-LLaVA. Firstly, we incorporate the modality alignment module
and multi-task modules into LLM. Then, we reorganize or rebuild multi-type
public datasets to enable efficient modality alignment and instruction
following. Finally, task-specific information is extracted from the trained LLM
and provided to different modules for solving downstream tasks. The overall
framework is simple, effective, and achieves state-of-the-art performance
across multiple benchmarks. We also release our model, the generated data, and
the code base publicly available at https://github.com/OPPOMKLab/u-LLaVA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10319">Shifting to Machine Supervision: Annotation-Efficient Semi and Self-Supervised Learning for Automatic Medical Image Segmentation and Classification. (arXiv:2311.10319v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1">Pranav Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Chukkapalli_R/0/1/0/all/0/1">Raviteja Chukkapalli</a>, <a href="http://arxiv.org/find/cs/1/au:+Chaudhari_S/0/1/0/all/0/1">Shravan Chaudhari</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Luoyao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Mei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1">Jinqian Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Smuda_C/0/1/0/all/0/1">Craig Smuda</a>, <a href="http://arxiv.org/find/cs/1/au:+Cirrone_J/0/1/0/all/0/1">Jacopo Cirrone</a></p>
<p>Advancements in clinical treatment are increasingly constrained by the
limitations of supervised learning techniques, which depend heavily on large
volumes of annotated data. The annotation process is not only costly but also
demands substantial time from clinical specialists. Addressing this issue, we
introduce the S4MI (Self-Supervision and Semi-Supervision for Medical Imaging)
pipeline, a novel approach that leverages the advancements in self-supervised
and semi-supervised learning. These techniques engage in auxiliary tasks that
do not require labeling, thus simplifying the scaling of machine supervision
compared to fully-supervised methods. Our study benchmarks these techniques on
three distinct medical imaging datasets to evaluate their effectiveness in
classification and segmentation tasks. Remarkably, we observed that
self-supervised learning with only 10% of the annotation surpassed the
performance of full annotation in the classification of most datasets.
Similarly, the semi-supervised approach demonstrated superior outcomes in
segmentation, outperforming fully-supervised methods with 50% fewer labels
across all datasets. In line with our commitment to contributing to the
scientific community, we have made the S4MI code openly accessible, allowing
for broader application and further development of these methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12893">A Safer Vision-based Autonomous Planning System for Quadrotor UAVs with Dynamic Obstacle Trajectory Prediction and Its Application with LLMs. (arXiv:2311.12893v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhong_J/0/1/0/all/0/1">Jiageng Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Ming Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yinliang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1">Zihang Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1">Fan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1">Haoran Shen</a></p>
<p>For intelligent quadcopter UAVs, a robust and reliable autonomous planning
system is crucial. Most current trajectory planning methods for UAVs are
suitable for static environments but struggle to handle dynamic obstacles,
which can pose challenges and even dangers to flight. To address this issue,
this paper proposes a vision-based planning system that combines tracking and
trajectory prediction of dynamic obstacles to achieve efficient and reliable
autonomous flight. We use a lightweight object detection algorithm to identify
dynamic obstacles and then use Kalman Filtering to track and estimate their
motion states. During the planning phase, we not only consider static obstacles
but also account for the potential movements of dynamic obstacles. For
trajectory generation, we use a B-spline-based trajectory search algorithm,
which is further optimized with various constraints to enhance safety and
alignment with the UAV's motion characteristics. We conduct experiments in both
simulation and real-world environments, and the results indicate that our
approach can successfully detect and avoid obstacles in dynamic environments in
real-time, offering greater reliability compared to existing approaches.
Furthermore, with the advancements in Natural Language Processing (NLP)
technology demonstrating exceptional zero-shot generalization capabilities,
more user-friendly human-machine interactions have become feasible, and this
study also explores the integration of autonomous planning systems with Large
Language Models (LLMs).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12914">Attention Deficit is Ordered! Fooling Deformable Vision Transformers with Collaborative Adversarial Patches. (arXiv:2311.12914v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alam_Q/0/1/0/all/0/1">Quazi Mishkatul Alam</a>, <a href="http://arxiv.org/find/cs/1/au:+Tarchoun_B/0/1/0/all/0/1">Bilel Tarchoun</a>, <a href="http://arxiv.org/find/cs/1/au:+Alouani_I/0/1/0/all/0/1">Ihsen Alouani</a>, <a href="http://arxiv.org/find/cs/1/au:+Abu_Ghazaleh_N/0/1/0/all/0/1">Nael Abu-Ghazaleh</a></p>
<p>The latest generation of transformer-based vision models has proven to be
superior to Convolutional Neural Network (CNN)-based models across several
vision tasks, largely attributed to their remarkable prowess in relation
modeling. Deformable vision transformers significantly reduce the quadratic
complexity of attention modeling by using sparse attention structures, enabling
them to incorporate features across different scales and be used in large-scale
applications, such as multi-view vision systems. Recent work has demonstrated
adversarial attacks against conventional vision transformers; we show that
these attacks do not transfer to deformable transformers due to their sparse
attention structure. Specifically, attention in deformable transformers is
modeled using pointers to the most relevant other tokens. In this work, we
contribute for the first time adversarial attacks that manipulate the attention
of deformable transformers, redirecting it to focus on irrelevant parts of the
image. We also develop new collaborative attacks where a source patch
manipulates attention to point to a target patch, which contains the
adversarial noise to fool the model. In our experiments, we observe that
altering less than 1% of the patched area in the input field results in a
complete drop to 0% AP in single-view object detection using MS COCO and a 0%
MODA in multi-view object detection using Wildtrack.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14773">Set Features for Anomaly Detection. (arXiv:2311.14773v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cohen_N/0/1/0/all/0/1">Niv Cohen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tzachor_I/0/1/0/all/0/1">Issar Tzachor</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoshen_Y/0/1/0/all/0/1">Yedid Hoshen</a></p>
<p>This paper proposes set features for detecting anomalies in samples that
consist of unusual combinations of normal elements. Many leading methods
discover anomalies by detecting an unusual part of a sample. For example,
state-of-the-art segmentation-based approaches, first classify each element of
the sample (e.g., image patch) as normal or anomalous and then classify the
entire sample as anomalous if it contains anomalous elements. However, such
approaches do not extend well to scenarios where the anomalies are expressed by
an unusual combination of normal elements. In this paper, we overcome this
limitation by proposing set features that model each sample by the distribution
of its elements. We compute the anomaly score of each sample using a simple
density estimation method, using fixed features. Our approach outperforms the
previous state-of-the-art in image-level logical anomaly detection and
sequence-level time series anomaly detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17532">Weakly-Supervised Emotion Transition Learning for Diverse 3D Co-speech Gesture Generation. (arXiv:2311.17532v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1">Xingqun Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1">Jiahao Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Peng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_R/0/1/0/all/0/1">Ruibin Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chi_X/0/1/0/all/0/1">Xiaowei Chi</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Mengfei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1">Wenhan Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1">Wei Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shanghang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qifeng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yike Guo</a></p>
<p>Generating vivid and emotional 3D co-speech gestures is crucial for virtual
avatar animation in human-machine interaction applications. While the existing
methods enable generating the gestures to follow a single emotion label, they
overlook that long gesture sequence modeling with emotion transition is more
practical in real scenes. In addition, the lack of large-scale available
datasets with emotional transition speech and corresponding 3D human gestures
also limits the addressing of this task. To fulfill this goal, we first
incorporate the ChatGPT-4 and an audio inpainting approach to construct the
high-fidelity emotion transition human speeches. Considering obtaining the
realistic 3D pose annotations corresponding to the dynamically inpainted
emotion transition audio is extremely difficult, we propose a novel weakly
supervised training strategy to encourage authority gesture transitions.
Specifically, to enhance the coordination of transition gestures w.r.t
different emotional ones, we model the temporal association representation
between two different emotional gesture sequences as style guidance and infuse
it into the transition generation. We further devise an emotion mixture
mechanism that provides weak supervision based on a learnable mixed emotion
label for transition gestures. Last, we present a keyframe sampler to supply
effective initial posture cues in long sequences, enabling us to generate
diverse gestures. Extensive experiments demonstrate that our method outperforms
the state-of-the-art models constructed by adapting single emotion-conditioned
counterparts on our newly defined emotion transition task and datasets. Our
code and dataset will be released on the project page:
https://xingqunqi-lab.github.io/Emo-Transition/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17812">DAP: Domain-aware Prompt Learning for Vision-and-Language Navigation. (arXiv:2311.17812v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Ting Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yue Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1">Wansen Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Youkai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Kai Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_Q/0/1/0/all/0/1">Quanjun Yin</a></p>
<p>Following language instructions to navigate in unseen environments is a
challenging task for autonomous embodied agents. With strong representation
capabilities, pretrained vision-and-language models are widely used in VLN.
However, most of them are trained on web-crawled general-purpose datasets,
which incurs a considerable domain gap when used for VLN tasks. To address the
problem, we propose a novel and model-agnostic domain-aware prompt learning
(DAP) framework. For equipping the pretrained models with specific object-level
and scene-level cross-modal alignment in VLN tasks, DAP applies a low-cost
prompt tuning paradigm to learn soft visual prompts for extracting in-domain
image semantics. Specifically, we first generate a set of in-domain image-text
pairs with the help of the CLIP model. Then we introduce soft visual prompts in
the input space of the visual encoder in a pretrained model. DAP injects
in-domain visual knowledge into the visual encoder of the pretrained model in
an efficient way. Experimental results on both R2R and REVERIE show the
superiority of DAP compared to existing state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18433">E2PNet: Event to Point Cloud Registration with Spatio-Temporal Representation Learning. (arXiv:2311.18433v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1">Xiuhong Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_C/0/1/0/all/0/1">Changjie Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1">Zhipeng Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1">Siqi Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zang_Y/0/1/0/all/0/1">Yu Zang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Weiquan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bian_X/0/1/0/all/0/1">Xuesheng Bian</a>, <a href="http://arxiv.org/find/cs/1/au:+Muller_M/0/1/0/all/0/1">Matthias M&#xfc;ller</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Cheng Wang</a></p>
<p>Event cameras have emerged as a promising vision sensor in recent years due
to their unparalleled temporal resolution and dynamic range. While registration
of 2D RGB images to 3D point clouds is a long-standing problem in computer
vision, no prior work studies 2D-3D registration for event cameras. To this
end, we propose E2PNet, the first learning-based method for event-to-point
cloud registration. The core of E2PNet is a novel feature representation
network called Event-Points-to-Tensor (EP2T), which encodes event data into a
2D grid-shaped feature tensor. This grid-shaped feature enables matured
RGB-based frameworks to be easily used for event-to-point cloud registration,
without changing hyper-parameters and the training procedure. EP2T treats the
event input as spatio-temporal point clouds. Unlike standard 3D learning
architectures that treat all dimensions of point clouds equally, the novel
sampling and information aggregation modules in EP2T are designed to handle the
inhomogeneity of the spatial and temporal dimensions. Experiments on the MVSEC
and VECtor datasets demonstrate the superiority of E2PNet over hand-crafted and
other learning-based methods. Compared to RGB-based registration, E2PNet is
more robust to extreme illumination or fast motion due to the use of event
data. Beyond 2D-3D registration, we also show the potential of EP2T for other
vision tasks such as flow estimation, event-to-image reconstruction and object
recognition. The source code can be found at:
https://github.com/Xmu-qcj/E2PNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00085">X-Dreamer: Creating High-quality 3D Content by Bridging the Domain Gap Between Text-to-2D and Text-to-3D Generation. (arXiv:2312.00085v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yiwei Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1">Yijun Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1">Jiayi Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haowei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xiaoshuai Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_G/0/1/0/all/0/1">Guannan Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shu_A/0/1/0/all/0/1">Annan Shu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1">Rongrong Ji</a></p>
<p>In recent times, automatic text-to-3D content creation has made significant
progress, driven by the development of pretrained 2D diffusion models. Existing
text-to-3D methods typically optimize the 3D representation to ensure that the
rendered image aligns well with the given text, as evaluated by the pretrained
2D diffusion model. Nevertheless, a substantial domain gap exists between 2D
images and 3D assets, primarily attributed to variations in camera-related
attributes and the exclusive presence of foreground objects. Consequently,
employing 2D diffusion models directly for optimizing 3D representations may
lead to suboptimal outcomes. To address this issue, we present X-Dreamer, a
novel approach for high-quality text-to-3D content creation that effectively
bridges the gap between text-to-2D and text-to-3D synthesis. The key components
of X-Dreamer are two innovative designs: Camera-Guided Low-Rank Adaptation
(CG-LoRA) and Attention-Mask Alignment (AMA) Loss. CG-LoRA dynamically
incorporates camera information into the pretrained diffusion models by
employing camera-dependent generation for trainable parameters. This
integration enhances the alignment between the generated 3D assets and the
camera's perspective. AMA loss guides the attention map of the pretrained
diffusion model using the binary mask of the 3D object, prioritizing the
creation of the foreground object. This module ensures that the model focuses
on generating accurate and detailed foreground objects. Extensive evaluations
demonstrate the effectiveness of our proposed method compared to existing
text-to-3D approaches. Our project webpage:
https://xmu-xiaoma666.github.io/Projects/X-Dreamer/ .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01632">GaussianHead: Impressive Head Avatars with Learnable Gaussian Diffusion. (arXiv:2312.01632v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1">Jiu-Cheng Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xianyan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1">Feng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pun_C/0/1/0/all/0/1">Chi-Man Pun</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1">Hao Gao</a></p>
<p>Previous head avatar methods have primarily relied on fixed-shape scene
primitives, lacking a balance between geometric topology, texture details, and
computational efficiency. Some hybrid neural network methods (e.g., planes and
voxels) gained advantages in fast rendering, but they all used axis-aligned
mappings to extract features explicitly, leading to issues of axis-aligned bias
and feature dilution. We present GaussianHead, which utilizes deformable 3D
Gaussians as building blocks for the head avatars. We propose a novel
methodology where the core Gaussians designated for rendering undergo dynamic
diffusion before being mapped onto a factor plane to acquire canonical
sub-factors. Through our factor blending strategy, the canonical features for
the core Gaussians used in rendering are obtained. This approach deviates from
the previous practice of utilizing axis-aligned mappings, especially improving
the representation capability of subtle structures such as teeth, wrinkles,
hair, and even facial pores. In comparison to state-of-the-art methods, our
unique primitive selection and factor decomposition in GaussianHead deliver
superior visual results while maintaining rendering performance (0.1 seconds
per frame). Code will released for research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01697">Hulk: A Universal Knowledge Translator for Human-Centric Tasks. (arXiv:2312.01697v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yizhou Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yixuan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1">Shixiang Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1">Weizhen He</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1">Xun Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1">Feng Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1">Lei Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1">Rui Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jian Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1">Tong He</a>, <a href="http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1">Wanli Ouyang</a></p>
<p>Human-centric perception tasks, e.g., pedestrian detection, skeleton-based
action recognition, and pose estimation, have wide industrial applications,
such as metaverse and sports analysis. There is a recent surge to develop
human-centric foundation models that can benefit a broad range of human-centric
perception tasks. While many human-centric foundation models have achieved
success, they did not explore 3D and vision-language tasks for human-centric
and required task-specific finetuning. These limitations restrict their
application to more downstream tasks and situations. To tackle these problems,
we present Hulk, the first multimodal human-centric generalist model, capable
of addressing 2D vision, 3D vision, skeleton-based, and vision-language tasks
without task-specific finetuning. The key to achieving this is condensing
various task-specific heads into two general heads, one for discrete
representations, e.g., languages, and the other for continuous representations,
e.g., location coordinates. The outputs of two heads can be further stacked
into four distinct input and output modalities. This uniform representation
enables Hulk to treat diverse human-centric tasks as modality translation,
integrating knowledge across a wide range of tasks. Comprehensive evaluations
of Hulk on 12 benchmarks covering 8 human-centric tasks demonstrate the
superiority of our proposed method, achieving state-of-the-art performance in
11 benchmarks. The code will be available on
https://github.com/OpenGVLab/HumanBench.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02197">Exploiting Diffusion Priors for All-in-One Image Restoration. (arXiv:2312.02197v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gou_Y/0/1/0/all/0/1">Yuanbiao Gou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Haiyu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Boyun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1">Xinyan Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1">Xi Peng</a></p>
<p>All-in-one aims to solve various tasks of image restoration in a single
model. To this end, we present a feasible way of exploiting the image priors
captured by the pretrained diffusion model, through addressing the two
challenges, i.e., degradation modeling and diffusion guidance. The former aims
to simulate the process of the clean image degenerated by the unknown
degradations, and the latter aims at guiding the diffusion model to generate
the desired clean image. With the motivations, we propose a zero-shot framework
for all-in-one image restoration, termed ZeroAIR, which alternatively performs
the test-time degradation modeling (TDM) and the three-stage diffusion guidance
(TDG) at each timestep of the reverse sampling. To be specific, TDM exploits
the diffusion priors to learn a degradation model from a given degraded image,
and TDG divides the timesteps into three stages for taking full advantages of
the varying diffusion priors. Thanks to their degradation-agnostic property,
all-in-one restoration could be achieved in a zero-shot way. Through extensive
experiments, we show that our ZeroAIR achieves comparable even better
performance than those task-specific methods. The code will be available on
Github.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02225">Digital Histopathology with Graph Neural Networks: Concepts and Explanations for Clinicians. (arXiv:2312.02225v2 [physics.med-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Villaforesta_A/0/1/0/all/0/1">Alessandro Farace di Villaforesta</a>, <a href="http://arxiv.org/find/physics/1/au:+Magister_L/0/1/0/all/0/1">Lucie Charlotte Magister</a>, <a href="http://arxiv.org/find/physics/1/au:+Barbiero_P/0/1/0/all/0/1">Pietro Barbiero</a>, <a href="http://arxiv.org/find/physics/1/au:+Lio_P/0/1/0/all/0/1">Pietro Li&#xf2;</a></p>
<p>To address the challenge of the ``black-box" nature of deep learning in
medical settings, we combine GCExplainer - an automated concept discovery
solution - along with Logic Explained Networks to provide global explanations
for Graph Neural Networks. We demonstrate this using a generally applicable
graph construction and classification pipeline, involving panoptic segmentation
with HoVer-Net and cancer prediction with Graph Convolution Networks. By
training on H&amp;E slides of breast cancer, we show promising results in offering
explainable and trustworthy AI tools for clinicians.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02366">Towards General Purpose Vision Foundation Models for Medical Image Analysis: An Experimental Study of DINOv2 on Radiology Benchmarks. (arXiv:2312.02366v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Baharoon_M/0/1/0/all/0/1">Mohammed Baharoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Qureshi_W/0/1/0/all/0/1">Waseem Qureshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ouyang_J/0/1/0/all/0/1">Jiahong Ouyang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yanwu Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Aljouie_A/0/1/0/all/0/1">Abdulrhman Aljouie</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1">Wei Peng</a></p>
<p>The integration of deep learning systems into healthcare has been hindered by
the resource-intensive process of data annotation and the inability of these
systems to generalize to different data distributions. Foundation models, which
are models pre-trained on large datasets, have emerged as a solution to reduce
reliance on annotated data and enhance model generalizability and robustness.
DINOv2 is an open-source foundation model pre-trained with self-supervised
learning on 142 million curated natural images that exhibits promising
capabilities across various vision tasks. Nevertheless, a critical question
remains unanswered regarding DINOv2's adaptability to radiological imaging, and
whether its features are sufficiently general to benefit radiology image
analysis. Therefore, this study comprehensively evaluates DINOv2 for radiology,
conducting over 100 experiments across diverse modalities (X-ray, CT, and MRI).
To measure the effectiveness and generalizability of DINOv2's feature
representations, we analyze the model across medical image analysis tasks
including disease classification and organ segmentation on both 2D and 3D
images, and under different settings like kNN, few-shot learning,
linear-probing, end-to-end fine-tuning, and parameter-efficient fine-tuning.
Comparative analyses with established supervised, self-supervised, and
weakly-supervised models reveal DINOv2's superior performance and cross-task
generalizability. The findings contribute insights to potential avenues for
optimizing pre-training strategies for medical imaging and enhancing the
broader understanding of DINOv2's role in bridging the gap between natural and
radiological image analysis. Our code is available at
https://github.com/MohammedSB/DINOv2ForRadiology
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04316">Towards Knowledge-driven Autonomous Driving. (arXiv:2312.04316v3 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1">Yeqi Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1">Pinlong Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1">Licheng Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1">Daocheng Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Bo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xuemeng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1">Xinyu Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1">Tao Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jianfei Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xing Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Dou_M/0/1/0/all/0/1">Min Dou</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yikang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1">Botian Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1">Liang He</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a></p>
<p>This paper explores the emerging knowledge-driven autonomous driving
technologies. Our investigation highlights the limitations of current
autonomous driving systems, in particular their sensitivity to data bias,
difficulty in handling long-tail scenarios, and lack of interpretability.
Conversely, knowledge-driven methods with the abilities of cognition,
generalization and life-long learning emerge as a promising way to overcome
these challenges. This paper delves into the essence of knowledge-driven
autonomous driving and examines its core components: dataset \&amp; benchmark,
environment, and driver agent. By leveraging large language models, world
models, neural rendering, and other advanced artificial intelligence
techniques, these components collectively contribute to a more holistic,
adaptive, and intelligent autonomous driving system. The paper systematically
organizes and reviews previous research efforts in this area, and provides
insights and guidance for future research and practical applications of
autonomous driving. We will continually share the latest updates on
cutting-edge developments in knowledge-driven autonomous driving along with the
relevant valuable open-source resources at:
\url{https://github.com/PJLab-ADG/awesome-knowledge-driven-AD}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06704">SIFU: Side-view Conditioned Implicit Function for Real-world Usable Clothed Human Reconstruction. (arXiv:2312.06704v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zechuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zongxin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yi Yang</a></p>
<p>Creating high-quality 3D models of clothed humans from single images for
real-world applications is crucial. Despite recent advancements, accurately
reconstructing humans in complex poses or with loose clothing from in-the-wild
images, along with predicting textures for unseen areas, remains a significant
challenge. A key limitation of previous methods is their insufficient prior
guidance in transitioning from 2D to 3D and in texture prediction. In response,
we introduce SIFU (Side-view Conditioned Implicit Function for Real-world
Usable Clothed Human Reconstruction), a novel approach combining a Side-view
Decoupling Transformer with a 3D Consistent Texture Refinement pipeline.SIFU
employs a cross-attention mechanism within the transformer, using SMPL-X
normals as queries to effectively decouple side-view features in the process of
mapping 2D features to 3D. This method not only improves the precision of the
3D models but also their robustness, especially when SMPL-X estimates are not
perfect. Our texture refinement process leverages text-to-image diffusion-based
prior to generate realistic and consistent textures for invisible views.
Through extensive experiments, SIFU surpasses SOTA methods in both geometry and
texture reconstruction, showcasing enhanced robustness in complex scenarios and
achieving an unprecedented Chamfer and P2S measurement. Our approach extends to
practical applications such as 3D printing and scene building, demonstrating
its broad utility in real-world scenarios. Project page
https://river-zhang.github.io/SIFU-projectpage/ .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06709">AM-RADIO: Agglomerative Model -- Reduce All Domains Into One. (arXiv:2312.06709v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ranzinger_M/0/1/0/all/0/1">Mike Ranzinger</a>, <a href="http://arxiv.org/find/cs/1/au:+Heinrich_G/0/1/0/all/0/1">Greg Heinrich</a>, <a href="http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1">Jan Kautz</a>, <a href="http://arxiv.org/find/cs/1/au:+Molchanov_P/0/1/0/all/0/1">Pavlo Molchanov</a></p>
<p>A handful of visual foundation models (VFMs) have recently emerged as the
backbones for numerous downstream tasks. VFMs like CLIP, DINOv2, SAM are
trained with distinct objectives, exhibiting unique characteristics for various
downstream tasks. We find that despite their conceptual differences, these
models can be effectively merged into a unified model through multi-teacher
distillation. We name this approach AM-RADIO (Agglomerative Model -- Reduce All
Domains Into One). This integrative approach not only surpasses the performance
of individual teacher models but also amalgamates their distinctive features,
such as zero-shot vision-language comprehension, detailed pixel-level
understanding, and open vocabulary segmentation capabilities. In pursuit of the
most hardware-efficient backbone, we evaluated numerous architectures in our
multi-teacher distillation pipeline using the same training recipe. This led to
the development of a novel architecture (E-RADIO) that exceeds the performance
of its predecessors and is at least 7x faster than the teacher models. Our
comprehensive benchmarking process covers downstream tasks including ImageNet
classification, ADE20k semantic segmentation, COCO object detection and
LLaVa-1.5 framework.
</p>
<p>Code: https://github.com/NVlabs/RADIO
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06721">Counterfactual World Modeling for Physical Dynamics Understanding. (arXiv:2312.06721v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Venkatesh_R/0/1/0/all/0/1">Rahul Venkatesh</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Honglin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Feigelis_K/0/1/0/all/0/1">Kevin Feigelis</a>, <a href="http://arxiv.org/find/cs/1/au:+Bear_D/0/1/0/all/0/1">Daniel M. Bear</a>, <a href="http://arxiv.org/find/cs/1/au:+Jedoui_K/0/1/0/all/0/1">Khaled Jedoui</a>, <a href="http://arxiv.org/find/cs/1/au:+Kotar_K/0/1/0/all/0/1">Klemen Kotar</a>, <a href="http://arxiv.org/find/cs/1/au:+Binder_F/0/1/0/all/0/1">Felix Binder</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1">Wanhee Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Sherry Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1">Kevin A. Smith</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1">Judith E. Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yamins_D/0/1/0/all/0/1">Daniel L. K. Yamins</a></p>
<p>The ability to understand physical dynamics is essential to learning agents
acting in the world. This paper presents Counterfactual World Modeling (CWM), a
candidate pure vision foundational model for physical dynamics understanding.
CWM consists of three basic concepts. First, we propose a simple and powerful
temporally-factored masking policy for masked prediction of video data, which
encourages the model to learn disentangled representations of scene appearance
and dynamics. Second, as a result of the factoring, CWM is capable of
generating counterfactual next-frame predictions by manipulating a few patch
embeddings to exert meaningful control over scene dynamics. Third, the
counterfactual modeling capability enables the design of counterfactual queries
to extract vision structures similar to keypoints, optical flows, and
segmentations, which are useful for dynamics understanding. We show that
zero-shot readouts of these structures extracted by the counterfactual queries
attain competitive performance to prior methods on real-world datasets.
Finally, we demonstrate that CWM achieves state-of-the-art performance on the
challenging Physion benchmark for evaluating physical dynamics understanding.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07935">Comparing YOLOv8 and Mask RCNN for object segmentation in complex orchard environments. (arXiv:2312.07935v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sapkota_R/0/1/0/all/0/1">Ranjan Sapkota</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_D/0/1/0/all/0/1">Dawood Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Karkee_M/0/1/0/all/0/1">Manoj Karkee</a></p>
<p>Instance segmentation, an important image processing operation for automation
in agriculture, is used to precisely delineate individual objects of interest
within images, which provides foundational information for various automated or
robotic tasks such as selective harvesting and precision pruning. This study
compares the one-stage YOLOv8 and the two-stage Mask R-CNN machine learning
models for instance segmentation under varying orchard conditions across two
datasets. Dataset 1, collected in dormant season, includes images of dormant
apple trees, which were used to train multi-object segmentation models
delineating tree branches and trunks. Dataset 2, collected in the early growing
season, includes images of apple tree canopies with green foliage and immature
(green) apples (also called fruitlet), which were used to train single-object
segmentation models delineating only immature green apples. The results showed
that YOLOv8 performed better than Mask R-CNN, achieving good precision and
near-perfect recall across both datasets at a confidence threshold of 0.5.
Specifically, for Dataset 1, YOLOv8 achieved a precision of 0.90 and a recall
of 0.95 for all classes. In comparison, Mask R-CNN demonstrated a precision of
0.81 and a recall of 0.81 for the same dataset. With Dataset 2, YOLOv8 achieved
a precision of 0.93 and a recall of 0.97. Mask R-CNN, in this single-class
scenario, achieved a precision of 0.85 and a recall of 0.88. Additionally, the
inference times for YOLOv8 were 10.9 ms for multi-class segmentation (Dataset
1) and 7.8 ms for single-class segmentation (Dataset 2), compared to 15.6 ms
and 12.8 ms achieved by Mask R-CNN's, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08078">Fine-Grained Image-Text Alignment in Medical Imaging Enables Cyclic Image-Report Generation. (arXiv:2312.08078v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wenting Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1">Linlin Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1">Yixuan Yuan</a></p>
<p>To address these issues, we propose a novel Adaptive patch-word Matching
(AdaMatch) model to correlate chest X-ray (CXR) image regions with words in
medical reports and apply it to CXR-report generation to provide explainability
for the generation process. AdaMatch exploits the fine-grained relation between
adaptive patches and words to provide explanations of specific image regions
with corresponding words. To capture the abnormal regions of varying sizes and
positions, we introduce the Adaptive Patch extraction (AdaPatch) module to
acquire the adaptive patches for these regions adaptively. In order to provide
explicit explainability for CXR-report generation task, we propose an
AdaMatch-based bidirectional large language model for Cyclic CXR-report
generation (AdaMatch-Cyclic). It employs the AdaMatch to obtain the keywords
for CXR images and `keypatches' for medical reports as hints to guide
CXR-report generation. Extensive experiments on two publicly available CXR
datasets prove the effectiveness of our method and its superior performance to
existing methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08563">Efficient-NeRF2NeRF: Streamlining Text-Driven 3D Editing with Multiview Correspondence-Enhanced Diffusion Models. (arXiv:2312.08563v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1">Liangchen Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1">Liangliang Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jiatao Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yifan Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1">Junsong Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1">Hao Tang</a></p>
<p>The advancement of text-driven 3D content editing has been blessed by the
progress from 2D generative diffusion models. However, a major obstacle
hindering the widespread adoption of 3D content editing is its time-intensive
processing. This challenge arises from the iterative and refining steps
required to achieve consistent 3D outputs from 2D image-based generative
models. Recent state-of-the-art methods typically require optimization time
ranging from tens of minutes to several hours to edit a 3D scene using a single
GPU. In this work, we propose that by incorporating correspondence
regularization into diffusion models, the process of 3D editing can be
significantly accelerated. This approach is inspired by the notion that the
estimated samples during diffusion should be multiview-consistent during the
diffusion generation process. By leveraging this multiview consistency, we can
edit 3D content at a much faster speed. In most scenarios, our proposed
technique brings a 10$\times$ speed-up compared to the baseline method and
completes the editing of a 3D scene in 2 minutes with comparable quality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.09507">WAVER: Writing-style Agnostic Video Retrieval via Distilling Vision-Language Models Through Open-Vocabulary Knowledge. (arXiv:2312.09507v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1">Huy Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Kieu_T/0/1/0/all/0/1">Tung Kieu</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1">Anh Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1">Ngan Le</a></p>
<p>Text-video retrieval, a prominent sub-field within the domain of multimodal
information retrieval, has witnessed remarkable growth in recent years.
However, existing methods assume video scenes are consistent with unbiased
descriptions. These limitations fail to align with real-world scenarios since
descriptions can be influenced by annotator biases, diverse writing styles, and
varying textual perspectives. To overcome the aforementioned problems, we
introduce WAVER, a cross-domain knowledge distillation framework via
vision-language models through open-vocabulary knowledge designed to tackle the
challenge of handling different writing styles in video descriptions. WAVER
capitalizes on the open-vocabulary properties that lie in pre-trained
vision-language models and employs an implicit knowledge distillation approach
to transfer text-based knowledge from a teacher model to a vision-based
student. Empirical studies conducted across four standard benchmark datasets,
encompassing various settings, provide compelling evidence that WAVER can
achieve state-of-the-art performance in text-video retrieval task while
handling writing-style variations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10835">Your Student is Better Than Expected: Adaptive Teacher-Student Collaboration for Text-Conditional Diffusion Models. (arXiv:2312.10835v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Starodubcev_N/0/1/0/all/0/1">Nikita Starodubcev</a>, <a href="http://arxiv.org/find/cs/1/au:+Fedorov_A/0/1/0/all/0/1">Artem Fedorov</a>, <a href="http://arxiv.org/find/cs/1/au:+Babenko_A/0/1/0/all/0/1">Artem Babenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Baranchuk_D/0/1/0/all/0/1">Dmitry Baranchuk</a></p>
<p>Knowledge distillation methods have recently shown to be a promising
direction to speedup the synthesis of large-scale diffusion models by requiring
only a few inference steps. While several powerful distillation methods were
recently proposed, the overall quality of student samples is typically lower
compared to the teacher ones, which hinders their practical usage. In this
work, we investigate the relative quality of samples produced by the teacher
text-to-image diffusion model and its distilled student version. As our main
empirical finding, we discover that a noticeable portion of student samples
exhibit superior fidelity compared to the teacher ones, despite the
``approximate'' nature of the student. Based on this finding, we propose an
adaptive collaboration between student and teacher diffusion models for
effective text-to-image synthesis. Specifically, the distilled model produces
the initial sample, and then an oracle decides whether it needs further
improvements with a slow teacher model. Extensive experiments demonstrate that
the designed pipeline surpasses state-of-the-art text-to-image alternatives for
various inference budgets in terms of human preference. Furthermore, the
proposed approach can be naturally used in popular applications such as
text-guided image editing and controllable generation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11285">Adv-Diffusion: Imperceptible Adversarial Face Identity Attack via Latent Diffusion Model. (arXiv:2312.11285v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1">Decheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xijun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1">Chunlei Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1">Nannan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1">Ruiming Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xinbo Gao</a></p>
<p>Adversarial attacks involve adding perturbations to the source image to cause
misclassification by the target model, which demonstrates the potential of
attacking face recognition models. Existing adversarial face image generation
methods still can't achieve satisfactory performance because of low
transferability and high detectability. In this paper, we propose a unified
framework Adv-Diffusion that can generate imperceptible adversarial identity
perturbations in the latent space but not the raw pixel space, which utilizes
strong inpainting capabilities of the latent diffusion model to generate
realistic adversarial images. Specifically, we propose the identity-sensitive
conditioned diffusion generative model to generate semantic perturbations in
the surroundings. The designed adaptive strength-based adversarial perturbation
algorithm can ensure both attack transferability and stealthiness. Extensive
qualitative and quantitative experiments on the public FFHQ and CelebA-HQ
datasets prove the proposed method achieves superior performance compared with
the state-of-the-art methods without an extra generative model training
process. The source code is available at
https://github.com/kopper-xdu/Adv-Diffusion.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13271">Repaint123: Fast and High-quality One Image to 3D Generation with Progressive Controllable 2D Repainting. (arXiv:2312.13271v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Junwu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1">Zhenyu Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1">Yatian Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1">Xinhua Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1">Peng Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1">Yida Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Ning_M/0/1/0/all/0/1">Munan Ning</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1">Li Yuan</a></p>
<p>Recent one image to 3D generation methods commonly adopt Score Distillation
Sampling (SDS). Despite the impressive results, there are multiple deficiencies
including multi-view inconsistency, over-saturated and over-smoothed textures,
as well as the slow generation speed. To address these deficiencies, we present
Repaint123 to alleviate multi-view bias as well as texture degradation and
speed up the generation process. The core idea is to combine the powerful image
generation capability of the 2D diffusion model and the texture alignment
ability of the repainting strategy for generating high-quality multi-view
images with consistency. We further propose visibility-aware adaptive
repainting strength for overlap regions to enhance the generated image quality
in the repainting process. The generated high-quality and multi-view consistent
images enable the use of simple Mean Square Error (MSE) loss for fast 3D
content generation. We conduct extensive experiments and show that our method
has a superior ability to generate high-quality 3D content with multi-view
consistency and fine textures in 2 minutes from scratch. Our project page is
available at https://pku-yuangroup.github.io/repaint123/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14149">TagAlign: Improving Vision-Language Alignment with Multi-Tag Classification. (arXiv:2312.14149v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qinying Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1">Kecheng Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1">Wei Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tong_Z/0/1/0/all/0/1">Zhan Tong</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zilei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yujun Shen</a></p>
<p>The crux of learning vision-language models is to extract semantically
aligned information from visual and linguistic data. Existing attempts usually
face the problem of coarse alignment, e.g., the vision encoder struggles in
localizing an attribute-specified object. In this work, we propose an
embarrassingly simple approach to better align image and text features with no
need of additional data formats other than image-text pairs. Concretely, given
an image and its paired text, we manage to parse objects (e.g., cat) and
attributes (e.g., black) from the description, which are highly likely to exist
in the image. It is noteworthy that the parsing pipeline is fully automatic and
thus enjoys good scalability. With these parsed semantics as supervision
signals, we can complement the commonly used image-text contrastive loss with
the multi-tag classification loss. Extensive experimental results on a broad
suite of semantic segmentation datasets substantiate the average 3.65\%
improvement of our framework over existing alternatives. Furthermore, the
visualization results indicate that attribute supervision makes vision-language
models accurately localize attribute-specified objects. Project page and code
can be found at https://qinying-liu.github.io/Tag-Align.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14206">LLM4VG: Large Language Models Evaluation for Video Grounding. (arXiv:2312.14206v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1">Wei Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zeyang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1">Zihan Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yuwei Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1">Wenwu Zhu</a></p>
<p>Recently, researchers have attempted to investigate the capability of LLMs in
handling videos and proposed several video LLM models. However, the ability of
LLMs to handle video grounding (VG), which is an important time-related video
task requiring the model to precisely locate the start and end timestamps of
temporal moments in videos that match the given textual queries, still remains
unclear and unexplored in literature. To fill the gap, in this paper, we
propose the LLM4VG benchmark, which systematically evaluates the performance of
different LLMs on video grounding tasks. Based on our proposed LLM4VG, we
design extensive experiments to examine two groups of video LLM models on video
grounding: (i) the video LLMs trained on the text-video pairs (denoted as
VidLLM), and (ii) the LLMs combined with pretrained visual description models
such as the video/image captioning model. We propose prompt methods to
integrate the instruction of VG and description from different kinds of
generators, including caption-based generators for direct visual description
and VQA-based generators for information enhancement. We also provide
comprehensive comparisons of various VidLLMs and explore the influence of
different choices of visual models, LLMs, prompt designs, etc, as well. Our
experimental evaluations lead to two conclusions: (i) the existing VidLLMs are
still far away from achieving satisfactory video grounding performance, and
more time-related video tasks should be included to further fine-tune these
models, and (ii) the combination of LLMs and visual models shows preliminary
abilities for video grounding with considerable potential for improvement by
resorting to more reliable models and further guidance of prompt instructions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14232">Parrot Captions Teach CLIP to Spot Text. (arXiv:2312.14232v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yiqi Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1">Conghui He</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1">Alex Jinpeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Weijia Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1">Mike Zheng Shou</a></p>
<p>Despite CLIP being the foundation model in numerous vision-language
applications, the CLIP suffers from a severe text spotting bias. Such bias
causes CLIP models to 'Parrot' the visual text embedded within images while
disregarding the authentic visual semantics. We uncover that in the most
popular image-text dataset LAION-2B, the captions also densely parrot (spell)
the text embedded in images. Our analysis shows that around 50% of images are
embedded with visual text content, and 90% of their captions more or less
parrot the visual text. Based on such observation, we thoroughly inspect the
different released versions of CLIP models and verify that the visual text is
the dominant factor in measuring the LAION-style image-text similarity for
these models. To examine whether these parrot captions shape the text spotting
bias, we train a series of CLIP models with LAION subsets curated by different
parrot-caption-oriented criteria. We show that training with parrot captions
easily shapes such bias but harms the expected visual-language representation
learning in CLIP models. This suggests that it is urgent to revisit either the
design of CLIP-like models or the existing image-text dataset curation pipeline
built on CLIP score filtering.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14238">InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks. (arXiv:2312.14238v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhe Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jiannan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenhai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1">Weijie Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1">Guo Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_S/0/1/0/all/0/1">Sen Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1">Muyan Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qinglong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xizhou Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1">Lewei Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1">Ping Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1">Tong Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1">Jifeng Dai</a></p>
<p>The exponential growth of large language models (LLMs) has opened up numerous
possibilities for multimodal AGI systems. However, the progress in vision and
vision-language foundation models, which are also critical elements of
multi-modal AGI, has not kept pace with LLMs. In this work, we design a
large-scale vision-language foundation model (InternVL), which scales up the
vision foundation model to 6 billion parameters and progressively aligns it
with the LLM, using web-scale image-text data from various sources. This model
can be broadly applied to and achieve state-of-the-art performance on 32
generic visual-linguistic benchmarks including visual perception tasks such as
image-level or pixel-level recognition, vision-language tasks such as zero-shot
image/video classification, zero-shot image/video-text retrieval, and link with
LLMs to create multi-modal dialogue systems. It has powerful visual
capabilities and can be a good alternative to the ViT-22B. We hope that our
research could contribute to the development of multi-modal large models. Code
and models are available at https://github.com/OpenGVLab/InternVL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14998">Synthetic images aid the recognition of human-made art forgeries. (arXiv:2312.14998v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ostmeyer_J/0/1/0/all/0/1">Johann Ostmeyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Schaerf_L/0/1/0/all/0/1">Ludovica Schaerf</a>, <a href="http://arxiv.org/find/cs/1/au:+Buividovich_P/0/1/0/all/0/1">Pavel Buividovich</a>, <a href="http://arxiv.org/find/cs/1/au:+Charles_T/0/1/0/all/0/1">Tessa Charles</a>, <a href="http://arxiv.org/find/cs/1/au:+Postma_E/0/1/0/all/0/1">Eric Postma</a>, <a href="http://arxiv.org/find/cs/1/au:+Popovici_C/0/1/0/all/0/1">Carina Popovici</a></p>
<p>Previous research has shown that Artificial Intelligence is capable of
distinguishing between authentic paintings by a given artist and human-made
forgeries with remarkable accuracy, provided sufficient training. However, with
the limited amount of existing known forgeries, augmentation methods for
forgery detection are highly desirable. In this work, we examine the potential
of incorporating synthetic artworks into training datasets to enhance the
performance of forgery detection. Our investigation focuses on paintings by
Vincent van Gogh, for which we release the first dataset specialized for
forgery detection. To reinforce our results, we conduct the same analyses on
the artists Amedeo Modigliani and Raphael. We train a classifier to distinguish
original artworks from forgeries. For this, we use human-made forgeries and
imitations in the style of well-known artists and augment our training sets
with images in a similar style generated by Stable Diffusion and StyleGAN. We
find that the additional synthetic forgeries consistently improve the detection
of human-made forgeries. In addition, we find that, in line with previous
research, the inclusion of synthetic forgeries in the training also enables the
detection of AI-generated forgeries, especially if created using a similar
generator.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15516">A-SDM: Accelerating Stable Diffusion through Redundancy Removal and Performance Optimization. (arXiv:2312.15516v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jinchao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuxuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_X/0/1/0/all/0/1">Xiaobing Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1">Siyuan Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_P/0/1/0/all/0/1">Pengfei Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1">Gao Huang</a></p>
<p>The Stable Diffusion Model (SDM) is a popular and efficient text-to-image
(t2i) generation and image-to-image (i2i) generation model. Although there have
been some attempts to reduce sampling steps, model distillation, and network
quantization, these previous methods generally retain the original network
architecture. Billion scale parameters and high computing requirements make the
research of model architecture adjustment scarce. In this work, we first
explore the computational redundancy part of the network, and then prune the
redundancy blocks of the model and maintain the network performance through a
progressive incubation strategy. Secondly, in order to maintaining the model
performance, we add cross-layer multi-expert conditional convolution
(CLME-Condconv) to the block pruning part to inherit the original convolution
parameters. Thirdly, we propose a global-regional interactive (GRI) attention
to speed up the computationally intensive attention part. Finally, we use
semantic-aware supervision (SAS) to align the outputs of the teacher model and
student model at the semantic level. Experiments show that this method can
effectively train a lightweight model close to the performance of the original
SD model, and effectively improve the model speed under limited resources.
Experiments show that the proposed method can effectively train a light-weight
model close to the performance of the original SD model, and effectively
improve the model speed under limited resources. After acceleration, the UNet
part of the model is 22% faster and the overall speed is 19% faster.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15707">High-Fidelity Diffusion-based Image Editing. (arXiv:2312.15707v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hou_C/0/1/0/all/0/1">Chen Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_G/0/1/0/all/0/1">Guoqiang Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhibo Chen</a></p>
<p>Diffusion models have attained remarkable success in the domains of image
generation and editing. It is widely recognized that employing larger inversion
and denoising steps in diffusion model leads to improved image reconstruction
quality. However, the editing performance of diffusion models tends to be no
more satisfactory even with increasing denoising steps. The deficiency in
editing could be attributed to the conditional Markovian property of the
editing process, where errors accumulate throughout denoising steps. To tackle
this challenge, we first propose an innovative framework where a rectifier
module is incorporated to modulate diffusion model weights with residual
features, thereby providing compensatory information to bridge the fidelity
gap. Furthermore, we introduce a novel learning paradigm aimed at minimizing
error propagation during the editing process, which trains the editing
procedure in a manner similar to denoising score-matching. Extensive
experiments demonstrate that our proposed framework and training strategy
achieve high-fidelity reconstruction and editing results across various levels
of denoising steps, meanwhile exhibits exceptional performance in terms of both
quantitative metric and qualitative assessments. Moreover, we explore our
model's generalization through several applications like image-to-image
translation and out-of-domain image editing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15731">Adaptive FSS: A Novel Few-Shot Segmentation Framework via Prototype Enhancement. (arXiv:2312.15731v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jing Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jinagyun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yisi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1">Haoran Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tianxiang Zhang</a></p>
<p>The Few-Shot Segmentation (FSS) aims to accomplish the novel class
segmentation task with a few annotated images. Current FSS research based on
meta-learning focus on designing a complex interaction mechanism between the
query and support feature. However, unlike humans who can rapidly learn new
things from limited samples, the existing approach relies solely on fixed
feature matching to tackle new tasks, lacking adaptability. In this paper, we
propose a novel framework based on the adapter mechanism, namely Adaptive FSS,
which can efficiently adapt the existing FSS model to the novel classes. In
detail, we design the Prototype Adaptive Module (PAM), which utilizes accurate
category information provided by the support set to derive class prototypes,
enhancing class-specific information in the multi-stage representation. In
addition, our approach is compatible with in diverse FSS methods with different
backbones by simply inserting PAM between the layers of the encoder.
Experiments demonstrate that our method effectively improves the performance of
the FSS models (e.g., MSANet, HDMNet, FPTrans, and DCAMA) and achieve new
state-of-the-art (SOTA) results (i.e., 72.4\% and 79.1\% mIoU on PASCAL-5$^i$
1-shot and 5-shot settings, 52.7\% and 60.0\% mIoU on COCO-20$^i$ 1-shot and
5-shot settings). Our code can be available at
https://github.com/jingw193/AdaptiveFSS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15840">Masked Contrastive Reconstruction for Cross-modal Medical Image-Report Retrieval. (arXiv:2312.15840v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1">Zeqiang Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_K/0/1/0/all/0/1">Kai Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xiuzhuang Zhou</a></p>
<p>Cross-modal medical image-report retrieval task plays a significant role in
clinical diagnosis and various medical generative tasks. Eliminating
heterogeneity between different modalities to enhance semantic consistency is
the key challenge of this task. The current Vision-Language Pretraining (VLP)
models, with cross-modal contrastive learning and masked reconstruction as
joint training tasks, can effectively enhance the performance of cross-modal
retrieval. This framework typically employs dual-stream inputs, using unmasked
data for cross-modal contrastive learning and masked data for reconstruction.
However, due to task competition and information interference caused by
significant differences between the inputs of the two proxy tasks, the
effectiveness of representation learning for intra-modal and cross-modal
features is limited. In this paper, we propose an efficient VLP framework named
Masked Contrastive and Reconstruction (MCR), which takes masked data as the
sole input for both tasks. This enhances task connections, reducing information
interference and competition between them, while also substantially decreasing
the required GPU memory and training time. Moreover, we introduce a new
modality alignment strategy named Mapping before Aggregation (MbA). Unlike
previous methods, MbA maps different modalities to a common feature space
before conducting local feature aggregation, thereby reducing the loss of
fine-grained semantic information necessary for improved modality alignment.
Qualitative and quantitative experiments conducted on the MIMIC-CXR dataset
validate the effectiveness of our approach, demonstrating state-of-the-art
performance in medical cross-modal retrieval tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15890">Towards Robust Multimodal Prompting With Missing Modalities. (arXiv:2312.15890v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1">Jaehyuk Jang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yooseung Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1">Changick Kim</a></p>
<p>Recently, multimodal prompting, which introduces learnable missing-aware
prompts for all missing modality cases, has exhibited impressive performance.
However, it encounters two critical issues: 1) The number of prompts grows
exponentially as the number of modalities increases; and 2) It lacks robustness
in scenarios with different missing modality settings between training and
inference. In this paper, we propose a simple yet effective prompt design to
address these challenges. Instead of using missing-aware prompts, we utilize
prompts as modality-specific tokens, enabling them to capture the unique
characteristics of each modality. Furthermore, our prompt design leverages
orthogonality between prompts as a key element to learn distinct information
across different modalities and promote diversity in the learned
representations. Extensive experiments demonstrate that our prompt design
enhances both performance and robustness while reducing the number of prompts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16151">Large-scale Long-tailed Disease Diagnosis on Radiology Images. (arXiv:2312.16151v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1">Qiaoyu Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Weike Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1">Chaoyi Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaoman Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Ya Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yanfeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1">Weidi Xie</a></p>
<p>In this study, we aim to investigate the problem of large-scale,
large-vocabulary disease classification for radiologic images, which can be
formulated as a multi-modal, multi-anatomy, multi-label, long-tailed
classification. Our main contributions are three folds: (i), on dataset
construction, we build up an academically accessible, large-scale diagnostic
dataset that encompasses 5568 disorders linked with 930 unique ICD-10-CM codes,
containing 39,026 cases (192,675 scans). (ii), on model design, we present a
novel architecture that enables to process arbitrary number of input scans,
from various imaging modalities, which is trained with knowledge enhancement to
leverage the rich domain knowledge; (iii), on evaluation, we initialize a new
benchmark for multi-modal multi-anatomy long-tailed diagnosis. Our method shows
superior results on it. Additionally, our final model serves as a pre-trained
model, and can be finetuned to benefit diagnosis on various external datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.03447">Using Set Covering to Generate Databases for Holistic Steganalysis. (arXiv:2211.03447v2 [cs.MM] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abecidan_R/0/1/0/all/0/1">Rony Abecidan</a> (CRIStAL, CNRS), <a href="http://arxiv.org/find/cs/1/au:+Itier_V/0/1/0/all/0/1">Vincent Itier</a> (CRIStAL, IMT Nord Europe, CNRS), <a href="http://arxiv.org/find/cs/1/au:+Boulanger_J/0/1/0/all/0/1">J&#xe9;r&#xe9;mie Boulanger</a> (CRIStAL, CNRS), <a href="http://arxiv.org/find/cs/1/au:+Bas_P/0/1/0/all/0/1">Patrick Bas</a> (CRIStAL, CNRS), <a href="http://arxiv.org/find/cs/1/au:+Pevny_T/0/1/0/all/0/1">Tom&#xe1;&#x161; Pevn&#xfd;</a> (CTU)</p>
<p>Within an operational framework, covers used by a steganographer are likely
to come from different sensors and different processing pipelines than the ones
used by researchers for training their steganalysis models. Thus, a performance
gap is unavoidable when it comes to out-of-distributions covers, an extremely
frequent scenario called Cover Source Mismatch (CSM). Here, we explore a grid
of processing pipelines to study the origins of CSM, to better understand it,
and to better tackle it. A set-covering greedy algorithm is used to select
representative pipelines minimizing the maximum regret between the
representative and the pipelines within the set. Our main contribution is a
methodology for generating relevant bases able to tackle operational CSM.
Experimental validation highlights that, for a given number of training
samples, our set covering selection is a better strategy than selecting random
pipelines or using all the available pipelines. Our analysis also shows that
parameters as denoising, sharpening, and downsampling are very important to
foster diversity. Finally, different benchmarks for classical and wild
databases show the good generalization property of the extracted databases.
Additional resources are available at
github.com/RonyAbecidan/HolisticSteganalysisWithSetCovering.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10482">A new method color MS-BSIF Features learning for the robust kinship verification. (arXiv:2312.10482v1 [cs.CV] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aliradi_R/0/1/0/all/0/1">Rachid Aliradi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ouamane_A/0/1/0/all/0/1">Abdealmalik Ouamane</a>, <a href="http://arxiv.org/find/cs/1/au:+Amrane_A/0/1/0/all/0/1">Abdeslam Amrane</a></p>
<p>the paper presents a new method color MS-BSIF learning and MS-LBP for the
kinship verification is the machine's ability to identify the genetic and blood
the relationship and its degree between the facial images of humans. Facial
verification of kinship refers to the task of training a machine to recognize
the blood relationship between a pair of faces parent and non-parent
(verification) based on features extracted from facial images, and determining
the exact type or degree of this genetic relationship. We use the LBP and color
BSIF learning features for the comparison and the TXQDA method for
dimensionality reduction and data classification. We let's test the kinship
facial verification application is namely the kinface Cornell database. This
system improves the robustness of learning while controlling efficiency. The
experimental results obtained and compared to other methods have proven the
reliability of our framework and surpass the performance of other
state-of-the-art techniques.
</p>
</p>
</div>

    </div>
    </body>
    