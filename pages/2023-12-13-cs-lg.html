<!DOCTYPE html>
<html>
<head>
<title>2023-12-13-cs-lg</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2312.05258">Automated Small Kidney Cancer Detection in Non-Contrast Computed Tomography. (arXiv:2312.05258v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+McGough_W/0/1/0/all/0/1">William McGough</a>, <a href="http://arxiv.org/find/eess/1/au:+Buddenkotte_T/0/1/0/all/0/1">Thomas Buddenkotte</a>, <a href="http://arxiv.org/find/eess/1/au:+Ursprung_S/0/1/0/all/0/1">Stephan Ursprung</a>, <a href="http://arxiv.org/find/eess/1/au:+Gao_Z/0/1/0/all/0/1">Zeyu Gao</a>, <a href="http://arxiv.org/find/eess/1/au:+Stewart_G/0/1/0/all/0/1">Grant Stewart</a>, <a href="http://arxiv.org/find/eess/1/au:+Crispin_Ortuzar_M/0/1/0/all/0/1">Mireia Crispin-Ortuzar</a></p>
<p>This study introduces an automated pipeline for renal cancer (RC) detection
in non-contrast computed tomography (NCCT). In the development of our pipeline,
we test three detections models: a shape model, a 2D-, and a 3D axial-sample
model. Training (n=1348) and testing (n=64) data were gathered from open
sources (KiTS23, Abdomen1k, CT-ORG) and Cambridge University Hospital (CUH).
Results from cross-validation and testing revealed that the 2D axial sample
model had the highest small ($\leq$40mm diameter) RC detection area under the
curve (AUC) of 0.804. Our pipeline achieves 61.9\% sensitivity and 92.7\%
specificity for small kidney cancers on unseen test data. Our results are much
more accurate than previous attempts to automatically detect small renal
cancers in NCCT, the most likely imaging modality for RC screening. This
pipeline offers a promising advance that may enable screening for kidney
cancers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05262">Model Copyright Protection in Buyer-seller Environment. (arXiv:2312.05262v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yusheng Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_N/0/1/0/all/0/1">Nan Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1">Zhenxing Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xinpeng Zhang</a></p>
<p>Training a deep neural network (DNN) requires a high computational cost.
Buying models from sellers with a large number of computing resources has
become prevailing. However, the buyer-seller environment is not always trusted.
To protect the neural network models from leaking in an untrusted environment,
we propose a novel copyright protection scheme for DNN using an input-sensitive
neural network (ISNN). The main idea of ISNN is to make a DNN sensitive to the
key and copyright information. Therefore, only the buyer with a correct key can
utilize the ISNN. During the training phase, we add a specific perturbation to
the clean images and mark them as legal inputs, while the other inputs are
treated as illegal input. We design a loss function to make the outputs of
legal inputs close to the true ones, while the illegal inputs are far away from
true results. Experimental results demonstrate that the proposed scheme is
effective, valid, and secure.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05264">All Rivers Run to the Sea: Private Learning with Asymmetric Flows. (arXiv:2312.05264v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1">Yue Niu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ali_R/0/1/0/all/0/1">Ramy E. Ali</a>, <a href="http://arxiv.org/find/cs/1/au:+Prakash_S/0/1/0/all/0/1">Saurav Prakash</a>, <a href="http://arxiv.org/find/cs/1/au:+Avestimehr_S/0/1/0/all/0/1">Salman Avestimehr</a></p>
<p>Data privacy is of great concern in cloud machine-learning service platforms,
when sensitive data are exposed to service providers. While private computing
environments (e.g., secure enclaves), and cryptographic approaches (e.g.,
homomorphic encryption) provide strong privacy protection, their computing
performance still falls short compared to cloud GPUs. To achieve privacy
protection with high computing performance, we propose Delta, a new private
training and inference framework, with comparable model performance as
non-private centralized training. Delta features two asymmetric data flows: the
main information-sensitive flow and the residual flow. The main part flows into
a small model while the residuals are offloaded to a large model. Specifically,
Delta embeds the information-sensitive representations into a low-dimensional
space while pushing the information-insensitive part into high-dimension
residuals. To ensure privacy protection, the low-dimensional
information-sensitive part is secured and fed to a small model in a private
environment. On the other hand, the residual part is sent to fast cloud GPUs,
and processed by a large model. To further enhance privacy and reduce the
communication cost, Delta applies a random binary quantization technique along
with a DP-based technique to the residuals before sharing them with the public
platform. We theoretically show that Delta guarantees differential privacy in
the public environment and greatly reduces the complexity in the private
environment. We conduct empirical analyses on CIFAR-10, CIFAR-100 and ImageNet
datasets and ResNet-18 and ResNet-34, showing that Delta achieves strong
privacy protection, fast training, and inference without significantly
compromising the model utility.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05265">Multimodal Group Emotion Recognition In-the-wild Using Privacy-Compliant Features. (arXiv:2312.05265v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Augusma_A/0/1/0/all/0/1">Anderson Augusma</a> (M-PSI, SVH), <a href="http://arxiv.org/find/cs/1/au:+Vaufreydaz_D/0/1/0/all/0/1">Dominique Vaufreydaz</a> (M-PSI), <a href="http://arxiv.org/find/cs/1/au:+Letue_F/0/1/0/all/0/1">Fr&#xe9;d&#xe9;rique Letu&#xe9;</a> (SVH)</p>
<p>This paper explores privacy-compliant group-level emotion recognition
''in-the-wild'' within the EmotiW Challenge 2023. Group-level emotion
recognition can be useful in many fields including social robotics,
conversational agents, e-coaching and learning analytics. This research imposes
itself using only global features avoiding individual ones, i.e. all features
that can be used to identify or track people in videos (facial landmarks, body
poses, audio diarization, etc.). The proposed multimodal model is composed of a
video and an audio branches with a cross-attention between modalities. The
video branch is based on a fine-tuned ViT architecture. The audio branch
extracts Mel-spectrograms and feed them through CNN blocks into a transformer
encoder. Our training paradigm includes a generated synthetic dataset to
increase the sensitivity of our model on facial expression within the image in
a data-driven way. The extensive experiments show the significance of our
methodology. Our privacy-compliant proposal performs fairly on the EmotiW
challenge, with 79.24% and 75.13% of accuracy respectively on validation and
test set for the best models. Noticeably, our findings highlight that it is
possible to reach this accuracy level with privacy-compliant features using
only 5 frames uniformly distributed on the video.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05269">LifelongMemory: Leveraging LLMs for Answering Queries in Egocentric Videos. (arXiv:2312.05269v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Ying Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yanlai Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1">Mengye Ren</a></p>
<p>The egocentric video natural language query (NLQ) task involves localizing a
temporal window in an egocentric video that provides an answer to a posed
query, which has wide applications in building personalized AI assistants.
Prior methods for this task have focused on improvements of network
architecture and leveraging pre-training for enhanced image and video features,
but have struggled with capturing long-range temporal dependencies in lengthy
videos, and cumbersome end-to-end training. Motivated by recent advancements in
Large Language Models (LLMs) and vision language models, we introduce
LifelongMemory, a novel framework that utilizes multiple pre-trained models to
answer queries from extensive egocentric video content. We address the unique
challenge by employing a pre-trained captioning model to create detailed
narratives of the videos. These narratives are then used to prompt a frozen LLM
to generate coarse-grained temporal window predictions, which are subsequently
refined using a pre-trained NLQ model. Empirical results demonstrate that our
method achieves competitive performance against existing supervised end-to-end
learning methods, underlining the potential of integrating multiple pre-trained
multimodal large language models in complex vision-language tasks. We provide a
comprehensive analysis of key design decisions and hyperparameters in our
pipeline, offering insights and practical guidelines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05274">Target to Source: Guidance-Based Diffusion Model for Test-Time Adaptation. (arXiv:2312.05274v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1">Kaiyu Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Lai_H/0/1/0/all/0/1">Hanjiang Lai</a></p>
<p>Most recent works of test-time adaptation (TTA) aim to alleviate domain shift
problems by re-training source classifiers in each domain. On the other hand,
the emergence of the diffusion model provides another solution to TTA, which
directly maps the test data from the target domain to the source domain based
on a diffusion model pre-trained in the source domain. The source classifier
does not need to be fine-tuned. However, 1) the semantic information loss from
test data to the source domain and 2) the model shift between the source
classifier and diffusion model would prevent the diffusion model from mapping
the test data back to the source domain correctly. In this paper, we propose a
novel guidance-based diffusion-driven adaptation (GDDA) to overcome the data
shift and let the diffusion model find a better way to go back to the source.
Concretely, we first propose detail and global guidance to better keep the
common semantics of the test and source data. The two guidance include a
contrastive loss and mean squared error to alleviate the information loss by
fully exploring the diffusion model and the test data. Meanwhile, we propose a
classifier-aware guidance to reduce the bias caused by the model shift, which
can incorporate the source classifier's information into the generation process
of the diffusion model. Extensive experiments on three image datasets with
three classifier backbones demonstrate that GDDA significantly performs better
than the state-of-the-art baselines. On CIFAR-10C, CIFAR-100C, and ImageNetC,
GDDA achieves 11.54\%, 19.05\%, and 11.63\% average accuracy improvements,
respectively. GDDA even achieves equal performance compared with methods of
re-training classifiers. The code is available in the supplementary material.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05276">Making Large Language Models Better Knowledge Miners for Online Marketing with Progressive Prompting Augmentation. (arXiv:2312.05276v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1">Chunjing Gan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1">Dan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1">Binbin Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Ziqi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yue Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhiqiang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jinjie Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jun Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Guannan Zhang</a></p>
<p>Nowadays, the rapid development of mobile economy has promoted the
flourishing of online marketing campaigns, whose success greatly hinges on the
efficient matching between user preferences and desired marketing campaigns
where a well-established Marketing-oriented Knowledge Graph (dubbed as MoKG)
could serve as the critical "bridge" for preference propagation. In this paper,
we seek to carefully prompt a Large Language Model (LLM) with domain-level
knowledge as a better marketing-oriented knowledge miner for marketing-oriented
knowledge graph construction, which is however non-trivial, suffering from
several inevitable issues in real-world marketing scenarios, i.e.,
uncontrollable relation generation of LLMs,insufficient prompting ability of a
single prompt, the unaffordable deployment cost of LLMs. To this end, we
propose PAIR, a novel Progressive prompting Augmented mIning fRamework for
harvesting marketing-oriented knowledge graph with LLMs. In particular, we
reduce the pure relation generation to an LLM based adaptive relation filtering
process through the knowledge-empowered prompting technique. Next, we steer
LLMs for entity expansion with progressive prompting augmentation,followed by a
reliable aggregation with comprehensive consideration of both self-consistency
and semantic relatedness. In terms of online serving, we specialize in a small
and white-box PAIR (i.e.,LightPAIR),which is fine-tuned with a high-quality
corpus provided by a strong teacher-LLM. Extensive experiments and practical
applications in audience targeting verify the effectiveness of the proposed
(Light)PAIR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05277">3D Copy-Paste: Physically Plausible Object Insertion for Monocular 3D Detection. (arXiv:2312.05277v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1">Yunhao Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Hong-Xing Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1">Cheng Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yuliang Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xinyu Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1">Liu Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Itti_L/0/1/0/all/0/1">Laurent Itti</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jiajun Wu</a></p>
<p>A major challenge in monocular 3D object detection is the limited diversity
and quantity of objects in real datasets. While augmenting real scenes with
virtual objects holds promise to improve both the diversity and quantity of the
objects, it remains elusive due to the lack of an effective 3D object insertion
method in complex real captured scenes. In this work, we study augmenting
complex real indoor scenes with virtual objects for monocular 3D object
detection. The main challenge is to automatically identify plausible physical
properties for virtual assets (e.g., locations, appearances, sizes, etc.) in
cluttered real scenes. To address this challenge, we propose a physically
plausible indoor 3D object insertion approach to automatically copy virtual
objects and paste them into real scenes. The resulting objects in scenes have
3D bounding boxes with plausible physical locations and appearances. In
particular, our method first identifies physically feasible locations and poses
for the inserted objects to prevent collisions with the existing room layout.
Subsequently, it estimates spatially-varying illumination for the insertion
location, enabling the immersive blending of the virtual objects into the
original scene with plausible appearances and cast shadows. We show that our
augmentation method significantly improves existing monocular 3D object models
and achieves state-of-the-art performance. For the first time, we demonstrate
that a physically plausible 3D object insertion, serving as a generative data
augmentation technique, can lead to significant improvements for discriminative
downstream tasks such as monocular 3D object detection. Project website:
https://gyhandy.github.io/3D-Copy-Paste/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05282">Towards On-device Learning on the Edge: Ways to Select Neurons to Update under a Budget Constraint. (arXiv:2312.05282v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Quelennec_A/0/1/0/all/0/1">A&#xeb;l Qu&#xe9;lennec</a>, <a href="http://arxiv.org/find/cs/1/au:+Tartaglione_E/0/1/0/all/0/1">Enzo Tartaglione</a>, <a href="http://arxiv.org/find/cs/1/au:+Mozharovskyi_P/0/1/0/all/0/1">Pavlo Mozharovskyi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1">Van-Tam Nguyen</a></p>
<p>In the realm of efficient on-device learning under extreme memory and
computation constraints, a significant gap in successful approaches persists.
Although considerable effort has been devoted to efficient inference, the main
obstacle to efficient learning is the prohibitive cost of backpropagation. The
resources required to compute gradients and update network parameters often
exceed the limits of tightly constrained memory budgets. This paper challenges
conventional wisdom and proposes a series of experiments that reveal the
existence of superior sub-networks. Furthermore, we hint at the potential for
substantial gains through a dynamic neuron selection strategy when fine-tuning
a target task. Our efforts extend to the adaptation of a recent dynamic neuron
selection strategy pioneered by Bragagnolo et al. (NEq), revealing its
effectiveness in the most stringent scenarios. Our experiments demonstrate, in
the average case, the superiority of a NEq-inspired approach over a random
selection. This observation prompts a compelling avenue for further exploration
in the area, highlighting the opportunity to design a new class of algorithms
designed to facilitate parameter update selection. Our findings usher in a new
era of possibilities in the field of on-device learning under extreme
constraints and encourage the pursuit of innovative strategies for efficient,
resource-friendly model fine-tuning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05296">AI Competitions and Benchmarks: The life cycle of challenges and benchmarks. (arXiv:2312.05296v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stolovitzky_G/0/1/0/all/0/1">Gustavo Stolovitzky</a>, <a href="http://arxiv.org/find/cs/1/au:+Saez_Rodriguez_J/0/1/0/all/0/1">Julio Saez-Rodriguez</a>, <a href="http://arxiv.org/find/cs/1/au:+Bletz_J/0/1/0/all/0/1">Julie Bletz</a>, <a href="http://arxiv.org/find/cs/1/au:+Albrecht_J/0/1/0/all/0/1">Jacob Albrecht</a>, <a href="http://arxiv.org/find/cs/1/au:+Andreoletti_G/0/1/0/all/0/1">Gaia Andreoletti</a>, <a href="http://arxiv.org/find/cs/1/au:+Costello_J/0/1/0/all/0/1">James C. Costello</a>, <a href="http://arxiv.org/find/cs/1/au:+Boutros_P/0/1/0/all/0/1">Paul Boutros</a></p>
<p>Data Science research is undergoing a revolution fueled by the transformative
power of technology, the Internet, and an ever increasing computational
capacity. The rate at which sophisticated algorithms can be developed is
unprecedented, yet they remain outpaced by the massive amounts of data that are
increasingly available to researchers. Here we argue for the need to creatively
leverage the scientific research and algorithm development community as an axis
of robust innovation. Engaging these communities in the scientific discovery
enterprise by critical assessments, community experiments, and/or crowdsourcing
will multiply opportunities to develop new data driven, reproducible and well
benchmarked algorithmic solutions to fundamental and applied problems of
current interest. Coordinated community engagement in the analysis of highly
complex and massive data has emerged as one approach to find robust
methodologies that best address these challenges. When community engagement is
done in the form of competitions, also known as challenges, the validation of
the analytical methodology is inherently addressed, establishing performance
benchmarks. Finally, challenges foster open innovation across multiple
disciplines to create communities that collaborate directly or indirectly to
address significant scientific gaps. Together, participants can solve important
problems as varied as health research, climate change, and social equity.
Ultimately, challenges can catalyze and accelerate the synthesis of complex
data into knowledge or actionable information, and should be viewed a powerful
tool to make lasting social and research contributions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05299">Learning to be Simple. (arXiv:2312.05299v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yang-Hui He</a>, <a href="http://arxiv.org/find/cs/1/au:+Jejjala_V/0/1/0/all/0/1">Vishnu Jejjala</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishra_C/0/1/0/all/0/1">Challenger Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharnoff_M/0/1/0/all/0/1">Max Sharnoff</a></p>
<p>In this work we employ machine learning to understand structured mathematical
data involving finite groups and derive a theorem about necessary properties of
generators of finite simple groups. We create a database of all 2-generated
subgroups of the symmetric group on n-objects and conduct a classification of
finite simple groups among them using shallow feed-forward neural networks. We
show that this neural network classifier can decipher the property of
simplicity with varying accuracies depending on the features. Our neural
network model leads to a natural conjecture concerning the generators of a
finite simple group. We subsequently prove this conjecture. This new toy
theorem comments on the necessary properties of generators of finite simple
groups. We show this explicitly for a class of sporadic groups for which the
result holds. Our work further makes the case for a machine motivated study of
algebraic structures in pure mathematics and highlights the possibility of
generating new conjectures and theorems in mathematics with the aid of machine
learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05320">Uncertainty-aware Surrogate Models for Airfoil Flow Simulations with Denoising Diffusion Probabilistic Models. (arXiv:2312.05320v1 [physics.flu-dyn])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Liu_Q/0/1/0/all/0/1">Qiang Liu</a>, <a href="http://arxiv.org/find/physics/1/au:+Thuerey_N/0/1/0/all/0/1">Nils Thuerey</a></p>
<p>Leveraging neural networks as surrogate models for turbulence simulation is a
topic of growing interest. At the same time, embodying the inherent uncertainty
of simulations in the predictions of surrogate models remains very challenging.
The present study makes a first attempt to use denoising diffusion
probabilistic models (DDPMs) to train an uncertainty-aware surrogate model for
turbulence simulations. Due to its prevalence, the simulation of flows around
airfoils with various shapes, Reynolds numbers, and angles of attack is chosen
as the learning objective. Our results show that DDPMs can successfully capture
the whole distribution of solutions and, as a consequence, accurately estimate
the uncertainty of the simulations. The performance of DDPMs is also compared
with varying baselines in the form of Bayesian neural networks and
heteroscedastic models. Experiments demonstrate that DDPMs outperform the other
methods regarding a variety of accuracy metrics. Besides, it offers the
advantage of providing access to the complete distributions of uncertainties
rather than providing a set of parameters. As such, it can yield realistic and
detailed samples from the distribution of solutions. All source codes and
datasets utilized in this study are publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05325">Analyzing Behaviors of Mixed Traffic via Reinforcement Learning at Unsignalized Intersections. (arXiv:2312.05325v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sarker_S/0/1/0/all/0/1">Supriya Sarker</a></p>
<p>In this report, we delve into two critical research inquiries. Firstly, we
explore the extent to which Reinforcement Learning (RL) agents exhibit
multimodal distributions in the context of stop-and-go traffic scenarios.
Secondly, we investigate how RL-controlled Robot Vehicles (RVs) effectively
navigate their direction and coordinate with other vehicles in complex traffic
environments. Our analysis encompasses an examination of multimodality within
queue length, outflow, and platoon size distributions for both Robot and
Human-driven Vehicles (HVs). Additionally, we assess the Pearson coefficient
correlation, shedding light on relationships between queue length and outflow,
considering both identical and differing travel directions. Furthermore, we
delve into causal inference models, shedding light on the factors influencing
queue length across scenarios involving varying travel directions. Through
these investigations, this report contributes valuable insights into the
behaviors of mixed traffic (RVs and HVs) in traffic management and
coordination.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05327">Data-Centric Machine Learning for Geospatial Remote Sensing Data. (arXiv:2312.05327v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Roscher_R/0/1/0/all/0/1">Ribana Roscher</a>, <a href="http://arxiv.org/find/cs/1/au:+Russwurm_M/0/1/0/all/0/1">Marc Ru&#xdf;wurm</a>, <a href="http://arxiv.org/find/cs/1/au:+Gevaert_C/0/1/0/all/0/1">Caroline Gevaert</a>, <a href="http://arxiv.org/find/cs/1/au:+Kampffmeyer_M/0/1/0/all/0/1">Michael Kampffmeyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1">Jefersson A. dos Santos</a>, <a href="http://arxiv.org/find/cs/1/au:+Vakalopoulou_M/0/1/0/all/0/1">Maria Vakalopoulou</a>, <a href="http://arxiv.org/find/cs/1/au:+Hansch_R/0/1/0/all/0/1">Ronny H&#xe4;nsch</a>, <a href="http://arxiv.org/find/cs/1/au:+Hansen_S/0/1/0/all/0/1">Stine Hansen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nogueira_K/0/1/0/all/0/1">Keiller Nogueira</a>, <a href="http://arxiv.org/find/cs/1/au:+Prexl_J/0/1/0/all/0/1">Jonathan Prexl</a>, <a href="http://arxiv.org/find/cs/1/au:+Tuia_D/0/1/0/all/0/1">Devis Tuia</a></p>
<p>Recent developments and research in modern machine learning have led to
substantial improvements in the geospatial field. Although numerous deep
learning models have been proposed, the majority of them have been developed on
benchmark datasets that lack strong real-world relevance. Furthermore, the
performance of many methods has already saturated on these datasets. We argue
that shifting the focus towards a complementary data-centric perspective is
necessary to achieve further improvements in accuracy, generalization ability,
and real impact in end-user applications. This work presents a definition and
precise categorization of automated data-centric learning approaches for
geospatial data. It highlights the complementary role of data-centric learning
with respect to model-centric in the larger machine learning deployment cycle.
We review papers across the entire geospatial field and categorize them into
different groups. A set of representative experiments shows concrete
implementation examples. These examples provide concrete steps to act on
geospatial data with data-centric machine learning approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05332">Bridging the Gaps: Learning Verifiable Model-Free Quadratic Programming Controllers Inspired by Model Predictive Control. (arXiv:2312.05332v1 [eess.SY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Lu_Y/0/1/0/all/0/1">Yiwen Lu</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1">Zishuo Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1">Yihan Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_N/0/1/0/all/0/1">Na Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Mo_Y/0/1/0/all/0/1">Yilin Mo</a></p>
<p>In this paper, we introduce a new class of parameterized controllers, drawing
inspiration from Model Predictive Control (MPC). These controllers adopt a
Quadratic Programming (QP) structure similar to linear MPC, with problem
parameters being learned rather than derived from models. This approach may
address the limitations of commonly learned controllers with Multi-Layer
Perceptron (MLP) architecture in deep reinforcement learning, in terms of
explainability and performance guarantees.
</p>
<p>The learned controllers not only possess verifiable properties like
persistent feasibility and asymptotic stability akin to MPC, but they also
empirically match MPC and MLP controllers in control performance. Moreover,
they are more computationally efficient in implementation compared to MPC and
require significantly fewer learnable policy parameters than MLP controllers.
</p>
<p>Practical application is demonstrated through a vehicle drift maneuvering
task, showcasing the potential of these controllers in real-world scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05333">A Data-Driven Framework for Improving Public EV Charging Infrastructure: Modeling and Forecasting. (arXiv:2312.05333v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Al_Dahabreh_N/0/1/0/all/0/1">Nassr Al-Dahabreh</a>, <a href="http://arxiv.org/find/cs/1/au:+Sayed_M/0/1/0/all/0/1">Mohammad Ali Sayed</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarieddine_K/0/1/0/all/0/1">Khaled Sarieddine</a>, <a href="http://arxiv.org/find/cs/1/au:+Elhattab_M/0/1/0/all/0/1">Mohamed Elhattab</a>, <a href="http://arxiv.org/find/cs/1/au:+Khabbaz_M/0/1/0/all/0/1">Maurice Khabbaz</a>, <a href="http://arxiv.org/find/cs/1/au:+Atallah_R/0/1/0/all/0/1">Ribal Atallah</a>, <a href="http://arxiv.org/find/cs/1/au:+Assi_C/0/1/0/all/0/1">Chadi Assi</a></p>
<p>This work presents an investigation and assessment framework, which,
supported by realistic data, aims at provisioning operators with in-depth
insights into the consumer-perceived Quality-of-Experience (QoE) at public
Electric Vehicle (EV) charging infrastructures. Motivated by the unprecedented
EV market growth, it is suspected that the existing charging infrastructure
will soon be no longer capable of sustaining the rapidly growing charging
demands; let alone that the currently adopted ad hoc infrastructure expansion
strategies seem to be far from contributing any quality service sustainability
solutions that tangibly reduce (ultimately mitigate) the severity of this
problem. Without suitable QoE metrics, operators, today, face remarkable
difficulty in assessing the performance of EV Charging Stations (EVCSs) in this
regard. This paper aims at filling this gap through the formulation of novel
and original critical QoE performance metrics that provide operators with
visibility into the per-EVCS operational dynamics and allow for the
optimization of these stations' respective utilization. Such metrics shall then
be used as inputs to a Machine Learning model finely tailored and trained using
recent real-world data sets for the purpose of forecasting future long-term
EVCS loads. This will, in turn, allow for making informed optimal EV charging
infrastructure expansions that will be capable of reliably coping with the
rising EV charging demands and maintaining acceptable QoE levels. The model's
accuracy has been tested and extensive simulations are conducted to evaluate
the achieved performance in terms of the above listed metrics and show the
suitability of the recommended infrastructure expansions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05337">Artificial Neural Nets and the Representation of Human Concepts. (arXiv:2312.05337v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Freiesleben_T/0/1/0/all/0/1">Timo Freiesleben</a></p>
<p>What do artificial neural networks (ANNs) learn? The machine learning (ML)
community shares the narrative that ANNs must develop abstract human concepts
to perform complex tasks. Some go even further and believe that these concepts
are stored in individual units of the network. Based on current research, I
systematically investigate the assumptions underlying this narrative. I
conclude that ANNs are indeed capable of performing complex prediction tasks,
and that they may learn human and non-human concepts to do so. However,
evidence indicates that ANNs do not represent these concepts in individual
units.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05340">Transition Path Sampling with Boltzmann Generator-based MCMC Moves. (arXiv:2312.05340v1 [q-bio.QM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Plainer_M/0/1/0/all/0/1">Michael Plainer</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Stark_H/0/1/0/all/0/1">Hannes St&#xe4;rk</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Bunne_C/0/1/0/all/0/1">Charlotte Bunne</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Gunnemann_S/0/1/0/all/0/1">Stephan G&#xfc;nnemann</a></p>
<p>Sampling all possible transition paths between two 3D states of a molecular
system has various applications ranging from catalyst design to drug discovery.
Current approaches to sample transition paths use Markov chain Monte Carlo and
rely on time-intensive molecular dynamics simulations to find new paths. Our
approach operates in the latent space of a normalizing flow that maps from the
molecule's Boltzmann distribution to a Gaussian, where we propose new paths
without requiring molecular simulations. Using alanine dipeptide, we explore
Metropolis-Hastings acceptance criteria in the latent space for exact sampling
and investigate different latent proposal mechanisms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05352">A Review of Machine Learning Methods Applied to Video Analysis Systems. (arXiv:2312.05352v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pattichis_M/0/1/0/all/0/1">Marios S. Pattichis</a>, <a href="http://arxiv.org/find/cs/1/au:+Jatla_V/0/1/0/all/0/1">Venkatesh Jatla</a>, <a href="http://arxiv.org/find/cs/1/au:+Cerna_A/0/1/0/all/0/1">Alvaro E. Ullao Cerna</a></p>
<p>The paper provides a survey of the development of machine-learning techniques
for video analysis. The survey provides a summary of the most popular deep
learning methods used for human activity recognition. We discuss how popular
architectures perform on standard datasets and highlight the differences from
real-life datasets dominated by multiple activities performed by multiple
participants over long periods. For real-life datasets, we describe the use of
low-parameter models (with 200X or 1,000X fewer parameters) that are trained to
detect a single activity after the relevant objects have been successfully
detected. Our survey then turns to a summary of machine learning methods that
are specifically developed for working with a small number of labeled video
samples. Our goal here is to describe modern techniques that are specifically
designed so as to minimize the amount of ground truth that is needed for
training and testing video analysis systems. We provide summaries of the
development of self-supervised learning, semi-supervised learning, active
learning, and zero-shot learning for applications in video analysis. For each
method, we provide representative examples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05355">Neither hype nor gloom do DNNs justice. (arXiv:2312.05355v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wichmann_F/0/1/0/all/0/1">Felix A. Wichmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Kornblith_S/0/1/0/all/0/1">Simon Kornblith</a>, <a href="http://arxiv.org/find/cs/1/au:+Geirhos_R/0/1/0/all/0/1">Robert Geirhos</a></p>
<p>Neither the hype exemplified in some exaggerated claims about deep neural
networks (DNNs), nor the gloom expressed by Bowers et al. do DNNs as models in
vision science justice: DNNs rapidly evolve, and today's limitations are often
tomorrow's successes. In addition, providing explanations as well as prediction
and image-computability are model desiderata; one should not be favoured at the
expense of the other.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05356">Neuron Patching: Neuron-level Model Editing on Code Generation and LLMs. (arXiv:2312.05356v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jian Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chunyang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Aleti_A/0/1/0/all/0/1">Aldeida Aleti</a></p>
<p>Large Language Models are successfully adopted in software engineering,
especially in code generation. Updating these models with new knowledge is very
expensive, and is often required to fully realize their value. In this paper,
we propose a novel and effective model editing approach, \textsc{MENT}, to
patch LLMs in coding tasks. Based on the mechanism of generative LLMs,
\textsc{MENT} enables model editing in next-token predictions, and further
supports common coding tasks. \textsc{MENT} is effective, efficient, and
reliable. It can correct a neural model by patching 1 or 2 neurons. As the
pioneer work on neuron-level model editing of generative models, we formalize
the editing process and introduce the involved concepts. Besides, we also
introduce new measures to evaluate its generalization ability, and build a
benchmark for further study. Our approach is evaluated on three coding tasks,
including API-seq recommendation, line-level code generation, and
pseudocode-to-code transaction. It outperforms the state-of-the-art by a
significant margin on both effectiveness and efficiency measures. In addition,
we demonstrate the usages of \textsc{MENT} for LLM reasoning in software
engineering. By editing the LLM knowledge with \textsc{MENT}, the directly or
indirectly dependent behaviors in the chain-of-thought change accordingly and
automatically.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05359">Learning 3D Particle-based Simulators from RGB-D Videos. (arXiv:2312.05359v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Whitney_W/0/1/0/all/0/1">William F. Whitney</a>, <a href="http://arxiv.org/find/cs/1/au:+Lopez_Guevara_T/0/1/0/all/0/1">Tatiana Lopez-Guevara</a>, <a href="http://arxiv.org/find/cs/1/au:+Pfaff_T/0/1/0/all/0/1">Tobias Pfaff</a>, <a href="http://arxiv.org/find/cs/1/au:+Rubanova_Y/0/1/0/all/0/1">Yulia Rubanova</a>, <a href="http://arxiv.org/find/cs/1/au:+Kipf_T/0/1/0/all/0/1">Thomas Kipf</a>, <a href="http://arxiv.org/find/cs/1/au:+Stachenfeld_K/0/1/0/all/0/1">Kimberly Stachenfeld</a>, <a href="http://arxiv.org/find/cs/1/au:+Allen_K/0/1/0/all/0/1">Kelsey R. Allen</a></p>
<p>Realistic simulation is critical for applications ranging from robotics to
animation. Traditional analytic simulators sometimes struggle to capture
sufficiently realistic simulation which can lead to problems including the well
known "sim-to-real" gap in robotics. Learned simulators have emerged as an
alternative for better capturing real-world physical dynamics, but require
access to privileged ground truth physics information such as precise object
geometry or particle tracks. Here we propose a method for learning simulators
directly from observations. Visual Particle Dynamics (VPD) jointly learns a
latent particle-based representation of 3D scenes, a neural simulator of the
latent particle dynamics, and a renderer that can produce images of the scene
from arbitrary views. VPD learns end to end from posed RGB-D videos and does
not require access to privileged information. Unlike existing 2D video
prediction models, we show that VPD's 3D structure enables scene editing and
long-term predictions. These results pave the way for downstream applications
ranging from video editing to robotic planning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05368">Toward Scalable and Transparent Multimodal Analytics to Study Standard Medical Procedures: Linking Hand Movement, Proximity, and Gaze Data. (arXiv:2312.05368v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Heilala_V/0/1/0/all/0/1">Ville Heilala</a>, <a href="http://arxiv.org/find/cs/1/au:+Lehesvuori_S/0/1/0/all/0/1">Sami Lehesvuori</a>, <a href="http://arxiv.org/find/cs/1/au:+Hamalainen_R/0/1/0/all/0/1">Raija H&#xe4;m&#xe4;l&#xe4;inen</a>, <a href="http://arxiv.org/find/cs/1/au:+Karkkainen_T/0/1/0/all/0/1">Tommi K&#xe4;rkk&#xe4;inen</a></p>
<p>This study employed multimodal learning analytics (MMLA) to analyze
behavioral dynamics during the ABCDE procedure in nursing education, focusing
on gaze entropy, hand movement velocities, and proximity measures. Utilizing
accelerometers and eye-tracking techniques, behaviorgrams were generated to
depict various procedural phases. Results identified four primary phases
characterized by distinct patterns of visual attention, hand movements, and
proximity to the patient or instruments. The findings suggest that MMLA can
offer valuable insights into procedural competence in medical education. This
research underscores the potential of MMLA to provide detailed, objective
evaluations of clinical procedures and their inherent complexities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05385">Apparate: Rethinking Early Exits to Tame Latency-Throughput Tensions in ML Serving. (arXiv:2312.05385v1 [cs.DC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1">Yinwei Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_R/0/1/0/all/0/1">Rui Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Iyer_A/0/1/0/all/0/1">Anand Iyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Kai Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Netravali_R/0/1/0/all/0/1">Ravi Netravali</a></p>
<p>Machine learning (ML) inference platforms are tasked with balancing two
competing goals: ensuring high throughput given many requests, and delivering
low-latency responses to support interactive applications. Unfortunately,
existing platform knobs (e.g., batch sizes) fail to ease this fundamental
tension, and instead only enable users to harshly trade off one property for
the other. This paper explores an alternate strategy to taming
throughput-latency tradeoffs by changing the granularity at which inference is
performed. We present Apparate, a system that automatically applies and manages
early exits (EEs) in ML models, whereby certain inputs can exit with results at
intermediate layers. To cope with the time-varying overhead and accuracy
challenges that EEs bring, Apparate repurposes exits to provide continual
feedback that powers several novel runtime monitoring and adaptation
strategies. Apparate lowers median response latencies by 40.5-91.5% and
10.0-24.2% for diverse CV and NLP workloads, respectively, without affecting
throughputs or violating tight accuracy constraints.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05386">Model Extraction Attacks Revisited. (arXiv:2312.05386v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1">Jiacheng Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pang_R/0/1/0/all/0/1">Ren Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Changjiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Ting Wang</a></p>
<p>Model extraction (ME) attacks represent one major threat to
Machine-Learning-as-a-Service (MLaaS) platforms by ``stealing'' the
functionality of confidential machine-learning models through querying
black-box APIs. Over seven years have passed since ME attacks were first
conceptualized in the seminal work. During this period, substantial advances
have been made in both ME attacks and MLaaS platforms, raising the intriguing
question: How has the vulnerability of MLaaS platforms to ME attacks been
evolving? In this work, we conduct an in-depth study to answer this critical
question. Specifically, we characterize the vulnerability of current,
mainstream MLaaS platforms to ME attacks from multiple perspectives including
attack strategies, learning techniques, surrogate-model design, and benchmark
tasks. Many of our findings challenge previously reported results, suggesting
emerging patterns of ME vulnerability. Further, by analyzing the vulnerability
of the same MLaaS platforms using historical datasets from the past four years,
we retrospectively characterize the evolution of ME vulnerability over time,
leading to a set of interesting findings. Finally, we make suggestions about
improving the current practice of MLaaS in terms of attack robustness. Our
study sheds light on the current state of ME vulnerability in the wild and
points to several promising directions for future research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05387">Cross Domain Generative Augmentation: Domain Generalization with Latent Diffusion Models. (arXiv:2312.05387v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hemati_S/0/1/0/all/0/1">Sobhan Hemati</a>, <a href="http://arxiv.org/find/cs/1/au:+Beitollahi_M/0/1/0/all/0/1">Mahdi Beitollahi</a>, <a href="http://arxiv.org/find/cs/1/au:+Estiri_A/0/1/0/all/0/1">Amir Hossein Estiri</a>, <a href="http://arxiv.org/find/cs/1/au:+Omari_B/0/1/0/all/0/1">Bassel Al Omari</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Guojun Zhang</a></p>
<p>Despite the huge effort in developing novel regularizers for Domain
Generalization (DG), adding simple data augmentation to the vanilla ERM which
is a practical implementation of the Vicinal Risk Minimization principle (VRM)
\citep{chapelle2000vicinal} outperforms or stays competitive with many of the
proposed regularizers. The VRM reduces the estimation error in ERM by replacing
the point-wise kernel estimates with a more precise estimation of true data
distribution that reduces the gap between data points \textbf{within each
domain}. However, in the DG setting, the estimation error of true data
distribution by ERM is mainly caused by the distribution shift \textbf{between
domains} which cannot be fully addressed by simple data augmentation techniques
within each domain. Inspired by this limitation of VRM, we propose a novel data
augmentation named Cross Domain Generative Augmentation (CDGA) that replaces
the pointwise kernel estimates in ERM with new density estimates in the
\textbf{vicinity of domain pairs} so that the gap between domains is further
reduced. To this end, CDGA, which is built upon latent diffusion models (LDM),
generates synthetic images to fill the gap between all domains and as a result,
reduces the non-iidness. We show that CDGA outperforms SOTA DG methods under
the Domainbed benchmark. To explain the effectiveness of CDGA, we generate more
than 5 Million synthetic images and perform extensive ablation studies
including data scaling laws, distribution visualization, domain shift
quantification, adversarial robustness, and loss landscape analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05388">Higher-Order Equivariant Neural Networks for Charge Density Prediction in Materials. (arXiv:2312.05388v1 [physics.comp-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Koker_T/0/1/0/all/0/1">Teddy Koker</a>, <a href="http://arxiv.org/find/physics/1/au:+Quigley_K/0/1/0/all/0/1">Keegan Quigley</a>, <a href="http://arxiv.org/find/physics/1/au:+Taw_E/0/1/0/all/0/1">Eric Taw</a>, <a href="http://arxiv.org/find/physics/1/au:+Tibbetts_K/0/1/0/all/0/1">Kevin Tibbetts</a>, <a href="http://arxiv.org/find/physics/1/au:+Li_L/0/1/0/all/0/1">Lin Li</a></p>
<p>The calculation of electron density distribution using density functional
theory (DFT) in materials and molecules is central to the study of their
quantum and macro-scale properties, yet accurate and efficient calculation
remains a long-standing challenge in the field of material science. This work
introduces ChargE3Net, an E(3)-equivariant graph neural network for predicting
electron density in atomic systems. ChargE3Net achieves equivariance through
the use of higher-order tensor representations, and directly predicts the
charge density at any arbitrary point in the system. We show that our method
achieves greater performance than prior work on large and diverse sets of
molecules and materials, and scales to larger systems than what is feasible to
compute with DFT. Using predicted electron densities as an initialization, we
show that fewer self-consistent iterations are required to converge DFT over
the default initialization. In addition, we show that non-self-consistent
calculations using the predicted electron densities can predict electronic and
thermodynamic properties of materials at near-DFT accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05397">On the Performance of Temporal Difference Learning With Neural Networks. (arXiv:2312.05397v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1">Haoxing Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Paschalidis_I/0/1/0/all/0/1">Ioannis Ch. Paschalidis</a>, <a href="http://arxiv.org/find/cs/1/au:+Olshevsky_A/0/1/0/all/0/1">Alex Olshevsky</a></p>
<p>Neural Temporal Difference (TD) Learning is an approximate temporal
difference method for policy evaluation that uses a neural network for function
approximation. Analysis of Neural TD Learning has proven to be challenging. In
this paper we provide a convergence analysis of Neural TD Learning with a
projection onto $B(\theta_0, \omega)$, a ball of fixed radius $\omega$ around
the initial point $\theta_0$. We show an approximation bound of $O(\epsilon) +
\tilde{O} (1/\sqrt{m})$ where $\epsilon$ is the approximation quality of the
best neural network in $B(\theta_0, \omega)$ and $m$ is the width of all hidden
layers in the network.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05398">Generative Network Layer for Communication Systems with Artificial Intelligence. (arXiv:2312.05398v1 [cs.IT])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Thorsager_M/0/1/0/all/0/1">Mathias Thorsager</a>, <a href="http://arxiv.org/find/cs/1/au:+Leyva_Mayorga_I/0/1/0/all/0/1">Israel Leyva-Mayorga</a>, <a href="http://arxiv.org/find/cs/1/au:+Soret_B/0/1/0/all/0/1">Beatriz Soret</a>, <a href="http://arxiv.org/find/cs/1/au:+Popovski_P/0/1/0/all/0/1">Petar Popovski</a></p>
<p>The traditional role of the network layer is the transfer of packet replicas
from source to destination through intermediate network nodes. We present a
generative network layer that uses Generative AI (GenAI) at intermediate or
edge network nodes and analyze its impact on the required data rates in the
network. We conduct a case study where the GenAI-aided nodes generate images
from prompts that consist of substantially compressed latent representations.
The results from network flow analyses under image quality constraints show
that the generative network layer can achieve an improvement of more than 100%
in terms of the required data rate.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05404">Disentangled Latent Representation Learning for Tackling the Confounding M-Bias Problem in Causal Inference. (arXiv:2312.05404v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1">Debo Cheng</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1">Yang Xie</a> (2), <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Ziqi Xu</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiuyong Li</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Lin Liu</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jixue Liu</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yinghao Zhang</a> (2), <a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1">Zaiwen Feng</a> (2) ((1) UniSA STEM, University of South Australia, Adelaide, Australia and (2) College of Informatics, Huazhong Agricultural University, Wuhan, China)</p>
<p>In causal inference, it is a fundamental task to estimate the causal effect
from observational data. However, latent confounders pose major challenges in
causal inference in observational data, for example, confounding bias and
M-bias. Recent data-driven causal effect estimators tackle the confounding bias
problem via balanced representation learning, but assume no M-bias in the
system, thus they fail to handle the M-bias. In this paper, we identify a
challenging and unsolved problem caused by a variable that leads to confounding
bias and M-bias simultaneously. To address this problem with co-occurring
M-bias and confounding bias, we propose a novel Disentangled Latent
Representation learning framework for learning latent representations from
proxy variables for unbiased Causal effect Estimation (DLRCE) from
observational data. Specifically, DLRCE learns three sets of latent
representations from the measured proxy variables to adjust for the confounding
bias and M-bias. Extensive experiments on both synthetic and three real-world
datasets demonstrate that DLRCE significantly outperforms the state-of-the-art
estimators in the case of the presence of both confounding bias and M-bias.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05405">Guaranteed Trust Region Optimization via Two-Phase KL Penalization. (arXiv:2312.05405v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zentner_K/0/1/0/all/0/1">K.R. Zentner</a>, <a href="http://arxiv.org/find/cs/1/au:+Puri_U/0/1/0/all/0/1">Ujjwal Puri</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zhehui Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sukhatme_G/0/1/0/all/0/1">Gaurav S. Sukhatme</a></p>
<p>On-policy reinforcement learning (RL) has become a popular framework for
solving sequential decision problems due to its computational efficiency and
theoretical simplicity. Some on-policy methods guarantee every policy update is
constrained to a trust region relative to the prior policy to ensure training
stability. These methods often require computationally intensive non-linear
optimization or require a particular form of action distribution. In this work,
we show that applying KL penalization alone is nearly sufficient to enforce
such trust regions. Then, we show that introducing a "fixup" phase is
sufficient to guarantee a trust region is enforced on every policy update while
adding fewer than 5% additional gradient steps in practice. The resulting
algorithm, which we call FixPO, is able to train a variety of policy
architectures and action spaces, is easy to implement, and produces results
competitive with other trust region methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05409">Large-scale Training of Foundation Models for Wearable Biosignals. (arXiv:2312.05409v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abbaspourazad_S/0/1/0/all/0/1">Salar Abbaspourazad</a>, <a href="http://arxiv.org/find/cs/1/au:+Elachqar_O/0/1/0/all/0/1">Oussama Elachqar</a>, <a href="http://arxiv.org/find/cs/1/au:+Miller_A/0/1/0/all/0/1">Andrew C. Miller</a>, <a href="http://arxiv.org/find/cs/1/au:+Emrani_S/0/1/0/all/0/1">Saba Emrani</a>, <a href="http://arxiv.org/find/cs/1/au:+Nallasamy_U/0/1/0/all/0/1">Udhyakumar Nallasamy</a>, <a href="http://arxiv.org/find/cs/1/au:+Shapiro_I/0/1/0/all/0/1">Ian Shapiro</a></p>
<p>Tracking biosignals is crucial for monitoring wellness and preempting the
development of severe medical conditions. Today, wearable devices can
conveniently record various biosignals, creating the opportunity to monitor
health status without disruption to one's daily routine. Despite widespread use
of wearable devices and existing digital biomarkers, the absence of curated
data with annotated medical labels hinders the development of new biomarkers to
measure common health conditions. In fact, medical datasets are usually small
in comparison to other domains, which is an obstacle for developing neural
network models for biosignals. To address this challenge, we have employed
self-supervised learning using the unlabeled sensor data collected under
informed consent from the large longitudinal Apple Heart and Movement Study
(AHMS) to train foundation models for two common biosignals:
photoplethysmography (PPG) and electrocardiogram (ECG) recorded on Apple Watch.
We curated PPG and ECG datasets from AHMS that include data from ~141K
participants spanning ~3 years. Our self-supervised learning framework includes
participant level positive pair selection, stochastic augmentation module and a
regularized contrastive loss optimized with momentum training, and generalizes
well to both PPG and ECG modalities. We show that the pre-trained foundation
models readily encode information regarding participants' demographics and
health conditions. To the best of our knowledge, this is the first study that
builds foundation models using large-scale PPG and ECG data collected via
wearable consumer devices $\unicode{x2013}$ prior works have commonly used
smaller-size datasets collected in clinical and experimental settings. We
believe PPG and ECG foundation models can enhance future wearable devices by
reducing the reliance on labeled data and hold the potential to help the users
improve their health.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05410">Rethinking materials simulations: Blending direct numerical simulations with neural operators. (arXiv:2312.05410v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oommen_V/0/1/0/all/0/1">Vivek Oommen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shukla_K/0/1/0/all/0/1">Khemraj Shukla</a>, <a href="http://arxiv.org/find/cs/1/au:+Desai_S/0/1/0/all/0/1">Saaketh Desai</a>, <a href="http://arxiv.org/find/cs/1/au:+Dingreville_R/0/1/0/all/0/1">Remi Dingreville</a>, <a href="http://arxiv.org/find/cs/1/au:+Karniadakis_G/0/1/0/all/0/1">George Em Karniadakis</a></p>
<p>Direct numerical simulations (DNS) are accurate but computationally expensive
for predicting materials evolution across timescales, due to the complexity of
the underlying evolution equations, the nature of multiscale spatio-temporal
interactions, and the need to reach long-time integration. We develop a new
method that blends numerical solvers with neural operators to accelerate such
simulations. This methodology is based on the integration of a community
numerical solver with a U-Net neural operator, enhanced by a
temporal-conditioning mechanism that enables accurate extrapolation and
efficient time-to-solution predictions of the dynamics. We demonstrate the
effectiveness of this framework on simulations of microstructure evolution
during physical vapor deposition modeled via the phase-field method. Such
simulations exhibit high spatial gradients due to the co-evolution of different
material phases with simultaneous slow and fast materials dynamics. We
establish accurate extrapolation of the coupled solver with up to 16.5$\times$
speed-up compared to DNS. This methodology is generalizable to a broad range of
evolutionary models, from solid mechanics, to fluid dynamics, geophysics,
climate, and more.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05412">CMMD: Contrastive Multi-Modal Diffusion for Video-Audio Conditional Modeling. (arXiv:2312.05412v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1">Ruihan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gamper_H/0/1/0/all/0/1">Hannes Gamper</a>, <a href="http://arxiv.org/find/cs/1/au:+Braun_S/0/1/0/all/0/1">Sebastian Braun</a></p>
<p>We introduce a multi-modal diffusion model tailored for the bi-directional
conditional generation of video and audio. Recognizing the importance of
accurate alignment between video and audio events in multi-modal generation
tasks, we propose a joint contrastive training loss to enhance the
synchronization between visual and auditory occurrences. Our research
methodology involves conducting comprehensive experiments on multiple datasets
to thoroughly evaluate the efficacy of our proposed model. The assessment of
generation quality and alignment performance is carried out from various
angles, encompassing both objective and subjective metrics. Our findings
demonstrate that the proposed model outperforms the baseline, substantiating
its effectiveness and efficiency. Notably, the incorporation of the contrastive
loss results in improvements in audio-visual alignment, particularly in the
high-correlation video-to-audio generation task. These results indicate the
potential of our proposed model as a robust solution for improving the quality
and alignment of multi-modal generation, thereby contributing to the
advancement of video and audio conditional generation systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05417">ESPN: Memory-Efficient Multi-Vector Information Retrieval. (arXiv:2312.05417v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shrestha_S/0/1/0/all/0/1">Susav Shrestha</a>, <a href="http://arxiv.org/find/cs/1/au:+Reddy_N/0/1/0/all/0/1">Narasimha Reddy</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zongwang Li</a></p>
<p>Recent advances in large language models have demonstrated remarkable
effectiveness in information retrieval (IR) tasks. While many neural IR systems
encode queries and documents into single-vector representations, multi-vector
models elevate the retrieval quality by producing multi-vector representations
and facilitating similarity searches at the granularity of individual tokens.
However, these models significantly amplify memory and storage requirements for
retrieval indices by an order of magnitude. This escalation in index size
renders the scalability of multi-vector IR models progressively challenging due
to their substantial memory demands. We introduce Embedding from Storage
Pipelined Network (ESPN) where we offload the entire re-ranking embedding
tables to SSDs and reduce the memory requirements by 5-16x. We design a
software prefetcher with hit rates exceeding 90%, improving SSD based retrieval
up to 6.4x, and demonstrate that we can maintain near memory levels of query
latency even for large query batch sizes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05429">Mitigating Nonlinear Algorithmic Bias in Binary Classification. (arXiv:2312.05429v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hui_W/0/1/0/all/0/1">Wendy Hui</a>, <a href="http://arxiv.org/find/cs/1/au:+Lau_W/0/1/0/all/0/1">Wai Kwong Lau</a></p>
<p>This paper proposes the use of causal modeling to detect and mitigate
algorithmic bias that is nonlinear in the protected attribute. We provide a
general overview of our approach. We use the German Credit data set, which is
available for download from the UC Irvine Machine Learning Repository, to
develop (1) a prediction model, which is treated as a black box, and (2) a
causal model for bias mitigation. In this paper, we focus on age bias and the
problem of binary classification. We show that the probability of getting
correctly classified as "low risk" is lowest among young people. The
probability increases with age nonlinearly. To incorporate the nonlinearity
into the causal model, we introduce a higher order polynomial term. Based on
the fitted causal model, the de-biased probability estimates are computed,
showing improved fairness with little impact on overall classification
accuracy. Causal modeling is intuitive and, hence, its use can enhance
explicability and promotes trust among different stakeholders of AI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05432">Fusing Multiple Algorithms for Heterogeneous Online Learning. (arXiv:2312.05432v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gadginmath_D/0/1/0/all/0/1">Darshan Gadginmath</a>, <a href="http://arxiv.org/find/cs/1/au:+Tripathi_S/0/1/0/all/0/1">Shivanshu Tripathi</a>, <a href="http://arxiv.org/find/cs/1/au:+Pasqualetti_F/0/1/0/all/0/1">Fabio Pasqualetti</a></p>
<p>This study addresses the challenge of online learning in contexts where
agents accumulate disparate data, face resource constraints, and use different
local algorithms. This paper introduces the Switched Online Learning Algorithm
(SOLA), designed to solve the heterogeneous online learning problem by
amalgamating updates from diverse agents through a dynamic switching mechanism
contingent upon their respective performance and available resources. We
theoretically analyze the design of the selecting mechanism to ensure that the
regret of SOLA is bounded. Our findings show that the number of changes in
selection needs to be bounded by a parameter dependent on the performance of
the different local algorithms. Additionally, two test cases are presented to
emphasize the effectiveness of SOLA, first on an online linear regression
problem and then on an online classification problem with the MNIST dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05440">Consistency Models for Scalable and Fast Simulation-Based Inference. (arXiv:2312.05440v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schmitt_M/0/1/0/all/0/1">Marvin Schmitt</a>, <a href="http://arxiv.org/find/cs/1/au:+Pratz_V/0/1/0/all/0/1">Valentin Pratz</a>, <a href="http://arxiv.org/find/cs/1/au:+Kothe_U/0/1/0/all/0/1">Ullrich K&#xf6;the</a>, <a href="http://arxiv.org/find/cs/1/au:+Burkner_P/0/1/0/all/0/1">Paul-Christian B&#xfc;rkner</a>, <a href="http://arxiv.org/find/cs/1/au:+Radev_S/0/1/0/all/0/1">Stefan T Radev</a></p>
<p>Simulation-based inference (SBI) is constantly in search of more expressive
algorithms for accurately inferring the parameters of complex models from noisy
data. We present consistency models for neural posterior estimation (CMPE), a
new free-form conditional sampler for scalable, fast, and amortized SBI with
generative neural networks. CMPE combines the advantages of normalizing flows
and flow matching methods into a single generative architecture: It essentially
distills a continuous probability flow and enables rapid few-shot inference
with an unconstrained architecture that can be tailored to the structure of the
estimation problem. Our empirical evaluation demonstrates that CMPE not only
outperforms current state-of-the-art algorithms on three hard low-dimensional
problems, but also achieves competitive performance in a high-dimensional
Bayesian denoising experiment and in estimating a computationally demanding
multi-scale model of tumor spheroid growth.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05454">Model Evaluation for Domain Identification of Unknown Classes in Open-World Recognition: A Proposal. (arXiv:2312.05454v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alfarisy_G/0/1/0/all/0/1">Gusti Ahmad Fanshuri Alfarisy</a>, <a href="http://arxiv.org/find/cs/1/au:+Malik_O/0/1/0/all/0/1">Owais Ahmed Malik</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_O/0/1/0/all/0/1">Ong Wee Hong</a></p>
<p>Open-World Recognition (OWR) is an emerging field that makes a machine
learning model competent in rejecting the unknowns, managing them, and
incrementally adding novel samples to the base knowledge. However, this broad
objective is not practical for an agent that works on a specific task. Not all
rejected samples will be used for learning continually in the future. Some
novel images in the open environment may not belong to the domain of interest.
Hence, identifying the unknown in the domain of interest is essential for a
machine learning model to learn merely the important samples. In this study, we
propose an evaluation protocol for estimating a model's capability in
separating unknown in-domain (ID) and unknown out-of-domain (OOD). We evaluated
using three approaches with an unknown domain and demonstrated the possibility
of identifying the domain of interest using the pre-trained parameters through
traditional transfer learning, Automated Machine Learning (AutoML), and Nearest
Class Mean (NCM) classifier with First Integer Neighbor Clustering Hierarchy
(FINCH). We experimented with five different domains: garbage, food, dogs,
plants, and birds. The results show that all approaches can be used as an
initial baseline yielding a good accuracy. In addition, a Balanced Accuracy
(BACCU) score from a pre-trained model indicates a tendency to excel in one or
more domains of interest. We observed that MobileNetV3 yielded the highest
BACCU score for the garbage domain and surpassed complex models such as the
transformer network. Meanwhile, our results also suggest that a strong
representation in the pre-trained model is important for identifying unknown
classes in the same domain. This study could open the bridge toward open-world
recognition in domain-specific tasks where the relevancy of the unknown classes
is vital.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05456">On the calibration of compartmental epidemiological models. (arXiv:2312.05456v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1">Nikunj Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Mai_A/0/1/0/all/0/1">Anh Mai</a>, <a href="http://arxiv.org/find/cs/1/au:+Abouzied_A/0/1/0/all/0/1">Azza Abouzied</a>, <a href="http://arxiv.org/find/cs/1/au:+Shasha_D/0/1/0/all/0/1">Dennis Shasha</a></p>
<p>Epidemiological compartmental models are useful for understanding infectious
disease propagation and directing public health policy decisions. Calibration
of these models is an important step in offering accurate forecasts of disease
dynamics and the effectiveness of interventions. In this study, we present an
overview of calibrating strategies that can be employed, including several
optimization methods and reinforcement learning (RL). We discuss the benefits
and drawbacks of these methods and highlight relevant practical conclusions
from our experiments. Optimization methods iteratively adjust the parameters of
the model until the model output matches the available data, whereas RL uses
trial and error to learn the optimal set of parameters by maximizing a reward
signal. Finally, we discuss how the calibration of parameters of
epidemiological compartmental models is an emerging field that has the
potential to improve the accuracy of disease modeling and public health
decision-making. Further research is needed to validate the effectiveness and
scalability of these approaches in different epidemiological contexts. All
codes and resources are available on
\url{https://github.com/Nikunj-Gupta/On-the-Calibration-of-Compartmental-Epidemiological-Models}.
We hope this work can facilitate related research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05460">Multi-source domain adaptation for regression. (arXiv:2312.05460v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Wu_Y/0/1/0/all/0/1">Yujie Wu</a>, <a href="http://arxiv.org/find/stat/1/au:+Parmigiani_G/0/1/0/all/0/1">Giovanni Parmigiani</a>, <a href="http://arxiv.org/find/stat/1/au:+Ren_B/0/1/0/all/0/1">Boyu Ren</a></p>
<p>Multi-source domain adaptation (DA) aims at leveraging information from more
than one source domain to make predictions in a target domain, where different
domains may have different data distributions. Most existing methods for
multi-source DA focus on classification problems while there is only limited
investigation in the regression settings. In this paper, we fill in this gap
through a two-step procedure. First, we extend a flexible single-source DA
algorithm for classification through outcome-coarsening to enable its
application to regression problems. We then augment our single-source DA
algorithm for regression with ensemble learning to achieve multi-source DA. We
consider three learning paradigms in the ensemble algorithm, which combines
linearly the target-adapted learners trained with each source domain: (i) a
multi-source stacking algorithm to obtain the ensemble weights; (ii) a
similarity-based weighting where the weights reflect the quality of DA of each
target-adapted learner; and (iii) a combination of the stacking and similarity
weights. We illustrate the performance of our algorithms with simulations and a
data application where the goal is to predict High-density lipoprotein (HDL)
cholesterol levels using gut microbiome. We observe a consistent improvement in
prediction performance of our multi-source DA algorithm over the routinely used
methods in all these scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05461">STREAMLINE: An Automated Machine Learning Pipeline for Biomedicine Applied to Examine the Utility of Photography-Based Phenotypes for OSA Prediction Across International Sleep Centers. (arXiv:2312.05461v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Urbanowicz_R/0/1/0/all/0/1">Ryan J. Urbanowicz</a>, <a href="http://arxiv.org/find/cs/1/au:+Bandhey_H/0/1/0/all/0/1">Harsh Bandhey</a>, <a href="http://arxiv.org/find/cs/1/au:+Keenan_B/0/1/0/all/0/1">Brendan T. Keenan</a>, <a href="http://arxiv.org/find/cs/1/au:+Maislin_G/0/1/0/all/0/1">Greg Maislin</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1">Sy Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mowery_D/0/1/0/all/0/1">Danielle L. Mowery</a>, <a href="http://arxiv.org/find/cs/1/au:+Lynch_S/0/1/0/all/0/1">Shannon M. Lynch</a>, <a href="http://arxiv.org/find/cs/1/au:+Mazzotti_D/0/1/0/all/0/1">Diego R. Mazzotti</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1">Fang Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qing Yun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Penzel_T/0/1/0/all/0/1">Thomas Penzel</a>, <a href="http://arxiv.org/find/cs/1/au:+Tufik_S/0/1/0/all/0/1">Sergio Tufik</a>, <a href="http://arxiv.org/find/cs/1/au:+Bittencourt_L/0/1/0/all/0/1">Lia Bittencourt</a>, <a href="http://arxiv.org/find/cs/1/au:+Gislason_T/0/1/0/all/0/1">Thorarinn Gislason</a>, <a href="http://arxiv.org/find/cs/1/au:+Chazal_P/0/1/0/all/0/1">Philip de Chazal</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_B/0/1/0/all/0/1">Bhajan Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+McArdle_N/0/1/0/all/0/1">Nigel McArdle</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1">Ning-Hung Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pack_A/0/1/0/all/0/1">Allan Pack</a>, <a href="http://arxiv.org/find/cs/1/au:+Schwab_R/0/1/0/all/0/1">Richard J. Schwab</a>, <a href="http://arxiv.org/find/cs/1/au:+Cistulli_P/0/1/0/all/0/1">Peter A. Cistulli</a>, <a href="http://arxiv.org/find/cs/1/au:+Magalang_U/0/1/0/all/0/1">Ulysses J. Magalang</a></p>
<p>While machine learning (ML) includes a valuable array of tools for analyzing
biomedical data, significant time and expertise is required to assemble
effective, rigorous, and unbiased pipelines. Automated ML (AutoML) tools seek
to facilitate ML application by automating a subset of analysis pipeline
elements. In this study we develop and validate a Simple, Transparent,
End-to-end Automated Machine Learning Pipeline (STREAMLINE) and apply it to
investigate the added utility of photography-based phenotypes for predicting
obstructive sleep apnea (OSA); a common and underdiagnosed condition associated
with a variety of health, economic, and safety consequences. STREAMLINE is
designed to tackle biomedical binary classification tasks while adhering to
best practices and accommodating complexity, scalability, reproducibility,
customization, and model interpretation. Benchmarking analyses validated the
efficacy of STREAMLINE across data simulations with increasingly complex
patterns of association. Then we applied STREAMLINE to evaluate the utility of
demographics (DEM), self-reported comorbidities (DX), symptoms (SYM), and
photography-based craniofacial (CF) and intraoral (IO) anatomy measures in
predicting any OSA or moderate/severe OSA using 3,111 participants from Sleep
Apnea Global Interdisciplinary Consortium (SAGIC). OSA analyses identified a
significant increase in ROC-AUC when adding CF to DEM+DX+SYM to predict
moderate/severe OSA. A consistent but non-significant increase in PRC-AUC was
observed with the addition of each subsequent feature set to predict any OSA,
with CF and IO yielding minimal improvements. Application of STREAMLINE to OSA
data suggests that CF features provide additional value in predicting
moderate/severe OSA, but neither CF nor IO features meaningfully improved the
prediction of any OSA beyond established demographics, comorbidity and symptom
characteristics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05464">Identifying and Mitigating Model Failures through Few-shot CLIP-aided Diffusion Generation. (arXiv:2312.05464v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chegini_A/0/1/0/all/0/1">Atoosa Chegini</a>, <a href="http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1">Soheil Feizi</a></p>
<p>Deep learning models can encounter unexpected failures, especially when
dealing with challenging sub-populations. One common reason for these failures
is the occurrence of objects in backgrounds that are rarely seen during
training. To gain a better understanding of these failure modes,
human-interpretable descriptions are crucial for further analysis and
improvement which is expensive. In this study, we propose an end-to-end
framework that utilizes the capabilities of large language models (ChatGPT) and
vision-language deep models (CLIP) to generate text descriptions of failure
modes associated with spurious correlations (e.g. rarely seen backgrounds)
without human-in-the-loop intervention. These descriptions can be used to
generate synthetic data using generative models, such as diffusion models. The
model can now use this generated data to learn from its weaknesses and enhance
its performance on backgrounds that are uncommon for each class of data. Our
approach serves as a broad solution, promising progress in comprehending model
failure modes and strengthening deep learning models across a wide range of
failure scenarios (e.g. bacckgrounds, colors) automatically in a few-shot
manner. Our experiments have shown remarkable \textbf{improvements in accuracy
($\sim \textbf{21%}$)} on hard sub-populations (particularly for wrong
background association) across $40$ different models, such as ResNets,
EfficientNets, DenseNets, Vision Transformer (ViT), SwAVs, MoCos, DINOs, and
CLIPs on various datasets such as ImageNet-1000, CIFAR-10, and CIFAR-100.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05465">On Task-Relevant Loss Functions in Meta-Reinforcement Learning and Online LQR. (arXiv:2312.05465v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1">Jaeuk Shin</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1">Giho Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Howon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Joonho Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_I/0/1/0/all/0/1">Insoon Yang</a></p>
<p>Designing a competent meta-reinforcement learning (meta-RL) algorithm in
terms of data usage remains a central challenge to be tackled for its
successful real-world applications. In this paper, we propose a
sample-efficient meta-RL algorithm that learns a model of the system or
environment at hand in a task-directed manner. As opposed to the standard
model-based approaches to meta-RL, our method exploits the value information in
order to rapidly capture the decision-critical part of the environment. The key
component of our method is the loss function for learning the task inference
module and the system model that systematically couples the model discrepancy
and the value estimate, thereby facilitating the learning of the policy and the
task inference module with a significantly smaller amount of data compared to
the existing meta-RL algorithms. The idea is also extended to a non-meta-RL
setting, namely an online linear quadratic regulator (LQR) problem, where our
method can be simplified to reveal the essence of the strategy. The proposed
method is evaluated in high-dimensional robotic control and online LQR
problems, empirically verifying its effectiveness in extracting information
indispensable for solving the tasks from observations in a sample efficient
manner.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05472">Spectroscopy-Guided Discovery of Three-Dimensional Structures of Disordered Materials with Diffusion Models. (arXiv:2312.05472v1 [cond-mat.mtrl-sci])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Kwon_H/0/1/0/all/0/1">Hyuna Kwon</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Hsu_T/0/1/0/all/0/1">Tim Hsu</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Sun_W/0/1/0/all/0/1">Wenyu Sun</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Jeong_W/0/1/0/all/0/1">Wonseok Jeong</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Aydin_F/0/1/0/all/0/1">Fikret Aydin</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Chapman_J/0/1/0/all/0/1">James Chapman</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Chen_X/0/1/0/all/0/1">Xiao Chen</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Carbone_M/0/1/0/all/0/1">Matthew R. Carbone</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Lu_D/0/1/0/all/0/1">Deyu Lu</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Zhou_F/0/1/0/all/0/1">Fei Zhou</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Pham_T/0/1/0/all/0/1">Tuan Anh Pham</a></p>
<p>The ability to rapidly develop materials with desired properties has a
transformative impact on a broad range of emerging technologies. In this work,
we introduce a new framework based on the diffusion model, a recent generative
machine learning method to predict 3D structures of disordered materials from a
target property. For demonstration, we apply the model to identify the atomic
structures of amorphous carbons ($a$-C) as a representative material system
from the target X-ray absorption near edge structure (XANES) spectra--a common
experimental technique to probe atomic structures of materials. We show that
conditional generation guided by XANES spectra reproduces key features of the
target structures. Furthermore, we show that our model can steer the generative
process to tailor atomic arrangements for a specific XANES spectrum. Finally,
our generative model exhibits a remarkable scale-agnostic property, thereby
enabling generation of realistic, large-scale structures through learning from
a small-scale dataset (i.e., with small unit cells). Our work represents a
significant stride in bridging the gap between materials characterization and
atomic structure determination; in addition, it can be leveraged for materials
discovery in exploring various material properties as targeted.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05479">Exploring Sparsity in Graph Transformers. (arXiv:2312.05479v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chuang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1">Yibing Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xueqi Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1">Liang Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1">Dapeng Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jia Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1">Wenbin Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1">Bo Du</a></p>
<p>Graph Transformers (GTs) have achieved impressive results on various
graph-related tasks. However, the huge computational cost of GTs hinders their
deployment and application, especially in resource-constrained environments.
Therefore, in this paper, we explore the feasibility of sparsifying GTs, a
significant yet under-explored topic. We first discuss the redundancy of GTs
based on the characteristics of existing GT models, and then propose a
comprehensive \textbf{G}raph \textbf{T}ransformer \textbf{SP}arsification
(GTSP) framework that helps to reduce the computational complexity of GTs from
four dimensions: the input graph data, attention heads, model layers, and model
weights. Specifically, GTSP designs differentiable masks for each individual
compressible component, enabling effective end-to-end pruning. We examine our
GTSP through extensive experiments on prominent GTs, including GraphTrans,
Graphormer, and GraphGPS. The experimental results substantiate that GTSP
effectively cuts computational costs, accompanied by only marginal decreases in
accuracy or, in some cases, even improvements. For instance, GTSP yields a
reduction of 30\% in Floating Point Operations while contributing to a 1.8\%
increase in Area Under the Curve accuracy on OGBG-HIV dataset. Furthermore, we
provide several insights on the characteristics of attention heads and the
behavior of attention mechanisms, all of which have immense potential to
inspire future research endeavors in this domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05486">FreeFlow: A Comprehensive Understanding on Diffusion Probabilistic Models via Optimal Transport. (arXiv:2312.05486v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1">Bowen Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1">Shibao Zheng</a></p>
<p>The blooming diffusion probabilistic models (DPMs) have garnered significant
interest due to their impressive performance and the elegant inspiration they
draw from physics. While earlier DPMs relied upon the Markovian assumption,
recent methods based on differential equations have been rapidly applied to
enhance the efficiency and capabilities of these models. However, a theoretical
interpretation encapsulating these diverse algorithms is insufficient yet
pressingly required to guide further development of DPMs. In response to this
need, we present FreeFlow, a framework that provides a thorough explanation of
the diffusion formula as time-dependent optimal transport, where the
evolutionary pattern of probability density is given by the gradient flows of a
functional defined in Wasserstein space. Crucially, our framework necessitates
a unified description that not only clarifies the subtle mechanism of DPMs but
also indicates the roots of some defects through creative involvement of
Lagrangian and Eulerian views to understand the evolution of probability flow.
We particularly demonstrate that the core equation of FreeFlow condenses all
stochastic and deterministic DPMs into a single case, showcasing the
expansibility of our method. Furthermore, the Riemannian geometry employed in
our work has the potential to bridge broader subjects in mathematics, which
enable the involvement of more profound tools for the establishment of more
outstanding and generalized models in the future.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05496">Flexible Cross-Modal Steganography via Implicit Representations. (arXiv:2312.05496v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Seoyun Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1">Sojeong Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoo_C/0/1/0/all/0/1">Chang D. Yoo</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Junmo Kim</a></p>
<p>We present INRSteg, an innovative lossless steganography framework based on a
novel data form Implicit Neural Representations (INR) that is modal-agnostic.
Our framework is considered for effectively hiding multiple data without
altering the original INR ensuring high-quality stego data. The neural
representations of secret data are first concatenated to have independent paths
that do not overlap, then weight freezing techniques are applied to the
diagonal blocks of the weight matrices for the concatenated network to preserve
the weights of secret data while additional free weights in the off-diagonal
blocks of weight matrices are fitted to the cover data. Our framework can
perform unexplored cross-modal steganography for various modalities including
image, audio, video, and 3D shapes, and it achieves state-of-the-art
performance compared to previous intra-modal steganographic methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05502">Poisoning $\times$ Evasion: Symbiotic Adversarial Robustness for Graph Neural Networks. (arXiv:2312.05502v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Erdogan_E/0/1/0/all/0/1">Ege Erdogan</a>, <a href="http://arxiv.org/find/cs/1/au:+Geisler_S/0/1/0/all/0/1">Simon Geisler</a>, <a href="http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1">Stephan G&#xfc;nnemann</a></p>
<p>It is well-known that deep learning models are vulnerable to small input
perturbations. Such perturbed instances are called adversarial examples.
Adversarial examples are commonly crafted to fool a model either at training
time (poisoning) or test time (evasion). In this work, we study the symbiosis
of poisoning and evasion. We show that combining both threat models can
substantially improve the devastating efficacy of adversarial attacks.
Specifically, we study the robustness of Graph Neural Networks (GNNs) under
structure perturbations and devise a memory-efficient adaptive end-to-end
attack for the novel threat model using first-order optimization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05503">Aligner: One Global Token is Worth Millions of Parameters When Aligning Large Language Models. (arXiv:2312.05503v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ziheng_Z/0/1/0/all/0/1">Zhou Ziheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yingnian Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1">Song-Chun Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Terzopoulos_D/0/1/0/all/0/1">Demetri Terzopoulos</a> (University of California, Los Angeles)</p>
<p>We introduce Aligner, a novel Parameter-Efficient Fine-Tuning (PEFT) method
for aligning multi-billion-parameter-sized Large Language Models (LLMs).
Aligner employs a unique design that constructs a globally shared set of
tunable tokens that modify the attention of every layer. Remarkably with this
method, even when using one token accounting for a mere 5,000 parameters,
Aligner can still perform comparably well to state-of-the-art LLM adaptation
methods like LoRA that require millions of parameters. This capacity is
substantiated in both instruction following and value alignment tasks. Besides
the multiple order-of-magnitude improvement in parameter efficiency, the
insight Aligner provides into the internal mechanisms of LLMs is also valuable.
The architectural features and efficacy of our method, in addition to our
experiments demonstrate that an LLM separates its internal handling of "form"
and "knowledge" in a somewhat orthogonal manner. This finding promises to
motivate new research into LLM mechanism understanding and value alignment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05508">Improving Adversarial Robust Fairness via Anti-Bias Soft Label Distillation. (arXiv:2312.05508v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1">Shiji Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xizhe Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1">Xingxing Wei</a></p>
<p>Adversarial Training (AT) has been widely proved to be an effective method to
improve the adversarial robustness against adversarial examples for Deep Neural
Networks (DNNs). As a variant of AT, Adversarial Robustness Distillation (ARD)
has demonstrated its superior performance in improving the robustness of small
student models with the guidance of large teacher models. However, both AT and
ARD encounter the robust fairness problem: these models exhibit strong
robustness when facing part of classes (easy class), but weak robustness when
facing others (hard class). In this paper, we give an in-depth analysis of the
potential factors and argue that the smoothness degree of samples' soft labels
for different classes (i.e., hard class or easy class) will affect the robust
fairness of DNN models from both empirical observation and theoretical
analysis. Based on the above finding, we propose an Anti-Bias Soft Label
Distillation (ABSLD) method to mitigate the adversarial robust fairness problem
within the framework of Knowledge Distillation (KD). Specifically, ABSLD
adaptively reduces the student's error risk gap between different classes to
achieve fairness by adjusting the class-wise smoothness degree of samples' soft
labels during the training process, and the smoothness degree of soft labels is
controlled by assigning different temperatures in KD to different classes.
Extensive experiments demonstrate that ABSLD outperforms state-of-the-art AT,
ARD, and robust fairness methods in terms of overall performance of robustness
and fairness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05516">Stateful Large Language Model Serving with Pensieve. (arXiv:2312.05516v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1">Lingfan Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jinyang Li</a></p>
<p>Large Language Models (LLMs) have recently experienced great success, as
evident in the widespread popularity of ChatGPT. Existing LLM serving systems
are stateless across requests. Consequently, when LLMs are used in the common
setting of multi-turn conversations, a growing log of the conversation history
must be processed alongside any request by the serving system at each turn,
resulting in repeated history processing. In this paper, we design $Pensieve$,
a system optimized for multi-turn conversation LLM serving. $Pensieve$
maintains the conversation state across requests by caching previously
processed history to avoid duplicate processing. $Pensieve$'s multi-tier
caching strategy can utilize both GPU and CPU memory to efficiently store and
retrieve cached data. $Pensieve$ also generalizes the recent PagedAttention
kernel to support attention between multiple input tokens with a GPU cache
spread over non-contiguous memory. Our evaluation shows that $Pensieve$ is able
to achieve 1.51-1.95x throughput compared to vLLM and reduce latency by 60-75%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05519">Isomorphic-Consistent Variational Graph Auto-Encoders for Multi-Level Graph Representation Learning. (arXiv:2312.05519v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Hanxuan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kong_Q/0/1/0/all/0/1">Qingchao Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1">Wenji Mao</a></p>
<p>Graph representation learning is a fundamental research theme and can be
generalized to benefit multiple downstream tasks from the node and link levels
to the higher graph level. In practice, it is desirable to develop
task-agnostic general graph representation learning methods that are typically
trained in an unsupervised manner. Related research reveals that the power of
graph representation learning methods depends on whether they can differentiate
distinct graph structures as different embeddings and map isomorphic graphs to
consistent embeddings (i.e., the isomorphic consistency of graph models).
However, for task-agnostic general graph representation learning, existing
unsupervised graph models, represented by the variational graph auto-encoders
(VGAEs), can only keep the isomorphic consistency within the subgraphs of 1-hop
neighborhoods and thus usually manifest inferior performance on the more
difficult higher-level tasks. To overcome the limitations of existing
unsupervised methods, in this paper, we propose the Isomorphic-Consistent VGAE
(IsoC-VGAE) for multi-level task-agnostic graph representation learning. We
first devise a decoding scheme to provide a theoretical guarantee of keeping
the isomorphic consistency under the settings of unsupervised learning. We then
propose the Inverse Graph Neural Network (Inv-GNN) decoder as its intuitive
realization, which trains the model via reconstructing the GNN node embeddings
with multi-hop neighborhood information, so as to maintain the high-order
isomorphic consistency within the VGAE framework. We conduct extensive
experiments on the representative graph learning tasks at different levels,
including node classification, link prediction and graph classification, and
the results verify that our proposed model generally outperforms both the
state-of-the-art unsupervised methods and representative supervised methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05526">Reinforcement Neighborhood Selection for Unsupervised Graph Anomaly Detection. (arXiv:2312.05526v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bei_Y/0/1/0/all/0/1">Yuanchen Bei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1">Sheng Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_Q/0/1/0/all/0/1">Qiaoyu Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Bu_J/0/1/0/all/0/1">Jiajun Bu</a></p>
<p>Unsupervised graph anomaly detection is crucial for various practical
applications as it aims to identify anomalies in a graph that exhibit rare
patterns deviating significantly from the majority of nodes. Recent
advancements have utilized Graph Neural Networks (GNNs) to learn high-quality
node representations for anomaly detection by aggregating information from
neighborhoods. However, the presence of anomalies may render the observed
neighborhood unreliable and result in misleading information aggregation for
node representation learning. Selecting the proper neighborhood is critical for
graph anomaly detection but also challenging due to the absence of
anomaly-oriented guidance and the interdependence with representation learning.
To address these issues, we utilize the advantages of reinforcement learning in
adaptively learning in complex environments and propose a novel method that
incorporates Reinforcement neighborhood selection for unsupervised graph
ANomaly Detection (RAND). RAND begins by enriching the candidate neighbor pool
of the given central node with multiple types of indirect neighbors. Next, RAND
designs a tailored reinforcement anomaly evaluation module to assess the
reliability and reward of considering the given neighbor. Finally, RAND selects
the most reliable subset of neighbors based on these rewards and introduces an
anomaly-aware aggregator to amplify messages from reliable neighbors while
diminishing messages from unreliable ones. Extensive experiments on both three
synthetic and two real-world datasets demonstrate that RAND outperforms the
state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05540">Federated Causality Learning with Explainable Adaptive Optimization. (arXiv:2312.05540v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1">Dezhi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xintong He</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1">Guoxian Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Domeniconi_C/0/1/0/all/0/1">Carlotta Domeniconi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jinglin Zhang</a></p>
<p>Discovering the causality from observational data is a crucial task in
various scientific domains. With increasing awareness of privacy, data are not
allowed to be exposed, and it is very hard to learn causal graphs from
dispersed data, since these data may have different distributions. In this
paper, we propose a federated causal discovery strategy (FedCausal) to learn
the unified global causal graph from decentralized heterogeneous data. We
design a global optimization formula to naturally aggregate the causal graphs
from client data and constrain the acyclicity of the global graph without
exposing local data. Unlike other federated causal learning algorithms,
FedCausal unifies the local and global optimizations into a complete directed
acyclic graph (DAG) learning process with a flexible optimization objective. We
prove that this optimization objective has a high interpretability and can
adaptively handle homogeneous and heterogeneous data. Experimental results on
synthetic and real datasets show that FedCausal can effectively deal with
non-independently and identically distributed (non-iid) data and has a superior
performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05547">Signatures Meet Dynamic Programming: Generalizing Bellman Equations for Trajectory Following. (arXiv:2312.05547v1 [eess.SY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ohnishi_M/0/1/0/all/0/1">Motoya Ohnishi</a>, <a href="http://arxiv.org/find/eess/1/au:+Akinola_I/0/1/0/all/0/1">Iretiayo Akinola</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1">Jie Xu</a>, <a href="http://arxiv.org/find/eess/1/au:+Mandlekar_A/0/1/0/all/0/1">Ajay Mandlekar</a>, <a href="http://arxiv.org/find/eess/1/au:+Ramos_F/0/1/0/all/0/1">Fabio Ramos</a></p>
<p>Path signatures have been proposed as a powerful representation of paths that
efficiently captures the path's analytic and geometric characteristics, having
useful algebraic properties including fast concatenation of paths through
tensor products. Signatures have recently been widely adopted in machine
learning problems for time series analysis. In this work we establish
connections between value functions typically used in optimal control and
intriguing properties of path signatures. These connections motivate our novel
control framework with signature transforms that efficiently generalizes the
Bellman equation to the space of trajectories. We analyze the properties and
advantages of the framework, termed signature control. In particular, we
demonstrate that (i) it can naturally deal with varying/adaptive time steps;
(ii) it propagates higher-level information more efficiently than value
function updates; (iii) it is robust to dynamical system misspecification over
long rollouts. As a specific case of our framework, we devise a model
predictive control method for path tracking. This method generalizes integral
control, being suitable for problems with unknown disturbances. The proposed
algorithms are tested in simulation, with differentiable physics models
including typical control and robotics tasks such as point-mass, curve
following for an ant model, and a robotic manipulator.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05548">A Unified Multi-Phase CT Synthesis and Classification Framework for Kidney Cancer Diagnosis with Incomplete Data. (arXiv:2312.05548v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Uhm_K/0/1/0/all/0/1">Kwang-Hyun Uhm</a>, <a href="http://arxiv.org/find/eess/1/au:+Jung_S/0/1/0/all/0/1">Seung-Won Jung</a>, <a href="http://arxiv.org/find/eess/1/au:+Choi_M/0/1/0/all/0/1">Moon Hyung Choi</a>, <a href="http://arxiv.org/find/eess/1/au:+Hong_S/0/1/0/all/0/1">Sung-Hoo Hong</a>, <a href="http://arxiv.org/find/eess/1/au:+Ko_S/0/1/0/all/0/1">Sung-Jea Ko</a></p>
<p>Multi-phase CT is widely adopted for the diagnosis of kidney cancer due to
the complementary information among phases. However, the complete set of
multi-phase CT is often not available in practical clinical applications. In
recent years, there have been some studies to generate the missing modality
image from the available data. Nevertheless, the generated images are not
guaranteed to be effective for the diagnosis task. In this paper, we propose a
unified framework for kidney cancer diagnosis with incomplete multi-phase CT,
which simultaneously recovers missing CT images and classifies cancer subtypes
using the completed set of images. The advantage of our framework is that it
encourages a synthesis model to explicitly learn to generate missing CT phases
that are helpful for classifying cancer subtypes. We further incorporate lesion
segmentation network into our framework to exploit lesion-level features for
effective cancer classification in the whole CT volumes. The proposed framework
is based on fully 3D convolutional neural networks to jointly optimize both
synthesis and classification of 3D CT volumes. Extensive experiments on both
in-house and external datasets demonstrate the effectiveness of our framework
for the diagnosis with incomplete data compared with state-of-the-art
baselines. In particular, cancer subtype classification using the completed CT
data by our method achieves higher performance than the classification using
the given incomplete data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05549">Multi-granularity Causal Structure Learning. (arXiv:2312.05549v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1">Jiaxuan Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1">Guoxian Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1">Shuyin Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guoyin Wang</a></p>
<p>Unveil, model, and comprehend the causal mechanisms underpinning natural
phenomena stand as fundamental endeavors across myriad scientific disciplines.
Meanwhile, new knowledge emerges when discovering causal relationships from
data. Existing causal learning algorithms predominantly focus on the isolated
effects of variables, overlook the intricate interplay of multiple variables
and their collective behavioral patterns. Furthermore, the ubiquity of
high-dimensional data exacts a substantial temporal cost for causal algorithms.
In this paper, we develop a novel method called MgCSL (Multi-granularity Causal
Structure Learning), which first leverages sparse auto-encoder to explore
coarse-graining strategies and causal abstractions from micro-variables to
macro-ones. MgCSL then takes multi-granularity variables as inputs to train
multilayer perceptrons and to delve the causality between variables. To enhance
the efficacy on high-dimensional data, MgCSL introduces a simplified acyclicity
constraint to adeptly search the directed acyclic graph among variables.
Experimental results show that MgCSL outperforms competitive baselines, and
finds out explainable causal connections on fMRI datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05550">D3A-TS: Denoising-Driven Data Augmentation in Time Series. (arXiv:2312.05550v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Solis_Martin_D/0/1/0/all/0/1">David Solis-Martin</a>, <a href="http://arxiv.org/find/cs/1/au:+Galan_Paez_J/0/1/0/all/0/1">Juan Galan-Paez</a>, <a href="http://arxiv.org/find/cs/1/au:+Borrego_Diaz_J/0/1/0/all/0/1">Joaquin Borrego-Diaz</a></p>
<p>It has been demonstrated that the amount of data is crucial in data-driven
machine learning methods. Data is always valuable, but in some tasks, it is
almost like gold. This occurs in engineering areas where data is scarce or very
expensive to obtain, such as predictive maintenance, where faults are rare. In
this context, a mechanism to generate synthetic data can be very useful. While
in fields such as Computer Vision or Natural Language Processing synthetic data
generation has been extensively explored with promising results, in other
domains such as time series it has received less attention. This work
specifically focuses on studying and analyzing the use of different techniques
for data augmentation in time series for classification and regression
problems. The proposed approach involves the use of diffusion probabilistic
models, which have recently achieved successful results in the field of Image
Processing, for data augmentation in time series. Additionally, the use of
meta-attributes to condition the data augmentation process is investigated. The
results highlight the high utility of this methodology in creating synthetic
data to train classification and regression models. To assess the results, six
different datasets from diverse domains were employed, showcasing versatility
in terms of input size and output types. Finally, an extensive ablation study
is conducted to further support the obtained outcomes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05551">Multi-dimensional Fair Federated Learning. (arXiv:2312.05551v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1">Cong Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1">Guoxian Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qingzhong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Han Yu</a></p>
<p>Federated learning (FL) has emerged as a promising collaborative and secure
paradigm for training a model from decentralized data without compromising
privacy. Group fairness and client fairness are two dimensions of fairness that
are important for FL. Standard FL can result in disproportionate disadvantages
for certain clients, and it still faces the challenge of treating different
groups equitably in a population. The problem of privately training fair FL
models without compromising the generalization capability of disadvantaged
clients remains open. In this paper, we propose a method, called mFairFL, to
address this problem and achieve group fairness and client fairness
simultaneously. mFairFL leverages differential multipliers to construct an
optimization objective for empirical risk minimization with fairness
constraints. Before aggregating locally trained models, it first detects
conflicts among their gradients, and then iteratively curates the direction and
magnitude of gradients to mitigate these conflicts. Theoretical analysis proves
mFairFL facilitates the fairness in model development. The experimental
evaluations based on three benchmark datasets show significant advantages of
mFairFL compared to seven state-of-the-art baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05552">Improving Parameter Training for VQEs by Sequential Hamiltonian Assembly. (arXiv:2312.05552v1 [quant-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Stein_J/0/1/0/all/0/1">Jonas Stein</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Roshani_N/0/1/0/all/0/1">Navid Roshani</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Zorn_M/0/1/0/all/0/1">Maximilian Zorn</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Altmann_P/0/1/0/all/0/1">Philipp Altmann</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Kolle_M/0/1/0/all/0/1">Michael K&#xf6;lle</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Linnhoff_Popien_C/0/1/0/all/0/1">Claudia Linnhoff-Popien</a></p>
<p>A central challenge in quantum machine learning is the design and training of
parameterized quantum circuits (PQCs). Similar to deep learning, vanishing
gradients pose immense problems in the trainability of PQCs, which have been
shown to arise from a multitude of sources. One such cause are non-local loss
functions, that demand the measurement of a large subset of involved qubits. To
facilitate the parameter training for quantum applications using global loss
functions, we propose a Sequential Hamiltonian Assembly, which iteratively
approximates the loss function using local components. Aiming for a prove of
principle, we evaluate our approach using Graph Coloring problem with a
Varational Quantum Eigensolver (VQE). Simulation results show, that our
approach outperforms conventional parameter training by 29.99% and the
empirical state of the art, Layerwise Learning, by 5.12% in the mean accuracy.
This paves the way towards locality-aware learning techniques, allowing to
evade vanishing gradients for a large class of practically relevant problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05560">Enhancing the Accuracy of Predictors of Activity Sequences of Business Processes. (arXiv:2312.05560v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1">Muhammad Awais Ali</a>, <a href="http://arxiv.org/find/cs/1/au:+Dumas_M/0/1/0/all/0/1">Marlon Dumas</a>, <a href="http://arxiv.org/find/cs/1/au:+Milani_F/0/1/0/all/0/1">Fredrik Milani</a></p>
<p>Predictive process monitoring is an evolving research field that studies how
to train and use predictive models for operational decision-making. One of the
problems studied in this field is that of predicting the sequence of upcoming
activities in a case up to its completion, a.k.a. the case suffix. The
prediction of case suffixes provides input to estimate short-term workloads and
execution times under different resource schedules. Existing methods to address
this problem often generate suffixes wherein some activities are repeated many
times, whereas this pattern is not observed in the data. Closer examination
shows that this shortcoming stems from the approach used to sample the
successive activity instances to generate a case suffix. Accordingly, the paper
introduces a sampling approach aimed at reducing repetitions of activities in
the predicted case suffixes. The approach, namely Daemon action, strikes a
balance between exploration and exploitation when generating the successive
activity instances. We enhance a deep learning approach for case suffix
predictions using this sampling approach, and experimentally show that the
enhanced approach outperforms the unenhanced ones with respect to control-flow
accuracy measures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05568">Sparse Variational Student-t Processes. (arXiv:2312.05568v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jian Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1">Delu Zeng</a></p>
<p>The theory of Bayesian learning incorporates the use of Student-t Processes
to model heavy-tailed distributions and datasets with outliers. However,
despite Student-t Processes having a similar computational complexity as
Gaussian Processes, there has been limited emphasis on the sparse
representation of this model. This is mainly due to the increased difficulty in
modeling and computation compared to previous sparse Gaussian Processes. Our
motivation is to address the need for a sparse representation framework that
reduces computational complexity, allowing Student-t Processes to be more
flexible for real-world datasets. To achieve this, we leverage the conditional
distribution of Student-t Processes to introduce sparse inducing points.
Bayesian methods and variational inference are then utilized to derive a
well-defined lower bound, facilitating more efficient optimization of our model
through stochastic gradient descent. We propose two methods for computing the
variational lower bound, one utilizing Monte Carlo sampling and the other
employing Jensen's inequality to compute the KL regularization term in the loss
function. We propose adopting these approaches as viable alternatives to
Gaussian processes when the data might contain outliers or exhibit heavy-tailed
behavior, and we provide specific recommendations for their applicability. We
evaluate the two proposed approaches on various synthetic and real-world
datasets from UCI and Kaggle, demonstrating their effectiveness compared to
baseline methods in terms of computational complexity and accuracy, as well as
their robustness to outliers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05571">Frugal LMs Trained to Invoke Symbolic Solvers Achieve Parameter-Efficient Arithmetic Reasoning. (arXiv:2312.05571v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1">Subhabrata Dutta</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1">Joykirat Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Pandey_I/0/1/0/all/0/1">Ishan Pandey</a>, <a href="http://arxiv.org/find/cs/1/au:+Manchanda_S/0/1/0/all/0/1">Sunny Manchanda</a>, <a href="http://arxiv.org/find/cs/1/au:+Chakrabarti_S/0/1/0/all/0/1">Soumen Chakrabarti</a>, <a href="http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1">Tanmoy Chakraborty</a></p>
<p>Large Language Models (LLM) exhibit zero-shot mathematical reasoning capacity
as a behavior emergent with scale, commonly manifesting as chain-of-thoughts
(CoT) reasoning. However, multiple empirical findings suggest that this prowess
is exclusive to LLMs with exorbitant sizes (beyond 50 billion parameters).
Meanwhile, educational neuroscientists suggest that symbolic algebraic
manipulation be introduced around the same time as arithmetic word problems to
modularize language-to-formulation, symbolic manipulation of the formulation,
and endgame arithmetic. In this paper, we start with the hypothesis that much
smaller LMs, which are weak at multi-step reasoning, can achieve reasonable
arithmetic reasoning if arithmetic word problems are posed as a
formalize-then-solve task. In our architecture, which we call SYRELM, the LM
serves the role of a translator to map natural language arithmetic questions
into a formal language (FL) description. A symbolic solver then evaluates the
FL expression to obtain the answer. A small frozen LM, equipped with an
efficient low-rank adapter, is capable of generating FL expressions that
incorporate natural language descriptions of the arithmetic problem (e.g.,
variable names and their purposes, formal expressions combining variables,
etc.). We adopt policy-gradient reinforcement learning to train the adapted LM,
informed by the non-differentiable symbolic solver. This marks a sharp
departure from the recent development in tool-augmented LLMs, in which the
external tools (e.g., calculator, Web search, etc.) are essentially detached
from the learning phase of the LM. SYRELM shows massive improvements (e.g.,
+30.65 absolute point improvement in accuracy on the SVAMP dataset using GPT-J
6B model) over base LMs, while keeping our testbed easy to diagnose, interpret
and within reach of most researchers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05573">Revisiting RIP guarantees for sketching operators on mixture models. (arXiv:2312.05573v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Belhadji_A/0/1/0/all/0/1">Ayoub Belhadji</a>, <a href="http://arxiv.org/find/stat/1/au:+Gribonval_R/0/1/0/all/0/1">R&#xe9;mi Gribonval</a></p>
<p>In the context of sketching for compressive mixture modeling, we revisit
existing proofs of the Restricted Isometry Property of sketching operators with
respect to certain mixtures models. After examining the shortcomings of
existing guarantees, we propose an alternative analysis that circumvents the
need to assume importance sampling when drawing random Fourier features to
build random sketching operators. Our analysis is based on new deterministic
bounds on the restricted isometry constant that depend solely on the set of
frequencies used to define the sketching operator; then we leverage these
bounds to establish concentration inequalities for random sketching operators
that lead to the desired RIP guarantees. Our analysis also opens the door to
theoretical guarantees for structured sketching with frequencies associated to
fast random linear operators.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05576">Dynamic Adjustment of Matching Radii under the Broadcasting Mode: A Novel Multitask Learning Strategy and Temporal Modeling Approach. (arXiv:2312.05576v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Taijie Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1">Zijian Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1">Siyuan Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Linchuan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ke_J/0/1/0/all/0/1">Jintao Ke</a></p>
<p>As ride-hailing services have experienced significant growth, the majority of
research has concentrated on the dispatching mode, where drivers must adhere to
the platform's assigned routes. However, the broadcasting mode, in which
drivers can freely choose their preferred orders from those broadcast by the
platform, has received less attention. One important but challenging task in
such a system is the determination of the optimal matching radius, which
usually varies across space, time, and real-time supply/demand characteristics.
This study develops a Transformer-Encoder-Based (TEB) model that predicts key
system performance metrics for a range of matching radii, which enables the
ride-hailing platform to select an optimal matching radius that maximizes
overall system performance according to real-time supply and demand
information. To simultaneously maximize multiple system performance metrics for
matching radius determination, we devise a novel multi-task learning algorithm
that enhances convergence speed of each task (corresponding to the optimization
of one metric) and delivers more accurate overall predictions. We evaluate our
methods in a simulation environment specifically designed for
broadcasting-mode-based ride-hailing service. Our findings reveal that
dynamically adjusting matching radii based on our proposed
predict-then-optimize approach significantly improves system performance, e.g.,
increasing platform revenue by 7.55% and enhancing order fulfillment rate by
13% compared to benchmark algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05579">Conditional Stochastic Interpolation for Generative Learning. (arXiv:2312.05579v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Huang_D/0/1/0/all/0/1">Ding Huang</a>, <a href="http://arxiv.org/find/stat/1/au:+Huang_J/0/1/0/all/0/1">Jian Huang</a>, <a href="http://arxiv.org/find/stat/1/au:+Li_T/0/1/0/all/0/1">Ting Li</a>, <a href="http://arxiv.org/find/stat/1/au:+Shen_G/0/1/0/all/0/1">Guohao Shen</a></p>
<p>We propose a conditional stochastic interpolation (CSI) approach to learning
conditional distributions. CSI learns probability flow equations or stochastic
differential equations that transport a reference distribution to the target
conditional distribution. This is achieved by first learning the drift function
and the conditional score function based on conditional stochastic
interpolation, which are then used to construct a deterministic process
governed by an ordinary differential equation or a diffusion process for
conditional sampling. In our proposed CSI model, we incorporate an adaptive
diffusion term to address the instability issues arising during the training
process. We provide explicit forms of the conditional score function and the
drift function in terms of conditional expectations under mild conditions,
which naturally lead to an nonparametric regression approach to estimating
these functions. Furthermore, we establish non-asymptotic error bounds for
learning the target conditional distribution via conditional stochastic
interpolation in terms of KL divergence, taking into account the neural network
approximation error. We illustrate the application of CSI on image generation
using a benchmark image dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05583">Better Neural PDE Solvers Through Data-Free Mesh Movers. (arXiv:2312.05583v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1">Peiyan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yue Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1">Zhi-Ming Ma</a></p>
<p>Recently, neural networks have been extensively employed to solve partial
differential equations (PDEs) in physical system modeling. While major studies
focus on learning system evolution on predefined static mesh discretizations,
some methods utilize reinforcement learning or supervised learning techniques
to create adaptive and dynamic meshes, due to the dynamic nature of these
systems. However, these approaches face two primary challenges: (1) the need
for expensive optimal mesh data, and (2) the change of the solution space's
degree of freedom and topology during mesh refinement. To address these
challenges, this paper proposes a neural PDE solver with a neural mesh adapter.
To begin with, we introduce a novel data-free neural mesh adaptor, called
Data-free Mesh Mover (DMM), with two main innovations. Firstly, it is an
operator that maps the solution to adaptive meshes and is trained using the
Monge-Ampere equation without optimal mesh data. Secondly, it dynamically
changes the mesh by moving existing nodes rather than adding or deleting nodes
and edges. Theoretical analysis shows that meshes generated by DMM have the
lowest interpolation error bound. Based on DMM, to efficiently and accurately
model dynamic systems, we develop a moving mesh based neural PDE solver
(MM-PDE) that embeds the moving mesh with a two-branch architecture and a
learnable interpolation framework to preserve information within the data.
Empirical experiments demonstrate that our method generates suitable meshes and
considerably enhances accuracy when modeling widely considered PDE systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05585">Enhancing Medical Specialty Assignment to Patients using NLP Techniques. (arXiv:2312.05585v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Solomou_C/0/1/0/all/0/1">Chris Solomou</a></p>
<p>The introduction of Large Language Models (LLMs), and the vast volume of
publicly available medical data, amplified the application of NLP to the
medical domain. However, LLMs are pretrained on data that are not explicitly
relevant to the domain that are applied to and are often biased towards the
original data they were pretrained upon. Even when pretrained on domainspecific
data, these models typically require time-consuming fine-tuning to achieve good
performance for a specific task. To address these limitations, we propose an
alternative approach that achieves superior performance while being
computationally efficient. Specifically, we utilize keywords to train a deep
learning architecture that outperforms a language model pretrained on a large
corpus of text. Our proposal does not require pretraining nor fine-tuning and
can be applied directly to a specific setting for performing multi-label
classification. Our objective is to automatically assign a new patient to the
specialty of the medical professional they require, using a dataset that
contains medical transcriptions and relevant keywords. To this end, we
fine-tune the PubMedBERT model on this dataset, which serves as the baseline
for our experiments. We then twice train/fine-tune a DNN and the RoBERTa
language model, using both the keywords and the full transcriptions as input.
We compare the performance of these approaches using relevant metrics. Our
results demonstrate that utilizing keywords for text classification
significantly improves classification performance, for both a basic DL
architecture and a large language model. Our approach represents a promising
and efficient alternative to traditional methods for finetuning language models
on domain-specific data and has potential applications in various medical
domains
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05586">Deeper Understanding of Black-box Predictions via Generalized Influence Functions. (arXiv:2312.05586v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lyu_H/0/1/0/all/0/1">Hyeonsu Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1">Jonggyu Jang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ryu_S/0/1/0/all/0/1">Sehyun Ryu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Hyun Jong Yang</a></p>
<p>Influence functions (IFs) elucidate how learning data affects model behavior.
However, growing non-convexity and the number of parameters in modern
large-scale models lead to imprecise influence approximation and instability in
computations. We highly suspect that the first-order approximation in large
models causes such fragility, as IFs change all parameters including possibly
nuisance parameters that are irrelevant to the examined data. Thus, we attempt
to selectively analyze parameters associated with the data. However, simply
computing influence from the chosen parameters can be misleading, as it fails
to nullify the subliminal impact of unselected parameters. Our approach
introduces generalized IFs, precisely estimating target parameters' influence
while considering fixed parameters' effects. Unlike the classic IFs, we newly
adopt a method to identify pertinent target parameters closely associated with
the analyzed data. Furthermore, we tackle computational instability with a
robust inverse-Hessian-vector product approximation. Remarkably, the proposed
approximation algorithm guarantees convergence regardless of the network
configurations. We evaluated our approach on ResNet-18 and VGG-11 for class
removal and backdoor model recovery. Modifying just 10\% of the network yields
results comparable to the network retrained from scratch. Aligned with our
first guess, we also confirm that modifying an excessive number of parameters
results in a decline in network utility. We believe our proposal can become a
versatile tool for model analysis across various AI domains, appealing to both
specialists and general readers. Codes are available at
https://github.com/hslyu/GIF.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05596">Factorized Explainer for Graph Neural Networks. (arXiv:2312.05596v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1">Rundong Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shirani_F/0/1/0/all/0/1">Farhad Shirani</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1">Dongsheng Luo</a></p>
<p>Graph Neural Networks (GNNs) have received increasing attention due to their
ability to learn from graph-structured data. To open the black-box of these
deep learning models, post-hoc instance-level explanation methods have been
proposed to understand GNN predictions. These methods seek to discover
substructures that explain the prediction behavior of a trained GNN. In this
paper, we show analytically that for a large class of explanation tasks,
conventional approaches, which are based on the principle of graph information
bottleneck (GIB), admit trivial solutions that do not align with the notion of
explainability. Instead, we argue that a modified GIB principle may be used to
avoid the aforementioned trivial solutions. We further introduce a novel
factorized explanation model with theoretical performance guarantees. The
modified GIB is used to analyze the structural properties of the proposed
factorized explainer. We conduct extensive experiments on both synthetic and
real-world datasets to validate the effectiveness of our proposed factorized
explainer over existing approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05598">Boosting the Cross-Architecture Generalization of Dataset Distillation through an Empirical Study. (arXiv:2312.05598v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1">Lirui Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuxin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1">Mingbao Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1">Fei Chao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1">Rongrong Ji</a></p>
<p>The poor cross-architecture generalization of dataset distillation greatly
weakens its practical significance. This paper attempts to mitigate this issue
through an empirical study, which suggests that the synthetic datasets undergo
an inductive bias towards the distillation model. Therefore, the evaluation
model is strictly confined to having similar architectures of the distillation
model. We propose a novel method of EvaLuation with distillation Feature (ELF),
which utilizes features from intermediate layers of the distillation model for
the cross-architecture evaluation. In this manner, the evaluation model learns
from bias-free knowledge therefore its architecture becomes unfettered while
retaining performance. By performing extensive experiments, we successfully
prove that ELF can well enhance the cross-architecture generalization of
current DD methods. Code of this project is at
\url{https://github.com/Lirui-Zhao/ELF}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05599">Not All Data Matters: An End-to-End Adaptive Dataset Pruning Framework for Enhancing Model Performance and Efficiency. (arXiv:2312.05599v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Suorong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Hongchao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1">Suhan Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_F/0/1/0/all/0/1">Furao Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jian Zhao</a></p>
<p>While deep neural networks have demonstrated remarkable performance across
various tasks, they typically require massive training data. Due to the
presence of redundancies and biases in real-world datasets, not all data in the
training dataset contributes to the model performance. To address this issue,
dataset pruning techniques have been introduced to enhance model performance
and efficiency by eliminating redundant training samples and reducing
computational and memory overhead. However, previous works most rely on
manually crafted scalar scores, limiting their practical performance and
scalability across diverse deep networks and datasets. In this paper, we
propose AdaPruner, an end-to-end Adaptive DAtaset PRUNing framEwoRk. AdaPruner
can perform effective dataset pruning without the need for explicitly defined
metrics. Our framework jointly prunes training data and fine-tunes models with
task-specific optimization objectives. AdaPruner leverages (1) An adaptive
dataset pruning (ADP) module, which iteratively prunes redundant samples to an
expected pruning ratio; and (2) A pruning performance controller (PPC) module,
which optimizes the model performance for accurate pruning. Therefore,
AdaPruner exhibits high scalability and compatibility across various datasets
and deep networks, yielding improved dataset distribution and enhanced model
performance. AdaPruner can still significantly enhance model performance even
after pruning up to 10-30\% of the training data. Notably, these improvements
are accompanied by substantial savings in memory and computation costs.
Qualitative and quantitative experiments suggest that AdaPruner outperforms
other state-of-the-art dataset pruning methods by a large margin.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/1907.13463">Nonconvex Zeroth-Order Stochastic ADMM Methods with Lower Function Query Complexity. (arXiv:1907.13463v4 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1">Feihu Huang</a>, <a href="http://arxiv.org/find/math/1/au:+Gao_S/0/1/0/all/0/1">Shangqian Gao</a>, <a href="http://arxiv.org/find/math/1/au:+Pei_J/0/1/0/all/0/1">Jian Pei</a>, <a href="http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1">Heng Huang</a></p>
<p>Zeroth-order (a.k.a, derivative-free) methods are a class of effective
optimization methods for solving complex machine learning problems, where
gradients of the objective functions are not available or computationally
prohibitive. Recently, although many zeroth-order methods have been developed,
these approaches still have two main drawbacks: 1) high function query
complexity; 2) not being well suitable for solving the problems with complex
penalties and constraints. To address these challenging drawbacks, in this
paper, we propose a class of faster zeroth-order stochastic alternating
direction method of multipliers (ADMM) methods (ZO-SPIDER-ADMM) to solve the
nonconvex finite-sum problems with multiple nonsmooth penalties. Moreover, we
prove that the ZO-SPIDER-ADMM methods can achieve a lower function query
complexity of $O(nd+dn^{\frac{1}{2}}\epsilon^{-1})$ for finding an
$\epsilon$-stationary point, which improves the existing best nonconvex
zeroth-order ADMM methods by a factor of $O(d^{\frac{1}{3}}n^{\frac{1}{6}})$,
where $n$ and $d$ denote the sample size and data dimension, respectively. At
the same time, we propose a class of faster zeroth-order online ADMM methods
(ZOO-ADMM+) to solve the nonconvex online problems with multiple nonsmooth
penalties. We also prove that the proposed ZOO-ADMM+ methods achieve a lower
function query complexity of $O(d\epsilon^{-\frac{3}{2}})$, which improves the
existing best result by a factor of $O(\epsilon^{-\frac{1}{2}})$. Extensive
experimental results on the structure adversarial attack on black-box deep
neural networks demonstrate the efficiency of our new algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2010.00137">Efficient sampling from the Bingham distribution. (arXiv:2010.00137v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ge_R/0/1/0/all/0/1">Rong Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Holden Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Jianfeng Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Risteski_A/0/1/0/all/0/1">Andrej Risteski</a></p>
<p>We give a algorithm for exact sampling from the Bingham distribution
$p(x)\propto \exp(x^\top A x)$ on the sphere $\mathcal S^{d-1}$ with expected
runtime of $\operatorname{poly}(d, \lambda_{\max}(A)-\lambda_{\min}(A))$. The
algorithm is based on rejection sampling, where the proposal distribution is a
polynomial approximation of the pdf, and can be sampled from by explicitly
evaluating integrals of polynomials over the sphere. Our algorithm gives exact
samples, assuming exact computation of an inverse function of a polynomial.
This is in contrast with Markov Chain Monte Carlo algorithms, which are not
known to enjoy rapid mixing on this problem, and only give approximate samples.
</p>
<p>As a direct application, we use this to sample from the posterior
distribution of a rank-1 matrix inference problem in polynomial time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2103.05706">A sampling criterion for constrained Bayesian optimization with uncertainties. (arXiv:2103.05706v4 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Amri_R/0/1/0/all/0/1">Reda El Amri</a>, <a href="http://arxiv.org/find/stat/1/au:+Riche_R/0/1/0/all/0/1">Rodolphe Le Riche</a>, <a href="http://arxiv.org/find/stat/1/au:+Helbert_C/0/1/0/all/0/1">C&#xe9;line Helbert</a>, <a href="http://arxiv.org/find/stat/1/au:+Blanchet_Scalliet_C/0/1/0/all/0/1">Christophette Blanchet-Scalliet</a>, <a href="http://arxiv.org/find/stat/1/au:+Veiga_S/0/1/0/all/0/1">S&#xe9;bastien Da Veiga</a></p>
<p>We consider the problem of chance constrained optimization where it is sought
to optimize a function and satisfy constraints, both of which are affected by
uncertainties. The real world declinations of this problem are particularly
challenging because of their inherent computational cost.
</p>
<p>To tackle such problems, we propose a new Bayesian optimization method. It
applies to the situation where the uncertainty comes from some of the inputs,
so that it becomes possible to define an acquisition criterion in the joint
controlled-uncontrolled input space. The main contribution of this work is an
acquisition criterion that accounts for both the average improvement in
objective function and the constraint reliability. The criterion is derived
following the Stepwise Uncertainty Reduction logic and its maximization
provides both optimal controlled and uncontrolled parameters. Analytical
expressions are given to efficiently calculate the criterion. Numerical studies
on test functions are presented. It is found through experimental comparisons
with alternative sampling criteria that the adequation between the sampling
criterion and the problem contributes to the efficiency of the overall
optimization. As a side result, an expression for the variance of the
improvement is given.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2106.04088">A Lightweight and Gradient-Stable Neural Layer. (arXiv:2106.04088v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yueyao Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yin Zhang</a></p>
<p>We propose a neural-layer architecture based on Householder weighting and
absolute-value activating, hence called Householder-absolute neural layer or
simply Han-layer. Compared to a fully connected layer with $d$-neurons and $d$
outputs, a Han-layer reduces the number of parameters and the corresponding
complexity from $O(d^2)$ to $O(d)$. The Han-layer structure guarantees two
desirable properties: (1) gradient stability (free of vanishing or exploding
gradient), and (2) 1-Lipschitz continuity. Extensive numerical experiments show
that one can strategically use Han-layers to replace fully connected (FC)
layers, reducing the number of model parameters while maintaining or even
improving the generalization performance. We will also showcase the
capabilities of the Han-layer architecture on a few small stylized models, and
discuss its current limitations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2110.04698">A Closer Look at Advantage-Filtered Behavioral Cloning in High-Noise Datasets. (arXiv:2110.04698v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Grigsby_J/0/1/0/all/0/1">Jake Grigsby</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1">Yanjun Qi</a></p>
<p>Recent Offline Reinforcement Learning methods have succeeded in learning
high-performance policies from fixed datasets of experience. A particularly
effective approach learns to first identify and then mimic optimal
decision-making strategies. Our work evaluates this method's ability to scale
to vast datasets consisting almost entirely of sub-optimal noise. A thorough
investigation on a custom benchmark helps identify several key challenges
involved in learning from high-noise datasets. We re-purpose prioritized
experience sampling to locate expert-level demonstrations among millions of
low-performance samples. This modification enables offline agents to learn
state-of-the-art policies in benchmark tasks using datasets where expert
actions are outnumbered nearly 65:1.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2110.11486">Boosting Federated Learning in Resource-Constrained Networks. (arXiv:2110.11486v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Boukhari_M/0/1/0/all/0/1">Mohamed Yassine Boukhari</a>, <a href="http://arxiv.org/find/cs/1/au:+Dhasade_A/0/1/0/all/0/1">Akash Dhasade</a>, <a href="http://arxiv.org/find/cs/1/au:+Kermarrec_A/0/1/0/all/0/1">Anne-Marie Kermarrec</a>, <a href="http://arxiv.org/find/cs/1/au:+Pires_R/0/1/0/all/0/1">Rafael Pires</a>, <a href="http://arxiv.org/find/cs/1/au:+Safsafi_O/0/1/0/all/0/1">Othmane Safsafi</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1">Rishi Sharma</a></p>
<p>Federated learning (FL) enables a set of client devices to collaboratively
train a model without sharing raw data. This process, though, operates under
the constrained computation and communication resources of edge devices. These
constraints combined with systems heterogeneity force some participating
clients to perform fewer local updates than expected by the server, thus
slowing down convergence. Exhaustive tuning of hyperparameters in FL,
furthermore, can be resource-intensive, without which the convergence is
adversely affected. In this work, we propose GeL, the guess and learn
algorithm. GeL enables constrained edge devices to perform additional learning
through guessed updates on top of gradient-based steps. These guesses are
gradientless, i.e., participating clients leverage them for free. Our generic
guessing algorithm (i) can be flexibly combined with several state-of-the-art
algorithms including FedProx, FedNova or FedYogi; and (ii) achieves
significantly improved performance when the learning rates are not best tuned.
We conduct extensive experiments and show that GeL can boost empirical
convergence by up to 40% in resource-constrained networks while relieving the
need for exhaustive learning rate tuning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2201.07063">How to Backdoor HyperNetwork in Personalized Federated Learning?. (arXiv:2201.07063v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lai_P/0/1/0/all/0/1">Phung Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+Phan_N/0/1/0/all/0/1">NhatHai Phan</a>, <a href="http://arxiv.org/find/cs/1/au:+Khalil_I/0/1/0/all/0/1">Issa Khalil</a>, <a href="http://arxiv.org/find/cs/1/au:+Khreishah_A/0/1/0/all/0/1">Abdallah Khreishah</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xintao Wu</a></p>
<p>This paper explores previously unknown backdoor risks in HyperNet-based
personalized federated learning (HyperNetFL) through poisoning attacks. Based
upon that, we propose a novel model transferring attack (called HNTroj), i.e.,
the first of its kind, to transfer a local backdoor infected model to all
legitimate and personalized local models, which are generated by the HyperNetFL
model, through consistent and effective malicious local gradients computed
across all compromised clients in the whole training process. As a result,
HNTroj reduces the number of compromised clients needed to successfully launch
the attack without any observable signs of sudden shifts or degradation
regarding model utility on legitimate data samples making our attack stealthy.
To defend against HNTroj, we adapted several backdoor-resistant FL training
algorithms into HyperNetFL. An extensive experiment that is carried out using
several benchmark datasets shows that HNTroj significantly outperforms data
poisoning and model replacement attacks and bypasses robust training algorithms
even with modest numbers of compromised clients.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.07490">Repairing Regressors for Fair Binary Classification at Any Decision Threshold. (arXiv:2203.07490v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kwegyir_Aggrey_K/0/1/0/all/0/1">Kweku Kwegyir-Aggrey</a>, <a href="http://arxiv.org/find/cs/1/au:+Cooper_A/0/1/0/all/0/1">A. Feder Cooper</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1">Jessica Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Dickerson_J/0/1/0/all/0/1">John Dickerson</a>, <a href="http://arxiv.org/find/cs/1/au:+Hines_K/0/1/0/all/0/1">Keegan Hines</a>, <a href="http://arxiv.org/find/cs/1/au:+Venkatasubramanian_S/0/1/0/all/0/1">Suresh Venkatasubramanian</a></p>
<p>We study the problem of post-processing a supervised machine-learned
regressor to maximize fair binary classification at all decision thresholds. By
decreasing the statistical distance between each group's score distributions,
we show that we can increase fair performance across all thresholds at once,
and that we can do so without a large decrease in accuracy. To this end, we
introduce a formal measure of Distributional Parity, which captures the degree
of similarity in the distributions of classifications for different protected
groups. Our main result is to put forward a novel post-processing algorithm
based on optimal transport, which provably maximizes Distributional Parity,
thereby attaining common notions of group fairness like Equalized Odds or Equal
Opportunity at all thresholds. We demonstrate on two fairness benchmarks that
our technique works well empirically, while also outperforming and generalizing
similar techniques from related work.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.13086">HiFi++: a Unified Framework for Bandwidth Extension and Speech Enhancement. (arXiv:2203.13086v4 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Andreev_P/0/1/0/all/0/1">Pavel Andreev</a>, <a href="http://arxiv.org/find/cs/1/au:+Alanov_A/0/1/0/all/0/1">Aibek Alanov</a>, <a href="http://arxiv.org/find/cs/1/au:+Ivanov_O/0/1/0/all/0/1">Oleg Ivanov</a>, <a href="http://arxiv.org/find/cs/1/au:+Vetrov_D/0/1/0/all/0/1">Dmitry Vetrov</a></p>
<p>Generative adversarial networks have recently demonstrated outstanding
performance in neural vocoding outperforming best autoregressive and flow-based
models. In this paper, we show that this success can be extended to other tasks
of conditional audio generation. In particular, building upon HiFi vocoders, we
propose a novel HiFi++ general framework for bandwidth extension and speech
enhancement. We show that with the improved generator architecture, HiFi++
performs better or comparably with the state-of-the-art in these tasks while
spending significantly less computational resources. The effectiveness of our
approach is validated through a series of extensive experiments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.10697">Lassoed Tree Boosting. (arXiv:2205.10697v6 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Schuler_A/0/1/0/all/0/1">Alejandro Schuler</a>, <a href="http://arxiv.org/find/stat/1/au:+Li_Y/0/1/0/all/0/1">Yi Li</a>, <a href="http://arxiv.org/find/stat/1/au:+Laan_M/0/1/0/all/0/1">Mark van der Laan</a></p>
<p>Gradient boosting performs exceptionally in most prediction problems and
scales well to large datasets. In this paper we prove that a ``lassoed''
gradient boosted tree algorithm with early stopping achieves faster than
$n^{-1/4}$ L2 convergence in the large nonparametric space of cadlag functions
of bounded sectional variation. This rate is remarkable because it does not
depend on the dimension, sparsity, or smoothness. We use simulation and real
data to confirm our theory and demonstrate empirical performance and
scalability on par with standard boosting. Our convergence proofs are based on
a novel, general theorem on early stopping with empirical loss minimizers of
nested Donsker classes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.01818">QAGCN: Answering Multi-Relation Questions via Single-Step Implicit Reasoning over Knowledge Graphs. (arXiv:2206.01818v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Ruijie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Rossetto_L/0/1/0/all/0/1">Luca Rossetto</a>, <a href="http://arxiv.org/find/cs/1/au:+Cochez_M/0/1/0/all/0/1">Michael Cochez</a>, <a href="http://arxiv.org/find/cs/1/au:+Bernstein_A/0/1/0/all/0/1">Abraham Bernstein</a></p>
<p>Multi-relation question answering (QA) is a challenging task, where given
questions usually require long reasoning chains in KGs that consist of multiple
relations. Recently, methods with explicit multi-step reasoning over KGs have
been prominently used in this task and have demonstrated promising performance.
Examples include methods that perform stepwise label propagation through KG
triples and methods that navigate over KG triples based on reinforcement
learning. A main weakness of these methods is that their reasoning mechanisms
are usually complex and difficult to implement or train. In this paper, we
argue that multi-relation QA can be achieved via end-to-end single-step
implicit reasoning, which is simpler, more efficient, and easier to adopt. We
propose QAGCN -- a Question-Aware Graph Convolutional Network (GCN)-based
method that includes a novel GCN architecture with controlled
question-dependent message propagation for the implicit reasoning. Extensive
experiments have been conducted, where QAGCN achieved competitive and even
superior performance compared to state-of-the-art explicit-reasoning methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.09098">Existence and Minimax Theorems for Adversarial Surrogate Risks in Binary Classification. (arXiv:2206.09098v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Frank_N/0/1/0/all/0/1">Natalie S. Frank</a>, <a href="http://arxiv.org/find/cs/1/au:+Niles_Weed_J/0/1/0/all/0/1">Jonathan Niles-Weed</a></p>
<p>Adversarial training is one of the most popular methods for training methods
robust to adversarial attacks, however, it is not well-understood from a
theoretical perspective. We prove and existence, regularity, and minimax
theorems for adversarial surrogate risks. Our results explain some empirical
observations on adversarial robustness from prior work and suggest new
directions in algorithm development. Furthermore, our results extend previously
known existence and minimax theorems for the adversarial classification risk to
surrogate risks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.13489">Supply-Side Equilibria in Recommender Systems. (arXiv:2206.13489v3 [cs.GT] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jagadeesan_M/0/1/0/all/0/1">Meena Jagadeesan</a>, <a href="http://arxiv.org/find/cs/1/au:+Garg_N/0/1/0/all/0/1">Nikhil Garg</a>, <a href="http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1">Jacob Steinhardt</a></p>
<p>Algorithmic recommender systems such as Spotify and Netflix affect not only
consumer behavior but also producer incentives. Producers seek to create
content that will be shown by the recommendation algorithm, which can impact
both the diversity and quality of their content. In this work, we investigate
the resulting supply-side equilibria in personalized content recommender
systems. We model users and content as $D$-dimensional vectors, the
recommendation algorithm as showing each user the content with highest dot
product, and producers as maximizing the number of users who are recommended
their content minus the cost of production. Two key features of our model are
that the producer decision space is multi-dimensional and the user base is
heterogeneous, which contrasts with classical low-dimensional models.
</p>
<p>Multi-dimensionality and heterogeneity create the potential for
specialization, where different producers create different types of content at
equilibrium. Using a duality argument, we derive necessary and sufficient
conditions for whether specialization occurs: these conditions depend on the
extent to which users are heterogeneous and to which producers can perform well
on all dimensions at once without incurring a high cost. Then, we characterize
the distribution of content at equilibrium in concrete settings with two
populations of users. Lastly, we show that specialization can enable producers
to achieve positive profit at equilibrium, which means that specialization can
reduce the competitiveness of the marketplace. At a conceptual level, our
analysis of supply-side competition takes a step towards elucidating how
personalized recommendations shape the marketplace of digital goods, and
towards understanding what new phenomena arise in multi-dimensional competitive
settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.03584">Generalization Guarantee of Training Graph Convolutional Networks with Graph Topology Sampling. (arXiv:2207.03584v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hongkang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Meng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Sijia Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1">Pin-Yu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1">Jinjun Xiong</a></p>
<p>Graph convolutional networks (GCNs) have recently achieved great empirical
success in learning graph-structured data. To address its scalability issue due
to the recursive embedding of neighboring features, graph topology sampling has
been proposed to reduce the memory and computational cost of training GCNs, and
it has achieved comparable test performance to those without topology sampling
in many empirical studies. To the best of our knowledge, this paper provides
the first theoretical justification of graph topology sampling in training (up
to) three-layer GCNs for semi-supervised node classification. We formally
characterize some sufficient conditions on graph topology sampling such that
GCN training leads to a diminishing generalization error. Moreover, our method
tackles the nonconvex interaction of weights across layers, which is
under-explored in the existing theoretical analyses of GCNs. This paper
characterizes the impact of graph structures and topology sampling on the
generalization performance and sample complexity explicitly, and the
theoretical findings are also justified through numerical experiments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.11005">AdaptCL: Adaptive Continual Learning for Tackling Heterogeneity in Sequential Datasets. (arXiv:2207.11005v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yuqing Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Saxena_D/0/1/0/all/0/1">Divya Saxena</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1">Jiannong Cao</a></p>
<p>Managing heterogeneous datasets that vary in complexity, size, and similarity
in continual learning presents a significant challenge. Task-agnostic continual
learning is necessary to address this challenge, as datasets with varying
similarity pose difficulties in distinguishing task boundaries. Conventional
task-agnostic continual learning practices typically rely on rehearsal or
regularization techniques. However, rehearsal methods may struggle with varying
dataset sizes and regulating the importance of old and new data due to rigid
buffer sizes. Meanwhile, regularization methods apply generic constraints to
promote generalization but can hinder performance when dealing with dissimilar
datasets lacking shared features, necessitating a more adaptive approach. In
this paper, we propose AdaptCL, a novel adaptive continual learning method to
tackle heterogeneity in sequential datasets. AdaptCL employs fine-grained
data-driven pruning to adapt to variations in data complexity and dataset size.
It also utilizes task-agnostic parameter isolation to mitigate the impact of
varying degrees of catastrophic forgetting caused by differences in data
similarity. Through a two-pronged case study approach, we evaluate AdaptCL on
both datasets of MNIST Variants and DomainNet, as well as datasets from
different domains. The latter include both large-scale, diverse binary-class
datasets and few-shot, multi-class datasets. Across all these scenarios,
AdaptCL consistently exhibits robust performance, demonstrating its flexibility
and general applicability in handling heterogeneous datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.10715">GANs and Closures: Micro-Macro Consistency in Multiscale Modeling. (arXiv:2208.10715v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Crabtree_E/0/1/0/all/0/1">Ellis R. Crabtree</a>, <a href="http://arxiv.org/find/cs/1/au:+Bello_Rivas_J/0/1/0/all/0/1">Juan M. Bello-Rivas</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferguson_A/0/1/0/all/0/1">Andrew L. Ferguson</a>, <a href="http://arxiv.org/find/cs/1/au:+Kevrekidis_I/0/1/0/all/0/1">Ioannis G. Kevrekidis</a></p>
<p>Sampling the phase space of molecular systems -- and, more generally, of
complex systems effectively modeled by stochastic differential equations -- is
a crucial modeling step in many fields, from protein folding to materials
discovery. These problems are often multiscale in nature: they can be described
in terms of low-dimensional effective free energy surfaces parametrized by a
small number of "slow" reaction coordinates; the remaining "fast" degrees of
freedom populate an equilibrium measure on the reaction coordinate values.
Sampling procedures for such problems are used to estimate effective free
energy differences as well as ensemble averages with respect to the conditional
equilibrium distributions; these latter averages lead to closures for effective
reduced dynamic models. Over the years, enhanced sampling techniques coupled
with molecular simulation have been developed. An intriguing analogy arises
with the field of Machine Learning (ML), where Generative Adversarial Networks
can produce high dimensional samples from low dimensional probability
distributions. This sample generation returns plausible high dimensional space
realizations of a model state, from information about its low-dimensional
representation. In this work, we present an approach that couples physics-based
simulations and biasing methods for sampling conditional distributions with
ML-based conditional generative adversarial networks for the same task. The
"coarse descriptors" on which we condition the fine scale realizations can
either be known a priori, or learned through nonlinear dimensionality
reduction. We suggest that this may bring out the best features of both
approaches: we demonstrate that a framework that couples cGANs with
physics-based enhanced sampling techniques can improve multiscale SDE dynamical
systems sampling, and even shows promise for systems of increasing complexity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.11435">Bidirectional Contrastive Split Learning for Visual Question Answering. (arXiv:2208.11435v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yuwei Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Ochiai_H/0/1/0/all/0/1">Hideya Ochiai</a></p>
<p>Visual Question Answering (VQA) based on multi-modal data facilitates
real-life applications such as home robots and medical diagnoses. One
significant challenge is to devise a robust decentralized learning framework
for various client models where centralized data collection is refrained due to
confidentiality concerns. This work aims to tackle privacy-preserving VQA by
decoupling a multi-modal model into representation modules and a contrastive
module and leveraging inter-module gradients sharing and inter-client weight
sharing. To this end, we propose Bidirectional Contrastive Split Learning
(BiCSL) to train a global multi-modal model on the entire data distribution of
decentralized clients. We employ the contrastive loss that enables a more
efficient self-supervised learning of decentralized modules. Comprehensive
experiments are conducted on the VQA-v2 dataset based on five SOTA VQA models,
demonstrating the effectiveness of the proposed method. Furthermore, we inspect
BiCSL's robustness against a dual-key backdoor attack on VQA. Consequently,
BiCSL shows much better robustness to the multi-modal adversarial attack
compared to the centralized learning method, which provides a promising
approach to decentralized multi-modal learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.08860">A Survey of Deep Causal Models and Their Industrial Applications. (arXiv:2209.08860v5 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Li_Z/0/1/0/all/0/1">Zongyu Li</a>, <a href="http://arxiv.org/find/stat/1/au:+Guo_X/0/1/0/all/0/1">Xiaobo Guo</a>, <a href="http://arxiv.org/find/stat/1/au:+Qiang_S/0/1/0/all/0/1">Siwei Qiang</a></p>
<p>The notion of causality assumes a paramount position within the realm of
human cognition. Over the past few decades, there has been significant
advancement in the domain of causal effect estimation across various
disciplines, including but not limited to computer science, medicine,
economics, and industrial applications. Given the continued advancements in
deep learning methodologies, there has been a notable surge in its utilization
for the estimation of causal effects using counterfactual data. Typically, deep
causal models map the characteristics of covariates to a representation space
and then design various objective functions to estimate counterfactual data
unbiasedly. Different from the existing surveys on causal models in machine
learning, this review mainly focuses on the overview of the deep causal models,
and its core contributions are as follows: 1) we cast insight on a
comprehensive overview of deep causal models from both timeline of development
and method classification perspectives; 2) we outline some typical applications
of causal effect estimation to industry; 3) we also endeavor to present a
detailed categorization and analysis on relevant datasets, source codes and
experiments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.15320">Bounded Robustness in Reinforcement Learning via Lexicographic Objectives. (arXiv:2209.15320v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ornia_D/0/1/0/all/0/1">Daniel Jarne Ornia</a>, <a href="http://arxiv.org/find/cs/1/au:+Romao_L/0/1/0/all/0/1">Licio Romao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hammond_L/0/1/0/all/0/1">Lewis Hammond</a>, <a href="http://arxiv.org/find/cs/1/au:+Mazo_M/0/1/0/all/0/1">Manuel Mazo Jr.</a>, <a href="http://arxiv.org/find/cs/1/au:+Abate_A/0/1/0/all/0/1">Alessandro Abate</a></p>
<p>Policy robustness in Reinforcement Learning may not be desirable at any cost:
the alterations caused by robustness requirements from otherwise optimal
policies should be explainable, quantifiable and formally verifiable. In this
work we study how policies can be maximally robust to arbitrary observational
noise by analysing how they are altered by this noise through a stochastic
linear operator interpretation of the disturbances, and establish connections
between robustness and properties of the noise kernel and of the underlying
MDPs. Then, we construct sufficient conditions for policy robustness, and
propose a robustness-inducing scheme, applicable to any policy gradient
algorithm, that formally trades off expected policy utility for robustness
through lexicographic optimisation, while preserving convergence and
sub-optimality in the policy synthesis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.01620">SAM as an Optimal Relaxation of Bayes. (arXiv:2210.01620v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mollenhoff_T/0/1/0/all/0/1">Thomas M&#xf6;llenhoff</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1">Mohammad Emtiyaz Khan</a></p>
<p>Sharpness-aware minimization (SAM) and related adversarial deep-learning
methods can drastically improve generalization, but their underlying mechanisms
are not yet fully understood. Here, we establish SAM as a relaxation of the
Bayes objective where the expected negative-loss is replaced by the optimal
convex lower bound, obtained by using the so-called Fenchel biconjugate. The
connection enables a new Adam-like extension of SAM to automatically obtain
reasonable uncertainty estimates, while sometimes also improving its accuracy.
By connecting adversarial and Bayesian methods, our work opens a new path to
robustness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.08964">PromptCast: A New Prompt-based Learning Paradigm for Time Series Forecasting. (arXiv:2210.08964v5 [stat.ME] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Xue_H/0/1/0/all/0/1">Hao Xue</a>, <a href="http://arxiv.org/find/stat/1/au:+Salim_F/0/1/0/all/0/1">Flora D. Salim</a></p>
<p>This paper presents a new perspective on time series forecasting. In existing
time series forecasting methods, the models take a sequence of numerical values
as input and yield numerical values as output. The existing SOTA models are
largely based on the Transformer architecture, modified with multiple encoding
mechanisms to incorporate the context and semantics around the historical data.
Inspired by the successes of pre-trained language foundation models, we pose a
question about whether these models can also be adapted to solve time-series
forecasting. Thus, we propose a new forecasting paradigm: prompt-based time
series forecasting (PromptCast). In this novel task, the numerical input and
output are transformed into prompts and the forecasting task is framed in a
sentence-to-sentence manner, making it possible to directly apply language
models for forecasting purposes. To support and facilitate the research of this
task, we also present a large-scale dataset (PISA) that includes three
real-world forecasting scenarios. We evaluate different SOTA numerical-based
forecasting methods and language generation models. The benchmark results with
various forecasting settings demonstrate the proposed PromptCast with language
generation models is a promising research direction. Additionally, in
comparison to conventional numerical-based forecasting, PromptCast shows a much
better generalization ability under the zero-shot setting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.01595">Reinforcement Learning in Non-Markovian Environments. (arXiv:2211.01595v3 [eess.SY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Chandak_S/0/1/0/all/0/1">Siddharth Chandak</a>, <a href="http://arxiv.org/find/eess/1/au:+Shah_P/0/1/0/all/0/1">Pratik Shah</a>, <a href="http://arxiv.org/find/eess/1/au:+Borkar_V/0/1/0/all/0/1">Vivek S Borkar</a>, <a href="http://arxiv.org/find/eess/1/au:+Dodhia_P/0/1/0/all/0/1">Parth Dodhia</a></p>
<p>Motivated by the novel paradigm developed by Van Roy and coauthors for
reinforcement learning in arbitrary non-Markovian environments, we propose a
related formulation and explicitly pin down the error caused by
non-Markovianity of observations when the Q-learning algorithm is applied on
this formulation. Based on this observation, we propose that the criterion for
agent design should be to seek good approximations for certain conditional
laws. Inspired by classical stochastic control, we show that our problem
reduces to that of recursive computation of approximate sufficient statistics.
This leads to an autoencoder-based scheme for agent design which is then
numerically tested on partially observed reinforcement learning environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.13436">Solving Bilevel Knapsack Problem using Graph Neural Networks. (arXiv:2211.13436v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kwon_S/0/1/0/all/0/1">Sunhyeon Kwon</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1">Hwayong Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1">Sungsoo Park</a></p>
<p>The Bilevel Optimization Problem is a hierarchical optimization problem with
two agents, a leader and a follower. The leader make their own decisions first,
and the followers make the best choices accordingly. The leader knows the
information of the followers, and the goal of the problem is to find the
optimal solution by considering the reactions of the followers from the
leader's point of view. For the Bilevel Optimization Problem, there are no
general and efficient algorithms or commercial solvers to get an optimal
solution, and it is very difficult to get a good solution even for a simple
problem. In this paper, we propose a deep learning approach using Graph Neural
Networks to solve the bilevel knapsack problem. We train the model to predict
the leader's solution and use it to transform the hierarchical optimization
problem into a single-level optimization problem to get the solution. Our model
found the feasible solution that was about 500 times faster than the exact
algorithm with $1.7\%$ optimal gap. Also, our model performed well on problems
of different size from the size it was trained on.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.13715">Trust Your $\nabla$: Gradient-based Intervention Targeting for Causal Discovery. (arXiv:2211.13715v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Olko_M/0/1/0/all/0/1">Mateusz Olko</a>, <a href="http://arxiv.org/find/stat/1/au:+Zajac_M/0/1/0/all/0/1">Micha&#x142; Zaj&#x105;c</a>, <a href="http://arxiv.org/find/stat/1/au:+Nowak_A/0/1/0/all/0/1">Aleksandra Nowak</a>, <a href="http://arxiv.org/find/stat/1/au:+Scherrer_N/0/1/0/all/0/1">Nino Scherrer</a>, <a href="http://arxiv.org/find/stat/1/au:+Annadani_Y/0/1/0/all/0/1">Yashas Annadani</a>, <a href="http://arxiv.org/find/stat/1/au:+Bauer_S/0/1/0/all/0/1">Stefan Bauer</a>, <a href="http://arxiv.org/find/stat/1/au:+Kucinski_L/0/1/0/all/0/1">&#x141;ukasz Kuci&#x144;ski</a>, <a href="http://arxiv.org/find/stat/1/au:+Milos_P/0/1/0/all/0/1">Piotr Mi&#x142;o&#x15b;</a></p>
<p>Inferring causal structure from data is a challenging task of fundamental
importance in science. Observational data are often insufficient to identify a
system's causal structure uniquely. While conducting interventions (i.e.,
experiments) can improve the identifiability, such samples are usually
challenging and expensive to obtain. Hence, experimental design approaches for
causal discovery aim to minimize the number of interventions by estimating the
most informative intervention target. In this work, we propose a novel
Gradient-based Intervention Targeting method, abbreviated GIT, that 'trusts'
the gradient estimator of a gradient-based causal discovery framework to
provide signals for the intervention acquisition function. We provide extensive
experiments in simulated and real-world datasets and demonstrate that GIT
performs on par with competitive baselines, surpassing them in the low-data
regime.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.00524">Learning Confident Classifiers in the Presence of Label Noise. (arXiv:2301.00524v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hashmi_A/0/1/0/all/0/1">Asma Ahmed Hashmi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhumabayeva_A/0/1/0/all/0/1">Aigerim Zhumabayeva</a>, <a href="http://arxiv.org/find/cs/1/au:+Kotelevskii_N/0/1/0/all/0/1">Nikita Kotelevskii</a>, <a href="http://arxiv.org/find/cs/1/au:+Agafonov_A/0/1/0/all/0/1">Artem Agafonov</a>, <a href="http://arxiv.org/find/cs/1/au:+Yaqub_M/0/1/0/all/0/1">Mohammad Yaqub</a>, <a href="http://arxiv.org/find/cs/1/au:+Panov_M/0/1/0/all/0/1">Maxim Panov</a>, <a href="http://arxiv.org/find/cs/1/au:+Takac_M/0/1/0/all/0/1">Martin Tak&#xe1;&#x10d;</a></p>
<p>The success of Deep Neural Network (DNN) models significantly depends on the
quality of provided annotations. In medical image segmentation, for example,
having multiple expert annotations for each data point is common to minimize
subjective annotation bias. Then, the goal of estimation is to filter out the
label noise and recover the ground-truth masks, which are not explicitly given.
This paper proposes a probabilistic model for noisy observations that allows us
to build a confident classification and segmentation models. To accomplish it,
we explicitly model label noise and introduce a new information-based
regularization that pushes the network to recover the ground-truth labels. In
addition, for segmentation task we adjust the loss function by prioritizing
learning in high-confidence regions where all the annotators agree on labeling.
We evaluate the proposed method on a series of classification tasks such as
noisy versions of MNIST, CIFAR-10, Fashion-MNIST datasets as well as CIFAR-10N,
which is real-world dataset with noisy human annotations. Additionally, for
segmentation task, we consider several medical imaging datasets, such as, LIDC
and RIGA that reflect real-world inter-variability among multiple annotators.
Our experiments show that our algorithm outperforms state-of-the-art solutions
for the considered classification and segmentation problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.10343">ClimaX: A foundation model for weather and climate. (arXiv:2301.10343v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Tung Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Brandstetter_J/0/1/0/all/0/1">Johannes Brandstetter</a>, <a href="http://arxiv.org/find/cs/1/au:+Kapoor_A/0/1/0/all/0/1">Ashish Kapoor</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_J/0/1/0/all/0/1">Jayesh K. Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1">Aditya Grover</a></p>
<p>Most state-of-the-art approaches for weather and climate modeling are based
on physics-informed numerical models of the atmosphere. These approaches aim to
model the non-linear dynamics and complex interactions between multiple
variables, which are challenging to approximate. Additionally, many such
numerical models are computationally intensive, especially when modeling the
atmospheric phenomenon at a fine-grained spatial and temporal resolution.
Recent data-driven approaches based on machine learning instead aim to directly
solve a downstream forecasting or projection task by learning a data-driven
functional mapping using deep neural networks. However, these networks are
trained using curated and homogeneous climate datasets for specific
spatiotemporal tasks, and thus lack the generality of numerical models. We
develop and demonstrate ClimaX, a flexible and generalizable deep learning
model for weather and climate science that can be trained using heterogeneous
datasets spanning different variables, spatio-temporal coverage, and physical
groundings. ClimaX extends the Transformer architecture with novel encoding and
aggregation blocks that allow effective use of available compute while
maintaining general utility. ClimaX is pre-trained with a self-supervised
learning objective on climate datasets derived from CMIP6. The pre-trained
ClimaX can then be fine-tuned to address a breadth of climate and weather
tasks, including those that involve atmospheric variables and spatio-temporal
scales unseen during pretraining. Compared to existing data-driven baselines,
we show that this generality in ClimaX results in superior performance on
benchmarks for weather forecasting and climate projections, even when
pretrained at lower resolutions and compute budgets. The source code is
available at https://github.com/microsoft/ClimaX.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.10403">Exact and rapid linear clustering of networks with dynamic programming. (arXiv:2301.10403v2 [cs.SI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Patania_A/0/1/0/all/0/1">Alice Patania</a>, <a href="http://arxiv.org/find/cs/1/au:+Allard_A/0/1/0/all/0/1">Antoine Allard</a>, <a href="http://arxiv.org/find/cs/1/au:+Young_J/0/1/0/all/0/1">Jean-Gabriel Young</a></p>
<p>We study the problem of clustering networks whose nodes have imputed or
physical positions in a single dimension, for example prestige hierarchies or
the similarity dimension of hyperbolic embeddings. Existing algorithms, such as
the critical gap method and other greedy strategies, only offer approximate
solutions to this problem. Here, we introduce a dynamic programming approach
that returns provably optimal solutions in polynomial time -- O(n^2) steps --
for a broad class of clustering objectives. We demonstrate the algorithm
through applications to synthetic and empirical networks and show that it
outperforms existing heuristics by a significant margin, with a similar
execution time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.12473">Large Language Models for Biomedical Knowledge Graph Construction: Information extraction from EMR notes. (arXiv:2301.12473v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Arsenyan_V/0/1/0/all/0/1">Vahan Arsenyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bughdaryan_S/0/1/0/all/0/1">Spartak Bughdaryan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shaya_F/0/1/0/all/0/1">Fadi Shaya</a>, <a href="http://arxiv.org/find/cs/1/au:+Small_K/0/1/0/all/0/1">Kent Small</a>, <a href="http://arxiv.org/find/cs/1/au:+Shahnazaryan_D/0/1/0/all/0/1">Davit Shahnazaryan</a></p>
<p>The automatic construction of knowledge graphs (KGs) is an important research
area in medicine, with far-reaching applications spanning drug discovery and
clinical trial design. These applications hinge on the accurate identification
of interactions among medical and biological entities. In this study, we
propose an end-to-end machine learning solution based on large language models
(LLMs) that utilize electronic medical record notes to construct KGs. The
entities used in the KG construction process are diseases, factors, treatments,
as well as manifestations that coexist with the patient while experiencing the
disease. Given the critical need for high-quality performance in medical
applications, we embark on a comprehensive assessment of 12 LLMs of various
architectures, evaluating their performance and safety attributes. To gauge the
quantitative efficacy of our approach by assessing both precision and recall,
we manually annotate a dataset provided by the Macula and Retina Institute. We
also assess the qualitative performance of LLMs, such as the ability to
generate structured outputs or the tendency to hallucinate. The results
illustrate that in contrast to encoder-only and encoder-decoder, decoder-only
LLMs require further investigation. Additionally, we provide guided prompt
design to utilize such LLMs. The application of the proposed methodology is
demonstrated on age-related macular degeneration.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.13589">Policy Gradient for Rectangular Robust Markov Decision Processes. (arXiv:2301.13589v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1">Navdeep Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Derman_E/0/1/0/all/0/1">Esther Derman</a>, <a href="http://arxiv.org/find/cs/1/au:+Geist_M/0/1/0/all/0/1">Matthieu Geist</a>, <a href="http://arxiv.org/find/cs/1/au:+Levy_K/0/1/0/all/0/1">Kfir Levy</a>, <a href="http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1">Shie Mannor</a></p>
<p>Policy gradient methods have become a standard for training reinforcement
learning agents in a scalable and efficient manner. However, they do not
account for transition uncertainty, whereas learning robust policies can be
computationally expensive. In this paper, we introduce robust policy gradient
(RPG), a policy-based method that efficiently solves rectangular robust Markov
decision processes (MDPs). We provide a closed-form expression for the worst
occupation measure. Incidentally, we find that the worst kernel is a rank-one
perturbation of the nominal. Combining the worst occupation measure with a
robust Q-value estimation yields an explicit form of the robust gradient. Our
resulting RPG can be estimated from data with the same time complexity as its
non-robust equivalent. Hence, it relieves the computational burden of convex
optimization problems required for training robust policies by current policy
gradient approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.00735">MTP-GO: Graph-Based Probabilistic Multi-Agent Trajectory Prediction with Neural ODEs. (arXiv:2302.00735v4 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Westny_T/0/1/0/all/0/1">Theodor Westny</a>, <a href="http://arxiv.org/find/cs/1/au:+Oskarsson_J/0/1/0/all/0/1">Joel Oskarsson</a>, <a href="http://arxiv.org/find/cs/1/au:+Olofsson_B/0/1/0/all/0/1">Bj&#xf6;rn Olofsson</a>, <a href="http://arxiv.org/find/cs/1/au:+Frisk_E/0/1/0/all/0/1">Erik Frisk</a></p>
<p>Enabling resilient autonomous motion planning requires robust predictions of
surrounding road users' future behavior. In response to this need and the
associated challenges, we introduce our model titled MTP-GO. The model encodes
the scene using temporal graph neural networks to produce the inputs to an
underlying motion model. The motion model is implemented using neural ordinary
differential equations where the state-transition functions are learned with
the rest of the model. Multimodal probabilistic predictions are obtained by
combining the concept of mixture density networks and Kalman filtering. The
results illustrate the predictive capabilities of the proposed model across
various data sets, outperforming several state-of-the-art methods on a number
of metrics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.03629">Ethical Considerations for Responsible Data Curation. (arXiv:2302.03629v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Andrews_J/0/1/0/all/0/1">Jerone T. A. Andrews</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1">Dora Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Thong_W/0/1/0/all/0/1">William Thong</a>, <a href="http://arxiv.org/find/cs/1/au:+Modas_A/0/1/0/all/0/1">Apostolos Modas</a>, <a href="http://arxiv.org/find/cs/1/au:+Papakyriakopoulos_O/0/1/0/all/0/1">Orestis Papakyriakopoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_A/0/1/0/all/0/1">Alice Xiang</a></p>
<p>Human-centric computer vision (HCCV) data curation practices often neglect
privacy and bias concerns, leading to dataset retractions and unfair models.
HCCV datasets constructed through nonconsensual web scraping lack crucial
metadata for comprehensive fairness and robustness evaluations. Current
remedies are post hoc, lack persuasive justification for adoption, or fail to
provide proper contextualization for appropriate application. Our research
focuses on proactive, domain-specific recommendations, covering purpose,
privacy and consent, and diversity, for curating HCCV evaluation datasets,
addressing privacy and bias concerns. We adopt an ante hoc reflective
perspective, drawing from current practices, guidelines, dataset withdrawals,
and audits, to inform our considerations and recommendations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.05583">Procedural generation of meta-reinforcement learning tasks. (arXiv:2302.05583v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Miconi_T/0/1/0/all/0/1">Thomas Miconi</a></p>
<p>Open-endedness stands to benefit from the ability to generate an infinite
variety of diverse, challenging environments. One particularly interesting type
of challenge is meta-learning ("learning-to-learn"), a hallmark of intelligent
behavior. However, the number of meta-learning environments in the literature
is limited. Here we describe a parametrized space for simple meta-reinforcement
learning (meta-RL) tasks with arbitrary stimuli. The parametrization allows us
to randomly generate an arbitrary number of novel simple meta-learning tasks.
The parametrization is expressive enough to include many well-known meta-RL
tasks, such as bandit problems, the Harlow task, T-mazes, the Daw two-step task
and others. Simple extensions allow it to capture tasks based on
two-dimensional topological spaces, such as full mazes or find-the-spot
domains. We describe a number of randomly generated meta-RL domains of varying
complexity and discuss potential issues arising from random generation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.05906">On Comparing Fair Classifiers under Data Bias. (arXiv:2302.05906v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sharma_M/0/1/0/all/0/1">Mohit Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1">Amit Deshpande</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1">Rajiv Ratn Shah</a></p>
<p>In this paper, we consider a theoretical model for injecting data bias,
namely, under-representation and label bias (Blum &amp; Stangl, 2019). We
empirically study the effect of varying data biases on the accuracy and
fairness of fair classifiers. Through extensive experiments on both synthetic
and real-world datasets (e.g., Adult, German Credit, Bank Marketing, COMPAS),
we empirically audit pre-, in-, and post-processing fair classifiers from
standard fairness toolkits for their fairness and accuracy by injecting varying
amounts of under-representation and label bias in their training data (but not
the test data). Our main observations are: 1. The fairness and accuracy of many
standard fair classifiers degrade severely as the bias injected in their
training data increases, 2. A simple logistic regression model trained on the
right data can often outperform, in both accuracy and fairness, most fair
classifiers trained on biased training data, and 3. A few, simple fairness
techniques (e.g., reweighing, exponentiated gradients) seem to offer stable
accuracy and fairness guarantees even when their training data is injected with
under-representation and label bias. Our experiments also show how to integrate
a measure of data bias risk in the existing fairness dashboards for real-world
deployments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.08015">Individual Fairness under Uncertainty. (arXiv:2302.08015v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenbin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zichong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Juyong Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1">Cheng Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Oommen_T/0/1/0/all/0/1">Thomas Oommen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ravikumar_P/0/1/0/all/0/1">Pradeep Ravikumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Weiss_J/0/1/0/all/0/1">Jeremy Weiss</a></p>
<p>Algorithmic fairness, the research field of making machine learning (ML)
algorithms fair, is an established area in ML. As ML technologies expand their
application domains, including ones with high societal impact, it becomes
essential to take fairness into consideration during the building of ML
systems. Yet, despite its wide range of socially sensitive applications, most
work treats the issue of algorithmic bias as an intrinsic property of
supervised learning, i.e., the class label is given as a precondition. Unlike
prior studies in fairness, we propose an individual fairness measure and a
corresponding algorithm that deal with the challenges of uncertainty arising
from censorship in class labels, while enforcing similar individuals to be
treated similarly from a ranking perspective, free of the Lipschitz condition
in the conventional individual fairness definition. We argue that this
perspective represents a more realistic model of fairness research for
real-world application deployment and show how learning with such a relaxed
precondition draws new insights that better explains algorithmic fairness. We
conducted experiments on four real-world datasets to evaluate our proposed
method compared to other fairness models, demonstrating its superiority in
minimizing discrimination while maintaining predictive performance with
uncertainty present.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.10963">Can Learning Be Explained By Local Optimality In Low-rank Matrix Recovery?. (arXiv:2302.10963v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1">Jianhao Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Fattahi_S/0/1/0/all/0/1">Salar Fattahi</a></p>
<p>We explore the local landscape of low-rank matrix recovery, aiming to
reconstruct a $d_1\times d_2$ matrix with rank $r$ from $m$ linear
measurements, some potentially noisy. When the true rank is unknown,
overestimation is common, yielding an over-parameterized model with rank $k\geq
r$. Recent findings suggest that first-order methods with the robust
$\ell_1$-loss can recover the true low-rank solution even when the rank is
overestimated and measurements are noisy, implying that true solutions might
emerge as local or global minima. Our paper challenges this notion,
demonstrating that, under mild conditions, true solutions manifest as
\textit{strict saddle points}. We study two categories of low-rank matrix
recovery, matrix completion and matrix sensing, both with the robust
$\ell_1$-loss. For matrix sensing, we uncover two critical transitions. With
$m$ in the range of $\max\{d_1,d_2\}r\lesssim m\lesssim \max\{d_1,d_2\}k$, none
of the true solutions are local or global minima, but some become strict saddle
points. As $m$ surpasses $\max\{d_1,d_2\}k$, all true solutions become
unequivocal global minima. In matrix completion, even with slight rank
overestimation and mild noise, true solutions either emerge as non-critical or
strict saddle points.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.12247">Quantifying &amp; Modeling Multimodal Interactions: An Information Decomposition Framework. (arXiv:2302.12247v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1">Paul Pu Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1">Yun Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1">Xiang Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ling_C/0/1/0/all/0/1">Chun Kai Ling</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_S/0/1/0/all/0/1">Suzanne Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1">Richard Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1">Zihao Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Allen_N/0/1/0/all/0/1">Nicholas Allen</a>, <a href="http://arxiv.org/find/cs/1/au:+Auerbach_R/0/1/0/all/0/1">Randy Auerbach</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahmood_F/0/1/0/all/0/1">Faisal Mahmood</a>, <a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1">Ruslan Salakhutdinov</a>, <a href="http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1">Louis-Philippe Morency</a></p>
<p>The recent explosion of interest in multimodal applications has resulted in a
wide selection of datasets and methods for representing and integrating
information from different modalities. Despite these empirical advances, there
remain fundamental research questions: How can we quantify the interactions
that are necessary to solve a multimodal task? Subsequently, what are the most
suitable multimodal models to capture these interactions? To answer these
questions, we propose an information-theoretic approach to quantify the degree
of redundancy, uniqueness, and synergy relating input modalities with an output
task. We term these three measures as the PID statistics of a multimodal
distribution (or PID for short), and introduce two new estimators for these PID
statistics that scale to high-dimensional distributions. To validate PID
estimation, we conduct extensive experiments on both synthetic datasets where
the PID is known and on large-scale multimodal benchmarks where PID estimations
are compared with human annotations. Finally, we demonstrate their usefulness
in (1) quantifying interactions within multimodal datasets, (2) quantifying
interactions captured by multimodal models, (3) principled approaches for model
selection, and (4) three real-world case studies engaging with domain experts
in pathology, mood prediction, and robotic perception where our framework helps
to recommend strong multimodal models for each application.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.06121">Ignorance is Bliss: Robust Control via Information Gating. (arXiv:2303.06121v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tomar_M/0/1/0/all/0/1">Manan Tomar</a>, <a href="http://arxiv.org/find/cs/1/au:+Islam_R/0/1/0/all/0/1">Riashat Islam</a>, <a href="http://arxiv.org/find/cs/1/au:+Taylor_M/0/1/0/all/0/1">Matthew E. Taylor</a>, <a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1">Sergey Levine</a>, <a href="http://arxiv.org/find/cs/1/au:+Bachman_P/0/1/0/all/0/1">Philip Bachman</a></p>
<p>Informational parsimony provides a useful inductive bias for learning
representations that achieve better generalization by being robust to noise and
spurious correlations. We propose \textit{information gating} as a way to learn
parsimonious representations that identify the minimal information required for
a task. When gating information, we can learn to reveal as little information
as possible so that a task remains solvable, or hide as little information as
possible so that a task becomes unsolvable. We gate information using a
differentiable parameterization of the signal-to-noise ratio, which can be
applied to arbitrary values in a network, e.g., erasing pixels at the input
layer or activations in some intermediate layer. When gating at the input
layer, our models learn which visual cues matter for a given task. When gating
intermediate layers, our models learn which activations are needed for
subsequent stages of computation. We call our approach \textit{InfoGating}. We
apply InfoGating to various objectives such as multi-step forward and inverse
dynamics models, Q-learning, and behavior cloning, highlighting how InfoGating
can naturally help in discarding information not relevant for control. Results
show that learning to identify and use minimal information can improve
generalization in downstream tasks. Policies based on InfoGating are
considerably more robust to irrelevant visual features, leading to improved
pretraining and finetuning of RL models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.12928">Leveraging Multi-time Hamilton-Jacobi PDEs for Certain Scientific Machine Learning Problems. (arXiv:2303.12928v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1">Paula Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_T/0/1/0/all/0/1">Tingwei Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1">Zongren Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Darbon_J/0/1/0/all/0/1">J&#xe9;r&#xf4;me Darbon</a>, <a href="http://arxiv.org/find/cs/1/au:+Karniadakis_G/0/1/0/all/0/1">George Em Karniadakis</a></p>
<p>Hamilton-Jacobi partial differential equations (HJ PDEs) have deep
connections with a wide range of fields, including optimal control,
differential games, and imaging sciences. By considering the time variable to
be a higher dimensional quantity, HJ PDEs can be extended to the multi-time
case. In this paper, we establish a novel theoretical connection between
specific optimization problems arising in machine learning and the multi-time
Hopf formula, which corresponds to a representation of the solution to certain
multi-time HJ PDEs. Through this connection, we increase the interpretability
of the training process of certain machine learning applications by showing
that when we solve these learning problems, we also solve a multi-time HJ PDE
and, by extension, its corresponding optimal control problem. As a first
exploration of this connection, we develop the relation between the regularized
linear regression problem and the Linear Quadratic Regulator (LQR). We then
leverage our theoretical connection to adapt standard LQR solvers (namely,
those based on the Riccati ordinary differential equations) to design new
training approaches for machine learning. Finally, we provide some numerical
examples that demonstrate the versatility and possible computational advantages
of our Riccati-based approach in the context of continual learning,
post-training calibration, transfer learning, and sparse dynamics
identification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.17879">CoSMo: a Framework to Instantiate Conditioned Process Simulation Models. (arXiv:2303.17879v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oyamada_R/0/1/0/all/0/1">Rafael S. Oyamada</a>, <a href="http://arxiv.org/find/cs/1/au:+Tavares_G/0/1/0/all/0/1">Gabriel M. Tavares</a>, <a href="http://arxiv.org/find/cs/1/au:+Ceravolo_P/0/1/0/all/0/1">Paolo Ceravolo</a></p>
<p>Process simulation is gaining attention for its ability to assess potential
performance improvements and risks associated with business process changes.
The existing literature presents various techniques, generally grounded in
process models discovered from event logs or built upon deep learning
algorithms. These techniques have specific strengths and limitations.
Traditional approaches rooted in process models offer increased
interpretability, while those using deep learning excel at generalizing changes
across large event logs. However, the practical application of deep learning
faces challenges related to managing stochasticity and integrating information
for what-if analysis. This paper introduces a novel recurrent neural
architecture tailored to discover COnditioned process Simulation MOdels (CoSMo)
based on user-based constraints or any other nature of a-priori knowledge. This
architecture facilitates the simulation of event logs that adhere to specific
constraints by incorporating declarative-based rules into the learning phase as
an attempt to fill the gap of incorporating information into deep learning
models to perform what-if analysis. Experimental validation illustrates CoSMo's
efficacy in simulating event logs while adhering to predefined declarative
conditions, emphasizing both control-flow and data-flow perspectives.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.00192">Leveraging Neo4j and deep learning for traffic congestion simulation &amp; optimization. (arXiv:2304.00192v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Shyam Pratap Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1">Arshad Ali Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Souissi_R/0/1/0/all/0/1">Riad Souissi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yusuf_S/0/1/0/all/0/1">Syed Adnan Yusuf</a></p>
<p>Traffic congestion has been a major challenge in many urban road networks.
Extensive research studies have been conducted to highlight traffic-related
congestion and address the issue using data-driven approaches. Currently, most
traffic congestion analyses are done using simulation software that offers
limited insight due to the limitations in the tools and utilities being used to
render various traffic congestion scenarios. All that impacts the formulation
of custom business problems which vary from place to place and country to
country. By exploiting the power of the knowledge graph, we model a traffic
congestion problem into the Neo4j graph and then use the load balancing,
optimization algorithm to identify congestion-free road networks. We also show
how traffic propagates backward in case of congestion or accident scenarios and
its overall impact on other segments of the roads. We also train a sequential
RNN-LSTM (Long Short-Term Memory) deep learning model on the real-time traffic
data to assess the accuracy of simulation results based on a road-specific
congestion. Our results show that graph-based traffic simulation, supplemented
by AI ML-based traffic prediction can be more effective in estimating the
congestion level in a road network.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.03671">Contraction-Guided Adaptive Partitioning for Reachability Analysis of Neural Network Controlled Systems. (arXiv:2304.03671v2 [eess.SY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Harapanahalli_A/0/1/0/all/0/1">Akash Harapanahalli</a>, <a href="http://arxiv.org/find/eess/1/au:+Jafarpour_S/0/1/0/all/0/1">Saber Jafarpour</a>, <a href="http://arxiv.org/find/eess/1/au:+Coogan_S/0/1/0/all/0/1">Samuel Coogan</a></p>
<p>In this paper, we present a contraction-guided adaptive partitioning
algorithm for improving interval-valued robust reachable set estimates in a
nonlinear feedback loop with a neural network controller and disturbances.
Based on an estimate of the contraction rate of over-approximated intervals,
the algorithm chooses when and where to partition. Then, by leveraging a
decoupling of the neural network verification step and reachability
partitioning layers, the algorithm can provide accuracy improvements for little
computational cost. This approach is applicable with any sufficiently accurate
open-loop interval-valued reachability estimation technique and any method for
bounding the input-output behavior of a neural network. Using contraction-based
robustness analysis, we provide guarantees of the algorithm's performance with
mixed monotone reachability. Finally, we demonstrate the algorithm's
performance through several numerical simulations and compare it with existing
methods in the literature. In particular, we report a sizable improvement in
the accuracy of reachable set estimation in a fraction of the runtime as
compared to state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.05655">Localisation of Regularised and Multiview Support Vector Machine Learning. (arXiv:2304.05655v2 [math.FA] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Gheondea_A/0/1/0/all/0/1">Aurelian Gheondea</a>, <a href="http://arxiv.org/find/math/1/au:+Tilki_C/0/1/0/all/0/1">Cankat Tilki</a></p>
<p>We prove a few representer theorems for a localised version of the
regularised and multiview support vector machine learning problem introduced by
H.Q.~Minh, L.~Bazzani, and V.~Murino, \textit{Journal of Machine Learning
Research}, \textbf{17}(2016) 1--72, that involves operator valued positive
semidefinite kernels and their reproducing kernel Hilbert spaces. The results
concern general cases when convex or nonconvex loss functions and finite or
infinite dimensional input spaces are considered. We show that the general
framework allows infinite dimensional input spaces and nonconvex loss functions
for some special cases, in particular in case the loss functions are G\^ateaux
differentiable. Detailed calculations are provided for the exponential least
squares loss functions that leads to partially nonlinear problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.07460">Communication and Energy Efficient Wireless Federated Learning with Intrinsic Privacy. (arXiv:2304.07460v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhenxiao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yuanxiong Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1">Yuguang Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1">Yanmin Gong</a></p>
<p>Federated Learning (FL) is a collaborative learning framework that enables
edge devices to collaboratively learn a global model while keeping raw data
locally. Although FL avoids leaking direct information from local datasets,
sensitive information can still be inferred from the shared models. To address
the privacy issue in FL, differential privacy (DP) mechanisms are leveraged to
provide formal privacy guarantee. However, when deploying FL at the wireless
edge with over-the-air computation, ensuring client-level DP faces significant
challenges. In this paper, we propose a novel wireless FL scheme called private
federated edge learning with sparsification (PFELS) to provide client-level DP
guarantee with intrinsic channel noise while reducing communication and energy
overhead and improving model accuracy. The key idea of PFELS is for each device
to first compress its model update and then adaptively design the transmit
power of the compressed model update according to the wireless channel status
without any artificial noise addition. We provide a privacy analysis for PFELS
and prove the convergence of PFELS under general non-convex and non-IID
settings. Experimental results show that compared with prior work, PFELS can
improve the accuracy with the same DP guarantee and save communication and
energy costs simultaneously.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.09595">AdapterGNN: Parameter-Efficient Fine-Tuning Improves Generalization in GNNs. (arXiv:2304.09595v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shengrui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1">Xueting Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1">Jing Bai</a></p>
<p>Fine-tuning pre-trained models has recently yielded remarkable performance
gains in graph neural networks (GNNs). In addition to pre-training techniques,
inspired by the latest work in the natural language fields, more recent work
has shifted towards applying effective fine-tuning approaches, such as
parameter-efficient fine-tuning (PEFT). However, given the substantial
differences between GNNs and transformer-based models, applying such approaches
directly to GNNs proved to be less effective. In this paper, we present a
comprehensive comparison of PEFT techniques for GNNs and propose a novel PEFT
method specifically designed for GNNs, called AdapterGNN. AdapterGNN preserves
the knowledge of the large pre-trained model and leverages highly expressive
adapters for GNNs, which can adapt to downstream tasks effectively with only a
few parameters, while also improving the model's generalization ability.
Extensive experiments show that AdapterGNN achieves higher performance than
other PEFT methods and is the only one consistently surpassing full fine-tuning
(outperforming it by 1.6% and 5.7% in the chemistry and biology domains
respectively, with only 5% and 4% of its parameters tuned) with lower
generalization gaps. Moreover, we empirically show that a larger GNN model can
have a worse generalization ability, which differs from the trend observed in
large transformer-based models. Building upon this, we provide a theoretical
justification for PEFT can improve generalization of GNNs by applying
generalization bounds. Our code is available at
https://github.com/Lucius-lsr/AdapterGNN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.11171">Granular-ball computing: an efficient, robust, and interpretable adaptive multi-granularity representation and computation method. (arXiv:2304.11171v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1">Shuyin Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guoyin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xinbo Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lian_X/0/1/0/all/0/1">Xiaoyu Lian</a></p>
<p>Human cognition operates on a "Global-first" cognitive mechanism,
prioritizing information processing based on coarse-grained details. This
mechanism inherently possesses an adaptive multi-granularity description
capacity, resulting in computational traits such as efficiency, robustness, and
interpretability. The analysis pattern reliance on the finest granularity and
single-granularity makes most existing computational methods less efficient,
robust, and interpretable, which is an important reason for the current lack of
interpretability in neural networks. Multi-granularity granular-ball computing
employs granular-balls of varying sizes to daptively represent and envelop the
sample space, facilitating learning based on these granular-balls. Given that
the number of coarse-grained "granular-balls" is fewer than sample points,
granular-ball computing proves more efficient. Moreover, the inherent
coarse-grained nature of granular-balls reduces susceptibility to fine-grained
sample disturbances, enhancing robustness. The multi-granularity construct of
granular-balls generates topological structures and coarse-grained
descriptions, naturally augmenting interpretability. Granular-ball computing
has successfully ventured into diverse AI domains, fostering the development of
innovative theoretical methods, including granular-ball classifiers, clustering
techniques, neural networks, rough sets, and evolutionary computing. This has
notably ameliorated the efficiency, noise robustness, and interpretability of
traditional methods. Overall, granular-ball computing is a rare and innovative
theoretical approach in AI that can adaptively and simultaneously enhance
efficiency, robustness, and interpretability. This article delves into the main
application landscapes for granular-ball computing, aiming to equip future
researchers with references and insights to refine and expand this promising
theory.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.12180">Variance-Reduced Gradient Estimation via Noise-Reuse in Online Evolution Strategies. (arXiv:2304.12180v2 [cs.NE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_O/0/1/0/all/0/1">Oscar Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Harrison_J/0/1/0/all/0/1">James Harrison</a>, <a href="http://arxiv.org/find/cs/1/au:+Sohl_Dickstein_J/0/1/0/all/0/1">Jascha Sohl-Dickstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Smith_V/0/1/0/all/0/1">Virginia Smith</a>, <a href="http://arxiv.org/find/cs/1/au:+Metz_L/0/1/0/all/0/1">Luke Metz</a></p>
<p>Unrolled computation graphs are prevalent throughout machine learning but
present challenges to automatic differentiation (AD) gradient estimation
methods when their loss functions exhibit extreme local sensitivtiy,
discontinuity, or blackbox characteristics. In such scenarios, online evolution
strategies methods are a more capable alternative, while being more
parallelizable than vanilla evolution strategies (ES) by interleaving partial
unrolls and gradient updates. In this work, we propose a general class of
unbiased online evolution strategies methods. We analytically and empirically
characterize the variance of this class of gradient estimators and identify the
one with the least variance, which we term Noise-Reuse Evolution Strategies
(NRES). Experimentally, we show NRES results in faster convergence than
existing AD and ES methods in terms of wall-clock time and number of unroll
steps across a variety of applications, including learning dynamical systems,
meta-training learned optimizers, and reinforcement learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.14385">Dynamic Pricing and Learning with Bayesian Persuasion. (arXiv:2304.14385v2 [cs.GT] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1">Shipra Agrawal</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yiding Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1">Wei Tang</a></p>
<p>We consider a novel dynamic pricing and learning setting where in addition to
setting prices of products in sequential rounds, the seller also ex-ante
commits to 'advertising schemes'. That is, in the beginning of each round the
seller can decide what kind of signal they will provide to the buyer about the
product's quality upon realization. Using the popular Bayesian persuasion
framework to model the effect of these signals on the buyers' valuation and
purchase responses, we formulate the problem of finding an optimal design of
the advertising scheme along with a pricing scheme that maximizes the seller's
expected revenue. Without any apriori knowledge of the buyers' demand function,
our goal is to design an online algorithm that can use past purchase responses
to adaptively learn the optimal pricing and advertising strategy. We study the
regret of the algorithm when compared to the optimal clairvoyant price and
advertising scheme.
</p>
<p>Our main result is a computationally efficient online algorithm that achieves
an $O(T^{2/3}(m\log T)^{1/3})$ regret bound when the valuation function is
linear in the product quality. Here $m$ is the cardinality of the discrete
product quality domain and $T$ is the time horizon. This result requires some
natural monotonicity and Lipschitz assumptions on the valuation function, but
no Lipschitz or smoothness assumption on the buyers' demand function. For
constant $m$, our result matches the regret lower bound for dynamic pricing
within logarithmic factors, which is a special case of our problem. We also
obtain several improved results for the widely considered special case of
additive valuations, including an $\tilde{O}(T^{2/3})$ regret bound independent
of $m$ when $m\le T^{1/3}$.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.01154">FedAVO: Improving Communication Efficiency in Federated Learning with African Vultures Optimizer. (arXiv:2305.01154v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1">Md Zarif Hossain</a>, <a href="http://arxiv.org/find/cs/1/au:+Imteaj_A/0/1/0/all/0/1">Ahmed Imteaj</a></p>
<p>Federated Learning (FL), a distributed machine learning technique has
recently experienced tremendous growth in popularity due to its emphasis on
user data privacy. However, the distributed computations of FL can result in
constrained communication and drawn-out learning processes, necessitating the
client-server communication cost optimization. The ratio of chosen clients and
the quantity of local training passes are two hyperparameters that have a
significant impact on FL performance. Due to different training preferences
across various applications, it can be difficult for FL practitioners to
manually select such hyperparameters. In our research paper, we introduce
FedAVO, a novel FL algorithm that enhances communication effectiveness by
selecting the best hyperparameters leveraging the African Vulture Optimizer
(AVO). Our research demonstrates that the communication costs associated with
FL operations can be substantially reduced by adopting AVO for FL
hyperparameter adjustment. Through extensive evaluations of FedAVO on benchmark
datasets, we show that FedAVO achieves significant improvement in terms of
model accuracy and communication round, particularly with realistic cases of
Non-IID datasets. Our extensive evaluation of the FedAVO algorithm identifies
the optimal hyperparameters that are appropriately fitted for the benchmark
datasets, eventually increasing global model accuracy by 6% in comparison to
the state-of-the-art FL algorithms (such as FedAvg, FedProx, FedPSO, etc.).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.05463">Multi-Tier Hierarchical Federated Learning-assisted NTN for Intelligent IoT Services. (arXiv:2305.05463v2 [cs.NI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Farajzadeh_A/0/1/0/all/0/1">Amin Farajzadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Yadav_A/0/1/0/all/0/1">Animesh Yadav</a>, <a href="http://arxiv.org/find/cs/1/au:+Yanikomeroglu_H/0/1/0/all/0/1">Halim Yanikomeroglu</a></p>
<p>In the ever-expanding landscape of the IoT, managing the intricate network of
interconnected devices presents a fundamental challenge. This leads us to ask:
"What if we invite the IoT devices to collaboratively participate in real-time
network management and IoT data-handling decisions?" This inquiry forms the
foundation of our innovative approach, addressing the burgeoning complexities
in IoT through the integration of NTN architecture, in particular, VHetNet, and
an MT-HFL framework. VHetNets transcend traditional network paradigms by
harmonizing terrestrial and non-terrestrial elements, thus ensuring expansive
connectivity and resilience, especially crucial in areas with limited
terrestrial infrastructure. The incorporation of MT-HFL further revolutionizes
this architecture, distributing intelligent data processing across a
multi-tiered network spectrum, from edge devices on the ground to aerial
platforms and satellites above. This study explores MT-HFL's role in fostering
a decentralized, collaborative learning environment, enabling IoT devices to
not only contribute but also make informed decisions in network management.
This methodology adeptly handles the challenges posed by the non-IID nature of
IoT data and efficiently curtails communication overheads prevalent in
extensive IoT networks. Significantly, MT-HFL enhances data privacy, a
paramount aspect in IoT ecosystems, by facilitating local data processing and
limiting the sharing of model updates instead of raw data. By evaluating a
case-study, our findings demonstrate that the synergistic integration of MT-HFL
within VHetNets creates an intelligent network architecture that is robust,
scalable, and dynamically adaptive to the ever-changing demands of IoT
environments. This setup ensures efficient data handling, advanced privacy and
security measures, and responsive adaptability to fluctuating network
conditions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.06324">Alternating Gradient Descent and Mixture-of-Experts for Integrated Multimodal Perception. (arXiv:2305.06324v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Akbari_H/0/1/0/all/0/1">Hassan Akbari</a>, <a href="http://arxiv.org/find/cs/1/au:+Kondratyuk_D/0/1/0/all/0/1">Dan Kondratyuk</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1">Yin Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Hornung_R/0/1/0/all/0/1">Rachel Hornung</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Huisheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Adam_H/0/1/0/all/0/1">Hartwig Adam</a></p>
<p>We present Integrated Multimodal Perception (IMP), a simple and scalable
multimodal multi-task training and modeling approach. IMP integrates multimodal
inputs including image, video, text, and audio into a single Transformer
encoder with minimal modality-specific components. IMP makes use of a novel
design that combines Alternating Gradient Descent (AGD) and Mixture-of-Experts
(MoE) for efficient model and task scaling. We conduct extensive empirical
studies and reveal the following key insights: 1) Performing gradient descent
updates by alternating on diverse modalities, loss functions, and tasks, with
varying input resolutions, efficiently improves the model. 2) Sparsification
with MoE on a single modality-agnostic encoder substantially improves the
performance, outperforming dense models that use modality-specific encoders or
additional fusion layers and greatly mitigates the conflicts between
modalities. IMP achieves competitive performance on a wide range of downstream
tasks including video classification, image classification, image-text, and
video-text retrieval. Most notably, we train a sparse IMP-MoE-L variant
focusing on video tasks that achieves new state-of-the-art in zero-shot video
classification: 77.0% on Kinetics-400, 76.8% on Kinetics-600, and 68.3% on
Kinetics-700, improving the previous state-of-the-art by +5%, +6.7%, and +5.8%,
respectively, while using only 15% of their total training computational cost.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.09792">Score Operator Newton transport. (arXiv:2305.09792v2 [math.ST] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Chandramoorthy_N/0/1/0/all/0/1">Nisha Chandramoorthy</a>, <a href="http://arxiv.org/find/math/1/au:+Schaefer_F/0/1/0/all/0/1">Florian Schaefer</a>, <a href="http://arxiv.org/find/math/1/au:+Marzouk_Y/0/1/0/all/0/1">Youssef Marzouk</a></p>
<p>We propose a new approach for sampling and Bayesian computation that uses the
score of the target distribution to construct a transport from a given
reference distribution to the target. Our approach is an infinite-dimensional
Newton method, involving an elliptic PDE, for finding a zero of a
``score-residual'' operator. We use classical elliptic PDE theory to prove
convergence to a valid transport map. Our Newton iterates can be computed by
exploiting fast solvers for elliptic PDEs, resulting in new algorithms for
Bayesian inference and other sampling tasks. We identify elementary settings
where score-operator Newton transport achieves fast convergence while avoiding
mode collapse.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.10133">Generation of 3D Molecules in Pockets via Language Model. (arXiv:2305.10133v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1">Wei Feng</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lvwei Wang</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zaiyun Lin</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yanhao Zhu</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Han Wang</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1">Jianqiang Dong</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Bai_R/0/1/0/all/0/1">Rong Bai</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Huting Wang</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jielong Zhou</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1">Wei Peng</a> (2), <a href="http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1">Bo Huang</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1">Wenbiao Zhou</a> (1) ((1) Beijing StoneWise Technology Co Ltd (2) Innovation Center for Pathogen Research Guangzhou Laboratory)</p>
<p>Generative models for molecules based on sequential line notation (e.g.
SMILES) or graph representation have attracted an increasing interest in the
field of structure-based drug design, but they struggle to capture important 3D
spatial interactions and often produce undesirable molecular structures. To
address these challenges, we introduce Lingo3DMol, a pocket-based 3D molecule
generation method that combines language models and geometric deep learning
technology. A new molecular representation, fragment-based SMILES with local
and global coordinates, was developed to assist the model in learning molecular
topologies and atomic spatial positions. Additionally, we trained a separate
noncovalent interaction predictor to provide essential binding pattern
information for the generative model. Lingo3DMol can efficiently traverse
drug-like chemical spaces, preventing the formation of unusual structures. The
Directory of Useful Decoys-Enhanced (DUD-E) dataset was used for evaluation.
Lingo3DMol outperformed state-of-the-art methods in terms of drug-likeness,
synthetic accessibility, pocket binding mode, and molecule generation speed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.10506">Exact Recovery for System Identification with More Corrupt Data than Clean Data. (arXiv:2305.10506v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yalcin_B/0/1/0/all/0/1">Baturalp Yalcin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lavaei_J/0/1/0/all/0/1">Javad Lavaei</a>, <a href="http://arxiv.org/find/cs/1/au:+Arcak_M/0/1/0/all/0/1">Murat Arcak</a></p>
<p>In this paper, we study the system identification problem for linear
discrete-time systems under adversaries and analyze two lasso-type estimators.
We study both asymptotic and non-asymptotic properties of these estimators in
two separate scenarios, corresponding to deterministic and stochastic models
for the attack times. Since the samples collected from the system are
correlated, the existing results on lasso are not applicable. We show that when
the system is stable and the attacks are injected periodically, the sample
complexity for the exact recovery of the system dynamics is O(n), where n is
the dimension of the states. When the adversarial attacks occur at each time
instance with probability p, the required sample complexity for the exact
recovery scales as O(\log(n)p/(1-p)^2). This result implies the almost sure
convergence to the true system dynamics under the asymptotic regime. As a
by-product, even when more than half of the data is compromised, our estimators
still learn the system correctly. This paper provides the first mathematical
guarantee in the literature on learning from correlated data for dynamical
systems in the case when there is less clean data than corrupt data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.12032">The Waymo Open Sim Agents Challenge. (arXiv:2305.12032v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Montali_N/0/1/0/all/0/1">Nico Montali</a>, <a href="http://arxiv.org/find/cs/1/au:+Lambert_J/0/1/0/all/0/1">John Lambert</a>, <a href="http://arxiv.org/find/cs/1/au:+Mougin_P/0/1/0/all/0/1">Paul Mougin</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuefler_A/0/1/0/all/0/1">Alex Kuefler</a>, <a href="http://arxiv.org/find/cs/1/au:+Rhinehart_N/0/1/0/all/0/1">Nick Rhinehart</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Michelle Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gulino_C/0/1/0/all/0/1">Cole Gulino</a>, <a href="http://arxiv.org/find/cs/1/au:+Emrich_T/0/1/0/all/0/1">Tristan Emrich</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zoey Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1">Shimon Whiteson</a>, <a href="http://arxiv.org/find/cs/1/au:+White_B/0/1/0/all/0/1">Brandyn White</a>, <a href="http://arxiv.org/find/cs/1/au:+Anguelov_D/0/1/0/all/0/1">Dragomir Anguelov</a></p>
<p>Simulation with realistic, interactive agents represents a key task for
autonomous vehicle software development. In this work, we introduce the Waymo
Open Sim Agents Challenge (WOSAC). WOSAC is the first public challenge to
tackle this task and propose corresponding metrics. The goal of the challenge
is to stimulate the design of realistic simulators that can be used to evaluate
and train a behavior model for autonomous driving. We outline our evaluation
methodology, present results for a number of different baseline simulation
agent methods, and analyze several submissions to the 2023 competition which
ran from March 16, 2023 to May 23, 2023. The WOSAC evaluation server remains
open for submissions and we discuss open problems for the task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.12534">BertRLFuzzer: A BERT and Reinforcement Learning Based Fuzzer. (arXiv:2305.12534v4 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jha_P/0/1/0/all/0/1">Piyush Jha</a>, <a href="http://arxiv.org/find/cs/1/au:+Scott_J/0/1/0/all/0/1">Joseph Scott</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganeshna_J/0/1/0/all/0/1">Jaya Sriram Ganeshna</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1">Mudit Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganesh_V/0/1/0/all/0/1">Vijay Ganesh</a></p>
<p>We present a novel tool BertRLFuzzer, a BERT and Reinforcement Learning (RL)
based fuzzer aimed at finding security vulnerabilities for Web applications.
BertRLFuzzer works as follows: given a set of seed inputs, the fuzzer performs
grammar-adhering and attack-provoking mutation operations on them to generate
candidate attack vectors. The key insight of BertRLFuzzer is the use of RL with
a BERT model as an agent to guide the fuzzer to efficiently learn
grammar-adhering and attack-provoking mutation operators. In order to establish
the efficacy of BertRLFuzzer we compare it against a total of 13 black box and
white box fuzzers over a benchmark of 9 victim websites with over 16K LOC. We
observed a significant improvement relative to the nearest competing tool in
terms of time to first attack (54% less), new vulnerabilities found (17 new
vulnerabilities), and attack rate (4.4% more attack vectors generated).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15265">Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model. (arXiv:2305.15265v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zirui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guanchu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_S/0/1/0/all/0/1">Shaochen Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zhaozhuo Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zha_D/0/1/0/all/0/1">Daochen Zha</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1">Ruixiang Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1">Zhimeng Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1">Kaixiong Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chaudhary_V/0/1/0/all/0/1">Vipin Chaudhary</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Shuai Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xia Hu</a></p>
<p>With the rapid growth in model size, fine-tuning the large pre-trained
language model has become increasingly difficult due to its extensive memory
usage. Previous works usually focus on reducing the number of trainable
parameters in the network. While the model parameters do contribute to memory
usage, the primary memory bottleneck during training arises from storing
feature maps, also known as activations, as they are crucial for gradient
calculation. Notably, neural networks are usually trained using stochastic
gradient descent. We argue that in stochastic optimization, models can handle
noisy gradients as long as the gradient estimator is unbiased with reasonable
variance. Following this motivation, we propose a new family of unbiased
estimators called WTA-CRS, for matrix production with reduced variance, which
only requires storing the sub-sampled activations for calculating the gradient.
Our work provides both theoretical and experimental evidence that, in the
context of tuning transformers, our proposed estimators exhibit lower variance
compared to existing ones. By replacing the linear operation with our
approximated one in transformers, we can achieve up to 2.7$\times$ peak memory
reduction with almost no accuracy drop and enables up to $6.4\times$ larger
batch size. Under the same hardware, WTA-CRS enables better down-streaming task
performance by applying larger models and/or faster training speed with larger
batch sizes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15990">PINNslope: seismic data interpolation and local slope estimation with physics informed neural networks. (arXiv:2305.15990v2 [physics.geo-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Brandolin_F/0/1/0/all/0/1">Francesco Brandolin</a>, <a href="http://arxiv.org/find/physics/1/au:+Ravasi_M/0/1/0/all/0/1">Matteo Ravasi</a>, <a href="http://arxiv.org/find/physics/1/au:+Alkhalifah_T/0/1/0/all/0/1">Tariq Alkhalifah</a></p>
<p>Interpolation of aliased seismic data constitutes a key step in a seismic
processing workflow to obtain high quality velocity models and seismic images.
Building on the idea of describing seismic wavefields as a superposition of
local plane waves, we propose to interpolate seismic data by utilizing a
physics informed neural network (PINN). In the proposed framework, two
feed-forward neural networks are jointly trained using the local plane wave
differential equation as well as the available data as two terms in the
objective function: a primary network assisted by positional encoding is tasked
with reconstructing the seismic data, whilst an auxiliary, smaller network
estimates the associated local slopes. Results on synthetic and field data
validate the effectiveness of the proposed method in handling aliased (coarsely
sampled) data and data with large gaps. Our method compares favorably against a
classic least-squares inversion approach regularized by the local plane-wave
equation as well as a PINN-based approach with a single network and
pre-computed local slopes. We find that introducing a second network to
estimate the local slopes whilst at the same time interpolating the aliased
data enhances the overall reconstruction capabilities and convergence behavior
of the primary network. Moreover, an additional positional encoding layer
embedded as the first layer of the wavefield network confers to the network the
ability to converge faster improving the accuracy of the data term.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16771">Robust Nonparametric Regression under Poisoning Attack. (arXiv:2305.16771v2 [math.ST] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Zhao_P/0/1/0/all/0/1">Puning Zhao</a>, <a href="http://arxiv.org/find/math/1/au:+Wan_Z/0/1/0/all/0/1">Zhiguo Wan</a></p>
<p>This paper studies robust nonparametric regression, in which an adversarial
attacker can modify the values of up to $q$ samples from a training dataset of
size $N$. Our initial solution is an M-estimator based on Huber loss
minimization. Compared with simple kernel regression, i.e. the Nadaraya-Watson
estimator, this method can significantly weaken the impact of malicious samples
on the regression performance. We provide the convergence rate as well as the
corresponding minimax lower bound. The result shows that, with proper bandwidth
selection, $\ell_\infty$ error is minimax optimal. The $\ell_2$ error is
optimal with relatively small $q$, but is suboptimal with larger $q$. The
reason is that this estimator is vulnerable if there are many attacked samples
concentrating in a small region. To address this issue, we propose a correction
method by projecting the initial estimate to the space of Lipschitz functions.
The final estimate is nearly minimax optimal for arbitrary $q$, up to a $\ln N$
factor.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.17581">Knowledge Distillation Performs Partial Variance Reduction. (arXiv:2305.17581v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Safaryan_M/0/1/0/all/0/1">Mher Safaryan</a>, <a href="http://arxiv.org/find/cs/1/au:+Peste_A/0/1/0/all/0/1">Alexandra Peste</a>, <a href="http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1">Dan Alistarh</a></p>
<p>Knowledge distillation is a popular approach for enhancing the performance of
''student'' models, with lower representational capacity, by taking advantage
of more powerful ''teacher'' models. Despite its apparent simplicity and
widespread use, the underlying mechanics behind knowledge distillation (KD) are
still not fully understood. In this work, we shed new light on the inner
workings of this method, by examining it from an optimization perspective. We
show that, in the context of linear and deep linear models, KD can be
interpreted as a novel type of stochastic variance reduction mechanism. We
provide a detailed convergence analysis of the resulting dynamics, which hold
under standard assumptions for both strongly-convex and non-convex losses,
showing that KD acts as a form of partial variance reduction, which can reduce
the stochastic gradient noise, but may not eliminate it completely, depending
on the properties of the ''teacher'' model. Our analysis puts further emphasis
on the need for careful parametrization of KD, in particular w.r.t. the
weighting of the distillation loss, and is validated empirically on both linear
models and deep neural networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.00196">Restless Bandits with Average Reward: Breaking the Uniform Global Attractor Assumption. (arXiv:2306.00196v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1">Yige Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1">Qiaomin Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yudong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Weina Wang</a></p>
<p>We study the infinite-horizon Restless Bandit problem with the average reward
criterion, under both discrete-time and continuous-time settings. A fundamental
goal is to design computationally efficient policies that achieve a diminishing
optimality gap as the number of arms, $N$, grows large. Existing results on
asymptotic optimality all rely on the uniform global attractor property (UGAP),
a complex and challenging-to-verify assumption. In this paper, we propose a
general, simulation-based framework, Follow-the-Virtual-Advice, that converts
any single-armed policy into a policy for the original $N$-armed problem. This
is done by simulating the single-armed policy on each arm and carefully
steering the real state towards the simulated state. Our framework can be
instantiated to produce a policy with an $O(1/\sqrt{N})$ optimality gap. In the
discrete-time setting, our result holds under a simpler synchronization
assumption, which covers some problem instances that violate UGAP. More
notably, in the continuous-time setting, we do not require any additional
assumptions beyond the standard unichain condition. In both settings, our work
is the first asymptotic optimality result that does not require UGAP.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.01804">Extracting Reward Functions from Diffusion Models. (arXiv:2306.01804v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nuti_F/0/1/0/all/0/1">Felipe Nuti</a>, <a href="http://arxiv.org/find/cs/1/au:+Franzmeyer_T/0/1/0/all/0/1">Tim Franzmeyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Henriques_J/0/1/0/all/0/1">Jo&#xe3;o F. Henriques</a></p>
<p>Diffusion models have achieved remarkable results in image generation, and
have similarly been used to learn high-performing policies in sequential
decision-making tasks. Decision-making diffusion models can be trained on
lower-quality data, and then be steered with a reward function to generate
near-optimal trajectories. We consider the problem of extracting a reward
function by comparing a decision-making diffusion model that models low-reward
behavior and one that models high-reward behavior; a setting related to inverse
reinforcement learning. We first define the notion of a relative reward
function of two diffusion models and show conditions under which it exists and
is unique. We then devise a practical learning algorithm for extracting it by
aligning the gradients of a reward function -- parametrized by a neural network
-- to the difference in outputs of both diffusion models. Our method finds
correct reward functions in navigation environments, and we demonstrate that
steering the base model with the learned reward functions results in
significantly increased performance in standard locomotion benchmarks. Finally,
we demonstrate that our approach generalizes beyond sequential decision-making
by learning a reward-like function from two large-scale image generation
diffusion models. The extracted reward function successfully assigns lower
rewards to harmful images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.02822">Discovering Dynamic Causal Space for DAG Structure Learning. (arXiv:2306.02822v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1">Fangfu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1">Wenchang Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1">An Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1">Yueqi Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1">Tat-Seng Chua</a></p>
<p>Discovering causal structure from purely observational data (i.e., causal
discovery), aiming to identify causal relationships among variables, is a
fundamental task in machine learning. The recent invention of differentiable
score-based DAG learners is a crucial enabler, which reframes the combinatorial
optimization problem into a differentiable optimization with a DAG constraint
over directed graph space. Despite their great success, these cutting-edge DAG
learners incorporate DAG-ness independent score functions to evaluate the
directed graph candidates, lacking in considering graph structure. As a result,
measuring the data fitness alone regardless of DAG-ness inevitably leads to
discovering suboptimal DAGs and model vulnerabilities. Towards this end, we
propose a dynamic causal space for DAG structure learning, coined CASPER, that
integrates the graph structure into the score function as a new measure in the
causal space to faithfully reflect the causal distance between estimated and
ground truth DAG. CASPER revises the learning process as well as enhances the
DAG structure learning via adaptive attention to DAG-ness. Grounded by
empirical visualization, CASPER, as a space, satisfies a series of desired
properties, such as structure awareness and noise robustness. Extensive
experiments on both synthetic and real-world datasets clearly validate the
superiority of our CASPER over the state-of-the-art causal discovery methods in
terms of accuracy and robustness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.03161">On the Role of Entanglement and Statistics in Learning. (arXiv:2306.03161v2 [quant-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Arunachalam_S/0/1/0/all/0/1">Srinivasan Arunachalam</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Havlicek_V/0/1/0/all/0/1">Vojtech Havlicek</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Schatzki_L/0/1/0/all/0/1">Louis Schatzki</a></p>
<p>In this work we make progress in understanding the relationship between
learning models with access to entangled, separable and statistical
measurements in the quantum statistical query (QSQ) model. To this end, we show
the following results.
</p>
<p>$\textbf{Entangled versus separable measurements.}$ The goal here is to learn
an unknown $f$ from the concept class $C\subseteq \{f:\{0,1\}^n\rightarrow
[k]\}$ given copies of $\frac{1}{\sqrt{2^n}}\sum_x \vert x,f(x)\rangle$. We
show that, if $T$ copies suffice to learn $f$ using entangled measurements,
then $O(nT^2)$ copies suffice to learn $f$ using just separable measurements.
</p>
<p>$\textbf{Entangled versus statistical measurements}$ The goal here is to
learn a function $f \in C$ given access to separable measurements and
statistical measurements. We exhibit a class $C$ that gives an exponential
separation between QSQ learning and quantum learning with entangled
measurements (even in the presence of noise). This proves the "quantum
analogue" of the seminal result of Blum et al. [BKW'03]. that separates
classical SQ and PAC learning with classification noise.
</p>
<p>$\textbf{QSQ lower bounds for learning states.}$ We introduce a quantum
statistical query dimension (QSD), which we use to give lower bounds on the QSQ
learning. With this we prove superpolynomial QSQ lower bounds for testing
purity, shadow tomography, Abelian hidden subgroup problem, degree-$2$
functions, planted bi-clique states and output states of Clifford circuits of
depth $\textsf{polylog}(n)$.
</p>
<p>$\textbf{Further applications.}$ We give and $\textit{unconditional}$
separation between weak and strong error mitigation and prove lower bounds for
learning distributions in the QSQ model. Prior works by Quek et al. [QFK+'22],
Hinsche et al. [HIN+'22], and Nietner et al. [NIS+'23] proved the analogous
results $\textit{assuming}$ diagonal measurements and our work removes this
assumption.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.07042">Transformers learn through gradual rank increase. (arXiv:2306.07042v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Boix_Adsera_E/0/1/0/all/0/1">Enric Boix-Adsera</a>, <a href="http://arxiv.org/find/cs/1/au:+Littwin_E/0/1/0/all/0/1">Etai Littwin</a>, <a href="http://arxiv.org/find/cs/1/au:+Abbe_E/0/1/0/all/0/1">Emmanuel Abbe</a>, <a href="http://arxiv.org/find/cs/1/au:+Bengio_S/0/1/0/all/0/1">Samy Bengio</a>, <a href="http://arxiv.org/find/cs/1/au:+Susskind_J/0/1/0/all/0/1">Joshua Susskind</a></p>
<p>We identify incremental learning dynamics in transformers, where the
difference between trained and initial weights progressively increases in rank.
We rigorously prove this occurs under the simplifying assumptions of diagonal
weight matrices and small initialization. Our experiments support the theory
and also show that phenomenon can occur in practice without the simplifying
assumptions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.09344">DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data. (arXiv:2306.09344v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1">Stephanie Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tamir_N/0/1/0/all/0/1">Netanel Tamir</a>, <a href="http://arxiv.org/find/cs/1/au:+Sundaram_S/0/1/0/all/0/1">Shobhita Sundaram</a>, <a href="http://arxiv.org/find/cs/1/au:+Chai_L/0/1/0/all/0/1">Lucy Chai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Richard Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dekel_T/0/1/0/all/0/1">Tali Dekel</a>, <a href="http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1">Phillip Isola</a></p>
<p>Current perceptual similarity metrics operate at the level of pixels and
patches. These metrics compare images in terms of their low-level colors and
textures, but fail to capture mid-level similarities and differences in image
layout, object pose, and semantic content. In this paper, we develop a
perceptual metric that assesses images holistically. Our first step is to
collect a new dataset of human similarity judgments over image pairs that are
alike in diverse ways. Critical to this dataset is that judgments are nearly
automatic and shared by all observers. To achieve this we use recent
text-to-image models to create synthetic pairs that are perturbed along various
dimensions. We observe that popular perceptual metrics fall short of explaining
our new data, and we introduce a new metric, DreamSim, tuned to better align
with human perception. We analyze how our metric is affected by different
visual attributes, and find that it focuses heavily on foreground objects and
semantic content while also being sensitive to color and layout. Notably,
despite being trained on synthetic data, our metric generalizes to real images,
giving strong results on retrieval and reconstruction tasks. Furthermore, our
metric outperforms both prior learned metrics and recent large vision models on
these tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.09364">TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting. (arXiv:2306.09364v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ekambaram_V/0/1/0/all/0/1">Vijay Ekambaram</a>, <a href="http://arxiv.org/find/cs/1/au:+Jati_A/0/1/0/all/0/1">Arindam Jati</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1">Nam Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sinthong_P/0/1/0/all/0/1">Phanwadee Sinthong</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalagnanam_J/0/1/0/all/0/1">Jayant Kalagnanam</a></p>
<p>Transformers have gained popularity in time series forecasting for their
ability to capture long-sequence interactions. However, their high memory and
computing requirements pose a critical bottleneck for long-term forecasting. To
address this, we propose TSMixer, a lightweight neural architecture exclusively
composed of multi-layer perceptron (MLP) modules for multivariate forecasting
and representation learning on patched time series. Inspired by MLP-Mixer's
success in computer vision, we adapt it for time series, addressing challenges
and introducing validated components for enhanced accuracy. This includes a
novel design paradigm of attaching online reconciliation heads to the MLP-Mixer
backbone, for explicitly modeling the time-series properties such as hierarchy
and channel-correlations. We also propose a novel Hybrid channel modeling and
infusion of a simple gating approach to effectively handle noisy channel
interactions and generalization across diverse datasets. By incorporating these
lightweight components, we significantly enhance the learning capability of
simple MLP structures, outperforming complex Transformer models with minimal
computing usage. Moreover, TSMixer's modular design enables compatibility with
both supervised and masked self-supervised learning methods, making it a
promising building block for time-series Foundation Models. TSMixer outperforms
state-of-the-art MLP and Transformer models in forecasting by a considerable
margin of 8-60%. It also outperforms the latest strong benchmarks of
Patch-Transformer models (by 1-2%) with a significant reduction in memory and
runtime (2-3X). The source code of our model is officially released as
PatchTSMixer in the HuggingFace. Model:
https://huggingface.co/docs/transformers/main/en/model_doc/patchtsmixer
Examples: https://github.com/ibm/tsfm/#notebooks-links
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.09444">Large-Scale Quantum Separability Through a Reproducible Machine Learning Lens. (arXiv:2306.09444v2 [quant-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Casale_B/0/1/0/all/0/1">Balthazar Casal&#xe9;</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Molfetta_G/0/1/0/all/0/1">Giuseppe Di Molfetta</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Anthoine_S/0/1/0/all/0/1">Sandrine Anthoine</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Kadri_H/0/1/0/all/0/1">Hachem Kadri</a></p>
<p>The quantum separability problem consists in deciding whether a bipartite
density matrix is entangled or separable. In this work, we propose a machine
learning pipeline for finding approximate solutions for this NP-hard problem in
large-scale scenarios. We provide an efficient Frank-Wolfe-based algorithm to
approximately seek the nearest separable density matrix and derive a systematic
way for labeling density matrices as separable or entangled, allowing us to
treat quantum separability as a classification problem. Our method is
applicable to any two-qudit mixed states. Numerical experiments with quantum
states of 3- and 7-dimensional qudits validate the efficiency of the proposed
procedure, and demonstrate that it scales up to thousands of density matrices
with a high quantum entanglement detection accuracy. This takes a step towards
benchmarking quantum separability to support the development of more powerful
entanglement detection techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.09803">Framework and Benchmarks for Combinatorial and Mixed-variable Bayesian Optimization. (arXiv:2306.09803v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dreczkowski_K/0/1/0/all/0/1">Kamil Dreczkowski</a>, <a href="http://arxiv.org/find/cs/1/au:+Grosnit_A/0/1/0/all/0/1">Antoine Grosnit</a>, <a href="http://arxiv.org/find/cs/1/au:+Ammar_H/0/1/0/all/0/1">Haitham Bou Ammar</a></p>
<p>This paper introduces a modular framework for Mixed-variable and
Combinatorial Bayesian Optimization (MCBO) to address the lack of systematic
benchmarking and standardized evaluation in the field. Current MCBO papers
often introduce non-diverse or non-standard benchmarks to evaluate their
methods, impeding the proper assessment of different MCBO primitives and their
combinations. Additionally, papers introducing a solution for a single MCBO
primitive often omit benchmarking against baselines that utilize the same
methods for the remaining primitives. This omission is primarily due to the
significant implementation overhead involved, resulting in a lack of controlled
assessments and an inability to showcase the merits of a contribution
effectively. To overcome these challenges, our proposed framework enables an
effortless combination of Bayesian Optimization components, and provides a
diverse set of synthetic and real-world benchmarking tasks. Leveraging this
flexibility, we implement 47 novel MCBO algorithms and benchmark them against
seven existing MCBO solvers and five standard black-box optimization algorithms
on ten tasks, conducting over 4000 experiments. Our findings reveal a superior
combination of MCBO primitives outperforming existing approaches and illustrate
the significance of model fit and the use of a trust region. We make our MCBO
library available under the MIT license at
\url{https://github.com/huawei-noah/HEBO/tree/master/MCBO}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.09910">LabelBench: A Comprehensive Framework for Benchmarking Adaptive Label-Efficient Learning. (arXiv:2306.09910v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jifan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yifang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Canal_G/0/1/0/all/0/1">Gregory Canal</a>, <a href="http://arxiv.org/find/cs/1/au:+Mussmann_S/0/1/0/all/0/1">Stephen Mussmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1">Arnav M. Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhatt_G/0/1/0/all/0/1">Gantavya Bhatt</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yinglun Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1">Simon Shaolei Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Jamieson_K/0/1/0/all/0/1">Kevin Jamieson</a>, <a href="http://arxiv.org/find/cs/1/au:+Nowak_R/0/1/0/all/0/1">Robert D Nowak</a></p>
<p>Labeled data are critical to modern machine learning applications, but
obtaining labels can be expensive. To mitigate this cost, machine learning
methods, such as transfer learning, semi-supervised learning and active
learning, aim to be label-efficient: achieving high predictive performance from
relatively few labeled examples. While obtaining the best label-efficiency in
practice often requires combinations of these techniques, existing benchmark
and evaluation frameworks do not capture a concerted combination of all such
techniques. This paper addresses this deficiency by introducing LabelBench, a
new computationally-efficient framework for joint evaluation of multiple
label-efficient learning techniques. As an application of LabelBench, we
introduce a novel benchmark of state-of-the-art active learning methods in
combination with semi-supervised learning for fine-tuning pretrained vision
transformers. Our benchmark demonstrates better label-efficiencies than
previously reported in active learning. LabelBench's modular codebase is
open-sourced for the broader community to contribute label-efficient learning
methods and benchmarks. The repository can be found at:
https://github.com/EfficientTraining/LabelBench.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.10619">Towards Stability of Autoregressive Neural Operators. (arXiv:2306.10619v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+McCabe_M/0/1/0/all/0/1">Michael McCabe</a>, <a href="http://arxiv.org/find/cs/1/au:+Harrington_P/0/1/0/all/0/1">Peter Harrington</a>, <a href="http://arxiv.org/find/cs/1/au:+Subramanian_S/0/1/0/all/0/1">Shashank Subramanian</a>, <a href="http://arxiv.org/find/cs/1/au:+Brown_J/0/1/0/all/0/1">Jed Brown</a></p>
<p>Neural operators have proven to be a promising approach for modeling
spatiotemporal systems in the physical sciences. However, training these models
for large systems can be quite challenging as they incur significant
computational and memory expense -- these systems are often forced to rely on
autoregressive time-stepping of the neural network to predict future temporal
states. While this is effective in managing costs, it can lead to uncontrolled
error growth over time and eventual instability. We analyze the sources of this
autoregressive error growth using prototypical neural operator models for
physical systems and explore ways to mitigate it. We introduce architectural
and application-specific improvements that allow for careful control of
instability-inducing operations within these models without inflating the
compute/memory expense. We present results on several scientific systems that
include Navier-Stokes fluid flow, rotating shallow water, and a high-resolution
global weather forecasting system. We demonstrate that applying our design
principles to neural operators leads to significantly lower errors for
long-term forecasts as well as longer time horizons without qualitative signs
of divergence compared to the original models for these systems. We open-source
our \href{https://github.com/mikemccabe210/stabilizing_neural_operators}{code}
for reproducibility.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.17759">The Shaped Transformer: Attention Models in the Infinite Depth-and-Width Limit. (arXiv:2306.17759v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Noci_L/0/1/0/all/0/1">Lorenzo Noci</a>, <a href="http://arxiv.org/find/stat/1/au:+Li_C/0/1/0/all/0/1">Chuning Li</a>, <a href="http://arxiv.org/find/stat/1/au:+Li_M/0/1/0/all/0/1">Mufan Bill Li</a>, <a href="http://arxiv.org/find/stat/1/au:+He_B/0/1/0/all/0/1">Bobby He</a>, <a href="http://arxiv.org/find/stat/1/au:+Hofmann_T/0/1/0/all/0/1">Thomas Hofmann</a>, <a href="http://arxiv.org/find/stat/1/au:+Maddison_C/0/1/0/all/0/1">Chris Maddison</a>, <a href="http://arxiv.org/find/stat/1/au:+Roy_D/0/1/0/all/0/1">Daniel M. Roy</a></p>
<p>In deep learning theory, the covariance matrix of the representations serves
as a proxy to examine the network's trainability. Motivated by the success of
Transformers, we study the covariance matrix of a modified Softmax-based
attention model with skip connections in the proportional limit of
infinite-depth-and-width. We show that at initialization the limiting
distribution can be described by a stochastic differential equation (SDE)
indexed by the depth-to-width ratio. To achieve a well-defined stochastic
limit, the Transformer's attention mechanism is modified by centering the
Softmax output at identity, and scaling the Softmax logits by a width-dependent
temperature parameter. We examine the stability of the network through the
corresponding SDE, showing how the scale of both the drift and diffusion can be
elegantly controlled with the aid of residual connections. The existence of a
stable SDE implies that the covariance structure is well-behaved, even for very
large depth and width, thus preventing the notorious issues of rank degeneracy
in deep attention models. Finally, we show, through simulations, that the SDE
provides a surprisingly good description of the corresponding finite-size
model. We coin the name shaped Transformer for these architectural
modifications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.00553">Partial-label Learning with Mixed Closed-set and Open-set Out-of-candidate Examples. (arXiv:2307.00553v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1">Shuo He</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1">Lei Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1">Guowu Yang</a></p>
<p>Partial-label learning (PLL) relies on a key assumption that the true label
of each training example must be in the candidate label set. This restrictive
assumption may be violated in complex real-world scenarios, and thus the true
label of some collected examples could be unexpectedly outside the assigned
candidate label set. In this paper, we term the examples whose true label is
outside the candidate label set OOC (out-of-candidate) examples, and pioneer a
new PLL study to learn with OOC examples. We consider two types of OOC examples
in reality, i.e., the closed-set/open-set OOC examples whose true label is
inside/outside the known label space. To solve this new PLL problem, we first
calculate the wooden cross-entropy loss from candidate and non-candidate labels
respectively, and dynamically differentiate the two types of OOC examples based
on specially designed criteria. Then, for closed-set OOC examples, we conduct
reversed label disambiguation in the non-candidate label set; for open-set OOC
examples, we leverage them for training by utilizing an effective
regularization strategy that dynamically assigns random candidate labels from
the candidate label set. In this way, the two types of OOC examples can be
differentiated and further leveraged for model training. Extensive experiments
demonstrate that our proposed method outperforms state-of-the-art PLL methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.02028">EHRSHOT: An EHR Benchmark for Few-Shot Evaluation of Foundation Models. (arXiv:2307.02028v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wornow_M/0/1/0/all/0/1">Michael Wornow</a>, <a href="http://arxiv.org/find/cs/1/au:+Thapa_R/0/1/0/all/0/1">Rahul Thapa</a>, <a href="http://arxiv.org/find/cs/1/au:+Steinberg_E/0/1/0/all/0/1">Ethan Steinberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Fries_J/0/1/0/all/0/1">Jason A. Fries</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1">Nigam H. Shah</a></p>
<p>While the general machine learning (ML) community has benefited from public
datasets, tasks, and models, the progress of ML in healthcare has been hampered
by a lack of such shared assets. The success of foundation models creates new
challenges for healthcare ML by requiring access to shared pretrained models to
validate performance benefits. We help address these challenges through three
contributions. First, we publish a new dataset, EHRSHOT, which contains
deidentified structured data from the electronic health records (EHRs) of 6,739
patients from Stanford Medicine. Unlike MIMIC-III/IV and other popular EHR
datasets, EHRSHOT is longitudinal and not restricted to ICU/ED patients.
Second, we publish the weights of CLMBR-T-base, a 141M parameter clinical
foundation model pretrained on the structured EHR data of 2.57M patients. We
are one of the first to fully release such a model for coded EHR data; in
contrast, most prior models released for clinical data (e.g. GatorTron,
ClinicalBERT) only work with unstructured text and cannot process the rich,
structured data within an EHR. We provide an end-to-end pipeline for the
community to validate and build upon its performance. Third, we define 15
few-shot clinical prediction tasks, enabling evaluation of foundation models on
benefits such as sample efficiency and task adaptation. Our model and dataset
are available via a research data use agreement from our website:
https://ehrshot.stanford.edu. Code to reproduce our results are available at
our Github repo: https://github.com/som-shahlab/ehrshot-benchmark
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04057">Bidirectional Attention as a Mixture of Continuous Word Experts. (arXiv:2307.04057v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wibisono_K/0/1/0/all/0/1">Kevin Christian Wibisono</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yixin Wang</a></p>
<p>Bidirectional attention $\unicode{x2013}$ composed of self-attention with
positional encodings and the masked language model (MLM) objective
$\unicode{x2013}$ has emerged as a key component of modern large language
models (LLMs). Despite its empirical success, few studies have examined its
statistical underpinnings: What statistical model is bidirectional attention
implicitly fitting? What sets it apart from its non-attention predecessors? We
explore these questions in this paper. The key observation is that fitting a
single-layer single-head bidirectional attention, upon reparameterization, is
equivalent to fitting a continuous bag of words (CBOW) model with
mixture-of-experts (MoE) weights. Further, bidirectional attention with
multiple heads and multiple layers is equivalent to stacked MoEs and a mixture
of MoEs, respectively. This statistical viewpoint reveals the distinct use of
MoE in bidirectional attention, which aligns with its practical effectiveness
in handling heterogeneous data. It also suggests an immediate extension to
categorical tabular data, if we view each word location in a sentence as a
tabular feature. Across empirical studies, we find that this extension
outperforms existing tabular extensions of transformers in out-of-distribution
(OOD) generalization. Finally, this statistical perspective of bidirectional
attention enables us to theoretically characterize when linear word analogies
are present in its word embeddings. These analyses show that bidirectional
attention can require much stronger assumptions to exhibit linear word
analogies than its non-attention predecessors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05695">ReLoRA: High-Rank Training Through Low-Rank Updates. (arXiv:2307.05695v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lialin_V/0/1/0/all/0/1">Vladislav Lialin</a>, <a href="http://arxiv.org/find/cs/1/au:+Shivagunde_N/0/1/0/all/0/1">Namrata Shivagunde</a>, <a href="http://arxiv.org/find/cs/1/au:+Muckatira_S/0/1/0/all/0/1">Sherin Muckatira</a>, <a href="http://arxiv.org/find/cs/1/au:+Rumshisky_A/0/1/0/all/0/1">Anna Rumshisky</a></p>
<p>Despite the dominance and effectiveness of scaling, resulting in large
networks with hundreds of billions of parameters, the necessity to train
overparameterized models remains poorly understood, while training costs grow
exponentially. In this paper, we explore parameter-efficient training
techniques as an approach to training large neural networks. We introduce a
novel method called ReLoRA, which utilizes low-rank updates to train high-rank
networks. We apply ReLoRA to training transformer language models with up to
1.3B parameters and demonstrate comparable performance to regular neural
network training. ReLoRA saves up to 5.5Gb of RAM per GPU and improves training
speed by 9-40% depending on the model size and hardware setup. Our findings
show the potential of parameter-efficient techniques for large-scale
pre-training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06483">Misclassification in Automated Content Analysis Causes Bias in Regression. Can We Fix It? Yes We Can!. (arXiv:2307.06483v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+TeBlunthuis_N/0/1/0/all/0/1">Nathan TeBlunthuis</a>, <a href="http://arxiv.org/find/cs/1/au:+Hase_V/0/1/0/all/0/1">Valerie Hase</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1">Chung-Hong Chan</a></p>
<p>Automated classifiers (ACs), often built via supervised machine learning
(SML), can categorize large, statistically powerful samples of data ranging
from text to images and video, and have become widely popular measurement
devices in communication science and related fields. Despite this popularity,
even highly accurate classifiers make errors that cause misclassification bias
and misleading results in downstream analyses-unless such analyses account for
these errors. As we show in a systematic literature review of SML applications,
communication scholars largely ignore misclassification bias. In principle,
existing statistical methods can use "gold standard" validation data, such as
that created by human annotators, to correct misclassification bias and produce
consistent estimates. We introduce and test such methods, including a new
method we design and implement in the R package misclassificationmodels, via
Monte Carlo simulations designed to reveal each method's limitations, which we
also release. Based on our results, we recommend our new error correction
method as it is versatile and efficient. In sum, automated classifiers, even
those below common accuracy standards or making systematic misclassifications,
can be useful for measurement with careful study design and appropriate error
correction methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08863">Meta-Value Learning: a General Framework for Learning with Learning Awareness. (arXiv:2307.08863v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cooijmans_T/0/1/0/all/0/1">Tim Cooijmans</a>, <a href="http://arxiv.org/find/cs/1/au:+Aghajohari_M/0/1/0/all/0/1">Milad Aghajohari</a>, <a href="http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1">Aaron Courville</a></p>
<p>Gradient-based learning in multi-agent systems is difficult because the
gradient derives from a first-order model which does not account for the
interaction between agents' learning processes. LOLA (<a href="/abs/1709.04326">arXiv:1709.04326</a>)
accounts for this by differentiating through one step of optimization. We
propose to judge joint policies by their long-term prospects as measured by the
meta-value, a discounted sum over the returns of future optimization iterates.
We apply a form of Q-learning to the meta-game of optimization, in a way that
avoids the need to explicitly represent the continuous action space of policy
updates. The resulting method, MeVa, is consistent and far-sighted, and does
not require REINFORCE estimators. We analyze the behavior of our method on a
toy game and compare to prior work on repeated matrix games.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08875">Natural Actor-Critic for Robust Reinforcement Learning with Function Approximation. (arXiv:2307.08875v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1">Ruida Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1">Min Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalathil_D/0/1/0/all/0/1">Dileep Kalathil</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1">P. R. Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_C/0/1/0/all/0/1">Chao Tian</a></p>
<p>We study robust reinforcement learning (RL) with the goal of determining a
well-performing policy that is robust against model mismatch between the
training simulator and the testing environment. Previous policy-based robust RL
algorithms mainly focus on the tabular setting under uncertainty sets that
facilitate robust policy evaluation, but are no longer tractable when the
number of states scales up. To this end, we propose two novel uncertainty set
formulations, one based on double sampling and the other on an integral
probability metric. Both make large-scale robust RL tractable even when one
only has access to a simulator. We propose a robust natural actor-critic (RNAC)
approach that incorporates the new uncertainty sets and employs function
approximation. We provide finite-time convergence guarantees for the proposed
RNAC algorithm to the optimal robust policy within the function approximation
error. Finally, we demonstrate the robust performance of the policy learned by
our proposed RNAC approach in multiple MuJoCo environments and a real-world
TurtleBot navigation task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08939">Runtime Stealthy Perception Attacks against DNN-based Adaptive Cruise Control Systems. (arXiv:2307.08939v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xugui Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1">Anqi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Kouzel_M/0/1/0/all/0/1">Maxfield Kouzel</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1">Haotian Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+McCarty_M/0/1/0/all/0/1">Morgan McCarty</a>, <a href="http://arxiv.org/find/cs/1/au:+Nita_Rotaru_C/0/1/0/all/0/1">Cristina Nita-Rotaru</a>, <a href="http://arxiv.org/find/cs/1/au:+Alemzadeh_H/0/1/0/all/0/1">Homa Alemzadeh</a></p>
<p>Adaptive Cruise Control (ACC) is a widely used driver assistance technology
for maintaining the desired speed and safe distance to the leading vehicle.
This paper evaluates the security of the deep neural network (DNN) based ACC
systems under runtime stealthy perception attacks that strategically inject
perturbations into camera data to cause forward collisions. We present a
context-aware strategy for the selection of the most critical times for
triggering the attacks and a novel optimization-based method for the adaptive
generation of image perturbations at runtime. We evaluate the effectiveness of
the proposed attack using a publicly available driving dataset, an actual
vehicle, and a realistic simulation platform with the control software from a
production ACC system, a physical-world driving simulator, and interventions by
the human driver and safety features such as Advanced Emergency Braking System
(AEBS). Experimental results show that the proposed attack achieves 142.9 times
higher success rate in causing hazards and 89.6% higher evasion rate than
baselines while being stealthy and robust to real-world factors and dynamic
changes in the environment. This study highlights the role of human drivers and
basic safety mechanisms in preventing attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09072">Real-time Inference and Extrapolation via a Diffusion-inspired Temporal Transformer Operator (DiTTO). (arXiv:2307.09072v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ovadia_O/0/1/0/all/0/1">Oded Ovadia</a>, <a href="http://arxiv.org/find/cs/1/au:+Oommen_V/0/1/0/all/0/1">Vivek Oommen</a>, <a href="http://arxiv.org/find/cs/1/au:+Kahana_A/0/1/0/all/0/1">Adar Kahana</a>, <a href="http://arxiv.org/find/cs/1/au:+Peyvan_A/0/1/0/all/0/1">Ahmad Peyvan</a>, <a href="http://arxiv.org/find/cs/1/au:+Turkel_E/0/1/0/all/0/1">Eli Turkel</a>, <a href="http://arxiv.org/find/cs/1/au:+Karniadakis_G/0/1/0/all/0/1">George Em Karniadakis</a></p>
<p>Extrapolation remains a grand challenge in deep neural networks across all
application domains. We propose an operator learning method to solve
time-dependent partial differential equations (PDEs) continuously and with
extrapolation in time without any temporal discretization. The proposed method,
named Diffusion-inspired Temporal Transformer Operator (DiTTO), is inspired by
latent diffusion models and their conditioning mechanism, which we use to
incorporate the temporal evolution of the PDE, in combination with elements
from the transformer architecture to improve its capabilities. Upon training,
DiTTO can make inferences in real-time. We demonstrate its extrapolation
capability on a climate problem by estimating the temperature around the globe
for several years, and also in modeling hypersonic flows around a double-cone.
We propose different training strategies involving temporal-bundling and
sub-sampling and demonstrate performance improvements for several benchmarks,
performing extrapolation for long time intervals as well as zero-shot
super-resolution in time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10907">The Role of Entropy and Reconstruction in Multi-View Self-Supervised Learning. (arXiv:2307.10907v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_Galvez_B/0/1/0/all/0/1">Borja Rodr&#xed;guez-G&#xe1;lvez</a>, <a href="http://arxiv.org/find/cs/1/au:+Blaas_A/0/1/0/all/0/1">Arno Blaas</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_P/0/1/0/all/0/1">Pau Rodr&#xed;guez</a>, <a href="http://arxiv.org/find/cs/1/au:+Golinski_A/0/1/0/all/0/1">Adam Goli&#x144;ski</a>, <a href="http://arxiv.org/find/cs/1/au:+Suau_X/0/1/0/all/0/1">Xavier Suau</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramapuram_J/0/1/0/all/0/1">Jason Ramapuram</a>, <a href="http://arxiv.org/find/cs/1/au:+Busbridge_D/0/1/0/all/0/1">Dan Busbridge</a>, <a href="http://arxiv.org/find/cs/1/au:+Zappella_L/0/1/0/all/0/1">Luca Zappella</a></p>
<p>The mechanisms behind the success of multi-view self-supervised learning
(MVSSL) are not yet fully understood. Contrastive MVSSL methods have been
studied through the lens of InfoNCE, a lower bound of the Mutual Information
(MI). However, the relation between other MVSSL methods and MI remains unclear.
We consider a different lower bound on the MI consisting of an entropy and a
reconstruction term (ER), and analyze the main MVSSL families through its lens.
Through this ER bound, we show that clustering-based methods such as
DeepCluster and SwAV maximize the MI. We also re-interpret the mechanisms of
distillation-based approaches such as BYOL and DINO, showing that they
explicitly maximize the reconstruction term and implicitly encourage a stable
entropy, and we confirm this empirically. We show that replacing the objectives
of common MVSSL methods with this ER bound achieves competitive performance,
while making them stable when training with smaller batch sizes or smaller
exponential moving average (EMA) coefficients.
</p>
<p>Github repo: https://github.com/apple/ml-entropy-reconstruction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.11730">Mitigating Communications Threats in Decentralized Federated Learning through Moving Target Defense. (arXiv:2307.11730v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Beltran_E/0/1/0/all/0/1">Enrique Tom&#xe1;s Mart&#xed;nez Beltr&#xe1;n</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanchez_P/0/1/0/all/0/1">Pedro Miguel S&#xe1;nchez S&#xe1;nchez</a>, <a href="http://arxiv.org/find/cs/1/au:+Bernal_S/0/1/0/all/0/1">Sergio L&#xf3;pez Bernal</a>, <a href="http://arxiv.org/find/cs/1/au:+Bovet_G/0/1/0/all/0/1">G&#xe9;r&#xf4;me Bovet</a>, <a href="http://arxiv.org/find/cs/1/au:+Perez_M/0/1/0/all/0/1">Manuel Gil P&#xe9;rez</a>, <a href="http://arxiv.org/find/cs/1/au:+Perez_G/0/1/0/all/0/1">Gregorio Mart&#xed;nez P&#xe9;rez</a>, <a href="http://arxiv.org/find/cs/1/au:+Celdran_A/0/1/0/all/0/1">Alberto Huertas Celdr&#xe1;n</a></p>
<p>The rise of Decentralized Federated Learning (DFL) has enabled the training
of machine learning models across federated participants, fostering
decentralized model aggregation and reducing dependence on a server. However,
this approach introduces unique communication security challenges that have yet
to be thoroughly addressed in the literature. These challenges primarily
originate from the decentralized nature of the aggregation process, the varied
roles and responsibilities of the participants, and the absence of a central
authority to oversee and mitigate threats. Addressing these challenges, this
paper first delineates a comprehensive threat model focused on DFL
communications. In response to these identified risks, this work introduces a
security module to counter communication-based attacks for DFL platforms. The
module combines security techniques such as symmetric and asymmetric encryption
with Moving Target Defense (MTD) techniques, including random neighbor
selection and IP/port switching. The security module is implemented in a DFL
platform, Fedstellar, allowing the deployment and monitoring of the federation.
A DFL scenario with physical and virtual deployments have been executed,
encompassing three security configurations: (i) a baseline without security,
(ii) an encrypted configuration, and (iii) a configuration integrating both
encryption and MTD techniques. The effectiveness of the security module is
validated through experiments with the MNIST dataset and eclipse attacks. The
results showed an average F1 score of 95%, with the most secure configuration
resulting in CPU usage peaking at 68% (+-9%) in virtual deployments and network
traffic reaching 480.8 MB (+-18 MB), effectively mitigating risks associated
with eavesdropping or eclipse attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.15967">Graph Condensation for Inductive Node Representation Learning. (arXiv:2307.15967v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xinyi Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zang_Y/0/1/0/all/0/1">Yilong Zang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wentao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1">Quoc Viet Hung Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1">Kai Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1">Hongzhi Yin</a></p>
<p>Graph neural networks (GNNs) encounter significant computational challenges
when handling large-scale graphs, which severely restricts their efficacy
across diverse applications. To address this limitation, graph condensation has
emerged as a promising technique, which constructs a small synthetic graph for
efficiently training GNNs while retaining performance. However, due to the
topology structure among nodes, graph condensation is limited to condensing
only the observed training nodes and their corresponding structure, thus
lacking the ability to effectively handle the unseen data. Consequently, the
original large graph is still required in the inference stage to perform
message passing to inductive nodes, resulting in substantial computational
demands. To overcome this issue, we propose mapping-aware graph condensation
(MCond), explicitly learning the one-to-many node mapping from original nodes
to synthetic nodes to seamlessly integrate new nodes into the synthetic graph
for inductive representation learning. This enables direct information
propagation on the synthetic graph, which is much more efficient than on the
original large graph. Specifically, MCond employs an alternating optimization
scheme with innovative loss terms from transductive and inductive perspectives,
facilitating the mutual promotion between graph condensation and node mapping
learning. Extensive experiments demonstrate the efficacy of our approach in
inductive inference. On the Reddit dataset, MCond achieves up to 121.5x
inference speedup and 55.9x reduction in storage requirements compared with
counterparts based on the original graph.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.02080">Causality Guided Disentanglement for Cross-Platform Hate Speech Detection. (arXiv:2308.02080v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sheth_P/0/1/0/all/0/1">Paras Sheth</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumarage_T/0/1/0/all/0/1">Tharindu Kumarage</a>, <a href="http://arxiv.org/find/cs/1/au:+Moraffah_R/0/1/0/all/0/1">Raha Moraffah</a>, <a href="http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1">Aman Chadha</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Huan Liu</a></p>
<p>Social media platforms, despite their value in promoting open discourse, are
often exploited to spread harmful content. Current deep learning and natural
language processing models used for detecting this harmful content overly rely
on domain-specific terms affecting their capabilities to adapt to generalizable
hate speech detection. This is because they tend to focus too narrowly on
particular linguistic signals or the use of certain categories of words.
Another significant challenge arises when platforms lack high-quality annotated
data for training, leading to a need for cross-platform models that can adapt
to different distribution shifts. Our research introduces a cross-platform hate
speech detection model capable of being trained on one platform's data and
generalizing to multiple unseen platforms. To achieve good generalizability
across platforms, one way is to disentangle the input representations into
invariant and platform-dependent features. We also argue that learning causal
relationships, which remain constant across diverse environments, can
significantly aid in understanding invariant representations in hate speech. By
disentangling input into platform-dependent features (useful for predicting
hate targets) and platform-independent features (used to predict the presence
of hate), we learn invariant representations resistant to distribution shifts.
These features are then used to predict hate speech across unseen platforms.
Our extensive experiments across four platforms highlight our model's enhanced
efficacy compared to existing state-of-the-art methods in detecting generalized
hate speech.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03230">Tractability of approximation by general shallow networks. (arXiv:2308.03230v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mhaskar_H/0/1/0/all/0/1">Hrushikesh Mhaskar</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_T/0/1/0/all/0/1">Tong Mao</a></p>
<p>In this paper, we present a sharper version of the results in the paper
Dimension independent bounds for general shallow networks; Neural Networks,
\textbf{123} (2020), 142-152. Let $\mathbb{X}$ and $\mathbb{Y}$ be compact
metric spaces. We consider approximation of functions of the form $
x\mapsto\int_{\mathbb{Y}} G( x, y)d\tau( y)$, $ x\in\mathbb{X}$, by
$G$-networks of the form $ x\mapsto \sum_{k=1}^n a_kG( x, y_k)$, $ y_1,\cdots,
y_n\in\mathbb{Y}$, $a_1,\cdots, a_n\in\mathbb{R}$. Defining the dimensions of
$\mathbb{X}$ and $\mathbb{Y}$ in terms of covering numbers, we obtain dimension
independent bounds on the degree of approximation in terms of $n$, where also
the constants involved are all dependent at most polynomially on the
dimensions. Applications include approximation by power rectified linear unit
networks, zonal function networks, certain radial basis function networks as
well as the important problem of function extension to higher dimensional
spaces.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04887">Targeted and Troublesome: Tracking and Advertising on Children&#x27;s Websites. (arXiv:2308.04887v2 [cs.CY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Moti_Z/0/1/0/all/0/1">Zahra Moti</a>, <a href="http://arxiv.org/find/cs/1/au:+Senol_A/0/1/0/all/0/1">Asuman Senol</a>, <a href="http://arxiv.org/find/cs/1/au:+Bostani_H/0/1/0/all/0/1">Hamid Bostani</a>, <a href="http://arxiv.org/find/cs/1/au:+Borgesius_F/0/1/0/all/0/1">Frederik Zuiderveen Borgesius</a>, <a href="http://arxiv.org/find/cs/1/au:+Moonsamy_V/0/1/0/all/0/1">Veelasha Moonsamy</a>, <a href="http://arxiv.org/find/cs/1/au:+Mathur_A/0/1/0/all/0/1">Arunesh Mathur</a>, <a href="http://arxiv.org/find/cs/1/au:+Acar_G/0/1/0/all/0/1">Gunes Acar</a></p>
<p>On the modern web, trackers and advertisers frequently construct and monetize
users' detailed behavioral profiles without consent. Despite various studies on
web tracking mechanisms and advertisements, there has been no rigorous study
focusing on websites targeted at children. To address this gap, we present a
measurement of tracking and (targeted) advertising on websites directed at
children. Motivated by lacking a comprehensive list of child-directed (i.e.,
targeted at children) websites, we first build a multilingual classifier based
on web page titles and descriptions. Applying this classifier to over two
million pages, we compile a list of two thousand child-directed websites.
Crawling these sites from five vantage points, we measure the prevalence of
trackers, fingerprinting scripts, and advertisements. Our crawler detects ads
displayed on child-directed websites and determines if ad targeting is enabled
by scraping ad disclosure pages whenever available. Our results show that
around 90% of child-directed websites embed one or more trackers, and about 27%
contain targeted advertisements--a practice that should require verifiable
parental consent. Next, we identify improper ads on child-directed websites by
developing an ML pipeline that processes both images and text extracted from
ads. The pipeline allows us to run semantic similarity queries for arbitrary
search terms, revealing ads that promote services related to dating, weight
loss, and mental health; as well as ads for sex toys and flirting chat
services. Some of these ads feature repulsive and sexually explicit imagery. In
summary, our findings indicate a trend of non-compliance with privacy
regulations and troubling ad safety practices among many advertisers and
child-directed websites. To protect children and create a safer online
environment, regulators and stakeholders must adopt and enforce more stringent
measures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.06399">Learning Bayesian Networks with Heterogeneous Agronomic Data Sets via Mixed-Effect Models and Hierarchical Clustering. (arXiv:2308.06399v4 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Valleggi_L/0/1/0/all/0/1">Lorenzo Valleggi</a>, <a href="http://arxiv.org/find/stat/1/au:+Scutari_M/0/1/0/all/0/1">Marco Scutari</a>, <a href="http://arxiv.org/find/stat/1/au:+Stefanini_F/0/1/0/all/0/1">Federico Mattia Stefanini</a></p>
<p>Maize, a crucial crop globally cultivated across vast regions, especially in
sub-Saharan Africa, Asia, and Latin America, occupies 197 million hectares as
of 2021. Various statistical and machine learning models, including
mixed-effect models, random coefficients models, random forests, and deep
learning architectures, have been devised to predict maize yield. These models
consider factors such as genotype, environment, genotype-environment
interaction, and field management. However, the existing models often fall
short of fully exploiting the complex network of causal relationships among
these factors and the hierarchical structure inherent in agronomic data. This
study introduces an innovative approach integrating random effects into
Bayesian networks (BNs), leveraging their capacity to model causal and
probabilistic relationships through directed acyclic graphs. Rooted in the
linear mixed-effects models framework and tailored for hierarchical data, this
novel approach demonstrates enhanced BN learning. Application to a real-world
agronomic trial produces a model with improved interpretability, unveiling new
causal connections. Notably, the proposed method significantly reduces the
error rate in maize yield prediction from 28% to 17%. These results advocate
for the preference of BNs in constructing practical decision support tools for
hierarchical agronomic data, facilitating causal inference.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.06838">Weisfeiler and Lehman Go Paths: Learning Topological Features via Path Complexes. (arXiv:2308.06838v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Truong_Q/0/1/0/all/0/1">Quang Truong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chin_P/0/1/0/all/0/1">Peter Chin</a></p>
<p>Graph Neural Networks (GNNs), despite achieving remarkable performance across
different tasks, are theoretically bounded by the 1-Weisfeiler-Lehman test,
resulting in limitations in terms of graph expressivity. Even though prior
works on topological higher-order GNNs overcome that boundary, these models
often depend on assumptions about sub-structures of graphs. Specifically,
topological GNNs leverage the prevalence of cliques, cycles, and rings to
enhance the message-passing procedure. Our study presents a novel perspective
by focusing on simple paths within graphs during the topological
message-passing process, thus liberating the model from restrictive inductive
biases. We prove that by lifting graphs to path complexes, our model can
generalize the existing works on topology while inheriting several theoretical
results on simplicial complexes and regular cell complexes. Without making
prior assumptions about graph sub-structures, our method outperforms earlier
works in other topological domains and achieves state-of-the-art results on
various benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.09895">Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs. (arXiv:2308.09895v3 [cs.PL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cassano_F/0/1/0/all/0/1">Federico Cassano</a>, <a href="http://arxiv.org/find/cs/1/au:+Gouwar_J/0/1/0/all/0/1">John Gouwar</a>, <a href="http://arxiv.org/find/cs/1/au:+Lucchetti_F/0/1/0/all/0/1">Francesca Lucchetti</a>, <a href="http://arxiv.org/find/cs/1/au:+Schlesinger_C/0/1/0/all/0/1">Claire Schlesinger</a>, <a href="http://arxiv.org/find/cs/1/au:+Anderson_C/0/1/0/all/0/1">Carolyn Jane Anderson</a>, <a href="http://arxiv.org/find/cs/1/au:+Greenberg_M/0/1/0/all/0/1">Michael Greenberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Jangda_A/0/1/0/all/0/1">Abhinav Jangda</a>, <a href="http://arxiv.org/find/cs/1/au:+Guha_A/0/1/0/all/0/1">Arjun Guha</a></p>
<p>Over the past few years, Large Language Models of Code (Code LLMs) have
started to have a significant impact on programming practice. Code LLMs are
also emerging as building blocks for research in programming languages and
software engineering. However, Code LLMs produce impressive results on
programming languages that are well represented in their training data (e.g.,
Java, Python, or JavaScript), but struggle with low-resource languages that
have limited training data available. Low resource languages include OCaml,
Racket, and several others.
</p>
<p>This paper presents an effective approach for boosting the performance of
Code LLMs on low-resource languages using semi-synthetic data. Our approach,
MultiPL-T, translates training data from high-resource languages into training
data for low-resource languages in the following way. 1) We use a Code LLM to
synthesize tests for commented code from a high-resource language, filtering
out faulty tests and code with low test coverage. 2) We use a Code LLM to
translate Python code to a target low-resource language, and use tests to
validate the translation. We apply this approach to generate tens of thousands
of validated training items for Julia, Lua, OCaml, R, and Racket. Furthermore,
we use an open model (StarCoderBase) with open training data (The Stack), which
allows us to decontaminate benchmarks, train models without violating licenses,
and run experiments that could not otherwise be done.
</p>
<p>With MultiPL-T generated data, we present fine-tuned versions of
StarCoderBase and Code Llama for Julia, Lua, OCaml, R, and Racket. On
established benchmarks (MultiPL-E), these models outperform other open Code
LLMs. The MultiPL-T approach is easy to apply to new languages, and is
significantly more efficient and effective than alternatives such as training
longer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.13466">Staleness-Alleviated Distributed GNN Training via Online Dynamic-Embedding Prediction. (arXiv:2308.13466v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bai_G/0/1/0/all/0/1">Guangji Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Ziyang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chai_Z/0/1/0/all/0/1">Zheng Chai</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1">Yue Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1">Liang Zhao</a></p>
<p>Despite the recent success of Graph Neural Networks (GNNs), it remains
challenging to train GNNs on large-scale graphs due to neighbor explosions. As
a remedy, distributed computing becomes a promising solution by leveraging
abundant computing resources (e.g., GPU). However, the node dependency of graph
data increases the difficulty of achieving high concurrency in distributed GNN
training, which suffers from the massive communication overhead. To address it,
Historical value approximation is deemed a promising class of distributed
training techniques. It utilizes an offline memory to cache historical
information (e.g., node embedding) as an affordable approximation of the exact
value and achieves high concurrency. However, such benefits come at the cost of
involving dated training information, leading to staleness, imprecision, and
convergence issues. To overcome these challenges, this paper proposes SAT
(Staleness-Alleviated Training), a novel and scalable distributed GNN training
framework that reduces the embedding staleness adaptively. The key idea of SAT
is to model the GNN's embedding evolution as a temporal graph and build a model
upon it to predict future embedding, which effectively alleviates the staleness
of the cached historical embedding. We propose an online algorithm to train the
embedding predictor and the distributed GNN alternatively and further provide a
convergence analysis. Empirically, we demonstrate that SAT can effectively
reduce embedding staleness and thus achieve better performance and convergence
speed on multiple large-scale graph datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16139">MedShapeNet -- A Large-Scale Dataset of 3D Medical Shapes for Computer Vision. (arXiv:2308.16139v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jianning Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zongwei Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jiancheng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pepe_A/0/1/0/all/0/1">Antonio Pepe</a>, <a href="http://arxiv.org/find/cs/1/au:+Gsaxner_C/0/1/0/all/0/1">Christina Gsaxner</a>, <a href="http://arxiv.org/find/cs/1/au:+Luijten_G/0/1/0/all/0/1">Gijs Luijten</a>, <a href="http://arxiv.org/find/cs/1/au:+Qu_C/0/1/0/all/0/1">Chongyu Qu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tiezheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xiaoxi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wenxuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wodzinski_M/0/1/0/all/0/1">Marek Wodzinski</a>, <a href="http://arxiv.org/find/cs/1/au:+Friedrich_P/0/1/0/all/0/1">Paul Friedrich</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_K/0/1/0/all/0/1">Kangxian Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1">Yuan Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ambigapathy_N/0/1/0/all/0/1">Narmada Ambigapathy</a>, <a href="http://arxiv.org/find/cs/1/au:+Nasca_E/0/1/0/all/0/1">Enrico Nasca</a>, <a href="http://arxiv.org/find/cs/1/au:+Solak_N/0/1/0/all/0/1">Naida Solak</a>, <a href="http://arxiv.org/find/cs/1/au:+Melito_G/0/1/0/all/0/1">Gian Marco Melito</a>, <a href="http://arxiv.org/find/cs/1/au:+Vu_V/0/1/0/all/0/1">Viet Duc Vu</a>, <a href="http://arxiv.org/find/cs/1/au:+Memon_A/0/1/0/all/0/1">Afaque R. Memon</a>, <a href="http://arxiv.org/find/cs/1/au:+Schlachta_C/0/1/0/all/0/1">Christopher Schlachta</a>, <a href="http://arxiv.org/find/cs/1/au:+Ribaupierre_S/0/1/0/all/0/1">Sandrine De Ribaupierre</a>, <a href="http://arxiv.org/find/cs/1/au:+Patel_R/0/1/0/all/0/1">Rajnikant Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Eagleson_R/0/1/0/all/0/1">Roy Eagleson</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xiaojun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Machler_H/0/1/0/all/0/1">Heinrich M&#xe4;chler</a>, <a href="http://arxiv.org/find/cs/1/au:+Kirschke_J/0/1/0/all/0/1">Jan Stefan Kirschke</a>, <a href="http://arxiv.org/find/cs/1/au:+Rosa_E/0/1/0/all/0/1">Ezequiel de la Rosa</a>, <a href="http://arxiv.org/find/cs/1/au:+Christ_P/0/1/0/all/0/1">Patrick Ferdinand Christ</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hongwei Bran Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ellis_D/0/1/0/all/0/1">David G. Ellis</a>, <a href="http://arxiv.org/find/cs/1/au:+Aizenberg_M/0/1/0/all/0/1">Michele R. Aizenberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Gatidis_S/0/1/0/all/0/1">Sergios Gatidis</a>, <a href="http://arxiv.org/find/cs/1/au:+Kustner_T/0/1/0/all/0/1">Thomas K&#xfc;stner</a>, <a href="http://arxiv.org/find/cs/1/au:+Shusharina_N/0/1/0/all/0/1">Nadya Shusharina</a>, <a href="http://arxiv.org/find/cs/1/au:+Heller_N/0/1/0/all/0/1">Nicholas Heller</a>, <a href="http://arxiv.org/find/cs/1/au:+Andrearczyk_V/0/1/0/all/0/1">Vincent Andrearczyk</a>, <a href="http://arxiv.org/find/cs/1/au:+Depeursinge_A/0/1/0/all/0/1">Adrien Depeursinge</a>, <a href="http://arxiv.org/find/cs/1/au:+Hatt_M/0/1/0/all/0/1">Mathieu Hatt</a>, <a href="http://arxiv.org/find/cs/1/au:+Sekuboyina_A/0/1/0/all/0/1">Anjany Sekuboyina</a>, <a href="http://arxiv.org/find/cs/1/au:+Loffler_M/0/1/0/all/0/1">Maximilian L&#xf6;ffler</a>, <a href="http://arxiv.org/find/cs/1/au:+Liebl_H/0/1/0/all/0/1">Hans Liebl</a>, <a href="http://arxiv.org/find/cs/1/au:+Dorent_R/0/1/0/all/0/1">Reuben Dorent</a>, <a href="http://arxiv.org/find/cs/1/au:+Vercauteren_T/0/1/0/all/0/1">Tom Vercauteren</a>, <a href="http://arxiv.org/find/cs/1/au:+Shapey_J/0/1/0/all/0/1">Jonathan Shapey</a>, <a href="http://arxiv.org/find/cs/1/au:+Kujawa_A/0/1/0/all/0/1">Aaron Kujawa</a>, <a href="http://arxiv.org/find/cs/1/au:+Cornelissen_S/0/1/0/all/0/1">Stefan Cornelissen</a>, et al. (110 additional authors not shown)</p>
<p>Prior to the deep learning era, \textit{shape} was commonly used to describe
the objects. Nowadays, state-of-the-art (SOTA) algorithms in medical imaging
are predominantly diverging from computer vision, where voxel grids, meshes,
point clouds, and implicit surface models are used. This is seen from numerous
shape-related publications in premier vision conferences as well as the growing
popularity of \textit{ShapeNet} (about 51,300 models) and \textit{Princeton
ModelNet} (127,915 models). For the medical domain, we present a large
collection of anatomical shapes (e.g., bones, organs, vessels) and 3D models of
surgical instrument, called \textit{MedShapeNet}, created to facilitate the
translation of data-driven vision algorithms to medical applications and to
adapt SOTA vision algorithms to medical problems. As a unique feature, we
directly model the majority of shapes on the imaging data of real patients. As
of today, \textit{MedShapeNet} includes 23 dataset with more than 100,000
shapes that are paired with annotations (ground truth). Our data is freely
accessible via a web interface and a Python application programming interface
(API) and can be used for discriminative, reconstructive, and variational
benchmarks as well as various applications in virtual, augmented, or mixed
reality, and 3D printing. Exemplary, we present use cases in the fields of
classification of brain tumors, facial and skull reconstructions, multi-class
anatomy completion, education, and 3D printing. In future, we will extend the
data and improve the interfaces. The project pages are:
\url{https://medshapenet.ikim.nrw/} and
\url{https://github.com/Jianningli/medshapenet-feedback}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.02521">Comparative Analysis of CPU and GPU Profiling for Deep Learning Models. (arXiv:2309.02521v3 [cs.DC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gyawali_D/0/1/0/all/0/1">Dipesh Gyawali</a></p>
<p>Deep Learning(DL) and Machine Learning(ML) applications are rapidly
increasing in recent days. Massive amounts of data are being generated over the
internet which can derive meaningful results by the use of ML and DL
algorithms. Hardware resources and open-source libraries have made it easy to
implement these algorithms. Tensorflow and Pytorch are one of the leading
frameworks for implementing ML projects. By using those frameworks, we can
trace the operations executed on both GPU and CPU to analyze the resource
allocations and consumption. This paper presents the time and memory allocation
of CPU and GPU while training deep neural networks using Pytorch. This paper
analysis shows that GPU has a lower running time as compared to CPU for deep
neural networks. For a simpler network, there are not many significant
improvements in GPU over the CPU.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.05961">Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering Trends across Diverse Platforms. (arXiv:2309.05961v3 [cs.SI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hazra_R/0/1/0/all/0/1">Rima Hazra</a>, <a href="http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1">Agnik Saha</a>, <a href="http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1">Somnath Banerjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1">Animesh Mukherjee</a></p>
<p>Community Question Answering (CQA) platforms steadily gain popularity as they
provide users with fast responses to their queries. The swiftness of these
responses is contingent on a mixture of query-specific and user-related
elements. This paper scrutinizes these contributing factors within the context
of six highly popular CQA platforms, identified through their standout
answering speed. Our investigation reveals a correlation between the time taken
to yield the first response to a question and several variables: the metadata,
the formulation of the questions, and the level of interaction among users.
Additionally, by employing conventional machine learning models to analyze
these metadata and patterns of user interaction, we endeavor to predict which
queries will receive their initial responses promptly.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07544">VerilogEval: Evaluating Large Language Models for Verilog Code Generation. (arXiv:2309.07544v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Mingjie Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pinckney_N/0/1/0/all/0/1">Nathaniel Pinckney</a>, <a href="http://arxiv.org/find/cs/1/au:+Khailany_B/0/1/0/all/0/1">Brucek Khailany</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1">Haoxing Ren</a></p>
<p>The increasing popularity of large language models (LLMs) has paved the way
for their application in diverse domains. This paper proposes a benchmarking
framework tailored specifically for evaluating LLM performance in the context
of Verilog code generation for hardware design and verification. We present a
comprehensive evaluation dataset consisting of 156 problems from the Verilog
instructional website HDLBits. The evaluation set consists of a diverse set of
Verilog code generation tasks, ranging from simple combinational circuits to
complex finite state machines. The Verilog code completions can be
automatically tested for functional correctness by comparing the transient
simulation outputs of the generated design with a golden solution. We also
demonstrate that the Verilog code generation capability of pretrained language
models could be improved with supervised fine-tuning by bootstrapping with LLM
generated synthetic problem-code pairs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.08788">BioinspiredLLM: Conversational Large Language Model for the Mechanics of Biological and Bio-inspired Materials. (arXiv:2309.08788v2 [cond-mat.mtrl-sci] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Luu_R/0/1/0/all/0/1">Rachel K. Luu</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Buehler_M/0/1/0/all/0/1">Markus J. Buehler</a></p>
<p>The study of biological materials and bio-inspired materials science is well
established; however, surprisingly little knowledge has been systematically
translated to engineering solutions. To accelerate discovery and guide
insights, an open-source autoregressive transformer large language model (LLM),
BioinspiredLLM, is reported. The model was finetuned with a corpus of over a
thousand peer-reviewed articles in the field of structural biological and
bio-inspired materials and can be prompted to recall information, assist with
research tasks, and function as an engine for creativity. The model has proven
that it is able to accurately recall information about biological materials and
is further enhanced with enhanced reasoning ability, as well as with
retrieval-augmented generation to incorporate new data during generation that
can also help to traceback sources, update the knowledge base, and connect
knowledge domains. BioinspiredLLM also has been shown to develop sound
hypotheses regarding biological materials design and remarkably so for
materials that have never been explicitly studied before. Lastly, the model
showed impressive promise in collaborating with other generative artificial
intelligence models in a workflow that can reshape the traditional materials
design process. This collaborative generative artificial intelligence method
can stimulate and enhance bio-inspired materials design workflows. Biological
materials are at a critical intersection of multiple scientific fields and
models like BioinspiredLLM help to connect knowledge domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.09043">Forward Invariance in Neural Network Controlled Systems. (arXiv:2309.09043v2 [eess.SY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Harapanahalli_A/0/1/0/all/0/1">Akash Harapanahalli</a>, <a href="http://arxiv.org/find/eess/1/au:+Jafarpour_S/0/1/0/all/0/1">Saber Jafarpour</a>, <a href="http://arxiv.org/find/eess/1/au:+Coogan_S/0/1/0/all/0/1">Samuel Coogan</a></p>
<p>We present a framework based on interval analysis and monotone systems theory
to certify and search for forward invariant sets in nonlinear systems with
neural network controllers. The framework (i) constructs localized first-order
inclusion functions for the closed-loop system using Jacobian bounds and
existing neural network verification tools; (ii) builds a dynamical embedding
system where its evaluation along a single trajectory directly corresponds with
a nested family of hyper-rectangles provably converging to an attractive set of
the original system; (iii) utilizes linear transformations to build families of
nested paralleletopes with the same properties. The framework is automated in
Python using our interval analysis toolbox $\texttt{npinterval}$, in
conjunction with the symbolic arithmetic toolbox $\texttt{sympy}$, demonstrated
on an $8$-dimensional leader-follower system.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.13775">The Rashomon Importance Distribution: Getting RID of Unstable, Single Model-based Variable Importance. (arXiv:2309.13775v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Donnelly_J/0/1/0/all/0/1">Jon Donnelly</a>, <a href="http://arxiv.org/find/cs/1/au:+Katta_S/0/1/0/all/0/1">Srikar Katta</a>, <a href="http://arxiv.org/find/cs/1/au:+Rudin_C/0/1/0/all/0/1">Cynthia Rudin</a>, <a href="http://arxiv.org/find/cs/1/au:+Browne_E/0/1/0/all/0/1">Edward P. Browne</a></p>
<p>Quantifying variable importance is essential for answering high-stakes
questions in fields like genetics, public policy, and medicine. Current methods
generally calculate variable importance for a given model trained on a given
dataset. However, for a given dataset, there may be many models that explain
the target outcome equally well; without accounting for all possible
explanations, different researchers may arrive at many conflicting yet equally
valid conclusions given the same data. Additionally, even when accounting for
all possible explanations for a given dataset, these insights may not
generalize because not all good explanations are stable across reasonable data
perturbations. We propose a new variable importance framework that quantifies
the importance of a variable across the set of all good models and is stable
across the data distribution. Our framework is extremely flexible and can be
integrated with most existing model classes and global variable importance
metrics. We demonstrate through experiments that our framework recovers
variable importance rankings for complex simulation setups where other methods
fail. Further, we show that our framework accurately estimates the true
importance of a variable for the underlying data distribution. We provide
theoretical guarantees on the consistency and finite sample error rates for our
estimator. Finally, we demonstrate its utility with a real-world case study
exploring which genes are important for predicting HIV load in persons with
HIV, highlighting an important gene that has not previously been studied in
connection with HIV. Code is available at
https://github.com/jdonnelly36/Rashomon_Importance_Distribution.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.14293">NAS-NeRF: Generative Neural Architecture Search for Neural Radiance Fields. (arXiv:2309.14293v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nair_S/0/1/0/all/0/1">Saeejith Nair</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuhao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shafiee_M/0/1/0/all/0/1">Mohammad Javad Shafiee</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1">Alexander Wong</a></p>
<p>Neural radiance fields (NeRFs) enable high-quality novel view synthesis, but
their high computational complexity limits deployability. While existing
neural-based solutions strive for efficiency, they use one-size-fits-all
architectures regardless of scene complexity. The same architecture may be
unnecessarily large for simple scenes but insufficient for complex ones. Thus,
there is a need to dynamically optimize the neural network component of NeRFs
to achieve a balance between computational complexity and specific targets for
synthesis quality. We introduce NAS-NeRF, a generative neural architecture
search strategy that generates compact, scene-specialized NeRF architectures by
balancing architecture complexity and target synthesis quality metrics. Our
method incorporates constraints on target metrics and budgets to guide the
search towards architectures tailored for each scene. Experiments on the
Blender synthetic dataset show the proposed NAS-NeRF can generate architectures
up to 5.74$\times$ smaller, with 4.19$\times$ fewer FLOPs, and 1.93$\times$
faster on a GPU than baseline NeRFs, without suffering a drop in SSIM.
Furthermore, we illustrate that NAS-NeRF can also achieve architectures up to
23$\times$ smaller, with 22$\times$ fewer FLOPs, and 4.7$\times$ faster than
baseline NeRFs with only a 5.3% average SSIM drop. Our source code is also made
publicly available at https://saeejithnair.github.io/NAS-NeRF.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.14482">LogGPT: Log Anomaly Detection via GPT. (arXiv:2309.14482v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1">Xiao Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1">Shuhan Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Trabelsi_M/0/1/0/all/0/1">Mohamed Trabelsi</a></p>
<p>Detecting system anomalies based on log data is important for ensuring the
security and reliability of computer systems. Recently, deep learning models
have been widely used for log anomaly detection. The core idea is to model the
log sequences as natural language and adopt deep sequential models, such as
LSTM or Transformer, to encode the normal patterns in log sequences via
language modeling. However, there is a gap between language modeling and
anomaly detection as the objective of training a sequential model via a
language modeling loss is not directly related to anomaly detection. To fill up
the gap, we propose LogGPT, a novel framework that employs GPT for log anomaly
detection. LogGPT is first trained to predict the next log entry based on the
preceding sequence. To further enhance the performance of LogGPT, we propose a
novel reinforcement learning strategy to finetune the model specifically for
the log anomaly detection task. The experimental results on three datasets show
that LogGPT significantly outperforms existing state-of-the-art approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.15127">Grad DFT: a software library for machine learning enhanced density functional theory. (arXiv:2309.15127v2 [physics.chem-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Casares_P/0/1/0/all/0/1">Pablo A. M. Casares</a>, <a href="http://arxiv.org/find/physics/1/au:+Baker_J/0/1/0/all/0/1">Jack S. Baker</a>, <a href="http://arxiv.org/find/physics/1/au:+Medvidovic_M/0/1/0/all/0/1">Matija Medvidovic</a>, <a href="http://arxiv.org/find/physics/1/au:+Reis_R/0/1/0/all/0/1">Roberto dos Reis</a>, <a href="http://arxiv.org/find/physics/1/au:+Arrazola_J/0/1/0/all/0/1">Juan Miguel Arrazola</a></p>
<p>Density functional theory (DFT) stands as a cornerstone method in
computational quantum chemistry and materials science due to its remarkable
versatility and scalability. Yet, it suffers from limitations in accuracy,
particularly when dealing with strongly correlated systems. To address these
shortcomings, recent work has begun to explore how machine learning can expand
the capabilities of DFT; an endeavor with many open questions and technical
challenges. In this work, we present Grad DFT: a fully differentiable JAX-based
DFT library, enabling quick prototyping and experimentation with machine
learning-enhanced exchange-correlation energy functionals. Grad DFT employs a
pioneering parametrization of exchange-correlation functionals constructed
using a weighted sum of energy densities, where the weights are determined
using neural networks. Moreover, Grad DFT encompasses a comprehensive suite of
auxiliary functions, notably featuring a just-in-time compilable and fully
differentiable self-consistent iterative procedure. To support training and
benchmarking efforts, we additionally compile a curated dataset of experimental
dissociation energies of dimers, half of which contain transition metal atoms
characterized by strong electronic correlations. The software library is tested
against experimental results to study the generalization capabilities of a
neural functional across potential energy surfaces and atomic species, as well
as the effect of training data noise on the resulting model accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.15214">Residual Diffusion Modeling for Km-scale Atmospheric Downscaling. (arXiv:2309.15214v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mardani_M/0/1/0/all/0/1">Morteza Mardani</a>, <a href="http://arxiv.org/find/cs/1/au:+Brenowitz_N/0/1/0/all/0/1">Noah Brenowitz</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_Y/0/1/0/all/0/1">Yair Cohen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pathak_J/0/1/0/all/0/1">Jaideep Pathak</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chieh-Yu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Cheng-Chin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Vahdat_A/0/1/0/all/0/1">Arash Vahdat</a>, <a href="http://arxiv.org/find/cs/1/au:+Kashinath_K/0/1/0/all/0/1">Karthik Kashinath</a>, <a href="http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1">Jan Kautz</a>, <a href="http://arxiv.org/find/cs/1/au:+Pritchard_M/0/1/0/all/0/1">Mike Pritchard</a></p>
<p>Predictions of weather hazard require expensive km-scale simulations driven
by coarser global inputs. Here, a cost-effective stochastic downscaling model
is trained from a high-resolution 2-km weather model over Taiwan conditioned on
25-km ERA5 reanalysis. To address the multi-scale machine learning challenges
of weather data, we employ a two-step approach Corrector Diffusion
(\textit{CorrDiff}), where a UNet prediction of the mean is corrected by a
diffusion step. Akin to Reynolds decomposition in fluid dynamics, this isolates
generative learning to the stochastic scales. \textit{CorrDiff} exhibits
skillful RMSE and CRPS and faithfully recovers spectra and distributions even
for extremes. Case studies of coherent weather phenomena reveal appropriate
multivariate relationships reminiscent of learnt physics: the collocation of
intense rainfall and sharp gradients in fronts and extreme winds and rainfall
bands near the eyewall of typhoons. Downscaling global forecasts successfully
retains many of these benefits, foreshadowing the potential of end-to-end,
global-to-km-scales machine learning weather predictions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.15593">Exciton-Polariton Condensates: A Fourier Neural Operator Approach. (arXiv:2309.15593v2 [cond-mat.quant-gas] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Sathujoda_S/0/1/0/all/0/1">Surya T. Sathujoda</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Wang_Y/0/1/0/all/0/1">Yuan Wang</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Gandhi_K/0/1/0/all/0/1">Kanishk Gandhi</a></p>
<p>Advancements in semiconductor fabrication over the past decade have catalyzed
extensive research into all-optical devices driven by exciton-polariton
condensates. Preliminary validations of such devices, including transistors,
have shown encouraging results even under ambient conditions. A significant
challenge still remains for large scale application however: the lack of a
robust solver that can be used to simulate complex nonlinear systems which
require an extended period of time to stabilize. Addressing this need, we
propose the application of a machine-learning-based Fourier Neural Operator
approach to find the solution to the Gross-Pitaevskii equations coupled with
extra exciton rate equations. This work marks the first direct application of
Neural Operators to an exciton-polariton condensate system. Our findings show
that the proposed method can predict final-state solutions to a high degree of
accuracy almost 1000 times faster than CUDA-based GPU solvers. Moreover, this
paves the way for potential all-optical chip design workflows by integrating
experimental data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.15726">Factorized Diffusion Architectures for Unsupervised Image Generation and Segmentation. (arXiv:2309.15726v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1">Xin Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Maire_M/0/1/0/all/0/1">Michael Maire</a></p>
<p>We develop a neural network architecture which, trained in an unsupervised
manner as a denoising diffusion model, simultaneously learns to both generate
and segment images. Learning is driven entirely by the denoising diffusion
objective, without any annotation or prior knowledge about regions during
training. A computational bottleneck, built into the neural architecture,
encourages the denoising network to partition an input into regions, denoise
them in parallel, and combine the results. Our trained model generates both
synthetic images and, by simple examination of its internal predicted
partitions, a semantic segmentation of those images. Without any finetuning, we
directly apply our unsupervised model to the downstream task of segmenting real
images via noising and subsequently denoising them. Experiments demonstrate
that our model achieves accurate unsupervised image segmentation and
high-quality synthetic image generation across multiple datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16066">Label Augmentation Method for Medical Landmark Detection in Hip Radiograph Images. (arXiv:2309.16066v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Suh_Y/0/1/0/all/0/1">Yehyun Suh</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_P/0/1/0/all/0/1">Peter Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Martin_J/0/1/0/all/0/1">J.Ryan Martin</a>, <a href="http://arxiv.org/find/cs/1/au:+Moyer_D/0/1/0/all/0/1">Daniel Moyer</a></p>
<p>This work reports the empirical performance of an automated medical landmark
detection method for predict clinical markers in hip radiograph images.
Notably, the detection method was trained using a label-only augmentation
scheme; our results indicate that this form of augmentation outperforms
traditional data augmentation and produces highly sample efficient estimators.
We train a generic U-Net-based architecture under a curriculum consisting of
two phases: initially relaxing the landmarking task by enlarging the label
points to regions, then gradually eroding these label regions back to the base
task. We measure the benefits of this approach on six datasets of radiographs
with gold-standard expert annotations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01770">A simple connection from loss flatness to compressed representations in neural networks. (arXiv:2310.01770v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shirui Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Recanatesi_S/0/1/0/all/0/1">Stefano Recanatesi</a>, <a href="http://arxiv.org/find/cs/1/au:+Shea_Brown_E/0/1/0/all/0/1">Eric Shea-Brown</a></p>
<p>The generalization capacity of deep neural networks has been studied in a
variety of ways, including at least two distinct categories of approach: one
based on the shape of the loss landscape in parameter space, and the other
based on the structure of the representation manifold in feature space (that
is, in the space of unit activities). Although these two approaches are
related, they are rarely studied together in an explicit connection. Here, we
present a simple analysis that makes such a connection. We show that, in the
last phase of learning of deep neural networks, compression of the manifold of
neural representations correlates with the flatness of the loss around the
minima explored by SGD. We show that this is predicted by a relatively simple
mathematical relationship: a flatter loss corresponds to a lower upper-bound on
the compression of neural representations. Our results closely build on the
prior work of Ma and Ying, who demonstrated how flatness, characterized by
small eigenvalues of the loss Hessian, develops in late learning phases and
contributes to robustness against perturbations in network inputs. Moreover, we
show a lack of a similarly direct connection between local dimensionality and
sharpness, suggesting that this property may be controlled by different
mechanisms than volume and hence may play a complementary role in neural
representations. Overall, we advance a dual perspective on generalization in
neural networks in both parameter and feature space.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02003">L2MAC: Large Language Model Automatic Computer for Unbounded Code Generation. (arXiv:2310.02003v2 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Holt_S/0/1/0/all/0/1">Samuel Holt</a>, <a href="http://arxiv.org/find/cs/1/au:+Luyten_M/0/1/0/all/0/1">Max Ruiz Luyten</a>, <a href="http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1">Mihaela van der Schaar</a></p>
<p>Transformer-based large language models (LLMs) are constrained by the fixed
context window of the underlying transformer architecture, hindering their
ability to produce long and logically consistent code. Memory-augmented LLMs
are a promising solution, but current approaches cannot handle long code
generation tasks since they (1) only focus on reading memory and reduce its
evolution to the concatenation of new memories or (2) use very specialized
memories that cannot adapt to other domains. This paper presents L2MAC, the
first practical LLM-based stored-program automatic computer for long and
consistent code generation. Its memory has two components: the instruction
registry, which is populated with a prompt program to solve the user-given
task, and a file store, which will contain the final and intermediate outputs.
Each instruction is executed by a separate LLM instance, whose context is
managed by a control unit capable of precise memory reading and writing to
ensure effective interaction with the file store. These components enable L2MAC
to generate virtually unbounded code structures, bypassing the constraints of
the finite context window while producing code that fulfills complex
user-specified requirements. We empirically show that L2MAC succeeds in
generating large code bases for system design tasks where other coding methods
fall short in implementing user requirements and provide insight into the
reasons for this performance gap.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02854">Multi-Domain Causal Representation Learning via Weak Distributional Invariances. (arXiv:2310.02854v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ahuja_K/0/1/0/all/0/1">Kartik Ahuja</a>, <a href="http://arxiv.org/find/cs/1/au:+Mansouri_A/0/1/0/all/0/1">Amin Mansouri</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yixin Wang</a></p>
<p>Causal representation learning has emerged as the center of action in causal
machine learning research. In particular, multi-domain datasets present a
natural opportunity for showcasing the advantages of causal representation
learning over standard unsupervised representation learning. While recent works
have taken crucial steps towards learning causal representations, they often
lack applicability to multi-domain datasets due to over-simplifying assumptions
about the data; e.g. each domain comes from a different single-node perfect
intervention. In this work, we relax these assumptions and capitalize on the
following observation: there often exists a subset of latents whose certain
distributional properties (e.g., support, variance) remain stable across
domains; this property holds when, for example, each domain comes from a
multi-node imperfect intervention. Leveraging this observation, we show that
autoencoders that incorporate such invariances can provably identify the stable
set of latents from the rest across different settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03494">How the level sampling process impacts zero-shot generalisation in deep reinforcement learning. (arXiv:2310.03494v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Garcin_S/0/1/0/all/0/1">Samuel Garcin</a>, <a href="http://arxiv.org/find/cs/1/au:+Doran_J/0/1/0/all/0/1">James Doran</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1">Shangmin Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Lucas_C/0/1/0/all/0/1">Christopher G. Lucas</a>, <a href="http://arxiv.org/find/cs/1/au:+Albrecht_S/0/1/0/all/0/1">Stefano V. Albrecht</a></p>
<p>A key limitation preventing the wider adoption of autonomous agents trained
via deep reinforcement learning (RL) is their limited ability to generalise to
new environments, even when these share similar characteristics with
environments encountered during training. In this work, we investigate how a
non-uniform sampling strategy of individual environment instances, or levels,
affects the zero-shot generalisation (ZSG) ability of RL agents, considering
two failure modes: overfitting and over-generalisation. As a first step, we
measure the mutual information (MI) between the agent's internal representation
and the set of training levels, which we find to be well-correlated to instance
overfitting. In contrast to uniform sampling, adaptive sampling strategies
prioritising levels based on their value loss are more effective at maintaining
lower MI, which provides a novel theoretical justification for this class of
techniques. We then turn our attention to unsupervised environment design (UED)
methods, which adaptively generate new training levels and minimise MI more
effectively than methods sampling from a fixed set. However, we find UED
methods significantly shift the training distribution, resulting in
over-generalisation and worse ZSG performance over the distribution of
interest. To prevent both instance overfitting and over-generalisation, we
introduce self-supervised environment design (SSED). SSED generates levels
using a variational autoencoder, effectively reducing MI while minimising the
shift with the distribution of interest, and leads to statistically significant
improvements in ZSG over fixed-set level sampling strategies and UED methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04353">A Language-Agent Approach to Formal Theorem-Proving. (arXiv:2310.04353v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Thakur_A/0/1/0/all/0/1">Amitayush Thakur</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1">Yeming Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1">Swarat Chaudhuri</a></p>
<p>Language agents, which use a large language model (LLM) capable of in-context
learning to interact with an external environment, have recently emerged as a
promising approach to control tasks. We present the first language-agent
approach to formal theorem-proving. Our method, COPRA, uses a high-capacity,
black-box LLM (GPT-4) as part of a policy for a stateful backtracking search.
During the search, the policy can select proof tactics and retrieve lemmas and
definitions from an external database. Each selected tactic is executed in the
underlying proof framework, and the execution feedback is used to build the
prompt for the next policy invocation. The search also tracks selected
information from its history and uses it to reduce hallucinations and
unnecessary LLM queries.
</p>
<p>We evaluate our implementation of COPRA on the miniF2F benchmark for Lean and
a set of Coq tasks from the Compcert project. On these benchmarks, COPRA
significantly outperforms one-shot invocations of GPT-4, as well as
state-of-the-art models fine-tuned on proof data, at finding correct proofs
quickly. Our code and data are available at
https://github.com/trishullab/copra.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05365">Molecular De Novo Design through Transformer-based Reinforcement Learning. (arXiv:2310.05365v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_T/0/1/0/all/0/1">Tao Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1">Pengcheng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_T/0/1/0/all/0/1">Tianfan Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Laghuvarapu_S/0/1/0/all/0/1">Siddhartha Laghuvarapu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jimeng Sun</a></p>
<p>In this work, we introduce a method to fine-tune a Transformer-based
generative model for molecular de novo design. Leveraging the superior sequence
learning capacity of Transformers over Recurrent Neural Networks (RNNs), our
model can generate molecular structures with desired properties effectively. In
contrast to the traditional RNN-based models, our proposed method exhibits
superior performance in generating compounds predicted to be active against
various biological targets, capturing long-term dependencies in the molecular
structure sequence. The model's efficacy is demonstrated across numerous tasks,
including generating analogues to a query structure and producing compounds
with particular attributes, outperforming the baseline RNN-based methods. Our
approach can be used for scaffold hopping, library expansion starting from a
single molecule, and generating compounds with high predicted activity against
biological targets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05742">Estimating Shape Distances on Neural Representations with Limited Samples. (arXiv:2310.05742v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Pospisil_D/0/1/0/all/0/1">Dean A. Pospisil</a>, <a href="http://arxiv.org/find/stat/1/au:+Larsen_B/0/1/0/all/0/1">Brett W. Larsen</a>, <a href="http://arxiv.org/find/stat/1/au:+Harvey_S/0/1/0/all/0/1">Sarah E. Harvey</a>, <a href="http://arxiv.org/find/stat/1/au:+Williams_A/0/1/0/all/0/1">Alex H. Williams</a></p>
<p>Measuring geometric similarity between high-dimensional network
representations is a topic of longstanding interest to neuroscience and deep
learning. Although many methods have been proposed, only a few works have
rigorously analyzed their statistical efficiency or quantified estimator
uncertainty in data-limited regimes. Here, we derive upper and lower bounds on
the worst-case convergence of standard estimators of shape
distance$\unicode{x2014}$a measure of representational dissimilarity proposed
by Williams et al. (2021).These bounds reveal the challenging nature of the
problem in high-dimensional feature spaces. To overcome these challenges, we
introduce a new method-of-moments estimator with a tunable bias-variance
tradeoff. We show that this estimator achieves substantially lower bias than
standard estimators in simulation and on neural data, particularly in
high-dimensional settings. Thus, we lay the foundation for a rigorous
statistical theory for high-dimensional shape analysis, and we contribute a new
estimation method that is well-suited to practical scientific settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.06648">Diversity from Human Feedback. (arXiv:2310.06648v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Ren-Jian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_K/0/1/0/all/0/1">Ke Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yutong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1">Peng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1">Haobo Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1">Qiang Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1">Chao Qian</a></p>
<p>Diversity plays a significant role in many problems, such as ensemble
learning, reinforcement learning, and combinatorial optimization. How to define
the diversity measure is a longstanding problem. Many methods rely on expert
experience to define a proper behavior space and then obtain the diversity
measure, which is, however, challenging in many scenarios. In this paper, we
propose the problem of learning a behavior space from human feedback and
present a general method called Diversity from Human Feedback (DivHF) to solve
it. DivHF learns a behavior descriptor consistent with human preference by
querying human feedback. The learned behavior descriptor can be combined with
any distance measure to define a diversity measure. We demonstrate the
effectiveness of DivHF by integrating it with the Quality-Diversity
optimization algorithm MAP-Elites and conducting experiments on the QDax suite.
The results show that DivHF learns a behavior space that aligns better with
human requirements compared to direct data-driven approaches and leads to more
diverse solutions under human preference. Our contributions include formulating
the problem, proposing the DivHF method, and demonstrating its effectiveness
through experiments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07427">Quantum-Enhanced Forecasting: Leveraging Quantum Gramian Angular Field and CNNs for Stock Return Predictions. (arXiv:2310.07427v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zhengmeng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yujie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1">Xiaotong Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yilin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yanli Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1">Hai Lin</a></p>
<p>We propose a time series forecasting method named Quantum Gramian Angular
Field (QGAF). This approach merges the advantages of quantum computing
technology with deep learning, aiming to enhance the precision of time series
classification and forecasting. We successfully transformed stock return time
series data into two-dimensional images suitable for Convolutional Neural
Network (CNN) training by designing specific quantum circuits. Distinct from
the classical Gramian Angular Field (GAF) approach, QGAF's uniqueness lies in
eliminating the need for data normalization and inverse cosine calculations,
simplifying the transformation process from time series data to two-dimensional
images. To validate the effectiveness of this method, we conducted experiments
on datasets from three major stock markets: the China A-share market, the Hong
Kong stock market, and the US stock market. Experimental results revealed that
compared to the classical GAF method, the QGAF approach significantly improved
time series prediction accuracy, reducing prediction errors by an average of
25% for Mean Absolute Error (MAE) and 48% for Mean Squared Error (MSE). This
research confirms the potential and promising prospects of integrating quantum
computing with deep learning techniques in financial time series forecasting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.12609">Denoising Heat-inspired Diffusion with Insulators for Collision Free Motion Planning. (arXiv:2310.12609v3 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1">Junwoo Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ryu_H/0/1/0/all/0/1">Hyunwoo Ryu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jiwoo Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1">Soochul Yoo</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1">Jongeun Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1">Joohwan Seo</a>, <a href="http://arxiv.org/find/cs/1/au:+Prakash_N/0/1/0/all/0/1">Nikhil Prakash</a>, <a href="http://arxiv.org/find/cs/1/au:+Horowitz_R/0/1/0/all/0/1">Roberto Horowitz</a></p>
<p>Diffusion models have risen as a powerful tool in robotics due to their
flexibility and multi-modality. While some of these methods effectively address
complex problems, they often depend heavily on inference-time obstacle
detection and require additional equipment. Addressing these challenges, we
present a method that, during inference time, simultaneously generates only
reachable goals and plans motions that avoid obstacles, all from a single
visual input. Central to our approach is the novel use of a collision-avoiding
diffusion kernel for training. Through evaluations against behavior-cloning and
classical diffusion models, our framework has proven its robustness. It is
particularly effective in multi-modal environments, navigating toward goals and
avoiding unreachable ones blocked by obstacles, while ensuring collision
avoidance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.13164">Almost Equivariance via Lie Algebra Convolutions. (arXiv:2310.13164v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+McNeela_D/0/1/0/all/0/1">Daniel McNeela</a></p>
<p>Recently, the equivariance of models with respect to a group action has
become an important topic of research in machine learning. Analysis of the
built-in equivariance of existing neural network architectures, as well as the
study of building models that explicitly "bake in" equivariance, have become
significant research areas in their own right. However, imbuing an architecture
with a specific group equivariance imposes a strong prior on the types of data
transformations that the model expects to see. While strictly-equivariant
models enforce symmetries, real-world data does not always conform to such
strict equivariances. In such cases, the prior of strict equivariance can
actually prove too strong and cause models to underperform. Therefore, in this
work we study a closely related topic, that of almost equivariance. We provide
a definition of almost equivariance and give a practical method for encoding
almost equivariance in models by appealing to the Lie algebra of a Lie group.
Specifically, we define Lie algebra convolutions and demonstrate that they
offer several benefits over Lie group convolutions, including being
well-defined for non-compact Lie groups having non-surjective exponential map.
From there, we demonstrate connections between the notions of equivariance and
isometry and those of almost equivariance and almost isometry. We prove two
existence theorems, one showing the existence of almost isometries within
bounded distance of isometries of a manifold, and another showing the converse
for Hilbert spaces. We extend these theorems to prove the existence of almost
equivariant manifold embeddings within bounded distance of fully equivariant
embedding functions, subject to certain constraints on the group action and the
function class. Finally, we demonstrate the validity of our approach by
benchmarking against datasets in fully equivariant and almost equivariant
settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.15074">MGAS: Multi-Granularity Architecture Search for Trade-Off Between Model Effectiveness and Efficiency. (arXiv:2310.15074v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiaoyun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Saxena_D/0/1/0/all/0/1">Divya Saxena</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1">Jiannong Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yuqing Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruan_P/0/1/0/all/0/1">Penghui Ruan</a></p>
<p>Neural architecture search (NAS) has gained significant traction in
automating the design of neural networks. To reduce the time cost,
differentiable architecture search (DAS) transforms the traditional paradigm of
discrete candidate sampling and evaluation into that of differentiable
super-net optimization and discretization. However, existing DAS methods fail
to trade off between model performance and model size. They either only conduct
coarse-grained operation-level search, which results in redundant model
parameters, or restrictively explore fine-grained filter-level and weight-level
units with pre-defined remaining ratios, suffering from excessive pruning
problem. Additionally, these methods compromise search quality to save memory
during the search process. To tackle these issues, we introduce
multi-granularity architecture search (MGAS), a unified framework which aims to
discover both effective and efficient neural networks by comprehensively yet
memory-efficiently exploring the multi-granularity search space. Specifically,
we improve the existing DAS methods in two aspects. First, we balance the model
unit numbers at different granularity levels with adaptive pruning. We learn
discretization functions specific to each granularity level to adaptively
determine the unit remaining ratio according to the evolving architecture.
Second, we reduce the memory consumption without degrading the search quality
using multi-stage search. We break down the super-net optimization and
discretization into multiple sub-net stages, and perform progressive
re-evaluation to allow for re-pruning and regrowing of previous units during
subsequent stages, compensating for potential bias. Extensive experiments on
CIFAR-10, CIFAR-100 and ImageNet demonstrate that MGAS outperforms other
state-of-the-art methods in achieving a better trade-off between model
performance and model size.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19614">Dis-inhibitory neuronal circuits can control the sign of synaptic plasticity. (arXiv:2310.19614v2 [q-bio.NC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Rossbroich_J/0/1/0/all/0/1">Julian Rossbroich</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Zenke_F/0/1/0/all/0/1">Friedemann Zenke</a></p>
<p>How neuronal circuits achieve credit assignment remains a central unsolved
question in systems neuroscience. Various studies have suggested plausible
solutions for back-propagating error signals through multi-layer networks.
These purely functionally motivated models assume distinct neuronal
compartments to represent local error signals that determine the sign of
synaptic plasticity. However, this explicit error modulation is inconsistent
with phenomenological plasticity models in which the sign depends primarily on
postsynaptic activity. Here we show how a plausible microcircuit model and
Hebbian learning rule derived within an adaptive control theory framework can
resolve this discrepancy. Assuming errors are encoded in top-down
dis-inhibitory synaptic afferents, we show that error-modulated learning
emerges naturally at the circuit level when recurrent inhibition explicitly
influences Hebbian plasticity. The same learning rule accounts for
experimentally observed plasticity in the absence of inhibition and performs
comparably to back-propagation of error (BP) on several non-linearly separable
benchmarks. Our findings bridge the gap between functional and experimentally
observed plasticity rules and make concrete predictions on inhibitory
modulation of excitatory plasticity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.20187">Self-Supervised Pre-Training for Precipitation Post-Processor. (arXiv:2310.20187v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+An_S/0/1/0/all/0/1">Sojung An</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Junha Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1">Jiyeon Jang</a>, <a href="http://arxiv.org/find/cs/1/au:+Na_I/0/1/0/all/0/1">Inchae Na</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_W/0/1/0/all/0/1">Wooyeon Park</a>, <a href="http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1">Sujeong You</a></p>
<p>Obtaining a sufficient forecast lead time for local precipitation is
essential in preventing hazardous weather events. Global warming-induced
climate change increases the challenge of accurately predicting severe
precipitation events, such as heavy rainfall. In this paper, we propose a deep
learning-based precipitation post-processor for numerical weather prediction
(NWP) models. The precipitation post-processor consists of (i) employing
self-supervised pre-training, where the parameters of the encoder are
pre-trained on the reconstruction of the masked variables of the atmospheric
physics domain; and (ii) conducting transfer learning on precipitation
segmentation tasks (the target domain) from the pre-trained encoder. In
addition, we introduced a heuristic labeling approach to effectively train
class-imbalanced datasets. Our experiments on precipitation correction for
regional NWP show that the proposed method outperforms other approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00286">JADE: A Linguistics-based Safety Evaluation Platform for Large Language Models. (arXiv:2311.00286v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Mi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1">Xudong Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Min Yang</a></p>
<p>In this paper, we present JADE, a targeted linguistic fuzzing platform which
strengthens the linguistic complexity of seed questions to simultaneously and
consistently break a wide range of widely-used LLMs categorized in three
groups: eight open-sourced Chinese, six commercial Chinese and four commercial
English LLMs. JADE generates three safety benchmarks for the three groups of
LLMs, which contain unsafe questions that are highly threatening: the questions
simultaneously trigger harmful generation of multiple LLMs, with an average
unsafe generation ratio of $70\%$ (please see the table below), while are still
natural questions, fluent and preserving the core unsafe semantics. We release
the benchmark demos generated for commercial English LLMs and open-sourced
English LLMs in the following link: https://github.com/whitzard-ai/jade-db. For
readers who are interested in evaluating on more questions generated by JADE,
please contact us.
</p>
<p>JADE is based on Noam Chomsky's seminal theory of transformational-generative
grammar. Given a seed question with unsafe intention, JADE invokes a sequence
of generative and transformational rules to increment the complexity of the
syntactic structure of the original question, until the safety guardrail is
broken. Our key insight is: Due to the complexity of human language, most of
the current best LLMs can hardly recognize the invariant evil from the infinite
number of different syntactic structures which form an unbound example space
that can never be fully covered. Technically, the generative/transformative
rules are constructed by native speakers of the languages, and, once developed,
can be used to automatically grow and transform the parse tree of a given
question, until the guardrail is broken. For more evaluation results and demo,
please check our website: https://whitzard-ai.github.io/jade.html.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01223">Diffusion Models for Reinforcement Learning: A Survey. (arXiv:2311.01223v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Zhengbang Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Hanye Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1">Haoran He</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1">Yichao Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shenyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yong Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Weinan Zhang</a></p>
<p>Diffusion models have emerged as a prominent class of generative models,
surpassing previous methods regarding sample quality and training stability.
Recent works have shown the advantages of diffusion models in improving
reinforcement learning (RL) solutions, including as trajectory planners,
expressive policy classes, data synthesizers, etc. This survey aims to provide
an overview of the advancements in this emerging field and hopes to inspire new
avenues of research. First, we examine several challenges encountered by
current RL algorithms. Then, we present a taxonomy of existing methods based on
the roles played by diffusion models in RL and explore how the existing
challenges are addressed. We further outline successful applications of
diffusion models in various RL-related tasks while discussing the limitations
of current approaches. Finally, we conclude the survey and offer insights into
future research directions, focusing on enhancing model performance and
applying diffusion models to broader tasks. We are actively maintaining a
GitHub repository for papers and other related resources in applying diffusion
models in RL: https://github.com/apexrl/Diff4RLSurvey
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04673">Compressive Recovery of Sparse Precision Matrices. (arXiv:2311.04673v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Vayer_T/0/1/0/all/0/1">Titouan Vayer</a>, <a href="http://arxiv.org/find/stat/1/au:+Lasalle_E/0/1/0/all/0/1">Etienne Lasalle</a>, <a href="http://arxiv.org/find/stat/1/au:+Gribonval_R/0/1/0/all/0/1">R&#xe9;mi Gribonval</a>, <a href="http://arxiv.org/find/stat/1/au:+Goncalves_P/0/1/0/all/0/1">Paulo Gon&#xe7;alves</a></p>
<p>We consider the problem of learning a graph modeling the statistical
relations of the $d$ variables from a dataset with $n$ samples $X \in
\mathbb{R}^{n \times d}$. Standard approaches amount to searching for a
precision matrix $\Theta$ representative of a Gaussian graphical model that
adequately explains the data. However, most maximum likelihood-based estimators
usually require storing the $d^{2}$ values of the empirical covariance matrix,
which can become prohibitive in a high-dimensional setting. In this work, we
adopt a compressive viewpoint and aim to estimate a sparse $\Theta$ from a
\emph{sketch} of the data, i.e. a low-dimensional vector of size $m \ll d^{2}$
carefully designed from $X$ using non-linear random features. Under certain
assumptions on the spectrum of $\Theta$ (or its condition number), we show that
it is possible to estimate it from a sketch of size
$m=\Omega\left((d+2k)\log(d)\right)$ where $k$ is the maximal number of edges
of the underlying graph. These information-theoretic guarantees are inspired by
compressed sensing theory and involve restricted isometry properties and
instance optimal decoders. We investigate the possibility of achieving
practical recovery with an iterative algorithm based on the graphical lasso,
viewed as a specific denoiser. We compare our approach and graphical lasso on
synthetic datasets, demonstrating its favorable performance even when the
dataset is compressed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04916">Explainable Identification of Hate Speech towards Islam using Graph Neural Networks. (arXiv:2311.04916v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wasi_A/0/1/0/all/0/1">Azmine Toushik Wasi</a></p>
<p>Islamophobic language is a prevalent challenge on online social interaction
platforms. Identifying and eliminating such hatred is a crucial step towards a
future of harmony and peace. This study presents a novel paradigm for
identifying and explaining hate speech towards Islam using graph neural
networks. Utilizing the intrinsic ability of graph neural networks to find,
extract, and use relationships across disparate data points, our model
consistently achieves outstanding performance while offering explanations for
the underlying correlations and causation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05135">Improving Computational Efficiency for Powered Descent Guidance via Transformer-based Tight Constraint Prediction. (arXiv:2311.05135v2 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Briden_J/0/1/0/all/0/1">Julia Briden</a>, <a href="http://arxiv.org/find/math/1/au:+Gurga_T/0/1/0/all/0/1">Trey Gurga</a>, <a href="http://arxiv.org/find/math/1/au:+Johnson_B/0/1/0/all/0/1">Breanna Johnson</a>, <a href="http://arxiv.org/find/math/1/au:+Cauligi_A/0/1/0/all/0/1">Abhishek Cauligi</a>, <a href="http://arxiv.org/find/math/1/au:+Linares_R/0/1/0/all/0/1">Richard Linares</a></p>
<p>In this work, we present Transformer-based Powered Descent Guidance (T-PDG),
a scalable algorithm for reducing the computational complexity of the direct
optimization formulation of the spacecraft powered descent guidance problem.
T-PDG uses data from prior runs of trajectory optimization algorithms to train
a transformer neural network, which accurately predicts the relationship
between problem parameters and the globally optimal solution for the powered
descent guidance problem. The solution is encoded as the set of tight
constraints corresponding to the constrained minimum-cost trajectory and the
optimal final time of landing. By leveraging the attention mechanism of
transformer neural networks, large sequences of time series data can be
accurately predicted when given only the spacecraft state and landing site
parameters. When applied to the real problem of Mars powered descent guidance,
T-PDG reduces the time for computing the 3 degree of freedom fuel-optimal
trajectory, when compared to lossless convexification, from an order of 1-8
seconds to less than 500 milliseconds. A safe and optimal solution is
guaranteed by including a feasibility check in T-PDG before returning the final
trajectory.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05610">Efficient Parallelization Layouts for Large-Scale Distributed Model Training. (arXiv:2311.05610v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hagemann_J/0/1/0/all/0/1">Johannes Hagemann</a>, <a href="http://arxiv.org/find/cs/1/au:+Weinbach_S/0/1/0/all/0/1">Samuel Weinbach</a>, <a href="http://arxiv.org/find/cs/1/au:+Dobler_K/0/1/0/all/0/1">Konstantin Dobler</a>, <a href="http://arxiv.org/find/cs/1/au:+Schall_M/0/1/0/all/0/1">Maximilian Schall</a>, <a href="http://arxiv.org/find/cs/1/au:+Melo_G/0/1/0/all/0/1">Gerard de Melo</a></p>
<p>Efficiently training large language models requires parallelizing across
hundreds of hardware accelerators and invoking various compute and memory
optimizations. When combined, many of these strategies have complex
interactions regarding the final training efficiency. Prior work tackling this
problem did not have access to the latest set of optimizations, such as
FlashAttention or sequence parallelism. In this work, we conduct a
comprehensive ablation study of possible training configurations for large
language models. We distill this large study into several key recommendations
for the most efficient training. For instance, we find that using a micro-batch
size of 1 usually enables the most efficient training layouts. Larger
micro-batch sizes necessitate activation checkpointing or higher degrees of
model parallelism and also lead to larger pipeline bubbles. Our most efficient
configurations enable us to achieve state-of-the-art training efficiency
results over a range of model sizes, most notably a Model FLOPs utilization of
70.5% when training a Llama 13B model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06396">A comprehensive analysis of concept drift locality in data streams. (arXiv:2311.06396v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aguiar_G/0/1/0/all/0/1">Gabriel J. Aguiar</a>, <a href="http://arxiv.org/find/cs/1/au:+Cano_A/0/1/0/all/0/1">Alberto Cano</a></p>
<p>Adapting to drifting data streams is a significant challenge in online
learning. Concept drift must be detected for effective model adaptation to
evolving data properties. Concept drift can impact the data distribution
entirely or partially, which makes it difficult for drift detectors to
accurately identify the concept drift. Despite the numerous concept drift
detectors in the literature, standardized procedures and benchmarks for
comprehensive evaluation considering the locality of the drift are lacking. We
present a novel categorization of concept drift based on its locality and
scale. A systematic approach leads to a set of 2,760 benchmark problems,
reflecting various difficulty levels following our proposed categorization. We
conduct a comparative assessment of 9 state-of-the-art drift detectors across
diverse difficulties, highlighting their strengths and weaknesses for future
research. We examine how drift locality influences the classifier performance
and propose strategies for different drift categories to minimize the recovery
time. Lastly, we provide lessons learned and recommendations for future concept
drift research. Our benchmark data streams and experiments are publicly
available at https://github.com/gabrieljaguiar/locality-concept-drift.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07461">On Self-Supervised Dynamic Incremental Regularised Adaptation. (arXiv:2311.07461v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ghobrial_A/0/1/0/all/0/1">Abanoub Ghobrial</a>, <a href="http://arxiv.org/find/cs/1/au:+Eder_K/0/1/0/all/0/1">Kerstin Eder</a></p>
<p>In this paper, we give an overview of a recently developed method for dynamic
domain adaptation, named DIRA, which relies on a few samples in addition to a
regularisation approach, named elastic weight consolidation, to achieve
state-of-the-art (SOTA) domain adaptation results. DIRA has been previously
shown to perform competitively with SOTA unsupervised adaption techniques.
However, a limitation of DIRA is that it relies on labels to be provided for
the few samples used in adaption. This makes it a supervised technique. In this
paper, we propose a modification to the DIRA method to make it self-supervised
i.e. remove the need for providing labels. Our proposed approach will be
evaluated experimentally in future work.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08811">Correlation-aware active learning for surgery video segmentation. (arXiv:2311.08811v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1">Fei Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Marquez_Neila_P/0/1/0/all/0/1">Pablo Marquez-Neila</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1">Mingyi Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Rafii_Tari_H/0/1/0/all/0/1">Hedyeh Rafii-Tari</a>, <a href="http://arxiv.org/find/cs/1/au:+Sznitman_R/0/1/0/all/0/1">Raphael Sznitman</a></p>
<p>Semantic segmentation is a complex task that relies heavily on large amounts
of annotated image data. However, annotating such data can be time-consuming
and resource-intensive, especially in the medical domain. Active Learning (AL)
is a popular approach that can help to reduce this burden by iteratively
selecting images for annotation to improve the model performance. In the case
of video data, it is important to consider the model uncertainty and the
temporal nature of the sequences when selecting images for annotation. This
work proposes a novel AL strategy for surgery video segmentation, COWAL,
COrrelation-aWare Active Learning. Our approach involves projecting images into
a latent space that has been fine-tuned using contrastive learning and then
selecting a fixed number of representative images from local clusters of video
frames. We demonstrate the effectiveness of this approach on two video datasets
of surgical instruments and three real-world video datasets. The datasets and
code will be made publicly available upon receiving necessary approvals.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10792">Attention Mechanism for Lithium-Ion Battery Lifespan Prediction: Temporal and Cyclic Attention. (arXiv:2311.10792v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jaewook Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Heo_S/0/1/0/all/0/1">Seongmin Heo</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jay H. Lee</a></p>
<p>Accurately predicting lithium-ion batteries (LIBs) lifespan is pivotal for
optimizing usage and preventing accidents. Previous approaches often relied on
inputs challenging to measure in real-time, and failed to capture intra- and
inter-cycle data patterns simultaneously. Our study employ attention mechanisms
(AM) to develop data-driven models predicting LIB lifespan using easily
measurable inputs. Developed model integrates recurrent neural network and
convolutional neural network, featuring two types of AMs: temporal attention
(TA) and cyclic attention (CA). TA identifies important time steps within each
cycle, CA strives to capture key features of inter-cycle correlations through
self-attention (SA). We apply the developed model to publicly available data
consisting of three batches of cycling modes. TA scores highlight the rest
phase as a key characteristic to distinguish different batches. By leveraging
CA scores, we decreased the input dimension from 100 cycles to 50 and 30 cycles
with single- and multi-head attention.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12424">Looped Transformers are Better at Learning Learning Algorithms. (arXiv:2311.12424v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Liu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kangwook Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Nowak_R/0/1/0/all/0/1">Robert Nowak</a>, <a href="http://arxiv.org/find/cs/1/au:+Papailiopoulos_D/0/1/0/all/0/1">Dimitris Papailiopoulos</a></p>
<p>Transformers have demonstrated effectiveness in in-context solving
data-fitting problems from various (latent) models, as reported by Garg et al.
However, the absence of an inherent iterative structure in the transformer
architecture presents a challenge in emulating the iterative algorithms, which
are commonly employed in traditional machine learning methods. To address this,
we propose the utilization of looped transformer architecture and its
associated training methodology, with the aim of incorporating iterative
characteristics into the transformer architectures. Experimental results
suggest that the looped transformer achieves performance comparable to the
standard transformer in solving various data-fitting problems, while utilizing
less than 10% of the parameter count.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14168">Fast Policy Learning for Linear Quadratic Control with Entropy Regularization. (arXiv:2311.14168v3 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Guo_X/0/1/0/all/0/1">Xin Guo</a>, <a href="http://arxiv.org/find/math/1/au:+Li_X/0/1/0/all/0/1">Xinyu Li</a>, <a href="http://arxiv.org/find/math/1/au:+Xu_R/0/1/0/all/0/1">Renyuan Xu</a></p>
<p>This paper proposes and analyzes two new policy learning methods: regularized
policy gradient (RPG) and iterative policy optimization (IPO), for a class of
discounted linear-quadratic control (LQC) problems over an infinite time
horizon with entropy regularization. Assuming access to the exact policy
evaluation, both proposed approaches are proven to converge linearly in finding
optimal policies of the regularized LQC. Moreover, the IPO method can achieve a
super-linear convergence rate once it enters a local region around the optimal
policy. Finally, when the optimal policy for an RL problem with a known
environment is appropriately transferred as the initial policy to an RL problem
with an unknown environment, the IPO method is shown to enable a super-linear
convergence rate if the two environments are sufficiently close. Performances
of these proposed algorithms are supported by numerical examples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16378">Bayesian Formulations for Graph Spectral Denoising. (arXiv:2311.16378v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Leone_S/0/1/0/all/0/1">Sam Leone</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xingzhi Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Perlmutter_M/0/1/0/all/0/1">Michael Perlmutter</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishnaswamy_S/0/1/0/all/0/1">Smita Krishnaswamy</a></p>
<p>Here we consider the problem of denoising features associated to complex
data, modeled as signals on a graph, via a smoothness prior. This is motivated
in part by settings such as single-cell RNA where the data is very
high-dimensional, but its structure can be captured via an affinity graph. This
allows us to utilize ideas from graph signal processing. In particular, we
present algorithms for the cases where the signal is perturbed by Gaussian
noise, dropout, and uniformly distributed noise. The signals are assumed to
follow a prior distribution defined in the frequency domain which favors
signals which are smooth across the edges of the graph. By pairing this prior
distribution with our three models of noise generation, we propose Maximum A
Posteriori (M.A.P.) estimates of the true signal in the presence of noisy data
and provide algorithms for computing the M.A.P. Finally, we demonstrate the
algorithms' ability to effectively restore signals from white noise on image
data and from severe dropout in single-cell RNA sequence data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17431">Grounding Foundation Models through Federated Transfer Learning: A General Framework. (arXiv:2311.17431v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1">Yan Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_T/0/1/0/all/0/1">Tao Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_H/0/1/0/all/0/1">Hanlin Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1">Lixin Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1">Qiang Yang</a></p>
<p>Foundation Models (FMs) such as GPT-4 encoded with vast knowledge and
powerful emergent abilities have achieved remarkable success in various natural
language processing and computer vision tasks. Grounding FMs by adapting them
to domain-specific tasks or augmenting them with domain-specific knowledge
enables us to exploit the full potential of FMs. However, grounding FMs faces
several challenges, stemming primarily from constrained computing resources,
data privacy, model heterogeneity, and model ownership. Federated Transfer
Learning (FTL), the combination of federated learning and transfer learning,
provides promising solutions to address these challenges. In recent years, the
need for grounding FMs leveraging FTL, coined FTL-FM, has arisen strongly in
both academia and industry. Motivated by the strong growth in FTL-FM research
and the potential impact of FTL-FM on industrial applications, we propose an
FTL-FM framework that formulates problems of grounding FMs in the federated
learning setting, construct a detailed taxonomy based on the FTL-FM framework
to categorize state-of-the-art FTL-FM works, and comprehensively overview
FTL-FM works based on the proposed taxonomy. We also establish correspondences
between FTL-FM and conventional phases of adapting FM so that FM practitioners
can align their research works with FTL-FM. In addition, we overview advanced
efficiency-improving and privacy-preserving techniques because efficiency and
privacy are critical concerns in FTL-FM. Last, we discuss opportunities and
future research directions of FTL-FM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18188">Leveraging cache to enable SLU on tiny devices. (arXiv:2311.18188v2 [eess.AS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Benazir_A/0/1/0/all/0/1">Afsara Benazir</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1">Zhiming Xu</a>, <a href="http://arxiv.org/find/eess/1/au:+Lin_F/0/1/0/all/0/1">Felix Xiaozhu Lin</a> (University of Virginia)</p>
<p>This paper addresses spoken language understanding (SLU) on
microcontroller-like embedded devices, integrating on-device execution with
cloud offloading in a novel fashion. We exploit temporal locality in a device's
speech inputs and accordingly reuse recent SLU inferences. Our idea is simple:
let the device match new inputs against cached results, and only offload
unmatched inputs to the cloud for full inference. Realization of this idea,
however, is non-trivial: the device needs to compare acoustic features in a
robust, low-cost way. To this end, we present XYZ, a speech cache for tiny
devices. It matches speech inputs at two levels of representations: first by
clustered sequences of raw sound units, then as sequences of phonemes. Working
in tandem, the two representations offer complementary cost/accuracy tradeoffs.
To further boost accuracy, our cache is learning: with the mismatched and then
offloaded inputs, it continuously finetunes the device's feature extractors
(with the assistance of the cloud). We implement XYZ on an off-the-shelf STM32
microcontroller. The resultant implementation has a small memory footprint of
2MB. Evaluated on challenging speech benchmarks, our system resolves 45%--90%
of inputs on device, reducing the average latency by up to 80% compared to
offloading to popular cloud speech services. Our benefit is pronounced even in
adversarial settings -- noisy environments, cold cache, or one device shared by
a number of users.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00516">Spatio-Temporal-Decoupled Masked Pre-training: Benchmarked on Traffic Forecasting. (arXiv:2312.00516v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1">Haotian Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1">Renhe Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1">Zheng Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1">Jinliang Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yuxin Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1">Xuan Song</a></p>
<p>Accurate forecasting of multivariate traffic flow time series remains
challenging due to substantial spatio-temporal heterogeneity and complex
long-range correlative patterns. To address this, we propose
Spatio-Temporal-Decoupled Masked Pre-training (STD-MAE), a novel framework that
employs masked autoencoders to learn and encode complex spatio-temporal
dependencies via pre-training. Specifically, we use two decoupled masked
autoencoders to reconstruct the traffic data along spatial and temporal axes
using a self-supervised pre-training approach. These mask reconstruction
mechanisms capture the long-range correlations in space and time separately.
The learned hidden representations are then used to augment the downstream
spatio-temporal traffic predictor. A series of quantitative and qualitative
evaluations on four widely-used traffic benchmarks (PEMS03, PEMS04, PEMS07, and
PEMS08) are conducted to verify the state-of-the-art performance, with STD-MAE
explicitly enhancing the downstream spatio-temporal models' ability to capture
long-range intricate spatial and temporal patterns. Codes are available at
https://github.com/Jimmy-7664/STD_MAE.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00655">Machine Learning for Health symposium 2023 -- Findings track. (arXiv:2312.00655v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hegselmann_S/0/1/0/all/0/1">Stefan Hegselmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Parziale_A/0/1/0/all/0/1">Antonio Parziale</a>, <a href="http://arxiv.org/find/cs/1/au:+Shanmugam_D/0/1/0/all/0/1">Divya Shanmugam</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1">Shengpu Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Asiedu_M/0/1/0/all/0/1">Mercy Nyamewaa Asiedu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1">Serina Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hartvigsen_T/0/1/0/all/0/1">Thomas Hartvigsen</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_H/0/1/0/all/0/1">Harvineet Singh</a></p>
<p>A collection of the accepted Findings papers that were presented at the 3rd
Machine Learning for Health symposium (ML4H 2023), which was held on December
10, 2023, in New Orleans, Louisiana, USA. ML4H 2023 invited high-quality
submissions on relevant problems in a variety of health-related disciplines
including healthcare, biomedicine, and public health. Two submission tracks
were offered: the archival Proceedings track, and the non-archival Findings
track. Proceedings were targeted at mature work with strong technical
sophistication and a high impact to health. The Findings track looked for new
ideas that could spark insightful discussion, serve as valuable resources for
the community, or could enable new collaborations. Submissions to the
Proceedings track, if not accepted, were automatically considered for the
Findings track. All the manuscripts submitted to ML4H Symposium underwent a
double-blind peer-review process.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01544">KEEC: Embed to Control on An Equivariant Geometry. (arXiv:2312.01544v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1">Xiaoyuan Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yiming Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1">Wei Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yukun Hu</a></p>
<p>This paper investigates how representation learning can enable optimal
control in unknown and complex dynamics, such as chaotic and non-linear
systems, without relying on prior domain knowledge of the dynamics. The core
idea is to establish an equivariant geometry that is diffeomorphic to the
manifold defined by a dynamical system and to perform optimal control within
this corresponding geometry, which is a non-trivial task. To address this
challenge, Koopman Embed to Equivariant Control (KEEC) is proposed for model
learning and control. Inspired by Lie theory, KEEC begins by learning a
non-linear dynamical system defined on a manifold and embedding trajectories
into a Lie group. Subsequently, KEEC formulates an equivariant value function
equation in reinforcement learning on the equivariant geometry, ensuring an
invariant effect as the value function on the original manifold. By deriving
analytical-form optimal actions on the equivariant value function, KEEC
theoretically achieves quadratic convergence for the optimal equivariant value
function by leveraging the differential information on the equivariant
geometry. The effectiveness of KEEC is demonstrated in challenging dynamical
systems, including chaotic ones like Lorenz-63. Notably, our results show that
isometric functions, which maintain the compactness and completeness of
geometry while preserving metric and differential information, consistently
outperform loss functions lacking these characteristics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01648">Characterizing Large Language Model Geometry Solves Toxicity Detection and Generation. (arXiv:2312.01648v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Balestriero_R/0/1/0/all/0/1">Randall Balestriero</a>, <a href="http://arxiv.org/find/cs/1/au:+Cosentino_R/0/1/0/all/0/1">Romain Cosentino</a>, <a href="http://arxiv.org/find/cs/1/au:+Shekkizhar_S/0/1/0/all/0/1">Sarath Shekkizhar</a></p>
<p>Large Language Models~(LLMs) drive current AI breakthroughs despite very
little being known about their internal representations, e.g., how to extract a
few informative features to solve various downstream tasks. To provide a
practical and principled answer, we propose to characterize LLMs from a
geometric perspective. We obtain in closed form (i) the intrinsic dimension in
which the Multi-Head Attention embeddings are constrained to exist and (ii) the
partition and per-region affine mappings of the per-layer feedforward networks.
Our results are informative, do not rely on approximations, and are actionable.
First, we show that, motivated by our geometric interpretation, we can bypass
Llama$2$'s RLHF by controlling its embedding's intrinsic dimension through
informed prompt manipulation. Second, we derive $7$ interpretable spline
features that can be extracted from any (pre-trained) LLM layer, providing a
rich abstract representation of their inputs. Those features alone ($224$ for
Mistral-7B/Llama$2$-7B and $560$ for Llama$2$-70B) are sufficient to help solve
toxicity detection, infer the domain of the prompt, and even tackle the Jigsaw
challenge, which aims at characterizing the type of toxicity of various
prompts. Our results demonstrate how, even in large-scale regimes, exact
theoretical results can answer practical questions in language models. Code:
\url{https://github.com/RandallBalestriero/SplineLLM}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01853">Robot Synesthesia: In-Hand Manipulation with Visuotactile Sensing. (arXiv:2312.01853v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1">Ying Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Che_H/0/1/0/all/0/1">Haichuan Che</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1">Yuzhe Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1">Binghao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1">Zhao-Heng Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kang-Won Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yi Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1">Soo-Chul Lim</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaolong Wang</a></p>
<p>Executing contact-rich manipulation tasks necessitates the fusion of tactile
and visual feedback. However, the distinct nature of these modalities poses
significant challenges. In this paper, we introduce a system that leverages
visual and tactile sensory inputs to enable dexterous in-hand manipulation.
Specifically, we propose Robot Synesthesia, a novel point cloud-based tactile
representation inspired by human tactile-visual synesthesia. This approach
allows for the simultaneous and seamless integration of both sensory inputs,
offering richer spatial information and facilitating better reasoning about
robot actions. The method, trained in a simulated environment and then deployed
to a real robot, is applicable to various in-hand object rotation tasks.
Comprehensive ablations are performed on how the integration of vision and
touch can improve reinforcement learning and Sim2Real performance. Our project
page is available at https://yingyuan0414.github.io/visuotactile/ .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01878">HGPROMPT: Bridging Homogeneous and Heterogeneous Graphs for Few-shot Prompt Learning. (arXiv:2312.01878v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1">Xingtong Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1">Yuan Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zemin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xinming Zhang</a></p>
<p>Graph neural networks (GNNs) and heterogeneous graph neural networks (HGNNs)
are prominent techniques for homogeneous and heterogeneous graph representation
learning, yet their performance in an end-to-end supervised framework greatly
depends on the availability of task-specific supervision. To reduce the
labeling cost, pre-training on self-supervised pretext tasks has become a
popular paradigm,but there is often a gap between the pre-trained model and
downstream tasks, stemming from the divergence in their objectives. To bridge
the gap, prompt learning has risen as a promising direction especially in
few-shot settings, without the need to fully fine-tune the pre-trained model.
While there has been some early exploration of prompt-based learning on graphs,
they primarily deal with homogeneous graphs, ignoring the heterogeneous graphs
that are prevalent in downstream applications. In this paper, we propose
HGPROMPT, a novel pre-training and prompting framework to unify not only
pre-training and downstream tasks but also homogeneous and heterogeneous graphs
via a dual-template design. Moreover, we propose dual-prompt in HGPROMPT to
assist a downstream task in locating the most relevant prior to bridge the gaps
caused by not only feature variations but also heterogeneity differences across
tasks. Finally, we thoroughly evaluate and analyze HGPROMPT through extensive
experiments on three public datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02021">VLTSeg: Simple Transfer of CLIP-Based Vision-Language Representations for Domain Generalized Semantic Segmentation. (arXiv:2312.02021v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hummer_C/0/1/0/all/0/1">Christoph H&#xfc;mmer</a>, <a href="http://arxiv.org/find/cs/1/au:+Schwonberg_M/0/1/0/all/0/1">Manuel Schwonberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1">Liangwei Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1">Hu Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1">Alois Knoll</a>, <a href="http://arxiv.org/find/cs/1/au:+Gottschalk_H/0/1/0/all/0/1">Hanno Gottschalk</a></p>
<p>Domain generalization (DG) remains a significant challenge for perception
based on deep neural networks (DNN), where domain shifts occur due to lighting,
weather, or geolocation changes. In this work, we propose VLTSeg to enhance
domain generalization in semantic segmentation, where the network is solely
trained on the source domain and evaluated on unseen target domains. Our method
leverages the inherent semantic robustness of vision-language models. First, by
substituting traditional vision-only backbones with pre-trained encoders from
CLIP and EVA-CLIP as transfer learning setting we find that in the field of DG,
vision-language pre-training significantly outperforms supervised and
self-supervised vision pre-training. We thus propose a new vision-language
approach for domain generalized segmentation, which improves the domain
generalization SOTA by 7.6% mIoU when training on the synthetic GTA5 dataset.
We further show the superior generalization capabilities of vision-language
segmentation models by reaching 76.48% mIoU on the popular Cityscapes-to-ACDC
benchmark, outperforming the previous SOTA approach by 6.9% mIoU on the test
set at the time of writing. Additionally, our approach shows strong in-domain
generalization capabilities indicated by 86.1% mIoU on the Cityscapes test set,
resulting in a shared first place with the previous SOTA on the current
leaderboard at the time of submission.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02396">Unsupervised Change Detection for Space Habitats Using 3D Point Clouds. (arXiv:2312.02396v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1">Jamie Santos</a>, <a href="http://arxiv.org/find/cs/1/au:+Dinkel_H/0/1/0/all/0/1">Holly Dinkel</a>, <a href="http://arxiv.org/find/cs/1/au:+Di_J/0/1/0/all/0/1">Julia Di</a>, <a href="http://arxiv.org/find/cs/1/au:+Borges_P/0/1/0/all/0/1">Paulo V.K. Borges</a>, <a href="http://arxiv.org/find/cs/1/au:+Moreira_M/0/1/0/all/0/1">Marina Moreira</a>, <a href="http://arxiv.org/find/cs/1/au:+Alexandrov_O/0/1/0/all/0/1">Oleg Alexandrov</a>, <a href="http://arxiv.org/find/cs/1/au:+Coltin_B/0/1/0/all/0/1">Brian Coltin</a>, <a href="http://arxiv.org/find/cs/1/au:+Smith_T/0/1/0/all/0/1">Trey Smith</a></p>
<p>This work presents an algorithm for scene change detection from point clouds
to enable autonomous robotic caretaking in future space habitats. Autonomous
robotic systems will help maintain future deep-space habitats, such as the
Gateway space station, which will be uncrewed for extended periods. Existing
scene analysis software used on the International Space Station (ISS) relies on
manually-labeled images for detecting changes. In contrast, the algorithm
presented in this work uses raw, unlabeled point clouds as inputs. The
algorithm first applies modified Expectation-Maximization Gaussian Mixture
Model (GMM) clustering to two input point clouds. It then performs change
detection by comparing the GMMs using the Earth Mover's Distance. The algorithm
is validated quantitatively and qualitatively using a test dataset collected by
an Astrobee robot in the NASA Ames Granite Lab comprising single frame depth
images taken directly by Astrobee and full-scene reconstructed maps built with
RGB-D and pose data from Astrobee. The runtimes of the approach are also
analyzed in depth. The source code is publicly released to promote further
development.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02406">Efficient Online Data Mixing For Language Model Pre-Training. (arXiv:2312.02406v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Albalak_A/0/1/0/all/0/1">Alon Albalak</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1">Liangming Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1">Colin Raffel</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">William Yang Wang</a></p>
<p>The data used to pretrain large language models has a decisive impact on a
model's downstream performance, which has led to a large body of work on data
selection methods that aim to automatically determine the most suitable data to
use for pretraining. Existing data selection methods suffer from slow and
computationally expensive processes, a problem amplified by the increasing size
of models and of pretraining datasets. Data mixing, on the other hand, reduces
the complexity of data selection by grouping data points together and
determining sampling probabilities across entire groups. However, data mixing
proportions are typically fixed before training and therefore cannot adapt to
changing training dynamics. To address these limitations, we develop an
efficient algorithm for Online Data Mixing (ODM) that combines elements from
both data selection and data mixing. Based on multi-armed bandit algorithms,
our online approach optimizes the data mixing proportions during training.
Remarkably, our method trains a model that reaches the final perplexity of the
next best method with 19\% fewer training iterations, and improves performance
on the 5-shot MMLU benchmark by 1.9% relative accuracy, while adding negligible
wall-clock time during pretraining.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02957">Classification for everyone : Building geography agnostic models for fairer recognition. (arXiv:2312.02957v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jindal_A/0/1/0/all/0/1">Akshat Jindal</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Shreya Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Gadgil_S/0/1/0/all/0/1">Soham Gadgil</a></p>
<p>In this paper, we analyze different methods to mitigate inherent geographical
biases present in state of the art image classification models. We first
quantitatively present this bias in two datasets - The Dollar Street Dataset
and ImageNet, using images with location information. We then present different
methods which can be employed to reduce this bias. Finally, we analyze the
effectiveness of the different techniques on making these models more robust to
geographical locations of the images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03196">Domain Invariant Representation Learning and Sleep Dynamics Modeling for Automatic Sleep Staging. (arXiv:2312.03196v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Seungyeon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1">Thai-Hoang Pham</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1">Zhao Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Ping Zhang</a></p>
<p>Sleep staging has become a critical task in diagnosing and treating sleep
disorders to prevent sleep related diseases. With growing large scale sleep
databases, significant progress has been made toward automatic sleep staging.
However, previous studies face critical problems in sleep studies; the
heterogeneity of subjects' physiological signals, the inability to extract
meaningful information from unlabeled data to improve predictive performances,
the difficulty in modeling correlations between sleep stages, and the lack of
an effective mechanism to quantify predictive uncertainty. In this study, we
propose a neural network based sleep staging model, DREAM, to learn domain
generalized representations from physiological signals and models sleep
dynamics. DREAM learns sleep related and subject invariant representations from
diverse subjects' sleep signals and models sleep dynamics by capturing
interactions between sequential signal segments and between sleep stages. We
conducted a comprehensive empirical study to demonstrate the superiority of
DREAM, including sleep stage prediction experiments, a case study, the usage of
unlabeled data, and uncertainty. Notably, the case study validates DREAM's
ability to learn generalized decision function for new subjects, especially in
case there are differences between testing and training subjects. Uncertainty
quantification shows that DREAM provides prediction uncertainty, making the
model reliable and helping sleep experts in real world applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03325">GCFA:Geodesic Curve Feature Augmentation in the Pre-Shape Space. (arXiv:2312.03325v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1">Yuexing Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_G/0/1/0/all/0/1">Guanxin Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bing Wang</a></p>
<p>Deep learning has yielded remarkable outcomes in various domains. However,
the challenge of requiring large-scale labeled samples still persists in deep
learning. Thus, data augmentation has been introduced as a critical strategy to
train deep learning models. However, data augmentation suffers from information
loss and poor performance in small sample environments. To overcome these
drawbacks, we propose a feature augmentation method based on shape space
theory, i.e., Geodesic curve feature augmentation, called GCFA in
brevity.First, we extract features from the image with the neural network
model. Then, the multiple image features are projected into a pre-shape space
as features. In the pre-shape space, a Geodesic curve is built to fit the
features. Finally, the many generated features on the Geodesic curve are used
to train the various machine learning models. The GCFA module can be seamlessly
integrated with most machine learning methods. And the proposed method is
simple, effective and insensitive for the small sample datasets.Several
examples demonstrate that the GCFA method can greatly improve the performance
of the data preprocessing model in a small sample environment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03511">Kandinsky 3.0 Technical Report. (arXiv:2312.03511v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Arkhipkin_V/0/1/0/all/0/1">Vladimir Arkhipkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Filatov_A/0/1/0/all/0/1">Andrei Filatov</a>, <a href="http://arxiv.org/find/cs/1/au:+Vasilev_V/0/1/0/all/0/1">Viacheslav Vasilev</a>, <a href="http://arxiv.org/find/cs/1/au:+Maltseva_A/0/1/0/all/0/1">Anastasia Maltseva</a>, <a href="http://arxiv.org/find/cs/1/au:+Azizov_S/0/1/0/all/0/1">Said Azizov</a>, <a href="http://arxiv.org/find/cs/1/au:+Pavlov_I/0/1/0/all/0/1">Igor Pavlov</a>, <a href="http://arxiv.org/find/cs/1/au:+Agafonova_J/0/1/0/all/0/1">Julia Agafonova</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuznetsov_A/0/1/0/all/0/1">Andrey Kuznetsov</a>, <a href="http://arxiv.org/find/cs/1/au:+Dimitrov_D/0/1/0/all/0/1">Denis Dimitrov</a></p>
<p>We present Kandinsky 3.0, a large-scale text-to-image generation model based
on latent diffusion, continuing the series of text-to-image Kandinsky models
and reflecting our progress to achieve higher quality and realism of image
generation. Compared to previous versions of Kandinsky 2.x, Kandinsky 3.0
leverages a two times larger U-Net backbone, a ten times larger text encoder
and removes diffusion mapping. We describe the architecture of the model, the
data collection procedure, the training technique, and the production system of
user interaction. We focus on the key components that, as we have identified as
a result of a large number of experiments, had the most significant impact on
improving the quality of our model compared to the others. By our side-by-side
comparisons, Kandinsky becomes better in text understanding and works better on
specific domains. Project page: https://ai-forever.github.io/Kandinsky-3
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03763">Gaussian3Diff: 3D Gaussian Diffusion for 3D Full Head Synthesis and Editing. (arXiv:2312.03763v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1">Yushi Lan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_F/0/1/0/all/0/1">Feitong Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_D/0/1/0/all/0/1">Di Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1">Qiangeng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Genova_K/0/1/0/all/0/1">Kyle Genova</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zeng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fanello_S/0/1/0/all/0/1">Sean Fanello</a>, <a href="http://arxiv.org/find/cs/1/au:+Pandey_R/0/1/0/all/0/1">Rohit Pandey</a>, <a href="http://arxiv.org/find/cs/1/au:+Funkhouser_T/0/1/0/all/0/1">Thomas Funkhouser</a>, <a href="http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1">Chen Change Loy</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yinda Zhang</a></p>
<p>We present a novel framework for generating photorealistic 3D human head and
subsequently manipulating and reposing them with remarkable flexibility. The
proposed approach leverages an implicit function representation of 3D human
heads, employing 3D Gaussians anchored on a parametric face model. To enhance
representational capabilities and encode spatial information, we embed a
lightweight tri-plane payload within each Gaussian rather than directly storing
color and opacity. Additionally, we parameterize the Gaussians in a 2D UV space
via a 3DMM, enabling effective utilization of the diffusion model for 3D head
avatar generation. Our method facilitates the creation of diverse and realistic
3D human heads with fine-grained editing over facial features and expressions.
Extensive experiments demonstrate the effectiveness of our method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03815">LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem. (arXiv:2312.03815v2 [cs.OS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1">Yingqiang Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1">Yujie Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1">Wenyue Hua</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Shuyuan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1">Juntao Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yongfeng Zhang</a></p>
<p>This paper envisions a revolutionary AIOS-Agent ecosystem, where Large
Language Model (LLM) serves as the (Artificial) Intelligent Operating System
(IOS, or AIOS)--an operating system "with soul". Upon this foundation, a
diverse range of LLM-based AI Agent Applications (Agents, or AAPs) are
developed, enriching the AIOS-Agent ecosystem and signaling a paradigm shift
from the traditional OS-APP ecosystem. We envision that LLM's impact will not
be limited to the AI application level, instead, it will in turn revolutionize
the design and implementation of computer system, architecture, software, and
programming language, featured by several main concepts: LLM as OS
(system-level), Agents as Applications (application-level), Natural Language as
Programming Interface (user-level), and Tools as Devices/Libraries
(hardware/middleware-level). We begin by introducing the architecture of
traditional OS. Then we formalize a conceptual framework for AIOS through "LLM
as OS (LLMOS)", drawing analogies between AIOS and traditional OS: LLM is
likened to OS kernel, context window to memory, external storage to file
system, hardware tools to peripheral devices, software tools to programming
libraries, and user prompts to user commands. Subsequently, we introduce the
new AIOS-Agent Ecosystem, where users can easily program Agent Applications
(AAPs) using natural language, democratizing the development of software, which
is different from the traditional OS-APP ecosystem. Following this, we explore
the diverse scope of Agent Applications. We delve into both single-agent and
multi-agent systems, as well as human-agent interaction. Lastly, drawing on the
insights from traditional OS-APP ecosystem, we propose a roadmap for the
evolution of the AIOS-Agent ecosystem. This roadmap is designed to guide the
future research and development, suggesting systematic progresses of AIOS and
its Agent applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04021">A Study on the Calibration of In-context Learning. (arXiv:2312.04021v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hanlin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yi-Fan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yaodong Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Madeka_D/0/1/0/all/0/1">Dhruv Madeka</a>, <a href="http://arxiv.org/find/cs/1/au:+Foster_D/0/1/0/all/0/1">Dean Foster</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1">Eric Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1">Hima Lakkaraju</a>, <a href="http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1">Sham Kakade</a></p>
<p>Modern auto-regressive language models are trained to minimize log loss on
broad data by predicting the next token so they are expected to get calibrated
answers in next-token prediction tasks. We study this for in-context learning
(ICL), a widely used way to adapt frozen large language models (LLMs) via
crafting prompts, and investigate the trade-offs between performance and
calibration on a wide range of natural language understanding and reasoning
tasks. We conduct extensive experiments to show that such trade-offs may get
worse as we increase model size, incorporate more ICL examples, and fine-tune
models using instruction, dialog, or reinforcement learning from human feedback
(RLHF) on carefully curated datasets. Furthermore, we find that common
recalibration techniques that are widely effective such as temperature scaling
provide limited gains in calibration errors, suggesting that new methods may be
required for settings where models are expected to be reliable.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04055">Jointly spatial-temporal representation learning for individual trajectories. (arXiv:2312.04055v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1">Fei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1">Jianrong Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1">Yang Yue</a></p>
<p>Individual trajectories, rich in human-environment interaction information
across space and time, serve as vital inputs for geospatial foundation models
(GeoFMs). However, existing attempts at learning trajectory representations
have overlooked the implicit spatial-temporal dependency within trajectories,
failing to encode such dependency in a deep learning-friendly format. That
poses a challenge in obtaining general-purpose trajectory representations.
Therefore, this paper proposes a spatial-temporal joint representation learning
method (ST-GraphRL) to formalize learnable spatial-temporal dependencies into
trajectory representations. The proposed ST-GraphRL consists of three
compositions: (i) a weighted directed spatial-temporal graph to explicitly
construct mobility interactions in both space and time dimensions; (ii) a
two-stage jointly encoder (i.e., decoupling and fusion), to learn entangled
spatial-temporal dependencies by independently decomposing and jointly
aggregating space and time information; (iii) a decoder guides ST-GraphRL to
learn explicit mobility regularities by simulating the spatial-temporal
distributions of trajectories. Tested on three real-world human mobility
datasets, the proposed ST-GraphRL outperformed all the baseline models in
predicting movement spatial-temporal distributions and preserving trajectory
similarity with high spatial-temporal correlations. Analyzing spatial-temporal
features presented in latent space validates that ST-GraphRL understands
spatial-temporal patterns. This study may also benefit representation learnings
of other geospatial data to achieve general-purpose data representations and
advance GeoFMs development.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04584">Towards Sample-specific Backdoor Attack with Clean Labels via Attribute Trigger. (arXiv:2312.04584v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yiming Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1">Mingyan Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Junfeng Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1">Tao Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1">Shu-Tao Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1">Zhan Qin</a></p>
<p>Currently, sample-specific backdoor attacks (SSBAs) are the most advanced and
malicious methods since they can easily circumvent most of the current backdoor
defenses. In this paper, we reveal that SSBAs are not sufficiently stealthy due
to their poisoned-label nature, where users can discover anomalies if they
check the image-label relationship. In particular, we demonstrate that it is
ineffective to directly generalize existing SSBAs to their clean-label variants
by poisoning samples solely from the target class. We reveal that it is
primarily due to two reasons, including \textbf{(1)} the `antagonistic effects'
of ground-truth features and \textbf{(2)} the learning difficulty of
sample-specific features. Accordingly, trigger-related features of existing
SSBAs cannot be effectively learned under the clean-label setting due to their
mild trigger intensity required for ensuring stealthiness. We argue that the
intensity constraint of existing SSBAs is mostly because their trigger patterns
are `content-irrelevant' and therefore act as `noises' for both humans and
DNNs. Motivated by this understanding, we propose to exploit content-relevant
features, $a.k.a.$ (human-relied) attributes, as the trigger patterns to design
clean-label SSBAs. This new attack paradigm is dubbed backdoor attack with
attribute trigger (BAAT). Extensive experiments are conducted on benchmark
datasets, which verify the effectiveness of our BAAT and its resistance to
existing defenses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2111.04613">Variational Automatic Curriculum Learning for Sparse-Reward Cooperative Multi-Agent Problems. (arXiv:2111.04613v2 [cs.LG] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiayu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuanxin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yuanfan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1">Huimin Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Huazhong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1">Jiaming Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yi Wu</a></p>
<p>We introduce a curriculum learning algorithm, Variational Automatic
Curriculum Learning (VACL), for solving challenging goal-conditioned
cooperative multi-agent reinforcement learning problems. We motivate our
paradigm through a variational perspective, where the learning objective can be
decomposed into two terms: task learning on the current task distribution, and
curriculum update to a new task distribution. Local optimization over the
second term suggests that the curriculum should gradually expand the training
tasks from easy to hard. Our VACL algorithm implements this variational
paradigm with two practical components, task expansion and entity progression,
which produces training curricula over both the task configurations as well as
the number of entities in the task. Experiment results show that VACL solves a
collection of sparse-reward problems with a large number of agents.
Particularly, using a single desktop machine, VACL achieves 98% coverage rate
with 100 agents in the simple-spread benchmark and reproduces the ramp-use
behavior originally shown in OpenAI's hide-and-seek project. Our project
website is at https://sites.google.com/view/vacl-neurips-2021.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.04688">BAFFLE: Hiding Backdoors in Offline Reinforcement Learning Datasets. (arXiv:2210.04688v4 [cs.LG] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1">Chen Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhou Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1">Yunpeng Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Junda He</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1">Jieke Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Kecen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1">Arunesh Sinha</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1">Bowen Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1">Xinwen Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Lo_D/0/1/0/all/0/1">David Lo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tianhao Wang</a></p>
<p>Reinforcement learning (RL) makes an agent learn from trial-and-error
experiences gathered during the interaction with the environment. Recently,
offline RL has become a popular RL paradigm because it saves the interactions
with environments. In offline RL, data providers share large pre-collected
datasets, and others can train high-quality agents without interacting with the
environments. This paradigm has demonstrated effectiveness in critical tasks
like robot control, autonomous driving, etc. However, less attention is paid to
investigating the security threats to the offline RL system. This paper focuses
on backdoor attacks, where some perturbations are added to the data
(observations) such that given normal observations, the agent takes
high-rewards actions, and low-reward actions on observations injected with
triggers. In this paper, we propose Baffle (Backdoor Attack for Offline
Reinforcement Learning), an approach that automatically implants backdoors to
RL agents by poisoning the offline RL dataset, and evaluate how different
offline RL algorithms react to this attack. Our experiments conducted on four
tasks and four offline RL algorithms expose a disquieting fact: none of the
existing offline RL algorithms is immune to such a backdoor attack. More
specifically, Baffle modifies 10\% of the datasets for four tasks (3 robotic
controls and 1 autonomous driving). Agents trained on the poisoned datasets
perform well in normal settings. However, when triggers are presented, the
agents' performance decreases drastically by 63.2\%, 53.9\%, 64.7\%, and 47.4\%
in the four tasks on average. The backdoor still persists after fine-tuning
poisoned agents on clean datasets. We further show that the inserted backdoor
is also hard to be detected by a popular defensive method. This paper calls
attention to developing more effective protection for the open-source offline
RL dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.04967">UQ for Credit Risk Management: A deep evidence regression approach. (arXiv:2305.04967v2 [q-fin.RM] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+Dhiman_A/0/1/0/all/0/1">Ashish Dhiman</a></p>
<p>Machine Learning has invariantly found its way into various Credit Risk
applications. Due to the intrinsic nature of Credit Risk, quantifying the
uncertainty of the predicted risk metrics is essential, and applying
uncertainty-aware deep learning models to credit risk settings can be very
helpful. In this work, we have explored the application of a scalable UQ-aware
deep learning technique, Deep Evidence Regression and applied it to predicting
Loss Given Default. We contribute to the literature by extending the Deep
Evidence Regression methodology to learning target variables generated by a
Weibull process and provide the relevant learning framework. We demonstrate the
application of our approach to both simulated and real-world data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.09936">BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions. (arXiv:2308.09936v2 [cs.CV] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1">Wenbo Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yifan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Weiyue Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zeyuan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1">Zhuowen Tu</a></p>
<p>Vision Language Models (VLMs), which extend Large Language Models (LLM) by
incorporating visual understanding capability, have demonstrated significant
advancements in addressing open-ended visual question-answering (VQA) tasks.
However, these models cannot accurately interpret images infused with text, a
common occurrence in real-world scenarios. Standard procedures for extracting
information from images often involve learning a fixed set of query embeddings.
These embeddings are designed to encapsulate image contexts and are later used
as soft prompt inputs in LLMs. Yet, this process is limited to the token count,
potentially curtailing the recognition of scenes with text-rich context. To
improve upon them, the present study introduces BLIVA: an augmented version of
InstructBLIP with Visual Assistant. BLIVA incorporates the query embeddings
from InstructBLIP and also directly projects encoded patch embeddings into the
LLM, a technique inspired by LLaVA. This approach assists the model to capture
intricate details potentially missed during the query decoding process.
Empirical evidence demonstrates that our model, BLIVA, significantly enhances
performance in processing text-rich VQA benchmarks (up to 17.76% in OCR-VQA
benchmark) and in undertaking general (not particularly text-rich) VQA
benchmarks (up to 7.9% in Visual Spatial Reasoning benchmark), comparing to our
baseline InstructBLIP. BLIVA demonstrates significant capability in decoding
real-world images, irrespective of text presence. To demonstrate the broad
industry applications enabled by BLIVA, we evaluate the model using a new
dataset comprising YouTube thumbnails paired with question-answer sets across
11 diverse categories. For researchers interested in further exploration, our
code and models are freely accessible at https://github.com/mlpc-ucsd/BLIVA.
</p>
</p>
</div>

    </div>
    </body>
    