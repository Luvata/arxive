<!DOCTYPE html>
<html>
<head>
<title>2024-01-19-cs-cl</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2401.08660">Gemini Pro Defeated by GPT-4V: Evidence from Education. (arXiv:2401.08660v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1">Gyeong-Geon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Latif_E/0/1/0/all/0/1">Ehsan Latif</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1">Lehong Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1">Xiaoming Zhai</a></p>
<p>This study compared the classification performance of Gemini Pro and GPT-4V
in educational settings. Employing visual question answering (VQA) techniques,
the study examined both models' abilities to read text-based rubrics and then
automatically score student-drawn models in science education. We employed both
quantitative and qualitative analyses using a dataset derived from
student-drawn scientific models and employing NERIF (Notation-Enhanced Rubrics
for Image Feedback) prompting methods. The findings reveal that GPT-4V
significantly outperforms Gemini Pro in terms of scoring accuracy and Quadratic
Weighted Kappa. The qualitative analysis reveals that the differences may be
due to the models' ability to process fine-grained texts in images and overall
image classification performance. Even adapting the NERIF approach by further
de-sizing the input images, Gemini Pro seems not able to perform as well as
GPT-4V. The findings suggest GPT-4V's superior capability in handling complex
multimodal educational tasks. The study concludes that while both models
represent advancements in AI, GPT-4V's higher performance makes it a more
suitable tool for educational applications involving multimodal data
interpretation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.08664">Adapting Large Language Models for Education: Foundational Capabilities, Potentials, and Challenges. (arXiv:2401.08664v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qingyao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_L/0/1/0/all/0/1">Lingyue Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Weiming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xianyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Jingwei Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1">Wei Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Weinan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1">Ruiming Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yong Yu</a></p>
<p>Online education platforms, leveraging the internet to distribute education
resources, seek to provide convenient education but often fall short in
real-time communication with students. They often struggle to offer
personalized education resources due to the challenge of addressing the diverse
obstacles students encounter throughout their learning journey. Recently, the
emergence of large language models (LLMs), such as ChatGPT, offers the
possibility for resolving this issue by comprehending individual requests.
Although LLMs have been successful in various fields, creating an LLM-based
education system is still challenging for the wide range of educational skills
required. This paper reviews the recently emerged LLM researches related to
educational capabilities, including mathematics, writing, programming,
reasoning, and knowledge-based question answering, with the aim to explore
their potential in constructing the next-generation intelligent education
system. Based on the current development status, we further outline two
approaches for an LLM-based education system: a unified approach and a
mixture-of-expert (MoE) approach. Finally, we explore the challenges and future
directions, providing new research opportunities and perspectives on adapting
LLMs for education.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.08688">Automated Answer Validation using Text Similarity. (arXiv:2401.08688v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ganesan_B/0/1/0/all/0/1">Balaji Ganesan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ravikumar_A/0/1/0/all/0/1">Arjun Ravikumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Piplani_L/0/1/0/all/0/1">Lakshay Piplani</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhaumik_R/0/1/0/all/0/1">Rini Bhaumik</a>, <a href="http://arxiv.org/find/cs/1/au:+Padmanaban_D/0/1/0/all/0/1">Dhivya Padmanaban</a>, <a href="http://arxiv.org/find/cs/1/au:+Narasimhamurthy_S/0/1/0/all/0/1">Shwetha Narasimhamurthy</a>, <a href="http://arxiv.org/find/cs/1/au:+Adhikary_C/0/1/0/all/0/1">Chetan Adhikary</a>, <a href="http://arxiv.org/find/cs/1/au:+Deshapogu_S/0/1/0/all/0/1">Subhash Deshapogu</a></p>
<p>Automated answer validation can help improve learning outcomes by providing
appropriate feedback to learners, and by making question answering systems and
online learning solutions more widely available. There have been some works in
science question answering which show that information retrieval methods
outperform neural methods, especially in the multiple choice version of this
problem. We implement Siamese neural network models and produce a generalised
solution to this problem. We compare our supervised model with other text
similarity based solutions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.08694">Combining Confidence Elicitation and Sample-based Methods for Uncertainty Quantification in Misinformation Mitigation. (arXiv:2401.08694v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rivera_M/0/1/0/all/0/1">Mauricio Rivera</a>, <a href="http://arxiv.org/find/cs/1/au:+Godbout_J/0/1/0/all/0/1">Jean-Fran&#xe7;ois Godbout</a>, <a href="http://arxiv.org/find/cs/1/au:+Rabbany_R/0/1/0/all/0/1">Reihaneh Rabbany</a>, <a href="http://arxiv.org/find/cs/1/au:+Pelrine_K/0/1/0/all/0/1">Kellin Pelrine</a></p>
<p>Large Language Models have emerged as prime candidates to tackle
misinformation mitigation. However, existing approaches struggle with
hallucinations and overconfident predictions. We propose an uncertainty
quantification framework that leverages both direct confidence elicitation and
sampled-based consistency methods to provide better calibration for NLP
misinformation mitigation solutions. We first investigate the calibration of
sample-based consistency methods that exploit distinct features of consistency
across sample sizes and stochastic levels. Next, we evaluate the performance
and distributional shift of a robust numeric verbalization prompt across single
vs. two-step confidence elicitation procedure. We also compare the performance
of the same prompt with different versions of GPT and different numerical
scales. Finally, we combine the sample-based consistency and verbalized methods
to propose a hybrid framework that yields a better uncertainty estimation for
GPT models. Overall, our work proposes novel uncertainty quantification methods
that will improve the reliability of Large Language Models in misinformation
mitigation applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.08743">MMToM-QA: Multimodal Theory of Mind Question Answering. (arXiv:2401.08743v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1">Chuanyang Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yutong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1">Jing Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_J/0/1/0/all/0/1">Jiannan Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuo_Y/0/1/0/all/0/1">Yen-Ling Kuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1">Zhiting Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ullman_T/0/1/0/all/0/1">Tomer Ullman</a>, <a href="http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1">Antonio Torralba</a>, <a href="http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1">Joshua B. Tenenbaum</a>, <a href="http://arxiv.org/find/cs/1/au:+Shu_T/0/1/0/all/0/1">Tianmin Shu</a></p>
<p>Theory of Mind (ToM), the ability to understand people's minds, is an
essential ingredient for developing machines with human-level social
intelligence. Recent machine learning models, particularly large language
models, seem to show some aspects of ToM understanding. However, existing ToM
benchmarks use unimodal datasets - either video or text. Human ToM, on the
other hand, is more than video or text understanding. People can flexibly
reason about another person's mind based on conceptual representations (e.g.,
goals, beliefs, plans) extracted from any available data, which can include
visual cues, linguistic narratives, or both. To address this, we introduce a
multimodal Theory of Mind question answering (MMToM-QA) benchmark. MMToM-QA
comprehensively evaluates machine ToM both on multimodal data and on different
kinds of unimodal data about a person's activity in a household environment. To
engineer multimodal ToM capacity, we propose a novel method, BIP-ALM (Bayesian
Inverse Planning Accelerated by Language Models). BIP-ALM extracts unified
representations from multimodal data and utilizes language models for scalable
Bayesian inverse planning. We conducted a systematic comparison of human
performance, BIP-ALM, and state-of-the-art models, including GPT-4. The
experiments demonstrate that large language models and large multimodal models
still lack robust ToM capacity. BIP-ALM, on the other hand, shows promising
results, by leveraging the power of both model-based mental inference and
language models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.08772">HuixiangDou: Overcoming Group Chat Scenarios with LLM-based Technical Assistance. (arXiv:2401.08772v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kong_H/0/1/0/all/0/1">Huanjun Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Songyang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Kai Chen</a></p>
<p>In this work, we present HuixiangDou, a technical assistant powered by Large
Language Models (LLM). This system is designed to assist algorithm developers
by providing insightful responses to questions related to open-source algorithm
projects, such as computer vision and deep learning projects from OpenMMLab. We
further explore the integration of this assistant into the group chats of
instant messaging (IM) tools such as WeChat and Lark. Through several iterative
improvements and trials, we have developed a sophisticated technical chat
assistant capable of effectively answering users' technical questions without
causing message flooding. This paper's contributions include: 1) Designing an
algorithm pipeline specifically for group chat scenarios; 2) Verifying the
reliable performance of text2vec in task rejection; 3) Identifying three
critical requirements for LLMs in technical-assistant-like products, namely
scoring ability, In-Context Learning (ICL), and Long Context. We have made the
software and source code available at https://github.com/internlm/huixiangdou
to aid in future research and application. HuixiangDou is applicable to any
group chat within IM tools.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.08825">AiGen-FoodReview: A Multimodal Dataset of Machine-Generated Restaurant Reviews and Images on Social Media. (arXiv:2401.08825v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gambetti_A/0/1/0/all/0/1">Alessandro Gambetti</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1">Qiwei Han</a></p>
<p>Online reviews in the form of user-generated content (UGC) significantly
impact consumer decision-making. However, the pervasive issue of not only human
fake content but also machine-generated content challenges UGC's reliability.
Recent advances in Large Language Models (LLMs) may pave the way to fabricate
indistinguishable fake generated content at a much lower cost. Leveraging
OpenAI's GPT-4-Turbo and DALL-E-2 models, we craft AiGen-FoodReview, a
multi-modal dataset of 20,144 restaurant review-image pairs divided into
authentic and machine-generated. We explore unimodal and multimodal detection
models, achieving 99.80% multimodal accuracy with FLAVA. We use attributes from
readability and photographic theories to score reviews and images,
respectively, demonstrating their utility as hand-crafted features in scalable
and interpretable detection models, with comparable performance. The paper
contributes by open-sourcing the dataset and releasing fake review detectors,
recommending its use in unimodal and multimodal fake review detection tasks,
and evaluating linguistic and visual features in synthetic versus authentic
data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.08833">Revisiting Self-supervised Learning of Speech Representation from a Mutual Information Perspective. (arXiv:2401.08833v1 [eess.AS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Liu_A/0/1/0/all/0/1">Alexander H. Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Yeh_S/0/1/0/all/0/1">Sung-Lin Yeh</a>, <a href="http://arxiv.org/find/eess/1/au:+Glass_J/0/1/0/all/0/1">James Glass</a></p>
<p>Existing studies on self-supervised speech representation learning have
focused on developing new training methods and applying pre-trained models for
different applications. However, the quality of these models is often measured
by the performance of different downstream tasks. How well the representations
access the information of interest is less studied. In this work, we take a
closer look into existing self-supervised methods of speech from an
information-theoretic perspective. We aim to develop metrics using mutual
information to help practical problems such as model design and selection. We
use linear probes to estimate the mutual information between the target
information and learned representations, showing another insight into the
accessibility to the target information from speech representations. Further,
we explore the potential of evaluating representations in a self-supervised
fashion, where we estimate the mutual information between different parts of
the data without using any labels. Finally, we show that both supervised and
unsupervised measures echo the performance of the models on layer-wise linear
probing and speech recognition.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.08835">Improving ASR Contextual Biasing with Guided Attention. (arXiv:2401.08835v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jiyang Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1">Kwangyoun Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Shon_S/0/1/0/all/0/1">Suwon Shon</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1">Felix Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sridhar_P/0/1/0/all/0/1">Prashant Sridhar</a>, <a href="http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1">Shinji Watanabe</a></p>
<p>In this paper, we propose a Guided Attention (GA) auxiliary training loss,
which improves the effectiveness and robustness of automatic speech recognition
(ASR) contextual biasing without introducing additional parameters. A common
challenge in previous literature is that the word error rate (WER) reduction
brought by contextual biasing diminishes as the number of bias phrases
increases. To address this challenge, we employ a GA loss as an additional
training objective besides the Transducer loss. The proposed GA loss aims to
teach the cross attention how to align bias phrases with text tokens or audio
frames. Compared to studies with similar motivations, the proposed loss
operates directly on the cross attention weights and is easier to implement.
Through extensive experiments based on Conformer Transducer with Contextual
Adapter, we demonstrate that the proposed method not only leads to a lower WER
but also retains its effectiveness as the number of bias phrases increases.
Specifically, the GA loss decreases the WER of rare vocabularies by up to 19.2%
on LibriSpeech compared to the contextual biasing baseline, and up to 49.3%
compared to a vanilla Transducer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.08851">Using i-vectors for subject-independent cross-session EEG transfer learning. (arXiv:2401.08851v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lasko_J/0/1/0/all/0/1">Jonathan Lasko</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1">Jeff Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Nicoletti_M/0/1/0/all/0/1">Mike Nicoletti</a>, <a href="http://arxiv.org/find/cs/1/au:+Sussman_Fort_J/0/1/0/all/0/1">Jonathan Sussman-Fort</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeong_S/0/1/0/all/0/1">Sooyoung Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Hartmann_W/0/1/0/all/0/1">William Hartmann</a></p>
<p>Cognitive load classification is the task of automatically determining an
individual's utilization of working memory resources during performance of a
task based on physiologic measures such as electroencephalography (EEG). In
this paper, we follow a cross-disciplinary approach, where tools and
methodologies from speech processing are used to tackle this problem. The
corpus we use was released publicly in 2021 as part of the first passive
brain-computer interface competition on cross-session workload estimation. We
present our approach which used i-vector-based neural network classifiers to
accomplish inter-subject cross-session EEG transfer learning, achieving 18%
relative improvement over equivalent subject-dependent models. We also report
experiments showing how our subject-independent models perform competitively on
held-out subjects and improve with additional subject data, suggesting that
subject-dependent training is not required for effective cognitive load
determination.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.08887">NOTSOFAR-1 Challenge: New Datasets, Baseline, and Tasks for Distant Meeting Transcription. (arXiv:2401.08887v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vinnikov_A/0/1/0/all/0/1">Alon Vinnikov</a>, <a href="http://arxiv.org/find/cs/1/au:+Ivry_A/0/1/0/all/0/1">Amir Ivry</a>, <a href="http://arxiv.org/find/cs/1/au:+Hurvitz_A/0/1/0/all/0/1">Aviv Hurvitz</a>, <a href="http://arxiv.org/find/cs/1/au:+Abramovski_I/0/1/0/all/0/1">Igor Abramovski</a>, <a href="http://arxiv.org/find/cs/1/au:+Koubi_S/0/1/0/all/0/1">Sharon Koubi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gurvich_I/0/1/0/all/0/1">Ilya Gurvich</a>, <a href="http://arxiv.org/find/cs/1/au:+Pe%60er_S/0/1/0/all/0/1">Shai Pe`er</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1">Xiong Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Elizalde_B/0/1/0/all/0/1">Benjamin Martinez Elizalde</a>, <a href="http://arxiv.org/find/cs/1/au:+Kanda_N/0/1/0/all/0/1">Naoyuki Kanda</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaofei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shaer_S/0/1/0/all/0/1">Shalev Shaer</a>, <a href="http://arxiv.org/find/cs/1/au:+Yagev_S/0/1/0/all/0/1">Stav Yagev</a>, <a href="http://arxiv.org/find/cs/1/au:+Asher_Y/0/1/0/all/0/1">Yossi Asher</a>, <a href="http://arxiv.org/find/cs/1/au:+Sivasankaran_S/0/1/0/all/0/1">Sunit Sivasankaran</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1">Yifan Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1">Min Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Huaming Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Krupka_E/0/1/0/all/0/1">Eyal Krupka</a></p>
<p>We introduce the first Natural Office Talkers in Settings of Far-field Audio
Recordings (``NOTSOFAR-1'') Challenge alongside datasets and baseline system.
The challenge focuses on distant speaker diarization and automatic speech
recognition (DASR) in far-field meeting scenarios, with single-channel and
known-geometry multi-channel tracks, and serves as a launch platform for two
new datasets: First, a benchmarking dataset of 315 meetings, averaging 6
minutes each, capturing a broad spectrum of real-world acoustic conditions and
conversational dynamics. It is recorded across 30 conference rooms, featuring
4-8 attendees and a total of 35 unique speakers. Second, a 1000-hour simulated
training dataset, synthesized with enhanced authenticity for real-world
generalization, incorporating 15,000 real acoustic transfer functions. The
tasks focus on single-device DASR, where multi-channel devices always share the
same known geometry. This is aligned with common setups in actual conference
rooms, and avoids technical complexities associated with multi-device tasks. It
also allows for the development of geometry-specific solutions. The NOTSOFAR-1
Challenge aims to advance research in the field of distant conversational
speech recognition, providing key resources to unlock the potential of
data-driven methods, which we believe are currently constrained by the absence
of comprehensive high-quality training and benchmarking datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.08919">Partial Diacritization: A Context-Contrastive Inference Approach. (arXiv:2401.08919v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+ElNokrashy_M/0/1/0/all/0/1">Muhammad ElNokrashy</a>, <a href="http://arxiv.org/find/cs/1/au:+AlKhamissi_B/0/1/0/all/0/1">Badr AlKhamissi</a></p>
<p>Diacritization plays a pivotal role in improving readability and
disambiguating the meaning of Arabic texts. Efforts have so far focused on
marking every eligible character (Full Diacritization). Comparatively
overlooked, Partial Diacritzation (PD) is the selection of a subset of
characters to be marked to aid comprehension where needed. Research has
indicated that excessive diacritic marks can hinder skilled readers--reducing
reading speed and accuracy. We conduct a behavioral experiment and show that
partially marked text is often easier to read than fully marked text, and
sometimes easier than plain text. In this light, we introduce
Context-Contrastive Partial Diacritization (CCPD)--a novel approach to PD which
integrates seamlessly with existing Arabic diacritization systems. CCPD
processes each word twice, once with context and once without, and diacritizes
only the characters with disparities between the two inferences. Further, we
introduce novel indicators for measuring partial diacritization quality (SR,
PDER, HDER, ERE), essential for establishing this as a machine learning task.
Lastly, we introduce TD2, a Transformer-variant of an established model which
offers a markedly different per formance profile on our proposed indicators
compared to all other known systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.08967">ReFT: Reasoning with Reinforced Fine-Tuning. (arXiv:2401.08967v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luong_T/0/1/0/all/0/1">Trung Quoc Luong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xinbo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jie_Z/0/1/0/all/0/1">Zhanming Jie</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1">Peng Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1">Xiaoran Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hang Li</a></p>
<p>One way to enhance the reasoning capability of Large Language Models (LLMs)
is to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT)
annotations. This approach does not show sufficiently strong generalization
ability, however, because the training only relies on the given CoT data. In
math problem-solving, for example, there is usually only one annotated
reasoning path for each question in the training data. Intuitively, it would be
better for the algorithm to learn from multiple annotated reasoning paths given
a question. To address this issue, we propose a simple yet effective approach
called Reinforced Fine-Tuning (ReFT) to enhance the generalizability of
learning LLMs for reasoning, with math problem-solving as an example. ReFT
first warmups the model with SFT, and then employs on-line reinforcement
learning, specifically the PPO algorithm in this paper, to further fine-tune
the model, where an abundance of reasoning paths are automatically sampled
given the question and the rewards are naturally derived from the ground-truth
answers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that
ReFT significantly outperforms SFT, and the performance can be potentially
further boosted by combining inference-time strategies such as majority voting
and re-ranking. Note that ReFT obtains the improvement by learning from the
same training questions as SFT, without relying on extra or augmented training
questions. This indicates a superior generalization ability for ReFT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.08973">OCTO+: A Suite for Automatic Open-Vocabulary Object Placement in Mixed Reality. (arXiv:2401.08973v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1">Aditya Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoffe_L/0/1/0/all/0/1">Luke Yoffe</a>, <a href="http://arxiv.org/find/cs/1/au:+Hollerer_T/0/1/0/all/0/1">Tobias H&#xf6;llerer</a></p>
<p>One key challenge in Augmented Reality is the placement of virtual content in
natural locations. Most existing automated techniques can only work with a
closed-vocabulary, fixed set of objects. In this paper, we introduce and
evaluate several methods for automatic object placement using recent advances
in open-vocabulary vision-language models. Through a multifaceted evaluation,
we identify a new state-of-the-art method, OCTO+. We also introduce a benchmark
for automatically evaluating the placement of virtual objects in augmented
reality, alleviating the need for costly user studies. Through this, in
addition to human evaluations, we find that OCTO+ places objects in a valid
region over 70% of the time, outperforming other methods on a range of metrics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.08992">Efficient Adapter Finetuning for Tail Languages in Streaming Multilingual ASR. (arXiv:2401.08992v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1">Junwen Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qiujia Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1">Tara N. Sainath</a>, <a href="http://arxiv.org/find/cs/1/au:+Strohman_T/0/1/0/all/0/1">Trevor Strohman</a></p>
<p>The end-to-end ASR model is often desired in the streaming multilingual
scenario since it is easier to deploy and can benefit from pre-trained speech
models such as powerful foundation models. Meanwhile, the heterogeneous nature
and imbalanced data abundance of different languages may cause performance
degradation, leading to asynchronous peak performance for different languages
during training, especially on tail ones. Sometimes even the data itself may
become unavailable as a result of the enhanced privacy protection. Existing
work tend to significantly increase the model size or learn language-specific
decoders to accommodate each language separately. In this study, we explore
simple yet effective Language-Dependent Adapter (LDA) finetuning under a
cascaded Conformer transducer framework enhanced by teacher pseudo-labeling for
tail languages in the streaming multilingual ASR. The adapter only accounts for
0.4% of the full model per language. It is plugged into the frozen foundation
model and is the only trainable module during the finetuning process with noisy
student training. The final model merges the adapter parameters from different
checkpoints for different languages. The model performance is validated on a
challenging multilingual dictation dataset, which includes 39 tail languages
across Latin, Greek, Arabic, etc. Our proposed method brings 12.2% word error
rate reduction on average and up to 37.5% on a single locale. Furthermore, we
show that our parameter-efficient LDA can match the quality of the full model
finetuning, thus greatly alleviating the asynchronous peak performance issue.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.09002">AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on Large Language Models. (arXiv:2401.09002v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+shu_D/0/1/0/all/0/1">Dong shu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1">Mingyu Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1">Suiyuan Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Beichen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zihao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yongfeng Zhang</a></p>
<p>In our research, we pioneer a novel approach to evaluate the effectiveness of
jailbreak attacks on Large Language Models (LLMs), such as GPT-4 and LLaMa2,
diverging from traditional robustness-focused binary evaluations. Our study
introduces two distinct evaluation frameworks: a coarse-grained evaluation and
a fine-grained evaluation. Each framework, using a scoring range from 0 to 1,
offers a unique perspective, enabling a more comprehensive and nuanced
evaluation of attack effectiveness and empowering attackers to refine their
attack prompts with greater understanding. Furthermore, we have developed a
comprehensive ground truth dataset specifically tailored for jailbreak tasks.
This dataset not only serves as a crucial benchmark for our current study but
also establishes a foundational resource for future research, enabling
consistent and comparative analyses in this evolving field. Upon meticulous
comparison with traditional evaluation methods, we discovered that our
evaluation aligns with the baseline's trend while offering a more profound and
detailed assessment. We believe that by accurately evaluating the effectiveness
of attack prompts in the Jailbreak task, our work lays a solid foundation for
assessing a wider array of similar or even more complex tasks in the realm of
prompt injection, potentially revolutionizing this field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.09003">Augmenting Math Word Problems via Iterative Question Composing. (arXiv:2401.09003v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Haoxiong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1">Andrew Chi-Chih Yao</a></p>
<p>Despite recent progress in improving the mathematical reasoning ability of
large language models(LLMs), solving competition-level math problems without
the use of external tools remains challenging for open-source LLMs. In this
work, we introduce the MMIQC dataset, a mixture of processed web data and
synthetic question-response pairs, to equip base models with better
mathematical reasoning skills. Mistral-7B-MMIQC, the model obtained by
fine-tuning Mistral-7B(<a href="/abs/2310.06825">arXiv:2310.06825</a>) on MMIQC, achieves 36.0\% accuracy on
MATH(<a href="/abs/2103.03874">arXiv:2103.03874</a>), 5.8\% higher than the previous (model size $\sim$7B)
SOTA. Our experiments also show that a large part of the improvement attributes
to our novel augmentation method IQC(Iterative Question Composing), where we
iteratively ask an LLM to compose new questions from the given seed problems
and do rejection sampling from another LLM. MMIQC has now been released on
https://huggingface.co/datasets/Vivacem/MMIQC.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.09023">Explain Thyself Bully: Sentiment Aided Cyberbullying Detection with Explanation. (arXiv:2401.09023v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maity_K/0/1/0/all/0/1">Krishanu Maity</a>, <a href="http://arxiv.org/find/cs/1/au:+Jha_P/0/1/0/all/0/1">Prince Jha</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1">Raghav Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1">Sriparna Saha</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1">Pushpak Bhattacharyya</a></p>
<p>Cyberbullying has become a big issue with the popularity of different social
media networks and online communication apps. While plenty of research is going
on to develop better models for cyberbullying detection in monolingual
language, there is very little research on the code-mixed languages and
explainability aspect of cyberbullying. Recent laws like "right to
explanations" of General Data Protection Regulation, have spurred research in
developing interpretable models rather than focusing on performance. Motivated
by this we develop the first interpretable multi-task model called {\em mExCB}
for automatic cyberbullying detection from code-mixed languages which can
simultaneously solve several tasks, cyberbullying detection,
explanation/rationale identification, target group detection and sentiment
analysis. We have introduced {\em BullyExplain}, the first benchmark dataset
for explainable cyberbullying detection in code-mixed language. Each post in
{\em BullyExplain} dataset is annotated with four labels, i.e., {\em bully
label, sentiment label, target and rationales (explainability)}, i.e., which
phrases are being responsible for annotating the post as a bully. The proposed
multitask framework (mExCB) based on CNN and GRU with word and sub-sentence
(SS) level attention is able to outperform several baselines and state of the
art models when applied on {\em BullyExplain} dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.09041">Textual Summarisation of Large Sets: Towards a General Approach. (arXiv:2401.09041v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kuptavanich_K/0/1/0/all/0/1">Kittipitch Kuptavanich</a>, <a href="http://arxiv.org/find/cs/1/au:+Reiter_E/0/1/0/all/0/1">Ehud Reiter</a>, <a href="http://arxiv.org/find/cs/1/au:+Deemter_K/0/1/0/all/0/1">Kees Van Deemter</a>, <a href="http://arxiv.org/find/cs/1/au:+Siddharthan_A/0/1/0/all/0/1">Advaith Siddharthan</a></p>
<p>We are developing techniques to generate summary descriptions of sets of
objects. In this paper, we present and evaluate a rule-based NLG technique for
summarising sets of bibliographical references in academic papers. This extends
our previous work on summarising sets of consumer products and shows how our
model generalises across these two very different domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.09042">LLMs for Relational Reasoning: How Far are We?. (arXiv:2401.09042v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhiming Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yushi Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiufeng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1">Junzhe Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Teo_Y/0/1/0/all/0/1">Yon Shin Teo</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1">Shang-wei Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a></p>
<p>Large language models (LLMs) have revolutionized many areas (e.g. natural
language processing, software engineering, etc.) by achieving state-of-the-art
performance on extensive downstream tasks. Aiming to achieve robust and general
artificial intelligence, there has been a surge of interest in investigating
the reasoning ability of the LLMs. Whereas the textual and numerical reasoning
benchmarks adopted by previous works are rather shallow and simple, it is hard
to conclude that the LLMs possess strong reasoning ability by merely achieving
positive results on these benchmarks. Recent efforts have demonstrated that the
LLMs are poor at solving sequential decision-making problems that require
common-sense planning by evaluating their performance on the reinforcement
learning benchmarks. In this work, we conduct an in-depth assessment of several
state-of-the-art LLMs' reasoning ability based on the inductive logic
programming (ILP) benchmark, which is broadly recognized as a representative
and challenging measurement for evaluating logic program induction/synthesis
systems as it requires inducing strict cause-effect logic to achieve robust
deduction on independent and identically distributed (IID) and
out-of-distribution (OOD) test samples. Our evaluations illustrate that
compared with the neural program induction systems which are much smaller in
model size, the state-of-the-art LLMs are much poorer in terms of reasoning
ability by achieving much lower performance and generalization using either
natural language prompting or truth-value matrix prompting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.09074">Code Simulation Challenges for Large Language Models. (arXiv:2401.09074v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Malfa_E/0/1/0/all/0/1">Emanuele La Malfa</a>, <a href="http://arxiv.org/find/cs/1/au:+Weinhuber_C/0/1/0/all/0/1">Christoph Weinhuber</a>, <a href="http://arxiv.org/find/cs/1/au:+Torre_O/0/1/0/all/0/1">Orazio Torre</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1">Fangru Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohn_A/0/1/0/all/0/1">Anthony Cohn</a>, <a href="http://arxiv.org/find/cs/1/au:+Shadbolt_N/0/1/0/all/0/1">Nigel Shadbolt</a>, <a href="http://arxiv.org/find/cs/1/au:+Wooldridge_M/0/1/0/all/0/1">Michael Wooldridge</a></p>
<p>We investigate the extent to which Large Language Models (LLMs) can simulate
the execution of computer code and algorithms. We begin by looking straight
line programs, and show that current LLMs demonstrate poor performance even
with such simple programs -- performance rapidly degrades with the length of
code. We then investigate the ability of LLMs to simulate programs that contain
critical paths and redundant instructions. We also go beyond straight line
program simulation with sorting algorithms and nested loops, and we show the
computational complexity of a routine directly affects the ability of an LLM to
simulate its execution. We observe that LLMs execute instructions sequentially
and with a low error margin only for short programs or standard procedures.
LLMs' code simulation is in tension with their pattern recognition and
memorisation capabilities: on tasks where memorisation is detrimental, we
propose a novel prompting method to simulate code execution line by line.
Empirically, our new Chain of Simulation (CoSm) method improves on the standard
Chain of Thought prompting approach by avoiding the pitfalls of memorisation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.09082">What makes for a &#x27;good&#x27; social actor? Using respect as a lens to evaluate interactions with language agents. (arXiv:2401.09082v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alberts_L/0/1/0/all/0/1">Lize Alberts</a>, <a href="http://arxiv.org/find/cs/1/au:+Keeling_G/0/1/0/all/0/1">Geoff Keeling</a>, <a href="http://arxiv.org/find/cs/1/au:+McCroskery_A/0/1/0/all/0/1">Amanda McCroskery</a></p>
<p>With the growing popularity of dialogue agents based on large language models
(LLMs), urgent attention has been drawn to finding ways to ensure their
behaviour is ethical and appropriate. These are largely interpreted in terms of
the 'HHH' criteria: making outputs more helpful and honest, and avoiding
harmful (biased, toxic, or inaccurate) statements. Whilst this semantic focus
is useful from the perspective of viewing LLM agents as mere mediums for
information, it fails to account for pragmatic factors that can make the same
utterance seem more or less offensive or tactless in different social
situations. We propose an approach to ethics that is more centred on relational
and situational factors, exploring what it means for a system, as a social
actor, to treat an individual respectfully in a (series of) interaction(s). Our
work anticipates a set of largely unexplored risks at the level of situated
interaction, and offers practical suggestions to help LLM technologies behave
as 'good' social actors and treat people respectfully.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.09135">Asynchronous Local-SGD Training for Language Modeling. (arXiv:2401.09135v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chhaparia_R/0/1/0/all/0/1">Rachita Chhaparia</a>, <a href="http://arxiv.org/find/cs/1/au:+Douillard_A/0/1/0/all/0/1">Arthur Douillard</a>, <a href="http://arxiv.org/find/cs/1/au:+Kale_S/0/1/0/all/0/1">Satyen Kale</a>, <a href="http://arxiv.org/find/cs/1/au:+Rusu_A/0/1/0/all/0/1">Andrei A. Rusu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1">Jiajun Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1">Arthur Szlam</a>, <a href="http://arxiv.org/find/cs/1/au:+Ranzato_M/0/1/0/all/0/1">Marc&#x27;Aurelio Ranzato</a></p>
<p>Local stochastic gradient descent (Local-SGD), also referred to as federated
averaging, is an approach to distributed optimization where each device
performs more than one SGD update per communication. This work presents an
empirical study of {\it asynchronous} Local-SGD for training language models;
that is, each worker updates the global parameters as soon as it has finished
its SGD steps. We conduct a comprehensive investigation by examining how worker
hardware heterogeneity, model size, number of workers, and optimizer could
impact the learning performance. We find that with naive implementations,
asynchronous Local-SGD takes more iterations to converge than its synchronous
counterpart despite updating the (global) model parameters more frequently. We
identify momentum acceleration on the global parameters when worker gradients
are stale as a key challenge. We propose a novel method that utilizes a delayed
Nesterov momentum update and adjusts the workers' local training steps based on
their computation speed. This approach, evaluated with models up to 150M
parameters on the C4 dataset, matches the performance of synchronous Local-SGD
in terms of perplexity per update step, and significantly surpasses it in terms
of wall clock time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.09150">Bridging Research and Readers: A Multi-Modal Automated Academic Papers Interpretation System. (arXiv:2401.09150v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_F/0/1/0/all/0/1">Feng Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kuang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Haizhou Li</a></p>
<p>In the contemporary information era, significantly accelerated by the advent
of Large-scale Language Models, the proliferation of scientific literature is
reaching unprecedented levels. Researchers urgently require efficient tools for
reading and summarizing academic papers, uncovering significant scientific
literature, and employing diverse interpretative methodologies. To address this
burgeoning demand, the role of automated scientific literature interpretation
systems has become paramount. However, prevailing models, both commercial and
open-source, confront notable challenges: they often overlook multimodal data,
grapple with summarizing over-length texts, and lack diverse user interfaces.
In response, we introduce an open-source multi-modal automated academic paper
interpretation system (MMAPIS) with three-step process stages, incorporating
LLMs to augment its functionality. Our system first employs the hybrid modality
preprocessing and alignment module to extract plain text, and tables or figures
from documents separately. It then aligns this information based on the section
names they belong to, ensuring that data with identical section names are
categorized under the same section. Following this, we introduce a hierarchical
discourse-aware summarization method. It utilizes the extracted section names
to divide the article into shorter text segments, facilitating specific
summarizations both within and between sections via LLMs with specific prompts.
Finally, we have designed four types of diversified user interfaces, including
paper recommendation, multimodal Q\&amp;A, audio broadcasting, and interpretation
blog, which can be widely applied across various scenarios. Our qualitative and
quantitative evaluations underscore the system's superiority, especially in
scientific summarization, where it outperforms solutions relying solely on
GPT-4.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.09168">Fine-tuning Strategies for Domain Specific Question Answering under Low Annotation Budget Constraints. (arXiv:2401.09168v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1">Kunpeng Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Diefenbach_D/0/1/0/all/0/1">Dennis Diefenbach</a>, <a href="http://arxiv.org/find/cs/1/au:+Gourru_A/0/1/0/all/0/1">Antoine Gourru</a>, <a href="http://arxiv.org/find/cs/1/au:+Gravier_C/0/1/0/all/0/1">Christophe Gravier</a></p>
<p>The progress introduced by pre-trained language models and their fine-tuning
has resulted in significant improvements in most downstream NLP tasks. The
unsupervised training of a language model combined with further target task
fine-tuning has become the standard QA fine-tuning procedure. In this work, we
demonstrate that this strategy is sub-optimal for fine-tuning QA models,
especially under a low QA annotation budget, which is a usual setting in
practice due to the extractive QA labeling cost. We draw our conclusions by
conducting an exhaustive analysis of the performance of the alternatives of the
sequential fine-tuning strategy on different QA datasets. Based on the
experiments performed, we observed that the best strategy to fine-tune the QA
model in low-budget settings is taking a pre-trained language model (PLM) and
then fine-tuning PLM with a dataset composed of the target dataset and SQuAD
dataset. With zero extra annotation effort, the best strategy outperforms the
standard strategy by 2.28% to 6.48%. Our experiments provide one of the first
investigations on how to best fine-tune a QA system under a low budget and are
therefore of the utmost practical interest to the QA practitioners.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.09175">QAnswer: Towards Question Answering Search over Websites. (arXiv:2401.09175v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1">Kunpeng Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Defretiere_C/0/1/0/all/0/1">Clement Defretiere</a>, <a href="http://arxiv.org/find/cs/1/au:+Diefenbach_D/0/1/0/all/0/1">Dennis Diefenbach</a>, <a href="http://arxiv.org/find/cs/1/au:+Gravier_C/0/1/0/all/0/1">Christophe Gravier</a>, <a href="http://arxiv.org/find/cs/1/au:+Gourru_A/0/1/0/all/0/1">Antoine Gourru</a></p>
<p>Question Answering (QA) is increasingly used by search engines to provide
results to their end-users, yet very few websites currently use QA technologies
for their search functionality. To illustrate the potential of QA technologies
for the website search practitioner, we demonstrate web searches that combine
QA over knowledge graphs and QA over free text -- each being usually tackled
separately. We also discuss the different benefits and drawbacks of both
approaches for web site searches. We use the case studies made of websites
hosted by the Wikimedia Foundation (namely Wikipedia and Wikidata). Differently
from a search engine (e.g. Google, Bing, etc), the data are indexed integrally,
i.e. we do not index only a subset, and they are indexed exclusively, i.e. we
index only data available on the corresponding website.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.09220">UniVIE: A Unified Label Space Approach to Visual Information Extraction from Form-like Documents. (arXiv:2401.09220v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1">Kai Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiawei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1">Weihong Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1">Zhuoyao Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Lei Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Huo_Q/0/1/0/all/0/1">Qiang Huo</a></p>
<p>Existing methods for Visual Information Extraction (VIE) from form-like
documents typically fragment the process into separate subtasks, such as key
information extraction, key-value pair extraction, and choice group extraction.
However, these approaches often overlook the hierarchical structure of form
documents, including hierarchical key-value pairs and hierarchical choice
groups. To address these limitations, we present a new perspective, reframing
VIE as a relation prediction problem and unifying labels of different tasks
into a single label space. This unified approach allows for the definition of
various relation types and effectively tackles hierarchical relationships in
form-like documents. In line with this perspective, we present UniVIE, a
unified model that addresses the VIE problem comprehensively. UniVIE functions
using a coarse-to-fine strategy. It initially generates tree proposals through
a tree proposal network, which are subsequently refined into hierarchical trees
by a relation decoder module. To enhance the relation prediction capabilities
of UniVIE, we incorporate two novel tree constraints into the relation decoder:
a tree attention mask and a tree level embedding. Extensive experimental
evaluations on both our in-house dataset HierForms and a publicly available
dataset SIBR, substantiate that our method achieves state-of-the-art results,
underscoring the effectiveness and potential of our unified approach in
advancing the field of VIE.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.09244">Cross-lingual Offensive Language Detection: A Systematic Review of Datasets, Transfer Approaches and Challenges. (arXiv:2401.09244v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_A/0/1/0/all/0/1">Aiqi Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zubiaga_A/0/1/0/all/0/1">Arkaitz Zubiaga</a></p>
<p>The growing prevalence and rapid evolution of offensive language in social
media amplify the complexities of detection, particularly highlighting the
challenges in identifying such content across diverse languages. This survey
presents a systematic and comprehensive exploration of Cross-Lingual Transfer
Learning (CLTL) techniques in offensive language detection in social media. Our
study stands as the first holistic overview to focus exclusively on the
cross-lingual scenario in this domain. We analyse 67 relevant papers and
categorise these studies across various dimensions, including the
characteristics of multilingual datasets used, the cross-lingual resources
employed, and the specific CLTL strategies implemented. According to "what to
transfer", we also summarise three main CLTL transfer approaches: instance,
feature, and parameter transfer. Additionally, we shed light on the current
challenges and future research opportunities in this field. Furthermore, we
have made our survey resources available online, including two comprehensive
tables that provide accessible references to the multilingual datasets and CLTL
methods used in the reviewed literature.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.09248">Learning from Emotions, Demographic Information and Implicit User Feedback in Task-Oriented Document-Grounded Dialogues. (arXiv:2401.09248v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Petrak_D/0/1/0/all/0/1">Dominic Petrak</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1">Thy Thy Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1">Iryna Gurevych</a></p>
<p>The success of task-oriented and document-grounded dialogue systems depends
on users accepting and enjoying using them. To achieve this, recently published
work in the field of Human-Computer Interaction suggests that the combination
of considering demographic information, user emotions and learning from the
implicit feedback in their utterances, is particularly important. However,
these findings have not yet been transferred to the field of Natural Language
Processing, where these data are primarily studied separately. Accordingly, no
sufficiently annotated dataset is available. To address this gap, we introduce
FEDI, the first English dialogue dataset for task-oriented document-grounded
dialogues annotated with demographic information, user emotions and implicit
feedback. Our experiments with FLAN-T5, GPT-2 and LLaMA-2 show that these data
have the potential to improve task completion and the factual consistency of
the generated responses and user acceptance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.00915">BiomedCLIP: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs. (arXiv:2303.00915v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Sheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yanbo Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1">Naoto Usuyama</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hanwen Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bagga_J/0/1/0/all/0/1">Jaspreet Bagga</a>, <a href="http://arxiv.org/find/cs/1/au:+Tinn_R/0/1/0/all/0/1">Robert Tinn</a>, <a href="http://arxiv.org/find/cs/1/au:+Preston_S/0/1/0/all/0/1">Sam Preston</a>, <a href="http://arxiv.org/find/cs/1/au:+Rao_R/0/1/0/all/0/1">Rajesh Rao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1">Mu Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Valluri_N/0/1/0/all/0/1">Naveen Valluri</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1">Cliff Wong</a>, <a href="http://arxiv.org/find/cs/1/au:+Tupini_A/0/1/0/all/0/1">Andrea Tupini</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mazzola_M/0/1/0/all/0/1">Matt Mazzola</a>, <a href="http://arxiv.org/find/cs/1/au:+Shukla_S/0/1/0/all/0/1">Swadheen Shukla</a>, <a href="http://arxiv.org/find/cs/1/au:+Liden_L/0/1/0/all/0/1">Lars Liden</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jianfeng Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1">Matthew P. Lungren</a>, <a href="http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1">Tristan Naumann</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Sheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1">Hoifung Poon</a></p>
<p>Biomedical data is inherently multimodal, comprising physical measurements
and natural language narratives. A generalist biomedical AI model needs to
simultaneously process different modalities of data, including text and images.
Therefore, training an effective generalist biomedical model requires
high-quality multimodal data, such as parallel image-text pairs. Here, we
present PMC-15M, a novel dataset that is two orders of magnitude larger than
existing biomedical multimodal datasets such as MIMIC-CXR, and spans a diverse
range of biomedical image types. PMC-15M contains 15 million biomedical
image-text pairs collected from 4.4 million scientific articles. Based on
PMC-15M, we have pretrained BiomedCLIP, a multimodal foundation model, with
domain-specific adaptations tailored to biomedical vision-language processing.
We conducted extensive experiments and ablation studies on standard biomedical
imaging tasks from retrieval to classification to visual question-answering
(VQA). BiomedCLIP achieved new state-of-the-art results in a wide range of
standard datasets, substantially outperforming prior approaches. Intriguingly,
by large-scale pretraining on diverse biomedical image types, BiomedCLIP even
outperforms state-of-the-art radiology-specific models such as BioViL in
radiology-specific tasks such as RSNA pneumonia detection. In summary,
BiomedCLIP is a fully open-access foundation model that achieves
state-of-the-art performance on various biomedical tasks, paving the way for
transformative multimodal biomedical discovery and applications. We release our
models at https://aka.ms/biomedclip to facilitate future research in multimodal
biomedical AI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.00969">CryCeleb: A Speaker Verification Dataset Based on Infant Cry Sounds. (arXiv:2305.00969v6 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Budaghyan_D/0/1/0/all/0/1">David Budaghyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Onu_C/0/1/0/all/0/1">Charles C. Onu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gorin_A/0/1/0/all/0/1">Arsenii Gorin</a>, <a href="http://arxiv.org/find/cs/1/au:+Subakan_C/0/1/0/all/0/1">Cem Subakan</a>, <a href="http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1">Doina Precup</a></p>
<p>This paper describes the Ubenwa CryCeleb dataset - a labeled collection of
infant cries - and the accompanying CryCeleb 2023 task, which is a public
speaker verification challenge based on cry sounds. We released more than 6
hours of manually segmented cry sounds from 786 newborns for academic use,
aiming to encourage research in infant cry analysis. The inaugural public
competition attracted 59 participants, 11 of whom improved the baseline
performance. The top-performing system achieved a significant improvement
scoring 25.8% equal error rate, which is still far from the performance of
state-of-the-art adult speaker verification systems. Therefore, we believe
there is room for further research on this dataset, potentially extending
beyond the verification task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.07895">On the Hidden Mystery of OCR in Large Multimodal Models. (arXiv:2305.07895v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuliang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1">Biao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chunyuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1">Xucheng Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Cheng-lin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1">Lianwen Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1">Xiang Bai</a></p>
<p>Large models have recently played a dominant role in natural language
processing and multimodal vision-language learning. However, their
effectiveness in text-related visual tasks remains relatively unexplored. In
this paper, we conducted a comprehensive evaluation of Large Multimodal Models,
such as GPT4V and Gemini, in various text-related visual tasks including Text
Recognition, Scene Text-Centric Visual Question Answering (VQA),
Document-Oriented VQA, Key Information Extraction (KIE), and Handwritten
Mathematical Expression Recognition (HMER). To facilitate the assessment of
Optical Character Recognition (OCR) capabilities in Large Multimodal Models, we
propose OCRBench, a comprehensive evaluation benchmark.Our study encompasses 29
datasets, making it the most comprehensive OCR evaluation benchmark available.
Furthermore, our study reveals both the strengths and weaknesses of these
models, particularly in handling multilingual text, handwritten text,
non-semantic text, and mathematical expression recognition. Most importantly,
the baseline results showcased in this study could provide a foundational
framework for the conception and assessment of innovative strategies targeted
at enhancing zero-shot multimodal techniques. The evaluation pipeline and
benchmark are available at https://github.com/Yuliang-Liu/MultimodalOCR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.13649">On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes. (arXiv:2306.13649v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Agarwal_R/0/1/0/all/0/1">Rishabh Agarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Vieillard_N/0/1/0/all/0/1">Nino Vieillard</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yongchao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Stanczyk_P/0/1/0/all/0/1">Piotr Stanczyk</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramos_S/0/1/0/all/0/1">Sabela Ramos</a>, <a href="http://arxiv.org/find/cs/1/au:+Geist_M/0/1/0/all/0/1">Matthieu Geist</a>, <a href="http://arxiv.org/find/cs/1/au:+Bachem_O/0/1/0/all/0/1">Olivier Bachem</a></p>
<p>Knowledge distillation (KD) is widely used for compressing a teacher model to
reduce its inference cost and memory footprint, by training a smaller student
model. However, current KD methods for auto-regressive sequence models suffer
from distribution mismatch between output sequences seen during training and
those generated by the student during inference. To address this issue, we
introduce Generalized Knowledge Distillation (GKD). Instead of solely relying
on a fixed set of output sequences, GKD trains the student on its
self-generated output sequences by leveraging feedback from the teacher on such
sequences. Unlike supervised KD approaches, GKD also offers the flexibility to
employ alternative loss functions between the student and teacher, which can be
useful when the student lacks the expressivity to mimic the teacher's
distribution. Furthermore, GKD facilitates the seamless integration of
distillation with RL fine-tuning (RLHF). We demonstrate the efficacy of GKD for
distilling auto-regressive language models on summarization, translation, and
arithmetic reasoning tasks, and task-agnostic distillation for
instruction-tuning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07421">SummaryMixing: A Linear-Complexity Alternative to Self-Attention for Speech Recognition and Understanding. (arXiv:2307.07421v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Parcollet_T/0/1/0/all/0/1">Titouan Parcollet</a>, <a href="http://arxiv.org/find/cs/1/au:+Dalen_R/0/1/0/all/0/1">Rogier van Dalen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shucong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1">Sourav Bhattacharya</a></p>
<p>Modern speech processing systems rely on self-attention. Unfortunately, token
mixing with self-attention takes quadratic time in the length of the speech
utterance, slowing down inference as well as training and increasing memory
consumption. Cheaper alternatives to self-attention for ASR have been
developed, but they fail to consistently reach the same level of accuracy. This
paper, therefore, proposes a novel linear-time alternative to self-attention.
It summarises an utterance with the mean over vectors for all time steps. This
single summary is then combined with time-specific information. We call this
method "SummaryMixing". Introducing SummaryMixing in state-of-the-art ASR
models makes it feasible to preserve or exceed previous speech recognition
performance while lowering the training and inference times by up to 28$\%$ and
reducing the memory budget by a factor of two. The benefits of SummaryMixing
can also be generalized to other speech-processing tasks, such as speech
understanding.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01497">Large Language Model Displays Emergent Ability to Interpret Novel Literary Metaphors. (arXiv:2308.01497v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ichien_N/0/1/0/all/0/1">Nicholas Ichien</a>, <a href="http://arxiv.org/find/cs/1/au:+Stamenkovic_D/0/1/0/all/0/1">Du&#x161;an Stamenkovi&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Holyoak_K/0/1/0/all/0/1">Keith J. Holyoak</a></p>
<p>Recent advances in the performance of large language models (LLMs) have
sparked debate over whether, given sufficient training, high-level human
abilities emerge in such generic forms of artificial intelligence (AI). Despite
the exceptional performance of LLMs on a wide range of tasks involving natural
language processing and reasoning, there has been sharp disagreement as to
whether their abilities extend to more creative human abilities. A core example
is the ability to interpret novel metaphors. Given the enormous and non curated
text corpora used to train LLMs, a serious obstacle to designing tests is the
requirement of finding novel yet high quality metaphors that are unlikely to
have been included in the training data. Here we assessed the ability of GPT4,
a state of the art large language model, to provide natural-language
interpretations of novel literary metaphors drawn from Serbian poetry and
translated into English. Despite exhibiting no signs of having been exposed to
these metaphors previously, the AI system consistently produced detailed and
incisive interpretations. Human judges, blind to the fact that an AI model was
involved, rated metaphor interpretations generated by GPT4 as superior to those
provided by a group of college students. In interpreting reversed metaphors,
GPT4, as well as humans, exhibited signs of sensitivity to the Gricean
cooperative principle. In addition, for several novel English poems GPT4
produced interpretations that were rated as excellent or good by a human
literary critic. These results indicate that LLMs such as GPT4 have acquired an
emergent ability to interpret complex metaphors, including those embedded in
novel poems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12519">Rational Decision-Making Agent with Internalized Utility Judgment. (arXiv:2308.12519v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1">Yining Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Cong_X/0/1/0/all/0/1">Xin Cong</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1">Shizuo Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1">Yujia Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yankai Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhiyuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1">Maosong Sun</a></p>
<p>Large language models (LLMs) have demonstrated remarkable advancements and
have attracted significant efforts to develop LLMs into agents capable of
executing intricate multi-step decision-making tasks beyond traditional NLP
applications. Existing approaches to LLM-based decision-making predominantly
build upon the manually-designed external performance metrics to guide the
decision-making process. However, reliance on the external performance metrics
as prior is problematic in real-world scenarios, where such prior may be
unavailable, flawed, or even erroneous. For genuine autonomous decision making,
it is imperative for the agent to develop its rationality from its posterior
experiences to judge decisions independently. Central to the development of
rationality is the construction of an internalized utility judgment, capable of
assigning numerical utilities to each decision. This paper proposes RadAgent
(Rational Decision-Making Agent), which fosters the development of its
rationality through an iterative framework involving Experience Exploration and
Utility Learning. Within this framework, Elo-based Utility Construction is
devised to assign Elo scores to individual decision steps to judge their
utilities via pairwise comparisons. Consequently, these Elo scores guide the
decision-making process to derive optimal outcomes. Experimental results on the
ToolBench dataset demonstrate RadAgent's superiority over baselines, achieving
over 10% improvement in Pass Rate on diverse tasks. It offers higher-quality
solutions and reduces costs (ChatGPT API calls), highlighting its effectiveness
and efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12697">Semantic similarity prediction is better than other semantic similarity measures. (arXiv:2309.12697v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Herbold_S/0/1/0/all/0/1">Steffen Herbold</a></p>
<p>Semantic similarity between natural language texts is typically measured
either by looking at the overlap between subsequences (e.g., BLEU) or by using
embeddings (e.g., BERTScore, S-BERT). Within this paper, we argue that when we
are only interested in measuring the semantic similarity, it is better to
directly predict the similarity using a fine-tuned model for such a task. Using
a fine-tuned model for the Semantic Textual Similarity Benchmark tasks (STS-B)
from the GLUE benchmark, we define the STSScore approach and show that the
resulting similarity is better aligned with our expectations on a robust
semantic similarity measure than other approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.13426">A Chat About Boring Problems: Studying GPT-based text normalization. (arXiv:2309.13426v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bartley_T/0/1/0/all/0/1">Travis M. Bartley</a>, <a href="http://arxiv.org/find/cs/1/au:+Graterol_Fuenmayor_M/0/1/0/all/0/1">Mariana Graterol-Fuenmayor</a>, <a href="http://arxiv.org/find/cs/1/au:+Lavrukhin_V/0/1/0/all/0/1">Vitaly Lavrukhin</a>, <a href="http://arxiv.org/find/cs/1/au:+Bakhturina_E/0/1/0/all/0/1">Evelina Bakhturina</a>, <a href="http://arxiv.org/find/cs/1/au:+Ginsburg_B/0/1/0/all/0/1">Boris Ginsburg</a></p>
<p>Text normalization - the conversion of text from written to spoken form - is
traditionally assumed to be an ill-formed task for language models. In this
work, we argue otherwise. We empirically show the capacity of Large-Language
Models (LLM) for text normalization in few-shot scenarios. Combining
self-consistency reasoning with linguistic-informed prompt engineering, we find
LLM based text normalization to achieve error rates around 40\% lower than top
normalization systems. Further, upon error analysis, we note key limitations in
the conventional design of text normalization tasks. We create a new taxonomy
of text normalization errors and apply it to results from GPT-3.5-Turbo and
GPT-4.0. Through this new framework, we can identify strengths and weaknesses
of GPT-based TN, opening opportunities for future work.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.14517">Watch Your Language: Investigating Content Moderation with Large Language Models. (arXiv:2309.14517v2 [cs.HC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1">Deepak Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+AbuHashem_Y/0/1/0/all/0/1">Yousef AbuHashem</a>, <a href="http://arxiv.org/find/cs/1/au:+Durumeric_Z/0/1/0/all/0/1">Zakir Durumeric</a></p>
<p>Large language models (LLMs) have exploded in popularity due to their ability
to perform a wide array of natural language tasks. Text-based content
moderation is one LLM use case that has received recent enthusiasm, however,
there is little research investigating how LLMs perform in content moderation
settings. In this work, we evaluate a suite of commodity LLMs on two common
content moderation tasks: rule-based community moderation and toxic content
detection. For rule-based community moderation, we instantiate 95 subcommunity
specific LLMs by prompting GPT-3.5 with rules from 95 Reddit subcommunities. We
find that GPT-3.5 is effective at rule-based moderation for many communities,
achieving a median accuracy of 64% and a median precision of 83%. For toxicity
detection, we evaluate a suite of commodity LLMs (GPT-3, GPT-3.5, GPT-4, Gemini
Pro, LLAMA 2) and show that LLMs significantly outperform currently widespread
toxicity classifiers. However, recent increases in model size add only marginal
benefit to toxicity detection, suggesting a potential performance plateau for
LLMs on toxicity detection tasks. We conclude by outlining avenues for future
work in studying LLMs and content moderation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16042">Towards Best Practices of Activation Patching in Language Models: Metrics and Methods. (arXiv:2309.16042v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1">Fred Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Nanda_N/0/1/0/all/0/1">Neel Nanda</a></p>
<p>Mechanistic interpretability seeks to understand the internal mechanisms of
machine learning models, where localization -- identifying the important model
components -- is a key step. Activation patching, also known as causal tracing
or interchange intervention, is a standard technique for this task (Vig et al.,
2020), but the literature contains many variants with little consensus on the
choice of hyperparameters or methodology. In this work, we systematically
examine the impact of methodological details in activation patching, including
evaluation metrics and corruption methods. In several settings of localization
and circuit discovery in language models, we find that varying these
hyperparameters could lead to disparate interpretability results. Backed by
empirical observations, we give conceptual arguments for why certain metrics or
methods may be preferred. Finally, we provide recommendations for the best
practices of activation patching going forwards.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.11374">DialogueLLM: Context and Emotion Knowledge-Tuned Large Language Models for Emotion Recognition in Conversations. (arXiv:2310.11374v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yazhou Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Mengyao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Youxi Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tiwari_P/0/1/0/all/0/1">Prayag Tiwari</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qiuchi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Benyou Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1">Jing Qin</a></p>
<p>Large language models (LLMs) and their variants have shown extraordinary
efficacy across numerous downstream natural language processing (NLP) tasks,
which has presented a new vision for the development of NLP. Despite their
remarkable performance in natural language generating (NLG), LLMs lack a
distinct focus on the emotion understanding domain. As a result, using LLMs for
emotion recognition may lead to suboptimal and inadequate precision. Another
limitation of LLMs is that they are typical trained without leveraging
multi-modal information. To overcome these limitations, we propose DialogueLLM,
a context and emotion knowledge tuned LLM that is obtained by fine-tuning LLaMA
models with 13,638 multi-modal (i.e., texts and videos) emotional dialogues.
The visual information is considered as the supplementary knowledge to
construct high-quality instructions. We offer a comprehensive evaluation of our
proposed model on three benchmarking emotion recognition in conversations (ERC)
datasets and compare the results against the SOTA baselines and other SOTA
LLMs. Additionally, DialogueLLM-7B can be easily trained using LoRA on a 40GB
A100 GPU in 5 hours, facilitating reproducibility for other researchers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01070">Multilingual DistilWhisper: Efficient Distillation of Multi-task Speech Models via Language-Specific Experts. (arXiv:2311.01070v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ferraz_T/0/1/0/all/0/1">Thomas Palmeira Ferraz</a>, <a href="http://arxiv.org/find/cs/1/au:+Boito_M/0/1/0/all/0/1">Marcely Zanon Boito</a>, <a href="http://arxiv.org/find/cs/1/au:+Brun_C/0/1/0/all/0/1">Caroline Brun</a>, <a href="http://arxiv.org/find/cs/1/au:+Nikoulina_V/0/1/0/all/0/1">Vassilina Nikoulina</a></p>
<p>Whisper is a multitask and multilingual speech model covering 99 languages.
It yields commendable automatic speech recognition (ASR) results in a subset of
its covered languages, but the model still underperforms on a non-negligible
number of under-represented languages, a problem exacerbated in smaller model
versions. In this work, we propose DistilWhisper, an approach able to bridge
the performance gap in ASR for these languages while retaining the advantages
of multitask and multilingual capabilities. Our approach involves two key
strategies: lightweight modular ASR fine-tuning of whisper-small using
language-specific experts, and knowledge distillation from whisper-large-v2.
This dual approach allows us to effectively boost ASR performance while keeping
the robustness inherited from the multitask and multilingual pre-training.
Results demonstrate that our approach is more effective than standard
fine-tuning or LoRA adapters, boosting performance in the targeted languages
for both in- and out-of-domain test sets, while introducing only a negligible
parameter overhead at inference.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05268">Modelling prospective memory and resilient situated communications via Wizard of Oz. (arXiv:2311.05268v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yanzhe Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Broz_F/0/1/0/all/0/1">Frank Broz</a>, <a href="http://arxiv.org/find/cs/1/au:+Neerincx_M/0/1/0/all/0/1">Mark Neerincx</a></p>
<p>This abstract presents a scenario for human-robot action in a home setting
involving an older adult and a robot. The scenario is designed to explore the
envisioned modelling of memory for communication with a socially assistive
robots (SAR). The scenario will enable the gathering of data on failures of
speech technology and human-robot communication involving shared memory that
may occur during daily activities such as a music-listening activity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12023">LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning. (arXiv:2311.12023v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1">Han Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Greengard_P/0/1/0/all/0/1">Philip Greengard</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1">Eric P. Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Yoon Kim</a></p>
<p>We propose a simple approach for memory-efficient adaptation of pretrained
language models. Our approach uses an iterative algorithm to decompose each
pretrained matrix into a high-precision low-rank component and a
memory-efficient quantized component. During finetuning, the quantized
component remains fixed and only the low-rank component is updated. We present
an integer linear programming formulation of the quantization component which
enables dynamic configuration of quantization parameters (e.g., bit-width,
block size) for each matrix given an overall target memory budget. We further
explore a data-aware version of the algorithm which uses an approximation of
the Fisher information matrix to weight the reconstruction objective during
matrix decomposition. Experiments on finetuning RoBERTa and LLaMA-2 (7B and
70B) demonstrate that our low-rank plus quantized matrix decomposition approach
(LQ-LoRA) outperforms strong QLoRA and GPTQ-LoRA baselines and enables
aggressive quantization to sub-3 bits with only minor performance degradations.
When finetuned on a language modeling calibration dataset, LQ-LoRA can also be
used for model compression; in this setting our 2.75-bit LLaMA-2-70B model
(which has 2.85 bits on average when including the low-rank components and
requires 27GB of GPU memory) performs respectably compared to the 16-bit
baseline.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13708">Dynamic Fault Analysis in Substations Based on Knowledge Graphs. (arXiv:2311.13708v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Weiwei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Sizhe Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1">Hui Fan</a></p>
<p>To address the challenge of identifying hidden danger in substations from
unstructured text, a novel dynamic analysis method is proposed. We first
extract relevant information from the unstructured text, and then leverages a
flexible distributed search engine built on Elastic-Search to handle the data.
Following this, the hidden Markov model is employed to train the data within
the engine. The Viterbi algorithm is integrated to decipher the hidden state
sequences, facilitating the segmentation and labeling of entities related to
hidden dangers. The final step involves using the Neo4j graph database to
dynamically create a knowledge graph that visualizes hidden dangers in the
substation. The effectiveness of the proposed method is demonstrated through a
case analysis from a specific substation with hidden dangers revealed in the
text records.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16522">Dynamic Fault Characteristics Evaluation in Power Grid. (arXiv:2311.16522v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pei_H/0/1/0/all/0/1">Hao Pei</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1">Si Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chuanfu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Che Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Haoming Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Sizhe Li</a></p>
<p>To enhance the intelligence degree in operation and maintenance, a novel
method for fault detection in power grids is proposed. The proposed GNN-based
approach first identifies fault nodes through a specialized feature extraction
method coupled with a knowledge graph. By incorporating temporal data, the
method leverages the status of nodes from preceding and subsequent time periods
to help current fault detection. To validate the effectiveness of the node
features, a correlation analysis of the output features from each node was
conducted. The results from experiments show that this method can accurately
locate fault nodes in simulation scenarios with a remarkable accuracy.
Additionally, the graph neural network based feature modeling allows for a
qualitative examination of how faults spread across nodes, which provides
valuable insights for analyzing fault nodes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04350">CLadder: Assessing Causal Reasoning in Language Models. (arXiv:2312.04350v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1">Zhijing Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Leeb_F/0/1/0/all/0/1">Felix Leeb</a>, <a href="http://arxiv.org/find/cs/1/au:+Gresele_L/0/1/0/all/0/1">Luigi Gresele</a>, <a href="http://arxiv.org/find/cs/1/au:+Kamal_O/0/1/0/all/0/1">Ojasv Kamal</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_Z/0/1/0/all/0/1">Zhiheng Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Blin_K/0/1/0/all/0/1">Kevin Blin</a>, <a href="http://arxiv.org/find/cs/1/au:+Adauto_F/0/1/0/all/0/1">Fernando Gonzalez Adauto</a>, <a href="http://arxiv.org/find/cs/1/au:+Kleiman_Weiner_M/0/1/0/all/0/1">Max Kleiman-Weiner</a>, <a href="http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1">Mrinmaya Sachan</a>, <a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1">Bernhard Sch&#xf6;lkopf</a></p>
<p>The ability to perform causal reasoning is widely considered a core feature
of intelligence. In this work, we investigate whether large language models
(LLMs) can coherently reason about causality. Much of the existing work in
natural language processing (NLP) focuses on evaluating commonsense causal
reasoning in LLMs, thus failing to assess whether a model can perform causal
inference in accordance with a set of well-defined formal rules. To address
this, we propose a new NLP task, causal inference in natural language, inspired
by the "causal inference engine" postulated by Judea Pearl et al. We compose a
large dataset, CLadder, with 10K samples: based on a collection of causal
graphs and queries (associational, interventional, and counterfactual), we
obtain symbolic questions and ground-truth answers, through an oracle causal
inference engine. These are then translated into natural language. We evaluate
multiple LLMs on our dataset, and we introduce and evaluate a bespoke
chain-of-thought prompting strategy, CausalCoT. We show that our task is highly
challenging for LLMs, and we conduct an in-depth analysis to gain deeper
insights into the causal reasoning abilities of LLMs. Our data is open-sourced
at https://huggingface.co/datasets/causalNLP/cladder, and our code can be found
at https://github.com/causalNLP/cladder.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08846">TiMix: Text-aware Image Mixing for Effective Vision-Language Pre-training. (arXiv:2312.08846v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1">Chaoya Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+ye_W/0/1/0/all/0/1">Wei ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Haiyang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1">Qinghao Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1">Ming Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Ji Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1">Fei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shikun Zhang</a></p>
<p>Self-supervised Multi-modal Contrastive Learning (SMCL) remarkably advances
modern Vision-Language Pre-training (VLP) models by aligning visual and
linguistic modalities. Due to noises in web-harvested text-image pairs,
however, scaling up training data volume in SMCL presents considerable
obstacles in terms of computational cost and data inefficiency. To improve data
efficiency in VLP, we propose Text-aware Image Mixing (TiMix), which integrates
mix-based data augmentation techniques into SMCL, yielding significant
performance improvements without significantly increasing computational
overhead. We provide a theoretical analysis of TiMixfrom a mutual information
(MI) perspective, showing that mixed data samples for cross-modal contrastive
learning implicitly serve as a regularizer for the contrastive loss. The
experimental results demonstrate that TiMix exhibits a comparable performance
on downstream tasks, even with a reduced amount of training data and shorter
training time, when benchmarked against existing methods. This work empirically
and theoretically demonstrates the potential of data mixing for data-efficient
and computationally viable VLP, benefiting broader VLP model adoption in
practical scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.09084">Language Modeling on a SpiNNaker 2 Neuromorphic Chip. (arXiv:2312.09084v2 [cs.NE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nazeer_K/0/1/0/all/0/1">Khaleelulla Khan Nazeer</a>, <a href="http://arxiv.org/find/cs/1/au:+Schone_M/0/1/0/all/0/1">Mark Sch&#xf6;ne</a>, <a href="http://arxiv.org/find/cs/1/au:+Mukherji_R/0/1/0/all/0/1">Rishav Mukherji</a>, <a href="http://arxiv.org/find/cs/1/au:+Vogginger_B/0/1/0/all/0/1">Bernhard Vogginger</a>, <a href="http://arxiv.org/find/cs/1/au:+Mayr_C/0/1/0/all/0/1">Christian Mayr</a>, <a href="http://arxiv.org/find/cs/1/au:+Kappel_D/0/1/0/all/0/1">David Kappel</a>, <a href="http://arxiv.org/find/cs/1/au:+Subramoney_A/0/1/0/all/0/1">Anand Subramoney</a></p>
<p>As large language models continue to scale in size rapidly, so too does the
computational power required to run them. Event-based networks on neuromorphic
devices offer a potential way to reduce energy consumption for inference
significantly. However, to date, most event-based networks that can run on
neuromorphic hardware, including spiking neural networks (SNNs), have not
achieved task performance even on par with LSTM models for language modeling.
As a result, language modeling on neuromorphic devices has seemed a distant
prospect. In this work, we demonstrate the first-ever implementation of a
language model on a neuromorphic device - specifically the SpiNNaker 2 chip -
based on a recently published event-based architecture called the EGRU.
SpiNNaker 2 is a many-core neuromorphic chip designed for large-scale
asynchronous processing, while the EGRU is architected to leverage such
hardware efficiently while maintaining competitive task performance. This
implementation marks the first time a neuromorphic language model matches
LSTMs, setting the stage for taking task performance to the level of large
language models. We also demonstrate results on a gesture recognition task
based on inputs from a DVS camera. Overall, our results showcase the
feasibility of this neuro-inspired neural network in hardware, highlighting
significant gains versus conventional hardware in energy efficiency for the
common use case of single batch inference.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15316">Paralinguistics-Enhanced Large Language Modeling of Spoken Dialogue. (arXiv:2312.15316v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1">Guan-Ting Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Shivakumar_P/0/1/0/all/0/1">Prashanth Gurunath Shivakumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Gandhe_A/0/1/0/all/0/1">Ankur Gandhe</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Chao-Han Huck Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1">Yile Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1">Shalini Ghosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Stolcke_A/0/1/0/all/0/1">Andreas Stolcke</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hung-yi Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Bulyko_I/0/1/0/all/0/1">Ivan Bulyko</a></p>
<p>Large Language Models (LLMs) have demonstrated superior abilities in tasks
such as chatting, reasoning, and question-answering. However, standard LLMs may
ignore crucial paralinguistic information, such as sentiment, emotion, and
speaking style, which are essential for achieving natural, human-like spoken
conversation, especially when such information is conveyed by acoustic cues. We
therefore propose Paralinguistics-enhanced Generative Pretrained Transformer
(ParalinGPT), an LLM that utilizes text and speech modalities to better model
the linguistic content and paralinguistic attributes of spoken dialogue. The
model takes the conversational context of text, speech embeddings, and
paralinguistic attributes as input prompts within a serialized multitasking
multimodal framework. Specifically, our framework serializes tasks in the order
of current paralinguistic attribute prediction, response paralinguistic
attribute prediction, and response text generation with autoregressive
conditioning. We utilize the Switchboard-1 corpus, including its sentiment
labels as the paralinguistic attribute, as our spoken dialogue dataset.
Experimental results indicate the proposed serialized multitasking method
outperforms typical sequence classification techniques on current and response
sentiment classification. Furthermore, leveraging conversational context and
speech embeddings significantly improves both response text generation and
sentiment prediction. Our proposed framework achieves relative improvements of
6.7%, 12.0%, and 3.5% in current sentiment accuracy, response sentiment
accuracy, and response text BLEU score, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02906">MLLM-Protector: Ensuring MLLM&#x27;s Safety without Hurting Performance. (arXiv:2401.02906v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pi_R/0/1/0/all/0/1">Renjie Pi</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1">Tianyang Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1">Yueqi Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_R/0/1/0/all/0/1">Rui Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lian_Q/0/1/0/all/0/1">Qing Lian</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1">Hanze Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jipeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tong Zhang</a></p>
<p>The deployment of multimodal large language models (MLLMs) has brought forth
a unique vulnerability: susceptibility to malicious attacks through visual
inputs. We delve into the novel challenge of defending MLLMs against such
attacks. We discovered that images act as a "foreign language" that is not
considered during alignment, which can make MLLMs prone to producing harmful
responses. Unfortunately, unlike the discrete tokens considered in text-based
LLMs, the continuous nature of image signals presents significant alignment
challenges, which poses difficulty to thoroughly cover the possible scenarios.
This vulnerability is exacerbated by the fact that open-source MLLMs are
predominantly fine-tuned on limited image-text pairs that is much less than the
extensive text-based pretraining corpus, which makes the MLLMs more prone to
catastrophic forgetting of their original abilities during explicit alignment
tuning. To tackle these challenges, we introduce MLLM-Protector, a
plug-and-play strategy combining a lightweight harm detector and a response
detoxifier. The harm detector's role is to identify potentially harmful outputs
from the MLLM, while the detoxifier corrects these outputs to ensure the
response stipulates to the safety standards. This approach effectively
mitigates the risks posed by malicious visual inputs without compromising the
model's overall performance. Our results demonstrate that MLLM-Protector offers
a robust solution to a previously unaddressed aspect of MLLM security.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05268">AUTOACT: Automatic Agent Learning from Scratch via Self-Planning. (arXiv:2401.05268v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1">Shuofei Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Ningyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_R/0/1/0/all/0/1">Runnan Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1">Yujie Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1">Wangchunshu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yuchen Eleanor Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_C/0/1/0/all/0/1">Chengfei Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huajun Chen</a></p>
<p>Language agents have achieved considerable performance on various complex
tasks. Despite the incessant exploration in this field, existing language agent
systems still struggle with costly, non-reproducible data reliance and face the
challenge of compelling a single model for multiple functions. To this end, we
introduce AutoAct, an automatic agent learning framework that does not rely on
large-scale annotated data and synthetic trajectories from closed-source models
(e.g., GPT-4). Given limited data with a tool library, AutoAct first
automatically synthesizes planning trajectories without any assistance from
humans or strong closed-source models. Then, AutoAct leverages a
division-of-labor strategy to automatically differentiate based on the target
task information and synthesized trajectories, producing a sub-agent group to
complete the task. We conduct comprehensive experiments with different LLMs,
which demonstrates that AutoAct yields better or parallel performance compared
to various strong baselines. We even notice that AutoAct, when using the
Llama-2-13b model, can achieve performance comparable to that of the zero-shot
GPT-3.5-Turbo agent. Code will be available at
https://github.com/zjunlp/AutoAct.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06408">AboutMe: Using Self-Descriptions in Webpages to Document the Effects of English Pretraining Data Filters. (arXiv:2401.06408v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lucy_L/0/1/0/all/0/1">Li Lucy</a>, <a href="http://arxiv.org/find/cs/1/au:+Gururangan_S/0/1/0/all/0/1">Suchin Gururangan</a>, <a href="http://arxiv.org/find/cs/1/au:+Soldaini_L/0/1/0/all/0/1">Luca Soldaini</a>, <a href="http://arxiv.org/find/cs/1/au:+Strubell_E/0/1/0/all/0/1">Emma Strubell</a>, <a href="http://arxiv.org/find/cs/1/au:+Bamman_D/0/1/0/all/0/1">David Bamman</a>, <a href="http://arxiv.org/find/cs/1/au:+Klein_L/0/1/0/all/0/1">Lauren Klein</a>, <a href="http://arxiv.org/find/cs/1/au:+Dodge_J/0/1/0/all/0/1">Jesse Dodge</a></p>
<p>Large language models' (LLMs) abilities are drawn from their pretraining
data, and model development begins with data curation. However, decisions
around what data is retained or removed during this initial stage is
under-scrutinized. In our work, we ground web text, which is a popular
pretraining data source, to its social and geographic contexts. We create a new
dataset of 10.3 million self-descriptions of website creators, and extract
information about who they are and where they are from: their topical
interests, social roles, and geographic affiliations. Then, we conduct the
first study investigating how ten "quality" and English language identification
(langID) filters affect webpages that vary along these social dimensions. Our
experiments illuminate a range of implicit preferences in data curation: we
show that some quality classifiers act like topical domain filters, and langID
can overlook English content from some regions of the world. Overall, we hope
that our work will encourage a new line of research on pretraining data
curation practices and its social implications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06792">LightHouse: A Survey of AGI Hallucination. (arXiv:2401.06792v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Feng Wang</a></p>
<p>With the development of artificial intelligence, large-scale models have
become increasingly intelligent. However, numerous studies indicate that
hallucinations within these large models are a bottleneck hindering the
development of AI research. In the pursuit of achieving strong artificial
intelligence, a significant volume of research effort is being invested in the
AGI (Artificial General Intelligence) hallucination research. Previous
explorations have been conducted in researching hallucinations within LLMs
(Large Language Models). As for multimodal AGI, research on hallucinations is
still in an early stage. To further the progress of research in the domain of
hallucinatory phenomena, we present a bird's eye view of hallucinations in AGI,
summarizing the current work on AGI hallucinations and proposing some
directions for future research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06855">Fine-grained Hallucination Detection and Editing for Language Models. (arXiv:2401.06855v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1">Abhika Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Asai_A/0/1/0/all/0/1">Akari Asai</a>, <a href="http://arxiv.org/find/cs/1/au:+Balachandran_V/0/1/0/all/0/1">Vidhisha Balachandran</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yizhong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1">Graham Neubig</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1">Yulia Tsvetkov</a>, <a href="http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1">Hannaneh Hajishirzi</a></p>
<p>Large language models (LMs) are prone to generate diverse factually incorrect
statements, which are widely called hallucinations. Current approaches
predominantly focus on coarse-grained automatic hallucination detection or
editing, overlooking nuanced error levels. In this paper, we propose a novel
task -- automatic fine-grained hallucination detection -- and present a
comprehensive taxonomy encompassing six hierarchically defined types of
hallucination. To facilitate evaluation, we introduce a new benchmark that
includes fine-grained human judgments on two LM outputs across various domains.
Our analysis reveals that ChatGPT and Llama 2-Chat exhibit hallucinations in
60% and 75% of their outputs, respectively, and a majority of these
hallucinations fall into categories that have been underexplored. As an initial
step to address this, we train FAVA, a retrieval-augmented LM by carefully
designing synthetic data generations to detect and correct fine-grained
hallucinations. On our benchmark, our automatic and human evaluations show that
FAVA significantly outperforms ChatGPT on fine-grained hallucination detection
by a large margin though a large room for future improvement still exists.
FAVA's suggested edits also improve the factuality of LM-generated text,
resulting in 5-10% FActScore improvements.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.07544">See the Unseen: Better Context-Consistent Knowledge-Editing by Noises. (arXiv:2401.07544v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Youcheng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1">Wenqiang Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1">Jiancheng Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1">Shuicheng Yan</a></p>
<p>Knowledge-editing updates knowledge of large language models (LLMs) and
contributes to the interpretability and application of LLMs. However, knowledge
applying is context-consistent: LLMs can recall the same knowledge in different
contexts. Existing works ignore this property and the editing lacks
generalization. In this paper, we empirically find that the effects of
different contexts upon LLMs in recalling the same knowledge follow a
Gaussian-like distribution. We then sample Gaussian noises to simulate the
effects of different contexts when updating LLMs. By such, we can make LLMs see
the unseen contexts where the edited knowledge will be applied, therefore
improving the editing generalization. Experimental results on three LLMs
demonstrate the effectiveness of our methods and also distinguish our methods
from the others of fine-tuning LLMs by noises.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.08350">Salute the Classic: Revisiting Challenges of Machine Translation in the Age of Large Language Models. (arXiv:2401.08350v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1">Jianhui Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1">Fanghua Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Longyue Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1">Dian Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_D/0/1/0/all/0/1">Derek F. Wong</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1">Shuming Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1">Zhaopeng Tu</a></p>
<p>The evolution of Neural Machine Translation (NMT) has been significantly
influenced by six core challenges (Koehn and Knowles, 2017), which have acted
as benchmarks for progress in this field. This study revisits these challenges,
offering insights into their ongoing relevance in the context of advanced Large
Language Models (LLMs): domain mismatch, amount of parallel data, rare word
prediction, translation of long sentences, attention model as word alignment,
and sub-optimal beam search. Our empirical findings indicate that LLMs
effectively lessen the reliance on parallel data for major languages in the
pretraining phase. Additionally, the LLM-based translation system significantly
enhances the translation of long sentences that contain approximately 80 words
and shows the capability to translate documents of up to 512 words. However,
despite these significant improvements, the challenges of domain mismatch and
prediction of rare words persist. While the challenges of word alignment and
beam search, specifically associated with NMT, may not apply to LLMs, we
identify three new challenges for LLMs in translation tasks: inference
efficiency, translation of low-resource languages in the pretraining phase, and
human-aligned evaluation. The datasets and models are released at
https://github.com/pangjh3/LLM4MT.
</p>
</p>
</div>

    </div>
    </body>
    