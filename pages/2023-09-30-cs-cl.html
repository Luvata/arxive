<!DOCTYPE html>
<html>
<head>
<title>2023-09-30-cs-cl</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2309.15857">A Survey on Image-text Multimodal Models. (arXiv:2309.15857v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1">Ruifeng Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1">Jingxuan Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Linzhuang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1">Bihui Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_G/0/1/0/all/0/1">Guiyong Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1">Dawei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Sibo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1">Zhengbing Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1">Mingjun Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bu_L/0/1/0/all/0/1">Liping Bu</a></p>
<p>Amidst the evolving landscape of artificial intelligence, the convergence of
visual and textual information has surfaced as a crucial frontier, leading to
the advent of image-text multimodal models. This paper provides a comprehensive
review of the evolution and current state of image-text multimodal models,
exploring their application value, challenges, and potential research
trajectories. Initially, we revisit the basic concepts and developmental
milestones of these models, introducing a novel classification that segments
their evolution into three distinct phases, based on their time of introduction
and subsequent impact on the discipline. Furthermore, based on the tasks'
significance and prevalence in the academic landscape, we propose a
categorization of the tasks associated with image-text multimodal models into
five major types, elucidating the recent progress and key technologies within
each category. Despite the remarkable accomplishments of these models, numerous
challenges and issues persist. This paper delves into the inherent challenges
and limitations of image-text multimodal models, fostering the exploration of
prospective research directions. Our objective is to offer an exhaustive
overview of the present research landscape of image-text multimodal models and
to serve as a valuable reference for future scholarly endeavors. We extend an
invitation to the broader community to collaborate in enhancing the image-text
multimodal model community, accessible at:
\href{https://github.com/i2vec/A-survey-on-image-text-multimodal-models}{https://github.com/i2vec/A-survey-on-image-text-multimodal-models}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.15869">Unsupervised Pre-Training for Vietnamese Automatic Speech Recognition in the HYKIST Project. (arXiv:2309.15869v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Le_Duc_K/0/1/0/all/0/1">Khai Le-Duc</a></p>
<p>In today's interconnected globe, moving abroad is more and more prevalent,
whether it's for employment, refugee resettlement, or other causes. Language
difficulties between natives and immigrants present a common issue on a daily
basis, especially in medical domain. This can make it difficult for patients
and doctors to communicate during anamnesis or in the emergency room, which
compromises patient care. The goal of the HYKIST Project is to develop a speech
translation system to support patient-doctor communication with ASR and MT.
</p>
<p>ASR systems have recently displayed astounding performance on particular
tasks for which enough quantities of training data are available, such as
LibriSpeech. Building a good model is still difficult due to a variety of
speaking styles, acoustic and recording settings, and a lack of in-domain
training data. In this thesis, we describe our efforts to construct ASR systems
for a conversational telephone speech recognition task in the medical domain
for Vietnamese language to assist emergency room contact between doctors and
patients across linguistic barriers. In order to enhance the system's
performance, we investigate various training schedules and data combining
strategies. We also examine how best to make use of the little data that is
available. The use of publicly accessible models like XLSR-53 is compared to
the use of customized pre-trained models, and both supervised and unsupervised
approaches are utilized using wav2vec 2.0 as architecture.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.15991">Targeted Image Data Augmentation Increases Basic Skills Captioning Robustness. (arXiv:2309.15991v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Barriere_V/0/1/0/all/0/1">Valentin Barriere</a>, <a href="http://arxiv.org/find/cs/1/au:+Rio_F/0/1/0/all/0/1">Felipe del Rio</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferari_A/0/1/0/all/0/1">Andres Carvallo De Ferari</a>, <a href="http://arxiv.org/find/cs/1/au:+Aspillaga_C/0/1/0/all/0/1">Carlos Aspillaga</a>, <a href="http://arxiv.org/find/cs/1/au:+Herrera_Berg_E/0/1/0/all/0/1">Eugenio Herrera-Berg</a>, <a href="http://arxiv.org/find/cs/1/au:+Calderon_C/0/1/0/all/0/1">Cristian Buc Calderon</a></p>
<p>Artificial neural networks typically struggle in generalizing to
out-of-context examples. One reason for this limitation is caused by having
datasets that incorporate only partial information regarding the potential
correlational structure of the world. In this work, we propose TIDA (Targeted
Image-editing Data Augmentation), a targeted data augmentation method focused
on improving models' human-like abilities (e.g., gender recognition) by filling
the correlational structure gap using a text-to-image generative model. More
specifically, TIDA identifies specific skills in captions describing images
(e.g., the presence of a specific gender in the image), changes the caption
(e.g., "woman" to "man"), and then uses a text-to-image model to edit the image
in order to match the novel caption (e.g., uniquely changing a woman to a man
while maintaining the context identical). Based on the Flickr30K benchmark, we
show that, compared with the original data set, a TIDA-enhanced dataset related
to gender, color, and counting abilities induces better performance in several
image captioning metrics. Furthermore, on top of relying on the classical BLEU
metric, we conduct a fine-grained analysis of the improvements of our models
against the baseline in different ways. We compared text-to-image generative
models and found different behaviors of the image captioning models in terms of
encoding visual encoding and textual decoding.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16035">MedEdit: Model Editing for Medical Question Answering with External Knowledge Bases. (arXiv:2309.16035v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yucheng Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Shaochen Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhengliang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tianming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1">Ninghao Liu</a></p>
<p>Large Language Models (LLMs), although powerful in general domains, often
perform poorly on domain-specific tasks like medical question answering (QA).
Moreover, they tend to function as "black-boxes," making it challenging to
modify their behavior. Addressing this, our study delves into model editing
utilizing in-context learning, aiming to improve LLM responses without the need
for fine-tuning or retraining. Specifically, we propose a comprehensive
retrieval strategy to extract medical facts from an external knowledge base,
and then we incorporate them into the query prompt for the LLM. Focusing on
medical QA using the MedQA-SMILE dataset, we evaluate the impact of different
retrieval models and the number of facts provided to the LLM. Notably, our
edited Vicuna model exhibited an accuracy improvement from 44.46% to 48.54%.
This work underscores the potential of model editing to enhance LLM
performance, offering a practical approach to mitigate the challenges of
black-box LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16039">Effective Long-Context Scaling of Foundation Models. (arXiv:2309.16039v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1">Wenhan Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jingyu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Molybog_I/0/1/0/all/0/1">Igor Molybog</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hejia Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhargava_P/0/1/0/all/0/1">Prajjwal Bhargava</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_R/0/1/0/all/0/1">Rui Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Martin_L/0/1/0/all/0/1">Louis Martin</a>, <a href="http://arxiv.org/find/cs/1/au:+Rungta_R/0/1/0/all/0/1">Rashi Rungta</a>, <a href="http://arxiv.org/find/cs/1/au:+Sankararaman_K/0/1/0/all/0/1">Karthik Abinav Sankararaman</a>, <a href="http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1">Barlas Oguz</a>, <a href="http://arxiv.org/find/cs/1/au:+Khabsa_M/0/1/0/all/0/1">Madian Khabsa</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1">Han Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mehdad_Y/0/1/0/all/0/1">Yashar Mehdad</a>, <a href="http://arxiv.org/find/cs/1/au:+Narang_S/0/1/0/all/0/1">Sharan Narang</a>, <a href="http://arxiv.org/find/cs/1/au:+Malik_K/0/1/0/all/0/1">Kshitiz Malik</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_A/0/1/0/all/0/1">Angela Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhosale_S/0/1/0/all/0/1">Shruti Bhosale</a>, <a href="http://arxiv.org/find/cs/1/au:+Edunov_S/0/1/0/all/0/1">Sergey Edunov</a>, <a href="http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1">Mike Lewis</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Sinong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1">Hao Ma</a></p>
<p>We present a series of long-context LLMs that support effective context
windows of up to 32,768 tokens. Our model series are built through continual
pretraining from Llama 2 with longer training sequences and on a dataset where
long texts are upsampled. We perform extensive evaluation on language modeling,
synthetic context probing tasks, and a wide range of research benchmarks. On
research benchmarks, our models achieve consistent improvements on most regular
tasks and significant improvements on long-context tasks over Llama 2. Notably,
with a cost-effective instruction tuning procedure that does not require
human-annotated long instruction data, the 70B variant can already surpass
gpt-3.5-turbo-16k's overall performance on a suite of long-context tasks.
Alongside these results, we provide an in-depth analysis on the individual
components of our method. We delve into Llama's position encodings and discuss
its limitation in modeling long dependencies. We also examine the impact of
various design choices in the pretraining process, including the data mix and
the training curriculum of sequence lengths -- our ablation experiments suggest
that having abundant long texts in the pretrain dataset is not the key to
achieving strong performance, and we empirically verify that long context
continual pretraining is more efficient and similarly effective compared to
pretraining from scratch with long sequences.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16042">Towards Best Practices of Activation Patching in Language Models: Metrics and Methods. (arXiv:2309.16042v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1">Fred Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Nanda_N/0/1/0/all/0/1">Neel Nanda</a></p>
<p>Mechanistic interpretability seeks to understand the internal mechanisms of
machine learning models, where localization -- identifying the important model
components -- is a key step. Activation patching, also known as causal tracing
or interchange intervention, is a standard technique for this task (Vig et al.,
2020), but the literature contains many variants with little consensus on the
choice of hyperparameters or methodology. In this work, we systematically
examine the impact of methodological details in activation patching, including
evaluation metrics and corruption methods. In several settings of localization
and circuit discovery in language models, we find that varying these
hyperparameters could lead to disparate interpretability results. Backed by
empirical observations, we give conceptual arguments for why certain metrics or
methods may be preferred. Finally, we provide recommendations for the best
practices of activation patching going forwards.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16058">AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model. (arXiv:2309.16058v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1">Seungwhan Moon</a>, <a href="http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1">Andrea Madotto</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zhaojiang Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Nagarajan_T/0/1/0/all/0/1">Tushar Nagarajan</a>, <a href="http://arxiv.org/find/cs/1/au:+Smith_M/0/1/0/all/0/1">Matt Smith</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1">Shashank Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1">Chun-Fu Yeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Murugesan_P/0/1/0/all/0/1">Prakash Murugesan</a>, <a href="http://arxiv.org/find/cs/1/au:+Heidari_P/0/1/0/all/0/1">Peyman Heidari</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yue Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Srinet_K/0/1/0/all/0/1">Kavya Srinet</a>, <a href="http://arxiv.org/find/cs/1/au:+Damavandi_B/0/1/0/all/0/1">Babak Damavandi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1">Anuj Kumar</a></p>
<p>We present Any-Modality Augmented Language Model (AnyMAL), a unified model
that reasons over diverse input modality signals (i.e. text, image, video,
audio, IMU motion sensor), and generates textual responses. AnyMAL inherits the
powerful text-based reasoning abilities of the state-of-the-art LLMs including
LLaMA-2 (70B), and converts modality-specific signals to the joint textual
space through a pre-trained aligner module. To further strengthen the
multimodal LLM's capabilities, we fine-tune the model with a multimodal
instruction set manually collected to cover diverse topics and tasks beyond
simple QAs. We conduct comprehensive empirical analysis comprising both human
and automatic evaluations, and demonstrate state-of-the-art performance on
various multimodal tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16082">Forgetting Private Textual Sequences in Language Models via Leave-One-Out Ensemble. (arXiv:2309.16082v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhe Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalinli_O/0/1/0/all/0/1">Ozlem Kalinli</a></p>
<p>Recent research has shown that language models have a tendency to memorize
rare or unique token sequences in the training corpus. After deploying a model,
practitioners might be asked to delete any personal information from the model
by individuals' requests. Re-training the underlying model every time
individuals would like to practice their rights to be forgotten is
computationally expensive. We employ a teacher-student framework and propose a
novel leave-one-out ensemble method to unlearn the targeted textual sequences
that need to be forgotten from the model. In our approach, multiple teachers
are trained on disjoint sets; for each targeted sequence to be removed, we
exclude the teacher trained on the set containing this sequence and aggregate
the predictions from remaining teachers to provide supervision during
fine-tuning. Experiments on LibriSpeech and WikiText-103 datasets show that the
proposed method achieves superior privacy-utility trade-offs than other
counterparts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16090">TPE: Towards Better Compositional Reasoning over Conceptual Tools with Multi-persona Collaboration. (arXiv:2309.16090v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hongru Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Huimin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lingzhi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1">Minda Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Rui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_B/0/1/0/all/0/1">Boyang Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1">Hongyuan Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1">Fei Mi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1">Kam-Fai Wong</a></p>
<p>Large language models (LLMs) have demonstrated exceptional performance in
planning the use of various functional tools, such as calculators and
retrievers, particularly in question-answering tasks. In this paper, we expand
the definition of these tools, centering on conceptual tools within the context
of dialogue systems. A conceptual tool specifies a cognitive concept that aids
systematic or investigative thought. These conceptual tools play important
roles in practice, such as multiple psychological or tutoring strategies being
dynamically applied in a single turn to compose helpful responses. To further
enhance the reasoning and planning capability of LLMs with these conceptual
tools, we introduce a multi-persona collaboration framework: Think-Plan-Execute
(TPE). This framework decouples the response generation process into three
distinct roles: Thinker, Planner, and Executor. Specifically, the Thinker
analyzes the internal status exhibited in the dialogue context, such as user
emotions and preferences, to formulate a global guideline. The Planner then
generates executable plans to call different conceptual tools (e.g., sources or
strategies), while the Executor compiles all intermediate results into a
coherent response. This structured approach not only enhances the
explainability and controllability of responses but also reduces token
redundancy. We demonstrate the effectiveness of TPE across various dialogue
response generation tasks, including multi-source (FoCus) and multi-strategy
interactions (CIMA and PsyQA). This reveals its potential to handle real-world
dialogue interactions that require more complicated tool learning beyond just
functional tools. The full code and data will be released for reproduction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16145">The Confidence-Competence Gap in Large Language Models: A Cognitive Study. (arXiv:2309.16145v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1">Aniket Kumar Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Devkota_S/0/1/0/all/0/1">Suman Devkota</a>, <a href="http://arxiv.org/find/cs/1/au:+Lamichhane_B/0/1/0/all/0/1">Bishal Lamichhane</a>, <a href="http://arxiv.org/find/cs/1/au:+Dhakal_U/0/1/0/all/0/1">Uttam Dhakal</a>, <a href="http://arxiv.org/find/cs/1/au:+Dhakal_C/0/1/0/all/0/1">Chandra Dhakal</a></p>
<p>Large Language Models (LLMs) have acquired ubiquitous attention for their
performances across diverse domains. Our study here searches through LLMs'
cognitive abilities and confidence dynamics. We dive deep into understanding
the alignment between their self-assessed confidence and actual performance. We
exploit these models with diverse sets of questionnaires and real-world
scenarios and extract how LLMs exhibit confidence in their responses. Our
findings reveal intriguing instances where models demonstrate high confidence
even when they answer incorrectly. This is reminiscent of the Dunning-Kruger
effect observed in human psychology. In contrast, there are cases where models
exhibit low confidence with correct answers revealing potential underestimation
biases. Our results underscore the need for a deeper understanding of their
cognitive processes. By examining the nuances of LLMs' self-assessment
mechanism, this investigation provides noteworthy revelations that serve to
advance the functionalities and broaden the potential applications of these
formidable language models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16150">AE-GPT: Using Large Language Models to Extract Adverse Events from Surveillance Reports-A Use Case with Influenza Vaccine Adverse Events. (arXiv:2309.16150v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yiming Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jianfu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Jianping He</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1">Cui Tao</a></p>
<p>Though Vaccines are instrumental in global health, mitigating infectious
diseases and pandemic outbreaks, they can occasionally lead to adverse events
(AEs). Recently, Large Language Models (LLMs) have shown promise in effectively
identifying and cataloging AEs within clinical reports. Utilizing data from the
Vaccine Adverse Event Reporting System (VAERS) from 1990 to 2016, this study
particularly focuses on AEs to evaluate LLMs' capability for AE extraction. A
variety of prevalent LLMs, including GPT-2, GPT-3 variants, GPT-4, and Llama 2,
were evaluated using Influenza vaccine as a use case. The fine-tuned GPT 3.5
model (AE-GPT) stood out with a 0.704 averaged micro F1 score for strict match
and 0.816 for relaxed match. The encouraging performance of the AE-GPT
underscores LLMs' potential in processing medical data, indicating a
significant stride towards advanced AE detection, thus presumably generalizable
to other AE extraction tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16155">The Trickle-down Impact of Reward (In-)consistency on RLHF. (arXiv:2309.16155v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1">Lingfeng Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Sihao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1">Linfeng Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1">Lifeng Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1">Baolin Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Mi_H/0/1/0/all/0/1">Haitao Mi</a>, <a href="http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1">Daniel Khashabi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1">Dong Yu</a></p>
<p>Standard practice within Reinforcement Learning from Human Feedback (RLHF)
involves optimizing against a Reward Model (RM), which itself is trained to
reflect human preferences for desirable generations. A notable subject that is
understudied is the (in-)consistency of RMs -- whether they can recognize the
semantic changes to different prompts and appropriately adapt their reward
assignments -- and their impact on the downstream RLHF model.
</p>
<p>In this paper, we visit a series of research questions relevant to RM
inconsistency: (1) How can we measure the consistency of reward models? (2) How
consistent are the existing RMs and how can we improve them? (3) In what ways
does reward inconsistency influence the chatbots resulting from the RLHF model
training?
</p>
<p>We propose Contrast Instructions -- a benchmarking strategy for the
consistency of RM. Each example in Contrast Instructions features a pair of
lexically similar instructions with different ground truth responses. A
consistent RM is expected to rank the corresponding instruction and response
higher than other combinations. We observe that current RMs trained with the
standard ranking objective fail miserably on Contrast Instructions compared to
average humans. To show that RM consistency can be improved efficiently without
using extra training budget, we propose two techniques ConvexDA and
RewardFusion, which enhance reward consistency through extrapolation during the
RM training and inference stage, respectively. We show that RLHF models trained
with a more consistent RM yield more useful responses, suggesting that reward
inconsistency exhibits a trickle-down effect on the downstream RLHF process.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16167">Large Language Model Soft Ideologization via AI-Self-Consciousness. (arXiv:2309.16167v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xiaotian Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaofeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1">Haixu Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiaozhong Liu</a></p>
<p>Large language models (LLMs) have demonstrated human-level performance on a
vast spectrum of natural language tasks. However, few studies have addressed
the LLM threat and vulnerability from an ideology perspective, especially when
they are increasingly being deployed in sensitive domains, e.g., elections and
education. In this study, we explore the implications of GPT soft
ideologization through the use of AI-self-consciousness. By utilizing GPT
self-conversations, AI can be granted a vision to "comprehend" the intended
ideology, and subsequently generate finetuning data for LLM ideology injection.
When compared to traditional government ideology manipulation techniques, such
as information censorship, LLM ideologization proves advantageous; it is easy
to implement, cost-effective, and powerful, thus brimming with risks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16175">Using Weak Supervision and Data Augmentation in Question Answering. (arXiv:2309.16175v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Basu_C/0/1/0/all/0/1">Chumki Basu</a>, <a href="http://arxiv.org/find/cs/1/au:+Garg_H/0/1/0/all/0/1">Himanshu Garg</a>, <a href="http://arxiv.org/find/cs/1/au:+McIntosh_A/0/1/0/all/0/1">Allen McIntosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Sablak_S/0/1/0/all/0/1">Sezai Sablak</a>, <a href="http://arxiv.org/find/cs/1/au:+Wullert_J/0/1/0/all/0/1">John R. Wullert II</a></p>
<p>The onset of the COVID-19 pandemic accentuated the need for access to
biomedical literature to answer timely and disease-specific questions. During
the early days of the pandemic, one of the biggest challenges we faced was the
lack of peer-reviewed biomedical articles on COVID-19 that could be used to
train machine learning models for question answering (QA). In this paper, we
explore the roles weak supervision and data augmentation play in training deep
neural network QA models. First, we investigate whether labels generated
automatically from the structured abstracts of scholarly papers using an
information retrieval algorithm, BM25, provide a weak supervision signal to
train an extractive QA model. We also curate new QA pairs using information
retrieval techniques, guided by the clinicaltrials.gov schema and the
structured abstracts of articles, in the absence of annotated data from
biomedical domain experts. Furthermore, we explore augmenting the training data
of a deep neural network model with linguistic features from external sources
such as lexical databases to account for variations in word morphology and
meaning. To better utilize our training data, we apply curriculum learning to
domain adaptation, fine-tuning our QA model in stages based on characteristics
of the QA pairs. We evaluate our methods in the context of QA models at the
core of a system to answer questions about COVID-19.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16202">Marathi-English Code-mixed Text Generation. (arXiv:2309.16202v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Amin_D/0/1/0/all/0/1">Dhiraj Amin</a>, <a href="http://arxiv.org/find/cs/1/au:+Govilkar_S/0/1/0/all/0/1">Sharvari Govilkar</a>, <a href="http://arxiv.org/find/cs/1/au:+Kulkarni_S/0/1/0/all/0/1">Sagar Kulkarni</a>, <a href="http://arxiv.org/find/cs/1/au:+Lalit_Y/0/1/0/all/0/1">Yash Shashikant Lalit</a>, <a href="http://arxiv.org/find/cs/1/au:+Khwaja_A/0/1/0/all/0/1">Arshi Ajaz Khwaja</a>, <a href="http://arxiv.org/find/cs/1/au:+Xavier_D/0/1/0/all/0/1">Daries Xavier</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1">Sahil Girijashankar Gupta</a></p>
<p>Code-mixing, the blending of linguistic elements from distinct languages to
form meaningful sentences, is common in multilingual settings, yielding hybrid
languages like Hinglish and Minglish. Marathi, India's third most spoken
language, often integrates English for precision and formality. Developing
code-mixed language systems, like Marathi-English (Minglish), faces resource
constraints. This research introduces a Marathi-English code-mixed text
generation algorithm, assessed with Code Mixing Index (CMI) and Degree of Code
Mixing (DCM) metrics. Across 2987 code-mixed questions, it achieved an average
CMI of 0.2 and an average DCM of 7.4, indicating effective and comprehensible
code-mixed sentences. These results offer potential for enhanced NLP tools,
bridging linguistic gaps in multilingual societies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16228">Brand Network Booster: A New System for Improving Brand Connectivity. (arXiv:2309.16228v1 [cs.SI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cancellieri_J/0/1/0/all/0/1">J. Cancellieri</a>, <a href="http://arxiv.org/find/cs/1/au:+Didimo_W/0/1/0/all/0/1">W. Didimo</a>, <a href="http://arxiv.org/find/cs/1/au:+Colladon_A/0/1/0/all/0/1">A. Fronzetti Colladon</a>, <a href="http://arxiv.org/find/cs/1/au:+Montecchiani_F/0/1/0/all/0/1">F. Montecchiani</a></p>
<p>This paper presents a new decision support system offered for an in-depth
analysis of semantic networks, which can provide insights for a better
exploration of a brand's image and the improvement of its connectivity. In
terms of network analysis, we show that this goal is achieved by solving an
extended version of the Maximum Betweenness Improvement problem, which includes
the possibility of considering adversarial nodes, constrained budgets, and
weighted networks - where connectivity improvement can be obtained by adding
links or increasing the weight of existing connections. We present this new
system together with two case studies, also discussing its performance. Our
tool and approach are useful both for network scholars and for supporting the
strategic decision-making processes of marketing and communication managers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16231">Controllable Text Generation with Residual Memory Transformer. (arXiv:2309.16231v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hanqing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1">Sun Si</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Haiming Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1">Dawei Song</a></p>
<p>Large-scale Causal Language Models (CLMs), e.g., GPT3 and ChatGPT, have
brought great success in text generation. However, it is still an open
challenge to control the generation process of CLM while balancing flexibility,
control granularity, and generation efficiency. In this paper, we provide a new
alternative for controllable text generation (CTG), by designing a
non-intrusive, lightweight control plugin to accompany the generation of CLM at
arbitrary time steps. The proposed control plugin, namely Residual Memory
Transformer (RMT), has an encoder-decoder setup, which can accept any types of
control conditions and cooperate with CLM through a residual learning paradigm,
to achieve a more flexible, general, and efficient CTG. Extensive experiments
are carried out on various control tasks, in the form of both automatic and
human evaluations. The results show the superiority of RMT over a range of
state-of-the-art approaches, proving the effectiveness and versatility of our
approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16234">Analyzing Political Figures in Real-Time: Leveraging YouTube Metadata for Sentiment Analysis. (arXiv:2309.16234v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Putra_D/0/1/0/all/0/1">Danendra Athallariq Harya Putra</a>, <a href="http://arxiv.org/find/cs/1/au:+Muharram_A/0/1/0/all/0/1">Arief Purnama Muharram</a></p>
<p>Sentiment analysis using big data from YouTube videos metadata can be
conducted to analyze public opinions on various political figures who represent
political parties. This is possible because YouTube has become one of the
platforms for people to express themselves, including their opinions on various
political figures. The resulting sentiment analysis can be useful for political
executives to gain an understanding of public sentiment and develop appropriate
and effective political strategies. This study aimed to build a sentiment
analysis system leveraging YouTube videos metadata. The sentiment analysis
system was built using Apache Kafka, Apache PySpark, and Hadoop for big data
handling; TensorFlow for deep learning handling; and FastAPI for deployment on
the server. The YouTube videos metadata used in this study is the video
description. The sentiment analysis model was built using LSTM algorithm and
produces two types of sentiments: positive and negative sentiments. The
sentiment analysis results are then visualized in the form a simple web-based
dashboard.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16235">Language models in molecular discovery. (arXiv:2309.16235v1 [physics.chem-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Janakarajan_N/0/1/0/all/0/1">Nikita Janakarajan</a>, <a href="http://arxiv.org/find/physics/1/au:+Erdmann_T/0/1/0/all/0/1">Tim Erdmann</a>, <a href="http://arxiv.org/find/physics/1/au:+Swaminathan_S/0/1/0/all/0/1">Sarath Swaminathan</a>, <a href="http://arxiv.org/find/physics/1/au:+Laino_T/0/1/0/all/0/1">Teodoro Laino</a>, <a href="http://arxiv.org/find/physics/1/au:+Born_J/0/1/0/all/0/1">Jannis Born</a></p>
<p>The success of language models, especially transformer-based architectures,
has trickled into other domains giving rise to "scientific language models"
that operate on small molecules, proteins or polymers. In chemistry, language
models contribute to accelerating the molecule discovery cycle as evidenced by
promising recent findings in early-stage drug discovery. Here, we review the
role of language models in molecular discovery, underlining their strength in
de novo drug design, property prediction and reaction chemistry. We highlight
valuable open-source software assets thus lowering the entry barrier to the
field of scientific language modeling. Last, we sketch a vision for future
molecular design that combines a chatbot interface with access to computational
chemistry tools. Our contribution serves as a valuable resource for
researchers, chemists, and AI enthusiasts interested in understanding how
language models can and will be used to accelerate chemical discovery.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16248">Spider4SPARQL: A Complex Benchmark for Evaluating Knowledge Graph Question Answering Systems. (arXiv:2309.16248v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kosten_C/0/1/0/all/0/1">Catherine Kosten</a>, <a href="http://arxiv.org/find/cs/1/au:+Cudre_Mauroux_P/0/1/0/all/0/1">Philippe Cudr&#xe9;-Mauroux</a>, <a href="http://arxiv.org/find/cs/1/au:+Stockinger_K/0/1/0/all/0/1">Kurt Stockinger</a></p>
<p>With the recent spike in the number and availability of Large Language Models
(LLMs), it has become increasingly important to provide large and realistic
benchmarks for evaluating Knowledge Graph Question Answering (KBQA) systems. So
far the majority of benchmarks rely on pattern-based SPARQL query generation
approaches. The subsequent natural language (NL) question generation is
conducted through crowdsourcing or other automated methods, such as rule-based
paraphrasing or NL question templates. Although some of these datasets are of
considerable size, their pitfall lies in their pattern-based generation
approaches, which do not always generalize well to the vague and linguistically
diverse questions asked by humans in real-world contexts.
</p>
<p>In this paper, we introduce Spider4SPARQL - a new SPARQL benchmark dataset
featuring 9,693 previously existing manually generated NL questions and 4,721
unique, novel, and complex SPARQL queries of varying complexity. In addition to
the NL/SPARQL pairs, we also provide their corresponding 166 knowledge graphs
and ontologies, which cover 138 different domains. Our complex benchmark
enables novel ways of evaluating the strengths and weaknesses of modern KGQA
systems. We evaluate the system with state-of-the-art KGQA systems as well as
LLMs, which achieve only up to 45\% execution accuracy, demonstrating that
Spider4SPARQL is a challenging benchmark for future research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16254">On the Challenges of Fully Incremental Neural Dependency Parsing. (arXiv:2309.16254v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ezquerro_A/0/1/0/all/0/1">Ana Ezquerro</a>, <a href="http://arxiv.org/find/cs/1/au:+Gomez_Rodriguez_C/0/1/0/all/0/1">Carlos G&#xf3;mez-Rodr&#xed;guez</a>, <a href="http://arxiv.org/find/cs/1/au:+Vilares_D/0/1/0/all/0/1">David Vilares</a></p>
<p>Since the popularization of BiLSTMs and Transformer-based bidirectional
encoders, state-of-the-art syntactic parsers have lacked incrementality,
requiring access to the whole sentence and deviating from human language
processing. This paper explores whether fully incremental dependency parsing
with modern architectures can be competitive. We build parsers combining
strictly left-to-right neural encoders with fully incremental sequence-labeling
and transition-based decoders. The results show that fully incremental parsing
with modern architectures considerably lags behind bidirectional parsing,
noting the challenges of psycholinguistically plausible parsing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16270">Social Media Fashion Knowledge Extraction as Captioning. (arXiv:2309.16270v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1">Yifei Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenxuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1">Yang Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1">Wai Lam</a></p>
<p>Social media plays a significant role in boosting the fashion industry, where
a massive amount of fashion-related posts are generated every day. In order to
obtain the rich fashion information from the posts, we study the task of social
media fashion knowledge extraction. Fashion knowledge, which typically consists
of the occasion, person attributes, and fashion item information, can be
effectively represented as a set of tuples. Most previous studies on fashion
knowledge extraction are based on the fashion product images without
considering the rich text information in social media posts. Existing work on
fashion knowledge extraction in social media is classification-based and
requires to manually determine a set of fashion knowledge categories in
advance. In our work, we propose to cast the task as a captioning problem to
capture the interplay of the multimodal post information. Specifically, we
transform the fashion knowledge tuples into a natural language caption with a
sentence transformation method. Our framework then aims to generate the
sentence-based fashion knowledge directly from the social media post. Inspired
by the big success of pre-trained models, we build our model based on a
multimodal pre-trained generative model and design several auxiliary tasks for
enhancing the knowledge extraction. Since there is no existing dataset which
can be directly borrowed to our task, we introduce a dataset consisting of
social media posts with manual fashion knowledge annotation. Extensive
experiments are conducted to demonstrate the effectiveness of our model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16275">UPB @ ACTI: Detecting Conspiracies using fine tuned Sentence Transformers. (arXiv:2309.16275v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Paraschiv_A/0/1/0/all/0/1">Andrei Paraschiv</a>, <a href="http://arxiv.org/find/cs/1/au:+Dascalu_M/0/1/0/all/0/1">Mihai Dascalu</a></p>
<p>Conspiracy theories have become a prominent and concerning aspect of online
discourse, posing challenges to information integrity and societal trust. As
such, we address conspiracy theory detection as proposed by the ACTI @ EVALITA
2023 shared task. The combination of pre-trained sentence Transformer models
and data augmentation techniques enabled us to secure first place in the final
leaderboard of both sub-tasks. Our methodology attained F1 scores of 85.71% in
the binary classification and 91.23% for the fine-grained conspiracy topic
classification, surpassing other competing systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16283">Self-supervised Cross-view Representation Reconstruction for Change Captioning. (arXiv:2309.16283v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tu_Y/0/1/0/all/0/1">Yunbin Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Liang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1">Li Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1">Zheng-Jun Zha</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1">Chenggang Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1">Qingming Huang</a></p>
<p>Change captioning aims to describe the difference between a pair of similar
images. Its key challenge is how to learn a stable difference representation
under pseudo changes caused by viewpoint change. In this paper, we address this
by proposing a self-supervised cross-view representation reconstruction
(SCORER) network. Concretely, we first design a multi-head token-wise matching
to model relationships between cross-view features from similar/dissimilar
images. Then, by maximizing cross-view contrastive alignment of two similar
images, SCORER learns two view-invariant image representations in a
self-supervised way. Based on these, we reconstruct the representations of
unchanged objects by cross-attention, thus learning a stable difference
representation for caption generation. Further, we devise a cross-modal
backward reasoning to improve the quality of caption. This module reversely
models a ``hallucination'' representation with the caption and ``before''
representation. By pushing it closer to the ``after'' representation, we
enforce the caption to be informative about the difference in a self-supervised
manner. Extensive experiments show our method achieves the state-of-the-art
results on four datasets. The code is available at
https://github.com/tuyunbin/SCORER.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16289">LawBench: Benchmarking Legal Knowledge of Large Language Models. (arXiv:2309.16289v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fei_Z/0/1/0/all/0/1">Zhiwei Fei</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1">Xiaoyu Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1">Dawei Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1">Fengzhe Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1">Zhuo Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Songyang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Kai Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1">Zongwen Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_J/0/1/0/all/0/1">Jidong Ge</a></p>
<p>Large language models (LLMs) have demonstrated strong capabilities in various
aspects. However, when applying them to the highly specialized, safe-critical
legal domain, it is unclear how much legal knowledge they possess and whether
they can reliably perform legal-related tasks. To address this gap, we propose
a comprehensive evaluation benchmark LawBench. LawBench has been meticulously
crafted to have precise assessment of the LLMs' legal capabilities from three
cognitive levels: (1) Legal knowledge memorization: whether LLMs can memorize
needed legal concepts, articles and facts; (2) Legal knowledge understanding:
whether LLMs can comprehend entities, events and relationships within legal
text; (3) Legal knowledge applying: whether LLMs can properly utilize their
legal knowledge and make necessary reasoning steps to solve realistic legal
tasks. LawBench contains 20 diverse tasks covering 5 task types: single-label
classification (SLC), multi-label classification (MLC), regression, extraction
and generation. We perform extensive evaluations of 51 LLMs on LawBench,
including 20 multilingual LLMs, 22 Chinese-oriented LLMs and 9 legal specific
LLMs. The results show that GPT-4 remains the best-performing LLM in the legal
domain, surpassing the others by a significant margin. While fine-tuning LLMs
on legal specific text brings certain improvements, we are still a long way
from obtaining usable and reliable LLMs in legal tasks. All data, model
predictions and evaluation code are released in
https://github.com/open-compass/LawBench/. We hope this benchmark provides
in-depth understanding of the LLMs' domain-specified capabilities and speed up
the development of LLMs in the legal domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16292">DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models. (arXiv:2309.16292v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1">Licheng Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1">Daocheng Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1">Xinyu Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1">Tao Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1">Pinlong Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Dou_M/0/1/0/all/0/1">Min Dou</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1">Botian Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1">Liang He</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a></p>
<p>Recent advancements in autonomous driving have relied on data-driven
approaches, which are widely adopted but face challenges including dataset
bias, overfitting, and uninterpretability. Drawing inspiration from the
knowledge-driven nature of human driving, we explore the question of how to
instill similar capabilities into autonomous driving systems and summarize a
paradigm that integrates an interactive environment, a driver agent, as well as
a memory component to address this question. Leveraging large language models
with emergent abilities, we propose the DiLu framework, which combines a
Reasoning and a Reflection module to enable the system to perform
decision-making based on common-sense knowledge and evolve continuously.
Extensive experiments prove DiLu's capability to accumulate experience and
demonstrate a significant advantage in generalization ability over
reinforcement learning-based methods. Moreover, DiLu is able to directly
acquire experiences from real-world datasets which highlights its potential to
be deployed on practical autonomous driving systems. To the best of our
knowledge, we are the first to instill knowledge-driven capability into
autonomous driving systems from the perspective of how humans drive.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16298">At Which Training Stage Does Cocde Data Help LLMs Reasoning?. (arXiv:2309.16298v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yingwei Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yue Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yue Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuanliang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yu Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Changjian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shanshan Li</a></p>
<p>Large Language Models (LLMs) have exhibited remarkable reasoning capabilities
and become the foundation of language technologies. Inspired by the great
success of code data in training LLMs, we naturally wonder at which training
stage introducing code data can really help LLMs reasoning. To this end, this
paper systematically explores the impact of code data on LLMs at different
stages. Concretely, we introduce the code data at the pre-training stage,
instruction-tuning stage, and both of them, respectively. Then, the reasoning
capability of LLMs is comprehensively and fairly evaluated via six reasoning
tasks in five domains. We critically analyze the experimental results and
provide conclusions with insights. First, pre-training LLMs with the mixture of
code and text can significantly enhance LLMs' general reasoning capability
almost without negative transfer on other tasks. Besides, at the
instruction-tuning stage, code data endows LLMs the task-specific reasoning
capability. Moreover, the dynamic mixing strategy of code and text data assists
LLMs to learn reasoning capability step-by-step during training. These insights
deepen the understanding of LLMs regarding reasoning ability for their
application, such as scientific question answering, legal support, etc. The
source code and model parameters are released at the
link:~\url{https://github.com/yingweima2022/CodeLLM}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16319">Augmenting transformers with recursively composed multi-grained representations. (arXiv:2309.16319v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xiang Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1">Qingyang Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1">Kewei Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1">Wei Wu</a></p>
<p>We present ReCAT, a recursive composition augmented Transformer that is able
to explicitly model hierarchical syntactic structures of raw texts without
relying on gold trees during both learning and inference. Existing research
along this line restricts data to follow a hierarchical tree structure and thus
lacks inter-span communications. To overcome the problem, we propose a novel
contextual inside-outside (CIO) layer that learns contextualized
representations of spans through bottom-up and top-down passes, where a
bottom-up pass forms representations of high-level spans by composing low-level
spans, while a top-down pass combines information inside and outside a span. By
stacking several CIO layers between the embedding layer and the attention
layers in Transformer, the ReCAT model can perform both deep intra-span and
deep inter-span interactions, and thus generate multi-grained representations
fully contextualized with other spans. Moreover, the CIO layers can be jointly
pre-trained with Transformers, making ReCAT enjoy scaling ability, strong
performance, and interpretability at the same time. We conduct experiments on
various sentence-level and span-level tasks. Evaluation results indicate that
ReCAT can significantly outperform vanilla Transformer models on all span-level
tasks and baselines that combine recursive networks with Transformers on
natural language inference tasks. More interestingly, the hierarchical
structures induced by ReCAT exhibit strong consistency with human-annotated
syntactic trees, indicating good interpretability brought by the CIO layers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16347">Intrinsic Language-Guided Exploration for Complex Long-Horizon Robotic Manipulation Tasks. (arXiv:2309.16347v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Triantafyllidis_E/0/1/0/all/0/1">Eleftherios Triantafyllidis</a>, <a href="http://arxiv.org/find/cs/1/au:+Christianos_F/0/1/0/all/0/1">Filippos Christianos</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhibin Li</a></p>
<p>Current reinforcement learning algorithms struggle in sparse and complex
environments, most notably in long-horizon manipulation tasks entailing a
plethora of different sequences. In this work, we propose the Intrinsically
Guided Exploration from Large Language Models (IGE-LLMs) framework. By
leveraging LLMs as an assistive intrinsic reward, IGE-LLMs guides the
exploratory process in reinforcement learning to address intricate long-horizon
with sparse rewards robotic manipulation tasks. We evaluate our framework and
related intrinsic learning methods in an environment challenged with
exploration, and a complex robotic manipulation task challenged by both
exploration and long-horizons. Results show IGE-LLMs (i) exhibit notably higher
performance over related intrinsic methods and the direct use of LLMs in
decision-making, (ii) can be combined and complement existing learning methods
highlighting its modularity, (iii) are fairly insensitive to different
intrinsic scaling parameters, and (iv) maintain robustness against increased
levels of uncertainty and horizons.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16349">Human Feedback is not Gold Standard. (arXiv:2309.16349v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hosking_T/0/1/0/all/0/1">Tom Hosking</a>, <a href="http://arxiv.org/find/cs/1/au:+Blunsom_P/0/1/0/all/0/1">Phil Blunsom</a>, <a href="http://arxiv.org/find/cs/1/au:+Bartolo_M/0/1/0/all/0/1">Max Bartolo</a></p>
<p>Human feedback has become the de facto standard for evaluating the
performance of Large Language Models, and is increasingly being used as a
training objective. However, it is not clear which properties of a generated
output this single `preference' score captures. We hypothesise that preference
scores are subjective and open to undesirable biases. We critically analyse the
use of human feedback for both training and evaluation, to verify whether it
fully captures a range of crucial error criteria. We find that while preference
scores have fairly good coverage, they under-represent important aspects like
factuality. We further hypothesise that both preference scores and error
annotation may be affected by confounders, and leverage instruction-tuned
models to generate outputs that vary along two possible confounding dimensions:
assertiveness and complexity. We find that the assertiveness of an output skews
the perceived rate of factuality errors, indicating that human annotations are
not a fully reliable evaluation metric or training objective. Finally, we offer
preliminary evidence that using human feedback as a training objective
disproportionately increases the assertiveness of model outputs. We encourage
future work to carefully consider whether preference scores are well aligned
with the desired objective.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16354">Transformer-VQ: Linear-Time Transformers via Vector Quantization. (arXiv:2309.16354v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lingle_L/0/1/0/all/0/1">Lucas D. Lingle</a></p>
<p>We introduce Transformer-VQ, a decoder-only transformer computing
softmax-based dense self-attention in linear time. Transformer-VQ's efficient
attention is enabled by vector-quantized keys and a novel caching mechanism. In
large-scale experiments, Transformer-VQ is shown highly competitive in quality,
with strong results on Enwik8 (0.99 bpb), PG-19 (26.6 ppl), and ImageNet64
(3.16 bpb). Code: https://github.com/transformer-vq/transformer_vq
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16396">A Comprehensive Survey of Document-level Relation Extraction (2016-2022). (arXiv:2309.16396v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Delaunay_J/0/1/0/all/0/1">Julien Delaunay</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1">Thi Hong Hanh Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Gonzalez_Gallardo_C/0/1/0/all/0/1">Carlos-Emiliano Gonz&#xe1;lez-Gallardo</a>, <a href="http://arxiv.org/find/cs/1/au:+Bordea_G/0/1/0/all/0/1">Georgeta Bordea</a>, <a href="http://arxiv.org/find/cs/1/au:+Sidere_N/0/1/0/all/0/1">Nicolas Sidere</a>, <a href="http://arxiv.org/find/cs/1/au:+Doucet_A/0/1/0/all/0/1">Antoine Doucet</a></p>
<p>Document-level relation extraction (DocRE) is an active area of research in
natural language processing (NLP) concerned with identifying and extracting
relationships between entities beyond sentence boundaries. Compared to the more
traditional sentence-level relation extraction, DocRE provides a broader
context for analysis and is more challenging because it involves identifying
relationships that may span multiple sentences or paragraphs. This task has
gained increased interest as a viable solution to build and populate knowledge
bases automatically from unstructured large-scale documents (e.g., scientific
papers, legal contracts, or news articles), in order to have a better
understanding of relationships between entities. This paper aims to provide a
comprehensive overview of recent advances in this field, highlighting its
different applications in comparison to sentence-level relation extraction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16424">Prompt-and-Align: Prompt-Based Social Alignment for Few-Shot Fake News Detection. (arXiv:2309.16424v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jiaying Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_A/0/1/0/all/0/1">Ailin Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_M/0/1/0/all/0/1">Miao Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1">Bryan Hooi</a></p>
<p>Despite considerable advances in automated fake news detection, due to the
timely nature of news, it remains a critical open question how to effectively
predict the veracity of news articles based on limited fact-checks. Existing
approaches typically follow a "Train-from-Scratch" paradigm, which is
fundamentally bounded by the availability of large-scale annotated data. While
expressive pre-trained language models (PLMs) have been adapted in a
"Pre-Train-and-Fine-Tune" manner, the inconsistency between pre-training and
downstream objectives also requires costly task-specific supervision. In this
paper, we propose "Prompt-and-Align" (P&amp;A), a novel prompt-based paradigm for
few-shot fake news detection that jointly leverages the pre-trained knowledge
in PLMs and the social context topology. Our approach mitigates label scarcity
by wrapping the news article in a task-related textual prompt, which is then
processed by the PLM to directly elicit task-specific knowledge. To supplement
the PLM with social context without inducing additional training overheads,
motivated by empirical observation on user veracity consistency (i.e., social
users tend to consume news of the same veracity type), we further construct a
news proximity graph among news articles to capture the veracity-consistent
signals in shared readerships, and align the prompting predictions along the
graph edges in a confidence-informed manner. Extensive experiments on three
real-world benchmarks demonstrate that P&amp;A sets new states-of-the-art for
few-shot fake news detection performance by significant margins.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16459">Augmenting LLMs with Knowledge: A survey on hallucination prevention. (arXiv:2309.16459v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Andriopoulos_K/0/1/0/all/0/1">Konstantinos Andriopoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Pouwelse_J/0/1/0/all/0/1">Johan Pouwelse</a></p>
<p>Large pre-trained language models have demonstrated their proficiency in
storing factual knowledge within their parameters and achieving remarkable
results when fine-tuned for downstream natural language processing tasks.
Nonetheless, their capacity to access and manipulate knowledge with precision
remains constrained, resulting in performance disparities on
knowledge-intensive tasks when compared to task-specific architectures.
Additionally, the challenges of providing provenance for model decisions and
maintaining up-to-date world knowledge persist as open research frontiers. To
address these limitations, the integration of pre-trained models with
differentiable access mechanisms to explicit non-parametric memory emerges as a
promising solution. This survey delves into the realm of language models (LMs)
augmented with the ability to tap into external knowledge sources, including
external knowledge bases and search engines. While adhering to the standard
objective of predicting missing tokens, these augmented LMs leverage diverse,
possibly non-parametric external modules to augment their contextual processing
capabilities, departing from the conventional language modeling paradigm.
Through an exploration of current advancements in augmenting large language
models with knowledge, this work concludes that this emerging research
direction holds the potential to address prevalent issues in traditional LMs,
such as hallucinations, un-grounded responses, and scalability challenges.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16511">Toloka Visual Question Answering Benchmark. (arXiv:2309.16511v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ustalov_D/0/1/0/all/0/1">Dmitry Ustalov</a>, <a href="http://arxiv.org/find/cs/1/au:+Pavlichenko_N/0/1/0/all/0/1">Nikita Pavlichenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Koshelev_S/0/1/0/all/0/1">Sergey Koshelev</a>, <a href="http://arxiv.org/find/cs/1/au:+Likhobaba_D/0/1/0/all/0/1">Daniil Likhobaba</a>, <a href="http://arxiv.org/find/cs/1/au:+Smirnova_A/0/1/0/all/0/1">Alisa Smirnova</a></p>
<p>In this paper, we present Toloka Visual Question Answering, a new
crowdsourced dataset allowing comparing performance of machine learning systems
against human level of expertise in the grounding visual question answering
task. In this task, given an image and a textual question, one has to draw the
bounding box around the object correctly responding to that question. Every
image-question pair contains the response, with only one correct response per
image. Our dataset contains 45,199 pairs of images and questions in English,
provided with ground truth bounding boxes, split into train and two test
subsets. Besides describing the dataset and releasing it under a CC BY license,
we conducted a series of experiments on open source zero-shot baseline models
and organized a multi-phase competition at WSDM Cup that attracted 48
participants worldwide. However, by the time of paper submission, no machine
learning model outperformed the non-expert crowdsourcing baseline according to
the intersection over union evaluation score.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16535">KLoB: a Benchmark for Assessing Knowledge Locating Methods in Language Models. (arXiv:2309.16535v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ju_Y/0/1/0/all/0/1">Yiming Ju</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zheng Zhang</a></p>
<p>Recently, Locate-Then-Edit paradigm has emerged as one of the main approaches
in changing factual knowledge stored in the Language models. However, there is
a lack of research on whether present locating methods can pinpoint the exact
parameters embedding the desired knowledge. Moreover, although many researchers
have questioned the validity of locality hypothesis of factual knowledge, no
method is provided to test the a hypothesis for more in-depth discussion and
research. Therefore, we introduce KLoB, a benchmark examining three essential
properties that a reliable knowledge locating method should satisfy. KLoB can
serve as a benchmark for evaluating existing locating methods in language
models, and can contributes a method to reassessing the validity of locality
hypothesis of factual knowledge. Our is publicly available at
\url{https://github.com/juyiming/KLoB}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16540">Unsupervised Fact Verification by Language Model Distillation. (arXiv:2309.16540v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bazaga_A/0/1/0/all/0/1">Adri&#xe1;n Bazaga</a>, <a href="http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1">Pietro Li&#xf2;</a>, <a href="http://arxiv.org/find/cs/1/au:+Micklem_G/0/1/0/all/0/1">Gos Micklem</a></p>
<p>Unsupervised fact verification aims to verify a claim using evidence from a
trustworthy knowledge base without any kind of data annotation. To address this
challenge, algorithms must produce features for every claim that are both
semantically meaningful, and compact enough to find a semantic alignment with
the source information. In contrast to previous work, which tackled the
alignment problem by learning over annotated corpora of claims and their
corresponding labels, we propose SFAVEL (Self-supervised Fact Verification via
Language Model Distillation), a novel unsupervised framework that leverages
pre-trained language models to distil self-supervised features into
high-quality claim-fact alignments without the need for annotations. This is
enabled by a novel contrastive loss function that encourages features to attain
high-quality claim and evidence alignments whilst preserving the semantic
relationships across the corpora. Notably, we present results that achieve a
new state-of-the-art on the standard FEVER fact verification benchmark (+8%
accuracy) with linear evaluation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16573">The ARRT of Language-Models-as-a-Service: Overview of a New Paradigm and its Challenges. (arXiv:2309.16573v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Malfa_E/0/1/0/all/0/1">Emanuele La Malfa</a>, <a href="http://arxiv.org/find/cs/1/au:+Petrov_A/0/1/0/all/0/1">Aleksandar Petrov</a>, <a href="http://arxiv.org/find/cs/1/au:+Frieder_S/0/1/0/all/0/1">Simon Frieder</a>, <a href="http://arxiv.org/find/cs/1/au:+Weinhuber_C/0/1/0/all/0/1">Christoph Weinhuber</a>, <a href="http://arxiv.org/find/cs/1/au:+Burnell_R/0/1/0/all/0/1">Ryan Burnell</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohn_A/0/1/0/all/0/1">Anthony G. Cohn</a>, <a href="http://arxiv.org/find/cs/1/au:+Shadbolt_N/0/1/0/all/0/1">Nigel Shadbolt</a>, <a href="http://arxiv.org/find/cs/1/au:+Wooldridge_M/0/1/0/all/0/1">Michael Wooldridge</a></p>
<p>Some of the most powerful language models currently are proprietary systems,
accessible only via (typically restrictive) web or software programming
interfaces. This is the Language-Models-as-a-Service (LMaaS) paradigm.
Contrasting with scenarios where full model access is available, as in the case
of open-source models, such closed-off language models create specific
challenges for evaluating, benchmarking, and testing them. This paper has two
goals: on the one hand, we delineate how the aforementioned challenges act as
impediments to the accessibility, replicability, reliability, and
trustworthiness (ARRT) of LMaaS. We systematically examine the issues that
arise from a lack of information about language models for each of these four
aspects. We shed light on current solutions, provide some recommendations, and
highlight the directions for future advancements. On the other hand, it serves
as a one-stop-shop for the extant knowledge about current, major LMaaS,
offering a synthesized overview of the licences and capabilities their
interfaces offer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16575">A Benchmark for Learning to Translate a New Language from One Grammar Book. (arXiv:2309.16575v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tanzer_G/0/1/0/all/0/1">Garrett Tanzer</a>, <a href="http://arxiv.org/find/cs/1/au:+Suzgun_M/0/1/0/all/0/1">Mirac Suzgun</a>, <a href="http://arxiv.org/find/cs/1/au:+Visser_E/0/1/0/all/0/1">Eline Visser</a>, <a href="http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1">Dan Jurafsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Melas_Kyriazi_L/0/1/0/all/0/1">Luke Melas-Kyriazi</a></p>
<p>Large language models (LLMs) can perform impressive feats with in-context
learning or lightweight finetuning. It is natural to wonder how well these
models adapt to genuinely new tasks, but how does one find tasks that are
unseen in internet-scale training sets? We turn to a field that is explicitly
motivated and bottlenecked by a scarcity of web data: low-resource languages.
In this paper, we introduce MTOB (Machine Translation from One Book), a
benchmark for learning to translate between English and Kalamang -- a language
with less than 200 speakers and therefore virtually no presence on the web --
using several hundred pages of field linguistics reference materials. This task
framing is novel in that it asks a model to learn a language from a single
human-readable book of grammar explanations, rather than a large mined corpus
of in-domain data, more akin to L2 learning than L1 acquisition. We demonstrate
that baselines using current LLMs are promising but fall short of human
performance, achieving 44.7 chrF on Kalamang to English translation and 45.8
chrF on English to Kalamang translation, compared to 51.6 and 57.0 chrF by a
human who learned Kalamang from the same reference materials. We hope that MTOB
will help measure LLM capabilities along a new dimension, and that the methods
developed to solve it could help expand access to language technology for
underserved communities by leveraging qualitatively different kinds of data
than traditional machine translation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16583">GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards GPT-4 and Beyond. (arXiv:2309.16583v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1">Shen Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yijie Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xi_C/0/1/0/all/0/1">Chenguang Xi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1">Pengyang Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xun Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1">Kevin Chen-Chuan Chang</a></p>
<p>With the rapid advancement of large language models (LLMs), there is a
pressing need for a comprehensive evaluation suite to assess their capabilities
and limitations. Existing LLM leaderboards often reference scores reported in
other papers without consistent settings and prompts, which may inadvertently
encourage cherry-picking favored settings and prompts for better results. In
this work, we introduce GPT-Fathom, an open-source and reproducible LLM
evaluation suite built on top of OpenAI Evals. We systematically evaluate 10+
leading LLMs as well as OpenAI's legacy models on 20+ curated benchmarks across
7 capability categories, all under aligned settings. Our retrospective study on
OpenAI's earlier models offers valuable insights into the evolutionary path
from GPT-3 to GPT-4. Currently, the community is eager to know how GPT-3
progressively improves to GPT-4, including technical details like whether
adding code data improves LLM's reasoning capability, which aspects of LLM
capability can be improved by SFT and RLHF, how much is the alignment tax, etc.
Our analysis sheds light on many of these questions, aiming to improve the
transparency of advanced LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16599">Unlikelihood Tuning on Negative Samples Amazingly Improves Zero-Shot Translation. (arXiv:2309.16599v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zan_C/0/1/0/all/0/1">Changtong Zan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1">Liang Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1">Li Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1">Yibin Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1">Yibing Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Weifeng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1">Dacheng Tao</a></p>
<p>Zero-shot translation (ZST), which is generally based on a multilingual
neural machine translation model, aims to translate between unseen language
pairs in training data. The common practice to guide the zero-shot language
mapping during inference is to deliberately insert the source and target
language IDs, e.g., &lt;EN&gt; for English and &lt;DE&gt; for German. Recent studies have
shown that language IDs sometimes fail to navigate the ZST task, making them
suffer from the off-target problem (non-target language words exist in the
generated translation) and, therefore, difficult to apply the current
multilingual translation model to a broad range of zero-shot language
scenarios. To understand when and why the navigation capabilities of language
IDs are weakened, we compare two extreme decoder input cases in the ZST
directions: Off-Target (OFF) and On-Target (ON) cases. By contrastively
visualizing the contextual word representations (CWRs) of these cases with
teacher forcing, we show that 1) the CWRs of different languages are
effectively distributed in separate regions when the sentence and ID are
matched (ON setting), and 2) if the sentence and ID are unmatched (OFF
setting), the CWRs of different languages are chaotically distributed. Our
analyses suggest that although they work well in ideal ON settings, language
IDs become fragile and lose their navigation ability when faced with off-target
tokens, which commonly exist during inference but are rare in training
scenarios. In response, we employ unlikelihood tuning on the negative (OFF)
samples to minimize their probability such that the language IDs can
discriminate between the on- and off-target tokens during training. Experiments
spanning 40 ZST directions show that our method reduces the off-target ratio by
-48.0% on average, leading to a +9.1 BLEU improvement with only an extra +0.3%
tuning cost.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16609">Qwen Technical Report. (arXiv:2309.16609v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1">Jinze Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1">Shuai Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_Y/0/1/0/all/0/1">Yunfei Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1">Zeyu Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Dang_K/0/1/0/all/0/1">Kai Dang</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1">Xiaodong Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1">Yang Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1">Wenbin Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1">Yu Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1">Fei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hui_B/0/1/0/all/0/1">Binyuan Hui</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1">Luo Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Mei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Junyang Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_R/0/1/0/all/0/1">Runji Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1">Dayiheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1">Gao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1">Chengqiang Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1">Keming Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1">Jianxin Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Men_R/0/1/0/all/0/1">Rui Men</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1">Xingzhang Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1">Xuancheng Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1">Chuanqi Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1">Sinan Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_J/0/1/0/all/0/1">Jianhong Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Peng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shijie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Shengguang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1">Benfeng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1">An Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Hao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jian Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Shusheng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1">Yang Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1">Bowen Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1">Hongyi Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1">Zheng Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jianwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xingxuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yichang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhenru Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1">Chang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jingren Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xiaohuan Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1">Tianhang Zhu</a></p>
<p>Large language models (LLMs) have revolutionized the field of artificial
intelligence, enabling natural language processing tasks that were previously
thought to be exclusive to humans. In this work, we introduce Qwen, the first
installment of our large language model series. Qwen is a comprehensive
language model series that encompasses distinct models with varying parameter
counts. It includes Qwen, the base pretrained language models, and Qwen-Chat,
the chat models finetuned with human alignment techniques. The base language
models consistently demonstrate superior performance across a multitude of
downstream tasks, and the chat models, particularly those trained using
Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The
chat models possess advanced tool-use and planning capabilities for creating
agent applications, showcasing impressive performance even when compared to
bigger models on complex tasks like utilizing a code interpreter. Furthermore,
we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as
well as mathematics-focused models, Math-Qwen-Chat, which are built upon base
language models. These models demonstrate significantly improved performance in
comparison with open-source models, and slightly fall behind the proprietary
models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16621">Stress Testing Chain-of-Thought Prompting for Large Language Models. (arXiv:2309.16621v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1">Aayush Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Thakkar_K/0/1/0/all/0/1">Karan Thakkar</a></p>
<p>This report examines the effectiveness of Chain-of-Thought (CoT) prompting in
improving the multi-step reasoning abilities of large language models (LLMs).
Inspired by previous studies \cite{Min2022RethinkingWork}, we analyze the
impact of three types of CoT prompt perturbations, namely CoT order, CoT
values, and CoT operators on the performance of GPT-3 on various tasks. Our
findings show that incorrect CoT prompting leads to poor performance on
accuracy metrics. Correct values in the CoT is crucial for predicting correct
answers. Moreover, incorrect demonstrations, where the CoT operators or the CoT
order are wrong, do not affect the performance as drastically when compared to
the value based perturbations. This research deepens our understanding of CoT
prompting and opens some new questions regarding the capability of LLMs to
learn reasoning in context.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16639">MindShift: Leveraging Large Language Models for Mental-States-Based Problematic Smartphone Use Intervention. (arXiv:2309.16639v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1">Ruolan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1">Chun Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1">Xiaole Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yujia Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Ningning Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1">Yue Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuhan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Zhi Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Li Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1">Qiaolei Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xuhai Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yuanchun Shi</a></p>
<p>Problematic smartphone use negatively affects physical and mental health.
Despite the wide range of prior research, existing persuasive techniques are
not flexible enough to provide dynamic persuasion content based on users'
physical contexts and mental states. We first conduct a Wizard-of-Oz study
(N=12) and an interview study (N=10) to summarize the mental states behind
problematic smartphone use: boredom, stress, and inertia. This informs our
design of four persuasion strategies: understanding, comforting, evoking, and
scaffolding habits. We leverage large language models (LLMs) to enable the
automatic and dynamic generation of effective persuasion content. We develop
MindShift, a novel LLM-powered problematic smartphone use intervention
technique. MindShift takes users' in-the-moment physical contexts, mental
states, app usage behaviors, users' goals &amp; habits as input, and generates
high-quality and flexible persuasive content with appropriate persuasion
strategies. We conduct a 5-week field experiment (N=25) to compare MindShift
with baseline techniques. The results show that MindShift significantly
improves intervention acceptance rates by 17.8-22.5% and reduces smartphone use
frequency by 12.1-14.4%. Moreover, users have a significant drop in smartphone
addiction scale scores and a rise in self-efficacy. Our study sheds light on
the potential of leveraging LLMs for context-aware persuasion in other behavior
change domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16671">Demystifying CLIP Data. (arXiv:2309.16671v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hu Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1">Saining Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1">Xiaoqing Ellen Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1">Po-Yao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Howes_R/0/1/0/all/0/1">Russell Howes</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_V/0/1/0/all/0/1">Vasu Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shang-Wen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghosh_G/0/1/0/all/0/1">Gargi Ghosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1">Luke Zettlemoyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Feichtenhofer_C/0/1/0/all/0/1">Christoph Feichtenhofer</a></p>
<p>Contrastive Language-Image Pre-training (CLIP) is an approach that has
advanced research and applications in computer vision, fueling modern
recognition systems and generative models. We believe that the main ingredient
to the success of CLIP is its data and not the model architecture or
pre-training objective. However, CLIP only provides very limited information
about its data and how it has been collected, leading to works that aim to
reproduce CLIP's data by filtering with its model parameters. In this work, we
intend to reveal CLIP's data curation approach and in our pursuit of making it
open to the community introduce Metadata-Curated Language-Image Pre-training
(MetaCLIP). MetaCLIP takes a raw data pool and metadata (derived from CLIP's
concepts) and yields a balanced subset over the metadata distribution. Our
experimental study rigorously isolates the model and training settings,
concentrating solely on data. MetaCLIP applied to CommonCrawl with 400M
image-text data pairs outperforms CLIP's data on multiple standard benchmarks.
In zero-shot ImageNet classification, MetaCLIP achieves 70.8% accuracy,
surpassing CLIP's 68.3% on ViT-B models. Scaling to 1B data, while maintaining
the same training budget, attains 72.4%. Our observations hold across various
model sizes, exemplified by ViT-H achieving 80.5%, without any
bells-and-whistles. Curation code and training data distribution on metadata is
made available at https://github.com/facebookresearch/MetaCLIP.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2106.00749">Higher-order Derivatives of Weighted Finite-state Machines. (arXiv:2106.00749v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zmigrod_R/0/1/0/all/0/1">Ran Zmigrod</a>, <a href="http://arxiv.org/find/cs/1/au:+Vieira_T/0/1/0/all/0/1">Tim Vieira</a>, <a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1">Ryan Cotterell</a></p>
<p>Weighted finite-state machines are a fundamental building block of NLP
systems. They have withstood the test of time -- from their early use in noisy
channel models in the 1990s up to modern-day neurally parameterized conditional
random fields. This work examines the computation of higher-order derivatives
with respect to the normalization constant for weighted finite-state machines.
We provide a general algorithm for evaluating derivatives of all orders, which
has not been previously described in the literature. In the case of
second-order derivatives, our scheme runs in the optimal $\mathcal{O}(A^2 N^4)$
time where $A$ is the alphabet size and $N$ is the number of states. Our
algorithm is significantly faster than prior algorithms. Additionally, our
approach leads to a significantly faster algorithm for computing second-order
expectations, such as covariance matrices and gradients of first-order
expectations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.07836">Personal Entity, Concept, and Named Entity Linking in Conversations. (arXiv:2206.07836v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Joko_H/0/1/0/all/0/1">Hideaki Joko</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasibi_F/0/1/0/all/0/1">Faegheh Hasibi</a></p>
<p>Building conversational agents that can have natural and knowledge-grounded
interactions with humans requires understanding user utterances. Entity Linking
(EL) is an effective and widely used method for understanding natural language
text and connecting it to external knowledge. It is, however, shown that
existing EL methods developed for annotating documents are suboptimal for
conversations, where personal entities (e.g., "my cars") and concepts are
essential for understanding user utterances. In this paper, we introduce a
collection and a tool for entity linking in conversations. We collect EL
annotations for 1327 conversational utterances, consisting of links to named
entities, concepts, and personal entities. The dataset is used for training our
toolkit for conversational entity linking, CREL. Unlike existing EL methods,
CREL is developed to identify both named entities and concepts. It also
utilizes coreference resolution techniques to identify personal entities and
references to the explicit entity mentions in the conversations. We compare
CREL with state-of-the-art techniques and show that it outperforms all existing
baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.00407">On the Role of Morphological Information for Contextual Lemmatization. (arXiv:2302.00407v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Toporkov_O/0/1/0/all/0/1">Olia Toporkov</a>, <a href="http://arxiv.org/find/cs/1/au:+Agerri_R/0/1/0/all/0/1">Rodrigo Agerri</a></p>
<p>Lemmatization is a natural language processing (NLP) task which consists of
producing, from a given inflected word, its canonical form or lemma.
Lemmatization is one of the basic tasks that facilitate downstream NLP
applications, and is of particular importance for high-inflected languages.
Given that the process to obtain a lemma from an inflected word can be
explained by looking at its morphosyntactic category, including fine-grained
morphosyntactic information to train contextual lemmatizers has become common
practice, without considering whether that is the optimum in terms of
downstream performance. In order to address this issue, in this paper we
empirically investigate the role of morphological information to develop
contextual lemmatizers in six languages within a varied spectrum of
morphological complexity: Basque, Turkish, Russian, Czech, Spanish and English.
Furthermore, and unlike the vast majority of previous work, we also evaluate
lemmatizers in out-of-domain settings, which constitutes, after all, their most
common application use. The results of our study are rather surprising. It
turns out that providing lemmatizers with fine-grained morphological features
during training is not that beneficial, not even for agglutinative languages.
In fact, modern contextual word representations seem to implicitly encode
enough morphological information to obtain competitive contextual lemmatizers
without seeing any explicit morphological signal. Moreover, our experiments
suggest that the best lemmatizers out-of-domain are those using simple UPOS
tags or those trained without morphology and, finally, that current evaluation
practices for lemmatization are not adequate to clearly discriminate between
models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.11364">Visualizing Linguistic Diversity of Text Datasets Synthesized by Large Language Models. (arXiv:2305.11364v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Reif_E/0/1/0/all/0/1">Emily Reif</a>, <a href="http://arxiv.org/find/cs/1/au:+Kahng_M/0/1/0/all/0/1">Minsuk Kahng</a>, <a href="http://arxiv.org/find/cs/1/au:+Petridis_S/0/1/0/all/0/1">Savvas Petridis</a></p>
<p>Large language models (LLMs) can be used to generate smaller, more refined
datasets via few-shot prompting for benchmarking, fine-tuning or other use
cases. However, understanding and evaluating these datasets is difficult, and
the failure modes of LLM-generated data are still not well understood.
Specifically, the data can be repetitive in surprising ways, not only
semantically but also syntactically and lexically. We present LinguisticLens, a
novel inter-active visualization tool for making sense of and analyzing
syntactic diversity of LLM-generated datasets. LinguisticLens clusters text
along syntactic, lexical, and semantic axes. It supports hierarchical
visualization of a text dataset, allowing users to quickly scan for an overview
and inspect individual examples. The live demo is available at
shorturl.at/zHOUV.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.11627">LLM-Pruner: On the Structural Pruning of Large Language Models. (arXiv:2305.11627v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xinyin Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_G/0/1/0/all/0/1">Gongfan Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinchao Wang</a></p>
<p>Large language models (LLMs) have shown remarkable capabilities in language
understanding and generation. However, such impressive capability typically
comes with a substantial model size, which presents significant challenges in
both the deployment, inference, and training stages. With LLM being a
general-purpose task solver, we explore its compression in a task-agnostic
manner, which aims to preserve the multi-task solving and language generation
ability of the original LLM. One challenge to achieving this is the enormous
size of the training corpus of LLM, which makes both data transfer and model
post-training over-burdensome. Thus, we tackle the compression of LLMs within
the bound of two constraints: being task-agnostic and minimizing the reliance
on the original training dataset. Our method, named LLM-Pruner, adopts
structural pruning that selectively removes non-critical coupled structures
based on gradient information, maximally preserving the majority of the LLM's
functionality. To this end, the performance of pruned models can be efficiently
recovered through tuning techniques, LoRA, in merely 3 hours, requiring only
50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna,
and ChatGLM, and demonstrate that the compressed models still exhibit
satisfactory capabilities in zero-shot classification and generation. The code
is available at: https://github.com/horseee/LLM-Pruner
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14091">Revisiting Acceptability Judgements. (arXiv:2305.14091v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1">Hai Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Ziyin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1">Weifang Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lai_J/0/1/0/all/0/1">Jackie Yan-Ki Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1">Aini Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Patterson_Y/0/1/0/all/0/1">Yina Patterson</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jiahui Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Peng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1">Chien-Jer Charles Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Rui Wang</a></p>
<p>In this work, we revisit linguistic acceptability in the context of large
language models. We introduce CoLAC - Corpus of Linguistic Acceptability in
Chinese, the first large-scale acceptability dataset for a non-Indo-European
language. It is verified by native speakers and is the first acceptability
dataset that comes with two sets of labels: a linguist label and a crowd label.
Our experiments show that even the largest InstructGPT model performs only at
chance level on CoLAC, while ChatGPT's performance (48.30 MCC) is also much
below supervised models (59.03 MCC) and human (65.11 MCC). Through
cross-lingual transfer experiments and fine-grained linguistic analysis, we
provide detailed analysis of the model predictions and demonstrate for the
first time that knowledge of linguistic acceptability can be transferred across
typologically distinct languages, as well as be traced back to pre-training.
Our dataset is publicly available at
\url{https://github.com/huhailinguist/CoLAC}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.12896">Corrections of Zipf&#x27;s and Heaps&#x27; Laws Derived from Hapax Rate Models. (arXiv:2307.12896v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Debowski_L/0/1/0/all/0/1">&#x141;ukasz D&#x119;bowski</a></p>
<p>The article introduces corrections to Zipf's and Heaps' laws based on
systematic models of the hapax rate. The derivation rests on two assumptions:
The first one is the standard urn model which predicts that marginal frequency
distributions for shorter texts look as if word tokens were sampled blindly
from a given longer text. The second assumption posits that the rate of hapaxes
is a simple function of the text size. Four such functions are discussed: the
constant model, the Davis model, the linear model, and the logistic model. It
is shown that the logistic model yields the best fit.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.06595">VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use. (arXiv:2308.06595v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1">Yonatan Bitton</a>, <a href="http://arxiv.org/find/cs/1/au:+Bansal_H/0/1/0/all/0/1">Hritik Bansal</a>, <a href="http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1">Jack Hessel</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1">Rulin Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1">Wanrong Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Awadalla_A/0/1/0/all/0/1">Anas Awadalla</a>, <a href="http://arxiv.org/find/cs/1/au:+Gardner_J/0/1/0/all/0/1">Josh Gardner</a>, <a href="http://arxiv.org/find/cs/1/au:+Taori_R/0/1/0/all/0/1">Rohan Taori</a>, <a href="http://arxiv.org/find/cs/1/au:+Schimdt_L/0/1/0/all/0/1">Ludwig Schimdt</a></p>
<p>We introduce VisIT-Bench (Visual InsTruction Benchmark), a benchmark for
evaluation of instruction-following vision-language models for real-world use.
Our starting point is curating 70 'instruction families' that we envision
instruction tuned vision-language models should be able to address. Extending
beyond evaluations like VQAv2 and COCO, tasks range from basic recognition to
game playing and creative generation. Following curation, our dataset comprises
592 test queries, each with a human-authored instruction-conditioned caption.
These descriptions surface instruction-specific factors, e.g., for an
instruction asking about the accessibility of a storefront for wheelchair
users, the instruction-conditioned caption describes ramps/potential obstacles.
These descriptions enable 1) collecting human-verified reference outputs for
each instance; and 2) automatic evaluation of candidate multimodal generations
using a text-only LLM, aligning with human judgment. We quantify quality gaps
between models and references using both human and automatic evaluations; e.g.,
the top-performing instruction-following model wins against the GPT-4 reference
in just 27% of the comparison. VisIT-Bench is dynamic to participate,
practitioners simply submit their model's response on the project website;
Data, code and leaderboard is available at visit-bench.github.io.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.10379">Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models. (arXiv:2308.10379v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sel_B/0/1/0/all/0/1">Bilgehan Sel</a>, <a href="http://arxiv.org/find/cs/1/au:+Al_Tawaha_A/0/1/0/all/0/1">Ahmad Al-Tawaha</a>, <a href="http://arxiv.org/find/cs/1/au:+Khattar_V/0/1/0/all/0/1">Vanshaj Khattar</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1">Ruoxi Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1">Ming Jin</a></p>
<p>Current literature, aiming to surpass the "Chain-of-Thought" approach, often
resorts to an external modus operandi involving halting, modifying, and then
resuming the generation process to boost Large Language Models' (LLMs)
reasoning capacities. This mode escalates the number of query requests, leading
to increased costs, memory, and computational overheads. Addressing this, we
propose the Algorithm of Thoughts -- a novel strategy that propels LLMs through
algorithmic reasoning pathways, pioneering a new mode of in-context learning.
By employing algorithmic examples, we exploit the innate recurrence dynamics of
LLMs, expanding their idea exploration with merely one or a few queries. Our
technique outperforms earlier single-query methods and stands on par with a
recent multi-query strategy that employs an extensive tree search algorithm.
Intriguingly, our results suggest that instructing an LLM using an algorithm
can lead to performance surpassing that of the algorithm itself, hinting at
LLM's inherent ability to weave its intuition into optimized searches. We probe
into the underpinnings of our method's efficacy and its nuances in application.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.14359">Effect of Attention and Self-Supervised Speech Embeddings on Non-Semantic Speech Tasks. (arXiv:2308.14359v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mohapatra_P/0/1/0/all/0/1">Payal Mohapatra</a>, <a href="http://arxiv.org/find/cs/1/au:+Pandey_A/0/1/0/all/0/1">Akash Pandey</a>, <a href="http://arxiv.org/find/cs/1/au:+Sui_Y/0/1/0/all/0/1">Yueyuan Sui</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1">Qi Zhu</a></p>
<p>Human emotion understanding is pivotal in making conversational technology
mainstream. We view speech emotion understanding as a perception task which is
a more realistic setting. With varying contexts (languages, demographics, etc.)
different share of people perceive the same speech segment as a non-unanimous
emotion. As part of the ACM Multimedia 2023 Computational Paralinguistics
ChallengE (ComParE) in the EMotion Share track, we leverage their rich dataset
of multilingual speakers and multi-label regression target of 'emotion share'
or perception of that emotion. We demonstrate that the training scheme of
different foundation models dictates their effectiveness for tasks beyond
speech recognition, especially for non-semantic speech tasks like emotion
understanding. This is a very complex task due to multilingual speakers,
variability in the target labels, and inherent imbalance in the regression
dataset. Our results show that HuBERT-Large with a self-attention-based
light-weight sequence model provides 4.6% improvement over the reported
baseline.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.05516">Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs. (arXiv:2309.05516v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1">Wenhua Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Weiwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1">Haihao Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1">Yiyang Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xin He</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_K/0/1/0/all/0/1">Kaokao Lv</a></p>
<p>Large Language Models (LLMs) have proven their exceptional capabilities in
performing language-related tasks. However, their deployment poses significant
challenges due to their considerable memory and storage requirements. In
response to this issue, weight-only quantization, particularly 3 and 4-bit
weight-only quantization, has emerged as one of the most viable solutions. As
the number of bits decreases, the quantization grid broadens, thus emphasizing
the importance of up and down rounding. While previous studies have
demonstrated that fine-tuning up and down rounding with the addition of
perturbations can enhance accuracy in some scenarios, our study is driven by
the precise and limited boundary of these perturbations, where only the
threshold for altering the rounding value is of significance. Consequently, we
propose a concise and highly effective approach for optimizing the weight
rounding task. Our method, named SignRound, involves lightweight block-wise
tuning using signed gradient descent, enabling us to achieve outstanding
results within 400 steps. SignRound competes impressively against recent
methods without introducing additional inference overhead. The source code will
be publicly available at \url{https://github.com/intel/neural-compressor} soon.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.10003">A novel approach to measuring patent claim scope based on probabilities obtained from (large) language models. (arXiv:2309.10003v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ragot_S/0/1/0/all/0/1">S&#xe9;bastien Ragot</a></p>
<p>This work proposes to measure the scope of a patent claim as the reciprocal
of the self-information contained in this claim. A probability of occurrence of
the claim is obtained from a language model and this probability is used to
compute the self-information. Grounded in information theory, this approach is
based on the assumption that an unlikely concept is more informative than a
usual concept, insofar as it is more surprising. In turn, the more surprising
the information required to defined the claim, the narrower its scope. Five
language models are considered, ranging from simplest models (each word or
character is assigned an identical probability) to intermediate models (using
average word or character frequencies), to a large language model (GPT2).
Interestingly, the scope resulting from the simplest language models is
proportional to the reciprocal of the number of words or characters involved in
the claim, a metric already used in previous works. Application is made to
multiple series of patent claims directed to distinct inventions, where each
series consists of claims devised to have a gradually decreasing scope. The
performance of the language models is assessed with respect to several ad hoc
tests. The more sophisticated the model, the better the results. I.e., the GPT2
probability model outperforms models based on word and character frequencies,
which themselves outdo the simplest models based on word or character counts.
Still, the character count appears to be a more reliable indicator than the
word count.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.15317">Joint Prediction and Denoising for Large-scale Multilingual Self-supervised Learning. (arXiv:2309.15317v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">William Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1">Jiatong Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1">Brian Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Berrebbi_D/0/1/0/all/0/1">Dan Berrebbi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wangyou Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1">Yifan Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1">Xuankai Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Maiti_S/0/1/0/all/0/1">Soumi Maiti</a>, <a href="http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1">Shinji Watanabe</a></p>
<p>Multilingual self-supervised learning (SSL) has often lagged behind
state-of-the-art (SOTA) methods due to the expenses and complexity required to
handle many languages. This further harms the reproducibility of SSL, which is
already limited to few research groups due to its resource usage. We show that
more powerful techniques can actually lead to more efficient pre-training,
opening SSL to more research groups. We propose WavLabLM, which extends WavLM's
joint prediction and denoising to 40k hours of data across 136 languages. To
build WavLabLM, we devise a novel multi-stage pre-training method, designed to
address the language imbalance of multilingual data. WavLabLM achieves
comparable performance to XLS-R on ML-SUPERB with less than 10% of the training
data, making SSL realizable with academic compute. We show that further
efficiency can be achieved with a vanilla HuBERT Base model, which can maintain
94% of XLS-R's performance with only 3% of the data, 4 GPUs, and limited
trials. We open-source all code and models in ESPnet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.15564">Jointly Training Large Autoregressive Multimodal Models. (arXiv:2309.15564v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aiello_E/0/1/0/all/0/1">Emanuele Aiello</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1">Lili Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1">Yixin Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Aghajanyan_A/0/1/0/all/0/1">Armen Aghajanyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1">Barlas Oguz</a></p>
<p>In recent years, advances in the large-scale pretraining of language and
text-to-image models have revolutionized the field of machine learning. Yet,
integrating these two modalities into a single, robust model capable of
generating seamless multimodal outputs remains a significant challenge. To
address this gap, we present the Joint Autoregressive Mixture (JAM) framework,
a modular approach that systematically fuses existing text and image generation
models. We also introduce a specialized, data-efficient instruction-tuning
strategy, tailored for mixed-modal generation tasks. Our final instruct-tuned
model demonstrates unparalleled performance in generating high-quality
multimodal outputs and represents the first model explicitly designed for this
purpose.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12481">HANS, are you clever? Clever Hans Effect Analysis of Neural Systems. (arXiv:2309.12481v1 [cs.CL] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ranaldi_L/0/1/0/all/0/1">Leonardo Ranaldi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zanzotto_F/0/1/0/all/0/1">Fabio Massimo Zanzotto</a></p>
<p>Instruction-tuned Large Language Models (It-LLMs) have been exhibiting
outstanding abilities to reason around cognitive states, intentions, and
reactions of all people involved, letting humans guide and comprehend
day-to-day social interactions effectively. In fact, several multiple-choice
questions (MCQ) benchmarks have been proposed to construct solid assessments of
the models' abilities. However, earlier works are demonstrating the presence of
inherent "order bias" in It-LLMs, posing challenges to the appropriate
evaluation. In this paper, we investigate It-LLMs' resilience abilities towards
a series of probing tests using four MCQ benchmarks. Introducing adversarial
examples, we show a significant performance gap, mainly when varying the order
of the choices, which reveals a selection bias and brings into discussion
reasoning abilities. Following a correlation between first positions and model
choices due to positional bias, we hypothesized the presence of structural
heuristics in the decision-making process of the It-LLMs, strengthened by
including significant examples in few-shot scenarios. Finally, by using the
Chain-of-Thought (CoT) technique, we elicit the model to reason and mitigate
the bias by obtaining more robust models.
</p>
</p>
</div>

    </div>
    </body>
    