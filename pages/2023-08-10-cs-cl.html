<!DOCTYPE html>
<html>
<head>
<title>2023-08-10-cs-cl</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2308.04444">Changes in Policy Preferences in German Tweets during the COVID Pandemic. (arXiv:2308.04444v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Biessmann_F/0/1/0/all/0/1">Felix Biessmann</a></p>
<p>Online social media have become an important forum for exchanging political
opinions. In response to COVID measures citizens expressed their policy
preferences directly on these platforms. Quantifying political preferences in
online social media remains challenging: The vast amount of content requires
scalable automated extraction of political preferences -- however fine grained
political preference extraction is difficult with current machine learning (ML)
technology, due to the lack of data sets. Here we present a novel data set of
tweets with fine grained political preference annotations. A text
classification model trained on this data is used to extract policy preferences
in a German Twitter corpus ranging from 2019 to 2022. Our results indicate that
in response to the COVID pandemic, expression of political opinions increased.
Using a well established taxonomy of policy preferences we analyse fine grained
political views and highlight changes in distinct political categories. These
analyses suggest that the increase in policy preference expression is dominated
by the categories pro-welfare, pro-education and pro-governmental
administration efficiency. All training data and code used in this study are
made publicly available to encourage other researchers to further improve
automated policy preference extraction methods. We hope that our findings
contribute to a better understanding of political statements in online social
media and to a better assessment of how COVID measures impact political
preferences.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04498">DialogRE^C+: An Extension of DialogRE to Investigate How Much Coreference Helps Relation Extraction in Dialogs. (arXiv:2308.04498v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1">Yiyun Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_M/0/1/0/all/0/1">Mengwei Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1">Fei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Fei_H/0/1/0/all/0/1">Hao Fei</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bobo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Shengqiong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_D/0/1/0/all/0/1">Donghong Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Teng_C/0/1/0/all/0/1">Chong Teng</a></p>
<p>Dialogue relation extraction (DRE) that identifies the relations between
argument pairs in dialogue text, suffers much from the frequent occurrence of
personal pronouns, or entity and speaker coreference. This work introduces a
new benchmark dataset DialogRE^C+, introducing coreference resolution into the
DRE scenario. With the aid of high-quality coreference knowledge, the reasoning
of argument relations is expected to be enhanced. In DialogRE^C+ dataset, we
manually annotate total 5,068 coreference chains over 36,369 argument mentions
based on the existing DialogRE data, where four different coreference chain
types namely speaker chain, person chain, location chain and organization chain
are explicitly marked. We further develop 4 coreference-enhanced graph-based
DRE models, which learn effective coreference representations for improving the
DRE task. We also train a coreference resolution model based on our annotations
and evaluate the effect of automatically extracted coreference chains
demonstrating the practicality of our dataset and its potential to other
domains and tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04502">Revisiting Disentanglement and Fusion on Modality and Context in Conversational Multimodal Emotion Recognition. (arXiv:2308.04502v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bobo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Fei_H/0/1/0/all/0/1">Hao Fei</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_L/0/1/0/all/0/1">Lizi Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Teng_C/0/1/0/all/0/1">Chong Teng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1">Tat-Seng Chua</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_D/0/1/0/all/0/1">Donghong Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1">Fei Li</a></p>
<p>It has been a hot research topic to enable machines to understand human
emotions in multimodal contexts under dialogue scenarios, which is tasked with
multimodal emotion analysis in conversation (MM-ERC). MM-ERC has received
consistent attention in recent years, where a diverse range of methods has been
proposed for securing better task performance. Most existing works treat MM-ERC
as a standard multimodal classification problem and perform multimodal feature
disentanglement and fusion for maximizing feature utility. Yet after revisiting
the characteristic of MM-ERC, we argue that both the feature multimodality and
conversational contextualization should be properly modeled simultaneously
during the feature disentanglement and fusion steps. In this work, we target
further pushing the task performance by taking full consideration of the above
insights. On the one hand, during feature disentanglement, based on the
contrastive learning technique, we devise a Dual-level Disentanglement
Mechanism (DDM) to decouple the features into both the modality space and
utterance space. On the other hand, during the feature fusion stage, we propose
a Contribution-aware Fusion Mechanism (CFM) and a Context Refusion Mechanism
(CRM) for multimodal and context integration, respectively. They together
schedule the proper integrations of multimodal and context features.
Specifically, CFM explicitly manages the multimodal feature contributions
dynamically, while CRM flexibly coordinates the introduction of dialogue
contexts. On two public MM-ERC datasets, our system achieves new
state-of-the-art performance consistently. Further analyses demonstrate that
all our proposed mechanisms greatly facilitate the MM-ERC task by making full
use of the multimodal and context features adaptively. Note that our proposed
methods have the great potential to facilitate a broader range of other
conversational multimodal tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04517">Capturing Spectral and Long-term Contextual Information for Speech Emotion Recognition Using Deep Learning Techniques. (arXiv:2308.04517v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Islam_S/0/1/0/all/0/1">Samiul Islam</a>, <a href="http://arxiv.org/find/cs/1/au:+Haque_M/0/1/0/all/0/1">Md. Maksudul Haque</a>, <a href="http://arxiv.org/find/cs/1/au:+Sadat_A/0/1/0/all/0/1">Abu Jobayer Md. Sadat</a></p>
<p>Traditional approaches in speech emotion recognition, such as LSTM, CNN, RNN,
SVM, and MLP, have limitations such as difficulty capturing long-term
dependencies in sequential data, capturing the temporal dynamics, and
struggling to capture complex patterns and relationships in multimodal data.
This research addresses these shortcomings by proposing an ensemble model that
combines Graph Convolutional Networks (GCN) for processing textual data and the
HuBERT transformer for analyzing audio signals. We found that GCNs excel at
capturing Long-term contextual dependencies and relationships within textual
data by leveraging graph-based representations of text and thus detecting the
contextual meaning and semantic relationships between words. On the other hand,
HuBERT utilizes self-attention mechanisms to capture long-range dependencies,
enabling the modeling of temporal dynamics present in speech and capturing
subtle nuances and variations that contribute to emotion recognition. By
combining GCN and HuBERT, our ensemble model can leverage the strengths of both
approaches. This allows for the simultaneous analysis of multimodal data, and
the fusion of these modalities enables the extraction of complementary
information, enhancing the discriminative power of the emotion recognition
system. The results indicate that the combined model can overcome the
limitations of traditional methods, leading to enhanced accuracy in recognizing
emotions from speech.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04519">DisCoCat for Donkey Sentences. (arXiv:2308.04519v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+McPheat_L/0/1/0/all/0/1">Lachlan McPheat</a> (University College London), <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Daphne Wang</a> (University College London)</p>
<p>We demonstrate how to parse Geach's Donkey sentences in a compositional
distributional model of meaning. We build on previous work on the DisCoCat
(Distributional Compositional Categorical) framework, including extensions that
model discourse, determiners, and relative pronouns. We present a type-logical
syntax for parsing donkey sentences, for which we define both relational and
vector space semantics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04524">Who should I Collaborate with? A Comparative Study of Academia and Industry Research Collaboration in NLP. (arXiv:2308.04524v1 [cs.DL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abuwala_H/0/1/0/all/0/1">Hussain Sadiq Abuwala</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Bohan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Mushi Wang</a></p>
<p>The goal of our research was to investigate the effects of collaboration
between academia and industry on Natural Language Processing (NLP). To do this,
we created a pipeline to extract affiliations and citations from NLP papers and
divided them into three categories: academia, industry, and hybrid
(collaborations between academia and industry). Our empirical analysis found
that there is a trend towards an increase in industry and academia-industry
collaboration publications and that these types of publications tend to have a
higher impact compared to those produced solely within academia.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04534">Ahead of the Text: Leveraging Entity Preposition for Financial Relation Extraction. (arXiv:2308.04534v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pasch_S/0/1/0/all/0/1">Stefan Pasch</a>, <a href="http://arxiv.org/find/cs/1/au:+Petridis_D/0/1/0/all/0/1">Dimitrios Petridis</a></p>
<p>In the context of the ACM KDF-SIGIR 2023 competition, we undertook an entity
relation task on a dataset of financial entity relations called REFind. Our
top-performing solution involved a multi-step approach. Initially, we inserted
the provided entities at their corresponding locations within the text.
Subsequently, we fine-tuned the transformer-based language model roberta-large
for text classification by utilizing a labeled training set to predict the
entity relations. Lastly, we implemented a post-processing phase to identify
and handle improbable predictions generated by the model. As a result of our
methodology, we achieved the 1st place ranking on the competition's public
leaderboard.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04566">Single-Sentence Reader: A Novel Approach for Addressing Answer Position Bias. (arXiv:2308.04566v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tran_S/0/1/0/all/0/1">Son Quoc Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Kretchmar_M/0/1/0/all/0/1">Matt Kretchmar</a></p>
<p>Machine Reading Comprehension (MRC) models tend to take advantage of spurious
correlations (also known as dataset bias or annotation artifacts in the
research community). Consequently, these models may perform the MRC task
without fully comprehending the given context and question, which is
undesirable since it may result in low robustness against distribution shift.
This paper delves into the concept of answer-position bias, where a significant
percentage of training questions have answers located solely in the first
sentence of the context. We propose a Single-Sentence Reader as a new approach
for addressing answer position bias in MRC. We implement this approach using
six different models and thoroughly analyze their performance. Remarkably, our
proposed Single-Sentence Readers achieve results that nearly match those of
models trained on conventional training sets, proving their effectiveness. Our
study also discusses several challenges our Single-Sentence Readers encounter
and proposes a potential solution.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04592">Shepherd: A Critic for Language Model Generation. (arXiv:2308.04592v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tianlu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1">Ping Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1">Xiaoqing Ellen Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+OBrien_S/0/1/0/all/0/1">Sean O&#x27;Brien</a>, <a href="http://arxiv.org/find/cs/1/au:+Pasunuru_R/0/1/0/all/0/1">Ramakanth Pasunuru</a>, <a href="http://arxiv.org/find/cs/1/au:+Dwivedi_Yu_J/0/1/0/all/0/1">Jane Dwivedi-Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Golovneva_O/0/1/0/all/0/1">Olga Golovneva</a>, <a href="http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1">Luke Zettlemoyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Fazel_Zarandi_M/0/1/0/all/0/1">Maryam Fazel-Zarandi</a>, <a href="http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1">Asli Celikyilmaz</a></p>
<p>As large language models improve, there is increasing interest in techniques
that leverage these models' capabilities to refine their own outputs. In this
work, we introduce Shepherd, a language model specifically tuned to critique
responses and suggest refinements, extending beyond the capabilities of an
untuned model to identify diverse errors and provide suggestions to remedy
them. At the core of our approach is a high quality feedback dataset, which we
curate from community feedback and human annotations. Even though Shepherd is
small (7B parameters), its critiques are either equivalent or preferred to
those from established models including ChatGPT. Using GPT-4 for evaluation,
Shepherd reaches an average win-rate of 53-87% compared to competitive
alternatives. In human evaluation, Shepherd strictly outperforms other models
and on average closely ties with ChatGPT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04623">Accelerating LLM Inference with Staged Speculative Decoding. (arXiv:2308.04623v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Spector_B/0/1/0/all/0/1">Benjamin Spector</a>, <a href="http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1">Chris Re</a></p>
<p>Recent advances with large language models (LLM) illustrate their diverse
capabilities. We propose a novel algorithm, staged speculative decoding, to
accelerate LLM inference in small-batch, on-device scenarios. We address the
low arithmetic intensity of small-batch inference by improving upon previous
work in speculative decoding. First, we restructure the speculative batch as a
tree, which reduces generation costs and increases the expected tokens per
batch. Second, we add a second stage of speculative decoding. Taken together,
we reduce single-batch decoding latency by 3.16x with a 762M parameter GPT-2-L
model while perfectly preserving output quality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04624">Benchmarking LLM powered Chatbots: Methods and Metrics. (arXiv:2308.04624v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Banerjee_D/0/1/0/all/0/1">Debarag Banerjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1">Pooja Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Avadhanam_A/0/1/0/all/0/1">Arjun Avadhanam</a>, <a href="http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1">Saksham Srivastava</a></p>
<p>Autonomous conversational agents, i.e. chatbots, are becoming an increasingly
common mechanism for enterprises to provide support to customers and partners.
In order to rate chatbots, especially ones powered by Generative AI tools like
Large Language Models (LLMs) we need to be able to accurately assess their
performance. This is where chatbot benchmarking becomes important. In this
paper, we propose the use of a novel benchmark that we call the E2E (End to
End) benchmark, and show how the E2E benchmark can be used to evaluate accuracy
and usefulness of the answers provided by chatbots, especially ones powered by
LLMs. We evaluate an example chatbot at different levels of sophistication
based on both our E2E benchmark, as well as other available metrics commonly
used in the state of art, and observe that the proposed benchmark show better
results compared to others. In addition, while some metrics proved to be
unpredictable, the metric associated with the E2E benchmark, which uses cosine
similarity performed well in evaluating chatbots. The performance of our best
models shows that there are several benefits of using the cosine similarity
score as a metric in the E2E benchmark.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04625">A Comparative Study of Sentence Embedding Models for Assessing Semantic Variation. (arXiv:2308.04625v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mistry_D/0/1/0/all/0/1">Deven M. Mistry</a>, <a href="http://arxiv.org/find/cs/1/au:+Minai_A/0/1/0/all/0/1">Ali A. Minai</a></p>
<p>Analyzing the pattern of semantic variation in long real-world texts such as
books or transcripts is interesting from the stylistic, cognitive, and
linguistic perspectives. It is also useful for applications such as text
segmentation, document summarization, and detection of semantic novelty. The
recent emergence of several vector-space methods for sentence embedding has
made such analysis feasible. However, this raises the issue of how consistent
and meaningful the semantic representations produced by various methods are in
themselves. In this paper, we compare several recent sentence embedding methods
via time-series of semantic similarity between successive sentences and
matrices of pairwise sentence similarity for multiple books of literature. In
contrast to previous work using target tasks and curated datasets to compare
sentence embedding methods, our approach provides an evaluation of the methods
'in the wild'. We find that most of the sentence embedding methods considered
do infer highly correlated patterns of semantic similarity in a given document,
but show interesting differences.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04645">Cross-Lingual Constituency Parsing for Middle High German: A Delexicalized Approach. (arXiv:2308.04645v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nie_E/0/1/0/all/0/1">Ercong Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmid_H/0/1/0/all/0/1">Helmut Schmid</a>, <a href="http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1">Hinrich Sch&#xfc;tze</a></p>
<p>Constituency parsing plays a fundamental role in advancing natural language
processing (NLP) tasks. However, training an automatic syntactic analysis
system for ancient languages solely relying on annotated parse data is a
formidable task due to the inherent challenges in building treebanks for such
languages. It demands extensive linguistic expertise, leading to a scarcity of
available resources. To overcome this hurdle, cross-lingual transfer techniques
which require minimal or even no annotated data for low-resource target
languages offer a promising solution. In this study, we focus on building a
constituency parser for $\mathbf{M}$iddle $\mathbf{H}$igh $\mathbf{G}$erman
$\mathbf{MHG}$ under realistic conditions, where no annotated MHG treebank is
available for training. In our approach, we leverage the linguistic continuity
and structural similarity between MHG and $\mathbf{M}$odern $\mathbf{G}$erman
$\mathbf{MG}$, along with the abundance of MG treebank resources. Specifically,
by employing the $\mathit{delexicalization}$ method, we train a constituency
parser on MG parse datasets and perform cross-lingual transfer to MHG parsing.
Our delexicalized constituency parser demonstrates remarkable performance on
the MHG test set, achieving an F1-score of 67.3%. It outperforms the best
zero-shot cross-lingual baseline by a margin of 28.6% points. These encouraging
results underscore the practicality and potential for automatic syntactic
analysis in other ancient languages that face similar challenges as MHG.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04665">Sudowoodo: a Chinese Lyric Imitation System with Source Lyrics. (arXiv:2308.04665v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1">Yongzhu Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Rongsheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1">Lin Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Qihang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Le Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pu_J/0/1/0/all/0/1">Jiashu Pu</a></p>
<p>Lyrics generation is a well-known application in natural language generation
research, with several previous studies focusing on generating accurate lyrics
using precise control such as keywords, rhymes, etc. However, lyrics imitation,
which involves writing new lyrics by imitating the style and content of the
source lyrics, remains a challenging task due to the lack of a parallel corpus.
In this paper, we introduce \textbf{\textit{Sudowoodo}}, a Chinese lyrics
imitation system that can generate new lyrics based on the text of source
lyrics. To address the issue of lacking a parallel training corpus for lyrics
imitation, we propose a novel framework to construct a parallel corpus based on
a keyword-based lyrics model from source lyrics. Then the pairs \textit{(new
lyrics, source lyrics)} are used to train the lyrics imitation model. During
the inference process, we utilize a post-processing module to filter and rank
the generated lyrics, selecting the highest-quality ones. We incorporated audio
information and aligned the lyrics with the audio to form the songs as a bonus.
The human evaluation results show that our framework can perform better lyric
imitation. Meanwhile, the \textit{Sudowoodo} system and demo video of the
system is available at
\href{https://Sudowoodo.apps-hp.danlu.netease.com/}{Sudowoodo} and
\href{https://youtu.be/u5BBT_j1L5M}{https://youtu.be/u5BBT\_j1L5M}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04679">Sci-CoT: Leveraging Large Language Models for Enhanced Knowledge Distillation in Small Models for Scientific QA. (arXiv:2308.04679v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yuhan Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1">Haiqi Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1">Chenyou Fan</a></p>
<p>Large Language Models (LLMs) have shown outstanding performance across wide
range of downstream tasks. This competency is attributed to their substantial
parameter size and pre-training on extensive corpus. Moreover, LLMs have
exhibited enhanced reasoning capabilities in tackling complex reasoning tasks,
owing to the utilization of a method named ``Chain-of-Thought (CoT)
prompting''. This method is designed to generate intermediate reasoning steps
that guide the inference of the final answer. However, it is essential to
highlight that these advanced reasoning abilities appear to emerge in models
with a minimum of 10 billion parameters, thereby limiting its efficacy in
situations where computational resources are constrained. In this paper, we
investigate the possibility of transferring the reasoning capabilities of LLMs
to smaller models via knowledge distillation. Specifically, we propose Sci-CoT,
a two-stage framework that separates the processes of generating rationales and
inferring answers. This method enables a more efficient use of rationales
during the answer inference stage, leading to improved performance on
scientific question-answering tasks. Utilizing Sci-CoT, our 80-million
parameter model is able to exceed the performance of BLOOM-176B in the ARC-Easy
dataset under the few shot setting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04688">Generating News-Centric Crossword Puzzles As A Constraint Satisfaction and Optimization Problem. (arXiv:2308.04688v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Majima_K/0/1/0/all/0/1">Kaito Majima</a>, <a href="http://arxiv.org/find/cs/1/au:+Ishihara_S/0/1/0/all/0/1">Shotaro Ishihara</a></p>
<p>Crossword puzzles have traditionally served not only as entertainment but
also as an educational tool that can be used to acquire vocabulary and language
proficiency. One strategy to enhance the educational purpose is
personalization, such as including more words on a particular topic. This paper
focuses on the case of encouraging people's interest in news and proposes a
framework for automatically generating news-centric crossword puzzles. We
designed possible scenarios and built a prototype as a constraint satisfaction
and optimization problem, that is, containing as many news-derived words as
possible. Our experiments reported the generation probabilities and time
required under several conditions. The results showed that news-centric
crossword puzzles can be generated even with few news-derived words. We
summarize the current issues and future research directions through a
qualitative evaluation of the prototype. This is the first proposal that a
formulation of a constraint satisfaction and optimization problem can be
beneficial as an educational application.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04709">A Comparative Study of Open-Source Large Language Models, GPT-4 and Claude 2: Multiple-Choice Test Taking in Nephrology. (arXiv:2308.04709v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Sean Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Koo_M/0/1/0/all/0/1">Michael Koo</a>, <a href="http://arxiv.org/find/cs/1/au:+Blum_L/0/1/0/all/0/1">Lesley Blum</a>, <a href="http://arxiv.org/find/cs/1/au:+Black_A/0/1/0/all/0/1">Andy Black</a>, <a href="http://arxiv.org/find/cs/1/au:+Kao_L/0/1/0/all/0/1">Liyo Kao</a>, <a href="http://arxiv.org/find/cs/1/au:+Scalzo_F/0/1/0/all/0/1">Fabien Scalzo</a>, <a href="http://arxiv.org/find/cs/1/au:+Kurtz_I/0/1/0/all/0/1">Ira Kurtz</a></p>
<p>In recent years, there have been significant breakthroughs in the field of
natural language processing, particularly with the development of large
language models (LLMs). These LLMs have showcased remarkable capabilities on
various benchmarks. In the healthcare field, the exact role LLMs and other
future AI models will play remains unclear. There is a potential for these
models in the future to be used as part of adaptive physician training, medical
co-pilot applications, and digital patient interaction scenarios. The ability
of AI models to participate in medical training and patient care will depend in
part on their mastery of the knowledge content of specific medical fields. This
study investigated the medical knowledge capability of LLMs, specifically in
the context of internal medicine subspecialty multiple-choice test-taking
ability. We compared the performance of several open-source LLMs (Koala 7B,
Falcon 7B, Stable-Vicuna 13B, and Orca Mini 13B), to GPT-4 and Claude 2 on
multiple-choice questions in the field of Nephrology. Nephrology was chosen as
an example of a particularly conceptually complex subspecialty field within
internal medicine. The study was conducted to evaluate the ability of LLM
models to provide correct answers to nephSAP (Nephrology Self-Assessment
Program) multiple-choice questions. The overall success of open-sourced LLMs in
answering the 858 nephSAP multiple-choice questions correctly was 17.1% -
25.5%. In contrast, Claude 2 answered 54.4% of the questions correctly, whereas
GPT-4 achieved a score of 73.3%. We show that current widely used open-sourced
LLMs do poorly in their ability for zero-shot reasoning when compared to GPT-4
and Claude 2. The findings of this study potentially have significant
implications for the future of subspecialty medical training and patient care.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04711">Answering Unseen Questions With Smaller Language\\Models Using Rationale Generation and Dense Retrieval. (arXiv:2308.04711v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hartill_T/0/1/0/all/0/1">Tim Hartill</a>, <a href="http://arxiv.org/find/cs/1/au:+Benavides_Prado_D/0/1/0/all/0/1">Diana Benavides-Prado</a>, <a href="http://arxiv.org/find/cs/1/au:+Witbrock_M/0/1/0/all/0/1">Michael Witbrock</a>, <a href="http://arxiv.org/find/cs/1/au:+Riddle_P/0/1/0/all/0/1">Patricia J. Riddle</a></p>
<p>When provided with sufficient explanatory context, smaller Language Models
have been shown to exhibit strong reasoning ability on challenging short-answer
question-answering tasks where the questions are unseen in training. We
evaluate two methods for further improvement in this setting. Both methods
focus on combining rationales generated by a larger Language Model with longer
contexts created from a multi-hop dense retrieval system. The first method
($\textit{RR}$) involves training a Rationale Ranking model to score both
generated rationales and retrieved contexts with respect to relevance and
truthfulness. We then use the scores to derive combined contexts from both
knowledge sources using a number of combinatory strategies. For the second
method ($\textit{RATD}$) we train a smaller Reasoning model using
retrieval-augmented training datasets such that it becomes proficient at
utilising relevant information from longer text sequences that may be only
partially evidential and frequently contain many irrelevant sentences.
Generally we find that both methods are effective but that the $\textit{RATD}$
method is more straightforward to apply and produces the strongest results in
the unseen setting on which we focus. Our single best Reasoning model using
only 440 million parameters materially improves upon strong comparable prior
baselines for unseen evaluation datasets (StrategyQA 58.9 $\rightarrow$ 61.7
acc., CommonsenseQA 63.6 $\rightarrow$ 72.7 acc., ARC-DA 31.6 $\rightarrow$
52.1 F1, IIRC 25.5 $\rightarrow$ 27.3 F1) and a version utilising our prior
knowledge of each type of question in selecting a context combination strategy
does even better. Our proposed models also generally outperform direct prompts
against much larger models (BLOOM 175B and StableVicuna 13B) in both few-shot
chain-of-thought and few-shot answer-only settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04712">Slot Induction via Pre-trained Language Model Probing and Multi-level Contrastive Learning. (arXiv:2308.04712v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1">Hoang H. Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chenwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Ye Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1">Philip S. Yu</a></p>
<p>Recent advanced methods in Natural Language Understanding for Task-oriented
Dialogue (TOD) Systems (e.g., intent detection and slot filling) require a
large amount of annotated data to achieve competitive performance. In reality,
token-level annotations (slot labels) are time-consuming and difficult to
acquire. In this work, we study the Slot Induction (SI) task whose objective is
to induce slot boundaries without explicit knowledge of token-level slot
annotations. We propose leveraging Unsupervised Pre-trained Language Model
(PLM) Probing and Contrastive Learning mechanism to exploit (1) unsupervised
semantic knowledge extracted from PLM, and (2) additional sentence-level intent
label signals available from TOD. Our approach is shown to be effective in SI
task and capable of bridging the gaps with token-level supervised models on two
NLU benchmark datasets. When generalized to emerging intents, our SI objectives
also provide enhanced slot label representations, leading to improved
performance on the Slot Filling tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04756">Building Interpretable and Reliable Open Information Retriever for New Domains Overnight. (arXiv:2308.04756v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1">Xiaodong Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1">Ben Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1">Dan Roth</a></p>
<p>Information retrieval (IR) or knowledge retrieval, is a critical component
for many down-stream tasks such as open-domain question answering (QA). It is
also very challenging, as it requires succinctness, completeness, and
correctness. In recent works, dense retrieval models have achieved
state-of-the-art (SOTA) performance on in-domain IR and QA benchmarks by
representing queries and knowledge passages with dense vectors and learning the
lexical and semantic similarity. However, using single dense vectors and
end-to-end supervision are not always optimal because queries may require
attention to multiple aspects and event implicit knowledge. In this work, we
propose an information retrieval pipeline that uses entity/event linking model
and query decomposition model to focus more accurately on different information
units of the query. We show that, while being more interpretable and reliable,
our proposed pipeline significantly improves passage coverages and denotation
accuracies across five IR and QA benchmarks. It will be the go-to system to use
for applications that need to perform IR on a new domain without much dedicated
effort, because of its superior interpretability and cross-domain performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04763">Automatically measuring speech fluency in people with aphasia: first achievements using read-speech data. (arXiv:2308.04763v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fontan_L/0/1/0/all/0/1">Lionel Fontan</a>, <a href="http://arxiv.org/find/cs/1/au:+Prince_T/0/1/0/all/0/1">Typhanie Prince</a> (Praxiling, LNPL), <a href="http://arxiv.org/find/cs/1/au:+Nowakowska_A/0/1/0/all/0/1">Aleksandra Nowakowska</a> (Praxiling), <a href="http://arxiv.org/find/cs/1/au:+Sahraoui_H/0/1/0/all/0/1">Halima Sahraoui</a> (LNPL), <a href="http://arxiv.org/find/cs/1/au:+Martinez_Ferreiro_S/0/1/0/all/0/1">Silvia Martinez-Ferreiro</a></p>
<p>Background: Speech and language pathologists (SLPs) often relyon judgements
of speech fluency for diagnosing or monitoringpatients with aphasia. However,
such subjective methods havebeen criticised for their lack of reliability and
their clinical cost interms of time. Aims: This study aims at assessing the
relevance of a signalprocessingalgorithm, initially developed in the field of
language acquisition, for the automatic measurement of speech fluency in people
with aphasia (PWA). Methods &amp; Procedures: Twenty-nine PWA and five control
participantswere recruited via non-profit organizations and SLP networks. All
participants were recorded while reading out loud a set ofsentences taken from
the French version of the Boston Diagnostic Aphasia Examination. Three trained
SLPs assessed the fluency of each sentence on a five-point qualitative scale. A
forward-backward divergence segmentation and a clustering algorithm were used
to compute, for each sentence, four automatic predictors of speech fluency:
pseudo-syllable rate, speech ratio, rate of silent breaks, and standard
deviation of pseudo-syllable length. The four predictors were finally combined
into multivariate regression models (a multiplelinear regression - MLR, and two
non-linear models) to predict the average SLP ratings of speech fluency, using
a leave-one speaker-out validation scheme. Outcomes &amp; Results: All models
achieved accurate predictions of speech fluency ratings, with average
root-mean-square errors as low as 0.5. The MLR yielded a correlation
coefficient of 0.87 with reference ratings at the sentence level, and of 0.93
when aggregating the data for each participant. The inclusion of an additional
predictor sensitive to repetitions improved further the predictions with a
correlation coefficient of 0.91 at the sentence level, and of 0.96 at the
participant level. Conclusions: The algorithms used in this study can
constitute a cost-effective and reliable tool for the assessment of the speech
fluency of patients with aphasia in read-aloud tasks. Perspectives for the
assessment of spontaneous speech are discussed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04800">ADMUS: A Progressive Question Answering Framework Adaptable to Multiple Knowledge Sources. (arXiv:2308.04800v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1">Yirui Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yanzeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Minhao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_L/0/1/0/all/0/1">Lei Zou</a></p>
<p>With the introduction of deep learning models, semantic parsingbased
knowledge base question answering (KBQA) systems have achieved high performance
in handling complex questions. However, most existing approaches primarily
focus on enhancing the model's effectiveness on individual benchmark datasets,
disregarding the high costs of adapting the system to disparate datasets in
real-world scenarios (e.g., multi-tenant platform). Therefore, we present
ADMUS, a progressive knowledge base question answering framework designed to
accommodate a wide variety of datasets, including multiple languages, diverse
backbone knowledge bases, and disparate question answering datasets. To
accomplish the purpose, we decouple the architecture of conventional KBQA
systems and propose this dataset-independent framework. Our framework supports
the seamless integration of new datasets with minimal effort, only requiring
creating a dataset-related micro-service at a negligible cost. To enhance the
usability of ADUMS, we design a progressive framework consisting of three
stages, ranges from executing exact queries, generating approximate queries and
retrieving open-domain knowledge referring from large language models. An
online demonstration of ADUMS is available at:
https://answer.gstore.cn/pc/index.html
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04811">A Bipartite Graph is All We Need for Enhancing Emotional Reasoning with Commonsense Knowledge. (arXiv:2308.04811v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1">Kailai Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tianlin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1">Shaoxiong Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Ananiadou_S/0/1/0/all/0/1">Sophia Ananiadou</a></p>
<p>The context-aware emotional reasoning ability of AI systems, especially in
conversations, is of vital importance in applications such as online opinion
mining from social media and empathetic dialogue systems. Due to the implicit
nature of conveying emotions in many scenarios, commonsense knowledge is widely
utilized to enrich utterance semantics and enhance conversation modeling.
However, most previous knowledge infusion methods perform empirical knowledge
filtering and design highly customized architectures for knowledge interaction
with the utterances, which can discard useful knowledge aspects and limit their
generalizability to different knowledge sources. Based on these observations,
we propose a Bipartite Heterogeneous Graph (BHG) method for enhancing emotional
reasoning with commonsense knowledge. In BHG, the extracted context-aware
utterance representations and knowledge representations are modeled as
heterogeneous nodes. Two more knowledge aggregation node types are proposed to
perform automatic knowledge filtering and interaction. BHG-based knowledge
infusion can be directly generalized to multi-type and multi-grained knowledge
sources. In addition, we propose a Multi-dimensional Heterogeneous Graph
Transformer (MHGT) to perform graph reasoning, which can retain unchanged
feature spaces and unequal dimensions for heterogeneous node types during
inference to prevent unnecessary loss of information. Experiments show that
BHG-based methods significantly outperform state-of-the-art knowledge infusion
methods and show generalized knowledge infusion ability with higher efficiency.
Further analysis proves that previous empirical knowledge filtering methods do
not guarantee to provide the most useful knowledge information. Our code is
available at: https://github.com/SteveKGYang/BHG.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04813">CLEVA: Chinese Language Models EVAluation Platform. (arXiv:2308.04813v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yanyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jianqiao Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1">Duo Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1">Zi-Yuan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1">Xiaohui Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yongfeng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Shijia Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1">Dahua Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1">Michael R. Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Liwei Wang</a></p>
<p>With the continuous emergence of Chinese Large Language Models (LLMs), how to
evaluate a model's capabilities has become an increasingly significant issue.
The absence of a comprehensive Chinese benchmark that thoroughly assesses a
model's performance, the unstandardized and incomparable prompting procedure,
and the prevalent risk of contamination pose major challenges in the current
evaluation of Chinese LLMs. We present CLEVA, a user-friendly platform crafted
to holistically evaluate Chinese LLMs. Our platform employs a standardized
workflow to assess LLMs' performance across various dimensions, regularly
updating a competitive leaderboard. To alleviate contamination, CLEVA curates a
significant proportion of new data and develops a sampling strategy that
guarantees a unique subset for each leaderboard round. Empowered by an
easy-to-use interface that requires just a few mouse clicks and a model API,
users can conduct a thorough evaluation with minimal coding. Large-scale
experiments featuring 23 influential Chinese LLMs have validated CLEVA's
efficacy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04823">Evaluating the Generation Capabilities of Large Chinese Language Models. (arXiv:2308.04823v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1">Hui Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1">Jingyuan Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Hao_M/0/1/0/all/0/1">Meng Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1">Chen Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Ning_B/0/1/0/all/0/1">Bin Ning</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Na Zhang</a></p>
<p>This paper presents CG-Eval, the first comprehensive evaluation of the
generation capabilities of large Chinese language models across a wide range of
academic disciplines. The models' performance was assessed based on their
ability to generate accurate and relevant responses to different types of
questions in six disciplines, namely, Science and Engineering, Humanities and
Social Sciences, Mathematical Calculations, Medical Practitioner Qualification
Examination, Judicial Examination, and Certified Public Accountant Examination.
This paper also presents Gscore, a composite index derived from the weighted
sum of multiple metrics to measure the quality of model's generation against a
reference. The test data and test results can be found at
<a href="http://cgeval.besteasy.com/.">this http URL</a>
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04832">TSSR: A Truncated and Signed Square Root Activation Function for Neural Networks. (arXiv:2308.04832v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1">Yuanhao Gong</a></p>
<p>Activation functions are essential components of neural networks. In this
paper, we introduce a new activation function called the Truncated and Signed
Square Root (TSSR) function. This function is distinctive because it is odd,
nonlinear, monotone and differentiable. Its gradient is continuous and always
positive. Thanks to these properties, it has the potential to improve the
numerical stability of neural networks. Several experiments confirm that the
proposed TSSR has better performance than other stat-of-the-art activation
functions. The proposed function has significant implications for the
development of neural network models and can be applied to a wide range of
applications in fields such as computer vision, natural language processing,
and speech recognition.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04857">Emotion-Conditioned Text Generation through Automatic Prompt Optimization. (arXiv:2308.04857v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Resendiz_Y/0/1/0/all/0/1">Yarik Menchaca Resendiz</a>, <a href="http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1">Roman Klinger</a></p>
<p>Conditional natural language generation methods often require either
expensive fine-tuning or training a large language model from scratch. Both are
unlikely to lead to good results without a substantial amount of data and
computational resources. Prompt learning without changing the parameters of a
large language model presents a promising alternative. It is a cost-effective
approach, while still achieving competitive results. While this procedure is
now established for zero- and few-shot text classification and structured
prediction, it has received limited attention in conditional text generation.
We present the first automatic prompt optimization approach for
emotion-conditioned text generation with instruction-fine-tuned models. Our
method uses an iterative optimization procedure that changes the prompt by
adding, removing, or replacing tokens. As objective function, we only require a
text classifier that measures the realization of the conditional variable in
the generated text. We evaluate the method on emotion-conditioned text
generation with a focus on event reports and compare it to manually designed
prompts that also act as the seed for the optimization procedure. The optimized
prompts achieve 0.75 macro-average F1 to fulfill the emotion condition in
contrast to manually designed seed prompts with only 0.22 macro-average F1.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04885">Information-Theoretic Characterization of Vowel Harmony: A Cross-Linguistic Study on Word Lists. (arXiv:2308.04885v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Steuer_J/0/1/0/all/0/1">Julius Steuer</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdullah_B/0/1/0/all/0/1">Badr Abdullah</a>, <a href="http://arxiv.org/find/cs/1/au:+List_J/0/1/0/all/0/1">Johann-Mattis List</a>, <a href="http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1">Dietrich Klakow</a></p>
<p>We present a cross-linguistic study that aims to quantify vowel harmony using
data-driven computational modeling. Concretely, we define an
information-theoretic measure of harmonicity based on the predictability of
vowels in a natural language lexicon, which we estimate using phoneme-level
language models (PLMs). Prior quantitative studies have relied heavily on
inflected word-forms in the analysis of vowel harmony. We instead train our
models using cross-linguistically comparable lemma forms with little or no
inflection, which enables us to cover more under-studied languages. Training
data for our PLMs consists of word lists with a maximum of 1000 entries per
language. Despite the fact that the data we employ are substantially smaller
than previously used corpora, our experiments demonstrate the neural PLMs
capture vowel harmony patterns in a set of languages that exhibit this
phenomenon. Our work also demonstrates that word lists are a valuable resource
for typological research, and offers new possibilities for future studies on
low-resource, under-studied languages.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04886">Unsupervised Out-of-Distribution Dialect Detection with Mahalanobis Distance. (arXiv:2308.04886v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1">Sourya Dipta Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Vadi_Y/0/1/0/all/0/1">Yash Vadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Unnam_A/0/1/0/all/0/1">Abhishek Unnam</a>, <a href="http://arxiv.org/find/cs/1/au:+Yadav_K/0/1/0/all/0/1">Kuldeep Yadav</a></p>
<p>Dialect classification is used in a variety of applications, such as machine
translation and speech recognition, to improve the overall performance of the
system. In a real-world scenario, a deployed dialect classification model can
encounter anomalous inputs that differ from the training data distribution,
also called out-of-distribution (OOD) samples. Those OOD samples can lead to
unexpected outputs, as dialects of those samples are unseen during model
training. Out-of-distribution detection is a new research area that has
received little attention in the context of dialect classification. Towards
this, we proposed a simple yet effective unsupervised Mahalanobis distance
feature-based method to detect out-of-distribution samples. We utilize the
latent embeddings from all intermediate layers of a wav2vec 2.0
transformer-based dialect classifier model for multi-task learning. Our
proposed approach outperforms other state-of-the-art OOD detection methods
significantly.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04913">LLaMA-E: Empowering E-commerce Authoring with Multi-Aspect Instruction Following. (arXiv:2308.04913v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1">Kaize Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xueyao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dingxian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1">Yinlin Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1">Guandong Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qing Li</a></p>
<p>E-commerce authoring involves creating attractive, abundant, and targeted
promotional content to drive product sales. The emergence of large language
models (LLMs) introduces an innovative paradigm, offering a unified solution to
address various authoring tasks within this scenario. However, mainstream LLMs
trained on general corpora with common sense knowledge reveal limitations in
fitting complex and personalized features unique to e-commerce products and
customers. Furthermore, LLMs like GPT-3.5 necessitate remote accessibility,
raising concerns about safeguarding voluminous customer privacy data during
transmission. This paper proposes the LLaMA-E, the unified and customized
instruction-following language models focusing on diverse e-commerce authoring
tasks. Specifically, the domain experts create the seed instruction set from
the tasks of ads generation, query-enhanced product title rewriting, product
classification, purchase intent speculation, and general Q&amp;A. These tasks
enable the models to comprehensively understand precise e-commerce authoring
knowledge by interleaving features covering typical service aspects of
customers, sellers, and platforms. The GPT-3.5 is introduced as a teacher
model, which expands the seed instructions to form a training set for the
LLaMA-E models with various scales. The experimental results show that the
proposed LLaMA-E models achieve state-of-the-art results in quantitative and
qualitative evaluations, also exhibiting the advantage in zero-shot scenes. To
the best of our knowledge, this study is the first to serve the LLMs to
specific e-commerce authoring scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04941">Integrating large language models and active inference to understand eye movements in reading and dyslexia. (arXiv:2308.04941v1 [q-bio.NC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Donnarumma_F/0/1/0/all/0/1">Francesco Donnarumma</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Frosolone_M/0/1/0/all/0/1">Mirco Frosolone</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Pezzulo_G/0/1/0/all/0/1">Giovanni Pezzulo</a></p>
<p>We present a novel computational model employing hierarchical active
inference to simulate reading and eye movements. The model characterizes
linguistic processing as inference over a hierarchical generative model,
facilitating predictions and inferences at various levels of granularity, from
syllables to sentences.
</p>
<p>Our approach combines the strengths of large language models for realistic
textual predictions and active inference for guiding eye movements to
informative textual information, enabling the testing of predictions. The model
exhibits proficiency in reading both known and unknown words and sentences,
adhering to the distinction between lexical and nonlexical routes in dual-route
theories of reading. Notably, our model permits the exploration of maladaptive
inference effects on eye movements during reading, such as in dyslexia. To
simulate this condition, we attenuate the contribution of priors during the
reading process, leading to incorrect inferences and a more fragmented reading
style, characterized by a greater number of shorter saccades. This alignment
with empirical findings regarding eye movements in dyslexic individuals
highlights the model's potential to aid in understanding the cognitive
processes underlying reading and eye movements, as well as how reading deficits
associated with dyslexia may emerge from maladaptive predictive processing.
</p>
<p>In summary, our model represents a significant advancement in comprehending
the intricate cognitive processes involved in reading and eye movements, with
potential implications for understanding and addressing dyslexia through the
simulation of maladaptive inference. It may offer valuable insights into this
condition and contribute to the development of more effective interventions for
treatment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04945">LLMeBench: A Flexible Framework for Accelerating LLMs Benchmarking. (arXiv:2308.04945v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dalvi_F/0/1/0/all/0/1">Fahim Dalvi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasanain_M/0/1/0/all/0/1">Maram Hasanain</a>, <a href="http://arxiv.org/find/cs/1/au:+Boughorbel_S/0/1/0/all/0/1">Sabri Boughorbel</a>, <a href="http://arxiv.org/find/cs/1/au:+Mousi_B/0/1/0/all/0/1">Basel Mousi</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdaljalil_S/0/1/0/all/0/1">Samir Abdaljalil</a>, <a href="http://arxiv.org/find/cs/1/au:+Nazar_N/0/1/0/all/0/1">Nizi Nazar</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdelali_A/0/1/0/all/0/1">Ahmed Abdelali</a>, <a href="http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1">Shammur Absar Chowdhury</a>, <a href="http://arxiv.org/find/cs/1/au:+Mubarak_H/0/1/0/all/0/1">Hamdy Mubarak</a>, <a href="http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1">Ahmed Ali</a>, <a href="http://arxiv.org/find/cs/1/au:+Hawasly_M/0/1/0/all/0/1">Majd Hawasly</a>, <a href="http://arxiv.org/find/cs/1/au:+Durrani_N/0/1/0/all/0/1">Nadir Durrani</a>, <a href="http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1">Firoj Alam</a></p>
<p>The recent development and success of Large Language Models (LLMs)
necessitate an evaluation of their performance across diverse NLP tasks in
different languages. Although several frameworks have been developed and made
publicly available, their customization capabilities for specific tasks and
datasets are often complex for different users. In this study, we introduce the
LLMeBench framework. Initially developed to evaluate Arabic NLP tasks using
OpenAI's GPT and BLOOM models; it can be seamlessly customized for any NLP task
and model, regardless of language. The framework also features zero- and
few-shot learning settings. A new custom dataset can be added in less than 10
minutes, and users can use their own model API keys to evaluate the task at
hand. The developed framework has been already tested on 31 unique NLP tasks
using 53 publicly available datasets within 90 experimental setups, involving
approximately 296K data points. We plan to open-source the framework for the
community (https://github.com/qcri/LLMeBench/). A video demonstrating the
framework is available online (https://youtu.be/FkQn4UjYA0s).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04948">Extrapolating Large Language Models to Non-English by Aligning Languages. (arXiv:2308.04948v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1">Wenhao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1">Yunzhe Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1">Qingxiu Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_F/0/1/0/all/0/1">Fei Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jingjing Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Shujian Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1">Lingpeng Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiajun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lei Li</a></p>
<p>Due to the unbalanced training data distribution, the language ability of
large language models (LLMs) is often biased towards English. In this paper, we
propose to empower pre-trained LLMs on non-English languages by building
semantic alignment across languages. We perform instruction-tuning on LLaMA
with both translation task data and cross-lingual general task data to obtain
cross-lingual models (x-LLaMA). Experiment results on cross-lingual benchmark
XQUAD and MLQA show that x-LLaMA models outperform the English
instruction-tuned counterpart (Alpaca) by 42.50% on average on six non-English
languages. Further experiments on Chinese benchmark C-Eval show that x-LLaMA
achieves significant improvement on Chinese humanities tasks, outperforming
Alpaca by 8.2%. We also discover that incorporating non-English text on the
target side of translation data is particularly effective for boosting
non-English ability. Besides, we find that semantic alignment within LLM can be
further strengthened as translation task data scales up and we present the
formulation of the underlying scaling law. Evaluation results on translation
dataset Flores-101 show that \method outperforms previous LLaMA-based models in
all evaluated directions. Code and data will be available at:
https://github.com/OwenNJU/x-LLM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04950">Performance Analysis of Transformer Based Models (BERT, ALBERT and RoBERTa) in Fake News Detection. (arXiv:2308.04950v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Azizah_S/0/1/0/all/0/1">Shafna Fitria Nur Azizah</a>, <a href="http://arxiv.org/find/cs/1/au:+Cahyono_H/0/1/0/all/0/1">Hasan Dwi Cahyono</a>, <a href="http://arxiv.org/find/cs/1/au:+Sihwi_S/0/1/0/all/0/1">Sari Widya Sihwi</a>, <a href="http://arxiv.org/find/cs/1/au:+Widiarto_W/0/1/0/all/0/1">Wisnu Widiarto</a></p>
<p>Fake news is fake material in a news media format but is not processed
properly by news agencies. The fake material can provoke or defame significant
entities or individuals or potentially even for the personal interests of the
creators, causing problems for society. Distinguishing fake news and real news
is challenging due to limited of domain knowledge and time constraints.
According to the survey, the top three areas most exposed to hoaxes and
misinformation by residents are in Banten, DKI Jakarta and West Java. The model
of transformers is referring to an approach in the field of artificial
intelligence (AI) in natural language processing utilizing the deep learning
architectures. Transformers exercise a powerful attention mechanism to process
text in parallel and produce rich and contextual word representations. A
previous study indicates a superior performance of a transformer model known as
BERT over and above non transformer approach. However, some studies suggest the
performance can be improved with the use of improved BERT models known as
ALBERT and RoBERTa. However, the modified BERT models are not well explored for
detecting fake news in Bahasa Indonesia. In this research, we explore those
transformer models and found that ALBERT outperformed other models with 87.6%
accuracy, 86.9% precision, 86.9% F1-score, and 174.5 run-time (s/epoch)
respectively. Source code available at:
https://github.com/Shafna81/fakenewsdetection.git
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04982">Exploring Multilingual Text Data Distillation. (arXiv:2308.04982v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sahni_S/0/1/0/all/0/1">Shivam Sahni</a>, <a href="http://arxiv.org/find/cs/1/au:+Patel_H/0/1/0/all/0/1">Harsh Patel</a></p>
<p>With the rise of deep learning, large datasets and complex models have become
common, requiring significant computing power. To address this, data
distillation has emerged as a technique to quickly train models with lower
memory and time requirements. However, data distillation on text-based datasets
hasn't been explored much because of the challenges rising due to its discrete
nature. Additionally, existing dataset distillation methods often struggle to
generalize to new architectures. In the paper, we propose several data
distillation techniques for multilingual text classification datasets using
language-model-based learning methods. We conduct experiments to analyze their
performance in terms of classification strength, and cross-architecture
generalization. Furthermore, we investigate the language-specific fairness of
the data summaries generated by these methods. Our approach builds upon
existing techniques, enhancing cross-architecture generalization in the text
data distillation domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04992">AspectMMKG: A Multi-modal Knowledge Graph with Aspect-aware Entities. (arXiv:2308.04992v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jingdan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiaan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaodan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhixu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1">Yanghua Xiao</a></p>
<p>Multi-modal knowledge graphs (MMKGs) combine different modal data (e.g., text
and image) for a comprehensive understanding of entities. Despite the recent
progress of large-scale MMKGs, existing MMKGs neglect the multi-aspect nature
of entities, limiting the ability to comprehend entities from various
perspectives. In this paper, we construct AspectMMKG, the first MMKG with
aspect-related images by matching images to different entity aspects.
Specifically, we collect aspect-related images from a knowledge base, and
further extract aspect-related sentences from the knowledge base as queries to
retrieve a large number of aspect-related images via an online image search
engine. Finally, AspectMMKG contains 2,380 entities, 18,139 entity aspects, and
645,383 aspect-related images. We demonstrate the usability of AspectMMKG in
entity aspect linking (EAL) downstream task and show that previous EAL models
achieve a new state-of-the-art performance with the help of AspectMMKG. To
facilitate the research on aspect-related MMKG, we further propose an
aspect-related image retrieval (AIR) model, that aims to correct and expand
aspect-related images in AspectMMKG. We train an AIR model to learn the
relationship between entity image and entity aspect-related images by
incorporating entity image, aspect, and aspect image information. Experimental
results indicate that the AIR model could retrieve suitable images for a given
entity w.r.t different aspects.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05046">RadGraph2: Modeling Disease Progression in Radiology Reports via Hierarchical Information Extraction. (arXiv:2308.05046v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khanna_S/0/1/0/all/0/1">Sameer Khanna</a>, <a href="http://arxiv.org/find/cs/1/au:+Dejl_A/0/1/0/all/0/1">Adam Dejl</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1">Kibo Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Truong_Q/0/1/0/all/0/1">Quoc Hung Truong</a>, <a href="http://arxiv.org/find/cs/1/au:+Duong_H/0/1/0/all/0/1">Hanh Duong</a>, <a href="http://arxiv.org/find/cs/1/au:+Saenz_A/0/1/0/all/0/1">Agustina Saenz</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1">Pranav Rajpurkar</a></p>
<p>We present RadGraph2, a novel dataset for extracting information from
radiology reports that focuses on capturing changes in disease state and device
placement over time. We introduce a hierarchical schema that organizes entities
based on their relationships and show that using this hierarchy during training
improves the performance of an information extraction model. Specifically, we
propose a modification to the DyGIE++ framework, resulting in our model HGIE,
which outperforms previous models in entity and relation extraction tasks. We
demonstrate that RadGraph2 enables models to capture a wider variety of
findings and perform better at relation extraction compared to those trained on
the original RadGraph dataset. Our work provides the foundation for developing
automated systems that can track disease progression over time and develop
information extraction models that leverage the natural hierarchy of labels in
the medical domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05081">Constructing Holistic Spatio-Temporal Scene Graph for Video Semantic Role Labeling. (arXiv:2308.05081v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Fei_H/0/1/0/all/0/1">Hao Fei</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yixin Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bobo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Meishan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1">Jianguo Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Min Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1">Tat-Seng Chua</a></p>
<p>Video Semantic Role Labeling (VidSRL) aims to detect the salient events from
given videos, by recognizing the predict-argument event structures and the
interrelationships between events. While recent endeavors have put forth
methods for VidSRL, they can be mostly subject to two key drawbacks, including
the lack of fine-grained spatial scene perception and the insufficiently
modeling of video temporality. Towards this end, this work explores a novel
holistic spatio-temporal scene graph (namely HostSG) representation based on
the existing dynamic scene graph structures, which well model both the
fine-grained spatial semantics and temporal dynamics of videos for VidSRL.
Built upon the HostSG, we present a nichetargeting VidSRL framework. A
scene-event mapping mechanism is first designed to bridge the gap between the
underlying scene structure and the high-level event semantic structure,
resulting in an overall hierarchical scene-event (termed ICE) graph structure.
We further perform iterative structure refinement to optimize the ICE graph,
such that the overall structure representation can best coincide with end task
demand. Finally, three subtask predictions of VidSRL are jointly decoded, where
the end-to-end paradigm effectively avoids error propagation. On the benchmark
dataset, our framework boosts significantly over the current best-performing
model. Further analyses are shown for a better understanding of the advances of
our methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.00595">A Universal Question-Answering Platform for Knowledge Graphs. (arXiv:2303.00595v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Omar_R/0/1/0/all/0/1">Reham Omar</a>, <a href="http://arxiv.org/find/cs/1/au:+Dhall_I/0/1/0/all/0/1">Ishika Dhall</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalnis_P/0/1/0/all/0/1">Panos Kalnis</a>, <a href="http://arxiv.org/find/cs/1/au:+Mansour_E/0/1/0/all/0/1">Essam Mansour</a></p>
<p>Knowledge from diverse application domains is organized as knowledge graphs
(KGs) that are stored in RDF engines accessible in the web via SPARQL
endpoints. Expressing a well-formed SPARQL query requires information about the
graph structure and the exact URIs of its components, which is impractical for
the average user. Question answering (QA) systems assist by translating natural
language questions to SPARQL. Existing QA systems are typically based on
application-specific human-curated rules, or require prior information,
expensive pre-processing and model adaptation for each targeted KG. Therefore,
they are hard to generalize to a broad set of applications and KGs.
</p>
<p>In this paper, we propose KGQAn, a universal QA system that does not need to
be tailored to each target KG. Instead of curated rules, KGQAn introduces a
novel formalization of question understanding as a text generation problem to
convert a question into an intermediate abstract representation via a neural
sequence-to-sequence model. We also develop a just-in-time linker that maps at
query time the abstract representation to a SPARQL query for a specific KG,
using only the publicly accessible APIs and the existing indices of the RDF
store, without requiring any pre-processing. Our experiments with several real
KGs demonstrate that KGQAn is easily deployed and outperforms by a large margin
the state-of-the-art in terms of quality of answers and processing time,
especially for arbitrary KGs, unseen during the training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.04673">Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference. (arXiv:2303.04673v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Susan Xueqing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1">Ahmed H. Awadallah</a></p>
<p>Large Language Models (LLMs) have sparked significant interest in their
generative capabilities, leading to the development of various commercial
applications. The high cost of using the models drives application builders to
maximize the value of generation under a limited inference budget. This paper
presents a study of optimizing inference hyperparameters such as the number of
responses, temperature and max tokens, which significantly affects the
utility/cost of text generation. We design a framework named EcoOptiGen which
leverages economical hyperparameter optimization and cost-based pruning.
Experiments with the GPT-3.5/GPT-4 models on a variety of tasks verify its
effectiveness. EcoOptiGen is implemented in the `autogen' package of the FLAML
library: \url{https://aka.ms/autogen}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.16839">MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks. (arXiv:2303.16839v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kuo_W/0/1/0/all/0/1">Weicheng Kuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Piergiovanni_A/0/1/0/all/0/1">AJ Piergiovanni</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1">Dahun Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1">Xiyang Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Caine_B/0/1/0/all/0/1">Ben Caine</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ogale_A/0/1/0/all/0/1">Abhijit Ogale</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1">Luowei Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1">Andrew Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhifeng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1">Claire Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Angelova_A/0/1/0/all/0/1">Anelia Angelova</a></p>
<p>The development of language models have moved from encoder-decoder to
decoder-only designs. In addition, we observe that the two most popular
multimodal tasks, the generative and contrastive tasks, are nontrivial to
accommodate in one architecture, and further need adaptations for downstream
tasks. We propose a novel paradigm of training with a decoder-only model for
multimodal tasks, which is surprisingly effective in jointly learning of these
disparate vision-language tasks. This is done with a simple model, called
MaMMUT. It consists of a single vision encoder and a text decoder, and is able
to accommodate contrastive and generative learning by a novel two-pass approach
on the text decoder. We demonstrate that joint learning of these diverse
objectives is simple, effective, and maximizes the weight-sharing of the model
across these tasks. Furthermore, the same architecture enables straightforward
extensions to open-vocabulary object detection and video-language tasks. The
model tackles a diverse range of tasks, while being modest in capacity. Our
model achieves the state of the art on image-text and text-image retrieval,
video question answering and open-vocabulary detection tasks, outperforming
much larger and more extensively trained foundational models. It shows very
competitive results on VQA and Video Captioning, especially considering its
capacity. Ablations confirm the flexibility and advantages of our approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.17408">A Prompt-based Multimodal Tabular Transformer Encoder For Medical Intervention Duration Estimation. (arXiv:2303.17408v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ruan_Y/0/1/0/all/0/1">Yucheng Ruan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lan_X/0/1/0/all/0/1">Xiang Lan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_D/0/1/0/all/0/1">Daniel J. Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdullah_H/0/1/0/all/0/1">Hairil Rizal Abdullah</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_M/0/1/0/all/0/1">Mengling Feng</a></p>
<p>Objective: This study focuses on estimating the duration of medical
interventions using electronic health records (EHRs) in clinical decision
support. Most existing models were designed for structured tabular data only
and often suffer from data corruption problem. The unstructured clinical
free-text data that provides valuable insights and is more resistant to data
corruption is often overlooked. The objective of this research is to develop a
multimodal deep learning framework that integrates different data modalities
from EHRs, thereby fully utilizing the predictive capability of EHRs for
medical intervention estimation.
</p>
<p>Materials and Methods: A novel prompt-based tabular transformer encoder
framework is proposed for medical intervention duration estimation based on
multimodal EHR data. The framework leverages a pre-trained sentence encoder
with medical prompts to harmonize language representations of various clinical
data modalities, which a tabular transformer encoder is developed to further
explore.
</p>
<p>Results: The developed model demonstrates superior performance compared to
the baselines in two EHR datasets. Furthermore, the model exhibits resilience
to data corruption in EHRs, with the RMSE curve increasing gradually with
higher corruption rates.
</p>
<p>Discussion: Other than the predictive effectiveness and robustness of the
proposed framework, the ablation study highlights the significance of critical
components, such as medical prompts, free-text information, and the pre-trained
sentence encoder, all contributing to the model's predictive ability.
</p>
<p>Conclusion: This research presents a promising pathway to enhance medical
intervention estimation by incorporating diverse data modalities from language
perspective, ultimately bolstering the reliability of deep learning models in
clinical care.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.03210">AttentionViz: A Global View of Transformer Attention. (arXiv:2305.03210v2 [cs.HC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1">Catherine Yeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yida Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1">Aoyu Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Cynthia Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Viegas_F/0/1/0/all/0/1">Fernanda Vi&#xe9;gas</a>, <a href="http://arxiv.org/find/cs/1/au:+Wattenberg_M/0/1/0/all/0/1">Martin Wattenberg</a></p>
<p>Transformer models are revolutionizing machine learning, but their inner
workings remain mysterious. In this work, we present a new visualization
technique designed to help researchers understand the self-attention mechanism
in transformers that allows these models to learn rich, contextual
relationships between elements of a sequence. The main idea behind our method
is to visualize a joint embedding of the query and key vectors used by
transformer models to compute attention. Unlike previous attention
visualization techniques, our approach enables the analysis of global patterns
across multiple input sequences. We create an interactive visualization tool,
AttentionViz (demo: <a href="http://attentionviz.com">this http URL</a>), based on these joint query-key
embeddings, and use it to study attention mechanisms in both language and
vision transformers. We demonstrate the utility of our approach in improving
model understanding and offering new insights about query-key interactions
through several application scenarios and expert feedback.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.09287">Adversarial Word Dilution as Text Data Augmentation in Low-Resource Regime. (arXiv:2305.09287v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Junfan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Richong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1">Zheyan Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1">Chunming Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1">Yongyi Mao</a></p>
<p>Data augmentation is widely used in text classification, especially in the
low-resource regime where a few examples for each class are available during
training. Despite the success, generating data augmentations as hard positive
examples that may increase their effectiveness is under-explored. This paper
proposes an Adversarial Word Dilution (AWD) method that can generate hard
positive examples as text data augmentations to train the low-resource text
classification model efficiently. Our idea of augmenting the text data is to
dilute the embedding of strong positive words by weighted mixing with
unknown-word embedding, making the augmented inputs hard to be recognized as
positive by the classification model. We adversarially learn the dilution
weights through a constrained min-max optimization process with the guidance of
the labels. Empirical studies on three benchmark datasets show that AWD can
generate more effective data augmentations and outperform the state-of-the-art
text data augmentation methods. The additional analysis demonstrates that the
data augmentations generated by AWD are interpretable and can flexibly extend
to new examples without further training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.04187">Knowing-how &amp; Knowing-that: A New Task for Machine Comprehension of User Manuals. (arXiv:2306.04187v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1">Hongru Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jia Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1">Weihong Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1">Dingnan Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1">Wenqiang Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1">Zujie Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1">Jiancheng Lv</a></p>
<p>The machine reading comprehension (MRC) of user manuals has huge potential in
customer service. However, current methods have trouble answering complex
questions. Therefore, we introduce the Knowing-how &amp; Knowing-that task that
requires the model to answer factoid-style, procedure-style, and inconsistent
questions about user manuals. We resolve this task by jointly representing the
steps and facts in a graph TARA, which supports a unified inference of various
questions. Towards a systematical benchmarking study, we design a heuristic
method to automatically parse user manuals into TARAs and build an annotated
dataset to test the model's ability in answering real-world questions.
Empirical results demonstrate that representing user manuals as TARAs is a
desired solution for the MRC of user manuals. An in-depth investigation of TARA
further sheds light on the issues and broader impacts of future representations
of user manuals. We hope our work can move the MRC of user manuals to a more
complex and realistic stage.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05113">Go Beyond The Obvious: Probing the gap of INFORMAL reasoning ability between Humanity and LLMs by Detective Reasoning Puzzle Benchmark. (arXiv:2307.05113v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gu_Z/0/1/0/all/0/1">Zhouhon Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zihan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1">Zhuozhi Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1">Haoning Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yikai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1">Wenhao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiaoxuan Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1">Qianyu He</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">Rui Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1">Sihang Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shusen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zili Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1">Hongwei Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhixu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1">Yanghua Xiao</a></p>
<p>Informal reasoning ability is the ability to reason based on common sense,
experience, and intuition.Humans use informal reasoning every day to extract
the most influential elements for their decision-making from a large amount of
life-like information.With the rapid development of language models, the
realization of general artificial intelligence has emerged with hope. Given the
outstanding informal reasoning ability of humans, how much informal reasoning
ability language models have has not been well studied by scholars.In order to
explore the gap between humans and language models in informal reasoning
ability, this paper constructs a Detective Reasoning Benchmark, which is an
assembly of 1,200 questions gathered from accessible online resources, aims at
evaluating the model's informal reasoning ability in real-life
context.Considering the improvement of the model's informal reasoning ability
restricted by the lack of benchmark, we further propose a Self-Question Prompt
Framework that mimics human thinking to enhance the model's informal reasoning
ability.The goals of self-question are to find key elements, deeply investigate
the connections between these elements, encourage the relationship between each
element and the problem, and finally, require the model to reasonably answer
the problem.The experimental results show that human performance greatly
outperforms the SoTA Language Models in Detective Reasoning Benchmark.Besides,
Self-Question is proven to be the most effective prompt engineering in
improving GPT-4's informal reasoning ability, but it still does not even
surpass the lowest score made by human participants.Upon acceptance of the
paper, the source code for the benchmark will be made publicly accessible.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06713">Unsupervised Calibration through Prior Adaptation for Text Classification using Large Language Models. (arXiv:2307.06713v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Estienne_L/0/1/0/all/0/1">Lautaro Estienne</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferrer_L/0/1/0/all/0/1">Luciana Ferrer</a>, <a href="http://arxiv.org/find/cs/1/au:+Vera_M/0/1/0/all/0/1">Mat&#xed;as Vera</a>, <a href="http://arxiv.org/find/cs/1/au:+Piantanida_P/0/1/0/all/0/1">Pablo Piantanida</a></p>
<p>A wide variety of natural language tasks are currently being addressed with
large-scale language models (LLMs). These models are usually trained with a
very large amount of unsupervised text data and adapted to perform a downstream
natural language task using methods like fine-tuning, calibration or in-context
learning. In this work, we propose an approach to adapt the prior class
distribution to perform text classification tasks without the need for labelled
samples and only few in-domain sample queries. The proposed approach treats the
LLM as a black box, adding a stage where the model posteriors are calibrated to
the task. Results show that these methods outperform the un-adapted model for
different number of training shots in the prompt and a previous approach were
calibration is performed without using any adaptation data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06775">A Novel Site-Agnostic Multimodal Deep Learning Model to Identify Pro-Eating Disorder Content on Social Media. (arXiv:2307.06775v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feldman_J/0/1/0/all/0/1">Jonathan Feldman</a></p>
<p>Over the last decade, there has been a vast increase in eating disorder
diagnoses and eating disorder-attributed deaths, reaching their zenith during
the Covid-19 pandemic. This immense growth derived in part from the stressors
of the pandemic but also from increased exposure to social media, which is rife
with content that promotes eating disorders. This study aimed to create a
multimodal deep learning model that can determine if a given social media post
promotes eating disorders based on a combination of visual and textual data. A
labeled dataset of Tweets was collected from Twitter, upon which twelve deep
learning models were trained and tested. Based on model performance, the most
effective deep learning model was the multimodal fusion of the RoBERTa natural
language processing model and the MaxViT image classification model, attaining
accuracy and F1 scores of 95.9% and 0.959, respectively. The RoBERTa and MaxViT
fusion model, deployed to classify an unlabeled dataset of posts from the
social media sites Tumblr and Reddit, generated results akin to those of
previous research studies that did not employ artificial intelligence-based
techniques, indicating that deep learning models can develop insights congruent
to those of researchers. Additionally, the model was used to conduct a
timeseries analysis of yet unseen Tweets from eight Twitter hashtags,
uncovering that, since 2014, the relative abundance of content that promotes
eating disorders has decreased drastically within those communities. Despite
this reduction, by 2018, content that promotes eating disorders had either
stopped declining or increased in ampleness anew on these hashtags.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07415">AutoHint: Automatic Prompt Optimization with Hint Generation. (arXiv:2307.07415v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Hong Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xue Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yinchuan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Homma_Y/0/1/0/all/0/1">Youkow Homma</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1">Qi Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1">Min Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1">Jian Jiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Charles_D/0/1/0/all/0/1">Denis Charles</a></p>
<p>This paper presents AutoHint, a novel framework for automatic prompt
engineering and optimization for Large Language Models (LLM). While LLMs have
demonstrated remarkable ability in achieving high-quality annotation in various
tasks, the key to applying this ability to specific tasks lies in developing
high-quality prompts. Thus we propose a framework to inherit the merits of both
in-context learning and zero-shot learning by incorporating enriched
instructions derived from input-output demonstrations to optimize original
prompt. We refer to the enrichment as the hint and propose a framework to
automatically generate the hint from labeled data. More concretely, starting
from an initial prompt, our method first instructs a LLM to deduce new hints
for selected samples from incorrect predictions, and then summarizes from
per-sample hints and adds the results back to the initial prompt to form a new,
enriched instruction. The proposed method is evaluated on the BIG-Bench
Instruction Induction dataset for both zero-shot and few-short prompts, where
experiments demonstrate our method is able to significantly boost accuracy for
multiple tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08621">Retentive Network: A Successor to Transformer for Large Language Models. (arXiv:2307.08621v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yutao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1">Li Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Shaohan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1">Shuming Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1">Yuqing Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1">Jilong Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jianyong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1">Furu Wei</a></p>
<p>In this work, we propose Retentive Network (RetNet) as a foundation
architecture for large language models, simultaneously achieving training
parallelism, low-cost inference, and good performance. We theoretically derive
the connection between recurrence and attention. Then we propose the retention
mechanism for sequence modeling, which supports three computation paradigms,
i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel
representation allows for training parallelism. The recurrent representation
enables low-cost $O(1)$ inference, which improves decoding throughput, latency,
and GPU memory without sacrificing performance. The chunkwise recurrent
representation facilitates efficient long-sequence modeling with linear
complexity, where each chunk is encoded parallelly while recurrently
summarizing the chunks. Experimental results on language modeling show that
RetNet achieves favorable scaling results, parallel training, low-cost
deployment, and efficient inference. The intriguing properties make RetNet a
strong successor to Transformer for large language models. Code will be
available at https://aka.ms/retnet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.12267">Towards Automatic Boundary Detection for Human-AI Collaborative Hybrid Essay in Education. (arXiv:2307.12267v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1">Zijie Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Sha_L/0/1/0/all/0/1">Lele Sha</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1">Kaixun Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gasevic_D/0/1/0/all/0/1">Dragan Ga&#x161;evi&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1">Guanliang Chen</a></p>
<p>The recent large language models (LLMs), e.g., ChatGPT, have been able to
generate human-like and fluent responses when provided with specific
instructions. While admitting the convenience brought by technological
advancement, educators also have concerns that students might leverage LLMs to
complete their writing assignments and pass them off as their original work.
Although many AI content detection studies have been conducted as a result of
such concerns, most of these prior studies modeled AI content detection as a
classification problem, assuming that a text is either entirely human-written
or entirely AI-generated. In this study, we investigated AI content detection
in a rarely explored yet realistic setting where the text to be detected is
collaboratively written by human and generative LLMs (i.e., hybrid text). We
first formalized the detection task as identifying the transition points
between human-written content and AI-generated content from a given hybrid text
(boundary detection). Then we proposed a two-step approach where we (1)
separated AI-generated content from human-written content during the encoder
training process; and (2) calculated the distances between every two adjacent
prototypes and assumed that the boundaries exist between the two adjacent
prototypes that have the furthest distance from each other. Through extensive
experiments, we observed the following main findings: (1) the proposed approach
consistently outperformed the baseline methods across different experiment
settings; (2) the encoder training process can significantly boost the
performance of the proposed approach; (3) when detecting boundaries for
single-boundary hybrid essays, the proposed approach could be enhanced by
adopting a relatively large prototype size, leading to a 22% improvement in the
In-Domain evaluation and an 18% improvement in the Out-of-Domain evaluation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.02582">Adapt and Decompose: Efficient Generalization of Text-to-SQL via Domain Adapted Least-To-Most Prompting. (arXiv:2308.02582v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1">Aseem Arora</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhaisaheb_S/0/1/0/all/0/1">Shabbirhussain Bhaisaheb</a>, <a href="http://arxiv.org/find/cs/1/au:+Nigam_H/0/1/0/all/0/1">Harshit Nigam</a>, <a href="http://arxiv.org/find/cs/1/au:+Patwardhan_M/0/1/0/all/0/1">Manasi Patwardhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Vig_L/0/1/0/all/0/1">Lovekesh Vig</a>, <a href="http://arxiv.org/find/cs/1/au:+Shroff_G/0/1/0/all/0/1">Gautam Shroff</a></p>
<p>Cross-domain and cross-compositional generalization of Text-to-SQL semantic
parsing is a challenging task. Existing Large Language Model (LLM) based
solutions rely on inference-time retrieval of few-shot exemplars from the
training set to synthesize a run-time prompt for each Natural Language (NL)
test query. In contrast, we devise an algorithm which performs offline sampling
of a minimal set-of few-shots from the training data, with complete coverage of
SQL clauses, operators and functions, and maximal domain coverage within the
allowed token length. This allows for synthesis of a fixed Generic Prompt (GP),
with a diverse set-of exemplars common across NL test queries, avoiding
expensive test time exemplar retrieval. We further auto-adapt the GP to the
target database domain (DA-GP), to better handle cross-domain generalization;
followed by a decomposed Least-To-Most-Prompting (LTMP-DA-GP) to handle
cross-compositional generalization. The synthesis of LTMP-DA-GP is an offline
task, to be performed one-time per new database with minimal human
intervention. Our approach demonstrates superior performance on the KaggleDBQA
dataset, designed to evaluate generalizability for the Text-to-SQL task. We
further showcase consistent performance improvement of LTMP-DA-GP over GP,
across LLMs and databases of KaggleDBQA, highlighting the efficacy and model
agnostic benefits of our prompt based adapt and decompose approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03131">Towards Multiple References Era -- Addressing Data Leakage and Limited Reference Diversity in NLG Evaluation. (arXiv:2308.03131v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1">Xianfeng Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yijin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1">Fandong Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jie Zhou</a></p>
<p>N-gram matching-based evaluation metrics, such as BLEU and chrF, are widely
utilized across a range of natural language generation (NLG) tasks. However,
recent studies have revealed a weak correlation between these matching-based
metrics and human evaluations, especially when compared with neural-based
metrics like BLEURT. In this paper, we conjecture that the performance
bottleneck in matching-based metrics may be caused by the limited diversity of
references. To address this issue, we propose to utilize \textit{multiple
references} to enhance the consistency between these metrics and human
evaluations. Within the WMT Metrics benchmarks, we observe that the
multi-references F200spBLEU surpasses the conventional single-reference one by
an accuracy improvement of 7.2\%. Remarkably, it also exceeds the neural-based
BERTscore by an accuracy enhancement of 3.9\%. Moreover, we observe that the
data leakage issue in large language models (LLMs) can be mitigated to a large
extent by our multi-reference metric. We release the code and data at
\url{https://github.com/SefaZeng/LLM-Ref}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04226">OpinionConv: Conversational Product Search with Grounded Opinions. (arXiv:2308.04226v1 [cs.HC] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Javadi_V/0/1/0/all/0/1">Vahid Sadiri Javadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1">Martin Potthast</a>, <a href="http://arxiv.org/find/cs/1/au:+Flek_L/0/1/0/all/0/1">Lucie Flek</a></p>
<p>When searching for products, the opinions of others play an important role in
making informed decisions. Subjective experiences about a product can be a
valuable source of information. This is also true in sales conversations, where
a customer and a sales assistant exchange facts and opinions about products.
However, training an AI for such conversations is complicated by the fact that
language models do not possess authentic opinions for their lack of real-world
experience. We address this problem by leveraging product reviews as a rich
source of product opinions to ground conversational AI in true subjective
narratives. With OpinionConv, we develop the first conversational AI for
simulating sales conversations. To validate the generated conversations, we
conduct several user studies showing that the generated opinions are perceived
as realistic. Our assessors also confirm the importance of opinions as an
informative basis for decision-making.
</p>
</p>
</div>

    </div>
    </body>
    