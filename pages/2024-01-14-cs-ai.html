<!DOCTYPE html>
<html>
<head>
<title>2024-01-14-cs-ai</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2401.05342">Most discriminative stimuli for functional cell type identification. (arXiv:2401.05342v1 [q-bio.NC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Burg_M/0/1/0/all/0/1">Max F. Burg</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Zenkel_T/0/1/0/all/0/1">Thomas Zenkel</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Vystrcilova_M/0/1/0/all/0/1">Michaela Vystr&#x10d;ilov&#xe1;</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Oesterle_J/0/1/0/all/0/1">Jonathan Oesterle</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Hofling_L/0/1/0/all/0/1">Larissa H&#xf6;fling</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Willeke_K/0/1/0/all/0/1">Konstantin F. Willeke</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Lause_J/0/1/0/all/0/1">Jan Lause</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Muller_S/0/1/0/all/0/1">Sarah M&#xfc;ller</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Fahey_P/0/1/0/all/0/1">Paul G. Fahey</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Ding_Z/0/1/0/all/0/1">Zhiwei Ding</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Restivo_K/0/1/0/all/0/1">Kelli Restivo</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Sridhar_S/0/1/0/all/0/1">Shashwat Sridhar</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Gollisch_T/0/1/0/all/0/1">Tim Gollisch</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Berens_P/0/1/0/all/0/1">Philipp Berens</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Tolias_A/0/1/0/all/0/1">Andreas S. Tolias</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Euler_T/0/1/0/all/0/1">Thomas Euler</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Bethge_M/0/1/0/all/0/1">Matthias Bethge</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Ecker_A/0/1/0/all/0/1">Alexander S. Ecker</a></p>
<p>Identifying cell types and understanding their functional properties is
crucial for unraveling the mechanisms underlying perception and cognition. In
the retina, functional types can be identified by carefully selected stimuli,
but this requires expert domain knowledge and biases the procedure towards
previously known cell types. In the visual cortex, it is still unknown what
functional types exist and how to identify them. Thus, for unbiased
identification of the functional cell types in retina and visual cortex, new
approaches are needed. Here we propose an optimization-based clustering
approach using deep predictive models to obtain functional clusters of neurons
using Most Discriminative Stimuli (MDS). Our approach alternates between
stimulus optimization with cluster reassignment akin to an
expectation-maximization algorithm. The algorithm recovers functional clusters
in mouse retina, marmoset retina and macaque visual area V4. This demonstrates
that our approach can successfully find discriminative stimuli across species,
stages of the visual system and recording techniques. The resulting most
discriminative stimuli can be used to assign functional cell types fast and on
the fly, without the need to train complex predictive models or show a large
natural scene dataset, paving the way for experiments that were previously
limited by experimental time. Crucially, MDS are interpretable: they visualize
the distinctive stimulus patterns that most unambiguously identify a specific
type of neuron. We will make our code available online upon publication.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05346">Task tree retrieval from FOON using search algorithms. (arXiv:2401.05346v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Attapu_A/0/1/0/all/0/1">Amitha Attapu</a></p>
<p>Robots can be very useful to automate tasks and reduce the human effort
required. But for the robot to know, how to perform tasks, we need to give it a
clear set of steps to follow. It is nearly impossible to provide a robot with
instructions for every possible task. Therefore we have a Universal Functional
object-oriented network (FOON) which was created and expanded and has a lot of
existing recipe information [1]. But certain tasks are complicated for robots
to perform and similarly, some tasks are complicated for humans to perform.
Therefore weights have been added to functional units to represent the chance
of successful execution of the motion by the robot [2]. Given a set of kitchen
items and a goal node, using Universal FOON, a robot must be able to determine
if the required items are present in the kitchen, and if yes, get the steps to
convert the required kitchen items to the goal node. Now through this paper, we
use two algorithms (IDS and GBFS) to retrieve a task tree (if possible) for a
goal node and a given set of kitchen items. The following would be the
different parts of the paper: Section II FOON creation, where we will discuss
the different terminologies related to FOON and visualization of FOON. In
Section III Methodology we discuss the IDS and GBFS search algorithms and the
two different heuristics implemented and used in GBFS. In Section IV
Experiment/Discussion, we compare the performance of different algorithms. In
the final section V, we specify the references of the papers that have been
cited.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05350">Adaptive operator selection utilising generalised experience. (arXiv:2401.05350v1 [cs.NE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aydin_M/0/1/0/all/0/1">Mehmet Emin Aydin</a>, <a href="http://arxiv.org/find/cs/1/au:+Durgut_R/0/1/0/all/0/1">Rafet Durgut</a>, <a href="http://arxiv.org/find/cs/1/au:+Rakib_A/0/1/0/all/0/1">Abdur Rakib</a></p>
<p>Optimisation problems, particularly combinatorial optimisation problems, are
difficult to solve due to their complexity and hardness. Such problems have
been successfully solved by evolutionary and swarm intelligence algorithms,
especially in binary format. However, the approximation may suffer due to the
the issues in balance between exploration and exploitation activities (EvE),
which remain as the major challenge in this context. Although the complementary
usage of multiple operators is becoming more popular for managing EvE with
adaptive operator selection schemes, a bespoke adaptive selection system is
still an important topic in research. Reinforcement Learning (RL) has recently
been proposed as a way to customise and shape up a highly effective adaptive
selection system. However, it is still challenging to handle the problem in
terms of scalability. This paper proposes and assesses a RL-based novel
approach to help develop a generalised framework for gaining, processing, and
utilising the experiences for both the immediate and future use. The
experimental results support the proposed approach with a certain level of
success.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05355">Developing a Resource-Constraint EdgeAI model for Surface Defect Detection. (arXiv:2401.05355v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mih_A/0/1/0/all/0/1">Atah Nuh Mih</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1">Hung Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Kawnine_A/0/1/0/all/0/1">Asfia Kawnine</a>, <a href="http://arxiv.org/find/cs/1/au:+Wachowicz_M/0/1/0/all/0/1">Monica Wachowicz</a></p>
<p>Resource constraints have restricted several EdgeAI applications to machine
learning inference approaches, where models are trained on the cloud and
deployed to the edge device. This poses challenges such as bandwidth, latency,
and privacy associated with storing data off-site for model building. Training
on the edge device can overcome these challenges by eliminating the need to
transfer data to another device for storage and model development. On-device
training also provides robustness to data variations as models can be retrained
on newly acquired data to improve performance. We, therefore, propose a
lightweight EdgeAI architecture modified from Xception, for on-device training
in a resource-constraint edge environment. We evaluate our model on a PCB
defect detection task and compare its performance against existing lightweight
models - MobileNetV2, EfficientNetV2B0, and MobileViT-XXS. The results of our
experiment show that our model has a remarkable performance with a test
accuracy of 73.45% without pre-training. This is comparable to the test
accuracy of non-pre-trained MobileViT-XXS (75.40%) and much better than other
non-pre-trained models (MobileNetV2 - 50.05%, EfficientNetV2B0 - 54.30%). The
test accuracy of our model without pre-training is comparable to pre-trained
MobileNetV2 model - 75.45% and better than pre-trained EfficientNetV2B0 model -
58.10%. In terms of memory efficiency, our model performs better than
EfficientNetV2B0 and MobileViT-XXS. We find that the resource efficiency of
machine learning models does not solely depend on the number of parameters but
also depends on architectural considerations. Our method can be applied to
other resource-constraint applications while maintaining significant
performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05362">DualTeacher: Bridging Coexistence of Unlabelled Classes for Semi-supervised Incremental Object Detection. (arXiv:2401.05362v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1">Ziqi Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Liyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1">Wenbo Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xingxing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_J/0/1/0/all/0/1">Jiachen Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Ai_J/0/1/0/all/0/1">Jianyong Ai</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jianmin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jun Zhu</a></p>
<p>In real-world applications, an object detector often encounters object
instances from new classes and needs to accommodate them effectively. Previous
work formulated this critical problem as incremental object detection (IOD),
which assumes the object instances of new classes to be fully annotated in
incremental data. However, as supervisory signals are usually rare and
expensive, the supervised IOD may not be practical for implementation. In this
work, we consider a more realistic setting named semi-supervised IOD (SSIOD),
where the object detector needs to learn new classes incrementally from a few
labelled data and massive unlabelled data without catastrophic forgetting of
old classes. A commonly-used strategy for supervised IOD is to encourage the
current model (as a student) to mimic the behavior of the old model (as a
teacher), but it generally fails in SSIOD because a dominant number of object
instances from old and new classes are coexisting and unlabelled, with the
teacher only recognizing a fraction of them. Observing that learning only the
classes of interest tends to preclude detection of other classes, we propose to
bridge the coexistence of unlabelled classes by constructing two teacher models
respectively for old and new classes, and using the concatenation of their
predictions to instruct the student. This approach is referred to as
DualTeacher, which can serve as a strong baseline for SSIOD with limited
resource overhead and no extra hyperparameters. We build various benchmarks for
SSIOD and perform extensive experiments to demonstrate the superiority of our
approach (e.g., the performance lead is up to 18.28 AP on MS-COCO). Our code is
available at \url{https://github.com/chuxiuhong/DualTeacher}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05370">Autoregressive fragment-based diffusion for pocket-aware ligand design. (arXiv:2401.05370v1 [q-bio.BM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Ghorbani_M/0/1/0/all/0/1">Mahdi Ghorbani</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Gendelev_L/0/1/0/all/0/1">Leo Gendelev</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Beroza_P/0/1/0/all/0/1">Paul Beroza</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Keiser_M/0/1/0/all/0/1">Michael J. Keiser</a></p>
<p>In this work, we introduce AutoFragDiff, a fragment-based autoregressive
diffusion model for generating 3D molecular structures conditioned on target
protein structures. We employ geometric vector perceptrons to predict atom
types and spatial coordinates of new molecular fragments conditioned on
molecular scaffolds and protein pockets. Our approach improves the local
geometry of the resulting 3D molecules while maintaining high predicted binding
affinity to protein targets. The model can also perform scaffold extension from
user-provided starting molecular scaffold.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05373">Dynamic Spiking Graph Neural Networks. (arXiv:2401.05373v1 [cs.NE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yin_N/0/1/0/all/0/1">Nan Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Mengzhu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhenghan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Masi_G/0/1/0/all/0/1">Giulia De Masi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_B/0/1/0/all/0/1">Bin Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1">Huan Xiong</a></p>
<p>The integration of Spiking Neural Networks (SNNs) and Graph Neural Networks
(GNNs) is gradually attracting attention due to the low power consumption and
high efficiency in processing the non-Euclidean data represented by graphs.
However, as a common problem, dynamic graph representation learning faces
challenges such as high complexity and large memory overheads. Current work
often uses SNNs instead of Recurrent Neural Networks (RNNs) by using binary
features instead of continuous ones for efficient training, which would
overlooks graph structure information and leads to the loss of details during
propagation. Additionally, optimizing dynamic spiking models typically requires
propagation of information across time steps, which increases memory
requirements. To address these challenges, we present a framework named
\underline{Dy}namic \underline{S}p\underline{i}king \underline{G}raph
\underline{N}eural Networks (\method{}). To mitigate the information loss
problem, \method{} propagates early-layer information directly to the last
layer for information compensation. To accommodate the memory requirements, we
apply the implicit differentiation on the equilibrium state, which does not
rely on the exact reverse of the forward computation. While traditional
implicit differentiation methods are usually used for static situations,
\method{} extends it to the dynamic graph setting. Extensive experiments on
three large-scale real-world dynamic graph datasets validate the effectiveness
of \method{} on dynamic node classification tasks with lower computational
costs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05375">Classical Sorting Algorithms as a Model of Morphogenesis: self-sorting arrays reveal unexpected competencies in a minimal model of basal intelligence. (arXiv:2401.05375v1 [cs.NE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Taining Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Goldstein_A/0/1/0/all/0/1">Adam Goldstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Levin_M/0/1/0/all/0/1">Michael Levin</a></p>
<p>The emerging field of Diverse Intelligence seeks to identify, formalize, and
understand commonalities in behavioral competencies across a wide range of
implementations. Especially interesting are simple systems that provide
unexpected examples of memory, decision-making, or problem-solving in
substrates that at first glance do not appear to be complex enough to implement
such capabilities. We seek to develop tools to help understand the minimal
requirements for such capabilities, and to learn to recognize and predict basal
forms of intelligence in unconventional substrates. Here, we apply novel
analyses to the behavior of classical sorting algorithms, short pieces of code
which have been studied for many decades. To study these sorting algorithms as
a model of biological morphogenesis and its competencies, we break two
formerly-ubiquitous assumptions: top-down control (instead, showing how each
element within a array of numbers can exert minimal agency and implement
sorting policies from the bottom up), and fully reliable hardware (instead,
allowing some of the elements to be "damaged" and fail to execute the
algorithm). We quantitatively characterize sorting activity as the traversal of
a problem space, showing that arrays of autonomous elements sort themselves
more reliably and robustly than traditional implementations in the presence of
errors. Moreover, we find the ability to temporarily reduce progress in order
to navigate around a defect, and unexpected clustering behavior among the
elements in chimeric arrays whose elements follow one of two different
algorithms. The discovery of emergent problem-solving capacities in simple,
familiar algorithms contributes a new perspective to the field of Diverse
Intelligence, showing how basal forms of intelligence can emerge in simple
systems without being explicitly encoded in their underlying mechanics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05384">From Good to Great: Improving Math Reasoning with Tool-Augmented Interleaf Prompting. (arXiv:2401.05384v1 [math.HO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Chen_N/0/1/0/all/0/1">Nuo Chen</a>, <a href="http://arxiv.org/find/math/1/au:+Li_H/0/1/0/all/0/1">Hongguang Li</a>, <a href="http://arxiv.org/find/math/1/au:+Wang_B/0/1/0/all/0/1">Baoyuan Wang</a>, <a href="http://arxiv.org/find/math/1/au:+Li_J/0/1/0/all/0/1">Jia Li</a></p>
<p>This paper investigates the performance of Large Language Models (LLMs) and
Tool-augmented LLMs in tackling complex mathematical reasoning tasks. We
introduce IMP-TIP: Improving Math Reasoning with Tool-augmented Interleaf
Prompting, a framework that combines the strengths of both LLMs and
Tool-augmented LLMs. IMP-TIP follows the ``From Good to Great" concept,
collecting multiple potential solutions from both LLMs and their Tool-Augmented
counterparts for the same math problem, and then selecting or re-generating the
most accurate answer after cross-checking these solutions via tool-augmented
interleaf prompting. The framework incorporates two key aspects: self-prompt
and tool-augmented interleaf prompting (TIP). The former allows LLMs to
autonomously refine and improve an initial prompt related to tool usage, while
the latter enables LLMs to derive the final answer by dynamically analyzing the
problem, cross-checking potential solutions, and revising previous reasoning
hints in an interleaved manner. Experimental analysis shows that IMP-TIP
achieves enhanced mathematical capabilities and outperforms traditional LLMs
and tool-augmented LLMs in accuracy and reasoning diversity on math reasoning
tasks. For instance, IMP-TIP can improve Tool-augmented ChatGPT on GSM8K-Hard
from 56.0% to 65.2%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05391">Efficient LLM inference solution on Intel GPU. (arXiv:2401.05391v1 [cs.AR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Hui Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gan_Y/0/1/0/all/0/1">Yi Gan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_F/0/1/0/all/0/1">Feng Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1">Jing Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1">Wei Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yutao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1">Hong Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yuhua Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiaoli Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jinghui Gu</a></p>
<p>Transformer based Large Language Models (LLMs) have been widely used in many
fields, and the efficiency of LLM inference becomes hot topic in real
applications. However, LLMs are usually complicatedly designed in model
structure with massive operations and perform inference in the auto-regressive
mode, making it a challenging task to design a system with high efficiency.
</p>
<p>In this paper, we propose an efficient LLM inference solution with low
latency and high throughput. Firstly, we simplify the LLM decoder layer by
fusing data movement and element-wise operations to reduce the memory access
frequency and lower system latency. We also propose a segment KV cache policy
to keep key/value of the request and response tokens in separate physical
memory for effective device memory management, helping enlarge the runtime
batch size and improve system throughput. A customized
Scaled-Dot-Product-Attention kernel is designed to match our fusion policy
based on the segment KV cache solution. We implement our LLM inference solution
on Intel GPU and publish it publicly. Compared with the standard HuggingFace
implementation, the proposed solution achieves up to 7x lower token latency and
27x higher throughput for some popular LLMs on Intel GPU.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05392">AT-2FF: Adaptive Type-2 Fuzzy Filter for De-noising Images Corrupted with Salt-and-Pepper. (arXiv:2401.05392v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singh_V/0/1/0/all/0/1">Vikas Singh</a></p>
<p>Noise is inevitably common in digital images, leading to visual image
deterioration. Therefore, a suitable filtering method is required to lessen the
noise while preserving the image features (edges, corners, etc.). This paper
presents the efficient type-2 fuzzy weighted mean filter with an adaptive
threshold to remove the SAP noise. The present filter has two primary steps:
The first stage categorizes images as lightly, medium, and heavily corrupted
based on an adaptive threshold by comparing the M-ALD of processed pixels with
the upper and lower MF of the type-2 fuzzy identifier. The second stage
eliminates corrupted pixels by computing the appropriate weight using GMF with
the mean and variance of the uncorrupted pixels in the filter window.
Simulation results vividly show that the obtained denoised images preserve
image features, i.e., edges, corners, and other sharp structures, compared with
different filtering methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05395">SRNI-CAR: A comprehensive dataset for analyzing the Chinese automotive market. (arXiv:2401.05395v1 [econ.GN])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/econ/1/au:+Ding_R/0/1/0/all/0/1">Ruixin Ding</a>, <a href="http://arxiv.org/find/econ/1/au:+Chen_B/0/1/0/all/0/1">Bowei Chen</a>, <a href="http://arxiv.org/find/econ/1/au:+Wilson_J/0/1/0/all/0/1">James M. Wilson</a>, <a href="http://arxiv.org/find/econ/1/au:+Yan_Z/0/1/0/all/0/1">Zhi Yan</a>, <a href="http://arxiv.org/find/econ/1/au:+Huang_Y/0/1/0/all/0/1">Yufei Huang</a></p>
<p>The automotive industry plays a critical role in the global economy, and
particularly important is the expanding Chinese automobile market due to its
immense scale and influence. However, existing automotive sector datasets are
limited in their coverage, failing to adequately consider the growing demand
for more and diverse variables. This paper aims to bridge this data gap by
introducing a comprehensive dataset spanning the years from 2016 to 2022,
encompassing sales data, online reviews, and a wealth of information related to
the Chinese automotive industry. This dataset serves as a valuable resource,
significantly expanding the available data. Its impact extends to various
dimensions, including improving forecasting accuracy, expanding the scope of
business applications, informing policy development and regulation, and
advancing academic research within the automotive sector. To illustrate the
dataset's potential applications in both business and academic contexts, we
present two application examples. Our developed dataset enhances our
understanding of the Chinese automotive market and offers a valuable tool for
researchers, policymakers, and industry stakeholders worldwide.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05398">GeoAI in Social Science. (arXiv:2401.05398v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wenwen Li</a></p>
<p>GeoAI, or geospatial artificial intelligence, is an exciting new area that
leverages artificial intelligence (AI), geospatial big data, and massive
computing power to solve problems with high automation and intelligence. This
paper reviews the progress of AI in social science research, highlighting
important advancements in using GeoAI to fill critical data and knowledge gaps.
It also discusses the importance of breaking down data silos, accelerating
convergence among GeoAI research methods, as well as moving GeoAI beyond
geospatial benefits.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05399">Automated Assessment of Students&#x27; Code Comprehension using LLMs. (arXiv:2401.05399v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oli_P/0/1/0/all/0/1">Priti Oli</a>, <a href="http://arxiv.org/find/cs/1/au:+Banjade_R/0/1/0/all/0/1">Rabin Banjade</a>, <a href="http://arxiv.org/find/cs/1/au:+Chapagain_J/0/1/0/all/0/1">Jeevan Chapagain</a>, <a href="http://arxiv.org/find/cs/1/au:+Rus_V/0/1/0/all/0/1">Vasile Rus</a></p>
<p>Assessing student's answers and in particular natural language answers is a
crucial challenge in the field of education. Advances in machine learning,
including transformer-based models such as Large Language Models(LLMs), have
led to significant progress in various natural language tasks. Nevertheless,
amidst the growing trend of evaluating LLMs across diverse tasks, evaluating
LLMs in the realm of automated answer assesment has not received much
attention. To address this gap, we explore the potential of using LLMs for
automated assessment of student's short and open-ended answer. Particularly, we
use LLMs to compare students' explanations with expert explanations in the
context of line-by-line explanations of computer programs.
</p>
<p>For comparison purposes, we assess both Large Language Models (LLMs) and
encoder-based Semantic Textual Similarity (STS) models in the context of
assessing the correctness of students' explanation of computer code. Our
findings indicate that LLMs, when prompted in few-shot and chain-of-thought
setting perform comparable to fine-tuned encoder-based models in evaluating
students' short answers in programming domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05400">Collaborative Learning with Artificial Intelligence Speakers (CLAIS): Pre-Service Elementary Science Teachers&#x27; Responses to the Prototype. (arXiv:2401.05400v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1">Gyeong-Geon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Mun_S/0/1/0/all/0/1">Seonyeong Mun</a>, <a href="http://arxiv.org/find/cs/1/au:+Shin_M/0/1/0/all/0/1">Myeong-Kyeong Shin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1">Xiaoming Zhai</a></p>
<p>This research aims to demonstrate that AI can function not only as a tool for
learning, but also as an intelligent agent with which humans can engage in
collaborative learning (CL) to change epistemic practices in science
classrooms. We adopted a design and development research approach, following
the Analysis, Design, Development, Implementation and Evaluation (ADDIE) model,
to prototype a tangible instructional system called Collaborative Learning with
AI Speakers (CLAIS). The CLAIS system is designed to have 3-4 human learners
join an AI speaker to form a small group, where humans and AI are considered as
peers participating in the Jigsaw learning process. The development was carried
out using the NUGU AI speaker platform. The CLAIS system was successfully
implemented in a Science Education course session with 15 pre-service
elementary science teachers. The participants evaluated the CLAIS system
through mixed methods surveys as teachers, learners, peers, and users.
Quantitative data showed that the participants' Intelligent-Technological,
Pedagogical, And Content Knowledge was significantly increased after the CLAIS
session, the perception of the CLAIS learning experience was positive, the peer
assessment on AI speakers and human peers was different, and the user
experience was ambivalent. Qualitative data showed that the participants
anticipated future changes in the epistemic process in science classrooms,
while acknowledging technical issues such as speech recognition performance and
response latency. This study highlights the potential of Human-AI Collaboration
for knowledge co-construction in authentic classroom settings and exemplify how
AI could shape the future landscape of epistemic practices in the classroom.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05401">Domain Similarity-Perceived Label Assignment for Domain Generalized Underwater Object Detection. (arXiv:2401.05401v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xisheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_P/0/1/0/all/0/1">Pinhao Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Mingjun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jie Zhou</a></p>
<p>The inherent characteristics and light fluctuations of water bodies give rise
to the huge difference between different layers and regions in underwater
environments. When the test set is collected in a different marine area from
the training set, the issue of domain shift emerges, significantly compromising
the model's ability to generalize. The Domain Adversarial Learning (DAL)
training strategy has been previously utilized to tackle such challenges.
However, DAL heavily depends on manually one-hot domain labels, which implies
no difference among the samples in the same domain. Such an assumption results
in the instability of DAL. This paper introduces the concept of Domain
Similarity-Perceived Label Assignment (DSP). The domain label for each image is
regarded as its similarity to the specified domains. Through domain-specific
data augmentation techniques, we achieved state-of-the-art results on the
underwater cross-domain object detection benchmark S-UODAC2020. Furthermore, we
validated the effectiveness of our method in the Cityscapes dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05402">Vector Field Oriented Diffusion Model for Crystal Material Generation. (arXiv:2401.05402v1 [cond-mat.mtrl-sci])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Klipfel_A/0/1/0/all/0/1">Astrid Klipfel</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Fregier_Y/0/1/0/all/0/1">Ya&#xeb;l Fregier</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Sayede_A/0/1/0/all/0/1">Adlane Sayede</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Bouraoui_Z/0/1/0/all/0/1">Zied Bouraoui</a></p>
<p>Discovering crystal structures with specific chemical properties has become
an increasingly important focus in material science. However, current models
are limited in their ability to generate new crystal lattices, as they only
consider atomic positions or chemical composition. To address this issue, we
propose a probabilistic diffusion model that utilizes a geometrically
equivariant GNN to consider atomic positions and crystal lattices jointly. To
evaluate the effectiveness of our model, we introduce a new generation metric
inspired by Frechet Inception Distance, but based on GNN energy prediction
rather than InceptionV3 used in computer vision. In addition to commonly used
metrics like validity, which assesses the plausibility of a structure, this new
metric offers a more comprehensive evaluation of our model's capabilities. Our
experiments on existing benchmarks show the significance of our diffusion
model. We also show that our method can effectively learn meaningful
representations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05403">The Key Artificial Intelligence Technologies in Early Childhood Education: A Review. (arXiv:2401.05403v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Honghu_Y/0/1/0/all/0/1">Yi Honghu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ting_L/0/1/0/all/0/1">Liu Ting</a>, <a href="http://arxiv.org/find/cs/1/au:+Gongjin_L/0/1/0/all/0/1">Lan Gongjin</a></p>
<p>Artificial Intelligence (AI) technologies have been applied in various
domains, including early childhood education (ECE). Integration of AI
educational technology is a recent significant trend in ECE. Currently, there
are more and more studies of AI in ECE. To date, there is a lack of survey
articles that discuss the studies of AI in ECE. In this paper, we provide an
up-to-date and in-depth overview of the key AI technologies in ECE that
provides a historical perspective, summarizes the representative works,
outlines open questions, discusses the trends and challenges through a detailed
bibliometric analysis, and provides insightful recommendations for future
research. We mainly discuss the studies that apply AI-based robots and AI
technologies to ECE, including improving the social interaction of children
with an autism spectrum disorder. This paper significantly contributes to
provide an up-to-date and in-depth survey that is suitable as introductory
material for beginners to AI in ECE, as well as supplementary material for
advanced users.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05406">RFRL Gym: A Reinforcement Learning Testbed for Cognitive Radio Applications. (arXiv:2401.05406v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Rosen_D/0/1/0/all/0/1">Daniel Rosen</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Rochez_I/0/1/0/all/0/1">Illa Rochez</a> (1), <a href="http://arxiv.org/find/eess/1/au:+McIrvin_C/0/1/0/all/0/1">Caleb McIrvin</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1">Joshua Lee</a> (1), <a href="http://arxiv.org/find/eess/1/au:+DAlessandro_K/0/1/0/all/0/1">Kevin D&#x27;Alessandro</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Wiecek_M/0/1/0/all/0/1">Max Wiecek</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Hoang_N/0/1/0/all/0/1">Nhan Hoang</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Saffarini_R/0/1/0/all/0/1">Ramzy Saffarini</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Philips_S/0/1/0/all/0/1">Sam Philips</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Jones_V/0/1/0/all/0/1">Vanessa Jones</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Ivey_W/0/1/0/all/0/1">Will Ivey</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Harris_Smart_Z/0/1/0/all/0/1">Zavier Harris-Smart</a> (2), <a href="http://arxiv.org/find/eess/1/au:+Harris_Smart_Z/0/1/0/all/0/1">Zavion Harris-Smart</a> (2), <a href="http://arxiv.org/find/eess/1/au:+Chin_Z/0/1/0/all/0/1">Zayden Chin</a> (2), <a href="http://arxiv.org/find/eess/1/au:+Johnson_A/0/1/0/all/0/1">Amos Johnson</a> (2), <a href="http://arxiv.org/find/eess/1/au:+Jones_A/0/1/0/all/0/1">Alyse M. Jones</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Headley_W/0/1/0/all/0/1">William C. Headley</a> (1) ((1) Virginia Tech, (2) Morehouse College)</p>
<p>Radio Frequency Reinforcement Learning (RFRL) is anticipated to be a widely
applicable technology in the next generation of wireless communication systems,
particularly 6G and next-gen military communications. Given this, our research
is focused on developing a tool to promote the development of RFRL techniques
that leverage spectrum sensing. In particular, the tool was designed to address
two cognitive radio applications, specifically dynamic spectrum access and
jamming. In order to train and test reinforcement learning (RL) algorithms for
these applications, a simulation environment is necessary to simulate the
conditions that an agent will encounter within the Radio Frequency (RF)
spectrum. In this paper, such an environment has been developed, herein
referred to as the RFRL Gym. Through the RFRL Gym, users can design their own
scenarios to model what an RL agent may encounter within the RF spectrum as
well as experiment with different spectrum sensing techniques. Additionally,
the RFRL Gym is a subclass of OpenAI gym, enabling the use of third-party ML/RL
Libraries. We plan to open-source this codebase to enable other researchers to
utilize the RFRL Gym to test their own scenarios and RL algorithms, ultimately
leading to the advancement of RL research in the wireless communications
domain. This paper describes in further detail the components of the Gym,
results from example scenarios, and plans for future additions.
</p>
<p>Index Terms-machine learning, reinforcement learning, wireless
communications, dynamic spectrum access, OpenAI gym
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05409">Image-based Data Representations of Time Series: A Comparative Analysis in EEG Artifact Detection. (arXiv:2401.05409v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Maiwald_A/0/1/0/all/0/1">Aaron Maiwald</a>, <a href="http://arxiv.org/find/eess/1/au:+Ackermann_L/0/1/0/all/0/1">Leon Ackermann</a>, <a href="http://arxiv.org/find/eess/1/au:+Kalcher_M/0/1/0/all/0/1">Maximilian Kalcher</a>, <a href="http://arxiv.org/find/eess/1/au:+Wu_D/0/1/0/all/0/1">Daniel J. Wu</a></p>
<p>Alternative data representations are powerful tools that augment the
performance of downstream models. However, there is an abundance of such
representations within the machine learning toolbox, and the field lacks a
comparative understanding of the suitability of each representation method.
</p>
<p>In this paper, we propose artifact detection and classification within EEG
data as a testbed for profiling image-based data representations of time series
data. We then evaluate eleven popular deep learning architectures on each of
six commonly-used representation methods.
</p>
<p>We find that, while the choice of representation entails a choice within the
tradeoff between bias and variance, certain representations are practically
more effective in highlighting features which increase the signal-to-noise
ratio of the data. We present our results on EEG data, and open-source our
testing framework to enable future comparative analyses in this vein.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05412">Spatial-Related Sensors Matters: 3D Human Motion Reconstruction Assisted with Textual Semantics. (arXiv:2401.05412v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xueyuan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_C/0/1/0/all/0/1">Chao Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ban_X/0/1/0/all/0/1">Xiaojuan Ban</a></p>
<p>Leveraging wearable devices for motion reconstruction has emerged as an
economical and viable technique. Certain methodologies employ sparse Inertial
Measurement Units (IMUs) on the human body and harness data-driven strategies
to model human poses. However, the reconstruction of motion based solely on
sparse IMUs data is inherently fraught with ambiguity, a consequence of
numerous identical IMU readings corresponding to different poses. In this
paper, we explore the spatial importance of multiple sensors, supervised by
text that describes specific actions. Specifically, uncertainty is introduced
to derive weighted features for each IMU. We also design a Hierarchical
Temporal Transformer (HTT) and apply contrastive learning to achieve precise
temporal and feature alignment of sensor data with textual semantics.
Experimental results demonstrate our proposed approach achieves significant
improvements in multiple metrics compared to existing methods. Notably, with
textual supervision, our method not only differentiates between ambiguous
actions such as sitting and standing but also produces more precise and natural
motion.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05416">Wavelet Dynamic Selection Network for Inertial Sensor Signal Enhancement. (arXiv:2401.05416v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1">Yifeng Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhao_Y/0/1/0/all/0/1">Yi Zhao</a></p>
<p>As attitude and motion sensing components, inertial sensors are widely used
in various portable devices. But the severe errors of inertial sensors restrain
their function, especially the trajectory recovery and semantic recognition. As
a mainstream signal processing method, wavelet is hailed as the mathematical
microscope of signal due to the plentiful and diverse wavelet basis functions.
However, complicated noise types and application scenarios of inertial sensors
make selecting wavelet basis perplexing. To this end, we propose a wavelet
dynamic selection network (WDSNet), which intelligently selects the appropriate
wavelet basis for variable inertial signals. In addition, existing deep
learning architectures excel at extracting features from input data but neglect
to learn the characteristics of target categories, which is essential to
enhance the category awareness capability, thereby improving the selection of
wavelet basis. Therefore, we propose a category representation mechanism (CRM),
which enables the network to extract and represent category features without
increasing trainable parameters. Furthermore, CRM transforms the common fully
connected network into category representations, which provide closer
supervision to the feature extractor than the far and trivial one-hot
classification labels. We call this process of imposing interpretability on a
network and using it to supervise the feature extractor the feature supervision
mechanism, and its effectiveness is demonstrated experimentally and
theoretically in this paper. The enhanced inertial signal can perform
impracticable tasks with regard to the original signal, such as trajectory
reconstruction. Both quantitative and visual results show that WDSNet
outperforms the existing methods. Remarkably, WDSNet, as a weakly-supervised
method, achieves the state-of-the-art performance of all the compared
fully-supervised methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05418">ANALYTiC: Understanding Decision Boundaries and Dimensionality Reduction in Machine Learning. (arXiv:2401.05418v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Haidri_S/0/1/0/all/0/1">Salman Haidri</a></p>
<p>The advent of compact, handheld devices has given us a pool of tracked
movement data that could be used to infer trends and patterns that can be made
to use. With this flooding of various trajectory data of animals, humans,
vehicles, etc., the idea of ANALYTiC originated, using active learning to infer
semantic annotations from the trajectories by learning from sets of labeled
data. This study explores the application of dimensionality reduction and
decision boundaries in combination with the already present active learning,
highlighting patterns and clusters in data. We test these features with three
different trajectory datasets with objective of exploiting the the already
labeled data and enhance their interpretability. Our experimental analysis
exemplifies the potential of these combined methodologies in improving the
efficiency and accuracy of trajectory labeling. This study serves as a
stepping-stone towards the broader integration of machine learning and visual
methods in context of movement data analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05422">Machine Learning (ML)-assisted Beam Management in millimeter (mm)Wave Distributed Multiple Input Multiple Output (D-MIMO) systems. (arXiv:2401.05422v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+M_K/0/1/0/all/0/1">Karthik R M</a>, <a href="http://arxiv.org/find/eess/1/au:+Hegde_D/0/1/0/all/0/1">Dhiraj Nagaraja Hegde</a>, <a href="http://arxiv.org/find/eess/1/au:+Sarajlic_M/0/1/0/all/0/1">Muris Sarajlic</a>, <a href="http://arxiv.org/find/eess/1/au:+Sarkar_A/0/1/0/all/0/1">Abhishek Sarkar</a></p>
<p>Beam management (BM) protocols are critical for establishing and maintaining
connectivity between network radio nodes and User Equipments (UEs). In
Distributed Multiple Input Multiple Output systems (D-MIMO), a number of access
points (APs), coordinated by a central processing unit (CPU), serves a number
of UEs. At mmWave frequencies, the problem of finding the best AP and beam to
serve the UEs is challenging due to a large number of beams that need to be
sounded with Downlink (DL) reference signals. The objective of this paper is to
investigate whether the best AP/beam can be reliably inferred from sounding
only a small subset of beams and leveraging AI/ML for inference of best
beam/AP. We use Random Forest (RF), MissForest (MF) and conditional Generative
Adversarial Networks (c-GAN) for demonstrating the performance benefits of
inference.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05426">CoSS: Co-optimizing Sensor and Sampling Rate for Data-Efficient AI in Human Activity Recognition. (arXiv:2401.05426v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Liu_M/0/1/0/all/0/1">Mengxi Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhao_Z/0/1/0/all/0/1">Zimin Zhao</a>, <a href="http://arxiv.org/find/eess/1/au:+Geissler_D/0/1/0/all/0/1">Daniel Gei&#xdf;ler</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_B/0/1/0/all/0/1">Bo Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Suh_S/0/1/0/all/0/1">Sungho Suh</a>, <a href="http://arxiv.org/find/eess/1/au:+Lukowicz_P/0/1/0/all/0/1">Paul Lukowicz</a></p>
<p>Recent advancements in Artificial Neural Networks have significantly improved
human activity recognition using multiple time-series sensors. While employing
numerous sensors with high-frequency sampling rates usually improves the
results, it often leads to data inefficiency and unnecessary expansion of the
ANN, posing a challenge for their practical deployment on edge devices.
Addressing these issues, our work introduces a pragmatic framework for
data-efficient utilization in HAR tasks, considering the optimization of both
sensor modalities and sampling rate simultaneously. Central to our approach are
the designed trainable parameters, termed 'Weight Scores,' which assess the
significance of each sensor modality and sampling rate during the training
phase. These scores guide the sensor modalities and sampling rate selection.
The pruning method allows users to make a trade-off between computational
budgets and performance by selecting the sensor modalities and sampling rates
according to the weight score ranking. We tested our framework's effectiveness
in optimizing sensor modality and sampling rate selection using three public
HAR benchmark datasets. The results show that the sensor and sampling rate
combination selected via CoSS achieves similar classification performance to
configurations using the highest sampling rate with all sensors but at a
reduced hardware cost.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05431">TRLS: A Time Series Representation Learning Framework via Spectrogram for Medical Signal Processing. (arXiv:2401.05431v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Xie_L/0/1/0/all/0/1">Luyuan Xie</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1">Cong Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1">Xin Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhai_S/0/1/0/all/0/1">Shengfang Zhai</a>, <a href="http://arxiv.org/find/eess/1/au:+Fang_Y/0/1/0/all/0/1">Yuejian Fang</a>, <a href="http://arxiv.org/find/eess/1/au:+Shen_Q/0/1/0/all/0/1">Qingni Shen</a>, <a href="http://arxiv.org/find/eess/1/au:+Wu_Z/0/1/0/all/0/1">Zhonghai Wu</a></p>
<p>Representation learning frameworks in unlabeled time series have been
proposed for medical signal processing. Despite the numerous excellent
progresses have been made in previous works, we observe the representation
extracted for the time series still does not generalize well. In this paper, we
present a Time series (medical signal) Representation Learning framework via
Spectrogram (TRLS) to get more informative representations. We transform the
input time-domain medical signals into spectrograms and design a time-frequency
encoder named Time Frequency RNN (TFRNN) to capture more robust multi-scale
representations from the augmented spectrograms. Our TRLS takes spectrogram as
input with two types of different data augmentations and maximizes the
similarity between positive ones, which effectively circumvents the problem of
designing negative samples. Our evaluation of four real-world medical signal
datasets focusing on medical signal classification shows that TRLS is superior
to the existing frameworks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05432">TEN-GUARD: Tensor Decomposition for Backdoor Attack Detection in Deep Neural Networks. (arXiv:2401.05432v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hossain_K/0/1/0/all/0/1">Khondoker Murad Hossain</a>, <a href="http://arxiv.org/find/cs/1/au:+Oates_T/0/1/0/all/0/1">Tim Oates</a></p>
<p>As deep neural networks and the datasets used to train them get larger, the
default approach to integrating them into research and commercial projects is
to download a pre-trained model and fine tune it. But these models can have
uncertain provenance, opening up the possibility that they embed hidden
malicious behavior such as trojans or backdoors, where small changes to an
input (triggers) can cause the model to produce incorrect outputs (e.g., to
misclassify). This paper introduces a novel approach to backdoor detection that
uses two tensor decomposition methods applied to network activations. This has
a number of advantages relative to existing detection methods, including the
ability to analyze multiple models at the same time, working across a wide
variety of network architectures, making no assumptions about the nature of
triggers used to alter network behavior, and being computationally efficient.
We provide a detailed description of the detection pipeline along with results
on models trained on the MNIST digit dataset, CIFAR-10 dataset, and two
difficult datasets from NIST's TrojAI competition. These results show that our
method detects backdoored networks more accurately and efficiently than current
state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05433">Enhancing Essay Scoring with Adversarial Weights Perturbation and Metric-specific AttentionPooling. (arXiv:2401.05433v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jiaxin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xinyu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Che_C/0/1/0/all/0/1">Chang Che</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1">Qunwei Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bo Liu</a></p>
<p>The objective of this study is to improve automated feedback tools designed
for English Language Learners (ELLs) through the utilization of data science
techniques encompassing machine learning, natural language processing, and
educational data analytics. Automated essay scoring (AES) research has made
strides in evaluating written essays, but it often overlooks the specific needs
of English Language Learners (ELLs) in language development. This study
explores the application of BERT-related techniques to enhance the assessment
of ELLs' writing proficiency within AES.
</p>
<p>To address the specific needs of ELLs, we propose the use of DeBERTa, a
state-of-the-art neural language model, for improving automated feedback tools.
DeBERTa, pretrained on large text corpora using self-supervised learning,
learns universal language representations adaptable to various natural language
understanding tasks. The model incorporates several innovative techniques,
including adversarial training through Adversarial Weights Perturbation (AWP)
and Metric-specific AttentionPooling (6 kinds of AP) for each label in the
competition.
</p>
<p>The primary focus of this research is to investigate the impact of
hyperparameters, particularly the adversarial learning rate, on the performance
of the model. By fine-tuning the hyperparameter tuning process, including the
influence of 6AP and AWP, the resulting models can provide more accurate
evaluations of language proficiency and support tailored learning tasks for
ELLs. This work has the potential to significantly benefit ELLs by improving
their English language proficiency and facilitating their educational journey.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05434">ECGformer: Leveraging transformer for ECG heartbeat arrhythmia classification. (arXiv:2401.05434v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Akan_T/0/1/0/all/0/1">Taymaz Akan</a>, <a href="http://arxiv.org/find/eess/1/au:+Alp_S/0/1/0/all/0/1">Sait Alp</a>, <a href="http://arxiv.org/find/eess/1/au:+Bhuiyan_M/0/1/0/all/0/1">Mohammad Alfrad Nobel Bhuiyan</a></p>
<p>An arrhythmia, also known as a dysrhythmia, refers to an irregular heartbeat.
There are various types of arrhythmias that can originate from different areas
of the heart, resulting in either a rapid, slow, or irregular heartbeat. An
electrocardiogram (ECG) is a vital diagnostic tool used to detect heart
irregularities and abnormalities, allowing experts to analyze the heart's
electrical signals to identify intricate patterns and deviations from the norm.
Over the past few decades, numerous studies have been conducted to develop
automated methods for classifying heartbeats based on ECG data. In recent
years, deep learning has demonstrated exceptional capabilities in tackling
various medical challenges, particularly with transformers as a model
architecture for sequence processing. By leveraging the transformers, we
developed the ECGformer model for the classification of various arrhythmias
present in electrocardiogram data. We assessed the suggested approach using the
MIT-BIH and PTB datasets. ECG heartbeat arrhythmia classification results show
that the proposed method is highly effective.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05437">Representation Learning for Wearable-Based Applications in the Case of Missing Data. (arXiv:2401.05437v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Jungo_J/0/1/0/all/0/1">Janosch Jungo</a>, <a href="http://arxiv.org/find/eess/1/au:+Xiang_Y/0/1/0/all/0/1">Yutong Xiang</a>, <a href="http://arxiv.org/find/eess/1/au:+Gashi_S/0/1/0/all/0/1">Shkurta Gashi</a>, <a href="http://arxiv.org/find/eess/1/au:+Holz_C/0/1/0/all/0/1">Christian Holz</a></p>
<p>Wearable devices continuously collect sensor data and use it to infer an
individual's behavior, such as sleep, physical activity, and emotions. Despite
the significant interest and advancements in this field, modeling multimodal
sensor data in real-world environments is still challenging due to low data
quality and limited data annotations. In this work, we investigate
representation learning for imputing missing wearable data and compare it with
state-of-the-art statistical approaches. We investigate the performance of the
transformer model on 10 physiological and behavioral signals with different
masking ratios. Our results show that transformers outperform baselines for
missing data imputation of signals that change more frequently, but not for
monotonic signals. We further investigate the impact of imputation strategies
and masking rations on downstream classification tasks. Our study provides
insights for the design and development of masking-based self-supervised
learning tasks and advocates the adoption of hybrid-based imputation strategies
to address the challenge of missing data in wearable devices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05442">Functional Graphical Models: Structure Enables Offline Data-Driven Optimization. (arXiv:2401.05442v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kuba_J/0/1/0/all/0/1">Jakub Grudzien Kuba</a>, <a href="http://arxiv.org/find/cs/1/au:+Uehara_M/0/1/0/all/0/1">Masatoshi Uehara</a>, <a href="http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1">Pieter Abbeel</a>, <a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1">Sergey Levine</a></p>
<p>While machine learning models are typically trained to solve prediction
problems, we might often want to use them for optimization problems. For
example, given a dataset of proteins and their corresponding fluorescence
levels, we might want to optimize for a new protein with the highest possible
fluorescence. This kind of data-driven optimization (DDO) presents a range of
challenges beyond those in standard prediction problems, since we need models
that successfully predict the performance of new designs that are better than
the best designs seen in the training set. It is not clear theoretically when
existing approaches can even perform better than the naive approach that simply
selects the best design in the dataset. In this paper, we study how structure
can enable sample-efficient data-driven optimization. To formalize the notion
of structure, we introduce functional graphical models (FGMs) and show
theoretically how they can provide for principled data-driven optimization by
decomposing the original high-dimensional optimization problem into smaller
sub-problems. This allows us to derive much more practical regret bounds for
DDO, and the result implies that DDO with FGMs can achieve nearly optimal
designs in situations where naive approaches fail due to insufficient coverage
of the offline data. We further present a data-driven optimization algorithm
that inferes the FGM structure itself, either over the original input variables
or a latent variable representation of the inputs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05443">LLM4PLC: Harnessing Large Language Models for Verifiable Programming of PLCs in Industrial Control Systems. (arXiv:2401.05443v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fakih_M/0/1/0/all/0/1">Mohamad Fakih</a>, <a href="http://arxiv.org/find/cs/1/au:+Dharmaji_R/0/1/0/all/0/1">Rahul Dharmaji</a>, <a href="http://arxiv.org/find/cs/1/au:+Moghaddas_Y/0/1/0/all/0/1">Yasamin Moghaddas</a>, <a href="http://arxiv.org/find/cs/1/au:+Araya_G/0/1/0/all/0/1">Gustavo Quiros Araya</a>, <a href="http://arxiv.org/find/cs/1/au:+Ogundare_O/0/1/0/all/0/1">Oluwatosin Ogundare</a>, <a href="http://arxiv.org/find/cs/1/au:+Faruque_M/0/1/0/all/0/1">Mohammad Abdullah Al Faruque</a></p>
<p>Although Large Language Models (LLMs) have established pre-dominance in
automated code generation, they are not devoid of shortcomings. The pertinent
issues primarily relate to the absence of execution guarantees for generated
code, a lack of explainability, and suboptimal support for essential but niche
programming languages. State-of-the-art LLMs such as GPT-4 and LLaMa2 fail to
produce valid programs for Industrial Control Systems (ICS) operated by
Programmable Logic Controllers (PLCs). We propose LLM4PLC, a user-guided
iterative pipeline leveraging user feedback and external verification tools
including grammar checkers, compilers and SMV verifiers to guide the LLM's
generation. We further enhance the generation potential of LLM by employing
Prompt Engineering and model fine-tuning through the creation and usage of
LoRAs. We validate this system using a FischerTechnik Manufacturing TestBed
(MFTB), illustrating how LLMs can evolve from generating structurally flawed
code to producing verifiably correct programs for industrial applications. We
run a complete test suite on GPT-3.5, GPT-4, Code Llama-7B, a fine-tuned Code
Llama-7B model, Code Llama-34B, and a fine-tuned Code Llama-34B model. The
proposed pipeline improved the generation success rate from 47% to 72%, and the
Survey-of-Experts code quality from 2.25/10 to 7.75/10. To promote open
research, we share the complete experimental setup, the LLM Fine-Tuning
Weights, and the video demonstrations of the different programs on our
dedicated webpage.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05444">Fully Spiking Actor Network with Intra-layer Connections for Reinforcement Learning. (arXiv:2401.05444v1 [cs.NE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Ding Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_P/0/1/0/all/0/1">Peixi Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1">Tiejun Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yonghong Tian</a></p>
<p>With the help of special neuromorphic hardware, spiking neural networks
(SNNs) are expected to realize artificial intelligence (AI) with less energy
consumption. It provides a promising energy-efficient way for realistic control
tasks by combining SNNs with deep reinforcement learning (DRL). In this paper,
we focus on the task where the agent needs to learn multi-dimensional
deterministic policies to control, which is very common in real scenarios.
Recently, the surrogate gradient method has been utilized for training
multi-layer SNNs, which allows SNNs to achieve comparable performance with the
corresponding deep networks in this task. Most existing spike-based RL methods
take the firing rate as the output of SNNs, and convert it to represent
continuous action space (i.e., the deterministic policy) through a
fully-connected (FC) layer. However, the decimal characteristic of the firing
rate brings the floating-point matrix operations to the FC layer, making the
whole SNN unable to deploy on the neuromorphic hardware directly. To develop a
fully spiking actor network without any floating-point matrix operations, we
draw inspiration from the non-spiking interneurons found in insects and employ
the membrane voltage of the non-spiking neurons to represent the action. Before
the non-spiking neurons, multiple population neurons are introduced to decode
different dimensions of actions. Since each population is used to decode a
dimension of action, we argue that the neurons in each population should be
connected in time domain and space domain. Hence, the intra-layer connections
are used in output populations to enhance the representation capacity. Finally,
we propose a fully spiking actor network with intra-layer connections
(ILC-SAN).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05446">Self-supervised Learning for Electroencephalogram: A Systematic Survey. (arXiv:2401.05446v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Weng_W/0/1/0/all/0/1">Weining Weng</a>, <a href="http://arxiv.org/find/eess/1/au:+Gu_Y/0/1/0/all/0/1">Yang Gu</a>, <a href="http://arxiv.org/find/eess/1/au:+Guo_S/0/1/0/all/0/1">Shuai Guo</a>, <a href="http://arxiv.org/find/eess/1/au:+Ma_Y/0/1/0/all/0/1">Yuan Ma</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_Z/0/1/0/all/0/1">Zhaohua Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1">Yuchen Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1">Yiqiang Chen</a></p>
<p>Electroencephalogram (EEG) is a non-invasive technique to record
bioelectrical signals. Integrating supervised deep learning techniques with EEG
signals has recently facilitated automatic analysis across diverse EEG-based
tasks. However, the label issues of EEG signals have constrained the
development of EEG-based deep models. Obtaining EEG annotations is difficult
that requires domain experts to guide collection and labeling, and the
variability of EEG signals among different subjects causes significant label
shifts. To solve the above challenges, self-supervised learning (SSL) has been
proposed to extract representations from unlabeled samples through
well-designed pretext tasks. This paper concentrates on integrating SSL
frameworks with temporal EEG signals to achieve efficient representation and
proposes a systematic review of the SSL for EEG signals. In this paper, 1) we
introduce the concept and theory of self-supervised learning and typical SSL
frameworks. 2) We provide a comprehensive review of SSL for EEG analysis,
including taxonomy, methodology, and technique details of the existing
EEG-based SSL frameworks, and discuss the difference between these methods. 3)
We investigate the adaptation of the SSL approach to various downstream tasks,
including the task description and related benchmark datasets. 4) Finally, we
discuss the potential directions for future SSL-EEG research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05447">Can ChatGPT Compute Trustworthy Sentiment Scores from Bloomberg Market Wraps?. (arXiv:2401.05447v1 [q-fin.ST])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+Lefort_B/0/1/0/all/0/1">Baptiste Lefort</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Benhamou_E/0/1/0/all/0/1">Eric Benhamou</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Ohana_J/0/1/0/all/0/1">Jean-Jacques Ohana</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Saltiel_D/0/1/0/all/0/1">David Saltiel</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Guez_B/0/1/0/all/0/1">Beatrice Guez</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Challet_D/0/1/0/all/0/1">Damien Challet</a></p>
<p>We used a dataset of daily Bloomberg Financial Market Summaries from 2010 to
2023, reposted on large financial media, to determine how global news headlines
may affect stock market movements using ChatGPT and a two-stage prompt
approach. We document a statistically significant positive correlation between
the sentiment score and future equity market returns over short to medium term,
which reverts to a negative correlation over longer horizons. Validation of
this correlation pattern across multiple equity markets indicates its
robustness across equity regions and resilience to non-linearity, evidenced by
comparison of Pearson and Spearman correlations. Finally, we provide an
estimate of the optimal horizon that strikes a balance between reactivity to
new information and correlation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05453">Dimensionality-Aware Outlier Detection: Theoretical and Experimental Analysis. (arXiv:2401.05453v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Anderberg_A/0/1/0/all/0/1">Alastair Anderberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Bailey_J/0/1/0/all/0/1">James Bailey</a>, <a href="http://arxiv.org/find/cs/1/au:+Campello_R/0/1/0/all/0/1">Ricardo J. G. B. Campello</a>, <a href="http://arxiv.org/find/cs/1/au:+Houle_M/0/1/0/all/0/1">Michael E. Houle</a>, <a href="http://arxiv.org/find/cs/1/au:+Marques_H/0/1/0/all/0/1">Henrique O. Marques</a>, <a href="http://arxiv.org/find/cs/1/au:+Radovanovic_M/0/1/0/all/0/1">Milo&#x161; Radovanovi&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Zimek_A/0/1/0/all/0/1">Arthur Zimek</a></p>
<p>We present a nonparametric method for outlier detection that takes full
account of local variations in intrinsic dimensionality within the dataset.
Using the theory of Local Intrinsic Dimensionality (LID), our
'dimensionality-aware' outlier detection method, DAO, is derived as an
estimator of an asymptotic local expected density ratio involving the query
point and a close neighbor drawn at random. The dimensionality-aware behavior
of DAO is due to its use of local estimation of LID values in a
theoretically-justified way. Through comprehensive experimentation on more than
800 synthetic and real datasets, we show that DAO significantly outperforms
three popular and important benchmark outlier detection methods: Local Outlier
Factor (LOF), Simplified LOF, and kNN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05458">CoLafier: Collaborative Noisy Label Purifier With Local Intrinsic Dimensionality Guidance. (arXiv:2401.05458v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dongyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1">Ruofan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Rundensteiner_E/0/1/0/all/0/1">Elke Rundensteiner</a></p>
<p>Deep neural networks (DNNs) have advanced many machine learning tasks, but
their performance is often harmed by noisy labels in real-world data.
Addressing this, we introduce CoLafier, a novel approach that uses Local
Intrinsic Dimensionality (LID) for learning with noisy labels. CoLafier
consists of two subnets: LID-dis and LID-gen. LID-dis is a specialized
classifier. Trained with our uniquely crafted scheme, LID-dis consumes both a
sample's features and its label to predict the label - which allows it to
produce an enhanced internal representation. We observe that LID scores
computed from this representation effectively distinguish between correct and
incorrect labels across various noise scenarios. In contrast to LID-dis,
LID-gen, functioning as a regular classifier, operates solely on the sample's
features. During training, CoLafier utilizes two augmented views per instance
to feed both subnets. CoLafier considers the LID scores from the two views as
produced by LID-dis to assign weights in an adapted loss function for both
subnets. Concurrently, LID-gen, serving as classifier, suggests pseudo-labels.
LID-dis then processes these pseudo-labels along with two views to derive LID
scores. Finally, these LID scores along with the differences in predictions
from the two subnets guide the label update decisions. This dual-view and
dual-subnet approach enhances the overall reliability of the framework. Upon
completion of the training, we deploy the LID-gen subnet of CoLafier as the
final classification model. CoLafier demonstrates improved prediction accuracy,
surpassing existing methods, particularly under severe label noise. For more
details, see the code at https://github.com/zdy93/CoLafier.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05459">Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security. (arXiv:2401.05459v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuanchun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1">Hao Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Weijun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiangyu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1">Yizhen Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1">Guohong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiacheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Wenxing Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yi Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Kong_R/0/1/0/all/0/1">Rui Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yile Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Geng_H/0/1/0/all/0/1">Hanfei Geng</a>, <a href="http://arxiv.org/find/cs/1/au:+Luan_J/0/1/0/all/0/1">Jian Luan</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1">Xuefeng Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1">Zilong Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_G/0/1/0/all/0/1">Guanjing Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1">Fan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1">Mengwei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhijun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Peng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Ya-Qin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yunxin Liu</a></p>
<p>Since the advent of personal computing devices, intelligent personal
assistants (IPAs) have been one of the key technologies that researchers and
engineers have focused on, aiming to help users efficiently obtain information
and execute tasks, and provide users with more intelligent, convenient, and
rich interaction experiences. With the development of smartphones and IoT,
computing and sensing devices have become ubiquitous, greatly expanding the
boundaries of IPAs. However, due to the lack of capabilities such as user
intent understanding, task planning, tool using, and personal data management
etc., existing IPAs still have limited practicality and scalability. Recently,
the emergence of foundation models, represented by large language models
(LLMs), brings new opportunities for the development of IPAs. With the powerful
semantic understanding and reasoning capabilities, LLM can enable intelligent
agents to solve complex problems autonomously. In this paper, we focus on
Personal LLM Agents, which are LLM-based agents that are deeply integrated with
personal data and personal devices and used for personal assistance. We
envision that Personal LLM Agents will become a major software paradigm for
end-users in the upcoming era. To realize this vision, we take the first step
to discuss several important questions about Personal LLM Agents, including
their architecture, capability, efficiency and security. We start by
summarizing the key components and design choices in the architecture of
Personal LLM Agents, followed by an in-depth analysis of the opinions collected
from domain experts. Next, we discuss several key challenges to achieve
intelligent, efficient and secure Personal LLM Agents, followed by a
comprehensive survey of representative solutions to address these challenges.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05461">The two-way knowledge interaction interface between humans and neural networks. (arXiv:2401.05461v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1">Zhanliang He</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_N/0/1/0/all/0/1">Nuoye Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hongsheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_P/0/1/0/all/0/1">Peiyi Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1">Guangming Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Liang Zhang</a></p>
<p>Despite neural networks (NN) have been widely applied in various fields and
generally outperforms humans, they still lack interpretability to a certain
extent, and humans are unable to intuitively understand the decision logic of
NN. This also hinders the knowledge interaction between humans and NN,
preventing humans from getting involved to give direct guidance when NN's
decisions go wrong. While recent research in explainable AI has achieved
interpretability of NN from various perspectives, it has not yet provided
effective methods for knowledge exchange between humans and NN. To address this
problem, we constructed a two-way interaction interface that uses structured
representations of visual concepts and their relationships as the "language"
for knowledge exchange between humans and NN. Specifically, NN provide
intuitive reasoning explanations to humans based on the class-specific
structural concepts graph (C-SCG). On the other hand, humans can modify the
biases present in the C-SCG through their prior knowledge and reasoning
ability, and thus provide direct knowledge guidance to NN through this
interface. Through experimental validation, based on this interaction
interface, NN can provide humans with easily understandable explanations of the
reasoning process. Furthermore, human involvement and prior knowledge can
directly and effectively contribute to enhancing the performance of NN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05467">Machine Teaching for Building Modular AI Agents based on Zero-shot Learners. (arXiv:2401.05467v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Taneja_K/0/1/0/all/0/1">Karan Taneja</a>, <a href="http://arxiv.org/find/cs/1/au:+Goel_A/0/1/0/all/0/1">Ashok Goel</a></p>
<p>The recent advances in large language models (LLMs) have led to the creation
of many modular AI agents. These agents employ LLMs as zero-shot learners to
perform sub-tasks in order to solve complex tasks set forth by human users. We
propose an approach to enhance the robustness and performance of modular AI
agents that utilize LLMs as zero-shot learners. Our iterative machine teaching
method offers an efficient way to teach AI agents over time with limited human
feedback, addressing the limit posed by the quality of zero-shot learning. We
advocate leveraging the data traces from initial deployments and outputs or
annotations from the zero-shot learners to train smaller and task-specific
substitute models which can reduce both the monetary costs and environmental
impact. Our machine teaching process avails human expertise to correct examples
with a high likelihood of misannotations. Results on three tasks, common to
conversational AI agents, show that close-to-oracle performance can be achieved
with supervision on 20-70% of the dataset depending upon the complexity of the
task and performance of zero-shot learners.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05468">Introducing New Node Prediction in Graph Mining: Predicting All Links from Isolated Nodes with Graph Neural Networks. (arXiv:2401.05468v1 [cs.SI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zanardini_D/0/1/0/all/0/1">Damiano Zanardini</a>, <a href="http://arxiv.org/find/cs/1/au:+Serrano_E/0/1/0/all/0/1">Emilio Serrano</a></p>
<p>This paper introduces a new problem in the field of graph mining and social
network analysis called new node prediction. More technically, the task can be
categorized as zero-shot out-of-graph all-links prediction. This challenging
problem aims to predict all links from a new, isolated, and unobserved node
that was previously disconnected from the graph. Unlike classic approaches to
link prediction (including few-shot out-of-graph link prediction), this problem
presents two key differences: (1) the new node has no existing links from which
to extract patterns for new predictions; and (2) the goal is to predict not
just one, but all the links of this new node, or at least a significant part of
them. Experiments demonstrate that an architecture based on Deep Graph Neural
Networks can learn to solve this challenging problem in a bibliographic
citation network.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05476">CADgpt: Harnessing Natural Language Processing for 3D Modelling to Enhance Computer-Aided Design Workflows. (arXiv:2401.05476v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kapsalis_T/0/1/0/all/0/1">Timo Kapsalis</a></p>
<p>This paper introduces CADgpt, an innovative plugin integrating Natural
Language Processing (NLP) with Rhino3D for enhancing 3D modelling in
computer-aided design (CAD) environments. Leveraging OpenAI's GPT-4, CADgpt
simplifies the CAD interface, enabling users, particularly beginners, to
perform complex 3D modelling tasks through intuitive natural language commands.
This approach significantly reduces the learning curve associated with
traditional CAD software, fostering a more inclusive and engaging educational
environment. The paper discusses CADgpt's technical architecture, including its
integration within Rhino3D and the adaptation of GPT-4 capabilities for CAD
tasks. It presents case studies demonstrating CADgpt's efficacy in various
design scenarios, highlighting its potential to democratise design education by
making sophisticated design tools accessible to a broader range of students.
The discussion further explores CADgpt's implications for pedagogy and
curriculum development, emphasising its role in enhancing creative exploration
and conceptual thinking in design education.
</p>
<p>Keywords: Natural Language Processing, Computer-Aided Design, 3D Modelling,
Design Automation, Design Education, Architectural Education
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05477">Standardizing Your Training Process for Human Activity Recognition Models: A Comprehensive Review in the Tunable Factors. (arXiv:2401.05477v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yiran Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Haibin Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yexu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Riedel_T/0/1/0/all/0/1">Till Riedel</a>, <a href="http://arxiv.org/find/cs/1/au:+Beigl_M/0/1/0/all/0/1">Michael Beigl</a></p>
<p>In recent years, deep learning has emerged as a potent tool across a
multitude of domains, leading to a surge in research pertaining to its
application in the wearable human activity recognition (WHAR) domain. Despite
the rapid development, concerns have been raised about the lack of
standardization and consistency in the procedures used for experimental model
training, which may affect the reproducibility and reliability of research
results. In this paper, we provide an exhaustive review of contemporary deep
learning research in the field of WHAR and collate information pertaining to
the training procedure employed in various studies. Our findings suggest that a
major trend is the lack of detail provided by model training protocols.
Besides, to gain a clearer understanding of the impact of missing descriptions,
we utilize a control variables approach to assess the impact of key tunable
components (e.g., optimization techniques and early stopping criteria) on the
inter-subject generalization capabilities of HAR models. With insights from the
analyses, we define a novel integrated training procedure tailored to the WHAR
model. Empirical results derived using five well-known \ac{whar} benchmark
datasets and three classical HAR model architectures demonstrate the
effectiveness of our proposed methodology: in particular, there is a
significant improvement in macro F1 leave one subject out cross-validation
performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05478">Population Graph Cross-Network Node Classification for Autism Detection Across Sample Groups. (arXiv:2401.05478v1 [cs.SI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stephens_A/0/1/0/all/0/1">Anna Stephens</a>, <a href="http://arxiv.org/find/cs/1/au:+Santos_F/0/1/0/all/0/1">Francisco Santos</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_P/0/1/0/all/0/1">Pang-Ning Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Esfahanian_A/0/1/0/all/0/1">Abdol-Hossein Esfahanian</a></p>
<p>Graph neural networks (GNN) are a powerful tool for combining imaging and
non-imaging medical information for node classification tasks. Cross-network
node classification extends GNN techniques to account for domain drift,
allowing for node classification on an unlabeled target network. In this paper
we present OTGCN, a powerful, novel approach to cross-network node
classification. This approach leans on concepts from graph convolutional
networks to harness insights from graph data structures while simultaneously
applying strategies rooted in optimal transport to correct for the domain drift
that can occur between samples from different data collection sites. This
blended approach provides a practical solution for scenarios with many distinct
forms of data collected across different locations and equipment. We
demonstrate the effectiveness of this approach at classifying Autism Spectrum
Disorder subjects using a blend of imaging and non-imaging data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05502">Diversity-aware clustering: Computational Complexity and Approximation Algorithms. (arXiv:2401.05502v1 [cs.DS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Thejaswi_S/0/1/0/all/0/1">Suhas Thejaswi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gadekar_A/0/1/0/all/0/1">Ameet Gadekar</a>, <a href="http://arxiv.org/find/cs/1/au:+Ordozgoiti_B/0/1/0/all/0/1">Bruno Ordozgoiti</a>, <a href="http://arxiv.org/find/cs/1/au:+Gionis_A/0/1/0/all/0/1">Aristides Gionis</a></p>
<p>In this work, we study diversity-aware clustering problems where the data
points are associated with multiple attributes resulting in intersecting
groups. A clustering solution need to ensure that a minimum number of cluster
centers are chosen from each group while simultaneously minimizing the
clustering objective, which can be either $k$-median, $k$-means or
$k$-supplier. We present parameterized approximation algorithms with
approximation ratios $1+ \frac{2}{e}$, $1+\frac{8}{e}$ and $3$ for
diversity-aware $k$-median, diversity-aware $k$-means and diversity-aware
$k$-supplier, respectively. The approximation ratios are tight assuming Gap-ETH
and FPT $\neq$ W[2]. For fair $k$-median and fair $k$-means with disjoint
faicility groups, we present parameterized approximation algorithm with
approximation ratios $1+\frac{2}{e}$ and $1+\frac{8}{e}$, respectively. For
fair $k$-supplier with disjoint facility groups, we present a polynomial-time
approximation algorithm with factor $3$, improving the previous best known
approximation ratio of factor $5$.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05507">InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks. (arXiv:2401.05507v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xueyu Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Ziyu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1">Shuang Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Chai_Z/0/1/0/all/0/1">Ziwei Chai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guoyin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xuwu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1">Jing Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jingjing Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1">Ming Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1">Yao Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1">Jianbo Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1">Kun Kuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Hongxia Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1">Fei Wu</a></p>
<p>In this paper, we introduce "InfiAgent-DABench", the first benchmark
specifically designed to evaluate LLM-based agents in data analysis tasks. This
benchmark contains DAEval, a dataset consisting of 311 data analysis questions
derived from 55 CSV files, and an agent framework to evaluate LLMs as data
analysis agents. We adopt a format-prompting technique, ensuring questions to
be closed-form that can be automatically evaluated. Our extensive benchmarking
of 23 state-of-the-art LLMs uncovers the current challenges encountered in data
analysis tasks. In addition, we have developed DAAgent, a specialized agent
trained on instruction-tuning datasets. Evaluation datasets and toolkits for
InfiAgent-DABench are released at https://github.com/InfiAgent/InfiAgent.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05509">Optimized Ensemble Model Towards Secured Industrial IoT Devices. (arXiv:2401.05509v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Injadat_M/0/1/0/all/0/1">MohammadNoor Injadat</a></p>
<p>The continued growth in the deployment of Internet-of-Things (IoT) devices
has been fueled by the increased connectivity demand, particularly in
industrial environments. However, this has led to an increase in the number of
network related attacks due to the increased number of potential attack
surfaces. Industrial IoT (IIoT) devices are prone to various network related
attacks that can have severe consequences on the manufacturing process as well
as on the safety of the workers in the manufacturing plant. One promising
solution that has emerged in recent years for attack detection is Machine
learning (ML). More specifically, ensemble learning models have shown great
promise in improving the performance of the underlying ML models. Accordingly,
this paper proposes a framework based on the combined use of Bayesian
Optimization-Gaussian Process (BO-GP) with an ensemble tree-based learning
model to improve the performance of intrusion and attack detection in IIoT
environments. The proposed framework's performance is evaluated using the
Windows 10 dataset collected by the Cyber Range and IoT labs at University of
New South Wales. Experimental results illustrate the improvement in detection
accuracy, precision, and F-score when compared to standard tree and ensemble
tree models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05516">FPRF: Feed-Forward Photorealistic Style Transfer of Large-Scale 3D Neural Radiance Fields. (arXiv:2401.05516v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1">GeonU Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Youwang_K/0/1/0/all/0/1">Kim Youwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_T/0/1/0/all/0/1">Tae-Hyun Oh</a></p>
<p>We present FPRF, a feed-forward photorealistic style transfer method for
large-scale 3D neural radiance fields. FPRF stylizes large-scale 3D scenes with
arbitrary, multiple style reference images without additional optimization
while preserving multi-view appearance consistency. Prior arts required tedious
per-style/-scene optimization and were limited to small-scale 3D scenes. FPRF
efficiently stylizes large-scale 3D scenes by introducing a style-decomposed 3D
neural radiance field, which inherits AdaIN's feed-forward stylization
machinery, supporting arbitrary style reference images. Furthermore, FPRF
supports multi-reference stylization with the semantic correspondence matching
and local AdaIN, which adds diverse user control for 3D scene styles. FPRF also
preserves multi-view consistency by applying semantic matching and style
transfer processes directly onto queried features in 3D space. In experiments,
we demonstrate that FPRF achieves favorable photorealistic quality 3D scene
stylization for large-scale scenes with diverse reference images. Project page:
https://kim-geonu.github.io/FPRF/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05518">Correlated Quantization for Faster Nonconvex Distributed Optimization. (arXiv:2401.05518v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Panferov_A/0/1/0/all/0/1">Andrei Panferov</a>, <a href="http://arxiv.org/find/cs/1/au:+Demidovich_Y/0/1/0/all/0/1">Yury Demidovich</a>, <a href="http://arxiv.org/find/cs/1/au:+Rammal_A/0/1/0/all/0/1">Ahmad Rammal</a>, <a href="http://arxiv.org/find/cs/1/au:+Richtarik_P/0/1/0/all/0/1">Peter Richt&#xe1;rik</a></p>
<p>Quantization (Alistarh et al., 2017) is an important (stochastic) compression
technique that reduces the volume of transmitted bits during each communication
round in distributed model training. Suresh et al. (2022) introduce correlated
quantizers and show their advantages over independent counterparts by analyzing
distributed SGD communication complexity. We analyze the forefront distributed
non-convex optimization algorithm MARINA (Gorbunov et al., 2022) utilizing the
proposed correlated quantizers and show that it outperforms the original MARINA
and distributed SGD of Suresh et al. (2022) with regard to the communication
complexity. We significantly refine the original analysis of MARINA without any
additional assumptions using the weighted Hessian variance (Tyurin et al.,
2022), and then we expand the theoretical framework of MARINA to accommodate a
substantially broader range of potentially correlated and biased compressors,
thus dilating the applicability of the method beyond the conventional
independent unbiased compressor setup. Extensive experimental results
corroborate our theoretical findings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05520">From Pampas to Pixels: Fine-Tuning Diffusion Models for Ga\&#x27;ucho Heritage. (arXiv:2401.05520v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Amadeus_M/0/1/0/all/0/1">Marcellus Amadeus</a>, <a href="http://arxiv.org/find/cs/1/au:+Castaneda_W/0/1/0/all/0/1">William Alberto Cruz Casta&#xf1;eda</a>, <a href="http://arxiv.org/find/cs/1/au:+Zanella_A/0/1/0/all/0/1">Andr&#xe9; Felipe Zanella</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahlow_F/0/1/0/all/0/1">Felipe Rodrigues Perche Mahlow</a></p>
<p>Generative AI has become pervasive in society, witnessing significant
advancements in various domains. Particularly in the realm of Text-to-Image
(TTI) models, Latent Diffusion Models (LDMs), showcase remarkable capabilities
in generating visual content based on textual prompts. This paper addresses the
potential of LDMs in representing local cultural concepts, historical figures,
and endangered species. In this study, we use the cultural heritage of Rio
Grande do Sul (RS), Brazil, as an illustrative case. Our objective is to
contribute to the broader understanding of how generative models can help to
capture and preserve the cultural and historical identity of regions. The paper
outlines the methodology, including subject selection, dataset creation, and
the fine-tuning process. The results showcase the images generated, alongside
the challenges and feasibility of each concept. In conclusion, this work shows
the power of these models to represent and preserve unique aspects of diverse
regions and communities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05521">Current Effect-eliminated Optimal Target Assignment and Motion Planning for a Multi-UUV System. (arXiv:2401.05521v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1">Danjie Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Simon X. Yang</a></p>
<p>The paper presents an innovative approach (CBNNTAP) that addresses the
complexities and challenges introduced by ocean currents when optimizing target
assignment and motion planning for a multi-unmanned underwater vehicle (UUV)
system. The core of the proposed algorithm involves the integration of several
key components. Firstly, it incorporates a bio-inspired neural network-based
(BINN) approach which predicts the most efficient paths for individual UUVs
while simultaneously ensuring collision avoidance among the vehicles. Secondly,
an efficient target assignment component is integrated by considering the path
distances determined by the BINN algorithm. In addition, a critical innovation
within the CBNNTAP algorithm is its capacity to address the disruptive effects
of ocean currents, where an adjustment component is seamlessly integrated to
counteract the deviations caused by these currents, which enhances the accuracy
of both motion planning and target assignment for the UUVs. The effectiveness
of the CBNNTAP algorithm is demonstrated through comprehensive simulation
results and the outcomes underscore the superiority of the developed algorithm
in nullifying the effects of static and dynamic ocean currents in 2D and 3D
scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05535">Improving the Accuracy and Interpretability of Random Forests via Forest Pruning. (arXiv:2401.05535v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Dorador_A/0/1/0/all/0/1">Albert Dorador</a></p>
<p>Decades after their inception, random forests continue to provide
state-of-the-art accuracy in a variety of learning problems, outperforming in
this respect alternative machine learning algorithms such as decision trees or
even neural networks. However, being an ensemble method, the one aspect where
random forests tend to severely underperform decision trees is
interpretability. In the present work, we propose a post-hoc approach that aims
to have the best of both worlds: the accuracy of random forests and the
interpretability of decision trees. To this end, we present two forest-pruning
methods to find an optimal sub-forest within a given random forest, and then,
when applicable, combine the selected trees into one. Our first method relies
on constrained exhaustive search, while our second method is based on an
adaptation of the LASSO methodology. Extensive experiments over synthetic and
real world datasets show that, in the majority of scenarios, at least one of
the two methods proposed is more accurate than the original random forest,
while just using a small fraction of the trees, aiding result interpretability.
Compared to current state-of-the-art forestpruning methods, namely sequential
forward selection and (a variation of) sequential backward selection, our
methods tend to outperform both of them, whether in terms of accuracy, number
of trees employed, or both.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05544">CodePrompt: Improving Source Code-Related Classification with Knowledge Features through Prompt Learning. (arXiv:2401.05544v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yong Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1">Senlin Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_Y/0/1/0/all/0/1">Yu-Ming Shang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yifei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhengjun Li</a></p>
<p>Researchers have explored the potential of utilizing pre-trained language
models, such as CodeBERT, to improve source code-related tasks. Previous
studies have mainly relied on CodeBERT's text embedding capability and the
`[CLS]' sentence embedding information as semantic representations for
fine-tuning downstream source code-related tasks. However, these methods
require additional neural network layers to extract effective features,
resulting in higher computational costs. Furthermore, existing approaches have
not leveraged the rich knowledge contained in both source code and related
text, which can lead to lower accuracy. This paper presents a novel approach,
CodePrompt, which utilizes rich knowledge recalled from a pre-trained model by
prompt learning and an attention mechanism to improve source code-related
classification tasks. Our approach initially motivates the language model with
prompt information to retrieve abundant knowledge associated with the input as
representative features, thus avoiding the need for additional neural network
layers and reducing computational costs. Subsequently, we employ an attention
mechanism to aggregate multiple layers of related knowledge for each task as
final features to boost their accuracy. We conducted extensive experiments on
four downstream source code-related tasks to evaluate our approach and our
results demonstrate that CodePrompt achieves new state-of-the-art performance
on the accuracy metric while also exhibiting computation cost-saving
capabilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05566">Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training. (arXiv:2401.05566v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hubinger_E/0/1/0/all/0/1">Evan Hubinger</a>, <a href="http://arxiv.org/find/cs/1/au:+Denison_C/0/1/0/all/0/1">Carson Denison</a>, <a href="http://arxiv.org/find/cs/1/au:+Mu_J/0/1/0/all/0/1">Jesse Mu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lambert_M/0/1/0/all/0/1">Mike Lambert</a>, <a href="http://arxiv.org/find/cs/1/au:+Tong_M/0/1/0/all/0/1">Meg Tong</a>, <a href="http://arxiv.org/find/cs/1/au:+MacDiarmid_M/0/1/0/all/0/1">Monte MacDiarmid</a>, <a href="http://arxiv.org/find/cs/1/au:+Lanham_T/0/1/0/all/0/1">Tamera Lanham</a>, <a href="http://arxiv.org/find/cs/1/au:+Ziegler_D/0/1/0/all/0/1">Daniel M. Ziegler</a>, <a href="http://arxiv.org/find/cs/1/au:+Maxwell_T/0/1/0/all/0/1">Tim Maxwell</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1">Newton Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Jermyn_A/0/1/0/all/0/1">Adam Jermyn</a>, <a href="http://arxiv.org/find/cs/1/au:+Askell_A/0/1/0/all/0/1">Amanda Askell</a>, <a href="http://arxiv.org/find/cs/1/au:+Radhakrishnan_A/0/1/0/all/0/1">Ansh Radhakrishnan</a>, <a href="http://arxiv.org/find/cs/1/au:+Anil_C/0/1/0/all/0/1">Cem Anil</a>, <a href="http://arxiv.org/find/cs/1/au:+Duvenaud_D/0/1/0/all/0/1">David Duvenaud</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganguli_D/0/1/0/all/0/1">Deep Ganguli</a>, <a href="http://arxiv.org/find/cs/1/au:+Barez_F/0/1/0/all/0/1">Fazl Barez</a>, <a href="http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1">Jack Clark</a>, <a href="http://arxiv.org/find/cs/1/au:+Ndousse_K/0/1/0/all/0/1">Kamal Ndousse</a>, <a href="http://arxiv.org/find/cs/1/au:+Sachan_K/0/1/0/all/0/1">Kshitij Sachan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sellitto_M/0/1/0/all/0/1">Michael Sellitto</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_M/0/1/0/all/0/1">Mrinank Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+DasSarma_N/0/1/0/all/0/1">Nova DasSarma</a>, <a href="http://arxiv.org/find/cs/1/au:+Grosse_R/0/1/0/all/0/1">Roger Grosse</a>, <a href="http://arxiv.org/find/cs/1/au:+Kravec_S/0/1/0/all/0/1">Shauna Kravec</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1">Yuntao Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Witten_Z/0/1/0/all/0/1">Zachary Witten</a>, <a href="http://arxiv.org/find/cs/1/au:+Favaro_M/0/1/0/all/0/1">Marina Favaro</a>, <a href="http://arxiv.org/find/cs/1/au:+Brauner_J/0/1/0/all/0/1">Jan Brauner</a>, <a href="http://arxiv.org/find/cs/1/au:+Karnofsky_H/0/1/0/all/0/1">Holden Karnofsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Christiano_P/0/1/0/all/0/1">Paul Christiano</a>, <a href="http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1">Samuel R. Bowman</a>, <a href="http://arxiv.org/find/cs/1/au:+Graham_L/0/1/0/all/0/1">Logan Graham</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaplan_J/0/1/0/all/0/1">Jared Kaplan</a>, <a href="http://arxiv.org/find/cs/1/au:+Mindermann_S/0/1/0/all/0/1">S&#xf6;ren Mindermann</a>, <a href="http://arxiv.org/find/cs/1/au:+Greenblatt_R/0/1/0/all/0/1">Ryan Greenblatt</a>, <a href="http://arxiv.org/find/cs/1/au:+Shlegeris_B/0/1/0/all/0/1">Buck Shlegeris</a>, <a href="http://arxiv.org/find/cs/1/au:+Schiefer_N/0/1/0/all/0/1">Nicholas Schiefer</a>, <a href="http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1">Ethan Perez</a></p>
<p>Humans are capable of strategically deceptive behavior: behaving helpfully in
most situations, but then behaving very differently in order to pursue
alternative objectives when given the opportunity. If an AI system learned such
a deceptive strategy, could we detect it and remove it using current
state-of-the-art safety training techniques? To study this question, we
construct proof-of-concept examples of deceptive behavior in large language
models (LLMs). For example, we train models that write secure code when the
prompt states that the year is 2023, but insert exploitable code when the
stated year is 2024. We find that such backdoored behavior can be made
persistent, so that it is not removed by standard safety training techniques,
including supervised fine-tuning, reinforcement learning, and adversarial
training (eliciting unsafe behavior and then training to remove it). The
backdoored behavior is most persistent in the largest models and in models
trained to produce chain-of-thought reasoning about deceiving the training
process, with the persistence remaining even when the chain-of-thought is
distilled away. Furthermore, rather than removing backdoors, we find that
adversarial training can teach models to better recognize their backdoor
triggers, effectively hiding the unsafe behavior. Our results suggest that,
once a model exhibits deceptive behavior, standard techniques could fail to
remove such deception and create a false impression of safety.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05570">Siamese Networks with Soft Labels for Unsupervised Lesion Detection and Patch Pretraining on Screening Mammograms. (arXiv:2401.05570v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vorst_K/0/1/0/all/0/1">Kevin Van Vorst</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1">Li Shen</a></p>
<p>Self-supervised learning has become a popular way to pretrain a deep learning
model and then transfer it to perform downstream tasks. However, most of these
methods are developed on large-scale image datasets that contain natural
objects with clear textures, outlines, and distinct color contrasts. It remains
uncertain whether these methods are equally effective for medical imaging,
where the regions of interest often blend subtly and indistinctly with the
surrounding tissues. In this study, we propose an alternative method that uses
contralateral mammograms to train a neural network to encode similar embeddings
when a pair contains both normal images and different embeddings when a pair
contains normal and abnormal images. Our approach leverages the natural
symmetry of human body as weak labels to learn to distinguish abnormal lesions
from background tissues in a fully unsupervised manner. Our findings suggest
that it's feasible by incorporating soft labels derived from the Euclidean
distances between the embeddings of the image pairs into the Siamese network
loss. Our method demonstrates superior performance in mammogram patch
classification compared to existing self-supervised learning methods. This
approach not only leverages a vast amount of image data effectively but also
minimizes reliance on costly labels, a significant advantage particularly in
the field of medical imaging.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05572">Innate-Values-driven Reinforcement Learning for Cooperative Multi-Agent Systems. (arXiv:2401.05572v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1">Qin Yang</a></p>
<p>Innate values describe agents' intrinsic motivations, which reflect their
inherent interests and preferences to pursue goals and drive them to develop
diverse skills satisfying their various needs. The essence of reinforcement
learning (RL) is learning from interaction based on reward-driven (such as
utilities) behaviors, much like natural agents. It is an excellent model to
describe the innate-values-driven (IV) behaviors of AI agents. Especially in
multi-agent systems (MAS), building the awareness of AI agents to balance the
group utilities and system costs and satisfy group members' needs in their
cooperation is a crucial problem for individuals learning to support their
community and integrate human society in the long term. This paper proposes a
hierarchical compound intrinsic value reinforcement learning model --
innate-values-driven reinforcement learning termed IVRL to describe the complex
behaviors of multi-agent interaction in their cooperation. We implement the
IVRL architecture in the StarCraft Multi-Agent Challenge (SMAC) environment and
compare the cooperative performance within three characteristics of innate
value agents (Coward, Neutral, and Reckless) through three benchmark
multi-agent RL algorithms: QMIX, IQL, and QTRAN. The results demonstrate that
by organizing individual various needs rationally, the group can achieve better
performance with lower costs effectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05584">FourCastNeXt: Improving FourCastNet Training with Limited Compute. (arXiv:2401.05584v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_E/0/1/0/all/0/1">Edison Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_M/0/1/0/all/0/1">Maruf Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yue Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahendru_R/0/1/0/all/0/1">Rahul Mahendru</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1">Rui Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cook_H/0/1/0/all/0/1">Harrison Cook</a>, <a href="http://arxiv.org/find/cs/1/au:+Leeuwenburg_T/0/1/0/all/0/1">Tennessee Leeuwenburg</a>, <a href="http://arxiv.org/find/cs/1/au:+Evans_B/0/1/0/all/0/1">Ben Evans</a></p>
<p>Recently, the FourCastNet Neural Earth System Model (NESM) has shown
impressive results on predicting various atmospheric variables, trained on the
ERA5 reanalysis dataset. While FourCastNet enjoys quasi-linear time and memory
complexity in sequence length compared to quadratic complexity in vanilla
transformers, training FourCastNet on ERA5 from scratch still requires large
amount of compute resources, which is expensive or even inaccessible to most
researchers. In this work, we will show improved methods that can train
FourCastNet using only 1% of the compute required by the baseline, while
maintaining model performance or par or even better than the baseline.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05596">POMP: Probability-driven Meta-graph Prompter for LLMs in Low-resource Unsupervised Neural Machine Translation. (arXiv:2401.05596v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1">Shilong Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1">Zhiliang Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1">Liang Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zhen Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1">Zhihua Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dongsheng Li</a></p>
<p>Low-resource languages (LRLs) face challenges in supervised neural machine
translation due to limited parallel data, prompting research into unsupervised
methods. Unsupervised neural machine translation (UNMT) methods, including
back-translation, transfer learning, and pivot-based translation, offer
practical solutions for LRL translation, but they are hindered by issues like
synthetic data noise, language bias, and error propagation, which can
potentially be mitigated by Large Language Models (LLMs). LLMs have advanced
NMT with in-context learning (ICL) and supervised fine-tuning methods, but
insufficient training data results in poor performance in LRLs. We argue that
LLMs can mitigate the linguistic noise with auxiliary languages to improve
translations in LRLs. In this paper, we propose Probability-driven Meta-graph
Prompter (POMP), a novel approach employing a dynamic, sampling-based graph of
multiple auxiliary languages to enhance LLMs' translation capabilities for
LRLs. POMP involves constructing a directed acyclic meta-graph for each source
language, from which we dynamically sample multiple paths to prompt LLMs to
mitigate the linguistic noise and improve translations during training. We use
the BLEURT metric to evaluate the translations and back-propagate rewards,
estimated by scores, to update the probabilities of auxiliary languages in the
paths. Our experiments show significant improvements in the translation quality
of three LRLs, demonstrating the effectiveness of our approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05604">REBUS: A Robust Evaluation Benchmark of Understanding Symbols. (arXiv:2401.05604v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gritsevskiy_A/0/1/0/all/0/1">Andrew Gritsevskiy</a>, <a href="http://arxiv.org/find/cs/1/au:+Panickssery_A/0/1/0/all/0/1">Arjun Panickssery</a>, <a href="http://arxiv.org/find/cs/1/au:+Kirtland_A/0/1/0/all/0/1">Aaron Kirtland</a>, <a href="http://arxiv.org/find/cs/1/au:+Kauffman_D/0/1/0/all/0/1">Derik Kauffman</a>, <a href="http://arxiv.org/find/cs/1/au:+Gundlach_H/0/1/0/all/0/1">Hans Gundlach</a>, <a href="http://arxiv.org/find/cs/1/au:+Gritsevskaya_I/0/1/0/all/0/1">Irina Gritsevskaya</a>, <a href="http://arxiv.org/find/cs/1/au:+Cavanagh_J/0/1/0/all/0/1">Joe Cavanagh</a>, <a href="http://arxiv.org/find/cs/1/au:+Chiang_J/0/1/0/all/0/1">Jonathan Chiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Roux_L/0/1/0/all/0/1">Lydia La Roux</a>, <a href="http://arxiv.org/find/cs/1/au:+Hung_M/0/1/0/all/0/1">Michelle Hung</a></p>
<p>We propose a new benchmark evaluating the performance of multimodal large
language models on rebus puzzles. The dataset covers 333 original examples of
image-based wordplay, cluing 13 categories such as movies, composers, major
cities, and food. To achieve good performance on the benchmark of identifying
the clued word or phrase, models must combine image recognition and string
manipulation with hypothesis testing, multi-step reasoning, and an
understanding of human cognition, making for a complex, multimodal evaluation
of capabilities. We find that proprietary models such as GPT-4V and Gemini Pro
significantly outperform all other tested models. However, even the best model
has a final accuracy of just 24%, highlighting the need for substantial
improvements in reasoning. Further, models rarely understand all parts of a
puzzle, and are almost always incapable of retroactively explaining the correct
answer. Our benchmark can therefore be used to identify major shortcomings in
the knowledge and reasoning of multimodal large language models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05610">Graph Q-Learning for Combinatorial Optimization. (arXiv:2401.05610v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dax_V/0/1/0/all/0/1">Victoria M. Dax</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiachen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Leahy_K/0/1/0/all/0/1">Kevin Leahy</a>, <a href="http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1">Mykel J. Kochenderfer</a></p>
<p>Graph-structured data is ubiquitous throughout natural and social sciences,
and Graph Neural Networks (GNNs) have recently been shown to be effective at
solving prediction and inference problems on graph data. In this paper, we
propose and demonstrate that GNNs can be applied to solve Combinatorial
Optimization (CO) problems. CO concerns optimizing a function over a discrete
solution space that is often intractably large. To learn to solve CO problems,
we formulate the optimization process as a sequential decision making problem,
where the return is related to how close the candidate solution is to
optimality. We use a GNN to learn a policy to iteratively build increasingly
promising candidate solutions. We present preliminary evidence that GNNs
trained through Q-Learning can solve CO problems with performance approaching
state-of-the-art heuristic-based solvers, using only a fraction of the
parameters and training time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05618">The Benefits of a Concise Chain of Thought on Problem-Solving in Large Language Models. (arXiv:2401.05618v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Renze_M/0/1/0/all/0/1">Matthew Renze</a>, <a href="http://arxiv.org/find/cs/1/au:+Guven_E/0/1/0/all/0/1">Erhan Guven</a></p>
<p>In this paper, we introduce Concise Chain-of-Thought (CCoT) prompting. We
compared standard CoT and CCoT prompts to see how conciseness impacts response
length and correct-answer accuracy. We evaluated this using GPT-3.5 and GPT-4
with a multiple-choice question-and-answer (MCQA) benchmark. CCoT reduced
average response length by 48.70% for both GPT-3.5 and GPT-4 while having a
negligible impact on problem-solving performance. However, on math problems,
GPT-3.5 with CCoT incurs a performance penalty of 27.69%. Overall, CCoT leads
to an average per-token cost reduction of 22.67%. These results have practical
implications for AI systems engineers using LLMs to solve real-world problems
with CoT prompt-engineering techniques. In addition, these results provide more
general insight for AI researchers studying the emergent behavior of
step-by-step reasoning in LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05631">DrawTalking: Building Interactive Worlds by Sketching and Speaking. (arXiv:2401.05631v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rosenberg_K/0/1/0/all/0/1">Karl Toby Rosenberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Kazi_R/0/1/0/all/0/1">Rubaiat Habib Kazi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1">Li-Yi Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_H/0/1/0/all/0/1">Haijun Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Perlin_K/0/1/0/all/0/1">Ken Perlin</a></p>
<p>We introduce an interactive approach, DrawTalking, in which the user builds
interactive worlds by sketching and speaking. It emphasizes user control and
flexibility, and gives programming-like capability without code. We implemented
it on the iPad. An open-ended study shows the mechanics resonate and are
applicable to many creative-exploratory use cases. We hope to inspire and
inform research in future natural user-centered interfaces.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05654">Towards Conversational Diagnostic AI. (arXiv:2401.05654v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tu_T/0/1/0/all/0/1">Tao Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Palepu_A/0/1/0/all/0/1">Anil Palepu</a>, <a href="http://arxiv.org/find/cs/1/au:+Schaekermann_M/0/1/0/all/0/1">Mike Schaekermann</a>, <a href="http://arxiv.org/find/cs/1/au:+Saab_K/0/1/0/all/0/1">Khaled Saab</a>, <a href="http://arxiv.org/find/cs/1/au:+Freyberg_J/0/1/0/all/0/1">Jan Freyberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Tanno_R/0/1/0/all/0/1">Ryutaro Tanno</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1">Amy Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Brenna Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Amin_M/0/1/0/all/0/1">Mohamed Amin</a>, <a href="http://arxiv.org/find/cs/1/au:+Tomasev_N/0/1/0/all/0/1">Nenad Tomasev</a>, <a href="http://arxiv.org/find/cs/1/au:+Azizi_S/0/1/0/all/0/1">Shekoofeh Azizi</a>, <a href="http://arxiv.org/find/cs/1/au:+Singhal_K/0/1/0/all/0/1">Karan Singhal</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1">Yong Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1">Le Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Webson_A/0/1/0/all/0/1">Albert Webson</a>, <a href="http://arxiv.org/find/cs/1/au:+Kulkarni_K/0/1/0/all/0/1">Kavita Kulkarni</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahdavi_S/0/1/0/all/0/1">S Sara Mahdavi</a>, <a href="http://arxiv.org/find/cs/1/au:+Semturs_C/0/1/0/all/0/1">Christopher Semturs</a>, <a href="http://arxiv.org/find/cs/1/au:+Gottweis_J/0/1/0/all/0/1">Juraj Gottweis</a>, <a href="http://arxiv.org/find/cs/1/au:+Barral_J/0/1/0/all/0/1">Joelle Barral</a>, <a href="http://arxiv.org/find/cs/1/au:+Chou_K/0/1/0/all/0/1">Katherine Chou</a>, <a href="http://arxiv.org/find/cs/1/au:+Corrado_G/0/1/0/all/0/1">Greg S Corrado</a>, <a href="http://arxiv.org/find/cs/1/au:+Matias_Y/0/1/0/all/0/1">Yossi Matias</a>, <a href="http://arxiv.org/find/cs/1/au:+Karthikesalingam_A/0/1/0/all/0/1">Alan Karthikesalingam</a>, <a href="http://arxiv.org/find/cs/1/au:+Natarajan_V/0/1/0/all/0/1">Vivek Natarajan</a></p>
<p>At the heart of medicine lies the physician-patient dialogue, where skillful
history-taking paves the way for accurate diagnosis, effective management, and
enduring trust. Artificial Intelligence (AI) systems capable of diagnostic
dialogue could increase accessibility, consistency, and quality of care.
However, approximating clinicians' expertise is an outstanding grand challenge.
Here, we introduce AMIE (Articulate Medical Intelligence Explorer), a Large
Language Model (LLM) based AI system optimized for diagnostic dialogue.
</p>
<p>AMIE uses a novel self-play based simulated environment with automated
feedback mechanisms for scaling learning across diverse disease conditions,
specialties, and contexts. We designed a framework for evaluating
clinically-meaningful axes of performance including history-taking, diagnostic
accuracy, management reasoning, communication skills, and empathy. We compared
AMIE's performance to that of primary care physicians (PCPs) in a randomized,
double-blind crossover study of text-based consultations with validated patient
actors in the style of an Objective Structured Clinical Examination (OSCE). The
study included 149 case scenarios from clinical providers in Canada, the UK,
and India, 20 PCPs for comparison with AMIE, and evaluations by specialist
physicians and patient actors. AMIE demonstrated greater diagnostic accuracy
and superior performance on 28 of 32 axes according to specialist physicians
and 24 of 26 axes according to patient actors. Our research has several
limitations and should be interpreted with appropriate caution. Clinicians were
limited to unfamiliar synchronous text-chat which permits large-scale
LLM-patient interactions but is not representative of usual clinical practice.
While further research is required before AMIE could be translated to
real-world settings, the results represent a milestone towards conversational
diagnostic AI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05667">EsaCL: Efficient Continual Learning of Sparse Models. (arXiv:2401.05667v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1">Weijieying Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Honavar_V/0/1/0/all/0/1">Vasant G Honavar</a></p>
<p>A key challenge in the continual learning setting is to efficiently learn a
sequence of tasks without forgetting how to perform previously learned tasks.
Many existing approaches to this problem work by either retraining the model on
previous tasks or by expanding the model to accommodate new tasks. However,
these approaches typically suffer from increased storage and computational
requirements, a problem that is worsened in the case of sparse models due to
need for expensive re-training after sparsification. To address this challenge,
we propose a new method for efficient continual learning of sparse models
(EsaCL) that can automatically prune redundant parameters without adversely
impacting the model's predictive power, and circumvent the need of retraining.
We conduct a theoretical analysis of loss landscapes with parameter pruning,
and design a directional pruning (SDP) strategy that is informed by the
sharpness of the loss function with respect to the model parameters. SDP
ensures model with minimal loss of predictive accuracy, accelerating the
learning of sparse models at each stage. To accelerate model update, we
introduce an intelligent data selection (IDS) strategy that can identify
critical instances for estimating loss landscape, yielding substantially
improved data efficiency. The results of our experiments show that EsaCL
achieves performance that is competitive with the state-of-the-art methods on
three continual learning benchmarks, while using substantially reduced memory
and computational resources.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05680">Use of Graph Neural Networks in Aiding Defensive Cyber Operations. (arXiv:2401.05680v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mitra_S/0/1/0/all/0/1">Shaswata Mitra</a>, <a href="http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1">Trisha Chakraborty</a>, <a href="http://arxiv.org/find/cs/1/au:+Neupane_S/0/1/0/all/0/1">Subash Neupane</a>, <a href="http://arxiv.org/find/cs/1/au:+Piplai_A/0/1/0/all/0/1">Aritran Piplai</a>, <a href="http://arxiv.org/find/cs/1/au:+Mittal_S/0/1/0/all/0/1">Sudip Mittal</a></p>
<p>In an increasingly interconnected world, where information is the lifeblood
of modern society, regular cyber-attacks sabotage the confidentiality,
integrity, and availability of digital systems and information. Additionally,
cyber-attacks differ depending on the objective and evolve rapidly to disguise
defensive systems. However, a typical cyber-attack demonstrates a series of
stages from attack initiation to final resolution, called an attack life cycle.
These diverse characteristics and the relentless evolution of cyber attacks
have led cyber defense to adopt modern approaches like Machine Learning to
bolster defensive measures and break the attack life cycle. Among the adopted
ML approaches, Graph Neural Networks have emerged as a promising approach for
enhancing the effectiveness of defensive measures due to their ability to
process and learn from heterogeneous cyber threat data. In this paper, we look
into the application of GNNs in aiding to break each stage of one of the most
renowned attack life cycles, the Lockheed Martin Cyber Kill Chain. We address
each phase of CKC and discuss how GNNs contribute to preparing and preventing
an attack from a defensive standpoint. Furthermore, We also discuss open
research areas and further improvement scopes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05683">Deep Learning Meets Mechanism Design: Key Results and Some Novel Applications. (arXiv:2401.05683v1 [cs.GT])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sankar_V/0/1/0/all/0/1">V. Udaya Sankar</a>, <a href="http://arxiv.org/find/cs/1/au:+Rao_V/0/1/0/all/0/1">Vishisht Srihari Rao</a>, <a href="http://arxiv.org/find/cs/1/au:+Narahari_Y/0/1/0/all/0/1">Y. Narahari</a></p>
<p>Mechanism design is essentially reverse engineering of games and involves
inducing a game among strategic agents in a way that the induced game satisfies
a set of desired properties in an equilibrium of the game. Desirable properties
for a mechanism include incentive compatibility, individual rationality,
welfare maximisation, revenue maximisation (or cost minimisation), fairness of
allocation, etc. It is known from mechanism design theory that only certain
strict subsets of these properties can be simultaneously satisfied exactly by
any given mechanism. Often, the mechanisms required by real-world applications
may need a subset of these properties that are theoretically impossible to be
simultaneously satisfied. In such cases, a prominent recent approach is to use
a deep learning based approach to learn a mechanism that approximately
satisfies the required properties by minimizing a suitably defined loss
function. In this paper, we present, from relevant literature, technical
details of using a deep learning approach for mechanism design and provide an
overview of key results in this topic. We demonstrate the power of this
approach for three illustrative case studies: (a) efficient energy management
in a vehicular network (b) resource allocation in a mobile network (c)
designing a volume discount procurement auction for agricultural inputs.
Section 6 concludes the paper.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05700">R-BI: Regularized Batched Inputs enhance Incremental Decoding Framework for Low-Latency Simultaneous Speech Translation. (arXiv:2401.05700v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jiaxin Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhanglin Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zongyao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_H/0/1/0/all/0/1">Hengchao Shang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1">Daimeng Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xiaoyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Rao_Z/0/1/0/all/0/1">Zhiqiang Rao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shaojun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Hao Yang</a></p>
<p>Incremental Decoding is an effective framework that enables the use of an
offline model in a simultaneous setting without modifying the original model,
making it suitable for Low-Latency Simultaneous Speech Translation. However,
this framework may introduce errors when the system outputs from incomplete
input. To reduce these output errors, several strategies such as Hold-$n$,
LA-$n$, and SP-$n$ can be employed, but the hyper-parameter $n$ needs to be
carefully selected for optimal performance. Moreover, these strategies are more
suitable for end-to-end systems than cascade systems. In our paper, we propose
a new adaptable and efficient policy named "Regularized Batched Inputs". Our
method stands out by enhancing input diversity to mitigate output errors. We
suggest particular regularization techniques for both end-to-end and cascade
systems. We conducted experiments on IWSLT Simultaneous Speech Translation
(SimulST) tasks, which demonstrate that our approach achieves low latency while
maintaining no more than 2 BLEU points loss compared to offline systems.
Furthermore, our SimulST systems attained several new state-of-the-art results
in various language directions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05730">Enhancing Contrastive Learning with Efficient Combinatorial Positive Pairing. (arXiv:2401.05730v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jaeill Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_D/0/1/0/all/0/1">Duhun Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1">Eunjung Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Suh_J/0/1/0/all/0/1">Jangwon Suh</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jimyeong Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Rhee_W/0/1/0/all/0/1">Wonjong Rhee</a></p>
<p>In the past few years, contrastive learning has played a central role for the
success of visual unsupervised representation learning. Around the same time,
high-performance non-contrastive learning methods have been developed as well.
While most of the works utilize only two views, we carefully review the
existing multi-view methods and propose a general multi-view strategy that can
improve learning speed and performance of any contrastive or non-contrastive
method. We first analyze CMC's full-graph paradigm and empirically show that
the learning speed of $K$-views can be increased by $_{K}\mathrm{C}_{2}$ times
for small learning rate and early training. Then, we upgrade CMC's full-graph
by mixing views created by a crop-only augmentation, adopting small-size views
as in SwAV multi-crop, and modifying the negative sampling. The resulting
multi-view strategy is called ECPP (Efficient Combinatorial Positive Pairing).
We investigate the effectiveness of ECPP by applying it to SimCLR and assessing
the linear evaluation performance for CIFAR-10 and ImageNet-100. For each
benchmark, we achieve a state-of-the-art performance. In case of ImageNet-100,
ECPP boosted SimCLR outperforms supervised learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05743">Consistent Query Answering for Existential Rules under Tuple-Deletion Semantics. (arXiv:2401.05743v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Marconi_L/0/1/0/all/0/1">Lorenzo Marconi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rosati_R/0/1/0/all/0/1">Riccardo Rosati</a></p>
<p>We study consistent query answering over knowledge bases expressed by
existential rules. Specifically, we establish the data complexity of consistent
query answering and repair checking under tuple-deletion semantics for a
general class of disjunctive existential rules and for several subclasses
thereof (acyclic, linear, full, guarded, and sticky). In particular, we
identify several cases in which the above problems are tractable or even
first-order rewritable, and present new query rewriting techniques that can be
the basis for practical inconsistency-tolerant query answering systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05749">A Shocking Amount of the Web is Machine Translated: Insights from Multi-Way Parallelism. (arXiv:2401.05749v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Thompson_B/0/1/0/all/0/1">Brian Thompson</a>, <a href="http://arxiv.org/find/cs/1/au:+Dhaliwal_M/0/1/0/all/0/1">Mehak Preet Dhaliwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Frisch_P/0/1/0/all/0/1">Peter Frisch</a>, <a href="http://arxiv.org/find/cs/1/au:+Domhan_T/0/1/0/all/0/1">Tobias Domhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Federico_M/0/1/0/all/0/1">Marcello Federico</a></p>
<p>We show that content on the web is often translated into many languages, and
the low quality of these multi-way translations indicates they were likely
created using Machine Translation (MT). Multi-way parallel, machine generated
content not only dominates the translations in lower resource languages; it
also constitutes a large fraction of the total web content in those languages.
We also find evidence of a selection bias in the type of content which is
translated into many languages, consistent with low quality English content
being translated en masse into many lower resource languages, via MT. Our work
raises serious concerns about training models such as multilingual large
language models on both monolingual and bilingual data scraped from the web.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05772">Knowledge Translation: A New Pathway for Model Compression. (arXiv:2401.05772v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1">Wujie Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Defang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiawei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yan Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Can Wang</a></p>
<p>Deep learning has witnessed significant advancements in recent years at the
cost of increasing training, inference, and model storage overhead. While
existing model compression methods strive to reduce the number of model
parameters while maintaining high accuracy, they inevitably necessitate the
re-training of the compressed model or impose architectural constraints. To
overcome these limitations, this paper presents a novel framework, termed
\textbf{K}nowledge \textbf{T}ranslation (KT), wherein a ``translation'' model
is trained to receive the parameters of a larger model and generate compressed
parameters. The concept of KT draws inspiration from language translation,
which effectively employs neural networks to convert different languages,
maintaining identical meaning. Accordingly, we explore the potential of neural
networks to convert models of disparate sizes, while preserving their
functionality. We propose a comprehensive framework for KT, introduce data
augmentation strategies to enhance model performance despite restricted
training data, and successfully demonstrate the feasibility of KT on the MNIST
dataset. Code is available at \url{https://github.com/zju-SWJ/KT}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05778">Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems. (arXiv:2401.05778v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cui_T/0/1/0/all/0/1">Tianyu Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yanling Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1">Chuanpu Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1">Yong Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Sijia Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1">Xinhao Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yunpeng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qinglin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1">Ziyi Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Peiyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1">Zhixing Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1">Junwu Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1">Xinyu Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1">Zujie Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Ke Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qi Li</a></p>
<p>Large language models (LLMs) have strong capabilities in solving diverse
natural language processing tasks. However, the safety and security issues of
LLM systems have become the major obstacle to their widespread application.
Many studies have extensively investigated risks in LLM systems and developed
the corresponding mitigation strategies. Leading-edge enterprises such as
OpenAI, Google, Meta, and Anthropic have also made lots of efforts on
responsible LLMs. Therefore, there is a growing need to organize the existing
studies and establish comprehensive taxonomies for the community. In this
paper, we delve into four essential modules of an LLM system, including an
input module for receiving prompts, a language model trained on extensive
corpora, a toolchain module for development and deployment, and an output
module for exporting LLM-generated content. Based on this, we propose a
comprehensive taxonomy, which systematically analyzes potential risks
associated with each module of an LLM system and discusses the corresponding
mitigation strategies. Furthermore, we review prevalent benchmarks, aiming to
facilitate the risk assessment of LLM systems. We hope that this paper can help
LLM participants embrace a systematic perspective to build their responsible
LLM systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05799">Designing Heterogeneous LLM Agents for Financial Sentiment Analysis. (arXiv:2401.05799v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1">Frank Xing</a></p>
<p>Large language models (LLMs) have drastically changed the possible ways to
design intelligent systems, shifting the focuses from massive data acquisition
and new modeling training to human alignment and strategical elicitation of the
full potential of existing pre-trained models. This paradigm shift, however, is
not fully realized in financial sentiment analysis (FSA), due to the
discriminative nature of this task and a lack of prescriptive knowledge of how
to leverage generative models in such a context. This study investigates the
effectiveness of the new paradigm, i.e., using LLMs without fine-tuning for
FSA. Rooted in Minsky's theory of mind and emotions, a design framework with
heterogeneous LLM agents is proposed. The framework instantiates specialized
agents using prior domain knowledge of the types of FSA errors and reasons on
the aggregated agent discussions. Comprehensive evaluation on FSA datasets show
that the framework yields better accuracies, especially when the discussions
are substantial. This study contributes to the design foundations and paves new
avenues for LLMs-based FSA. Implications on business and management are also
discussed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05800">Graph Spatiotemporal Process for Multivariate Time Series Anomaly Detection with Missing Values. (arXiv:2401.05800v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1">Yu Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Koh_H/0/1/0/all/0/1">Huan Yee Koh</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1">Ming Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chi_L/0/1/0/all/0/1">Lianhua Chi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haishuai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Phan_K/0/1/0/all/0/1">Khoa T. Phan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yi-Ping Phoebe Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1">Shirui Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1">Wei Xiang</a></p>
<p>The detection of anomalies in multivariate time series data is crucial for
various practical applications, including smart power grids, traffic flow
forecasting, and industrial process control. However, real-world time series
data is usually not well-structured, posting significant challenges to existing
approaches: (1) The existence of missing values in multivariate time series
data along variable and time dimensions hinders the effective modeling of
interwoven spatial and temporal dependencies, resulting in important patterns
being overlooked during model training; (2) Anomaly scoring with
irregularly-sampled observations is less explored, making it difficult to use
existing detectors for multivariate series without fully-observed values. In
this work, we introduce a novel framework called GST-Pro, which utilizes a
graph spatiotemporal process and anomaly scorer to tackle the aforementioned
challenges in detecting anomalies on irregularly-sampled multivariate time
series. Our approach comprises two main components. First, we propose a graph
spatiotemporal process based on neural controlled differential equations. This
process enables effective modeling of multivariate time series from both
spatial and temporal perspectives, even when the data contains missing values.
Second, we present a novel distribution-based anomaly scoring mechanism that
alleviates the reliance on complete uniform observations. By analyzing the
predictions of the graph spatiotemporal process, our approach allows anomalies
to be easily detected. Our experimental results show that the GST-Pro method
can effectively detect anomalies in time series data and outperforms
state-of-the-art methods, regardless of whether there are missing values
present in the data. Our code is available: https://github.com/huankoh/GST-Pro.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05811">Tuning LLMs with Contrastive Alignment Instructions for Machine Translation in Unseen, Low-resource Languages. (arXiv:2401.05811v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1">Zhuoyuan Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yen Yu</a></p>
<p>This article introduces contrastive alignment instructions (AlignInstruct) to
address two challenges in machine translation (MT) on large language models
(LLMs). One is the expansion of supported languages to previously unseen ones.
The second relates to the lack of data in low-resource languages. Model
fine-tuning through MT instructions (MTInstruct) is a straightforward approach
to the first challenge. However, MTInstruct is limited by weak cross-lingual
signals inherent in the second challenge. AlignInstruct emphasizes
cross-lingual supervision via a cross-lingual discriminator built using
statistical word alignments. Our results based on fine-tuning the BLOOMZ models
(1b1, 3b, and 7b1) in up to 24 unseen languages showed that: (1) LLMs can
effectively translate unseen languages using MTInstruct; (2) AlignInstruct led
to consistent improvements in translation quality across 48 translation
directions involving English; (3) Discriminator-based instructions outperformed
their generative counterparts as cross-lingual instructions; (4) AlignInstruct
improved performance in 30 zero-shot directions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05815">Cheetah: Bridging the Gap Between Machine Learning and Particle Accelerator Physics with High-Speed, Differentiable Simulations. (arXiv:2401.05815v1 [physics.acc-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Kaiser_J/0/1/0/all/0/1">Jan Kaiser</a>, <a href="http://arxiv.org/find/physics/1/au:+Xu_C/0/1/0/all/0/1">Chenran Xu</a>, <a href="http://arxiv.org/find/physics/1/au:+Eichler_A/0/1/0/all/0/1">Annika Eichler</a>, <a href="http://arxiv.org/find/physics/1/au:+Garcia_A/0/1/0/all/0/1">Andrea Santamaria Garcia</a></p>
<p>Machine learning has emerged as a powerful solution to the modern challenges
in accelerator physics. However, the limited availability of beam time, the
computational cost of simulations, and the high-dimensionality of optimisation
problems pose significant challenges in generating the required data for
training state-of-the-art machine learning models. In this work, we introduce
Cheetah, a PyTorch-based high-speed differentiable linear-beam dynamics code.
Cheetah enables the fast collection of large data sets by reducing computation
times by multiple orders of magnitude and facilitates efficient gradient-based
optimisation for accelerator tuning and system identification. This positions
Cheetah as a user-friendly, readily extensible tool that integrates seamlessly
with widely adopted machine learning tools. We showcase the utility of Cheetah
through five examples, including reinforcement learning training,
gradient-based beamline tuning, gradient-based system identification,
physics-informed Bayesian optimisation priors, and modular neural network
surrogate modelling of space charge effects. The use of such a high-speed
differentiable simulation code will simplify the development of machine
learning-based methods for particle accelerators and fast-track their
integration into everyday operations of accelerator facilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05822">Towards Goal-Oriented Agents for Evolving Problems Observed via Conversation. (arXiv:2401.05822v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Free_M/0/1/0/all/0/1">Michael Free</a>, <a href="http://arxiv.org/find/cs/1/au:+Langworthy_A/0/1/0/all/0/1">Andrew Langworthy</a>, <a href="http://arxiv.org/find/cs/1/au:+Dimitropoulaki_M/0/1/0/all/0/1">Mary Dimitropoulaki</a>, <a href="http://arxiv.org/find/cs/1/au:+Thompson_S/0/1/0/all/0/1">Simon Thompson</a></p>
<p>The objective of this work is to train a chatbot capable of solving evolving
problems through conversing with a user about a problem the chatbot cannot
directly observe. The system consists of a virtual problem (in this case a
simple game), a simulated user capable of answering natural language questions
that can observe and perform actions on the problem, and a Deep Q-Network
(DQN)-based chatbot architecture. The chatbot is trained with the goal of
solving the problem through dialogue with the simulated user using
reinforcement learning. The contributions of this paper are as follows: a
proposed architecture to apply a conversational DQN-based agent to evolving
problems, an exploration of training methods such as curriculum learning on
model performance and the effect of modified reward functions in the case of
increasing environment complexity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05827">Hallucination Benchmark in Medical Visual Question Answering. (arXiv:2401.05827v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jinge Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Yunsoo Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Honghan Wu</a></p>
<p>The recent success of large language and vision models on vision question
answering (VQA), particularly their applications in medicine (Med-VQA), has
shown a great potential of realizing effective visual assistants for
healthcare. However, these models are not extensively tested on the
hallucination phenomenon in clinical settings. Here, we created a hallucination
benchmark of medical images paired with question-answer sets and conducted a
comprehensive evaluation of the state-of-the-art models. The study provides an
in-depth analysis of current models limitations and reveals the effectiveness
of various prompting strategies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05831">Revisiting Silhouette: From Micro to Macro Aggregation. (arXiv:2401.05831v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vardakas_G/0/1/0/all/0/1">Georgios Vardakas</a>, <a href="http://arxiv.org/find/cs/1/au:+Pavlopoulos_J/0/1/0/all/0/1">John Pavlopoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Likas_A/0/1/0/all/0/1">Aristidis Likas</a></p>
<p>Silhouette coefficient is an established internal clustering evaluation
measure that produces a score per data point, assessing the quality of its
clustering assignment. To assess the quality of the clustering of the whole
dataset, the scores of all the points in the dataset are typically averaged
into a single value, a strategy which we call as micro-averaging. As we
illustrate in this work, by using a synthetic example, this micro-averaging
strategy is sensitive both to cluster imbalance and outliers (background
noise). To address these issues, we propose an alternative aggregation
strategy, which first averages the silhouette scores at a cluster level and
then (macro) averages the scores across the clusters. Based on the same
synthetic example, we show that the proposed macro-averaged silhouette score is
robust to cluster imbalance and background noise. We have conducted an
experimental study showing that our macro-averaged variant provides better
estimates of the ground truth number of clusters on several cases compared to
the typical micro-averaged score.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05840">Decoding AI&#x27;s Nudge: A Unified Framework to Predict Human Behavior in AI-assisted Decision Making. (arXiv:2401.05840v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhuoyan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Zhuoran Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1">Ming Yin</a></p>
<p>With the rapid development of AI-based decision aids, different forms of AI
assistance have been increasingly integrated into the human decision making
processes. To best support humans in decision making, it is essential to
quantitatively understand how diverse forms of AI assistance influence humans'
decision making behavior. To this end, much of the current research focuses on
the end-to-end prediction of human behavior using ``black-box'' models, often
lacking interpretations of the nuanced ways in which AI assistance impacts the
human decision making process. Meanwhile, methods that prioritize the
interpretability of human behavior predictions are often tailored for one
specific form of AI assistance, making adaptations to other forms of assistance
difficult. In this paper, we propose a computational framework that can provide
an interpretable characterization of the influence of different forms of AI
assistance on decision makers in AI-assisted decision making. By
conceptualizing AI assistance as the ``{\em nudge}'' in human decision making
processes, our approach centers around modelling how different forms of AI
assistance modify humans' strategy in weighing different information in making
their decisions. Evaluations on behavior data collected from real human
decision makers show that the proposed framework outperforms various baselines
in accurately predicting human behavior in AI-assisted decision making. Based
on the proposed framework, we further provide insights into how individuals
with different cognitive styles are nudged by AI assistance differently.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05848">Pushing the Pareto front of band gap and permittivity: ML-guided search for dielectric materials. (arXiv:2401.05848v1 [cond-mat.mtrl-sci])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Riebesell_J/0/1/0/all/0/1">Janosh Riebesell</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Surta_T/0/1/0/all/0/1">T. Wesley Surta</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Goodall_R/0/1/0/all/0/1">Rhys Goodall</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Gaultois_M/0/1/0/all/0/1">Michael Gaultois</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Lee_A/0/1/0/all/0/1">Alpha A Lee</a></p>
<p>Materials with high-dielectric constant easily polarize under external
electric fields, allowing them to perform essential functions in many modern
electronic devices. Their practical utility is determined by two conflicting
properties: high dielectric constants tend to occur in materials with narrow
band gaps, limiting the operating voltage before dielectric breakdown. We
present a high-throughput workflow that combines element substitution, ML
pre-screening, ab initio simulation and human expert intuition to efficiently
explore the vast space of unknown materials for potential dielectrics, leading
to the synthesis and characterization of two novel dielectric materials,
CsTaTeO6 and Bi2Zr2O7. Our key idea is to deploy ML in a multi-objective
optimization setting with concave Pareto front. While usually considered more
challenging than single-objective optimization, we argue and show preliminary
evidence that the $1/x$-correlation between band gap and permittivity in fact
makes the task more amenable to ML methods by allowing separate models for band
gap and permittivity to each operate in regions of good training support while
still predicting materials of exceptional merit. To our knowledge, this is the
first instance of successful ML-guided multi-objective materials optimization
achieving experimental synthesis and characterization. CsTaTeO6 is a structure
generated via element substitution not present in our reference data sources,
thus exemplifying successful de-novo materials design. Meanwhile, we report the
first high-purity synthesis and dielectric characterization of Bi2Zr2O7 with a
band gap of 2.27 eV and a permittivity of 20.5, meeting all target metrics of
our multi-objective search.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05849">Inferring Intentions to Speak Using Accelerometer Data In-the-Wild. (arXiv:2401.05849v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Litian Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Molhoek_J/0/1/0/all/0/1">Jord Molhoek</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jing Zhou</a></p>
<p>Humans have good natural intuition to recognize when another person has
something to say. It would be interesting if an AI can also recognize
intentions to speak. Especially in scenarios when an AI is guiding a group
discussion, this can be a useful skill. This work studies the inference of
successful and unsuccessful intentions to speak from accelerometer data. This
is chosen because it is privacy-preserving and feasible for in-the-wild
settings since it can be placed in a smart badge. Data from a real-life social
networking event is used to train a machine-learning model that aims to infer
intentions to speak. A subset of unsuccessful intention-to-speak cases in the
data is annotated. The model is trained on the successful intentions to speak
and evaluated on both the successful and unsuccessful cases. In conclusion,
there is useful information in accelerometer data, but not enough to reliably
capture intentions to speak. For example, posture shifts are correlated with
intentions to speak, but people also often shift posture without having an
intention to speak, or have an intention to speak without shifting their
posture. More modalities are likely needed to reliably infer intentions to
speak.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05870">HiCAST: Highly Customized Arbitrary Style Transfer with Adapter Enhanced Diffusion Models. (arXiv:2401.05870v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hanzhang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haoran Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jinze Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Zhongrui Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1">Zeke Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1">Lei Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1">Xinyan Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1">Junjun Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xianming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1">Mingming Sun</a></p>
<p>The goal of Arbitrary Style Transfer (AST) is injecting the artistic features
of a style reference into a given image/video. Existing methods usually focus
on pursuing the balance between style and content, whereas ignoring the
significant demand for flexible and customized stylization results and thereby
limiting their practical application. To address this critical issue, a novel
AST approach namely HiCAST is proposed, which is capable of explicitly
customizing the stylization results according to various source of semantic
clues. In the specific, our model is constructed based on Latent Diffusion
Model (LDM) and elaborately designed to absorb content and style instance as
conditions of LDM. It is characterized by introducing of \textit{Style
Adapter}, which allows user to flexibly manipulate the output results by
aligning multi-level style information and intrinsic knowledge in LDM. Lastly,
we further extend our model to perform video AST. A novel learning objective is
leveraged for video diffusion model training, which significantly improve
cross-frame temporal consistency in the premise of maintaining stylization
strength. Qualitative and quantitative comparisons as well as comprehensive
user studies demonstrate that our HiCAST outperforms the existing SoTA methods
in generating visually plausible stylization results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05895">Binary Linear Tree Commitment-based Ownership Protection for Distributed Machine Learning. (arXiv:2401.05895v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1">Tianxiu Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Gai_K/0/1/0/all/0/1">Keke Gai</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Jing Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1">Liehuang Zhu</a></p>
<p>Distributed machine learning enables parallel training of extensive datasets
by delegating computing tasks across multiple workers. Despite the cost
reduction benefits of distributed machine learning, the dissemination of final
model weights often leads to potential conflicts over model ownership as
workers struggle to substantiate their involvement in the training computation.
To address the above ownership issues and prevent accidental failures and
malicious attacks, verifying the computational integrity and effectiveness of
workers becomes particularly crucial in distributed machine learning. In this
paper, we proposed a novel binary linear tree commitment-based ownership
protection model to ensure computational integrity with limited overhead and
concise proof. Due to the frequent updates of parameters during training, our
commitment scheme introduces a maintainable tree structure to reduce the costs
of updating proofs. Distinguished from SNARK-based verifiable computation, our
model achieves efficient proof aggregation by leveraging inner product
arguments. Furthermore, proofs of model weights are watermarked by worker
identity keys to prevent commitments from being forged or duplicated. The
performance analysis and comparison with SNARK-based hash commitments validate
the efficacy of our model in preserving computational integrity within
distributed machine learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05914">How Teachers Can Use Large Language Models and Bloom&#x27;s Taxonomy to Create Educational Quizzes. (arXiv:2401.05914v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Elkins_S/0/1/0/all/0/1">Sabina Elkins</a>, <a href="http://arxiv.org/find/cs/1/au:+Kochmar_E/0/1/0/all/0/1">Ekaterina Kochmar</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheung_J/0/1/0/all/0/1">Jackie C.K. Cheung</a>, <a href="http://arxiv.org/find/cs/1/au:+Serban_I/0/1/0/all/0/1">Iulian Serban</a></p>
<p>Question generation (QG) is a natural language processing task with an
abundance of potential benefits and use cases in the educational domain. In
order for this potential to be realized, QG systems must be designed and
validated with pedagogical needs in mind. However, little research has assessed
or designed QG approaches with the input from real teachers or students. This
paper applies a large language model-based QG approach where questions are
generated with learning goals derived from Bloom's taxonomy. The automatically
generated questions are used in multiple experiments designed to assess how
teachers use them in practice. The results demonstrate that teachers prefer to
write quizzes with automatically generated questions, and that such quizzes
have no loss in quality compared to handwritten versions. Further, several
metrics indicate that automatically generated questions can even improve the
quality of the quizzes created, showing the promise for large scale use of QG
in the classroom setting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05925">CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians. (arXiv:2401.05925v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dou_B/0/1/0/all/0/1">Bin Dou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tianyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yongjia Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhaohui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1">Zejian Yuan</a></p>
<p>We propose Compact and Swift Segmenting 3D Gaussians(CoSSegGaussians), a
method for compact 3D-consistent scene segmentation at fast rendering speed
with only RGB images input. Previous NeRF-based 3D segmentation methods have
relied on implicit or voxel neural scene representation and ray-marching volume
rendering which are time consuming. Recent 3D Gaussian Splatting significantly
improves the rendering speed, however, existing Gaussians-based segmentation
methods(eg: Gaussian Grouping) fail to provide compact segmentation masks
especially in zero-shot segmentation, which is mainly caused by the lack of
robustness and compactness for straightforwardly assigning learnable parameters
to each Gaussian when encountering inconsistent 2D machine-generated labels.
Our method aims to achieve compact and reliable zero-shot scene segmentation
swiftly by mapping fused spatial and semantically meaningful features for each
Gaussian point with a shallow decoding network. Specifically, our method
firstly optimizes Gaussian points' position, convariance and color attributes
under the supervision of RGB images. After Gaussian Locating, we distill
multi-scale DINO features extracted from images through unprojection to each
Gaussian, which is then incorporated with spatial features from the fast point
features processing network, i.e. RandLA-Net. Then the shallow decoding MLP is
applied to the multi-scale fused features to obtain compact segmentation.
Experimental results show that our model can perform high-quality zero-shot
scene segmentation, as our model outperforms other segmentation methods on both
semantic and panoptic segmentation task, meanwhile consumes approximately only
10% segmenting time compared to NeRF-based segmentation. Code and more results
will be available at https://David-Dou.github.io/CoSSegGaussians
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05930">SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully. (arXiv:2401.05930v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kai_J/0/1/0/all/0/1">Jushi Kai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tianhang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1">Hai Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zhouhan Lin</a></p>
<p>Large language models (LLMs) demonstrate great performance in text
generation. However, LLMs are still suffering from hallucinations. In this
work, we propose an inference-time method, Self-Highlighted Hesitation (SH2),
to help LLMs decode more truthfully. SH2 is based on a simple fact rooted in
information theory that for an LLM, the tokens predicted with lower
probabilities are prone to be more informative than others. Our analysis shows
that the tokens assigned with lower probabilities by an LLM are more likely to
be closely related to factual information, such as nouns, proper nouns, and
adjectives. Therefore, we propose to ''highlight'' the factual information by
selecting the tokens with the lowest probabilities and concatenating them to
the original context, thus forcing the model to repeatedly read and hesitate on
these tokens before generation. During decoding, we also adopt contrastive
decoding to emphasize the difference in the output probabilities brought by the
hesitation. Experimental results demonstrate that our SH2, requiring no
additional data or models, can effectively help LLMs elicit factual knowledge
and distinguish hallucinated contexts. Significant and consistent improvements
are achieved by SH2 for LLaMA-7b and LLaMA2-7b on multiple hallucination tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05932">DiffDA: a diffusion model for weather-scale data assimilation. (arXiv:2401.05932v1 [cs.CE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1">Langwen Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gianinazzi_L/0/1/0/all/0/1">Lukas Gianinazzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yuejiang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dueben_P/0/1/0/all/0/1">Peter D. Dueben</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoefler_T/0/1/0/all/0/1">Torsten Hoefler</a></p>
<p>The generation of initial conditions via accurate data assimilation is
crucial for reliable weather forecasting and climate modeling. We propose the
DiffDA as a machine learning based data assimilation method capable of
assimilating atmospheric variables using predicted states and sparse
observations. We adapt the pretrained GraphCast weather forecast model as a
denoising diffusion model. Our method applies two-phase conditioning: on the
predicted state during both training and inference, and on sparse observations
during inference only. As a byproduct, this strategy also enables the
post-processing of predictions into the future, for which no observations are
available.Through experiments based on a reanalysis dataset, we have verified
that our method can produce assimilated global atmospheric data consistent with
observations at 0.25degree resolution. The experiments also show that the
initial conditions that are generated via our approach can be used for forecast
models with a loss of lead time of at most 24 hours when compared to initial
conditions of state-of-the-art data assimilation suites. This enables to apply
the method to real world applications such as the creation of reanalysis
datasets with autoregressive data assimilation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05939">DREQ: Document Re-Ranking Using Entity-based Query Understanding. (arXiv:2401.05939v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chatterjee_S/0/1/0/all/0/1">Shubham Chatterjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Mackie_I/0/1/0/all/0/1">Iain Mackie</a>, <a href="http://arxiv.org/find/cs/1/au:+Dalton_J/0/1/0/all/0/1">Jeff Dalton</a></p>
<p>While entity-oriented neural IR models have advanced significantly, they
often overlook a key nuance: the varying degrees of influence individual
entities within a document have on its overall relevance. Addressing this gap,
we present DREQ, an entity-oriented dense document re-ranking model. Uniquely,
we emphasize the query-relevant entities within a document's representation
while simultaneously attenuating the less relevant ones, thus obtaining a
query-specific entity-centric document representation. We then combine this
entity-centric document representation with the text-centric representation of
the document to obtain a "hybrid" representation of the document. We learn a
relevance score for the document using this hybrid representation. Using four
large-scale benchmarks, we show that DREQ outperforms state-of-the-art neural
and non-neural re-ranking methods, highlighting the effectiveness of our
entity-oriented representation approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05940">Mutation-based Consistency Testing for Evaluating the Code Understanding Capability of LLMs. (arXiv:2401.05940v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Ziyu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Shin_D/0/1/0/all/0/1">Donghwan Shin</a></p>
<p>Large Language Models (LLMs) have shown remarkable capabilities in processing
both natural and programming languages, which have enabled various applications
in software engineering, such as requirement engineering, code generation, and
software testing. However, existing code generation benchmarks do not
necessarily assess the code understanding performance of LLMs, especially for
the subtle inconsistencies that may arise between code and its semantics
described in natural language.
</p>
<p>In this paper, we propose a novel method to systematically assess the code
understanding performance of LLMs, particularly focusing on subtle differences
between code and its descriptions, by introducing code mutations to existing
code generation datasets. Code mutations are small changes that alter the
semantics of the original code, creating a mismatch with the natural language
description. We apply different types of code mutations, such as operator
replacement and statement deletion, to generate inconsistent code-description
pairs. We then use these pairs to test the ability of LLMs to correctly detect
the inconsistencies.
</p>
<p>We propose a new LLM testing method, called Mutation-based Consistency
Testing (MCT), and conduct a case study on the two popular LLMs, GPT-3.5 and
GPT-4, using the state-of-the-art code generation benchmark, HumanEval-X, which
consists of six programming languages (Python, C++, Java, Go, JavaScript, and
Rust). We compare the performance of the LLMs across different types of code
mutations and programming languages and analyze the results. We find that the
LLMs show significant variation in their code understanding performance and
that they have different strengths and weaknesses depending on the mutation
type and language.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05946">Learning Cognitive Maps from Transformer Representations for Efficient Planning in Partially Observed Environments. (arXiv:2401.05946v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dedieu_A/0/1/0/all/0/1">Antoine Dedieu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lehrach_W/0/1/0/all/0/1">Wolfgang Lehrach</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1">Guangyao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+George_D/0/1/0/all/0/1">Dileep George</a>, <a href="http://arxiv.org/find/cs/1/au:+Lazaro_Gredilla_M/0/1/0/all/0/1">Miguel L&#xe1;zaro-Gredilla</a></p>
<p>Despite their stellar performance on a wide range of tasks, including
in-context tasks only revealed during inference, vanilla transformers and
variants trained for next-token predictions (a) do not learn an explicit world
model of their environment which can be flexibly queried and (b) cannot be used
for planning or navigation. In this paper, we consider partially observed
environments (POEs), where an agent receives perceptually aliased observations
as it navigates, which makes path planning hard. We introduce a transformer
with (multiple) discrete bottleneck(s), TDB, whose latent codes learn a
compressed representation of the history of observations and actions. After
training a TDB to predict the future observation(s) given the history, we
extract interpretable cognitive maps of the environment from its active
bottleneck(s) indices. These maps are then paired with an external solver to
solve (constrained) path planning problems. First, we show that a TDB trained
on POEs (a) retains the near perfect predictive performance of a vanilla
transformer or an LSTM while (b) solving shortest path problems exponentially
faster. Second, a TDB extracts interpretable representations from text
datasets, while reaching higher in-context accuracy than vanilla sequence
models. Finally, in new POEs, a TDB (a) reaches near-perfect in-context
accuracy, (b) learns accurate in-context cognitive maps (c) solves in-context
path planning problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05949">Universal Vulnerabilities in Large Language Models: In-context Learning Backdoor Attacks. (arXiv:2401.05949v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1">Shuai Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_M/0/1/0/all/0/1">Meihuizi Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Tuan_L/0/1/0/all/0/1">Luu Anh Tuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1">Jinming Wen</a></p>
<p>In-context learning, a paradigm bridging the gap between pre-training and
fine-tuning, has demonstrated high efficacy in several NLP tasks, especially in
few-shot settings. Unlike traditional fine-tuning methods, in-context learning
adapts pre-trained models to unseen tasks without updating any parameters.
Despite being widely applied, in-context learning is vulnerable to malicious
attacks. In this work, we raise security concerns regarding this paradigm. Our
studies demonstrate that an attacker can manipulate the behavior of large
language models by poisoning the demonstration context, without the need for
fine-tuning the model. Specifically, we have designed a new backdoor attack
method, named ICLAttack, to target large language models based on in-context
learning. Our method encompasses two types of attacks: poisoning demonstration
examples and poisoning prompts, which can make models behave in accordance with
predefined intentions. ICLAttack does not require additional fine-tuning to
implant a backdoor, thus preserving the model's generality. Furthermore, the
poisoned examples are correctly labeled, enhancing the natural stealth of our
attack method. Extensive experimental results across several language models,
ranging in size from 1.3B to 40B parameters, demonstrate the effectiveness of
our attack method, exemplified by a high average attack success rate of 95.0%
across the three datasets on OPT models. Our findings highlight the
vulnerabilities of language models, and we hope this work will raise awareness
of the possible security threats associated with in-context learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05960">Machine Learning Insides OptVerse AI Solver: Design Principles and Applications. (arXiv:2401.05960v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xijun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1">Fangzhou Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhen_H/0/1/0/all/0/1">Hui-Ling Zhen</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1">Weilin Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1">Meng Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yimin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1">Zhenan Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zirui Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuang_Y/0/1/0/all/0/1">Yufei Kuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhihai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Geng_Z/0/1/0/all/0/1">Zijie Geng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Haoyang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+An_Z/0/1/0/all/0/1">Zhiwu An</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Muming Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jianshu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1">Junchi Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1">Defeng Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_T/0/1/0/all/0/1">Tao Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1">Jia Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1">Mingxuan Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1">Jianye Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1">Jun Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_K/0/1/0/all/0/1">Kun Mao</a></p>
<p>In an era of digital ubiquity, efficient resource management and
decision-making are paramount across numerous industries. To this end, we
present a comprehensive study on the integration of machine learning (ML)
techniques into Huawei Cloud's OptVerse AI Solver, which aims to mitigate the
scarcity of real-world mathematical programming instances, and to surpass the
capabilities of traditional optimization techniques. We showcase our methods
for generating complex SAT and MILP instances utilizing generative models that
mirror multifaceted structures of real-world problem. Furthermore, we introduce
a training framework leveraging augmentation policies to maintain solvers'
utility in dynamic environments. Besides the data generation and augmentation,
our proposed approaches also include novel ML-driven policies for personalized
solver strategies, with an emphasis on applications like graph convolutional
networks for initial basis selection and reinforcement learning for advanced
presolving and cut selection. Additionally, we detail the incorporation of
state-of-the-art parameter tuning algorithms which markedly elevate solver
performance. Compared with traditional solvers such as Gurobi and SCIP, our
ML-augmented OptVerse AI Solver demonstrates superior speed and precision
across both established benchmarks and real-world scenarios, reinforcing the
practical imperative and effectiveness of machine learning techniques in
mathematical programming solvers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05964">An attempt to generate new bridge types from latent space of PixelCNN. (arXiv:2401.05964v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongjun Zhang</a></p>
<p>Try to generate new bridge types using generative artificial intelligence
technology. Using symmetric structured image dataset of three-span beam bridge,
arch bridge, cable-stayed bridge and suspension bridge , based on Python
programming language, TensorFlow and Keras deep learning platform framework ,
PixelCNN is constructed and trained. The model can capture the statistical
structure of the images and calculate the probability distribution of the next
pixel when the previous pixels are given. From the obtained latent space
sampling, new bridge types different from the training dataset can be
generated. PixelCNN can organically combine different structural components on
the basis of human original bridge types, creating new bridge types that have a
certain degree of human original ability. Autoregressive models cannot
understand the meaning of the sequence, while multimodal models combine
regression and autoregressive models to understand the sequence. Multimodal
models should be the way to achieve artificial general intelligence in the
future.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05969">Spatial-Aware Deep Reinforcement Learning for the Traveling Officer Problem. (arXiv:2401.05969v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Strauss_N/0/1/0/all/0/1">Niklas Strau&#xdf;</a>, <a href="http://arxiv.org/find/cs/1/au:+Schubert_M/0/1/0/all/0/1">Matthias Schubert</a></p>
<p>The traveling officer problem (TOP) is a challenging stochastic optimization
task. In this problem, a parking officer is guided through a city equipped with
parking sensors to fine as many parking offenders as possible. A major
challenge in TOP is the dynamic nature of parking offenses, which randomly
appear and disappear after some time, regardless of whether they have been
fined. Thus, solutions need to dynamically adjust to currently fineable parking
offenses while also planning ahead to increase the likelihood that the officer
arrives during the offense taking place. Though various solutions exist, these
methods often struggle to take the implications of actions on the ability to
fine future parking violations into account. This paper proposes SATOP, a novel
spatial-aware deep reinforcement learning approach for TOP. Our novel state
encoder creates a representation of each action, leveraging the spatial
relationships between parking spots, the agent, and the action. Furthermore, we
propose a novel message-passing module for learning future inter-action
correlations in the given environment. Thus, the agent can estimate the
potential to fine further parking violations after executing an action. We
evaluate our method using an environment based on real-world data from
Melbourne. Our results show that SATOP consistently outperforms
state-of-the-art TOP agents and is able to fine up to 22% more parking
offenses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05975">End-to-end Learnable Clustering for Intent Learning in Recommendation. (arXiv:2401.05975v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yue Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1">Shihao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1">Jun Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yingwei Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1">Jian Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1">Wenliang Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Guannan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kejun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xinwang Liu</a></p>
<p>Mining users' intents plays a crucial role in sequential recommendation. The
recent approach, ICLRec, was introduced to extract underlying users' intents
using contrastive learning and clustering. While it has shown effectiveness,
the existing method suffers from complex and cumbersome alternating
optimization, leading to two main issues. Firstly, the separation of
representation learning and clustering optimization within a generalized
expectation maximization (EM) framework often results in sub-optimal
performance. Secondly, performing clustering on the entire dataset hampers
scalability for large-scale industry data. To address these challenges, we
propose a novel intent learning method called \underline{ELCRec}, which
integrates representation learning into an \underline{E}nd-to-end
\underline{L}earnable \underline{C}lustering framework for
\underline{Rec}ommendation. Specifically, we encode users' behavior sequences
and initialize the cluster centers as learnable network parameters.
Additionally, we design a clustering loss that guides the networks to
differentiate between different cluster centers and pull similar samples
towards their respective cluster centers. This allows simultaneous optimization
of recommendation and clustering using mini-batch data. Moreover, we leverage
the learned cluster centers as self-supervision signals for representation
learning, resulting in further enhancement of recommendation performance.
Extensive experiments conducted on open benchmarks and industry data validate
the superiority, effectiveness, and efficiency of our proposed ELCRec method.
Code is available at: https://github.com/yueliu1999/ELCRec.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05998">Combating Adversarial Attacks with Multi-Agent Debate. (arXiv:2401.05998v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chern_S/0/1/0/all/0/1">Steffi Chern</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1">Zhen Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1">Andy Liu</a></p>
<p>While state-of-the-art language models have achieved impressive results, they
remain susceptible to inference-time adversarial attacks, such as adversarial
prompts generated by red teams <a href="/abs/2209.07858">arXiv:2209.07858</a>. One approach proposed to
improve the general quality of language model generations is multi-agent
debate, where language models self-evaluate through discussion and feedback
<a href="/abs/2305.14325">arXiv:2305.14325</a>. We implement multi-agent debate between current
state-of-the-art language models and evaluate models' susceptibility to red
team attacks in both single- and multi-agent settings. We find that multi-agent
debate can reduce model toxicity when jailbroken or less capable models are
forced to debate with non-jailbroken or more capable models. We also find
marginal improvements through the general usage of multi-agent interactions. We
further perform adversarial prompt content classification via embedding
clustering, and analyze the susceptibility of different models to different
types of attack topics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06005">How does the primate brain combine generative and discriminative computations in vision?. (arXiv:2401.06005v1 [q-bio.NC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Peters_B/0/1/0/all/0/1">Benjamin Peters</a>, <a href="http://arxiv.org/find/q-bio/1/au:+DiCarlo_J/0/1/0/all/0/1">James J. DiCarlo</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Gureckis_T/0/1/0/all/0/1">Todd Gureckis</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Haefner_R/0/1/0/all/0/1">Ralf Haefner</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Isik_L/0/1/0/all/0/1">Leyla Isik</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Tenenbaum_J/0/1/0/all/0/1">Joshua Tenenbaum</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Konkle_T/0/1/0/all/0/1">Talia Konkle</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Naselaris_T/0/1/0/all/0/1">Thomas Naselaris</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Stachenfeld_K/0/1/0/all/0/1">Kimberly Stachenfeld</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Tavares_Z/0/1/0/all/0/1">Zenna Tavares</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Tsao_D/0/1/0/all/0/1">Doris Tsao</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Yildirim_I/0/1/0/all/0/1">Ilker Yildirim</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Kriegeskorte_N/0/1/0/all/0/1">Nikolaus Kriegeskorte</a></p>
<p>Vision is widely understood as an inference problem. However, two contrasting
conceptions of the inference process have each been influential in research on
biological vision as well as the engineering of machine vision. The first
emphasizes bottom-up signal flow, describing vision as a largely feedforward,
discriminative inference process that filters and transforms the visual
information to remove irrelevant variation and represent behaviorally relevant
information in a format suitable for downstream functions of cognition and
behavioral control. In this conception, vision is driven by the sensory data,
and perception is direct because the processing proceeds from the data to the
latent variables of interest. The notion of "inference" in this conception is
that of the engineering literature on neural networks, where feedforward
convolutional neural networks processing images are said to perform inference.
The alternative conception is that of vision as an inference process in
Helmholtz's sense, where the sensory evidence is evaluated in the context of a
generative model of the causal processes giving rise to it. In this conception,
vision inverts a generative model through an interrogation of the evidence in a
process often thought to involve top-down predictions of sensory data to
evaluate the likelihood of alternative hypotheses. The authors include
scientists rooted in roughly equal numbers in each of the conceptions and
motivated to overcome what might be a false dichotomy between them and engage
the other perspective in the realm of theory and experiment. The primate brain
employs an unknown algorithm that may combine the advantages of both
conceptions. We explain and clarify the terminology, review the key empirical
evidence, and propose an empirical research program that transcends the
dichotomy and sets the stage for revealing the mysterious hybrid algorithm of
primate vision.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06013">Surgical-DINO: Adapter Learning of Foundation Model for Depth Estimation in Endoscopic Surgery. (arXiv:2401.06013v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Beilei_C/0/1/0/all/0/1">Cui Beilei</a>, <a href="http://arxiv.org/find/cs/1/au:+Mobarakol_I/0/1/0/all/0/1">Islam Mobarakol</a>, <a href="http://arxiv.org/find/cs/1/au:+Long_B/0/1/0/all/0/1">Bai Long</a>, <a href="http://arxiv.org/find/cs/1/au:+Hongliang_R/0/1/0/all/0/1">Ren Hongliang</a></p>
<p>Purpose: Depth estimation in robotic surgery is vital in 3D reconstruction,
surgical navigation and augmented reality visualization. Although the
foundation model exhibits outstanding performance in many vision tasks,
including depth estimation (e.g., DINOv2), recent works observed its
limitations in medical and surgical domain-specific applications. This work
presents a low-ranked adaptation (LoRA) of the foundation model for surgical
depth estimation. Methods: We design a foundation model-based depth estimation
method, referred to as Surgical-DINO, a low-rank adaptation of the DINOv2 for
depth estimation in endoscopic surgery. We build LoRA layers and integrate them
into DINO to adapt with surgery-specific domain knowledge instead of
conventional fine-tuning. During training, we freeze the DINO image encoder,
which shows excellent visual representation capacity, and only optimize the
LoRA layers and depth decoder to integrate features from the surgical scene.
Results: Our model is extensively validated on a MICCAI challenge dataset of
SCARED, which is collected from da Vinci Xi endoscope surgery. We empirically
show that Surgical-DINO significantly outperforms all the state-of-the-art
models in endoscopic depth estimation tasks. The analysis with ablation studies
has shown evidence of the remarkable effect of our LoRA layers and adaptation.
Conclusion: Surgical-DINO shed some light on the successful adaptation of the
foundation models into the surgical domain for depth estimation. There is clear
evidence in the results that zero-shot prediction on pre-trained weights in
computer vision datasets or naive fine-tuning is not sufficient to use the
foundation model in the surgical domain directly. Code is available at
https://github.com/BeileiCui/SurgicalDINO.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06048">On the Power of Graph Neural Networks and Feature Augmentation Strategies to Classify Social Networks. (arXiv:2401.06048v1 [cs.SI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guettala_W/0/1/0/all/0/1">Walid Guettala</a>, <a href="http://arxiv.org/find/cs/1/au:+Gulyas_L/0/1/0/all/0/1">L&#xe1;szl&#xf3; Guly&#xe1;s</a></p>
<p>This paper studies four Graph Neural Network architectures (GNNs) for a graph
classification task on a synthetic dataset created using classic generative
models of Network Science. Since the synthetic networks do not contain (node or
edge) features, five different augmentation strategies (artificial feature
types) are applied to nodes. All combinations of the 4 GNNs (GCN with
Hierarchical and Global aggregation, GIN and GATv2) and the 5 feature types
(constant 1, noise, degree, normalized degree and ID -- a vector of the number
of cycles of various lengths) are studied and their performances compared as a
function of the hidden dimension of artificial neural networks used in the
GNNs. The generalisation ability of these models is also analysed using a
second synthetic network dataset (containing networks of different sizes).Our
results point towards the balanced importance of the computational power of the
GNN architecture and the the information level provided by the artificial
features. GNN architectures with higher computational power, like GIN and
GATv2, perform well for most augmentation strategies. On the other hand,
artificial features with higher information content, like ID or degree, not
only consistently outperform other augmentation strategies, but can also help
GNN architectures with lower computational power to achieve good performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06059">Investigating Data Contamination for Pre-training Language Models. (arXiv:2401.06059v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1">Minhao Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1">Ken Ziyu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1">Ming Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Schaeffer_R/0/1/0/all/0/1">Rylan Schaeffer</a>, <a href="http://arxiv.org/find/cs/1/au:+Ouyang_S/0/1/0/all/0/1">Siru Ouyang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Jiawei Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Koyejo_S/0/1/0/all/0/1">Sanmi Koyejo</a></p>
<p>Language models pre-trained on web-scale corpora demonstrate impressive
capabilities on diverse downstream tasks. However, there is increasing concern
whether such capabilities might arise from evaluation datasets being included
in the pre-training corpus -- a phenomenon known as \textit{data contamination}
-- in a manner that artificially increases performance. There has been little
understanding of how this potential contamination might influence LMs'
performance on downstream tasks. In this paper, we explore the impact of data
contamination at the pre-training stage by pre-training a series of GPT-2
models \textit{from scratch}. We highlight the effect of both text
contamination (\textit{i.e.}\ input text of the evaluation samples) and
ground-truth contamination (\textit{i.e.}\ the prompts asked on the input and
the desired outputs) from evaluation data. We also investigate the effects of
repeating contamination for various downstream tasks. Additionally, we examine
the prevailing n-gram-based definitions of contamination within current LLM
reports, pinpointing their limitations and inadequacy. Our findings offer new
insights into data contamination's effects on language model capabilities and
underscore the need for independent, comprehensive contamination assessments in
LLM studies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06072">Chain of History: Learning and Forecasting with LLMs for Temporal Knowledge Graph Completion. (arXiv:2401.06072v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1">Ruilin Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_T/0/1/0/all/0/1">Tianle Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Haoling Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Junzhe Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zicheng Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiayi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yujiu Yang</a></p>
<p>Temporal Knowledge Graph Completion (TKGC) is a challenging task of
predicting missing event links at future timestamps by leveraging established
temporal structural knowledge. Given the formidable generative capabilities
inherent in LLMs (LLMs), this paper proposes a novel approach to conceptualize
temporal link prediction as an event generation task within the context of a
historical event chain. We employ efficient fine-tuning methods to make LLMs
adapt to specific graph textual information and patterns discovered in temporal
timelines. Furthermore, we introduce structure-based historical data
augmentation and the integration of reverse knowledge to emphasize LLMs'
awareness of structural information, thereby enhancing their reasoning
capabilities. We conduct thorough experiments on multiple widely used datasets
and find that our fine-tuned model outperforms existing embedding-based models
on multiple metrics, achieving SOTA results. We also carry out sufficient
ablation experiments to explore the key influencing factors when LLMs perform
structured temporal knowledge inference tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06080">Secrets of RLHF in Large Language Models Part II: Reward Modeling. (arXiv:2401.06080v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Binghai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1">Rui Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dou_S/0/1/0/all/0/1">Shihan Dou</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1">Caishuang Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1">Wei Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1">Senjie Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_E/0/1/0/all/0/1">Enyu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1">Chenyu Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1">Songyang Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1">Nuo Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yuhao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1">Xiaoran Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1">Zhiheng Xi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jun Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_T/0/1/0/all/0/1">Tao Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1">Hang Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1">Lixing Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1">Tao Gui</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1">Xipeng Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xuanjing Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zuxuan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yu-Gang Jiang</a></p>
<p>Reinforcement Learning from Human Feedback (RLHF) has become a crucial
technology for aligning language models with human values and intentions,
enabling models to produce more helpful and harmless responses. Reward models
are trained as proxies for human preferences to drive reinforcement learning
optimization. While reward models are often considered central to achieving
high performance, they face the following challenges in practical applications:
(1) Incorrect and ambiguous preference pairs in the dataset may hinder the
reward model from accurately capturing human intent. (2) Reward models trained
on data from a specific distribution often struggle to generalize to examples
outside that distribution and are not suitable for iterative RLHF training.
</p>
<p>In this report, we attempt to address these two issues. (1) From a data
perspective, we propose a method to measure the strength of preferences within
the data, based on a voting mechanism of multiple reward models. Experimental
results confirm that data with varying preference strengths have different
impacts on reward model performance. We introduce a series of novel methods to
mitigate the influence of incorrect and ambiguous preferences in the dataset
and fully leverage high-quality preference data. (2) From an algorithmic
standpoint, we introduce contrastive learning to enhance the ability of reward
models to distinguish between chosen and rejected responses, thereby improving
model generalization. Furthermore, we employ meta-learning to enable the reward
model to maintain the ability to differentiate subtle differences in
out-of-distribution samples, and this approach can be utilized for iterative
RLHF optimization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06086">XGBoost Learning of Dynamic Wager Placement for In-Play Betting on an Agent-Based Model of a Sports Betting Exchange. (arXiv:2401.06086v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Terawong_C/0/1/0/all/0/1">Chawin Terawong</a>, <a href="http://arxiv.org/find/cs/1/au:+Cliff_D/0/1/0/all/0/1">Dave Cliff</a></p>
<p>We present first results from the use of XGBoost, a highly effective machine
learning (ML) method, within the Bristol Betting Exchange (BBE), an open-source
agent-based model (ABM) designed to simulate a contemporary sports-betting
exchange with in-play betting during track-racing events such as horse races.
We use the BBE ABM and its array of minimally-simple bettor-agents as a
synthetic data generator which feeds into our XGBoost ML system, with the
intention that XGBoost discovers profitable dynamic betting strategies by
learning from the more profitable bets made by the BBE bettor-agents. After
this XGBoost training, which results in one or more decision trees, a
bettor-agent with a betting strategy determined by the XGBoost-learned decision
tree(s) is added to the BBE ABM and made to bet on a sequence of races under
various conditions and betting-market scenarios, with profitability serving as
the primary metric of comparison and evaluation. Our initial findings presented
here show that XGBoost trained in this way can indeed learn profitable betting
strategies, and can generalise to learn strategies that outperform each of the
set of strategies used for creation of the training data. To foster further
research and enhancements, the complete version of our extended BBE, including
the XGBoost integration, has been made freely available as an open-source
release on GitHub.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06088">Autocompletion of Chief Complaints in the Electronic Health Records using Large Language Models. (arXiv:2401.06088v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Islam_K/0/1/0/all/0/1">K M Sajjadul Islam</a>, <a href="http://arxiv.org/find/cs/1/au:+Nipu_A/0/1/0/all/0/1">Ayesha Siddika Nipu</a>, <a href="http://arxiv.org/find/cs/1/au:+Madiraju_P/0/1/0/all/0/1">Praveen Madiraju</a>, <a href="http://arxiv.org/find/cs/1/au:+Deshpande_P/0/1/0/all/0/1">Priya Deshpande</a></p>
<p>The Chief Complaint (CC) is a crucial component of a patient's medical record
as it describes the main reason or concern for seeking medical care. It
provides critical information for healthcare providers to make informed
decisions about patient care. However, documenting CCs can be time-consuming
for healthcare providers, especially in busy emergency departments. To address
this issue, an autocompletion tool that suggests accurate and well-formatted
phrases or sentences for clinical notes can be a valuable resource for triage
nurses. In this study, we utilized text generation techniques to develop
machine learning models using CC data. In our proposed work, we train a Long
Short-Term Memory (LSTM) model and fine-tune three different variants of
Biomedical Generative Pretrained Transformers (BioGPT), namely
microsoft/biogpt, microsoft/BioGPT-Large, and microsoft/BioGPT-Large-PubMedQA.
Additionally, we tune a prompt by incorporating exemplar CC sentences,
utilizing the OpenAI API of GPT-4. We evaluate the models' performance based on
the perplexity score, modified BERTScore, and cosine similarity score. The
results show that BioGPT-Large exhibits superior performance compared to the
other models. It consistently achieves a remarkably low perplexity score of
1.65 when generating CC, whereas the baseline LSTM model achieves the best
perplexity score of 170. Further, we evaluate and assess the proposed models'
performance and the outcome of GPT-4.0. Our study demonstrates that utilizing
LLMs such as BioGPT, leads to the development of an effective autocompletion
tool for generating CC documentation in healthcare settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06102">Patchscope: A Unifying Framework for Inspecting Hidden Representations of Language Models. (arXiv:2401.06102v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ghandeharioun_A/0/1/0/all/0/1">Asma Ghandeharioun</a>, <a href="http://arxiv.org/find/cs/1/au:+Caciularu_A/0/1/0/all/0/1">Avi Caciularu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pearce_A/0/1/0/all/0/1">Adam Pearce</a>, <a href="http://arxiv.org/find/cs/1/au:+Dixon_L/0/1/0/all/0/1">Lucas Dixon</a>, <a href="http://arxiv.org/find/cs/1/au:+Geva_M/0/1/0/all/0/1">Mor Geva</a></p>
<p>Inspecting the information encoded in hidden representations of large
language models (LLMs) can explain models' behavior and verify their alignment
with human values. Given the capabilities of LLMs in generating
human-understandable text, we propose leveraging the model itself to explain
its internal representations in natural language. We introduce a framework
called Patchscopes and show how it can be used to answer a wide range of
research questions about an LLM's computation. We show that prior
interpretability methods based on projecting representations into the
vocabulary space and intervening on the LLM computation, can be viewed as
special instances of this framework. Moreover, several of their shortcomings
such as failure in inspecting early layers or lack of expressivity can be
mitigated by a Patchscope. Beyond unifying prior inspection techniques,
Patchscopes also opens up new possibilities such as using a more capable model
to explain the representations of a smaller model, and unlocks new applications
such as self-correction in multi-hop reasoning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06122">Manipulating Feature Visualizations with Gradient Slingshots. (arXiv:2401.06122v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bareeva_D/0/1/0/all/0/1">Dilyara Bareeva</a>, <a href="http://arxiv.org/find/cs/1/au:+Hohne_M/0/1/0/all/0/1">Marina M.-C. H&#xf6;hne</a>, <a href="http://arxiv.org/find/cs/1/au:+Warnecke_A/0/1/0/all/0/1">Alexander Warnecke</a>, <a href="http://arxiv.org/find/cs/1/au:+Pirch_L/0/1/0/all/0/1">Lukas Pirch</a>, <a href="http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1">Klaus-Robert M&#xfc;ller</a>, <a href="http://arxiv.org/find/cs/1/au:+Rieck_K/0/1/0/all/0/1">Konrad Rieck</a>, <a href="http://arxiv.org/find/cs/1/au:+Bykov_K/0/1/0/all/0/1">Kirill Bykov</a></p>
<p>Deep Neural Networks (DNNs) are capable of learning complex and versatile
representations, however, the semantic nature of the learned concepts remains
unknown. A common method used to explain the concepts learned by DNNs is
Activation Maximization (AM), which generates a synthetic input signal that
maximally activates a particular neuron in the network. In this paper, we
investigate the vulnerability of this approach to adversarial model
manipulations and introduce a novel method for manipulating feature
visualization without altering the model architecture or significantly
impacting the model's decision-making process. We evaluate the effectiveness of
our method on several neural network models and demonstrate its capabilities to
hide the functionality of specific neurons by masking the original explanations
of neurons with chosen target explanations during model auditing. As a remedy,
we propose a protective measure against such manipulations and provide
quantitative evidence which substantiates our findings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06127">E$^{2}$GAN: Efficient Training of Efficient GANs for Image-to-Image Translation. (arXiv:2401.06127v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1">Yifan Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhan_Z/0/1/0/all/0/1">Zheng Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1">Qing Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yanyu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Idelbayev_Y/0/1/0/all/0/1">Yerlan Idelbayev</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xian Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zharkov_A/0/1/0/all/0/1">Andrey Zharkov</a>, <a href="http://arxiv.org/find/cs/1/au:+Aberman_K/0/1/0/all/0/1">Kfir Aberman</a>, <a href="http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1">Sergey Tulyakov</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yanzhi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1">Jian Ren</a></p>
<p>One highly promising direction for enabling flexible real-time on-device
image editing is utilizing data distillation by leveraging large-scale
text-to-image diffusion models, such as Stable Diffusion, to generate paired
datasets used for training generative adversarial networks (GANs). This
approach notably alleviates the stringent requirements typically imposed by
high-end commercial GPUs for performing image editing with diffusion models.
However, unlike text-to-image diffusion models, each distilled GAN is
specialized for a specific image editing task, necessitating costly training
efforts to obtain models for various concepts. In this work, we introduce and
address a novel research direction: can the process of distilling GANs from
diffusion models be made significantly more efficient? To achieve this goal, we
propose a series of innovative techniques. First, we construct a base GAN model
with generalized features, adaptable to different concepts through fine-tuning,
eliminating the need for training from scratch. Second, we identify crucial
layers within the base GAN model and employ Low-Rank Adaptation (LoRA) with a
simple yet effective rank search process, rather than fine-tuning the entire
base model. Third, we investigate the minimal amount of data necessary for
fine-tuning, further reducing the overall training time. Extensive experiments
show that we can efficiently empower GANs with the ability to perform real-time
high-quality image editing on mobile devices with remarkable reduced training
cost and storage for each concept.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.08626">CP-PINNs: Changepoints Detection in PDEs using Physics Informed Neural Networks with Total-Variation Penalty. (arXiv:2208.08626v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Dong_Z/0/1/0/all/0/1">Zhikang Dong</a>, <a href="http://arxiv.org/find/stat/1/au:+Polak_P/0/1/0/all/0/1">Pawel Polak</a></p>
<p>The paper shows that Physics-Informed Neural Networks (PINNs) can fail to
estimate the correct Partial Differential Equations (PDEs) dynamics in cases of
unknown changepoints in the parameters. To address this, we propose a new
CP-PINNs model which integrates PINNs with Total-Variation penalty for accurate
changepoints detection and PDEs discovery. In order to optimally combine the
tasks of model fitting, PDEs discovery, and changepoints detection, we develop
a new meta-learning algorithm that exploits batch learning to dynamically
refines the optimization objective when moving over the consecutive batches of
the data. Empirically, in case of changepoints in the dynamics, our approach
demonstrates accurate parameter estimation and model alignment, and in case of
no changepoints in the data, it converges numerically to the solution from the
original PINNs model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.16162">Scalable Hierarchical Over-the-Air Federated Learning. (arXiv:2211.16162v3 [cs.IT] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Azimi_Abarghouyi_S/0/1/0/all/0/1">Seyed Mohammad Azimi-Abarghouyi</a>, <a href="http://arxiv.org/find/cs/1/au:+Fodor_V/0/1/0/all/0/1">Viktoria Fodor</a></p>
<p>When implementing hierarchical federated learning over wireless networks,
scalability assurance and the ability to handle both interference and device
data heterogeneity are crucial. This work introduces a new two-level learning
method designed to address these challenges, along with a scalable over-the-air
aggregation scheme for the uplink and a bandwidth-limited broadcast scheme for
the downlink that efficiently use a single wireless resource. To provide
resistance against data heterogeneity, we employ gradient aggregations.
Meanwhile, the impact of uplink and downlink interference is minimized through
optimized receiver normalizing factors. We present a comprehensive mathematical
approach to derive the convergence bound for the proposed algorithm, applicable
to a multi-cluster wireless network encompassing any count of collaborating
clusters, and provide special cases and design remarks. As a key step to enable
a tractable analysis, we develop a spatial model for the setup by modeling
devices as a Poisson cluster process over the edge servers and rigorously
quantify uplink and downlink error terms due to the interference. Finally, we
show that despite the interference and data heterogeneity, the proposed
algorithm not only achieves high learning accuracy for a variety of parameters
but also significantly outperforms the conventional hierarchical learning
algorithm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.01354">Designing Ecosystems of Intelligence from First Principles. (arXiv:2212.01354v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Friston_K/0/1/0/all/0/1">Karl J Friston</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramstead_M/0/1/0/all/0/1">Maxwell J D Ramstead</a>, <a href="http://arxiv.org/find/cs/1/au:+Kiefer_A/0/1/0/all/0/1">Alex B Kiefer</a>, <a href="http://arxiv.org/find/cs/1/au:+Tschantz_A/0/1/0/all/0/1">Alexander Tschantz</a>, <a href="http://arxiv.org/find/cs/1/au:+Buckley_C/0/1/0/all/0/1">Christopher L Buckley</a>, <a href="http://arxiv.org/find/cs/1/au:+Albarracin_M/0/1/0/all/0/1">Mahault Albarracin</a>, <a href="http://arxiv.org/find/cs/1/au:+Pitliya_R/0/1/0/all/0/1">Riddhi J Pitliya</a>, <a href="http://arxiv.org/find/cs/1/au:+Heins_C/0/1/0/all/0/1">Conor Heins</a>, <a href="http://arxiv.org/find/cs/1/au:+Klein_B/0/1/0/all/0/1">Brennan Klein</a>, <a href="http://arxiv.org/find/cs/1/au:+Millidge_B/0/1/0/all/0/1">Beren Millidge</a>, <a href="http://arxiv.org/find/cs/1/au:+Sakthivadivel_D/0/1/0/all/0/1">Dalton A R Sakthivadivel</a>, <a href="http://arxiv.org/find/cs/1/au:+Smithe_T/0/1/0/all/0/1">Toby St Clere Smithe</a>, <a href="http://arxiv.org/find/cs/1/au:+Koudahl_M/0/1/0/all/0/1">Magnus Koudahl</a>, <a href="http://arxiv.org/find/cs/1/au:+Tremblay_S/0/1/0/all/0/1">Safae Essafi Tremblay</a>, <a href="http://arxiv.org/find/cs/1/au:+Petersen_C/0/1/0/all/0/1">Capm Petersen</a>, <a href="http://arxiv.org/find/cs/1/au:+Fung_K/0/1/0/all/0/1">Kaiser Fung</a>, <a href="http://arxiv.org/find/cs/1/au:+Fox_J/0/1/0/all/0/1">Jason G Fox</a>, <a href="http://arxiv.org/find/cs/1/au:+Swanson_S/0/1/0/all/0/1">Steven Swanson</a>, <a href="http://arxiv.org/find/cs/1/au:+Mapes_D/0/1/0/all/0/1">Dan Mapes</a>, <a href="http://arxiv.org/find/cs/1/au:+Rene_G/0/1/0/all/0/1">Gabriel Ren&#xe9;</a></p>
<p>This white paper lays out a vision of research and development in the field
of artificial intelligence for the next decade (and beyond). Its denouement is
a cyber-physical ecosystem of natural and synthetic sense-making, in which
humans are integral participants -- what we call ''shared intelligence''. This
vision is premised on active inference, a formulation of adaptive behavior that
can be read as a physics of intelligence, and which inherits from the physics
of self-organization. In this context, we understand intelligence as the
capacity to accumulate evidence for a generative model of one's sensed world --
also known as self-evidencing. Formally, this corresponds to maximizing
(Bayesian) model evidence, via belief updating over several scales: i.e.,
inference, learning, and model selection. Operationally, this self-evidencing
can be realized via (variational) message passing or belief propagation on a
factor graph. Crucially, active inference foregrounds an existential imperative
of intelligent systems; namely, curiosity or the resolution of uncertainty.
This same imperative underwrites belief sharing in ensembles of agents, in
which certain aspects (i.e., factors) of each agent's generative world model
provide a common ground or frame of reference. Active inference plays a
foundational role in this ecology of belief sharing -- leading to a formal
account of collective intelligence that rests on shared narratives and goals.
We also consider the kinds of communication protocols that must be developed to
enable such an ecosystem of intelligences and motivate the development of a
shared hyper-spatial modeling language and transaction protocol, as a first --
and key -- step towards such an ecology.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.03350">To Be Forgotten or To Be Fair: Unveiling Fairness Implications of Machine Unlearning Methods. (arXiv:2302.03350v2 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dawen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1">Shidong Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1">Thong Hoang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1">Zhenchang Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Staples_M/0/1/0/all/0/1">Mark Staples</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiwei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1">Lina Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1">Qinghua Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1">Liming Zhu</a></p>
<p>The right to be forgotten (RTBF) is motivated by the desire of people not to
be perpetually disadvantaged by their past deeds. For this, data deletion needs
to be deep and permanent, and should be removed from machine learning models.
Researchers have proposed machine unlearning algorithms which aim to erase
specific data from trained models more efficiently. However, these methods
modify how data is fed into the model and how training is done, which may
subsequently compromise AI ethics from the fairness perspective. To help
software engineers make responsible decisions when adopting these unlearning
methods, we present the first study on machine unlearning methods to reveal
their fairness implications. We designed and conducted experiments on two
typical machine unlearning methods (SISA and AmnesiacML) along with a
retraining method (ORTR) as baseline using three fairness datasets under three
different deletion strategies. Experimental results show that under non-uniform
data deletion, SISA leads to better fairness compared with ORTR and AmnesiacML,
while initial training and uniform data deletion do not necessarily affect the
fairness of all three methods. These findings have exposed an important
research problem in software engineering, and can help practitioners better
understand the potential trade-offs on fairness when considering solutions for
RTBF.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.14383">Linear Spaces of Meanings: Compositional Structures in Vision-Language Models. (arXiv:2302.14383v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Trager_M/0/1/0/all/0/1">Matthew Trager</a>, <a href="http://arxiv.org/find/cs/1/au:+Perera_P/0/1/0/all/0/1">Pramuditha Perera</a>, <a href="http://arxiv.org/find/cs/1/au:+Zancato_L/0/1/0/all/0/1">Luca Zancato</a>, <a href="http://arxiv.org/find/cs/1/au:+Achille_A/0/1/0/all/0/1">Alessandro Achille</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhatia_P/0/1/0/all/0/1">Parminder Bhatia</a>, <a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1">Stefano Soatto</a></p>
<p>We investigate compositional structures in data embeddings from pre-trained
vision-language models (VLMs). Traditionally, compositionality has been
associated with algebraic operations on embeddings of words from a pre-existing
vocabulary. In contrast, we seek to approximate representations from an encoder
as combinations of a smaller set of vectors in the embedding space. These
vectors can be seen as "ideal words" for generating concepts directly within
the embedding space of the model. We first present a framework for
understanding compositional structures from a geometric perspective. We then
explain what these compositional structures entail probabilistically in the
case of VLM embeddings, providing intuitions for why they arise in practice.
Finally, we empirically explore these structures in CLIP's embeddings and we
evaluate their usefulness for solving different vision-language tasks such as
classification, debiasing, and retrieval. Our results show that simple linear
algebraic operations on embedding vectors can be used as compositional and
interpretable methods for regulating the behavior of VLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.08115">Human-Inspired Framework to Accelerate Reinforcement Learning. (arXiv:2303.08115v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Beikmohammadi_A/0/1/0/all/0/1">Ali Beikmohammadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Magnusson_S/0/1/0/all/0/1">Sindri Magn&#xfa;sson</a></p>
<p>Reinforcement learning (RL) is crucial for data science decision-making but
suffers from sample inefficiency, particularly in real-world scenarios with
costly physical interactions. This paper introduces a novel human-inspired
framework to enhance RL algorithm sample efficiency. It achieves this by
initially exposing the learning agent to simpler tasks that progressively
increase in complexity, ultimately leading to the main task. This method
requires no pre-training and involves learning simpler tasks for just one
iteration. The resulting knowledge can facilitate various transfer learning
approaches, such as value and policy transfer, without increasing computational
complexity. It can be applied across different goals, environments, and RL
algorithms, including value-based, policy-based, tabular, and deep RL methods.
Experimental evaluations demonstrate the framework's effectiveness in enhancing
sample efficiency, especially in challenging main tasks, demonstrated through
both a simple Random Walk and more complex optimal control problems with
constraints.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.05352">A Taxonomy of Foundation Model based Systems through the Lens of Software Architecture. (arXiv:2305.05352v5 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1">Qinghua Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1">Liming Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiwei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yue Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1">Zhenchang Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Whittle_J/0/1/0/all/0/1">Jon Whittle</a></p>
<p>The recent release of large language model (LLM) based chatbots, such as
ChatGPT, has attracted huge interest in foundation models. It is widely
believed that foundation models will serve as the fundamental building blocks
for future AI systems. As foundation models are in their early stages, the
design of foundation model based systems has not yet been systematically
explored. There is limited understanding about the impact of introducing
foundation models in software architecture. Therefore, in this paper, we
propose a taxonomy of foundation model based systems, which classifies and
compares the characteristics of foundation models and design options of
foundation model based systems. Our taxonomy comprises three categories: the
pretraining and adaptation of foundation models, the architecture design of
foundation model based systems, and responsible-AI-by-design. This taxonomy can
serve as concrete guidance for making major architectural design decisions when
designing foundation model based systems and highlights trade-offs arising from
design decisions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.11731">Persian Typographical Error Type Detection Using Deep Neural Networks on Algorithmically-Generated Misspellings. (arXiv:2305.11731v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1">Mohammad Dehghani</a>, <a href="http://arxiv.org/find/cs/1/au:+Faili_H/0/1/0/all/0/1">Heshaam Faili</a></p>
<p>Spelling correction is a remarkable challenge in the field of natural
language processing. The objective of spelling correction tasks is to recognize
and rectify spelling errors automatically. The development of applications that
can effectually diagnose and correct Persian spelling and grammatical errors
has become more important in order to improve the quality of Persian text. The
Typographical Error Type Detection in Persian is a relatively understudied
area. Therefore, this paper presents a compelling approach for detecting
typographical errors in Persian texts. Our work includes the presentation of a
publicly available dataset called FarsTypo, which comprises 3.4 million words
arranged in chronological order and tagged with their corresponding
part-of-speech. These words cover a wide range of topics and linguistic styles.
We develop an algorithm designed to apply Persian-specific errors to a scalable
portion of these words, resulting in a parallel dataset of correct and
incorrect words. By leveraging FarsTypo, we establish a strong foundation and
conduct a thorough comparison of various methodologies employing different
architectures. Additionally, we introduce a groundbreaking Deep Sequential
Neural Network that utilizes both word and character embeddings, along with
bidirectional LSTM layers, for token classification aimed at detecting
typographical errors across 51 distinct classes. Our approach is contrasted
with highly advanced industrial systems that, unlike this study, have been
developed using a diverse range of resources. The outcomes of our final method
proved to be highly competitive, achieving an accuracy of 97.62%, precision of
98.83%, recall of 98.61%, and surpassing others in terms of speed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.17147">Heterogeneous Value Alignment Evaluation for Large Language Models. (arXiv:2305.17147v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhaowei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Ceyao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1">Nian Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_S/0/1/0/all/0/1">Siyuan Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rong_Z/0/1/0/all/0/1">Ziqi Rong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1">Song-Chun Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1">Shuguang Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yaodong Yang</a></p>
<p>The emergent capabilities of Large Language Models (LLMs) have made it
crucial to align their values with those of humans. However, current
methodologies typically attempt to assign value as an attribute to LLMs, yet
lack attention to the ability to pursue value and the importance of
transferring heterogeneous values in specific practical applications. In this
paper, we propose a Heterogeneous Value Alignment Evaluation (HVAE) system,
designed to assess the success of aligning LLMs with heterogeneous values.
Specifically, our approach first brings the Social Value Orientation (SVO)
framework from social psychology, which corresponds to how much weight a person
attaches to the welfare of others in relation to their own. We then assign the
LLMs with different social values and measure whether their behaviors align
with the inducing values. We conduct evaluations with new auto-metric
\textit{value rationality} to represent the ability of LLMs to align with
specific values. Evaluating the value rationality of five mainstream LLMs, we
discern a propensity in LLMs towards neutral values over pronounced personal
values. By examining the behavior of these LLMs, we contribute to a deeper
insight into the value alignment of LLMs within a heterogeneous value system.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19604">Medication Recommendation via Domain Knowledge Informed Deep Learning. (arXiv:2305.19604v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Sicen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaolong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xianbing Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hao Chen</a></p>
<p>Medication recommendation is a fundamental yet crucial branch of healthcare,
which provides opportunities to support clinical physicians with more accurate
medication prescriptions for patients with complex health conditions. Learning
from electronic health records (EHR) to recommend medications is the most
common way in previous studies. However, most of them neglect incorporating
domain knowledge according to the clinical manifestations in the EHR of the
patient. To address these issues, we propose a novel \textbf{D}omain
\textbf{K}nowledge \textbf{I}nformed \textbf{Net}work (DKINet) to integrate
domain knowledge with observable clinical manifestations of the patient, which
is the first dynamic domain knowledge informed framework toward medication
recommendation. In particular, we first design a knowledge-driven encoder to
capture the domain information and then develop a data-driven encoder to
integrate domain knowledge into the observable EHR. To endow the model with the
capability of temporal decision, we design an explicit medication encoder for
learning the longitudinal dependence of the patient. Extensive experiments on
three publicly available datasets verify the superiority of our method. The
code will be public upon acceptance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.02717">TL-nvSRAM-CIM: Ultra-High-Density Three-Level ReRAM-Assisted Computing-in-nvSRAM with DC-Power Free Restore and Ternary MAC Operations. (arXiv:2307.02717v2 [cs.AR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dengfeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1">Liukai Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Songyuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yiming Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1">Weifeng He</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xueqing Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yanan Sun</a></p>
<p>Accommodating all the weights on-chip for large-scale NNs remains a great
challenge for SRAM based computing-in-memory (SRAM-CIM) with limited on-chip
capacity. Previous non-volatile SRAM-CIM (nvSRAM-CIM) addresses this issue by
integrating high-density single-level ReRAMs on the top of high-efficiency
SRAM-CIM for weight storage to eliminate the off-chip memory access. However,
previous SL-nvSRAM-CIM suffers from poor scalability for an increased number of
SL-ReRAMs and limited computing efficiency. To overcome these challenges, this
work proposes an ultra-high-density three-level ReRAMs-assisted
computing-in-nonvolatile-SRAM (TL-nvSRAM-CIM) scheme for large NN models. The
clustered n-selector-n-ReRAM (cluster-nSnRs) is employed for reliable
weight-restore with eliminated DC power. Furthermore, a ternary SRAM-CIM
mechanism with differential computing scheme is proposed for energy-efficient
ternary MAC operations while preserving high NN accuracy. The proposed
TL-nvSRAM-CIM achieves 7.8x higher storage density, compared with the
state-of-art works. Moreover, TL-nvSRAM-CIM shows up to 2.9x and 1.9x enhanced
energy-efficiency, respectively, compared to the baseline designs of SRAM-CIM
and ReRAM-CIM, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.02730">Fine-grained Action Analysis: A Multi-modality and Multi-task Dataset of Figure Skating. (arXiv:2307.02730v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Sheng-Lan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1">Yu-Ning Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_G/0/1/0/all/0/1">Gang Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Si-Fan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jin-Rong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wen-Yue Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_N/0/1/0/all/0/1">Ning Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xue-Hai Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hao Liu</a></p>
<p>The fine-grained action analysis of the existing action datasets is
challenged by insufficient action categories, low fine granularities, limited
modalities, and tasks. In this paper, we propose a Multi-modality and
Multi-task dataset of Figure Skating (MMFS) which was collected from the World
Figure Skating Championships. MMFS, which possesses action recognition and
action quality assessment, captures RGB, skeleton, and is collected the score
of actions from 11671 clips with 256 categories including spatial and temporal
labels. The key contributions of our dataset fall into three aspects as
follows. (1) Independently spatial and temporal categories are first proposed
to further explore fine-grained action recognition and quality assessment. (2)
MMFS first introduces the skeleton modality for complex fine-grained action
quality assessment. (3) Our multi-modality and multi-task dataset encourage
more action analysis models. To benchmark our dataset, we adopt RGB-based and
skeleton-based baseline methods for action recognition and action quality
assessment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05638">A Comprehensive Survey of Deep Transfer Learning for Anomaly Detection in Industrial Time Series: Methods, Applications, and Directions. (arXiv:2307.05638v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yan_P/0/1/0/all/0/1">Peng Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdulkadir_A/0/1/0/all/0/1">Ahmed Abdulkadir</a>, <a href="http://arxiv.org/find/cs/1/au:+Luley_P/0/1/0/all/0/1">Paul-Philipp Luley</a>, <a href="http://arxiv.org/find/cs/1/au:+Rosenthal_M/0/1/0/all/0/1">Matthias Rosenthal</a>, <a href="http://arxiv.org/find/cs/1/au:+Schatte_G/0/1/0/all/0/1">Gerrit A. Schatte</a>, <a href="http://arxiv.org/find/cs/1/au:+Grewe_B/0/1/0/all/0/1">Benjamin F. Grewe</a>, <a href="http://arxiv.org/find/cs/1/au:+Stadelmann_T/0/1/0/all/0/1">Thilo Stadelmann</a></p>
<p>Automating the monitoring of industrial processes has the potential to
enhance efficiency and optimize quality by promptly detecting abnormal events
and thus facilitating timely interventions. Deep learning, with its capacity to
discern non-trivial patterns within large datasets, plays a pivotal role in
this process. Standard deep learning methods are suitable to solve a specific
task given a specific type of data. During training, deep learning demands
large volumes of labeled data. However, due to the dynamic nature of the
industrial processes and environment, it is impractical to acquire large-scale
labeled data for standard deep learning training for every slightly different
case anew. Deep transfer learning offers a solution to this problem. By
leveraging knowledge from related tasks and accounting for variations in data
distributions, the transfer learning framework solves new tasks with little or
even no additional labeled data. The approach bypasses the need to retrain a
model from scratch for every new setup and dramatically reduces the labeled
data requirement. This survey first provides an in-depth review of deep
transfer learning, examining the problem settings of transfer learning and
classifying the prevailing deep transfer learning methods. Moreover, we delve
into applications of deep transfer learning in the context of a broad spectrum
of time series anomaly detection tasks prevalent in primary industrial domains,
e.g., manufacturing process monitoring, predictive maintenance, energy
management, and infrastructure facility monitoring. We discuss the challenges
and limitations of deep transfer learning in industrial contexts and conclude
the survey with practical directions and actionable suggestions to address the
need to leverage diverse time series data for anomaly detection in an
increasingly dynamic production environment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.00031">Reinforcement Learning for Generative AI: State of the Art, Opportunities and Open Research Challenges. (arXiv:2308.00031v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Franceschelli_G/0/1/0/all/0/1">Giorgio Franceschelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Musolesi_M/0/1/0/all/0/1">Mirco Musolesi</a></p>
<p>Generative Artificial Intelligence (AI) is one of the most exciting
developments in Computer Science of the last decade. At the same time,
Reinforcement Learning (RL) has emerged as a very successful paradigm for a
variety of machine learning tasks. In this survey, we discuss the state of the
art, opportunities and open research questions in applying RL to generative AI.
In particular, we will discuss three types of applications, namely, RL as an
alternative way for generation without specified objectives; as a way for
generating outputs while concurrently maximizing an objective function; and,
finally, as a way of embedding desired characteristics, which cannot be easily
captured by means of an objective function, into the generative process. We
conclude the survey with an in-depth discussion of the opportunities and
challenges in this fascinating emerging area.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.11339">ProAgent: Building Proactive Cooperative Agents with Large Language Models. (arXiv:2308.11339v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Ceyao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1">Kaijie Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1">Siyi Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zihao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1">Guanghe Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yihang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Cheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhaowei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1">Anji Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1">Song-Chun Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1">Xiaojun Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Junge Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_F/0/1/0/all/0/1">Feng Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1">Yitao Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yaodong Yang</a></p>
<p>Building agents with adaptive behavior in cooperative tasks stands as a
paramount goal in the realm of multi-agent systems. Current approaches to
developing cooperative agents rely primarily on learning-based methods, whose
policy generalization depends heavily on the diversity of teammates they
interact with during the training phase. Such reliance, however, constrains the
agents' capacity for strategic adaptation when cooperating with unfamiliar
teammates, which becomes a significant challenge in zero-shot coordination
scenarios. To address this challenge, we propose ProAgent, a novel framework
that harnesses large language models (LLMs) to create proactive agents capable
of dynamically adapting their behavior to enhance cooperation with teammates.
ProAgent can analyze the present state, and infer the intentions of teammates
from observations. It then updates its beliefs in alignment with the teammates'
subsequent actual behaviors. Moreover, ProAgent exhibits a high degree of
modularity and interpretability, making it easily integrated into various of
coordination scenarios. Experimental evaluations conducted within the
Overcooked-AI environment unveil the remarkable performance superiority of
ProAgent, outperforming five methods based on self-play and population-based
training when cooperating with AI agents. Furthermore, in partnered with human
proxy models, its performance exhibits an average improvement exceeding 10%
compared to the current state-of-the-art method. For more information about our
project, please visit~\url{https://pku-proagent.github.io}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.03581">Interactive Hyperparameter Optimization in Multi-Objective Problems via Preference Learning. (arXiv:2309.03581v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Giovanelli_J/0/1/0/all/0/1">Joseph Giovanelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Tornede_A/0/1/0/all/0/1">Alexander Tornede</a>, <a href="http://arxiv.org/find/cs/1/au:+Tornede_T/0/1/0/all/0/1">Tanja Tornede</a>, <a href="http://arxiv.org/find/cs/1/au:+Lindauer_M/0/1/0/all/0/1">Marius Lindauer</a></p>
<p>Hyperparameter optimization (HPO) is important to leverage the full potential
of machine learning (ML). In practice, users are often interested in
multi-objective (MO) problems, i.e., optimizing potentially conflicting
objectives, like accuracy and energy consumption. To tackle this, the vast
majority of MO-ML algorithms return a Pareto front of non-dominated machine
learning models to the user. Optimizing the hyperparameters of such algorithms
is non-trivial as evaluating a hyperparameter configuration entails evaluating
the quality of the resulting Pareto front. In literature, there are known
indicators that assess the quality of a Pareto front (e.g., hypervolume, R2) by
quantifying different properties (e.g., volume, proximity to a reference
point). However, choosing the indicator that leads to the desired Pareto front
might be a hard task for a user. In this paper, we propose a human-centered
interactive HPO approach tailored towards multi-objective ML leveraging
preference learning to extract desiderata from users that guide the
optimization. Instead of relying on the user guessing the most suitable
indicator for their needs, our approach automatically learns an appropriate
indicator. Concretely, we leverage pairwise comparisons of distinct Pareto
fronts to learn such an appropriate quality indicator. Then, we optimize the
hyperparameters of the underlying MO-ML algorithm towards this learned
indicator using a state-of-the-art HPO approach. In an experimental study
targeting the environmental impact of ML, we demonstrate that our approach
leads to substantially better Pareto fronts compared to optimizing based on a
wrong indicator pre-selected by the user, and performs comparable in the case
of an advanced user knowing which indicator to pick.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.09571">Heterogeneous Generative Knowledge Distillation with Masked Image Modeling. (arXiv:2309.09571v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Ziming Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1">Shumin Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaodi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1">Jing Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1">Xianbin Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Baochang Zhang</a></p>
<p>Small CNN-based models usually require transferring knowledge from a large
model before they are deployed in computationally resource-limited edge
devices. Masked image modeling (MIM) methods achieve great success in various
visual tasks but remain largely unexplored in knowledge distillation for
heterogeneous deep models. The reason is mainly due to the significant
discrepancy between the Transformer-based large model and the CNN-based small
network. In this paper, we develop the first Heterogeneous Generative Knowledge
Distillation (H-GKD) based on MIM, which can efficiently transfer knowledge
from large Transformer models to small CNN-based models in a generative
self-supervised fashion. Our method builds a bridge between Transformer-based
models and CNNs by training a UNet-style student with sparse convolution, which
can effectively mimic the visual representation inferred by a teacher over
masked modeling. Our method is a simple yet effective learning paradigm to
learn the visual representation and distribution of data from heterogeneous
teacher models, which can be pre-trained using advanced generative methods.
Extensive experiments show that it adapts well to various models and sizes,
consistently achieving state-of-the-art performance in image classification,
object detection, and semantic segmentation tasks. For example, in the Imagenet
1K dataset, H-GKD improves the accuracy of Resnet50 (sparse) from 76.98% to
80.01%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.09832">Task Selection and Assignment for Multi-modal Multi-task Dialogue Act Classification with Non-stationary Multi-armed Bandits. (arXiv:2309.09832v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xiangheng He</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Junjie Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1">Bj&#xf6;rn W. Schuller</a></p>
<p>Multi-task learning (MTL) aims to improve the performance of a primary task
by jointly learning with related auxiliary tasks. Traditional MTL methods
select tasks randomly during training. However, both previous studies and our
results suggest that such a random selection of tasks may not be helpful, and
can even be harmful to performance. Therefore, new strategies for task
selection and assignment in MTL need to be explored. This paper studies the
multi-modal, multi-task dialogue act classification task, and proposes a method
for selecting and assigning tasks based on non-stationary multi-armed bandits
(MAB) with discounted Thompson Sampling (TS) using Gaussian priors. Our
experimental results show that in different training stages, different tasks
have different utility. Our proposed method can effectively identify the task
utility, actively avoid useless or harmful tasks, and realise the task
assignment during training. Our proposed method is significantly superior in
terms of UAR and F1 to the single-task and multi-task baselines with p-values &lt;
0.05. Further analysis of experiments indicates that for the dataset with the
data imbalance problem, our proposed method has significantly higher stability
and can obtain consistent and decent performance for minority classes. Our
proposed method is superior to the current state-of-the-art model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11526">Likelihood-based Sensor Calibration using Affine Transformation. (arXiv:2309.11526v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Machhamer_R/0/1/0/all/0/1">R&#xfc;diger Machhamer</a>, <a href="http://arxiv.org/find/cs/1/au:+Fazlic_L/0/1/0/all/0/1">Lejla Begic Fazlic</a>, <a href="http://arxiv.org/find/cs/1/au:+Guven_E/0/1/0/all/0/1">Eray Guven</a>, <a href="http://arxiv.org/find/cs/1/au:+Junk_D/0/1/0/all/0/1">David Junk</a>, <a href="http://arxiv.org/find/cs/1/au:+Kurt_G/0/1/0/all/0/1">Gunes Karabulut Kurt</a>, <a href="http://arxiv.org/find/cs/1/au:+Naumann_S/0/1/0/all/0/1">Stefan Naumann</a>, <a href="http://arxiv.org/find/cs/1/au:+Didas_S/0/1/0/all/0/1">Stephan Didas</a>, <a href="http://arxiv.org/find/cs/1/au:+Gollmer_K/0/1/0/all/0/1">Klaus-Uwe Gollmer</a>, <a href="http://arxiv.org/find/cs/1/au:+Bergmann_R/0/1/0/all/0/1">Ralph Bergmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Timm_I/0/1/0/all/0/1">Ingo J. Timm</a>, <a href="http://arxiv.org/find/cs/1/au:+Dartmann_G/0/1/0/all/0/1">Guido Dartmann</a></p>
<p>An important task in the field of sensor technology is the efficient
implementation of adaptation procedures of measurements from one sensor to
another sensor of identical design. One idea is to use the estimation of an
affine transformation between different systems, which can be improved by the
knowledge of experts. This paper presents an improved solution from Glacier
Research that was published back in 1973. The results demonstrate the
adaptability of this solution for various applications, including software
calibration of sensors, implementation of expert-based adaptation, and paving
the way for future advancements such as distributed learning methods. One idea
here is to use the knowledge of experts for estimating an affine transformation
between different systems. We evaluate our research with simulations and also
with real measured data of a multi-sensor board with 8 identical sensors. Both
data set and evaluation script are provided for download. The results show an
improvement for both the simulation and the experiments with real data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04986">A new economic and financial theory of money. (arXiv:2310.04986v5 [econ.TH] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/econ/1/au:+Glinsky_M/0/1/0/all/0/1">Michael E. Glinsky</a>, <a href="http://arxiv.org/find/econ/1/au:+Sievert_S/0/1/0/all/0/1">Sharon Sievert</a></p>
<p>This paper fundamentally reformulates economic and financial theory to
include electronic currencies. The valuation of the electronic currencies will
be based on macroeconomic theory and the fundamental equation of monetary
policy, not the microeconomic theory of discounted cash flows. The view of
electronic currency as a transactional equity associated with tangible assets
of a sub-economy will be developed, in contrast to the view of stock as an
equity associated mostly with intangible assets of a sub-economy. The view will
be developed of the electronic currency management firm as an entity
responsible for coordinated monetary (electronic currency supply and value
stabilization) and fiscal (investment and operational) policies of a
substantial (for liquidity of the electronic currency) sub-economy. The risk
model used in the valuations and the decision-making will not be the
ubiquitous, yet inappropriate, exponential risk model that leads to discount
rates, but will be multi time scale models that capture the true risk. The
decision-making will be approached from the perspective of true systems control
based on a system response function given by the multi scale risk model and
system controllers that utilize the Deep Reinforcement Learning, Generative
Pretrained Transformers, and other methods of Artificial Intelligence
(DRL/GPT/AI). Finally, the sub-economy will be viewed as a nonlinear complex
physical system with both stable equilibriums that are associated with
short-term exploitation, and unstable equilibriums that need to be stabilized
with active nonlinear control based on the multi scale system response
functions and DRL/GPT/AI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.06266">CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model. (arXiv:2310.06266v2 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Di_P/0/1/0/all/0/1">Peng Di</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jianguo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Hang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1">Wei Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1">Wenting Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yang Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chaoyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Dajun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hongwei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Liang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_G/0/1/0/all/0/1">Gang Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_J/0/1/0/all/0/1">Jie Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1">Zi Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1">Wen Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1">Tingting Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1">Zhichao Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1">Ting Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1">Ming Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_C/0/1/0/all/0/1">Cong Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bingchang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiachen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhiwei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1">Shaojun Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_M/0/1/0/all/0/1">Min Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guangpei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Huan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zhaogui Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jiawei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1">Qing Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Gehao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zelin Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1">Xunjin Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Hailian Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1">Lifu Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xianying Zhu</a></p>
<p>Code Large Language Models (Code LLMs) have gained significant attention in
the industry due to their wide applications in the full lifecycle of software
engineering. However, the effectiveness of existing models in understanding
non-English inputs for multi-lingual code-related tasks is still far from well
studied. This paper introduces CodeFuse-13B, an open-sourced pre-trained code
LLM. It is specifically designed for code-related tasks with both English and
Chinese prompts and supports over 40 programming languages. CodeFuse achieves
its effectiveness by utilizing a high quality pre-training dataset that is
carefully filtered by program analyzers and optimized during the training
process. Extensive experiments are conducted using real-world usage scenarios,
the industry-standard benchmark HumanEval-x, and the specially designed
CodeFuseEval for Chinese prompts. To assess the effectiveness of CodeFuse, we
actively collected valuable human feedback from the AntGroup's software
development process where CodeFuse has been successfully deployed. The results
demonstrate that CodeFuse-13B achieves a HumanEval pass@1 score of 37.10%,
positioning it as one of the top multi-lingual code LLMs with similar parameter
sizes. In practical scenarios, such as code generation, code translation, code
comments, and testcase generation, CodeFuse performs better than other models
when confronted with Chinese prompts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.13191">Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy for Language Models. (arXiv:2310.13191v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jianwei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_Q/0/1/0/all/0/1">Qi Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1">Wei Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1">Dongkuan Xu</a></p>
<p>The pruning objective has recently extended beyond accuracy and sparsity to
robustness in language models. Despite this, existing methods struggle to
enhance robustness against adversarial attacks when continually increasing
model sparsity and require a retraining process. As humans step into the era of
large language models, these issues become increasingly prominent. This paper
proposes that the robustness of language models is proportional to the extent
of pre-trained knowledge they encompass. Accordingly, we introduce a
post-training pruning strategy designed to faithfully replicate the embedding
space and feature space of dense language models, aiming to conserve more
pre-trained knowledge during the pruning process. In this setup, each layer's
reconstruction error not only originates from itself but also includes
cumulative error from preceding layers, followed by an adaptive rectification.
Compared to other state-of-art baselines, our approach demonstrates a superior
balance between accuracy, sparsity, robustness, and pruning cost with BERT on
datasets SST2, IMDB, and AGNews, marking a significant stride towards robust
pruning in language models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02790">CausalCite: A Causal Formulation of Paper Citations. (arXiv:2311.02790v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kumar_I/0/1/0/all/0/1">Ishan Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1">Zhijing Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Mokhtarian_E/0/1/0/all/0/1">Ehsan Mokhtarian</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1">Siyuan Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1">Mrinmaya Sachan</a>, <a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1">Bernhard Sch&#xf6;lkopf</a></p>
<p>Evaluating the significance of a paper is pivotal yet challenging for the
scientific community. While the citation count is the most commonly used proxy
for this purpose, they are widely criticized for failing to accurately reflect
a paper's true impact. In this work, we propose a causal inference method,
TextMatch, which adapts the traditional matching framework to high-dimensional
text embeddings. Specifically, we encode each paper using the text embeddings
by large language models (LLMs), extract similar samples by cosine similarity,
and synthesize a counterfactual sample by the weighted average of similar
papers according to their similarity values. We apply the resulting metric,
called CausalCite, as a causal formulation of paper citations. We show its
effectiveness on various criteria, such as high correlation with paper impact
as reported by scientific experts on a previous dataset of 1K papers,
(test-of-time) awards for past papers, and its stability across various
sub-fields of AI. We also provide a set of findings that can serve as suggested
ways for future researchers to use our metric for a better understanding of a
paper's quality. Our code and data are at
https://github.com/causalNLP/causal-cite.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03220">ALYMPICS: Language Agents Meet Game Theory -- Exploring Strategic Decision-Making with AI Agents. (arXiv:2311.03220v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1">Shaoguang Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1">Yuzhe Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1">Yan Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1">Wenshan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Fengyi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1">Tao Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1">Furu Wei</a></p>
<p>This paper introduces Alympics (Olympics for Agents), a systematic simulation
framework utilizing Large Language Model (LLM) agents for game theory research.
Alympics creates a versatile platform for studying complex game theory
problems, bridging the gap between theoretical game theory and empirical
investigations by providing a controlled environment for simulating human-like
strategic interactions with LLM agents. In our pilot case study, the "Water
Allocation Challenge," we explore Alympics through a challenging strategic game
focused on the multi-round auction on scarce survival resources. This study
demonstrates the framework's ability to qualitatively and quantitatively
analyze game determinants, strategies, and outcomes. Additionally, we conduct a
comprehensive human assessment and an in-depth evaluation of LLM agents in
strategic decision-making scenarios. Our findings not only expand the
understanding of LLM agents' proficiency in emulating human strategic behavior
but also highlight their potential in advancing game theory knowledge, thereby
enriching our understanding of both game theory and empowering further research
into strategic decision-making domains with LLM agents. Codes, prompts, and all
related resources are available at https://github.com/microsoft/Alympics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13326">Curriculum Learning and Imitation Learning for Model-free Control on Financial Time-series. (arXiv:2311.13326v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Koh_W/0/1/0/all/0/1">Woosung Koh</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_I/0/1/0/all/0/1">Insu Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1">Yuntae Jang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_G/0/1/0/all/0/1">Gimin Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1">Woo Chang Kim</a></p>
<p>Curriculum learning and imitation learning have been leveraged extensively in
the robotics domain. However, minimal research has been done on leveraging
these ideas on control tasks over highly stochastic time-series data. Here, we
theoretically and empirically explore these approaches in a representative
control task over complex time-series data. We implement the fundamental ideas
of curriculum learning via data augmentation, while imitation learning is
implemented via policy distillation from an oracle. Our findings reveal that
curriculum learning should be considered a novel direction in improving
control-task performance over complex time-series. Our ample random-seed
out-sample empirics and ablation studies are highly encouraging for curriculum
learning for time-series control. These findings are especially encouraging as
we tune all overlapping hyperparameters on the baseline -- giving an advantage
to the baseline. On the other hand, we find that imitation learning should be
used with caution.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15816">Scale-Dropout: Estimating Uncertainty in Deep Neural Networks Using Stochastic Scale. (arXiv:2311.15816v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1">Soyed Tuhin Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Danouchi_K/0/1/0/all/0/1">Kamal Danouchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hefenbrock_M/0/1/0/all/0/1">Michael Hefenbrock</a>, <a href="http://arxiv.org/find/cs/1/au:+Prenat_G/0/1/0/all/0/1">Guillaume Prenat</a>, <a href="http://arxiv.org/find/cs/1/au:+Anghel_L/0/1/0/all/0/1">Lorena Anghel</a>, <a href="http://arxiv.org/find/cs/1/au:+Tahoori_M/0/1/0/all/0/1">Mehdi B. Tahoori</a></p>
<p>Uncertainty estimation in Neural Networks (NNs) is vital in improving
reliability and confidence in predictions, particularly in safety-critical
applications. Bayesian Neural Networks (BayNNs) with Dropout as an
approximation offer a systematic approach to quantifying uncertainty, but they
inherently suffer from high hardware overhead in terms of power, memory, and
computation. Thus, the applicability of BayNNs to edge devices with limited
resources or to high-performance applications is challenging. Some of the
inherent costs of BayNNs can be reduced by accelerating them in hardware on a
Computation-In-Memory (CIM) architecture with spintronic memories and
binarizing their parameters. However, numerous stochastic units are required to
implement conventional dropout-based BayNN. In this paper, we propose the Scale
Dropout, a novel regularization technique for Binary Neural Networks (BNNs),
and Monte Carlo-Scale Dropout (MC-Scale Dropout)-based BayNNs for efficient
uncertainty estimation. Our approach requires only one stochastic unit for the
entire model, irrespective of the model size, leading to a highly scalable
Bayesian NN. Furthermore, we introduce a novel Spintronic memory-based CIM
architecture for the proposed BayNN that achieves more than $100\times$ energy
savings compared to the state-of-the-art. We validated our method to show up to
a $1\%$ improvement in predictive performance and superior uncertainty
estimates compared to related works.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18252">Navigating Privacy and Copyright Challenges Across the Data Lifecycle of Generative AI. (arXiv:2311.18252v2 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dawen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_B/0/1/0/all/0/1">Boming Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yue Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiwei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1">Thong Hoang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1">Zhenchang Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Staples_M/0/1/0/all/0/1">Mark Staples</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1">Qinghua Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1">Liming Zhu</a></p>
<p>The advent of Generative AI has marked a significant milestone in artificial
intelligence, demonstrating remarkable capabilities in generating realistic
images, texts, and data patterns. However, these advancements come with
heightened concerns over data privacy and copyright infringement, primarily due
to the reliance on vast datasets for model training. Traditional approaches
like differential privacy, machine unlearning, and data poisoning only offer
fragmented solutions to these complex issues. Our paper delves into the
multifaceted challenges of privacy and copyright protection within the data
lifecycle. We advocate for integrated approaches that combines technical
innovation with ethical foresight, holistically addressing these concerns by
investigating and devising solutions that are informed by the lifecycle
perspective. This work aims to catalyze a broader discussion and inspire
concerted efforts towards data privacy and copyright integrity in Generative
AI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05720">Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer Inputs of Language Models in Federated Learning. (arXiv:2312.05720v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jianwei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Sheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_Q/0/1/0/all/0/1">Qi Lei</a></p>
<p>Federated learning (FL) emphasizes decentralized training by storing data
locally and sending only model updates, underlining user privacy. Recently, a
line of works on privacy attacks impairs user privacy by extracting sensitive
training text from language models in the context of FL. Yet, these attack
techniques face distinct hurdles: some work chiefly with limited batch sizes
(e.g., batch size of 1), and others are easily detectable. This paper
introduces an innovative approach that is challenging to detect, significantly
enhancing the recovery rate of text in various batch-size settings. Building on
fundamental gradient matching and domain prior knowledge, we enhance the attack
by recovering the input of the Pooler layer of language models, which enables
us to provide additional supervised signals at the feature level. Unlike
gradient data, these signals do not average across sentences and tokens,
thereby offering more nuanced and effective insights. We benchmark our method
using text classification tasks on datasets such as CoLA, SST-2, and Rotten
Tomatoes. Across different batch sizes and models, our approach consistently
outperforms previous state-of-the-art results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.09507">WAVER: Writing-style Agnostic Text-Video Retrieval via Distilling Vision-Language Models Through Open-Vocabulary Knowledge. (arXiv:2312.09507v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1">Huy Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Kieu_T/0/1/0/all/0/1">Tung Kieu</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1">Anh Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1">Ngan Le</a></p>
<p>Text-video retrieval, a prominent sub-field within the domain of multimodal
information retrieval, has witnessed remarkable growth in recent years.
However, existing methods assume video scenes are consistent with unbiased
descriptions. These limitations fail to align with real-world scenarios since
descriptions can be influenced by annotator biases, diverse writing styles, and
varying textual perspectives. To overcome the aforementioned problems, we
introduce $\texttt{WAVER}$, a cross-domain knowledge distillation framework via
vision-language models through open-vocabulary knowledge designed to tackle the
challenge of handling different writing styles in video descriptions.
$\texttt{WAVER}$ capitalizes on the open-vocabulary properties that lie in
pre-trained vision-language models and employs an implicit knowledge
distillation approach to transfer text-based knowledge from a teacher model to
a vision-based student. Empirical studies conducted across four standard
benchmark datasets, encompassing various settings, provide compelling evidence
that $\texttt{WAVER}$ can achieve state-of-the-art performance in text-video
retrieval task while handling writing-style variations. The code is available
at: https://github.com/Fsoft-AIC/WAVER
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11747">Robust Stochastic Graph Generator for Counterfactual Explanations. (arXiv:2312.11747v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Prado_Romero_M/0/1/0/all/0/1">Mario Alfonso Prado-Romero</a>, <a href="http://arxiv.org/find/cs/1/au:+Prenkaj_B/0/1/0/all/0/1">Bardh Prenkaj</a>, <a href="http://arxiv.org/find/cs/1/au:+Stilo_G/0/1/0/all/0/1">Giovanni Stilo</a></p>
<p>Counterfactual Explanation (CE) techniques have garnered attention as a means
to provide insights to the users engaging with AI systems. While extensively
researched in domains such as medical imaging and autonomous vehicles, Graph
Counterfactual Explanation (GCE) methods have been comparatively
under-explored. GCEs generate a new graph similar to the original one, with a
different outcome grounded on the underlying predictive model. Among these GCE
techniques, those rooted in generative mechanisms have received relatively
limited investigation despite demonstrating impressive accomplishments in other
domains, such as artistic styles and natural language modelling. The preference
for generative explainers stems from their capacity to generate counterfactual
instances during inference, leveraging autonomously acquired perturbations of
the input graph. Motivated by the rationales above, our study introduces
RSGG-CE, a novel Robust Stochastic Graph Generator for Counterfactual
Explanations able to produce counterfactual examples from the learned latent
space considering a partially ordered generation sequence. Furthermore, we
undertake quantitative and qualitative analyses to compare RSGG-CE's
performance against SoA generative explainers, highlighting its increased
ability to engendering plausible counterfactual candidates.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14187">WaveCoder: Widespread And Versatile Enhanced Instruction Tuning with Refined Data Generation. (arXiv:2312.14187v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Zhaojian Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_N/0/1/0/all/0/1">Ning Shang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yangyu Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Can Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yishujie Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1">Wenxiang Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_Q/0/1/0/all/0/1">Qiufeng Yin</a></p>
<p>Recent work demonstrates that, after being fine-tuned on a high-quality
instruction dataset, the resulting model can obtain impressive capabilities to
address a wide range of tasks. However, existing methods for instruction data
generation often produce duplicate data and are not controllable enough on data
quality. In this paper, we extend the generalization of instruction tuning by
classifying the instruction data to 4 code-related tasks and propose a
LLM-based Generator-Discriminator data process framework to generate diverse,
high-quality instruction data from open source code. Hence, we introduce
CodeOcean, a dataset comprising 20,000 instruction instances across 4 universal
code-related tasks,which is aimed at augmenting the effectiveness of
instruction tuning and improving the generalization ability of fine-tuned
model. Subsequently, we present WaveCoder, a fine-tuned Code LLM with
Widespread And Versatile Enhanced instruction tuning. This model is
specifically designed for enhancing instruction tuning of Code Language Models
(LLMs). Our experiments demonstrate that Wavecoder models outperform other
open-source models in terms of generalization ability across different
code-related tasks at the same level of fine-tuning scale. Moreover, Wavecoder
exhibits high efficiency in previous code generation tasks. This paper thus
offers a significant contribution to the field of instruction data generation
and fine-tuning models, providing new insights and tools for enhancing
performance in code-related tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02873">Optimal Chaining of Vehicle Plans with Time Windows. (arXiv:2401.02873v2 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Fiedler_D/0/1/0/all/0/1">David Fiedler</a>, <a href="http://arxiv.org/find/math/1/au:+Difonzo_F/0/1/0/all/0/1">Fabio V. Difonzo</a>, <a href="http://arxiv.org/find/math/1/au:+Mrkos_J/0/1/0/all/0/1">Jan Mrkos</a></p>
<p>For solving problems from the domain of Mobility-on-Demand (MoD), we often
need to connect vehicle plans into plans spanning longer time, a process we
call plan chaining. As we show in this work, chaining of the plans can be used
to reduce the size of MoD providers' fleet (fleet-sizing problem) but also to
reduce the total driven distance by providing high-quality vehicle dispatching
solutions in MoD systems. Recently, a solution that uses this principle has
been proposed to solve the fleet-sizing problem. The method does not consider
the time flexibility of the plans. Instead, plans are fixed in time and cannot
be delayed. However, time flexibility is an essential property of all vehicle
problems with time windows. This work presents a new plan chaining formulation
that considers delays as allowed by the time windows and a solution method for
solving it. Moreover, we prove that the proposed plan chaining method is
optimal, and we analyze its complexity. Finally, we list some practical
applications and perform a demonstration for one of them: a new heuristic
vehicle dispatching method for solving the static dial-a-ride problem. The
demonstration results show that our proposed method provides a better solution
than the two heuristic baselines for the majority of instances that cannot be
solved optimally. At the same time, our method does not have the largest
computational time requirements compared to the baselines. Therefore, we
conclude that the proposed optimal chaining method provides not only
theoretically sound results but is also practically applicable.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.03302">Realism in Action: Anomaly-Aware Diagnosis of Brain Tumors from Medical Images Using YOLOv8 and DeiT. (arXiv:2401.03302v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Hashemi_S/0/1/0/all/0/1">Seyed Mohammad Hossein Hashemi</a>, <a href="http://arxiv.org/find/eess/1/au:+Safari_L/0/1/0/all/0/1">Leila Safari</a>, <a href="http://arxiv.org/find/eess/1/au:+Taromi_A/0/1/0/all/0/1">Amirhossein Dadashzade Taromi</a></p>
<p>In the field of medical sciences, reliable detection and classification of
brain tumors from images remains a formidable challenge due to the rarity of
tumors within the population of patients. Therefore, the ability to detect
tumors in anomaly scenarios is paramount for ensuring timely interventions and
improved patient outcomes. This study addresses the issue by leveraging deep
learning (DL) techniques to detect and classify brain tumors in challenging
situations. The curated data set from the National Brain Mapping Lab (NBML)
comprises 81 patients, including 30 Tumor cases and 51 Normal cases. The
detection and classification pipelines are separated into two consecutive
tasks. The detection phase involved comprehensive data analysis and
pre-processing to modify the number of image samples and the number of patients
of each class to anomaly distribution (9 Normal per 1 Tumor) to comply with
real world scenarios. Next, in addition to common evaluation metrics for the
testing, we employed a novel performance evaluation method called Patient to
Patient (PTP), focusing on the realistic evaluation of the model. In the
detection phase, we fine-tuned a YOLOv8n detection model to detect the tumor
region. Subsequent testing and evaluation yielded competitive performance both
in Common Evaluation Metrics and PTP metrics. Furthermore, using the Data
Efficient Image Transformer (DeiT) module, we distilled a Vision Transformer
(ViT) model from a fine-tuned ResNet152 as a teacher in the classification
phase. This approach demonstrates promising strides in reliable tumor detection
and classification, offering potential advancements in tumor diagnosis for
real-world medical imaging scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.03621">Machine Learning Applications in Traumatic Brain Injury: A Spotlight on Mild TBI. (arXiv:2401.03621v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ellethy_H/0/1/0/all/0/1">Hanem Ellethy</a>, <a href="http://arxiv.org/find/eess/1/au:+Chandra_S/0/1/0/all/0/1">Shekhar S. Chandra</a>, <a href="http://arxiv.org/find/eess/1/au:+Vegh_V/0/1/0/all/0/1">Viktor Vegh</a></p>
<p>Traumatic Brain Injury (TBI) poses a significant global public health
challenge, contributing to high morbidity and mortality rates and placing a
substantial economic burden on healthcare systems worldwide. The diagnosis of
TBI relies on clinical information along with Computed Tomography (CT) scans.
Addressing the multifaceted challenges posed by TBI has seen the development of
innovative, data-driven approaches, for this complex condition. Particularly
noteworthy is the prevalence of mild TBI (mTBI), which constitutes the majority
of TBI cases where conventional methods often fall short. As such, we review
the state-of-the-art Machine Learning (ML) techniques applied to clinical
information and CT scans in TBI, with a particular focus on mTBI. We categorize
ML applications based on their data sources, and there is a spectrum of ML
techniques used to date. Most of these techniques have primarily focused on
diagnosis, with relatively few attempts at predicting the prognosis. This
review may serve as a source of inspiration for future research studies aimed
at improving the diagnosis of TBI using data-driven approaches and standard
diagnostic data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.03786">Long-term Safe Reinforcement Learning with Binary Feedback. (arXiv:2401.03786v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wachi_A/0/1/0/all/0/1">Akifumi Wachi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hashimoto_W/0/1/0/all/0/1">Wataru Hashimoto</a>, <a href="http://arxiv.org/find/cs/1/au:+Hashimoto_K/0/1/0/all/0/1">Kazumune Hashimoto</a></p>
<p>Safety is an indispensable requirement for applying reinforcement learning
(RL) to real problems. Although there has been a surge of safe RL algorithms
proposed in recent years, most existing work typically 1) relies on receiving
numeric safety feedback; 2) does not guarantee safety during the learning
process; 3) limits the problem to a priori known, deterministic transition
dynamics; and/or 4) assume the existence of a known safe policy for any states.
Addressing the issues mentioned above, we thus propose Long-term Binaryfeedback
Safe RL (LoBiSaRL), a safe RL algorithm for constrained Markov decision
processes (CMDPs) with binary safety feedback and an unknown, stochastic state
transition function. LoBiSaRL optimizes a policy to maximize rewards while
guaranteeing a long-term safety that an agent executes only safe state-action
pairs throughout each episode with high probability. Specifically, LoBiSaRL
models the binary safety function via a generalized linear model (GLM) and
conservatively takes only a safe action at every time step while inferring its
effect on future safety under proper assumptions. Our theoretical results show
that LoBiSaRL guarantees the long-term safety constraint, with high
probability. Finally, our empirical results demonstrate that our algorithm is
safer than existing methods without significantly compromising performance in
terms of reward.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.03854">TIER: Text-Image Encoder-based Regression for AIGC Image Quality Assessment. (arXiv:2401.03854v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1">Jiquan Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1">Xinyan Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Che_J/0/1/0/all/0/1">Jinming Che</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qinyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1">Sen Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1">Wei Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Jinlong Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1">Xixin Cao</a></p>
<p>Recently, AIGC image quality assessment (AIGCIQA), which aims to assess the
quality of AI-generated images (AIGIs) from a human perception perspective, has
emerged as a new topic in computer vision. Unlike common image quality
assessment tasks where images are derived from original ones distorted by
noise, blur, and compression, \textit{etc.}, in AIGCIQA tasks, images are
typically generated by generative models using text prompts. Considerable
efforts have been made in the past years to advance AIGCIQA. However, most
existing AIGCIQA methods regress predicted scores directly from individual
generated images, overlooking the information contained in the text prompts of
these images. This oversight partially limits the performance of these AIGCIQA
methods. To address this issue, we propose a text-image encoder-based
regression (TIER) framework. Specifically, we process the generated images and
their corresponding text prompts as inputs, utilizing a text encoder and an
image encoder to extract features from these text prompts and generated images,
respectively. To demonstrate the effectiveness of our proposed TIER method, we
conduct extensive experiments on several mainstream AIGCIQA databases,
including AGIQA-1K, AGIQA-3K, and AIGCIQA2023. The experimental results
indicate that our proposed TIER method generally demonstrates superior
performance compared to baseline in most cases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.03955">Tiny Time Mixers (TTMs): Fast Pretrained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series. (arXiv:2401.03955v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ekambaram_V/0/1/0/all/0/1">Vijay Ekambaram</a>, <a href="http://arxiv.org/find/cs/1/au:+Jati_A/0/1/0/all/0/1">Arindam Jati</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1">Nam H. Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Dayama_P/0/1/0/all/0/1">Pankaj Dayama</a>, <a href="http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1">Chandra Reddy</a>, <a href="http://arxiv.org/find/cs/1/au:+Gifford_W/0/1/0/all/0/1">Wesley M. Gifford</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalagnanam_J/0/1/0/all/0/1">Jayant Kalagnanam</a></p>
<p>Large Pretrained models for zero/few-shot learning excel in language and
vision domains but encounter challenges in multivariate time series (TS) due to
the diverse nature and scarcity of publicly available pretraining data.
Consequently, there has been a recent surge in utilizing pretrained large
language models (LLMs) with various adaptations for time series forecasting.
These approaches employ cross-domain transfer learning and surprisingly yield
impressive results. However, these models are typically very slow and large
($\sim$billion parameters) and do not consider cross-channel correlations. To
address this, we present Multi-level Tiny Time Mixers (TTM), a significantly
small model based on the lightweight TSMixer architecture. TTM marks the first
success in developing tiny general-pretrained models ($\le$1 million
parameters), exclusively trained on public TS datasets in a flash of just 4-8
hrs with effective transfer learning capabilities for forecasting. To tackle
the complexity of pretraining on multiple datasets with varied temporal
resolutions, we introduce several novel enhancements such as adaptive patching,
dataset augmentation via downsampling, and resolution prefix tuning. Moreover,
we employ a multi-level modeling strategy to effectively model channel
correlations and incorporate exogenous signals during fine-tuning, a crucial
capability lacking in existing benchmarks. TTM excels in few/zero-shot
forecasting, demonstrating significant accuracy gains (12-38%) over existing
benchmarks. Further, it achieves a remarkable 14-106X reduction in model
parameters, enabling 54-65X faster finetuning/inference as compared to the
LLM-TS benchmarks. In fact, TTM's zero-shot often surpasses the few-shot
results in many popular benchmarks, highlighting the efficacy of our approach.
Code and Pretrained Models will be open-sourced.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.04126">The Concept of the Tactile Signature System for Individuals with Visual Impairments. (arXiv:2401.04126v2 [cs.HC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kremenchutskiy_A/0/1/0/all/0/1">Anatoliy Kremenchutskiy</a>, <a href="http://arxiv.org/find/cs/1/au:+Gabdreshov_G/0/1/0/all/0/1">Galymzhan Gabdreshov</a></p>
<p>The lack of an accessible and effective system for blind individuals to
create handwritten signatures presents a significant barrier to their
independence and full participation in various aspects of life. This research
introduces the Tactile Signature System, a groundbreaking approach that
empowers individuals with visual impairments to form their unique handwritten
signatures. Key features of the system include: Personalized customization:
Through tactile interaction and voice algorithmic guidance, individuals create
signatures reflecting their preferences and natural writing style. Real-time
feedback: AI-powered voice prompts and analysis ensure accuracy and consistency
in signature formation. Accessibility: Installation in local service centers
provides a secure and supervised environment for signature creation. The
system's impact reaches beyond the individual level: Promotes inclusivity and
independence: Blind individuals can engage in legal and financial transactions
without relying on others. Empowers and fosters equal opportunities:
Participation in education, employment, and civic engagement becomes more
accessible. Aligns with international conventions: Upholds the right of persons
with disabilities to participate fully in society. The Tactile Signature System
represents a significant step towards an inclusive and accessible future for
individuals with visual impairments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.04206">Learning Racing From an AI Coach: Effects of Multimodal Autonomous Driving Explanations on Driving Performance, Cognitive Load, Expertise, and Trust. (arXiv:2401.04206v2 [cs.HC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kaufman_R/0/1/0/all/0/1">Robert Kaufman</a>, <a href="http://arxiv.org/find/cs/1/au:+Costa_J/0/1/0/all/0/1">Jean Costa</a>, <a href="http://arxiv.org/find/cs/1/au:+Kimani_E/0/1/0/all/0/1">Everlyne Kimani</a></p>
<p>In a pre-post experiment (n = 41), we test the impact of an AI Coach's
explanatory communications modeled after the instructions of human driving
experts. Participants were divided into four (4) groups to assess two (2)
dimensions of the AI coach's explanations: information type ('what' and
'why'-type explanations) and presentation modality (auditory and visual). We
directly compare how AI Coaching sessions employing these techniques impact
driving performance, cognitive load, confidence, expertise, and trust in an
observation learning context. Through interviews, we delineate the learning
process of our participants. Results show that an AI driving coach can be
useful for teaching performance driving skills to novices. Comparing between
groups, we find the type and modality of information influences performance
outcomes. We attribute differences to how information directed attention,
mitigated uncertainty, and influenced overload experienced by participants.
These, in turn, affected how successfully participants were able to learn.
Results suggest efficient, modality-appropriate explanations should be opted
for when designing effective HMI communications that can instruct without
overwhelming. Further, they support the need to align communications with human
learning and cognitive processes. Results are synthesized into eight design
implications for future autonomous vehicle HMI and AI coach design.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.04621">DebugBench: Evaluating Debugging Capability of Large Language Models. (arXiv:2401.04621v2 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tian_R/0/1/0/all/0/1">Runchu Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1">Yining Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1">Yujia Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Cong_X/0/1/0/all/0/1">Xin Cong</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yankai Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1">Yinxu Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yesai Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhiyuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1">Maosong Sun</a></p>
<p>Large Language Models (LLMs) have demonstrated exceptional coding capability.
However, as another critical component of programming proficiency, the
debugging capability of LLMs remains relatively unexplored. Previous
evaluations of LLMs' debugging ability are significantly limited by the risk of
data leakage, the scale of the dataset, and the variety of tested bugs. To
overcome these deficiencies, we introduce `DebugBench', an LLM debugging
benchmark consisting of 4,253 instances. It covers four major bug categories
and 18 minor types in C++, Java, and Python. To construct DebugBench, we
collect code snippets from the LeetCode community, implant bugs into source
data with GPT-4, and assure rigorous quality checks. We evaluate two commercial
and three open-source models in a zero-shot scenario. We find that (1) while
closed-source models like GPT-4 exhibit inferior debugging performance compared
to humans, open-source models such as Code Llama fail to attain any pass rate
scores; (2) the complexity of debugging notably fluctuates depending on the bug
category; (3) incorporating runtime feedback has a clear impact on debugging
performance which is not always helpful. As an extension, we also compare LLM
debugging and code generation, revealing a strong correlation between them for
closed-source models. These findings will benefit the development of LLMs in
debugging.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.04679">RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation. (arXiv:2401.04679v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nikdan_M/0/1/0/all/0/1">Mahdi Nikdan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tabesh_S/0/1/0/all/0/1">Soroush Tabesh</a>, <a href="http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1">Dan Alistarh</a></p>
<p>We investigate parameter-efficient fine-tuning (PEFT) methods that can
provide good accuracy under limited computational and memory budgets in the
context of large language models (LLMs). We present a new PEFT method called
Robust Adaptation (RoSA) inspired by robust principal component analysis (PCA)
that jointly trains $\textit{low-rank}$ and $\textit{highly-sparse}$ components
on top of a set of fixed pretrained weights to efficiently approximate the
performance of a full-fine-tuning (FFT) solution. Across a series of
challenging generative tasks such as grade-school math and SQL query
generation, which require fine-tuning for good performance, we show that RoSA
outperforms both LoRA and pure sparse fine-tuning, at the same parameter
budget. We provide system support for RoSA to complement the training
algorithm, specifically in the form of sparse GPU kernels which enable memory-
and computationally-efficient training. Our code will be made available at
$\href{https://github.com/IST-DASLab/RoSA}{\text{our github page}}$.
</p>
</p>
</div>

    </div>
    </body>
    