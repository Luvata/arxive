<!DOCTYPE html>
<html>
<head>
<title>2023-12-08-cs-cv</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2312.02984">Diff-GO: Diffusion Goal-Oriented Communications to Achieve Ultra-High Spectrum Efficiency. (arXiv:2312.02984v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wijesinghe_A/0/1/0/all/0/1">Achintha Wijesinghe</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Songyang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wanninayaka_S/0/1/0/all/0/1">Suchinthaka Wanninayaka</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Weiwei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1">Zhi Ding</a></p>
<p>The latest advances in artificial intelligence (AI) present many
unprecedented opportunities to achieve much improved bandwidth saving in
communications. Unlike conventional communication systems focusing on packet
transport, rich datasets and AI makes it possible to efficiently transfer only
the information most critical to the goals of message recipients. One of the
most exciting advances in generative AI known as diffusion model presents a
unique opportunity for designing ultra-fast communication systems well beyond
language-based messages. This work presents an ultra-efficient communication
design by utilizing generative AI-based on diffusion models as a specific
example of the general goal-oriented communication framework. To better control
the regenerated message at the receiver output, our diffusion system design
includes a local regeneration module with finite dimensional noise latent. The
critical significance of noise latent control and sharing residing on our
Diff-GO is the ability to introduce the concept of "local generative feedback"
(Local-GF), which enables the transmitter to monitor the quality and gauge the
quality or accuracy of the message recovery at the semantic system receiver. To
this end, we propose a new low-dimensional noise space for the training of
diffusion models, which significantly reduces the communication overhead and
achieves satisfactory message recovery performance. Our experimental results
demonstrate that the proposed noise space and the diffusion-based generative
model achieve ultra-high spectrum efficiency and accurate recovery of
transmitted image signals. By trading off computation for bandwidth efficiency
(C4BE), this new framework provides an important avenue to achieve exceptional
computation-bandwidth tradeoff.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02985">FocalPose++: Focal Length and Object Pose Estimation via Render and Compare. (arXiv:2312.02985v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cifka_M/0/1/0/all/0/1">Martin C&#xed;fka</a>, <a href="http://arxiv.org/find/cs/1/au:+Ponimatkin_G/0/1/0/all/0/1">Georgy Ponimatkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Labbe_Y/0/1/0/all/0/1">Yann Labb&#xe9;</a>, <a href="http://arxiv.org/find/cs/1/au:+Russell_B/0/1/0/all/0/1">Bryan Russell</a>, <a href="http://arxiv.org/find/cs/1/au:+Aubry_M/0/1/0/all/0/1">Mathieu Aubry</a>, <a href="http://arxiv.org/find/cs/1/au:+Petrik_V/0/1/0/all/0/1">Vladimir Petrik</a>, <a href="http://arxiv.org/find/cs/1/au:+Sivic_J/0/1/0/all/0/1">Josef Sivic</a></p>
<p>We introduce FocalPose++, a neural render-and-compare method for jointly
estimating the camera-object 6D pose and camera focal length given a single RGB
input image depicting a known object. The contributions of this work are
threefold. First, we derive a focal length update rule that extends an existing
state-of-the-art render-and-compare 6D pose estimator to address the joint
estimation task. Second, we investigate several different loss functions for
jointly estimating the object pose and focal length. We find that a combination
of direct focal length regression with a reprojection loss disentangling the
contribution of translation, rotation, and focal length leads to improved
results. Third, we explore the effect of different synthetic training data on
the performance of our method. Specifically, we investigate different
distributions used for sampling object's 6D pose and camera's focal length when
rendering the synthetic images, and show that parametric distribution fitted on
real training data works the best. We show results on three challenging
benchmark datasets that depict known 3D models in uncontrolled settings. We
demonstrate that our focal length and 6D pose estimates have lower error than
the existing state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02999">Efficient Incremental Potential Contact for Actuated Face Simulation. (arXiv:2312.02999v1 [cs.GR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Lingchen Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Solenthaler_B/0/1/0/all/0/1">Barbara Solenthaler</a></p>
<p>We present a quasi-static finite element simulator for human face animation.
We model the face as an actuated soft body, which can be efficiently simulated
using Projective Dynamics (PD). We adopt Incremental Potential Contact (IPC) to
handle self-intersection. However, directly integrating IPC into the simulation
would impede the high efficiency of the PD solver, since the stiffness matrix
in the global step is no longer constant and cannot be pre-factorized. We
notice that the actual number of vertices affected by the collision is only a
small fraction of the whole model, and by utilizing this fact we effectively
decrease the scale of the linear system to be solved. With the proposed
optimization method for collision, we achieve high visual fidelity at a
relatively low performance overhead.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03001">Computer Vision for Increased Operative Efficiency via Identification of Instruments in the Neurosurgical Operating Room: A Proof-of-Concept Study. (arXiv:2312.03001v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Zachem_T/0/1/0/all/0/1">Tanner J. Zachem</a> (1,2), <a href="http://arxiv.org/find/eess/1/au:+Chen_S/0/1/0/all/0/1">Sully F. Chen</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Venkatraman_V/0/1/0/all/0/1">Vishal Venkatraman</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Sykes_D/0/1/0/all/0/1">David AW Sykes</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Prakash_R/0/1/0/all/0/1">Ravi Prakash</a> (2), <a href="http://arxiv.org/find/eess/1/au:+Spellicy_S/0/1/0/all/0/1">Samantha Spellicy</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Suarez_A/0/1/0/all/0/1">Alexander D Suarez</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Ross_W/0/1/0/all/0/1">Weston Ross</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Codd_P/0/1/0/all/0/1">Patrick J. Codd</a> (1,2) ((1) Department of Neurosurgery, Duke University School of Medicine, Durham, NC, USA, (2) Department of Mechanical Engineering and Materials Science, Duke University, Durham, NC, USA)</p>
<p>Objectives Computer vision (CV) is a field of artificial intelligence that
enables machines to interpret and understand images and videos. CV has the
potential to be of assistance in the operating room (OR) to track surgical
instruments. We built a CV algorithm for identifying surgical instruments in
the neurosurgical operating room as a potential solution for surgical
instrument tracking and management to decrease surgical waste and opening of
unnecessary tools. Methods We collected 1660 images of 27 commonly used
neurosurgical instruments. Images were labeled using the VGG Image Annotator
and split into 80% training and 20% testing sets in order to train a U-Net
Convolutional Neural Network using 5-fold cross validation. Results Our U-Net
achieved a tool identification accuracy of 80-100% when distinguishing 25
classes of instruments, with 19/25 classes having accuracy over 90%. The model
performance was not adequate for sub classifying Adson, Gerald, and Debakey
forceps, which had accuracies of 60-80%. Conclusions We demonstrated the
viability of using machine learning to accurately identify surgical
instruments. Instrument identification could help optimize surgical tray
packing, decrease tool usage and waste, decrease incidence of instrument
misplacement events, and assist in timing of routine instrument maintenance.
More training data will be needed to increase accuracy across all surgical
instruments that would appear in a neurosurgical operating room. Such
technology has the potential to be used as a method to be used for proving what
tools are truly needed in each type of operation allowing surgeons across the
world to do more with less.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03005">Few-Shot Anomaly Detection with Adversarial Loss for Robust Feature Representations. (arXiv:2312.03005v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jae Young Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1">Wonjun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1">Jaehyun Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1">Yongkwi Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_Y/0/1/0/all/0/1">Young Seog Yoon</a></p>
<p>Anomaly detection is a critical and challenging task that aims to identify
data points deviating from normal patterns and distributions within a dataset.
Various methods have been proposed using a one-class-one-model approach, but
these techniques often face practical problems such as memory inefficiency and
the requirement of sufficient data for training. In particular, few-shot
anomaly detection presents significant challenges in industrial applications,
where limited samples are available before mass production. In this paper, we
propose a few-shot anomaly detection method that integrates adversarial
training loss to obtain more robust and generalized feature representations. We
utilize the adversarial loss previously employed in domain adaptation to align
feature distributions between source and target domains, to enhance feature
robustness and generalization in few-shot anomaly detection tasks. We
hypothesize that adversarial loss is effective when applied to features that
should have similar characteristics, such as those from the same layer in a
Siamese network's parallel branches or input-output pairs of
reconstruction-based methods. Experimental results demonstrate that the
proposed method generally achieves better performance when utilizing the
adversarial loss.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03009">I-PHYRE: Interactive Physical Reasoning. (arXiv:2312.03009v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shiqian Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1">Kewen Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yixin Zhu</a></p>
<p>Current evaluation protocols predominantly assess physical reasoning in
stationary scenes, creating a gap in evaluating agents' abilities to interact
with dynamic events. While contemporary methods allow agents to modify initial
scene configurations and observe consequences, they lack the capability to
interact with events in real time. To address this, we introduce I-PHYRE, a
framework that challenges agents to simultaneously exhibit intuitive physical
reasoning, multi-step planning, and in-situ intervention. Here, intuitive
physical reasoning refers to a quick, approximate understanding of physics to
address complex problems; multi-step denotes the need for extensive sequence
planning in I-PHYRE, considering each intervention can significantly alter
subsequent choices; and in-situ implies the necessity for timely object
manipulation within a scene, where minor timing deviations can result in task
failure. We formulate four game splits to scrutinize agents' learning and
generalization of essential principles of interactive physical reasoning,
fostering learning through interaction with representative scenarios. Our
exploration involves three planning strategies and examines several supervised
and reinforcement agents' zero-shot generalization proficiency on I-PHYRE. The
outcomes highlight a notable gap between existing learning algorithms and human
performance, emphasizing the imperative for more research in enhancing agents
with interactive physical reasoning capabilities. The environment and baselines
will be made publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03011">InstructBooth: Instruction-following Personalized Text-to-Image Generation. (arXiv:2312.03011v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chae_D/0/1/0/all/0/1">Daewon Chae</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1">Nokyung Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jinkyu Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kimin Lee</a></p>
<p>Personalizing text-to-image models using a limited set of images for a
specific object has been explored in subject-specific image generation.
However, existing methods often encounter challenges in aligning with text
prompts due to overfitting to the limited training images. In this work, we
introduce InstructBooth, a novel method designed to enhance image-text
alignment in personalized text-to-image models. Our approach first personalizes
text-to-image models with a small number of subject-specific images using a
unique identifier. After personalization, we fine-tune personalized
text-to-image models using reinforcement learning to maximize a reward that
quantifies image-text alignment. Additionally, we propose complementary
techniques to increase the synergy between these two processes. Our method
demonstrates superior image-text alignment compared to baselines while
maintaining personalization ability. In human evaluations, InstructBooth
outperforms DreamBooth when considering all comprehensive factors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03013">Breast Ultrasound Report Generation using LangChain. (arXiv:2312.03013v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Huh_J/0/1/0/all/0/1">Jaeyoung Huh</a>, <a href="http://arxiv.org/find/eess/1/au:+Park_H/0/1/0/all/0/1">Hyun Jeong Park</a>, <a href="http://arxiv.org/find/eess/1/au:+Ye_J/0/1/0/all/0/1">Jong Chul Ye</a></p>
<p>Breast ultrasound (BUS) is a critical diagnostic tool in the field of breast
imaging, aiding in the early detection and characterization of breast
abnormalities. Interpreting breast ultrasound images commonly involves creating
comprehensive medical reports, containing vital information to promptly assess
the patient's condition. However, the ultrasound imaging system necessitates
capturing multiple images of various parts to compile a single report,
presenting a time-consuming challenge. To address this problem, we propose the
integration of multiple image analysis tools through a LangChain using Large
Language Models (LLM), into the breast reporting process. Through a combination
of designated tools and text generation through LangChain, our method can
accurately extract relevant features from ultrasound images, interpret them in
a clinical context, and produce comprehensive and standardized reports. This
approach not only reduces the burden on radiologists and healthcare
professionals but also enhances the consistency and quality of reports. The
extensive experiments shows that each tools involved in the proposed method can
offer qualitatively and quantitatively significant results. Furthermore,
clinical evaluation on the generated reports demonstrates that the proposed
method can make report in clinically meaningful way.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03014">Foundation Models for Weather and Climate Data Understanding: A Comprehensive Survey. (arXiv:2312.03014v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shengchao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1">Guodong Long</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1">Jing Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1">Dikai Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chengqi Zhang</a></p>
<p>As artificial intelligence (AI) continues to rapidly evolve, the realm of
Earth and atmospheric sciences is increasingly adopting data-driven models,
powered by progressive developments in deep learning (DL). Specifically, DL
techniques are extensively utilized to decode the chaotic and nonlinear aspects
of Earth systems, and to address climate challenges via understanding weather
and climate data. Cutting-edge performance on specific tasks within narrower
spatio-temporal scales has been achieved recently through DL. The rise of large
models, specifically large language models (LLMs), has enabled fine-tuning
processes that yield remarkable outcomes across various downstream tasks,
thereby propelling the advancement of general AI. However, we are still
navigating the initial stages of crafting general AI for weather and climate.
In this survey, we offer an exhaustive, timely overview of state-of-the-art AI
methodologies specifically engineered for weather and climate data, with a
special focus on time series and text data. Our primary coverage encompasses
four critical aspects: types of weather and climate data, principal model
architectures, model scopes and applications, and datasets for weather and
climate. Furthermore, in relation to the creation and application of foundation
models for weather and climate data understanding, we delve into the field's
prevailing challenges, offer crucial insights, and propose detailed avenues for
future research. This comprehensive approach equips practitioners with the
requisite knowledge to make substantial progress in this domain. Our survey
encapsulates the most recent breakthroughs in research on large, data-driven
models for weather and climate data understanding, emphasizing robust
foundations, current advancements, practical applications, crucial resources,
and prospective research opportunities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03015">PartSLIP++: Enhancing Low-Shot 3D Part Segmentation via Multi-View Instance Segmentation and Maximum Likelihood Estimation. (arXiv:2312.03015v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yuchen Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jiayuan Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xuanlin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Minghua Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1">Yunhao Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1">Hao Su</a></p>
<p>Open-world 3D part segmentation is pivotal in diverse applications such as
robotics and AR/VR. Traditional supervised methods often grapple with limited
3D data availability and struggle to generalize to unseen object categories.
PartSLIP, a recent advancement, has made significant strides in zero- and
few-shot 3D part segmentation. This is achieved by harnessing the capabilities
of the 2D open-vocabulary detection module, GLIP, and introducing a heuristic
method for converting and lifting multi-view 2D bounding box predictions into
3D segmentation masks. In this paper, we introduce PartSLIP++, an enhanced
version designed to overcome the limitations of its predecessor. Our approach
incorporates two major improvements. First, we utilize a pre-trained 2D
segmentation model, SAM, to produce pixel-wise 2D segmentations, yielding more
precise and accurate annotations than the 2D bounding boxes used in PartSLIP.
Second, PartSLIP++ replaces the heuristic 3D conversion process with an
innovative modified Expectation-Maximization algorithm. This algorithm
conceptualizes 3D instance segmentation as unobserved latent variables, and
then iteratively refines them through an alternating process of 2D-3D matching
and optimization with gradient descent. Through extensive evaluations, we show
that PartSLIP++ demonstrates better performance over PartSLIP in both low-shot
3D semantic and instance-based object part segmentation tasks. Code released at
https://github.com/zyc00/PartSLIP2.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03018">DreamVideo: High-Fidelity Image-to-Video Generation with Image Retention and Text Guidance. (arXiv:2312.03018v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Cong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jiaxi Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1">Panwen Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Songcen Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1">Xiaodan Liang</a></p>
<p>Image-to-video generation, which aims to generate a video starting from a
given reference image, has drawn great attention. Existing methods try to
extend pre-trained text-guided image diffusion models to image-guided video
generation models. Nevertheless, these methods often result in either low
fidelity or flickering over time due to their limitation to shallow image
guidance and poor temporal consistency. To tackle these problems, we propose a
high-fidelity image-to-video generation method by devising a frame retention
branch on the basis of a pre-trained video diffusion model, named DreamVideo.
Instead of integrating the reference image into the diffusion process in a
semantic level, our DreamVideo perceives the reference image via convolution
layers and concatenate the features with the noisy latents as model input. By
this means, the details of the reference image can be preserved to the greatest
extent. In addition, by incorporating double-condition classifier-free
guidance, a single image can be directed to videos of different actions by
providing varying prompt texts. This has significant implications for
controllable video generation and holds broad application prospects. We conduct
comprehensive experiments on the public dataset, both quantitative and
qualitative results indicate that our method outperforms the state-of-the-art
method. Especially for fidelity, our model has powerful image retention ability
and result in high FVD in UCF101 compared to other image-to-video models. Also,
precise control can be achieved by giving different text prompts. Further
details and comprehensive results of our model will be presented in
https://anonymous0769.github.io/DreamVideo/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03020">Enhanced Breast Cancer Tumor Classification using MobileNetV2: A Detailed Exploration on Image Intensity, Error Mitigation, and Streamlit-driven Real-time Deployment. (arXiv:2312.03020v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Surya_A/0/1/0/all/0/1">Aaditya Surya</a>, <a href="http://arxiv.org/find/eess/1/au:+Shah_A/0/1/0/all/0/1">Aditya Shah</a>, <a href="http://arxiv.org/find/eess/1/au:+Kabore_J/0/1/0/all/0/1">Jarnell Kabore</a>, <a href="http://arxiv.org/find/eess/1/au:+Sasikumar_S/0/1/0/all/0/1">Subash Sasikumar</a></p>
<p>This research introduces a sophisticated transfer learning model based on
Google's MobileNetV2 for breast cancer tumor classification into normal,
benign, and malignant categories, utilizing a dataset of 1576 ultrasound images
(265 normal, 891 benign, 420 malignant). The model achieves an accuracy of
0.82, precision of 0.83, recall of 0.81, ROC-AUC of 0.94, PR-AUC of 0.88, and
MCC of 0.74. It examines image intensity distributions and misclassification
errors, offering improvements for future applications. Addressing dataset
imbalances, the study ensures a generalizable model. This work, using a dataset
from Baheya Hospital, Cairo, Egypt, compiled by Walid Al-Dhabyani et al.,
emphasizes MobileNetV2's potential in medical imaging, aiming to improve
diagnostic precision in oncology. Additionally, the paper explores
Streamlit-based deployment for real-time tumor classification, demonstrating
MobileNetV2's applicability in medical imaging and setting a benchmark for
future research in oncology diagnostics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03025">Training on Synthetic Data Beats Real Data in Multimodal Relation Extraction. (arXiv:2312.03025v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1">Zilin Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Haoxin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1">Xu Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Boyang Li</a></p>
<p>The task of multimodal relation extraction has attracted significant research
attention, but progress is constrained by the scarcity of available training
data. One natural thought is to extend existing datasets with cross-modal
generative models. In this paper, we consider a novel problem setting, where
only unimodal data, either text or image, are available during training. We aim
to train a multimodal classifier from synthetic data that perform well on real
multimodal test data. However, training with synthetic data suffers from two
obstacles: lack of data diversity and label information loss. To alleviate the
issues, we propose Mutual Information-aware Multimodal Iterated Relational dAta
GEneration (MI2RAGE), which applies Chained Cross-modal Generation (CCG) to
promote diversity in the generated data and exploits a teacher network to
select valuable training samples with high mutual information with the
ground-truth labels. Comparing our method to direct training on synthetic data,
we observed a significant improvement of 24.06% F1 with synthetic text and
26.42% F1 with synthetic images. Notably, our best model trained on completely
synthetic images outperforms prior state-of-the-art models trained on real
multimodal data by a margin of 3.76% in F1. Our codebase will be made available
upon acceptance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03026">Uni3DL: Unified Model for 3D and Language Understanding. (arXiv:2312.03026v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1">Jian Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhaoyang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1">Mohamed Elhoseiny</a></p>
<p>In this work, we present Uni3DL, a unified model for 3D and Language
understanding. Distinct from existing unified vision-language models in 3D
which are limited in task variety and predominantly dependent on projected
multi-view images, Uni3DL operates directly on point clouds. This approach
significantly expands the range of supported tasks in 3D, encompassing both
vision and vision-language tasks in 3D. At the core of Uni3DL, a query
transformer is designed to learn task-agnostic semantic and mask outputs by
attending to 3D visual features, and a task router is employed to selectively
generate task-specific outputs required for diverse tasks. With a unified
architecture, our Uni3DL model enjoys seamless task decomposition and
substantial parameter sharing across tasks. Uni3DL has been rigorously
evaluated across diverse 3D vision-language understanding tasks, including
semantic segmentation, object detection, instance segmentation, visual
grounding, 3D captioning, and text-3D cross-modal retrieval. It demonstrates
performance on par with or surpassing state-of-the-art (SOTA) task-specific
models. We hope our benchmark and Uni3DL model will serve as a solid step to
ease future research in unified models in the realm of 3D and language
understanding. Project page: https://uni3dl.github.io.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03027">Stable Diffusion Exposed: Gender Bias from Prompt to Image. (arXiv:2312.03027v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yankun Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Nakashima_Y/0/1/0/all/0/1">Yuta Nakashima</a>, <a href="http://arxiv.org/find/cs/1/au:+Garcia_N/0/1/0/all/0/1">Noa Garcia</a></p>
<p>Recent studies have highlighted biases in generative models, shedding light
on their predisposition towards gender-based stereotypes and imbalances. This
paper contributes to this growing body of research by introducing an evaluation
protocol designed to automatically analyze the impact of gender indicators on
Stable Diffusion images. Leveraging insights from prior work, we explore how
gender indicators not only affect gender presentation but also the
representation of objects and layouts within the generated images. Our findings
include the existence of differences in the depiction of objects, such as
instruments tailored for specific genders, and shifts in overall layouts. We
also reveal that neutral prompts tend to produce images more aligned with
masculine prompts than their feminine counterparts, providing valuable insights
into the nuanced gender biases inherent in Stable Diffusion.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03028">Double Integral Enhanced Zeroing Neural Network Optimized with ALSOA fostered Lung Cancer Classification using CT Images. (arXiv:2312.03028v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Sumitha_V/0/1/0/all/0/1">V S Priya Sumitha</a>, <a href="http://arxiv.org/find/eess/1/au:+Keerthika_V/0/1/0/all/0/1">V.Keerthika</a>, <a href="http://arxiv.org/find/eess/1/au:+Geetha_A/0/1/0/all/0/1">A. Geetha</a></p>
<p>Lung cancer is one of the deadliest diseases and the leading cause of illness
and death. Since lung cancer cannot predicted at premature stage, it able to
only be discovered more broadly once it has spread to other lung parts. The
risk grows when radiologists and other specialists determine whether lung
cancer is current. Owing to significance of determining type of treatment and
its depth based on severity of the illness, critical to develop smart and
automatic cancer prediction scheme is precise, at which stage of cancer. In
this paper, Double Integral Enhanced Zeroing Neural Network Optimized with
ALSOA fostered Lung Cancer Classification using CT Images (LCC-DIEZNN-ALSO-CTI)
is proposed. Initially, input CT image is amassed from lung cancer dataset. The
input CT image is pre-processing via Unscented Trainable Kalman Filtering
(UTKF) technique. In pre-processing stage unwanted noise are removed from CT
images. Afterwards, grayscale statistic features and Haralick texture features
extracted by Adaptive and Concise Empirical Wavelet Transform (ACEWT). The
proposed model is implemented on MATLAB. The performance of the proposed method
is analyzed through existing techniques. The proposed method attains 18.32%,
27.20%, and 34.32% higher accuracy analyzed with existing method likes Deep
Learning Assisted Predict of Lung Cancer on Computed Tomography Images
Utilizing AHHMM (LCC-AHHMM-CT), Convolutional neural networks based pulmonary
nodule malignancy assessment in pipeline for classifying lung cancer
(LCC-ICNN-CT), Automated Decision Support Scheme for Lung Cancer Identification
with Categorization (LCC-RFCN-MLRPN-CT) methods respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03029">Gaussian Head Avatar: Ultra High-fidelity Head Avatar via Dynamic Gaussians. (arXiv:2312.03029v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yuelang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Benwang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhe Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongwen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lizhen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Zerong Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yebin Liu</a></p>
<p>Creating high-fidelity 3D head avatars has always been a research hotspot,
but there remains a great challenge under lightweight sparse view setups. In
this paper, we propose Gaussian Head Avatar represented by controllable 3D
Gaussians for high-fidelity head avatar modeling. We optimize the neutral 3D
Gaussians and a fully learned MLP-based deformation field to capture complex
expressions. The two parts benefit each other, thereby our method can model
fine-grained dynamic details while ensuring expression accuracy. Furthermore,
we devise a well-designed geometry-guided initialization strategy based on
implicit SDF and Deep Marching Tetrahedra for the stability and convergence of
the training procedure. Experiments show our approach outperforms other
state-of-the-art sparse-view methods, achieving ultra high-fidelity rendering
quality at 2K resolution even under exaggerated expressions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03030">Generating Visually Realistic Adversarial Patch. (arXiv:2312.03030v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaosen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kunyu Wang</a></p>
<p>Deep neural networks (DNNs) are vulnerable to various types of adversarial
examples, bringing huge threats to security-critical applications. Among these,
adversarial patches have drawn increasing attention due to their good
applicability to fool DNNs in the physical world. However, existing works often
generate patches with meaningless noise or patterns, making it conspicuous to
humans. To address this issue, we explore how to generate visually realistic
adversarial patches to fool DNNs. Firstly, we analyze that a high-quality
adversarial patch should be realistic, position irrelevant, and printable to be
deployed in the physical world. Based on this analysis, we propose an effective
attack called VRAP, to generate visually realistic adversarial patches.
Specifically, VRAP constrains the patch in the neighborhood of a real image to
ensure the visual reality, optimizes the patch at the poorest position for
position irrelevance, and adopts Total Variance loss as well as gamma
transformation to make the generated patch printable without losing
information. Empirical evaluations on the ImageNet dataset demonstrate that the
proposed VRAP exhibits outstanding attack performance in the digital world.
Moreover, the generated adversarial patches can be disguised as the scrawl or
logo in the physical world to fool the deep models without being detected,
bringing significant threats to DNNs-enabled applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03031">Is Ego Status All You Need for Open-Loop End-to-End Autonomous Driving?. (arXiv:2312.03031v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhiqi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Zhiding Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lan_S/0/1/0/all/0/1">Shiyi Lan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiahan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1">Jan Kautz</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1">Tong Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1">Jose M. Alvarez</a></p>
<p>End-to-end autonomous driving recently emerged as a promising research
direction to target autonomy from a full-stack perspective. Along this line,
many of the latest works follow an open-loop evaluation setting on nuScenes to
study the planning behavior. In this paper, we delve deeper into the problem by
conducting thorough analyses and demystifying more devils in the details. We
initially observed that the nuScenes dataset, characterized by relatively
simple driving scenarios, leads to an under-utilization of perception
information in end-to-end models incorporating ego status, such as the ego
vehicle's velocity. These models tend to rely predominantly on the ego
vehicle's status for future path planning. Beyond the limitations of the
dataset, we also note that current metrics do not comprehensively assess the
planning quality, leading to potentially biased conclusions drawn from existing
benchmarks. To address this issue, we introduce a new metric to evaluate
whether the predicted trajectories adhere to the road. We further propose a
simple baseline able to achieve competitive results without relying on
perception annotations. Given the current limitations on the benchmark and
metrics, we suggest the community reassess relevant prevailing research and be
cautious whether the continued pursuit of state-of-the-art would yield
convincing and universal conclusions. Code and models are available at
\url{https://github.com/NVlabs/BEV-Planner}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03032">Zero-Shot Point Cloud Registration. (arXiv:2312.03032v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Weijie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mei_G/0/1/0/all/0/1">Guofeng Mei</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1">Bin Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xiaoshui Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Poiesi_F/0/1/0/all/0/1">Fabio Poiesi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1">Luc Van Gool</a>, <a href="http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1">Nicu Sebe</a>, <a href="http://arxiv.org/find/cs/1/au:+Lepri_B/0/1/0/all/0/1">Bruno Lepri</a></p>
<p>Learning-based point cloud registration approaches have significantly
outperformed their traditional counterparts. However, they typically require
extensive training on specific datasets. In this paper, we propose , the first
zero-shot point cloud registration approach that eliminates the need for
training on point cloud datasets. The cornerstone of ZeroReg is the novel
transfer of image features from keypoints to the point cloud, enriched by
aggregating information from 3D geometric neighborhoods. Specifically, we
extract keypoints and features from 2D image pairs using a frozen pretrained 2D
backbone. These features are then projected in 3D, and patches are constructed
by searching for neighboring points. We integrate the geometric and visual
features of each point using our novel parameter-free geometric decoder.
Subsequently, the task of determining correspondences between point clouds is
formulated as an optimal transport problem. Extensive evaluations of ZeroReg
demonstrate its competitive performance against both traditional and
learning-based methods. On benchmarks such as 3DMatch, 3DLoMatch, and ScanNet,
ZeroReg achieves impressive Recall Ratios (RR) of over 84%, 46%, and 75%,
respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03033">LiDAR-based Person Re-identification. (arXiv:2312.03033v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1">Wenxuan Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1">Zhiyu Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1">Yingping Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1">Ziheng Xi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1">Zhi Chen Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1">Jianjiang Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jie Zhou</a></p>
<p>Camera-based person re-identification (ReID) systems have been widely applied
in the field of public security. However, cameras often lack the perception of
3D morphological information of human and are susceptible to various
limitations, such as inadequate illumination, complex background, and personal
privacy. In this paper, we propose a LiDAR-based ReID framework, ReID3D, that
utilizes pre-training strategy to retrieve features of 3D body shape and
introduces Graph-based Complementary Enhancement Encoder for extracting
comprehensive features. Due to the lack of LiDAR datasets, we build LReID, the
first LiDAR-based person ReID dataset, which is collected in several outdoor
scenes with variations in natural conditions. Additionally, we introduce
LReID-sync, a simulated pedestrian dataset designed for pre-training encoders
with tasks of point cloud completion and shape parameter learning. Extensive
experiments on LReID show that ReID3D achieves exceptional performance with a
rank-1 accuracy of 94.0, highlighting the significant potential of LiDAR in
addressing person ReID tasks. To the best of our knowledge, we are the first to
propose a solution for LiDAR-based ReID. The code and datasets will be released
soon.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03035">SEVA: Leveraging sketches to evaluate alignment between human and machine visual abstraction. (arXiv:2312.03035v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mukherjee_K/0/1/0/all/0/1">Kushin Mukherjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Huey_H/0/1/0/all/0/1">Holly Huey</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1">Xuanchen Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Vinker_Y/0/1/0/all/0/1">Yael Vinker</a>, <a href="http://arxiv.org/find/cs/1/au:+Aguina_Kang_R/0/1/0/all/0/1">Rio Aguina-Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shamir_A/0/1/0/all/0/1">Ariel Shamir</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1">Judith E. Fan</a></p>
<p>Sketching is a powerful tool for creating abstract images that are sparse but
meaningful. Sketch understanding poses fundamental challenges for
general-purpose vision algorithms because it requires robustness to the
sparsity of sketches relative to natural visual inputs and because it demands
tolerance for semantic ambiguity, as sketches can reliably evoke multiple
meanings. While current vision algorithms have achieved high performance on a
variety of visual tasks, it remains unclear to what extent they understand
sketches in a human-like way. Here we introduce SEVA, a new benchmark dataset
containing approximately 90K human-generated sketches of 128 object concepts
produced under different time constraints, and thus systematically varying in
sparsity. We evaluated a suite of state-of-the-art vision algorithms on their
ability to correctly identify the target concept depicted in these sketches and
to generate responses that are strongly aligned with human response patterns on
the same sketch recognition task. We found that vision algorithms that better
predicted human sketch recognition performance also better approximated human
uncertainty about sketch meaning, but there remains a sizable gap between model
and human response patterns. To explore the potential of models that emulate
human visual abstraction in generative tasks, we conducted further evaluations
of a recently developed sketch generation algorithm (Vinker et al., 2022)
capable of generating sketches that vary in sparsity. We hope that public
release of this dataset and evaluation protocol will catalyze progress towards
algorithms with enhanced capacities for human-like visual abstraction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03043">Navigating the Synthetic Realm: Harnessing Diffusion-based Models for Laparoscopic Text-to-Image Generation. (arXiv:2312.03043v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Allmendinger_S/0/1/0/all/0/1">Simeon Allmendinger</a>, <a href="http://arxiv.org/find/eess/1/au:+Hemmer_P/0/1/0/all/0/1">Patrick Hemmer</a>, <a href="http://arxiv.org/find/eess/1/au:+Queisner_M/0/1/0/all/0/1">Moritz Queisner</a>, <a href="http://arxiv.org/find/eess/1/au:+Sauer_I/0/1/0/all/0/1">Igor Sauer</a>, <a href="http://arxiv.org/find/eess/1/au:+Muller_L/0/1/0/all/0/1">Leopold M&#xfc;ller</a>, <a href="http://arxiv.org/find/eess/1/au:+Jakubik_J/0/1/0/all/0/1">Johannes Jakubik</a>, <a href="http://arxiv.org/find/eess/1/au:+Vossing_M/0/1/0/all/0/1">Michael V&#xf6;ssing</a>, <a href="http://arxiv.org/find/eess/1/au:+Kuhl_N/0/1/0/all/0/1">Niklas K&#xfc;hl</a></p>
<p>Recent advances in synthetic imaging open up opportunities for obtaining
additional data in the field of surgical imaging. This data can provide
reliable supplements supporting surgical applications and decision-making
through computer vision. Particularly the field of image-guided surgery, such
as laparoscopic and robotic-assisted surgery, benefits strongly from synthetic
image datasets and virtual surgical training methods. Our study presents an
intuitive approach for generating synthetic laparoscopic images from short text
prompts using diffusion-based generative models. We demonstrate the usage of
state-of-the-art text-to-image architectures in the context of laparoscopic
imaging with regard to the surgical removal of the gallbladder as an example.
Results on fidelity and diversity demonstrate that diffusion-based models can
acquire knowledge about the style and semantics in the field of image-guided
surgery. A validation study with a human assessment survey underlines the
realistic nature of our synthetic data, as medical personnel detects actual
images in a pool with generated images causing a false-positive rate of 66%. In
addition, the investigation of a state-of-the-art machine learning model to
recognize surgical actions indicates enhanced results when trained with
additional generated images of up to 5.20%. Overall, the achieved image quality
contributes to the usage of computer-generated images in surgical applications
and enhances its path to maturity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03045">Customization Assistant for Text-to-image Generation. (arXiv:2312.03045v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yufan Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Ruiyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jiuxiang Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1">Tong Sun</a></p>
<p>Customizing pre-trained text-to-image generation model has attracted massive
research interest recently, due to its huge potential in real-world
applications. Although existing methods are able to generate creative content
for a novel concept contained in single user-input image, their capability are
still far from perfection. Specifically, most existing methods require
fine-tuning the generative model on testing images. Some existing methods do
not require fine-tuning, while their performance are unsatisfactory.
Furthermore, the interaction between users and models are still limited to
directive and descriptive prompts such as instructions and captions. In this
work, we build a customization assistant based on pre-trained large language
model and diffusion model, which can not only perform customized generation in
a tuning-free manner, but also enable more user-friendly interactions: users
can chat with the assistant and input either ambiguous text or clear
instruction. Specifically, we propose a new framework consists of a new model
design and a novel training strategy. The resulting assistant can perform
customized generation in 2-5 seconds without any test time fine-tuning.
Extensive experiments are conducted, competitive results have been obtained
across different domains, illustrating the effectiveness of the proposed
method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03046">Diversified in-domain synthesis with efficient fine-tuning for few-shot classification. (arXiv:2312.03046v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Costa_V/0/1/0/all/0/1">Victor G. Turrisi da Costa</a>, <a href="http://arxiv.org/find/cs/1/au:+DallAsen_N/0/1/0/all/0/1">Nicola Dall&#x27;Asen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yiming Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1">Nicu Sebe</a>, <a href="http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1">Elisa Ricci</a></p>
<p>Few-shot image classification aims to learn an image classifier using only a
small set of labeled examples per class. A recent research direction for
improving few-shot classifiers involves augmenting the labelled samples with
synthetic images created by state-of-the-art text-to-image generation models.
Following this trend, we propose Diversified in-domain synthesis with efficient
fine-tuning (DISEF), a novel approach which addresses the generalization
challenge in few-shot learning using synthetic data. DISEF consists of two main
components. First, we propose a novel text-to-image augmentation pipeline that,
by leveraging the real samples and their rich semantics coming from an advanced
captioning model, promotes in-domain sample diversity for better
generalization. Second, we emphasize the importance of effective model
fine-tuning in few-shot recognition, proposing to use Low-Rank Adaptation
(LoRA) for joint adaptation of the text and image encoders in a Vision Language
Model. We validate our method in ten different benchmarks, consistently
outperforming baselines and establishing a new state-of-the-art for few-shot
classification. Code is available at \url{https://github.com/vturrisi/disef}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03047">MagicStick: Controllable Video Editing via Control Handle Transformations. (arXiv:2312.03047v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yue Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Cun_X/0/1/0/all/0/1">Xiaodong Cun</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yingqing He</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_C/0/1/0/all/0/1">Chenyang Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xintao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1">Ying Shan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Qifeng Chen</a></p>
<p>Text-based video editing has recently attracted considerable interest in
changing the style or replacing the objects with a similar structure. Beyond
this, we demonstrate that properties such as shape, size, location, motion,
etc., can also be edited in videos. Our key insight is that the keyframe
transformations of the specific internal feature (e.g., edge maps of objects or
human pose), can easily propagate to other frames to provide generation
guidance. We thus propose MagicStick, a controllable video editing method that
edits the video properties by utilizing the transformation on the extracted
internal control signals. In detail, to keep the appearance, we inflate both
the pretrained image diffusion model and ControlNet to the temporal dimension
and train low-rank adaptions (LORA) layers to fit the specific scenes. Then, in
editing, we perform an inversion and editing framework. Differently, finetuned
ControlNet is introduced in both inversion and generation for attention
guidance with the proposed attention remix between the spatial attention maps
of inversion and editing. Yet succinct, our method is the first method to show
the ability of video property editing from the pre-trained text-to-image model.
We present experiments on numerous examples within our unified framework. We
also compare with shape-aware text-based editing and handcrafted motion video
generation, demonstrating our superior temporal consistency and editing
capability than previous works. The code and models will be made publicly
available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03048">DGInStyle: Domain-Generalizable Semantic Segmentation with Image Diffusion Models and Stylized Semantic Control. (arXiv:2312.03048v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1">Yuru Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoyer_L/0/1/0/all/0/1">Lukas Hoyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Shengyu Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tianfu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1">Luc Van Gool</a>, <a href="http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1">Konrad Schindler</a>, <a href="http://arxiv.org/find/cs/1/au:+Obukhov_A/0/1/0/all/0/1">Anton Obukhov</a></p>
<p>Large, pretrained latent diffusion models (LDMs) have demonstrated an
extraordinary ability to generate creative content, specialize to user data
through few-shot fine-tuning, and condition their output on other modalities,
such as semantic maps. However, are they usable as large-scale data generators,
e.g., to improve tasks in the perception stack, like semantic segmentation? We
investigate this question in the context of autonomous driving, and answer it
with a resounding "yes". We propose an efficient data generation pipeline
termed DGInStyle. First, we examine the problem of specializing a pretrained
LDM to semantically-controlled generation within a narrow domain. Second, we
design a Multi-resolution Latent Fusion technique to overcome the bias of LDMs
towards dominant objects. Third, we propose a Style Swap technique to endow the
rich generative prior with the learned semantic control. Using DGInStyle, we
generate a diverse dataset of street scenes, train a domain-agnostic semantic
segmentation model on it, and evaluate the model on multiple popular autonomous
driving datasets. Our approach consistently increases the performance of
several domain generalization methods, in some cases by +2.5 mIoU compared to
the previous state-of-the-art method without our generative augmentation
scheme. Source code and dataset are available at https://dginstyle.github.io .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03050">HIG: Hierarchical Interlacement Graph Approach to Scene Graph Generation in Video Understanding. (arXiv:2312.03050v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Trong-Thuan Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1">Pha Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Luu_K/0/1/0/all/0/1">Khoa Luu</a></p>
<p>Visual interactivity understanding within visual scenes presents a
significant challenge in computer vision. Existing methods focus on complex
interactivities while leveraging a simple relationship model. These methods,
however, struggle with a diversity of appearance, situation, position,
interaction, and relation in videos. This limitation hinders the ability to
fully comprehend the interplay within the complex visual dynamics of subjects.
In this paper, we delve into interactivities understanding within visual
content by deriving scene graph representations from dense interactivities
among humans and objects. To achieve this goal, we first present a new dataset
containing Appearance-Situation-Position-Interaction-Relation predicates, named
ASPIRe, offering an extensive collection of videos marked by a wide range of
interactivities. Then, we propose a new approach named Hierarchical
Interlacement Graph (HIG), which leverages a unified layer and graph within a
hierarchical structure to provide deep insights into scene changes across five
distinct tasks. Our approach demonstrates superior performance to other methods
through extensive experiments conducted in various scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03052">Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models. (arXiv:2312.03052v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yushi Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Stretcu_O/0/1/0/all/0/1">Otilia Stretcu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1">Chun-Ta Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Viswanathan_K/0/1/0/all/0/1">Krishnamurthy Viswanathan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hata_K/0/1/0/all/0/1">Kenji Hata</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_E/0/1/0/all/0/1">Enming Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1">Ranjay Krishna</a>, <a href="http://arxiv.org/find/cs/1/au:+Fuxman_A/0/1/0/all/0/1">Ariel Fuxman</a></p>
<p>Solving complex visual tasks such as "Who invented the musical instrument on
the right?" involves a composition of skills: understanding space, recognizing
instruments, and also retrieving prior knowledge. Recent work shows promise by
decomposing such tasks using a large language model (LLM) into an executable
program that invokes specialized vision models. However, generated programs are
error-prone: they omit necessary steps, include spurious ones, and are unable
to recover when the specialized models give incorrect outputs. Moreover, they
require loading multiple models, incurring high latency and computation costs.
We propose Visual Program Distillation (VPD), an instruction tuning framework
that produces a vision-language model (VLM) capable of solving complex visual
tasks with a single forward pass. VPD distills the reasoning ability of LLMs by
using them to sample multiple candidate programs, which are then executed and
verified to identify a correct one. It translates each correct program into a
language description of the reasoning steps, which are then distilled into a
VLM. Extensive experiments show that VPD improves the VLM's ability to count,
understand spatial relations, and reason compositionally. Our VPD-trained
PaLI-X outperforms all prior VLMs, achieving state-of-the-art performance
across complex vision tasks, including MMBench, OK-VQA, A-OKVQA, TallyQA, POPE,
and Hateful Memes. An evaluation with human annotators also confirms that VPD
improves model response factuality and consistency. Finally, experiments on
content moderation demonstrate that VPD is also helpful for adaptation to
real-world applications with limited data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03053">DiffusionPCR: Diffusion Models for Robust Multi-Step Point Cloud Registration. (arXiv:2312.03053v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1">Yufan Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dang_Z/0/1/0/all/0/1">Zheng Dang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_W/0/1/0/all/0/1">Wenbing Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Susstrunk_S/0/1/0/all/0/1">Sabine S&#xfc;sstrunk</a>, <a href="http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1">Mathieu Salzmann</a></p>
<p>Point Cloud Registration (PCR) estimates the relative rigid transformation
between two point clouds. We propose formulating PCR as a denoising diffusion
probabilistic process, mapping noisy transformations to the ground truth.
However, using diffusion models for PCR has nontrivial challenges, such as
adapting a generative model to a discriminative task and leveraging the
estimated nonlinear transformation from the previous step. Instead of training
a diffusion model to directly map pure noise to ground truth, we map the
predictions of an off-the-shelf PCR model to ground truth. The predictions of
off-the-shelf models are often imperfect, especially in challenging cases where
the two points clouds have low overlap, and thus could be seen as noisy
versions of the real rigid transformation. In addition, we transform the
rotation matrix into a spherical linear space for interpolation between samples
in the forward process, and convert rigid transformations into auxiliary
information to implicitly exploit last-step estimations in the reverse process.
As a result, conditioned on time step, the denoising model adapts to the
increasing accuracy across steps and refines registrations. Our extensive
experiments showcase the effectiveness of our DiffusionPCR, yielding
state-of-the-art registration recall rates (95.3%/81.6%) on 3DMatch and
3DLoMatch. The code will be made public upon publication.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03079">LooseControl: Lifting ControlNet for Generalized Depth Conditioning. (arXiv:2312.03079v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bhat_S/0/1/0/all/0/1">Shariq Farooq Bhat</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1">Niloy J. Mitra</a>, <a href="http://arxiv.org/find/cs/1/au:+Wonka_P/0/1/0/all/0/1">Peter Wonka</a></p>
<p>We present LooseControl to allow generalized depth conditioning for
diffusion-based image generation. ControlNet, the SOTA for depth-conditioned
image generation, produces remarkable results but relies on having access to
detailed depth maps for guidance. Creating such exact depth maps, in many
scenarios, is challenging. This paper introduces a generalized version of depth
conditioning that enables many new content-creation workflows. Specifically, we
allow (C1) scene boundary control for loosely specifying scenes with only
boundary conditions, and (C2) 3D box control for specifying layout locations of
the target objects rather than the exact shape and appearance of the objects.
Using LooseControl, along with text guidance, users can create complex
environments (e.g., rooms, street views, etc.) by specifying only scene
boundaries and locations of primary objects. Further, we provide two editing
mechanisms to refine the results: (E1) 3D box editing enables the user to
refine images by changing, adding, or removing boxes while freezing the style
of the image. This yields minimal changes apart from changes induced by the
edited boxes. (E2) Attribute editing proposes possible editing directions to
change one particular aspect of the scene, such as the overall object density
or a particular object. Extensive tests and comparisons with baselines
demonstrate the generality of our method. We believe that LooseControl can
become an important design tool for easily creating complex environments and be
extended to other forms of guidance channels. Code and more information are
available at https://shariqfarooq123.github.io/loose-control/ .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03085">ScAR: Scaling Adversarial Robustness for LiDAR Object Detection. (arXiv:2312.03085v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1">Xiaohu Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Radha_H/0/1/0/all/0/1">Hayder Radha</a></p>
<p>The adversarial robustness of a model is its ability to resist adversarial
attacks in the form of small perturbations to input data. Universal adversarial
attack methods such as Fast Sign Gradient Method (FSGM) and Projected Gradient
Descend (PGD) are popular for LiDAR object detection, but they are often
deficient compared to task-specific adversarial attacks. Additionally, these
universal methods typically require unrestricted access to the model's
information, which is difficult to obtain in real-world applications. To
address these limitations, we present a black-box Scaling Adversarial
Robustness (ScAR) method for LiDAR object detection. By analyzing the
statistical characteristics of 3D object detection datasets such as KITTI,
Waymo, and nuScenes, we have found that the model's prediction is sensitive to
scaling of 3D instances. We propose three black-box scaling adversarial attack
methods based on the available information: model-aware attack,
distribution-aware attack, and blind attack. We also introduce a strategy for
generating scaling adversarial examples to improve the model's robustness
against these three scaling adversarial attacks. Comparison with other methods
on public datasets under different 3D object detection architectures
demonstrates the effectiveness of our proposed method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03102">Fully Convolutional Slice-to-Volume Reconstruction for Single-Stack MRI. (arXiv:2312.03102v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Young_S/0/1/0/all/0/1">Sean I. Young</a>, <a href="http://arxiv.org/find/eess/1/au:+Balbastre_Y/0/1/0/all/0/1">Ya&#xeb;l Balbastre</a>, <a href="http://arxiv.org/find/eess/1/au:+Fischl_B/0/1/0/all/0/1">Bruce Fischl</a>, <a href="http://arxiv.org/find/eess/1/au:+Golland_P/0/1/0/all/0/1">Polina Golland</a>, <a href="http://arxiv.org/find/eess/1/au:+Iglesias_J/0/1/0/all/0/1">Juan Eugenio Iglesias</a></p>
<p>In magnetic resonance imaging (MRI), slice-to-volume reconstruction (SVR)
refers to computational reconstruction of an unknown 3D magnetic resonance
volume from stacks of 2D slices corrupted by motion. While promising, current
SVR methods require multiple slice stacks for accurate 3D reconstruction,
leading to long scans and limiting their use in time-sensitive applications
such as fetal fMRI. Here, we propose a SVR method that overcomes the
shortcomings of previous work and produces state-of-the-art reconstructions in
the presence of extreme inter-slice motion. Inspired by the recent success of
single-view depth estimation methods, we formulate SVR as a single-stack motion
estimation task and train a fully convolutional network to predict a motion
stack for a given slice stack, producing a 3D reconstruction as a byproduct of
the predicted motion. Extensive experiments on the SVR of adult and fetal
brains demonstrate that our fully convolutional method is twice as accurate as
previous SVR methods. Our code is available at github.com/seannz/svr.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03110">The Automated Bias Triangle Feature Extraction Framework. (arXiv:2312.03110v1 [cond-mat.mes-hall])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Kotzagiannidis_M/0/1/0/all/0/1">Madeleine Kotzagiannidis</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Schuff_J/0/1/0/all/0/1">Jonas Schuff</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Korda_N/0/1/0/all/0/1">Nathan Korda</a></p>
<p>Bias triangles represent features in stability diagrams of Quantum Dot (QD)
devices, whose occurrence and property analysis are crucial indicators for spin
physics. Nevertheless, challenges associated with quality and availability of
data as well as the subtlety of physical phenomena of interest have hindered an
automatic and bespoke analysis framework, often still relying (in part) on
human labelling and verification. We introduce a feature extraction framework
for bias triangles, built from unsupervised, segmentation-based computer vision
methods, which facilitates the direct identification and quantification of
physical properties of the former. Thereby, the need for human input or large
training datasets to inform supervised learning approaches is circumvented,
while additionally enabling the automation of pixelwise shape and feature
labeling. In particular, we demonstrate that Pauli Spin Blockade (PSB)
detection can be conducted effectively, efficiently and without any training
data as a direct result of this approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03119">AI-SAM: Automatic and Interactive Segment Anything Model. (arXiv:2312.03119v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1">Yimu Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Sitao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gernand_A/0/1/0/all/0/1">Alison D. Gernand</a>, <a href="http://arxiv.org/find/cs/1/au:+Goldstein_J/0/1/0/all/0/1">Jeffery A. Goldstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">James Z. Wang</a></p>
<p>Semantic segmentation is a core task in computer vision. Existing methods are
generally divided into two categories: automatic and interactive. Interactive
approaches, exemplified by the Segment Anything Model (SAM), have shown promise
as pre-trained models. However, current adaptation strategies for these models
tend to lean towards either automatic or interactive approaches. Interactive
methods depend on prompts user input to operate, while automatic ones bypass
the interactive promptability entirely. Addressing these limitations, we
introduce a novel paradigm and its first model: the Automatic and Interactive
Segment Anything Model (AI-SAM). In this paradigm, we conduct a comprehensive
analysis of prompt quality and introduce the pioneering Automatic and
Interactive Prompter (AI-Prompter) that automatically generates initial point
prompts while accepting additional user inputs. Our experimental results
demonstrate AI-SAM's effectiveness in the automatic setting, achieving
state-of-the-art performance. Significantly, it offers the flexibility to
incorporate additional user prompts, thereby further enhancing its performance.
The project page is available at https://github.com/ymp5078/AI-SAM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03133">Predicting Bone Degradation Using Vision Transformer and Synthetic Cellular Microstructures Dataset. (arXiv:2312.03133v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Hashemi_M/0/1/0/all/0/1">Mohammad Saber Hashemi</a>, <a href="http://arxiv.org/find/eess/1/au:+Sheidaei_A/0/1/0/all/0/1">Azadeh Sheidaei</a></p>
<p>Bone degradation, especially for astronauts in microgravity conditions, is
crucial for space exploration missions since the lower applied external forces
accelerate the diminution in bone stiffness and strength substantially.
Although existing computational models help us understand this phenomenon and
possibly restrict its effect in the future, they are time-consuming to simulate
the changes in the bones, not just the bone microstructures, of each individual
in detail. In this study, a robust yet fast computational method to predict and
visualize bone degradation has been developed. Our deep-learning method,
TransVNet, can take in different 3D voxelized images and predict their
evolution throughout months utilizing a hybrid 3D-CNN-VisionTransformer
autoencoder architecture. Because of limited available experimental data and
challenges of obtaining new samples, a digital twin dataset of diverse and
initial bone-like microstructures was generated to train our TransVNet on the
evolution of the 3D images through a previously developed degradation model for
microgravity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03154">ViscoNet: Bridging and Harmonizing Visual and Textual Conditioning for ControlNet. (arXiv:2312.03154v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheong_S/0/1/0/all/0/1">Soon Yau Cheong</a>, <a href="http://arxiv.org/find/cs/1/au:+Mustafa_A/0/1/0/all/0/1">Armin Mustafa</a>, <a href="http://arxiv.org/find/cs/1/au:+Gilbert_A/0/1/0/all/0/1">Andrew Gilbert</a></p>
<p>This paper introduces ViscoNet, a novel method that enhances text-to-image
human generation models with visual prompting. Unlike existing methods that
rely on lengthy text descriptions to control the image structure, ViscoNet
allows users to specify the visual appearance of the target object with a
reference image. ViscoNet disentangles the object's appearance from the image
background and injects it into a pre-trained latent diffusion model (LDM) model
via a ControlNet branch. This way, ViscoNet mitigates the style mode collapse
problem and enables precise and flexible visual control. We demonstrate the
effectiveness of ViscoNet on human image generation, where it can manipulate
visual attributes and artistic styles with text and image prompts. We also show
that ViscoNet can learn visual conditioning from small and specific object
domains while preserving the generative power of the LDM backbone.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03160">HybridNeRF: Efficient Neural Rendering via Adaptive Volumetric Surfaces. (arXiv:2312.03160v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Turki_H/0/1/0/all/0/1">Haithem Turki</a>, <a href="http://arxiv.org/find/cs/1/au:+Agrawal_V/0/1/0/all/0/1">Vasu Agrawal</a>, <a href="http://arxiv.org/find/cs/1/au:+Bulo_S/0/1/0/all/0/1">Samuel Rota Bul&#xf2;</a>, <a href="http://arxiv.org/find/cs/1/au:+Porzi_L/0/1/0/all/0/1">Lorenzo Porzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kontschieder_P/0/1/0/all/0/1">Peter Kontschieder</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1">Deva Ramanan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zollhofer_M/0/1/0/all/0/1">Michael Zollh&#xf6;fer</a>, <a href="http://arxiv.org/find/cs/1/au:+Richardt_C/0/1/0/all/0/1">Christian Richardt</a></p>
<p>Neural radiance fields provide state-of-the-art view synthesis quality but
tend to be slow to render. One reason is that they make use of volume
rendering, thus requiring many samples (and model queries) per ray at render
time. Although this representation is flexible and easy to optimize, most
real-world objects can be modeled more efficiently with surfaces instead of
volumes, requiring far fewer samples per ray. This observation has spurred
considerable progress in surface representations such as signed distance
functions, but these may struggle to model semi-opaque and thin structures. We
propose a method, HybridNeRF, that leverages the strengths of both
representations by rendering most objects as surfaces while modeling the
(typically) small fraction of challenging regions volumetrically. We evaluate
HybridNeRF against the challenging Eyeful Tower dataset along with other
commonly used view synthesis datasets. When comparing to state-of-the-art
baselines, including recent rasterization-based approaches, we improve error
rates by 15-30% while achieving real-time framerates (at least 36 FPS) for
virtual-reality resolutions (2Kx2K).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03187">FERGI: Automatic Annotation of User Preferences for Text-to-Image Generation from Spontaneous Facial Expression Reaction. (arXiv:2312.03187v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1">Shuangquan Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1">Junhua Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Sa_V/0/1/0/all/0/1">Virginia R. de Sa</a></p>
<p>Researchers have proposed to use data of human preference feedback to
fine-tune text-to-image generative models. However, the scalability of human
feedback collection has been limited by its reliance on manual annotation.
Therefore, we develop and test a method to automatically annotate user
preferences from their spontaneous facial expression reaction to the generated
images. We collect a dataset of Facial Expression Reaction to Generated Images
(FERGI) and show that the activations of multiple facial action units (AUs) are
highly correlated with user evaluations of the generated images. Specifically,
AU4 (brow lowerer) is most consistently reflective of negative evaluations of
the generated image. This can be useful in two ways. Firstly, we can
automatically annotate user preferences between image pairs with substantial
difference in AU4 responses to them with an accuracy significantly
outperforming state-of-the-art scoring models. Secondly, directly integrating
the AU4 responses with the scoring models improves their consistency with human
preferences. Additionally, the AU4 response best reflects the user's evaluation
of the image fidelity, making it complementary to the state-of-the-art scoring
models, which are generally better at reflecting image-text alignment. Finally,
this method of automatic annotation with facial expression analysis can be
potentially generalized to other generation tasks. The code is available at
https://github.com/ShuangquanFeng/FERGI, and the dataset is also available at
the same link for research purposes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03203">Feature 3DGS: Supercharging 3D Gaussian Splatting to Enable Distilled Feature Fields. (arXiv:2312.03203v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1">Shijie Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1">Haoran Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1">Sicheng Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1">Zhiwen Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Zehao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1">Dejia Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chari_P/0/1/0/all/0/1">Pradyumna Chari</a>, <a href="http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1">Suya You</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhangyang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kadambi_A/0/1/0/all/0/1">Achuta Kadambi</a></p>
<p>3D scene representations have gained immense popularity in recent years.
Methods that use Neural Radiance fields are versatile for traditional tasks
such as novel view synthesis. In recent times, some work has emerged that aims
to extend the functionality of NeRF beyond view synthesis, for semantically
aware tasks such as editing and segmentation using 3D feature field
distillation from 2D foundation models. However, these methods have two major
limitations: (a) they are limited by the rendering speed of NeRF pipelines, and
(b) implicitly represented feature fields suffer from continuity artifacts
reducing feature quality. Recently, 3D Gaussian Splatting has shown
state-of-the-art performance on real-time radiance field rendering. In this
work, we go one step further: in addition to radiance field rendering, we
enable 3D Gaussian splatting on arbitrary-dimension semantic features via 2D
foundation model distillation. This translation is not straightforward: naively
incorporating feature fields in the 3DGS framework leads to warp-level
divergence. We propose architectural and training changes to efficiently avert
this problem. Our proposed method is general, and our experiments showcase
novel view semantic segmentation, language-guided editing and segment anything
through learning feature fields from state-of-the-art 2D foundation models such
as SAM and CLIP-LSeg. Across experiments, our distillation method is able to
provide comparable or better results, while being significantly faster to both
train and render. Additionally, to the best of our knowledge, we are the first
method to enable point and bounding-box prompting for radiance field
manipulation, by leveraging the SAM model. Project website at:
https://feature-3dgs.github.io/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03207">Satellite Imagery and AI: A New Era in Ocean Conservation, from Research to Deployment and Impact. (arXiv:2312.03207v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Beukema_P/0/1/0/all/0/1">Patrick Beukema</a>, <a href="http://arxiv.org/find/cs/1/au:+Bastani_F/0/1/0/all/0/1">Favyen Bastani</a>, <a href="http://arxiv.org/find/cs/1/au:+Wolters_P/0/1/0/all/0/1">Piper Wolters</a>, <a href="http://arxiv.org/find/cs/1/au:+Herzog_H/0/1/0/all/0/1">Henry Herzog</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferdinando_J/0/1/0/all/0/1">Joe Ferdinando</a></p>
<p>Illegal, unreported, and unregulated (IUU) fishing poses a global threat to
ocean habitats. Publicly available satellite data offered by NASA and the
European Space Agency (ESA) provide an opportunity to actively monitor this
activity. Effectively leveraging satellite data for maritime conservation
requires highly reliable machine learning models operating globally with
minimal latency. This paper introduces three specialized computer vision models
designed for synthetic aperture radar (Sentinel-1), optical imagery
(Sentinel-2), and nighttime lights (Suomi-NPP/NOAA-20). It also presents best
practices for developing and delivering real-time computer vision services for
conservation. These models have been deployed in Skylight, a real time maritime
monitoring platform, which is provided at no cost to users worldwide.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03209">Cache Me if You Can: Accelerating Diffusion Models through Block Caching. (arXiv:2312.03209v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wimbauer_F/0/1/0/all/0/1">Felix Wimbauer</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1">Bichen Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Schoenfeld_E/0/1/0/all/0/1">Edgar Schoenfeld</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1">Xiaoliang Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1">Ji Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1">Zijian He</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanakoyeu_A/0/1/0/all/0/1">Artsiom Sanakoyeu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Peizhao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsai_S/0/1/0/all/0/1">Sam Tsai</a>, <a href="http://arxiv.org/find/cs/1/au:+Kohler_J/0/1/0/all/0/1">Jonas Kohler</a>, <a href="http://arxiv.org/find/cs/1/au:+Rupprecht_C/0/1/0/all/0/1">Christian Rupprecht</a>, <a href="http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1">Daniel Cremers</a>, <a href="http://arxiv.org/find/cs/1/au:+Vajda_P/0/1/0/all/0/1">Peter Vajda</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jialiang Wang</a></p>
<p>Diffusion models have recently revolutionized the field of image synthesis
due to their ability to generate photorealistic images. However, one of the
major drawbacks of diffusion models is that the image generation process is
costly. A large image-to-image network has to be applied many times to
iteratively refine an image from random noise. While many recent works propose
techniques to reduce the number of required steps, they generally treat the
underlying denoising network as a black box. In this work, we investigate the
behavior of the layers within the network and find that 1) the layers' output
changes smoothly over time, 2) the layers show distinct patterns of change, and
3) the change from step to step is often very small. We hypothesize that many
layer computations in the denoising network are redundant. Leveraging this, we
introduce block caching, in which we reuse outputs from layer blocks of
previous steps to speed up inference. Furthermore, we propose a technique to
automatically determine caching schedules based on each block's changes over
timesteps. In our experiments, we show through FID, human evaluation and
qualitative analysis that Block Caching allows to generate images with higher
visual quality at the same computational cost. We demonstrate this for
different state-of-the-art models (LDM and EMU) and solvers (DDIM and DPM).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03222">Predicting Scores of Various Aesthetic Attribute Sets by Learning from Overall Score Labels. (arXiv:2312.03222v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Heng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1">Xin Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yaqi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lou_H/0/1/0/all/0/1">Hao Lou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Chaoen Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1">Shuai Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xinning Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_D/0/1/0/all/0/1">Dongqing Zou</a></p>
<p>Now many mobile phones embed deep-learning models for evaluation or guidance
on photography. These models cannot provide detailed results like human pose
scores or scene color scores because of the rare of corresponding aesthetic
attribute data. However, the annotation of image aesthetic attribute scores
requires experienced artists and professional photographers, which hinders the
collection of large-scale fully-annotated datasets. In this paper, we propose
to replace image attribute labels with feature extractors. First, a novel
aesthetic attribute evaluation framework based on attribute features is
proposed to predict attribute scores and overall scores. We call it the F2S
(attribute features to attribute scores) model. We use networks from different
tasks to provide attribute features to our F2S models. Then, we define an
aesthetic attribute contribution to describe the role of aesthetic attributes
throughout an image and use it with the attribute scores and the overall scores
to train our F2S model. Sufficient experiments on publicly available datasets
demonstrate that our F2S model achieves comparable performance with those
trained on the datasets with fully-annotated aesthetic attribute score labels.
Our method makes it feasible to learn meaningful attribute scores for various
aesthetic attribute sets in different types of images with only overall
aesthetic scores.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03226">Rethinking Object Saliency Ranking: A Novel Whole-flow Processing Paradigm. (arXiv:2312.03226v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1">Mengke Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Linfeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1">Dunquan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1">Wenfeng Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chenglizhao Chen</a></p>
<p>Existing salient object detection methods are capable of predicting binary
maps that highlight visually salient regions. However, these methods are
limited in their ability to differentiate the relative importance of multiple
objects and the relationships among them, which can lead to errors and reduced
accuracy in downstream tasks that depend on the relative importance of multiple
objects. To conquer, this paper proposes a new paradigm for saliency ranking,
which aims to completely focus on ranking salient objects by their "importance
order". While previous works have shown promising performance, they still face
ill-posed problems. First, the saliency ranking ground truth (GT) orders
generation methods are unreasonable since determining the correct ranking order
is not well-defined, resulting in false alarms. Second, training a ranking
model remains challenging because most saliency ranking methods follow the
multi-task paradigm, leading to conflicts and trade-offs among different tasks.
Third, existing regression-based saliency ranking methods are complex for
saliency ranking models due to their reliance on instance mask-based saliency
ranking orders. These methods require a significant amount of data to perform
accurately and can be challenging to implement effectively. To solve these
problems, this paper conducts an in-depth analysis of the causes and proposes a
whole-flow processing paradigm of saliency ranking task from the perspective of
"GT data generation", "network structure design" and "training protocol". The
proposed approach outperforms existing state-of-the-art methods on the
widely-used SALICON set, as demonstrated by extensive experiments with fair and
reasonable comparisons. The saliency ranking task is still in its infancy, and
our proposed unified framework can serve as a fundamental strategy to guide
future work.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03227">Human Body Model based ID using Shape and Pose Parameters. (arXiv:2312.03227v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sundaresan_A/0/1/0/all/0/1">Aravind Sundaresan</a>, <a href="http://arxiv.org/find/cs/1/au:+Burns_B/0/1/0/all/0/1">Brian Burns</a>, <a href="http://arxiv.org/find/cs/1/au:+Sur_I/0/1/0/all/0/1">Indranil Sur</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1">Yi Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1">Xiao Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Sujeong Kim</a></p>
<p>We present a Human Body model based IDentification system (HMID) system that
is jointly trained for shape, pose and biometric identification. HMID is based
on the Human Mesh Recovery (HMR) network and we propose additional losses to
improve and stabilize shape estimation and biometric identification while
maintaining the pose and shape output. We show that when our HMID network is
trained using additional shape and pose losses, it shows a significant
improvement in biometric identification performance when compared to an
identical model that does not use such losses. The HMID model uses raw images
instead of silhouettes and is able to perform robust recognition on images
collected at range and altitude as many anthropometric properties are
reasonably invariant to clothing, view and range. We show results on the USF
dataset as well as the BRIAR dataset which includes probes with both clothing
and view changes. Our approach (using body model losses) shows a significant
improvement in Rank20 accuracy and True Accuracy Rate on the BRIAR evaluation
dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03231">Deep Multimodal Fusion for Surgical Feedback Classification. (arXiv:2312.03231v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kocielnik_R/0/1/0/all/0/1">Rafal Kocielnik</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_E/0/1/0/all/0/1">Elyssa Y. Wong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_T/0/1/0/all/0/1">Timothy N. Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1">Lydia Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1">De-An Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiayun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1">Anima Anandkumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Hung_A/0/1/0/all/0/1">Andrew J. Hung</a></p>
<p>Quantification of real-time informal feedback delivered by an experienced
surgeon to a trainee during surgery is important for skill improvements in
surgical training. Such feedback in the live operating room is inherently
multimodal, consisting of verbal conversations (e.g., questions and answers) as
well as non-verbal elements (e.g., through visual cues like pointing to
anatomic elements). In this work, we leverage a clinically-validated
five-category classification of surgical feedback: "Anatomic", "Technical",
"Procedural", "Praise" and "Visual Aid". We then develop a multi-label machine
learning model to classify these five categories of surgical feedback from
inputs of text, audio, and video modalities. The ultimate goal of our work is
to help automate the annotation of real-time contextual surgical feedback at
scale. Our automated classification of surgical feedback achieves AUCs ranging
from 71.5 to 77.6 with the fusion improving performance by 3.1%. We also show
that high-quality manual transcriptions of feedback audio from experts improve
AUCs to between 76.5 and 96.2, which demonstrates a clear path toward future
improvements. Empirically, we find that the Staged training strategy, with
first pre-training each modality separately and then training them jointly, is
more effective than training different modalities altogether. We also present
intuitive findings on the importance of modalities for different feedback
categories. This work offers an important first look at the feasibility of
automated classification of real-world live surgical feedback based on text,
audio, and video modalities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03266">SO-NeRF: Active View Planning for NeRF using Surrogate Objectives. (arXiv:2312.03266v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Keifer Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1">Shubham Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Sunglyoung Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Makwana_B/0/1/0/all/0/1">Bhargav Makwana</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1">Chen Feng</a></p>
<p>Despite the great success of Neural Radiance Fields (NeRF), its
data-gathering process remains vague with only a general rule of thumb of
sampling as densely as possible. The lack of understanding of what actually
constitutes good views for NeRF makes it difficult to actively plan a sequence
of views that yield the maximal reconstruction quality. We propose Surrogate
Objectives for Active Radiance Fields (SOAR), which is a set of interpretable
functions that evaluates the goodness of views using geometric and photometric
visual cues - surface coverage, geometric complexity, textural complexity, and
ray diversity. Moreover, by learning to infer the SOAR scores from a deep
network, SOARNet, we are able to effectively select views in mere seconds
instead of hours, without the need for prior visits to all the candidate views
or training any radiance field during such planning. Our experiments show
SOARNet outperforms the baselines with $\sim$80x speed-up while achieving
better or comparable reconstruction qualities. We finally show that SOAR is
model-agnostic, thus it generalizes across fully neural-implicit to fully
explicit approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03286">Indirect Gradient Matching for Adversarial Robust Distillation. (arXiv:2312.03286v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hongsin Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1">Seungju Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1">Changick Kim</a></p>
<p>Adversarial training significantly improves adversarial robustness, but
superior performance is primarily attained with large models. This substantial
performance gap for smaller models has spurred active research into adversarial
distillation (AD) to mitigate the difference. Existing AD methods leverage the
teacher's logits as a guide. In contrast to these approaches, we aim to
transfer another piece of knowledge from the teacher, the input gradient. In
this paper, we propose a distillation module termed Indirect Gradient
Distillation Module (IGDM) that indirectly matches the student's input gradient
with that of the teacher. We hypothesize that students can better acquire the
teacher's knowledge by matching the input gradient. Leveraging the observation
that adversarial training renders the model locally linear on the input space,
we employ Taylor approximation to effectively align gradients without directly
calculating them. Experimental results show that IGDM seamlessly integrates
with existing AD methods, significantly enhancing the performance of all AD
methods. Particularly, utilizing IGDM on the CIFAR-100 dataset improves the
AutoAttack accuracy from 28.06% to 30.32% with the ResNet-18 model and from
26.18% to 29.52% with the MobileNetV2 model when integrated into the SOTA
method without additional data augmentation. The code will be made available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03288">STEP CATFormer: Spatial-Temporal Effective Body-Part Cross Attention Transformer for Skeleton-based Action Recognition. (arXiv:2312.03288v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Long_N/0/1/0/all/0/1">Nguyen Huu Bao Long</a></p>
<p>Graph convolutional networks (GCNs) have been widely used and achieved
remarkable results in skeleton-based action recognition. We think the key to
skeleton-based action recognition is a skeleton hanging in frames, so we focus
on how the Graph Convolutional Convolution networks learn different topologies
and effectively aggregate joint features in the global temporal and local
temporal. In this work, we propose three Channel-wise Tolopogy Graph
Convolution based on Channel-wise Topology Refinement Graph Convolution
(CTR-GCN). Combining CTR-GCN with two joint cross-attention modules can capture
the upper-lower body part and hand-foot relationship skeleton features. After
that, to capture features of human skeletons changing in frames we design the
Temporal Attention Transformers to extract skeletons effectively. The Temporal
Attention Transformers can learn the temporal features of human skeleton
sequences. Finally, we fuse the temporal features output scale with MLP and
classification. We develop a powerful graph convolutional network named Spatial
Temporal Effective Body-part Cross Attention Transformer which notably
high-performance on the NTU RGB+D, NTU RGB+D 120 datasets. Our code and models
are available at https://github.com/maclong01/STEP-CATFormer
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03289">Class Incremental Learning for Adversarial Robustness. (arXiv:2312.03289v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1">Seungju Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hongshin Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1">Changick Kim</a></p>
<p>Adversarial training integrates adversarial examples during model training to
enhance robustness. However, its application in fixed dataset settings differs
from real-world dynamics, where data accumulates incrementally. In this study,
we investigate Adversarially Robust Class Incremental Learning (ARCIL), a
method that combines adversarial robustness with incremental learning. We
observe that combining incremental learning with naive adversarial training
easily leads to a loss of robustness. We discover that this is attributed to
the disappearance of the flatness of the loss function, a characteristic of
adversarial training. To address this issue, we propose the Flatness Preserving
Distillation (FPD) loss that leverages the output difference between
adversarial and clean examples. Additionally, we introduce the Logit Adjustment
Distillation (LAD) loss, which adapts the model's knowledge to perform well on
new tasks. Experimental results demonstrate the superiority of our method over
approaches that apply adversarial training to existing incremental learning
methods, which provides a strong baseline for incremental learning on
adversarial robustness in the future. Our method achieves AutoAttack accuracy
that is 5.99\%p, 5.27\%p, and 3.90\%p higher on average than the baseline on
split CIFAR-10, CIFAR-100, and Tiny ImageNet, respectively. The code will be
made available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03296">Cooperative Probabilistic Trajectory Forecasting under Occlusion. (arXiv:2312.03296v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nayak_A/0/1/0/all/0/1">Anshul Nayak</a>, <a href="http://arxiv.org/find/cs/1/au:+Eskandarian_A/0/1/0/all/0/1">Azim Eskandarian</a></p>
<p>Perception and planning under occlusion is essential for safety-critical
tasks. Occlusion-aware planning often requires communicating the information of
the occluded object to the ego agent for safe navigation. However,
communicating rich sensor information under adverse conditions during
communication loss and limited bandwidth may not be always feasible. Further,
in GPS denied environments and indoor navigation, localizing and sharing of
occluded objects can be challenging. To overcome this, relative pose estimation
between connected agents sharing a common field of view can be a
computationally effective way of communicating information about surrounding
objects. In this paper, we design an end-to-end network that cooperatively
estimates the current states of occluded pedestrian in the reference frame of
ego agent and then predicts the trajectory with safety guarantees.
Experimentally, we show that the uncertainty-aware trajectory prediction of
occluded pedestrian by the ego agent is almost similar to the ground truth
trajectory assuming no occlusion. The current research holds promise for
uncertainty-aware navigation among multiple connected agents under occlusion.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03298">DiffPMAE: Diffusion Masked Autoencoders for Point Cloud Reconstruction. (arXiv:2312.03298v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yanlong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Madarasingha_C/0/1/0/all/0/1">Chamara Madarasingha</a>, <a href="http://arxiv.org/find/cs/1/au:+Thilakarathna_K/0/1/0/all/0/1">Kanchana Thilakarathna</a></p>
<p>Point cloud streaming is increasingly getting popular, evolving into the norm
for interactive service delivery and the future Metaverse. However, the
substantial volume of data associated with point clouds presents numerous
challenges, particularly in terms of high bandwidth consumption and large
storage capacity. Despite various solutions proposed thus far, with a focus on
point cloud compression, upsampling, and completion, these
reconstruction-related methods continue to fall short in delivering high
fidelity point cloud output. As a solution, in DiffPMAE, we propose an
effective point cloud reconstruction architecture. Inspired by self-supervised
learning concepts, we combine Masked Auto-Encoding and Diffusion Model
mechanism to remotely reconstruct point cloud data. By the nature of this
reconstruction process, DiffPMAE can be extended to many related downstream
tasks including point cloud compression, upsampling and completion. Leveraging
ShapeNet-55 and ModelNet datasets with over 60000 objects, we validate the
performance of DiffPMAE exceeding many state-of-the-art methods in-terms of
auto-encoding and downstream tasks considered.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03318">Complementary Benefits of Contrastive Learning and Self-Training Under Distribution Shift. (arXiv:2312.03318v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1">Saurabh Garg</a>, <a href="http://arxiv.org/find/cs/1/au:+Setlur_A/0/1/0/all/0/1">Amrith Setlur</a>, <a href="http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1">Zachary Chase Lipton</a>, <a href="http://arxiv.org/find/cs/1/au:+Balakrishnan_S/0/1/0/all/0/1">Sivaraman Balakrishnan</a>, <a href="http://arxiv.org/find/cs/1/au:+Smith_V/0/1/0/all/0/1">Virginia Smith</a>, <a href="http://arxiv.org/find/cs/1/au:+Raghunathan_A/0/1/0/all/0/1">Aditi Raghunathan</a></p>
<p>Self-training and contrastive learning have emerged as leading techniques for
incorporating unlabeled data, both under distribution shift (unsupervised
domain adaptation) and when it is absent (semi-supervised learning). However,
despite the popularity and compatibility of these techniques, their efficacy in
combination remains unexplored. In this paper, we undertake a systematic
empirical investigation of this combination, finding that (i) in domain
adaptation settings, self-training and contrastive learning offer significant
complementary gains; and (ii) in semi-supervised learning settings,
surprisingly, the benefits are not synergistic. Across eight distribution shift
datasets (e.g., BREEDs, WILDS), we demonstrate that the combined method obtains
3--8% higher accuracy than either approach independently. We then theoretically
analyze these techniques in a simplified model of distribution shift,
demonstrating scenarios under which the features produced by contrastive
learning can yield a good initialization for self-training to further amplify
gains and achieve optimal performance, even when either method alone would
fail.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03322">Background Clustering Pre-training for Few-shot Segmentation. (arXiv:2312.03322v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Zhimiao Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1">Tiancheng Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yi Xu</a></p>
<p>Recent few-shot segmentation (FSS) methods introduce an extra pre-training
stage before meta-training to obtain a stronger backbone, which has become a
standard step in few-shot learning. Despite the effectiveness, current
pre-training scheme suffers from the merged background problem: only base
classes are labelled as foregrounds, making it hard to distinguish between
novel classes and actual background. In this paper, we propose a new
pre-training scheme for FSS via decoupling the novel classes from background,
called Background Clustering Pre-Training (BCPT). Specifically, we adopt online
clustering to the pixel embeddings of merged background to explore the
underlying semantic structures, bridging the gap between pre-training and
adaptation to novel classes. Given the clustering results, we further propose
the background mining loss and leverage base classes to guide the clustering
process, improving the quality and stability of clustering results. Experiments
on PASCAL-5i and COCO-20i show that BCPT yields advanced performance. Code will
be available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03325">GCFA:Geodesic Curve Feature Augmentation via Shape Space Theory. (arXiv:2312.03325v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1">Yuexing Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_G/0/1/0/all/0/1">Guanxin Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bing Wang</a></p>
<p>Deep learning has yielded remarkable outcomes in various domains. However,
the challenge of requiring large-scale labeled samples still persists in deep
learning. Thus, data augmentation has been introduced as a critical strategy to
train deep learning models. However, data augmentation suffers from information
loss and poor performance in small sample environments. To overcome these
drawbacks, we propose a feature augmentation method based on shape space
theory, i.e., Geodesic curve feature augmentation, called GCFA in brevity.
First, we extract features from the image with the neural network model. Then,
the multiple image features are projected into a pre-shape space as features.
In the pre-shape space, a Geodesic curve is built to fit the features. Finally,
the many generated features on the Geodesic curve are used to train the various
machine learning models. The GCFA module can be seamlessly integrated with most
machine learning methods. And the proposed method is simple, effective and
insensitive for the small sample datasets. Several examples demonstrate that
the GCFA method can greatly improve the performance of the data preprocessing
model in a small sample environment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03327">Building Category Graphs Representation with Spatial and Temporal Attention for Visual Navigation. (arXiv:2312.03327v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xiaobo Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Youfang Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1">HeHe Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhihao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_K/0/1/0/all/0/1">Kai Lv</a></p>
<p>Given an object of interest, visual navigation aims to reach the object's
location based on a sequence of partial observations. To this end, an agent
needs to 1) learn a piece of certain knowledge about the relations of object
categories in the world during training and 2) look for the target object based
on the pre-learned object category relations and its moving trajectory in the
current unseen environment. In this paper, we propose a Category Relation Graph
(CRG) to learn the knowledge of object category layout relations and a
Temporal-Spatial-Region (TSR) attention architecture to perceive the long-term
spatial-temporal dependencies of objects helping the navigation. We learn prior
knowledge of object layout, establishing a category relationship graph to
deduce the positions of specific objects. Subsequently, we introduced TSR to
capture the relationships of objects in temporal, spatial, and regions within
the observation trajectories. Specifically, we propose a Temporal attention
module (T) to model the temporal structure of the observation sequence, which
implicitly encodes the historical moving or trajectory information. Then, a
Spatial attention module (S) is used to uncover the spatial context of the
current observation objects based on the category relation graph and past
observations. Last, a Region attention module (R) shifts the attention to the
target-relevant region. Based on the visual representation extracted by our
method, the agent can better perceive the environment and easily learn superior
navigation policy. Experiments on AI2-THOR demonstrate our CRG-TSR method
significantly outperforms existing methods regarding both effectiveness and
efficiency. The code has been included in the supplementary material and will
be publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03339">PointJEM: Self-supervised Point Cloud Understanding for Reducing Feature Redundancy via Joint Entropy Maximization. (arXiv:2312.03339v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1">Xin Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_H/0/1/0/all/0/1">Huan Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1">Xinxin Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yifan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Kang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1">Linzhi Su</a></p>
<p>Most deep learning-based point cloud processing methods are supervised and
require large scale of labeled data. However, manual labeling of point cloud
data is laborious and time-consuming. Self-supervised representation learning
can address the aforementioned issue by learning robust and generalized
representations from unlabeled datasets. Nevertheless, the embedded features
obtained by representation learning usually contain redundant information, and
most current methods reduce feature redundancy by linear correlation
constraints. In this paper, we propose PointJEM, a self-supervised
representation learning method applied to the point cloud field. PointJEM
comprises an embedding scheme and a loss function based on joint entropy. The
embedding scheme divides the embedding vector into different parts, each part
can learn a distinctive feature. To reduce redundant information in the
features, PointJEM maximizes the joint entropy between the different parts,
thereby rendering the learned feature variables pairwise independent. To
validate the effectiveness of our method, we conducted experiments on multiple
datasets. The results demonstrate that our method can significantly reduce
feature redundancy beyond linear correlation. Furthermore, PointJEM achieves
competitive performance in downstream tasks such as classification and
segmentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03341">Online Vectorized HD Map Construction using Geometry. (arXiv:2312.03341v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhixin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yiyuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1">Xiaohan Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_F/0/1/0/all/0/1">Fusheng Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1">Xiangyu Yue</a></p>
<p>The construction of online vectorized High-Definition (HD) maps is critical
for downstream prediction and planning. Recent efforts have built strong
baselines for this task, however, shapes and relations of instances in urban
road systems are still under-explored, such as parallelism, perpendicular, or
rectangle-shape. In our work, we propose GeMap ($\textbf{Ge}$ometry
$\textbf{Map}$), which end-to-end learns Euclidean shapes and relations of map
instances beyond basic perception. Specifically, we design a geometric loss
based on angle and distance clues, which is robust to rigid transformations. We
also decouple self-attention to independently handle Euclidean shapes and
relations. Our method achieves new state-of-the-art performance on the NuScenes
and Argoverse 2 datasets. Remarkably, it reaches a 71.8% mAP on the large-scale
Argoverse 2 dataset, outperforming MapTR V2 by +4.4% and surpassing the 70% mAP
threshold for the first time. Code is available at
https://github.com/cnzzx/GeMap
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03345">GraNet: A Multi-Level Graph Network for 6-DoF Grasp Pose Generation in Cluttered Scenes. (arXiv:2312.03345v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haowen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Niu_W/0/1/0/all/0/1">Wanhao Niu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_C/0/1/0/all/0/1">Chungang Zhuang</a></p>
<p>6-DoF object-agnostic grasping in unstructured environments is a critical yet
challenging task in robotics. Most current works use non-optimized approaches
to sample grasp locations and learn spatial features without concerning the
grasping task. This paper proposes GraNet, a graph-based grasp pose generation
framework that translates a point cloud scene into multi-level graphs and
propagates features through graph neural networks. By building graphs at the
scene level, object level, and grasp point level, GraNet enhances feature
embedding at multiple scales while progressively converging to the ideal
grasping locations by learning. Our pipeline can thus characterize the spatial
distribution of grasps in cluttered scenes, leading to a higher rate of
effective grasping. Furthermore, we enhance the representation ability of
scalable graph networks by a structure-aware attention mechanism to exploit
local relations in graphs. Our method achieves state-of-the-art performance on
the large-scale GraspNet-1Billion benchmark, especially in grasping unseen
objects (+11.62 AP). The real robot experiment shows a high success rate in
grasping scattered objects, verifying the effectiveness of the proposed
approach in unstructured environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03350">PointMoment:Mixed-Moment-based Self-Supervised Representation Learning for 3D Point Clouds. (arXiv:2312.03350v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1">Xin Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1">Xinxin Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yifan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Mengna Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Kang Li</a></p>
<p>Large and rich data is a prerequisite for effective training of deep neural
networks. However, the irregularity of point cloud data makes manual annotation
time-consuming and laborious. Self-supervised representation learning, which
leverages the intrinsic structure of large-scale unlabelled data to learn
meaningful feature representations, has attracted increasing attention in the
field of point cloud research. However, self-supervised representation learning
often suffers from model collapse, resulting in reduced information and
diversity of the learned representation, and consequently degrading the
performance of downstream tasks. To address this problem, we propose
PointMoment, a novel framework for point cloud self-supervised representation
learning that utilizes a high-order mixed moment loss function rather than the
conventional contrastive loss function. Moreover, our framework does not
require any special techniques such as asymmetric network architectures,
gradient stopping, etc. Specifically, we calculate the high-order mixed moment
of the feature variables and force them to decompose into products of their
individual moment, thereby making multiple variables more independent and
minimizing the feature redundancy. We also incorporate a contrastive learning
approach to maximize the feature invariance under different data augmentations
of the same point cloud. Experimental results show that our approach
outperforms previous unsupervised learning methods on the downstream task of 3D
point cloud classification and segmentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03357">RING-NeRF: A Versatile Architecture based on Residual Implicit Neural Grids. (arXiv:2312.03357v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Petit_D/0/1/0/all/0/1">Doriand Petit</a>, <a href="http://arxiv.org/find/cs/1/au:+Bourgeois_S/0/1/0/all/0/1">Steve Bourgeois</a>, <a href="http://arxiv.org/find/cs/1/au:+Pavel_D/0/1/0/all/0/1">Dumitru Pavel</a>, <a href="http://arxiv.org/find/cs/1/au:+Gay_Bellile_V/0/1/0/all/0/1">Vincent Gay-Bellile</a>, <a href="http://arxiv.org/find/cs/1/au:+Chabot_F/0/1/0/all/0/1">Florian Chabot</a>, <a href="http://arxiv.org/find/cs/1/au:+Barthe_L/0/1/0/all/0/1">Loic Barthe</a></p>
<p>Since their introduction, Neural Fields have become very popular for 3D
reconstruction and new view synthesis. Recent researches focused on
accelerating the process, as well as improving the robustness to variation of
the observation distance and limited number of supervised viewpoints. However,
those approaches often led to dedicated solutions that cannot be easily
combined. To tackle this issue, we introduce a new simple but efficient
architecture named RING-NeRF, based on Residual Implicit Neural Grids, that
provides a control on the level of detail of the mapping function between the
scene and the latent spaces. Associated with a distance-aware forward mapping
mechanism and a continuous coarse-to-fine reconstruction process, our versatile
architecture demonstrates both fast training and state-of-the-art performances
in terms of: (1) anti-aliased rendering, (2) reconstruction quality from few
supervised viewpoints, and (3) robustness in the absence of appropriate
scene-specific initialization for SDF-based NeRFs. We also demonstrate that our
architecture can dynamically add grids to increase the details of the
reconstruction, opening the way to adaptive reconstruction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03368">Bottom-Up Instance Segmentation of Catheters for Chest X-Rays. (arXiv:2312.03368v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Boccardi_F/0/1/0/all/0/1">Francesca Boccardi</a>, <a href="http://arxiv.org/find/eess/1/au:+Saalbach_A/0/1/0/all/0/1">Axel Saalbach</a>, <a href="http://arxiv.org/find/eess/1/au:+Schulz_H/0/1/0/all/0/1">Heinrich Schulz</a>, <a href="http://arxiv.org/find/eess/1/au:+Salti_S/0/1/0/all/0/1">Samuele Salti</a>, <a href="http://arxiv.org/find/eess/1/au:+Sirazitdinov_I/0/1/0/all/0/1">Ilyas Sirazitdinov</a></p>
<p>Chest X-ray (CXR) is frequently employed in emergency departments and
intensive care units to verify the proper placement of central lines and tubes
and to rule out related complications. The automation of the X-ray reading
process can be a valuable support tool for non-specialist technicians and
minimize reporting delays due to non-availability of experts. While existing
solutions for automated catheter segmentation and malposition detection show
promising results, the disentanglement of individual catheters remains an open
challenge, especially in complex cases where multiple devices appear
superimposed in the X-ray projection. Moreover, conventional top-down instance
segmentation methods are ineffective on such thin and long devices, that often
extend through the entire image. In this paper, we propose a deep learning
approach based on associative embeddings for catheter instance segmentation,
able to overcome those limitations and effectively handle device intersections.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03372">Evaluating the point cloud of individual trees generated from images based on Neural Radiance fields (NeRF) method. (arXiv:2312.03372v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Hongyu Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_G/0/1/0/all/0/1">Guoji Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chongcheng Chen</a></p>
<p>Three-dimensional (3D) reconstruction of trees has always been a key task in
precision forestry management and research. Due to the complex branch
morphological structure of trees themselves and the occlusions from tree stems,
branches and foliage, it is difficult to recreate a complete three-dimensional
tree model from a two-dimensional image by conventional photogrammetric
methods. In this study, based on tree images collected by various cameras in
different ways, the Neural Radiance Fields (NeRF) method was used for
individual tree reconstruction and the exported point cloud models are compared
with point cloud derived from photogrammetric reconstruction and laser scanning
methods. The results show that the NeRF method performs well in individual tree
3D reconstruction, as it has higher successful reconstruction rate, better
reconstruction in the canopy area, it requires less amount of images as input.
Compared with photogrammetric reconstruction method, NeRF has significant
advantages in reconstruction efficiency and is adaptable to complex scenes, but
the generated point cloud tends to be noisy and low resolution. The accuracy of
tree structural parameters (tree height and diameter at breast height)
extracted from the photogrammetric point cloud is still higher than those of
derived from the NeRF point cloud. The results of this study illustrate the
great potential of NeRF method for individual tree reconstruction, and it
provides new ideas and research directions for 3D reconstruction and
visualization of complex forest scenes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03378">Riemannian Complex Matrix Convolution Network for PolSAR Image Classification. (arXiv:2312.03378v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1">Junfei Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1">Haiyan Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_M/0/1/0/all/0/1">Mengmeng Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1">Shanshan Ji</a></p>
<p>Recently, deep learning methods have achieved superior performance for
Polarimetric Synthetic Aperture Radar(PolSAR) image classification. Existing
deep learning methods learn PolSAR data by converting the covariance matrix
into a feature vector or complex-valued vector as the input. However, all these
methods cannot learn the structure of complex matrix directly and destroy the
channel correlation. To learn geometric structure of complex matrix, we propose
a Riemannian complex matrix convolution network for PolSAR image classification
in Riemannian space for the first time, which directly utilizes the complex
matrix as the network input and defines the Riemannian operations to learn
complex matrix's features. The proposed Riemannian complex matrix convolution
network considers PolSAR complex matrix endowed in Riemannian manifold, and
defines a series of new Riemannian convolution, ReLu and LogEig operations in
Riemannian space, which breaks through the Euclidean constraint of conventional
networks. Then, a CNN module is appended to enhance contextual Riemannian
features. Besides, a fast kernel learning method is developed for the proposed
method to learn class-specific features and reduce the computation time
effectively. Experiments are conducted on three sets of real PolSAR data with
different bands and sensors. Experiments results demonstrates the proposed
method can obtain superior performance than the state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03391">Action Scene Graphs for Long-Form Understanding of Egocentric Videos. (arXiv:2312.03391v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rodin_I/0/1/0/all/0/1">Ivan Rodin</a>, <a href="http://arxiv.org/find/cs/1/au:+Furnari_A/0/1/0/all/0/1">Antonino Furnari</a>, <a href="http://arxiv.org/find/cs/1/au:+Min_K/0/1/0/all/0/1">Kyle Min</a>, <a href="http://arxiv.org/find/cs/1/au:+Tripathi_S/0/1/0/all/0/1">Subarna Tripathi</a>, <a href="http://arxiv.org/find/cs/1/au:+Farinella_G/0/1/0/all/0/1">Giovanni Maria Farinella</a></p>
<p>We present Egocentric Action Scene Graphs (EASGs), a new representation for
long-form understanding of egocentric videos. EASGs extend standard
manually-annotated representations of egocentric videos, such as verb-noun
action labels, by providing a temporally evolving graph-based description of
the actions performed by the camera wearer, including interacted objects, their
relationships, and how actions unfold in time. Through a novel annotation
procedure, we extend the Ego4D dataset by adding manually labeled Egocentric
Action Scene Graphs offering a rich set of annotations designed for long-from
egocentric video understanding. We hence define the EASG generation task and
provide a baseline approach, establishing preliminary benchmarks. Experiments
on two downstream tasks, egocentric action anticipation and egocentric activity
summarization, highlight the effectiveness of EASGs for long-form egocentric
video understanding. We will release the dataset and the code to replicate
experiments and annotations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03401">Predicting Postoperative Intraocular Lens Dislocation in Cataract Surgery via Deep Learning. (arXiv:2312.03401v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ghamsarian_N/0/1/0/all/0/1">Negin Ghamsarian</a>, <a href="http://arxiv.org/find/eess/1/au:+Putzgruber_Adamitsch_D/0/1/0/all/0/1">Doris Putzgruber-Adamitsch</a>, <a href="http://arxiv.org/find/eess/1/au:+Sarny_S/0/1/0/all/0/1">Stephanie Sarny</a>, <a href="http://arxiv.org/find/eess/1/au:+Sznitman_R/0/1/0/all/0/1">Raphael Sznitman</a>, <a href="http://arxiv.org/find/eess/1/au:+Schoeffmann_K/0/1/0/all/0/1">Klaus Schoeffmann</a>, <a href="http://arxiv.org/find/eess/1/au:+El_Shabrawi_Y/0/1/0/all/0/1">Yosuf El-Shabrawi</a></p>
<p>A critical yet unpredictable complication following cataract surgery is
intraocular lens dislocation. Postoperative stability is imperative, as even a
tiny decentration of multifocal lenses or inadequate alignment of the torus in
toric lenses due to postoperative rotation can lead to a significant drop in
visual acuity. Investigating possible intraoperative indicators that can
predict post-surgical instabilities of intraocular lenses can help prevent this
complication. In this paper, we develop and evaluate the first fully-automatic
framework for the computation of lens unfolding delay, rotation, and
instability during surgery. Adopting a combination of three types of CNNs,
namely recurrent, region-based, and pixel-based, the proposed framework is
employed to assess the possibility of predicting post-operative lens
dislocation during cataract surgery. This is achieved via performing a
large-scale study on the statistical differences between the behavior of
different brands of intraocular lenses and aligning the results with expert
surgeons' hypotheses and observations about the lenses. We exploit a
large-scale dataset of cataract surgery videos featuring four intraocular lens
brands. Experimental results confirm the reliability of the proposed framework
in evaluating the lens' statistics during the surgery. The Pearson correlation
and t-test results reveal significant correlations between lens unfolding delay
and lens rotation and significant differences between the intra-operative
rotations stability of four groups of lenses. These results suggest that the
proposed framework can help surgeons select the lenses based on the patient's
eye conditions and predict post-surgical lens dislocation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03406">SVQ: Sparse Vector Quantization for Spatiotemporal Forecasting. (arXiv:2312.03406v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1">Tian Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yanjun Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Liang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1">Rong Jin</a></p>
<p>Spatiotemporal forecasting tasks, such as weather forecasting and traffic
prediction, offer significant societal benefits. These tasks can be effectively
approached as image forecasting problems using computer vision models. Vector
quantization (VQ) is a well-known method for discrete representation that
improves the latent space, leading to enhanced generalization and transfer
learning capabilities. One of the main challenges in using VQ for
spatiotemporal forecasting is how to balance between keeping enough details and
removing noises from the original patterns for better generalization. We
address this challenge by developing sparse vector quantization, or {\bf SVQ}
for short, that leverages sparse regression to make better trade-off between
the two objectives. The main innovation of this work is to approximate sparse
regression by a two-layer MLP and a randomly fixed or learnable matrix,
dramatically improving its computational efficiency. Through experiments
conducted on diverse datasets in multiple fields including weather forecasting,
traffic flow prediction, and video forecasting, we unequivocally demonstrate
that our proposed method consistently enhances the performance of base models
and achieves state-of-the-art results across all benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03408">Open-sourced Data Ecosystem in Autonomous Driving: the Present and Future. (arXiv:2312.03408v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hongyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Huijie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1">Jia Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1">Pinlong Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Huilin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1">Dahua Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1">Junchi Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1">Feng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1">Lu Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jingdong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1">Futang Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1">Kai Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Chunjing Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tiancai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mu_B/0/1/0/all/0/1">Beipeng Mu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1">Shaoqing Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1">Zhihui Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a></p>
<p>With the continuous maturation and application of autonomous driving
technology, a systematic examination of open-source autonomous driving datasets
becomes instrumental in fostering the robust evolution of the industry
ecosystem. Current autonomous driving datasets can broadly be categorized into
two generations. The first-generation autonomous driving datasets are
characterized by relatively simpler sensor modalities, smaller data scale, and
is limited to perception-level tasks. KITTI, introduced in 2012, serves as a
prominent representative of this initial wave. In contrast, the
second-generation datasets exhibit heightened complexity in sensor modalities,
greater data scale and diversity, and an expansion of tasks from perception to
encompass prediction and control. Leading examples of the second generation
include nuScenes and Waymo, introduced around 2019. This comprehensive review,
conducted in collaboration with esteemed colleagues from both academia and
industry, systematically assesses over seventy open-source autonomous driving
datasets from domestic and international sources. It offers insights into
various aspects, such as the principles underlying the creation of high-quality
datasets, the pivotal role of data engine systems, and the utilization of
generative foundation models to facilitate scalable data generation.
Furthermore, this review undertakes an exhaustive analysis and discourse
regarding the characteristics and data scales that future third-generation
autonomous driving datasets should possess. It also delves into the scientific
and technical challenges that warrant resolution. These endeavors are pivotal
in advancing autonomous innovation and fostering technological enhancement in
critical domains. For further details, please refer to
https://github.com/OpenDriveLab/DriveAGI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03409">DeepPyramid+: Medical Image Segmentation using Pyramid View Fusion and Deformable Pyramid Reception. (arXiv:2312.03409v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ghamsarian_N/0/1/0/all/0/1">Negin Ghamsarian</a>, <a href="http://arxiv.org/find/cs/1/au:+Wolf_S/0/1/0/all/0/1">Sebastian Wolf</a>, <a href="http://arxiv.org/find/cs/1/au:+Zinkernagel_M/0/1/0/all/0/1">Martin Zinkernagel</a>, <a href="http://arxiv.org/find/cs/1/au:+Schoeffmann_K/0/1/0/all/0/1">Klaus Schoeffmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Sznitman_R/0/1/0/all/0/1">Raphael Sznitman</a></p>
<p>Semantic Segmentation plays a pivotal role in many applications related to
medical image and video analysis. However, designing a neural network
architecture for medical image and surgical video segmentation is challenging
due to the diverse features of relevant classes, including heterogeneity,
deformability, transparency, blunt boundaries, and various distortions. We
propose a network architecture, DeepPyramid+, which addresses diverse
challenges encountered in medical image and surgical video segmentation. The
proposed DeepPyramid+ incorporates two major modules, namely "Pyramid View
Fusion" (PVF) and "Deformable Pyramid Reception," (DPR), to address the
outlined challenges. PVF replicates a deduction process within the neural
network, aligning with the human visual system, thereby enhancing the
representation of relative information at each pixel position. Complementarily,
DPR introduces shape- and scale-adaptive feature extraction techniques using
dilated deformable convolutions, enhancing accuracy and robustness in handling
heterogeneous classes and deformable shapes. Extensive experiments conducted on
diverse datasets, including endometriosis videos, MRI images, OCT scans, and
cataract and laparoscopy videos, demonstrate the effectiveness of DeepPyramid+
in handling various challenges such as shape and scale variation, reflection,
and blur degradation. DeepPyramid+ demonstrates significant improvements in
segmentation performance, achieving up to a 3.65% increase in Dice coefficient
for intra-domain segmentation and up to a 17% increase in Dice coefficient for
cross-domain segmentation. DeepPyramid+ consistently outperforms
state-of-the-art networks across diverse modalities considering different
backbone networks, showcasing its versatility.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03420">Artist-Friendly Relightable and Animatable Neural Heads. (arXiv:2312.03420v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yingyan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandran_P/0/1/0/all/0/1">Prashanth Chandran</a>, <a href="http://arxiv.org/find/cs/1/au:+Weiss_S/0/1/0/all/0/1">Sebastian Weiss</a>, <a href="http://arxiv.org/find/cs/1/au:+Gross_M/0/1/0/all/0/1">Markus Gross</a>, <a href="http://arxiv.org/find/cs/1/au:+Zoss_G/0/1/0/all/0/1">Gaspard Zoss</a>, <a href="http://arxiv.org/find/cs/1/au:+Bradley_D/0/1/0/all/0/1">Derek Bradley</a></p>
<p>An increasingly common approach for creating photo-realistic digital avatars
is through the use of volumetric neural fields. The original neural radiance
field (NeRF) allowed for impressive novel view synthesis of static heads when
trained on a set of multi-view images, and follow up methods showed that these
neural representations can be extended to dynamic avatars. Recently, new
variants also surpassed the usual drawback of baked-in illumination in neural
representations, showing that static neural avatars can be relit in any
environment. In this work we simultaneously tackle both the motion and
illumination problem, proposing a new method for relightable and animatable
neural heads. Our method builds on a proven dynamic avatar approach based on a
mixture of volumetric primitives, combined with a recently-proposed lightweight
hardware setup for relightable neural fields, and includes a novel architecture
that allows relighting dynamic neural avatars performing unseen expressions in
any environment, even with nearfield illumination and viewpoints.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03430">ShareCMP: Polarization-Aware RGB-P Semantic Segmentation. (arXiv:2312.03430v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhuoyan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lizhi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_C/0/1/0/all/0/1">Chenyu Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Ye Li</a></p>
<p>Multimodal semantic segmentation is developing rapidly, but the modality of
RGB-Polarization remains underexplored. To delve into this problem, we
construct a UPLight RGB-P segmentation benchmark with 12 typical underwater
semantic classes which provides data support for Autonomous Underwater Vehicles
(AUVs) to perform special perception tasks. In this work, we design the
ShareCMP, an RGB-P semantic segmentation framework with a shared dual-branch
architecture, which reduces the number of parameters by about 26-33% compared
to previous dual-branch models. It encompasses a Polarization Generate
Attention (PGA) module designed to generate polarization modal images with
richer polarization properties for the encoder. In addition, we introduce the
Class Polarization-Aware Loss (CPALoss) to improve the learning and
understanding of the encoder for polarization modal information and to optimize
the PGA module. With extensive experiments on a total of three RGB-P
benchmarks, our ShareCMP achieves state-of-the-art performance in mIoU with
fewer parameters on the UPLight (92.45%), ZJU (92.7%), and MCubeS (50.99%)
datasets. The code is available at https://github.com/LEFTeyex/ShareCMP.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03431">Gaussian-Flow: 4D Reconstruction with Dynamic 3D Gaussian Particle. (arXiv:2312.03431v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Youtian Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1">Zuozhuo Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1">Siyu Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1">Yao Yao</a></p>
<p>We introduce Gaussian-Flow, a novel point-based approach for fast dynamic
scene reconstruction and real-time rendering from both multi-view and monocular
videos. In contrast to the prevalent NeRF-based approaches hampered by slow
training and rendering speeds, our approach harnesses recent advancements in
point-based 3D Gaussian Splatting (3DGS). Specifically, a novel Dual-Domain
Deformation Model (DDDM) is proposed to explicitly model attribute deformations
of each Gaussian point, where the time-dependent residual of each attribute is
captured by a polynomial fitting in the time domain, and a Fourier series
fitting in the frequency domain. The proposed DDDM is capable of modeling
complex scene deformations across long video footage, eliminating the need for
training separate 3DGS for each frame or introducing an additional implicit
neural field to model 3D dynamics. Moreover, the explicit deformation modeling
for discretized Gaussian points ensures ultra-fast training and rendering of a
4D scene, which is comparable to the original 3DGS designed for static 3D
reconstruction. Our proposed approach showcases a substantial efficiency
improvement, achieving a $5\times$ faster training speed compared to the
per-frame 3DGS modeling. In addition, quantitative results demonstrate that the
proposed Gaussian-Flow significantly outperforms previous leading methods in
novel view rendering quality. Project page:
https://nju-3dv.github.io/projects/Gaussian-Flow
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03437">Data-Centric Digital Agriculture: A Perspective. (arXiv:2312.03437v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Roscher_R/0/1/0/all/0/1">Ribana Roscher</a>, <a href="http://arxiv.org/find/cs/1/au:+Roth_L/0/1/0/all/0/1">Lukas Roth</a>, <a href="http://arxiv.org/find/cs/1/au:+Stachniss_C/0/1/0/all/0/1">Cyrill Stachniss</a>, <a href="http://arxiv.org/find/cs/1/au:+Walter_A/0/1/0/all/0/1">Achim Walter</a></p>
<p>In response to the increasing global demand for food, feed, fiber, and fuel,
digital agriculture is rapidly evolving to meet these demands while reducing
environmental impact. This evolution involves incorporating data science,
machine learning, sensor technologies, robotics, and new management strategies
to establish a more sustainable agricultural framework. So far, machine
learning research in digital agriculture has predominantly focused on
model-centric approaches, focusing on model design and evaluation. These
efforts aim to optimize model accuracy and efficiency, often treating data as a
static benchmark. Despite the availability of agricultural data and
methodological advancements, a saturation point has been reached, with many
established machine learning methods achieving comparable levels of accuracy
and facing similar limitations. To fully realize the potential of digital
agriculture, it is crucial to have a comprehensive understanding of the role of
data in the field and to adopt data-centric machine learning. This involves
developing strategies to acquire and curate valuable data and implementing
effective learning and evaluation strategies that utilize the intrinsic value
of data. This approach has the potential to create accurate, generalizable, and
adaptable machine learning methods that effectively and sustainably address
agricultural tasks such as yield prediction, weed detection, and early disease
identification
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03441">UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity. (arXiv:2312.03441v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zuo_J/0/1/0/all/0/1">Jialong Zuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Hanyu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1">Ying Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1">Feng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1">Tianyu Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Sang_N/0/1/0/all/0/1">Nong Sang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yunhe Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1">Changxin Gao</a></p>
<p>Existing text-based person retrieval datasets often have relatively
coarse-grained text annotations. This hinders the model to comprehend the
fine-grained semantics of query texts in real scenarios. To address this
problem, we contribute a new benchmark named \textbf{UFineBench} for text-based
person retrieval with ultra-fine granularity.
</p>
<p>Firstly, we construct a new \textbf{dataset} named UFine6926. We collect a
large number of person images and manually annotate each image with two
detailed textual descriptions, averaging 80.8 words each. The average word
count is three to four times that of the previous datasets. In addition of
standard in-domain evaluation, we also propose a special \textbf{evaluation
paradigm} more representative of real scenarios. It contains a new evaluation
set with cross domains, cross textual granularity and cross textual styles,
named UFine3C, and a new evaluation metric for accurately measuring retrieval
ability, named mean Similarity Distribution (mSD). Moreover, we propose CFAM, a
more efficient \textbf{algorithm} especially designed for text-based person
retrieval with ultra fine-grained texts. It achieves fine granularity mining by
adopting a shared cross-modal granularity decoder and hard negative match
mechanism.
</p>
<p>With standard in-domain evaluation, CFAM establishes competitive performance
across various datasets, especially on our ultra fine-grained UFine6926.
Furthermore, by evaluating on UFine3C, we demonstrate that training on our
UFine6926 significantly improves generalization to real scenarios compared with
other coarse-grained datasets. The dataset and code will be made publicly
available at \url{https://github.com/Zplusdragon/UFineBench}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03442">High-Quality Facial Geometry and Appearance Capture at Home. (arXiv:2312.03442v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1">Yuxuan Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1">Junfeng Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1">Feng Xu</a></p>
<p>Facial geometry and appearance capture have demonstrated tremendous success
in 3D scanning real humans in studios. Recent works propose to democratize this
technique while keeping the results high quality. However, they are still
inconvenient for daily usage. In addition, they focus on an easier problem of
only capturing facial skin. This paper proposes a novel method for high-quality
face capture, featuring an easy-to-use system and the capability to model the
complete face with skin, mouth interior, hair, and eyes. We reconstruct facial
geometry and appearance from a single co-located smartphone flashlight sequence
captured in a dim room where the flashlight is the dominant light source (e.g.
rooms with curtains or at night). To model the complete face, we propose a
novel hybrid representation to effectively model both eyes and other facial
regions, along with novel techniques to learn it from images. We apply a
combined lighting model to compactly represent real illuminations and exploit a
morphable face albedo model as a reflectance prior to disentangle diffuse and
specular. Experiments show that our method can capture high-quality 3D
relightable scans.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03443">Data-driven Crop Growth Simulation on Time-varying Generated Images using Multi-conditional Generative Adversarial Networks. (arXiv:2312.03443v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Drees_L/0/1/0/all/0/1">Lukas Drees</a>, <a href="http://arxiv.org/find/cs/1/au:+Demie_D/0/1/0/all/0/1">Dereje T. Demie</a>, <a href="http://arxiv.org/find/cs/1/au:+Paul_M/0/1/0/all/0/1">Madhuri R. Paul</a>, <a href="http://arxiv.org/find/cs/1/au:+Leonhardt_J/0/1/0/all/0/1">Johannes Leonhardt</a>, <a href="http://arxiv.org/find/cs/1/au:+Seidel_S/0/1/0/all/0/1">Sabine J. Seidel</a>, <a href="http://arxiv.org/find/cs/1/au:+Doring_T/0/1/0/all/0/1">Thomas F. D&#xf6;ring</a>, <a href="http://arxiv.org/find/cs/1/au:+Roscher_R/0/1/0/all/0/1">Ribana Roscher</a></p>
<p>Image-based crop growth modeling can substantially contribute to precision
agriculture by revealing spatial crop development over time, which allows an
early and location-specific estimation of relevant future plant traits, such as
leaf area or biomass. A prerequisite for realistic and sharp crop image
generation is the integration of multiple growth-influencing conditions in a
model, such as an image of an initial growth stage, the associated growth time,
and further information about the field treatment. We present a two-stage
framework consisting first of an image prediction model and second of a growth
estimation model, which both are independently trained. The image prediction
model is a conditional Wasserstein generative adversarial network (CWGAN). In
the generator of this model, conditional batch normalization (CBN) is used to
integrate different conditions along with the input image. This allows the
model to generate time-varying artificial images dependent on multiple
influencing factors of different kinds. These images are used by the second
part of the framework for plant phenotyping by deriving plant-specific traits
and comparing them with those of non-artificial (real) reference images. For
various crop datasets, the framework allows realistic, sharp image predictions
with a slight loss of quality from short-term to long-term predictions.
Simulations of varying growth-influencing conditions performed with the trained
framework provide valuable insights into how such factors relate to crop
appearances, which is particularly useful in complex, less explored crop
mixture systems. Further results show that adding process-based simulated
biomass as a condition increases the accuracy of the derived phenotypic traits
from the predicted images. This demonstrates the potential of our framework to
serve as an interface between an image- and process-based crop growth model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03447">Quantum-Inspired Neural Network Model of Optical Illusions. (arXiv:2312.03447v1 [physics.soc-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Maksymov_I/0/1/0/all/0/1">Ivan S. Maksymov</a></p>
<p>Ambiguous optical illusions have been a paradigmatic object of fascination,
research and inspiration in arts, psychology and video games. However, accurate
computational models of perception of ambiguous figures have been elusive. In
this paper, we design and train a deep neural network model to simulate the
human's perception of the Necker cube, an ambiguous drawing with several
alternating possible interpretations. Defining the weights of the neural
network connection using a quantum generator of truly random numbers, in
agreement with the emerging concepts of quantum artificial intelligence and
quantum cognition we reveal that the actual perceptual state of the Necker cube
is a qubit-like superposition of the two fundamental perceptual states
predicted by classical theories. Our results will find applications in video
games and virtual reality systems employed for training of astronauts and
operators of unmanned aerial vehicles. They will also be useful for researchers
working in the fields of machine learning and vision, psychology of perception
and quantum-mechanical models of human mind and decision-making.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03455">Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data. (arXiv:2312.03455v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Namgyal_T/0/1/0/all/0/1">Tashi Namgyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Hepburn_A/0/1/0/all/0/1">Alexander Hepburn</a>, <a href="http://arxiv.org/find/cs/1/au:+Santos_Rodriguez_R/0/1/0/all/0/1">Raul Santos-Rodriguez</a>, <a href="http://arxiv.org/find/cs/1/au:+Laparra_V/0/1/0/all/0/1">Valero Laparra</a>, <a href="http://arxiv.org/find/cs/1/au:+Malo_J/0/1/0/all/0/1">Jesus Malo</a></p>
<p>Perceptual metrics are traditionally used to evaluate the quality of natural
signals, such as images and audio. They are designed to mimic the perceptual
behaviour of human observers and usually reflect structures found in natural
signals. This motivates their use as loss functions for training generative
models such that models will learn to capture the structure held in the metric.
We take this idea to the extreme in the audio domain by training a compressive
autoencoder to reconstruct uniform noise, in lieu of natural data. We show that
training with perceptual losses improves the reconstruction of spectrograms and
re-synthesized audio at test time over models trained with a standard Euclidean
loss. This demonstrates better generalisation to unseen natural signals when
using perceptual metrics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03459">F3-Pruning: A Training-Free and Generalized Pruning Strategy towards Faster and Finer Text-to-Video Synthesis. (arXiv:2312.03459v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1">Sitong Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jianzhi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1">Lianli Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1">Jingkuan Song</a></p>
<p>Recently Text-to-Video (T2V) synthesis has undergone a breakthrough by
training transformers or diffusion models on large-scale datasets.
Nevertheless, inferring such large models incurs huge costs.Previous inference
acceleration works either require costly retraining or are model-specific.To
address this issue, instead of retraining we explore the inference process of
two mainstream T2V models using transformers and diffusion models.The
exploration reveals the redundancy in temporal attention modules of both
models, which are commonly utilized to establish temporal relations among
frames.Consequently, we propose a training-free and generalized pruning
strategy called F3-Pruning to prune redundant temporal attention
weights.Specifically, when aggregate temporal attention values are ranked below
a certain ratio, corresponding weights will be pruned.Extensive experiments on
three datasets using a classic transformer-based model CogVideo and a typical
diffusion-based model Tune-A-Video verify the effectiveness of F3-Pruning in
inference acceleration, quality assurance and broad applicability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03461">HiFi4G: High-Fidelity Human Performance Rendering via Compact Gaussian Splatting. (arXiv:2312.03461v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yuheng Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1">Zhehao Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Penghao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1">Zhuo Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1">Yu Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yingliang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Jingyi Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1">Lan Xu</a></p>
<p>We have recently seen tremendous progress in photo-real human modeling and
rendering. Yet, efficiently rendering realistic human performance and
integrating it into the rasterization pipeline remains challenging. In this
paper, we present HiFi4G, an explicit and compact Gaussian-based approach for
high-fidelity human performance rendering from dense footage. Our core
intuition is to marry the 3D Gaussian representation with non-rigid tracking,
achieving a compact and compression-friendly representation. We first propose a
dual-graph mechanism to obtain motion priors, with a coarse deformation graph
for effective initialization and a fine-grained Gaussian graph to enforce
subsequent constraints. Then, we utilize a 4D Gaussian optimization scheme with
adaptive spatial-temporal regularizers to effectively balance the non-rigid
prior and Gaussian updating. We also present a companion compression scheme
with residual compensation for immersive experiences on various platforms. It
achieves a substantial compression rate of approximately 25 times, with less
than 2MB of storage per frame. Extensive experiments demonstrate the
effectiveness of our approach, which significantly outperforms existing
approaches in terms of optimization speed, rendering quality, and storage
overhead.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03477">From Detection to Action Recognition: An Edge-Based Pipeline for Robot Human Perception. (arXiv:2312.03477v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Toupas_P/0/1/0/all/0/1">Petros Toupas</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsamis_G/0/1/0/all/0/1">Georgios Tsamis</a>, <a href="http://arxiv.org/find/cs/1/au:+Giakoumis_D/0/1/0/all/0/1">Dimitrios Giakoumis</a>, <a href="http://arxiv.org/find/cs/1/au:+Votis_K/0/1/0/all/0/1">Konstantinos Votis</a>, <a href="http://arxiv.org/find/cs/1/au:+Tzovaras_D/0/1/0/all/0/1">Dimitrios Tzovaras</a></p>
<p>Mobile service robots are proving to be increasingly effective in a range of
applications, such as healthcare, monitoring Activities of Daily Living (ADL),
and facilitating Ambient Assisted Living (AAL). These robots heavily rely on
Human Action Recognition (HAR) to interpret human actions and intentions.
However, for HAR to function effectively on service robots, it requires prior
knowledge of human presence (human detection) and identification of individuals
to monitor (human tracking). In this work, we propose an end-to-end pipeline
that encompasses the entire process, starting from human detection and
tracking, leading to action recognition. The pipeline is designed to operate in
near real-time while ensuring all stages of processing are performed on the
edge, reducing the need for centralised computation. To identify the most
suitable models for our mobile robot, we conducted a series of experiments
comparing state-of-the-art solutions based on both their detection performance
and efficiency. To evaluate the effectiveness of our proposed pipeline, we
proposed a dataset comprising daily household activities. By presenting our
findings and analysing the results, we demonstrate the efficacy of our approach
in enabling mobile robots to understand and respond to human behaviour in
real-world scenarios relying mainly on the data from their RGB cameras.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03490">PneumoLLM: Harnessing the Power of Large Language Model for Pneumoconiosis Diagnosis. (arXiv:2312.03490v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Song_M/0/1/0/all/0/1">Meiyue Song</a>, <a href="http://arxiv.org/find/eess/1/au:+Yu_Z/0/1/0/all/0/1">Zhihua Yu</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1">Jiaxin Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1">Jiarui Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Lu_Y/0/1/0/all/0/1">Yuting Lu</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1">Baicun Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1">Xiaoxu Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Huang_Q/0/1/0/all/0/1">Qinghua Huang</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1">Zhijun Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Kanellakis_N/0/1/0/all/0/1">Nikolaos I.Kanellakis</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1">Jiangfeng Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1">Jing Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_B/0/1/0/all/0/1">Binglu Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1">Juntao Yang</a></p>
<p>The conventional pretraining-and-finetuning paradigm, while effective for
common diseases with ample data, faces challenges in diagnosing data-scarce
occupational diseases like pneumoconiosis. Recently, large language models
(LLMs) have exhibits unprecedented ability when conducting multiple tasks in
dialogue, bringing opportunities to diagnosis. A common strategy might involve
using adapter layers for vision-language alignment and diagnosis in a dialogic
manner. Yet, this approach often requires optimization of extensive learnable
parameters in the text branch and the dialogue head, potentially diminishing
the LLMs' efficacy, especially with limited training data. In our work, we
innovate by eliminating the text branch and substituting the dialogue head with
a classification head. This approach presents a more effective method for
harnessing LLMs in diagnosis with fewer learnable parameters. Furthermore, to
balance the retention of detailed image information with progression towards
accurate diagnosis, we introduce the contextual multi-token engine. This engine
is specialized in adaptively generating diagnostic tokens. Additionally, we
propose the information emitter module, which unidirectionally emits
information from image tokens to diagnosis tokens. Comprehensive experiments
validate the superiority of our methods and the effectiveness of proposed
modules. Our codes can be found at
https://github.com/CodeMonsterPHD/PneumoLLM/tree/main.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03502">Improving the Generalization of Segmentation Foundation Model under Distribution Shift via Weakly Supervised Adaptation. (arXiv:2312.03502v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haojie Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1">Yongyi Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xun Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1">Kui Jia</a></p>
<p>The success of large language models has inspired the computer vision
community to explore image segmentation foundation model that is able to
zero/few-shot generalize through prompt engineering. Segment-Anything(SAM),
among others, is the state-of-the-art image segmentation foundation model
demonstrating strong zero/few-shot generalization. Despite the success, recent
studies reveal the weakness of SAM under strong distribution shift. In
particular, SAM performs awkwardly on corrupted natural images, camouflaged
images, medical images, etc. Motivated by the observations, we aim to develop a
self-training based strategy to adapt SAM to target distribution. Given the
unique challenges of large source dataset, high computation cost and incorrect
pseudo label, we propose a weakly supervised self-training architecture with
anchor regularization and low-rank finetuning to improve the robustness and
computation efficiency of adaptation. We validate the effectiveness on 5 types
of downstream segmentation tasks including natural clean/corrupted images,
medical images, camouflaged images and robotic images. Our proposed method is
task-agnostic in nature and outperforms pre-trained SAM and state-of-the-art
domain adaptation methods on almost all downstream tasks with the same testing
prompt inputs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03509">Gravitational cell detection and tracking in fluorescence microscopy data. (arXiv:2312.03509v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Eftimiu_N/0/1/0/all/0/1">Nikomidisz Eftimiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kozubek_M/0/1/0/all/0/1">Michal Kozubek</a></p>
<p>Automatic detection and tracking of cells in microscopy images are major
applications of computer vision technologies in both biomedical research and
clinical practice. Though machine learning methods are increasingly common in
these fields, classical algorithms still offer significant advantages for both
tasks, including better explainability, faster computation, lower hardware
requirements and more consistent performance. In this paper, we present a novel
approach based on gravitational force fields that can compete with, and
potentially outperform modern machine learning models when applied to
fluorescence microscopy images. This method includes detection, segmentation,
and tracking elements, with the results demonstrated on a Cell Tracking
Challenge dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03511">Kandinsky 3.0 Technical Report. (arXiv:2312.03511v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Arkhipkin_V/0/1/0/all/0/1">Vladimir Arkhipkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Filatov_A/0/1/0/all/0/1">Andrei Filatov</a>, <a href="http://arxiv.org/find/cs/1/au:+Vasilev_V/0/1/0/all/0/1">Viacheslav Vasilev</a>, <a href="http://arxiv.org/find/cs/1/au:+Maltseva_A/0/1/0/all/0/1">Anastasia Maltseva</a>, <a href="http://arxiv.org/find/cs/1/au:+Azizov_S/0/1/0/all/0/1">Said Azizov</a>, <a href="http://arxiv.org/find/cs/1/au:+Pavlov_I/0/1/0/all/0/1">Igor Pavlov</a>, <a href="http://arxiv.org/find/cs/1/au:+Agafonova_J/0/1/0/all/0/1">Julia Agafonova</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuznetsov_A/0/1/0/all/0/1">Andrey Kuznetsov</a>, <a href="http://arxiv.org/find/cs/1/au:+Dimitrov_D/0/1/0/all/0/1">Denis Dimitrov</a></p>
<p>We present Kandinsky 3.0, a large-scale text-to-image generation model based
on latent diffusion, continuing the series of text-to-image Kandinsky models
and reflecting our progress to achieve higher quality and realism of image
generation. Compared to previous versions of Kandinsky 2.x, Kandinsky 3.0
leverages a two times larger U-Net backbone, a ten times larger text encoder
and removes diffusion mapping. We describe the architecture of the model, the
data collection procedure, the training technique, and the production system of
user interaction. We focus on the key components that, as we have identified as
a result of a large number of experiments, had the most significant impact on
improving the quality of our model compared to the others. By our side-by-side
comparisons, Kandinsky becomes better in text understanding and works better on
specific domains. Project page: https://ai-forever.github.io/Kandinsky-3
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03517">FRDiff: Feature Reuse for Exquisite Zero-shot Acceleration of Diffusion Models. (arXiv:2312.03517v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+So_J/0/1/0/all/0/1">Junhyuk So</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jungwon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_E/0/1/0/all/0/1">Eunhyeok Park</a></p>
<p>The substantial computational costs of diffusion models, particularly due to
the repeated denoising steps crucial for high-quality image generation, present
a major obstacle to their widespread adoption. While several studies have
attempted to address this issue by reducing the number of score function
evaluations using advanced ODE solvers without fine-tuning, the decreased
number of denoising iterations misses the opportunity to update fine details,
resulting in noticeable quality degradation. In our work, we introduce an
advanced acceleration technique that leverages the temporal redundancy inherent
in diffusion models. Reusing feature maps with high temporal similarity opens
up a new opportunity to save computation without sacrificing output quality. To
realize the practical benefits of this intuition, we conduct an extensive
analysis and propose a novel method, FRDiff. FRDiff is designed to harness the
advantages of both reduced NFE and feature reuse, achieving a Pareto frontier
that balances fidelity and latency trade-offs in various generative tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03519">Active Wildfires Detection and Dynamic Escape Routes Planning for Humans through Information Fusion between Drones and Satellites. (arXiv:2312.03519v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sziranyi_T/0/1/0/all/0/1">Tamas Sziranyi</a></p>
<p>UAVs are playing an increasingly important role in the field of wilderness
rescue by virtue of their flexibility. This paper proposes a fusion of UAV
vision technology and satellite image analysis technology for active wildfires
detection and road networks extraction of wildfire areas and real-time dynamic
escape route planning for people in distress. Firstly, the fire source location
and the segmentation of smoke and flames are targeted based on Sentinel 2
satellite imagery. Secondly, the road segmentation and the road condition
assessment are performed by D-linkNet and NDVI values in the central area of
the fire source by UAV. Finally, the dynamic optimal route planning for humans
in real time is performed by the weighted A* algorithm in the road network with
the dynamic fire spread model. Taking the Chongqing wildfire on August 24,
2022, as a case study, the results demonstrate that the dynamic escape route
planning algorithm can provide an optimal real-time navigation path for humans
in the presence of fire through the information fusion of UAVs and satellites.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03520">Defense Against Adversarial Attacks using Convolutional Auto-Encoders. (arXiv:2312.03520v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mandal_S/0/1/0/all/0/1">Shreyasi Mandal</a></p>
<p>Deep learning models, while achieving state-of-the-art performance on many
tasks, are susceptible to adversarial attacks that exploit inherent
vulnerabilities in their architectures. Adversarial attacks manipulate the
input data with imperceptible perturbations, causing the model to misclassify
the data or produce erroneous outputs. This work is based on enhancing the
robustness of targeted classifier models against adversarial attacks. To
achieve this, an convolutional autoencoder-based approach is employed that
effectively counters adversarial perturbations introduced to the input images.
By generating images closely resembling the input images, the proposed
methodology aims to restore the model's accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03526">On the Diversity and Realism of Distilled Dataset: An Efficient Dataset Distillation Paradigm. (arXiv:2312.03526v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1">Peng Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1">Bei Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1">Daiwei Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1">Tao Lin</a></p>
<p>Contemporary machine learning requires training large neural networks on
massive datasets and thus faces the challenges of high computational demands.
Dataset distillation, as a recent emerging strategy, aims to compress
real-world datasets for efficient training. However, this line of research
currently struggle with large-scale and high-resolution datasets, hindering its
practicality and feasibility. To this end, we re-examine the existing dataset
distillation methods and identify three properties required for large-scale
real-world applications, namely, realism, diversity, and efficiency. As a
remedy, we propose RDED, a novel computationally-efficient yet effective data
distillation paradigm, to enable both diversity and realism of the distilled
data. Extensive empirical results over various neural architectures and
datasets demonstrate the advancement of RDED: we can distill the full
ImageNet-1K to a small dataset comprising 10 images per class within 7 minutes,
achieving a notable 42% top-1 accuracy with ResNet-18 on a single RTX-4090 GPU
(while the SOTA only achieves 21% but requires 6 hours).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03528">Personalized Pose Forecasting. (arXiv:2312.03528v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Priisalu_M/0/1/0/all/0/1">Maria Priisalu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kronvall_T/0/1/0/all/0/1">Ted Kronvall</a>, <a href="http://arxiv.org/find/cs/1/au:+Sminchisescu_C/0/1/0/all/0/1">Cristian Sminchisescu</a></p>
<p>Human pose forecasting is the task of predicting articulated human motion
given past human motion. There exists a number of popular benchmarks that
evaluate an array of different models performing human pose forecasting. These
benchmarks do not reflect that a human interacting system, such as a delivery
robot, observes and plans for the motion of the same individual over an
extended period of time. Every individual has unique and distinct movement
patterns. This is however not reflected in existing benchmarks that evaluate a
model's ability to predict an average human's motion rather than a particular
individual's. We reformulate the human motion forecasting problem and present a
model-agnostic personalization method. Motion forecasting personalization can
be performed efficiently online by utilizing a low-parametric time-series
analysis model that personalizes neural network pose predictions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03533">Low-shot Object Learning with Mutual Exclusivity Bias. (arXiv:2312.03533v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Thai_A/0/1/0/all/0/1">Anh Thai</a>, <a href="http://arxiv.org/find/cs/1/au:+Humayun_A/0/1/0/all/0/1">Ahmad Humayun</a>, <a href="http://arxiv.org/find/cs/1/au:+Stojanov_S/0/1/0/all/0/1">Stefan Stojanov</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zixuan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Boote_B/0/1/0/all/0/1">Bikram Boote</a>, <a href="http://arxiv.org/find/cs/1/au:+Rehg_J/0/1/0/all/0/1">James M. Rehg</a></p>
<p>This paper introduces Low-shot Object Learning with Mutual Exclusivity Bias
(LSME), the first computational framing of mutual exclusivity bias, a
phenomenon commonly observed in infants during word learning. We provide a
novel dataset, comprehensive baselines, and a state-of-the-art method to enable
the ML community to tackle this challenging learning task. The goal of LSME is
to analyze an RGB image of a scene containing multiple objects and correctly
associate a previously-unknown object instance with a provided category label.
This association is then used to perform low-shot learning to test category
generalization. We provide a data generation pipeline for the LSME problem and
conduct a thorough analysis of the factors that contribute to its difficulty.
Additionally, we evaluate the performance of multiple baselines, including
state-of-the-art foundation models. Finally, we present a baseline approach
that outperforms state-of-the-art models in terms of low-shot accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03540">FoodFusion: A Latent Diffusion Model for Realistic Food Image Generation. (arXiv:2312.03540v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Markham_O/0/1/0/all/0/1">Olivia Markham</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuhao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tai_C/0/1/0/all/0/1">Chi-en Amy Tai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1">Alexander Wong</a></p>
<p>Current state-of-the-art image generation models such as Latent Diffusion
Models (LDMs) have demonstrated the capacity to produce visually striking
food-related images. However, these generated images often exhibit an artistic
or surreal quality that diverges from the authenticity of real-world food
representations. This inadequacy renders them impractical for applications
requiring realistic food imagery, such as training models for image-based
dietary assessment. To address these limitations, we introduce FoodFusion, a
Latent Diffusion model engineered specifically for the faithful synthesis of
realistic food images from textual descriptions. The development of the
FoodFusion model involves harnessing an extensive array of open-source food
datasets, resulting in over 300,000 curated image-caption pairs. Additionally,
we propose and employ two distinct data cleaning methodologies to ensure that
the resulting image-text pairs maintain both realism and accuracy. The
FoodFusion model, thus trained, demonstrates a remarkable ability to generate
food images that exhibit a significant improvement in terms of both realism and
diversity over the publicly available image generation models. We openly share
the dataset and fine-tuned models to support advancements in this critical
field of food image synthesis at https://bit.ly/genai4good.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03543">GPT-4 Enhanced Multimodal Grounding for Autonomous Driving: Leveraging Cross-Modal Attention with Large Language Models. (arXiv:2312.03543v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1">Haicheng Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1">Huanming Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhenning Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chengyue Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1">Guofa Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Bie_Y/0/1/0/all/0/1">Yiming Bie</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Chengzhong Xu</a></p>
<p>In the field of autonomous vehicles (AVs), accurately discerning commander
intent and executing linguistic commands within a visual context presents a
significant challenge. This paper introduces a sophisticated encoder-decoder
framework, developed to address visual grounding in AVs.Our Context-Aware
Visual Grounding (CAVG) model is an advanced system that integrates five core
encoders-Text, Image, Context, and Cross-Modal-with a Multimodal decoder. This
integration enables the CAVG model to adeptly capture contextual semantics and
to learn human emotional features, augmented by state-of-the-art Large Language
Models (LLMs) including GPT-4. The architecture of CAVG is reinforced by the
implementation of multi-head cross-modal attention mechanisms and a
Region-Specific Dynamic (RSD) layer for attention modulation. This
architectural design enables the model to efficiently process and interpret a
range of cross-modal inputs, yielding a comprehensive understanding of the
correlation between verbal commands and corresponding visual scenes. Empirical
evaluations on the Talk2Car dataset, a real-world benchmark, demonstrate that
CAVG establishes new standards in prediction accuracy and operational
efficiency. Notably, the model exhibits exceptional performance even with
limited training data, ranging from 50% to 75% of the full dataset. This
feature highlights its effectiveness and potential for deployment in practical
AV applications. Moreover, CAVG has shown remarkable robustness and
adaptability in challenging scenarios, including long-text command
interpretation, low-light conditions, ambiguous command contexts, inclement
weather conditions, and densely populated urban environments. The code for the
proposed model is available at our Github.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03548">Texture-Semantic Collaboration Network for ORSI Salient Object Detection. (arXiv:2312.03548v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1">Gongyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_Z/0/1/0/all/0/1">Zhen Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhi Liu</a></p>
<p>Salient object detection (SOD) in optical remote sensing images (ORSIs) has
become increasingly popular recently. Due to the characteristics of ORSIs,
ORSI-SOD is full of challenges, such as multiple objects, small objects, low
illuminations, and irregular shapes. To address these challenges, we propose a
concise yet effective Texture-Semantic Collaboration Network (TSCNet) to
explore the collaboration of texture cues and semantic cues for ORSI-SOD.
Specifically, TSCNet is based on the generic encoder-decoder structure. In
addition to the encoder and decoder, TSCNet includes a vital Texture-Semantic
Collaboration Module (TSCM), which performs valuable feature modulation and
interaction on basic features extracted from the encoder. The main idea of our
TSCM is to make full use of the texture features at the lowest level and the
semantic features at the highest level to achieve the expression enhancement of
salient regions on features. In the TSCM, we first enhance the position of
potential salient regions using semantic features. Then, we render and restore
the object details using the texture features. Meanwhile, we also perceive
regions of various scales, and construct interactions between different
regions. Thanks to the perfect combination of TSCM and generic structure, our
TSCNet can take care of both the position and details of salient objects,
effectively handling various scenes. Extensive experiments on three datasets
demonstrate that our TSCNet achieves competitive performance compared to 14
state-of-the-art methods. The code and results of our method are available at
https://github.com/MathLee/TSCNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03556">Personalized Face Inpainting with Diffusion Models by Parallel Visual Attention. (arXiv:2312.03556v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jianjin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Motamed_S/0/1/0/all/0/1">Saman Motamed</a>, <a href="http://arxiv.org/find/cs/1/au:+Vaddamanu_P/0/1/0/all/0/1">Praneetha Vaddamanu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1">Chen Henry Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Haene_C/0/1/0/all/0/1">Christian Haene</a>, <a href="http://arxiv.org/find/cs/1/au:+Bazin_J/0/1/0/all/0/1">Jean-Charles Bazin</a>, <a href="http://arxiv.org/find/cs/1/au:+Torre_F/0/1/0/all/0/1">Fernando de la Torre</a></p>
<p>Face inpainting is important in various applications, such as photo
restoration, image editing, and virtual reality. Despite the significant
advances in face generative models, ensuring that a person's unique facial
identity is maintained during the inpainting process is still an elusive goal.
Current state-of-the-art techniques, exemplified by MyStyle, necessitate
resource-intensive fine-tuning and a substantial number of images for each new
identity. Furthermore, existing methods often fall short in accommodating
user-specified semantic attributes, such as beard or expression. To improve
inpainting results, and reduce the computational complexity during inference,
this paper proposes the use of Parallel Visual Attention (PVA) in conjunction
with diffusion models. Specifically, we insert parallel attention matrices to
each cross-attention module in the denoising network, which attends to features
extracted from reference images by an identity encoder. We train the added
attention modules and identity encoder on CelebAHQ-IDI, a dataset proposed for
identity-preserving face inpainting. Experiments demonstrate that PVA attains
unparalleled identity resemblance in both face inpainting and face inpainting
with language guidance tasks, in comparison to various benchmarks, including
MyStyle, Paint by Example, and Custom Diffusion. Our findings reveal that PVA
ensures good identity preservation while offering effective
language-controllability. Additionally, in contrast to Custom Diffusion, PVA
requires just 40 fine-tuning steps for each new identity, which translates to a
significant speed increase of over 20 times.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03558">When an Image is Worth 1,024 x 1,024 Words: A Case Study in Computational Pathology. (arXiv:2312.03558v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenhui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1">Shuming Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hanwen Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1">Naoto Usuyama</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1">Jiayu Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1">Hoifung Poon</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1">Furu Wei</a></p>
<p>This technical report presents LongViT, a vision Transformer that can process
gigapixel images in an end-to-end manner. Specifically, we split the gigapixel
image into a sequence of millions of patches and project them linearly into
embeddings. LongNet is then employed to model the extremely long sequence,
generating representations that capture both short-range and long-range
dependencies. The linear computation complexity of LongNet, along with its
distributed algorithm, enables us to overcome the constraints of both
computation and memory. We apply LongViT in the field of computational
pathology, aiming for cancer diagnosis and prognosis within gigapixel
whole-slide images. Experimental results demonstrate that LongViT effectively
encodes gigapixel images and outperforms previous state-of-the-art methods on
cancer subtyping and survival prediction. Code and models will be available at
https://aka.ms/LongViT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03562">Enhancing Kinship Verification through Multiscale Retinex and Combined Deep-Shallow features. (arXiv:2312.03562v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Belabbaci_E/0/1/0/all/0/1">El Ouanas Belabbaci</a>, <a href="http://arxiv.org/find/cs/1/au:+Khammari_M/0/1/0/all/0/1">Mohammed Khammari</a>, <a href="http://arxiv.org/find/cs/1/au:+Chouchane_A/0/1/0/all/0/1">Ammar Chouchane</a>, <a href="http://arxiv.org/find/cs/1/au:+Bessaoudi_M/0/1/0/all/0/1">Mohcene Bessaoudi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ouamane_A/0/1/0/all/0/1">Abdelmalik Ouamane</a>, <a href="http://arxiv.org/find/cs/1/au:+Himeur_Y/0/1/0/all/0/1">Yassine Himeur</a>, <a href="http://arxiv.org/find/cs/1/au:+Atalla_S/0/1/0/all/0/1">Shadi Atalla</a>, <a href="http://arxiv.org/find/cs/1/au:+Mansoor_W/0/1/0/all/0/1">Wathiq Mansoor</a></p>
<p>The challenge of kinship verification from facial images represents a
cutting-edge and formidable frontier in the realms of pattern recognition and
computer vision. This area of study holds a myriad of potential applications,
spanning from image annotation and forensic analysis to social media research.
Our research stands out by integrating a preprocessing method named Multiscale
Retinex (MSR), which elevates image quality and amplifies contrast, ultimately
bolstering the end results. Strategically, our methodology capitalizes on the
harmonious blend of deep and shallow texture descriptors, merging them
proficiently at the score level through the Logistic Regression (LR) method. To
elucidate, we employ the Local Phase Quantization (LPQ) descriptor to extract
shallow texture characteristics. For deep feature extraction, we turn to the
prowess of the VGG16 model, which is pre-trained on a convolutional neural
network (CNN). The robustness and efficacy of our method have been put to the
test through meticulous experiments on three rigorous kinship datasets, namely:
Cornell Kin Face, UB Kin Face, and TS Kin Face.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03568">DocBinFormer: A Two-Level Transformer Network for Effective Document Image Binarization. (arXiv:2312.03568v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Biswas_R/0/1/0/all/0/1">Risab Biswas</a>, <a href="http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1">Swalpa Kumar Roy</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1">Ning Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pal_U/0/1/0/all/0/1">Umapada Pal</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1">Guang-Bin Huang</a></p>
<p>In real life, various degradation scenarios exist that might damage document
images, making it harder to recognize and analyze them, thus binarization is a
fundamental and crucial step for achieving the most optimal performance in any
document analysis task. We propose DocBinFormer (Document Binarization
Transformer), a novel two-level vision transformer (TL-ViT) architecture based
on vision transformers for effective document image binarization. The presented
architecture employs a two-level transformer encoder to effectively capture
both global and local feature representation from the input images. These
complimentary bi-level features are exploited for efficient document image
binarization, resulting in improved results for system-generated as well as
handwritten document images in a comprehensive approach. With the absence of
convolutional layers, the transformer encoder uses the pixel patches and
sub-patches along with their positional information to operate directly on
them, while the decoder generates a clean (binarized) output image from the
latent representation of the patches. Instead of using a simple vision
transformer block to extract information from the image patches, the proposed
architecture uses two transformer blocks for greater coverage of the extracted
feature space on a global and local scale. The encoded feature representation
is used by the decoder block to generate the corresponding binarized output.
Extensive experiments on a variety of DIBCO and H-DIBCO benchmarks show that
the proposed model outperforms state-of-the-art techniques on four metrics. The
source code will be made available at
https://github.com/RisabBiswas/DocBinFormer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03584">Context Diffusion: In-Context Aware Image Generation. (arXiv:2312.03584v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Najdenkoska_I/0/1/0/all/0/1">Ivona Najdenkoska</a>, <a href="http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1">Animesh Sinha</a>, <a href="http://arxiv.org/find/cs/1/au:+Dubey_A/0/1/0/all/0/1">Abhimanyu Dubey</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahajan_D/0/1/0/all/0/1">Dhruv Mahajan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramanathan_V/0/1/0/all/0/1">Vignesh Ramanathan</a>, <a href="http://arxiv.org/find/cs/1/au:+Radenovic_F/0/1/0/all/0/1">Filip Radenovic</a></p>
<p>We propose Context Diffusion, a diffusion-based framework that enables image
generation models to learn from visual examples presented in context. Recent
work tackles such in-context learning for image generation, where a query image
is provided alongside context examples and text prompts. However, the quality
and fidelity of the generated images deteriorate when the prompt is not
present, demonstrating that these models are unable to truly learn from the
visual context. To address this, we propose a novel framework that separates
the encoding of the visual context and preserving the structure of the query
images. This results in the ability to learn from the visual context and text
prompts, but also from either one of them. Furthermore, we enable our model to
handle few-shot settings, to effectively address diverse in-context learning
scenarios. Our experiments and user study demonstrate that Context Diffusion
excels in both in-domain and out-of-domain tasks, resulting in an overall
enhancement in image quality and fidelity compared to counterpart models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03585">Foundation Model Assisted Weakly Supervised Semantic Segmentation. (arXiv:2312.03585v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xiaobo Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1">Xiaojin Gong</a></p>
<p>This work aims to leverage pre-trained foundation models, such as contrastive
language-image pre-training (CLIP) and segment anything model (SAM), to address
weakly supervised semantic segmentation (WSSS) using image-level labels. To
this end, we propose a coarse-to-fine framework based on CLIP and SAM for
generating high-quality segmentation seeds. Specifically, we construct an image
classification task and a seed segmentation task, which are jointly performed
by CLIP with frozen weights and two sets of learnable task-specific prompts. A
SAM-based seeding (SAMS) module is designed and applied to each task to produce
either coarse or fine seed maps. Moreover, we design a multi-label contrastive
loss supervised by image-level labels and a CAM activation loss supervised by
the generated coarse seed map. These losses are used to learn the prompts,
which are the only parts need to be learned in our framework. Once the prompts
are learned, we input each image along with the learned segmentation-specific
prompts into CLIP and the SAMS module to produce high-quality segmentation
seeds. These seeds serve as pseudo labels to train an off-the-shelf
segmentation network like other two-stage WSSS methods. Experiments show that
our method achieves the state-of-the-art performance on PASCAL VOC 2012 and
competitive results on MS COCO 2014.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03587">Language-Informed Visual Concept Learning. (arXiv:2312.03587v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Sharon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yunzhi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Shangzhe Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jiajun Wu</a></p>
<p>Our understanding of the visual world is centered around various concept
axes, characterizing different aspects of visual entities. While different
concept axes can be easily specified by language, e.g. color, the exact visual
nuances along each axis often exceed the limitations of linguistic
articulations, e.g. a particular style of painting. In this work, our goal is
to learn a language-informed visual concept representation, by simply
distilling large pre-trained vision-language models. Specifically, we train a
set of concept encoders to encode the information pertinent to a set of
language-informed concept axes, with an objective of reproducing the input
image through a pre-trained Text-to-Image (T2I) model. To encourage better
disentanglement of different concept encoders, we anchor the concept embeddings
to a set of text embeddings obtained from a pre-trained Visual Question
Answering (VQA) model. At inference time, the model extracts concept embeddings
along various axes from new test images, which can be remixed to generate
images with novel compositions of visual concepts. With a lightweight test-time
finetuning procedure, it can also generalize to novel concepts unseen at
training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03594">A Task is Worth One Word: Learning with Task Prompts for High-Quality Versatile Image Inpainting. (arXiv:2312.03594v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1">Junhao Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1">Yanhong Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wenran Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1">Chun Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Kai Chen</a></p>
<p>Achieving high-quality versatile image inpainting, where user-specified
regions are filled with plausible content according to user intent, presents a
significant challenge. Existing methods face difficulties in simultaneously
addressing context-aware image inpainting and text-guided object inpainting due
to the distinct optimal training strategies required. To overcome this
challenge, we introduce PowerPaint, the first high-quality and versatile
inpainting model that excels in both tasks. First, we introduce learnable task
prompts along with tailored fine-tuning strategies to guide the model's focus
on different inpainting targets explicitly. This enables PowerPaint to
accomplish various inpainting tasks by utilizing different task prompts,
resulting in state-of-the-art performance. Second, we demonstrate the
versatility of the task prompt in PowerPaint by showcasing its effectiveness as
a negative prompt for object removal. Additionally, we leverage prompt
interpolation techniques to enable controllable shape-guided object inpainting.
Finally, we extensively evaluate PowerPaint on various inpainting benchmarks to
demonstrate its superior performance for versatile image inpainting. We release
our codes and models on our project page: https://powerpaint.github.io/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03596">MMM: Generative Masked Motion Model. (arXiv:2312.03596v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pinyoanuntapong_E/0/1/0/all/0/1">Ekkasit Pinyoanuntapong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Pu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1">Minwoo Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chen Chen</a></p>
<p>Recent advances in text-to-motion generation using diffusion and
autoregressive models have shown promising results. However, these models often
suffer from a trade-off between real-time performance, high fidelity, and
motion editability. To address this gap, we introduce MMM, a novel yet simple
motion generation paradigm based on Masked Motion Model. MMM consists of two
key components: (1) a motion tokenizer that transforms 3D human motion into a
sequence of discrete tokens in latent space, and (2) a conditional masked
motion transformer that learns to predict randomly masked motion tokens,
conditioned on the pre-computed text tokens. By attending to motion and text
tokens in all directions, MMM explicitly captures inherent dependency among
motion tokens and semantic mapping between motion and text tokens. During
inference, this allows parallel and iterative decoding of multiple motion
tokens that are highly consistent with fine-grained text descriptions,
therefore simultaneously achieving high-fidelity and high-speed motion
generation. In addition, MMM has innate motion editability. By simply placing
mask tokens in the place that needs editing, MMM automatically fills the gaps
while guaranteeing smooth transitions between editing and non-editing parts.
Extensive experiments on the HumanML3D and KIT-ML datasets demonstrate that MMM
surpasses current leading methods in generating high-quality motion (evidenced
by superior FID scores of 0.08 and 0.429), while offering advanced editing
features such as body-part modification, motion in-betweening, and the
synthesis of long motion sequences. In addition, MMM is two orders of magnitude
faster on a single mid-range GPU than editable motion diffusion models. Our
project page is available at \url{https://exitudio.github.io/MMM-page}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03606">DiffusionSat: A Generative Foundation Model for Satellite Imagery. (arXiv:2312.03606v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khanna_S/0/1/0/all/0/1">Samar Khanna</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1">Patrick Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1">Linqi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1">Chenlin Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Rombach_R/0/1/0/all/0/1">Robin Rombach</a>, <a href="http://arxiv.org/find/cs/1/au:+Burke_M/0/1/0/all/0/1">Marshall Burke</a>, <a href="http://arxiv.org/find/cs/1/au:+Lobell_D/0/1/0/all/0/1">David Lobell</a>, <a href="http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1">Stefano Ermon</a></p>
<p>Diffusion models have achieved state-of-the-art results on many modalities
including images, speech, and video. However, existing models are not tailored
to support remote sensing data, which is widely used in important applications
including environmental monitoring and crop-yield prediction. Satellite images
are significantly different from natural images -- they can be multi-spectral,
irregularly sampled across time -- and existing diffusion models trained on
images from the Web do not support them. Furthermore, remote sensing data is
inherently spatio-temporal, requiring conditional generation tasks not
supported by traditional methods based on captions or images. In this paper, we
present DiffusionSat, to date the largest generative foundation model trained
on a collection of publicly available large, high-resolution remote sensing
datasets. As text-based captions are sparsely available for satellite images,
we incorporate the associated metadata such as geolocation as conditioning
information. Our method produces realistic samples and can be used to solve
multiple generative tasks including temporal generation, superresolution given
multi-spectral inputs and in-painting. Our method outperforms previous
state-of-the-art methods for satellite image generation and is the first
large-scale $\textit{generative}$ foundation model for satellite imagery.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03608">Automated Multimodal Data Annotation via Calibration With Indoor Positioning System. (arXiv:2312.03608v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rubel_R/0/1/0/all/0/1">Ryan Rubel</a>, <a href="http://arxiv.org/find/cs/1/au:+Dudash_A/0/1/0/all/0/1">Andrew Dudash</a>, <a href="http://arxiv.org/find/cs/1/au:+Goli_M/0/1/0/all/0/1">Mohammad Goli</a>, <a href="http://arxiv.org/find/cs/1/au:+OHara_J/0/1/0/all/0/1">James O&#x27;Hara</a>, <a href="http://arxiv.org/find/cs/1/au:+Wunderlich_K/0/1/0/all/0/1">Karl Wunderlich</a></p>
<p>Learned object detection methods based on fusion of LiDAR and camera data
require labeled training samples, but niche applications, such as warehouse
robotics or automated infrastructure, require semantic classes not available in
large existing datasets. Therefore, to facilitate the rapid creation of
multimodal object detection datasets and alleviate the burden of human
labeling, we propose a novel automated annotation pipeline. Our method uses an
indoor positioning system (IPS) to produce accurate detection labels for both
point clouds and images and eliminates manual annotation entirely. In an
experiment, the system annotates objects of interest 261.8 times faster than a
human baseline and speeds up end-to-end dataset creation by 61.5%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03611">DreamComposer: Controllable 3D Object Generation via Multi-View Conditions. (arXiv:2312.03611v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yunhan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yukun Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xiaoyang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yuan-Chen Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Song-Hai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Hengshuang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1">Tong He</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xihui Liu</a></p>
<p>Utilizing pre-trained 2D large-scale generative models, recent works are
capable of generating high-quality novel views from a single in-the-wild image.
However, due to the lack of information from multiple views, these works
encounter difficulties in generating controllable novel views. In this paper,
we present DreamComposer, a flexible and scalable framework that can enhance
existing view-aware diffusion models by injecting multi-view conditions.
Specifically, DreamComposer first uses a view-aware 3D lifting module to obtain
3D representations of an object from multiple views. Then, it renders the
latent features of the target view from 3D representations with the multi-view
feature fusion module. Finally the target view features extracted from
multi-view inputs are injected into a pre-trained diffusion model. Experiments
show that DreamComposer is compatible with state-of-the-art diffusion models
for zero-shot novel view synthesis, further enhancing them to generate
high-fidelity novel view images with multi-view conditions, ready for
controllable 3D object reconstruction and various other applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03626">TokenCompose: Grounding Diffusion with Token-level Supervision. (arXiv:2312.03626v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zirui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sha_Z/0/1/0/all/0/1">Zhizhou Sha</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1">Zheng Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yilin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1">Zhuowen Tu</a></p>
<p>We present TokenCompose, a Latent Diffusion Model for text-to-image
generation that achieves enhanced consistency between user-specified text
prompts and model-generated images. Despite its tremendous success, the
standard denoising process in the Latent Diffusion Model takes text prompts as
conditions only, absent explicit constraint for the consistency between the
text prompts and the image contents, leading to unsatisfactory results for
composing multiple object categories. TokenCompose aims to improve
multi-category instance composition by introducing the token-wise consistency
terms between the image content and object segmentation maps in the finetuning
stage. TokenCompose can be applied directly to the existing training pipeline
of text-conditioned diffusion models without extra human labeling information.
By finetuning Stable Diffusion, the model exhibits significant improvements in
multi-category instance composition and enhanced photorealism for its generated
images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03628">Boosting Segment Anything Model Towards Open-Vocabulary Learning. (arXiv:2312.03628v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1">Xumeng Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1">Longhui Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1">Xuehui Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1">Zhiyang Dou</a>, <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xin He</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kuiran Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1">Zhenjun Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1">Qi Tian</a></p>
<p>The recent Segment Anything Model (SAM) has emerged as a new paradigmatic
vision foundation model, showcasing potent zero-shot generalization and
flexible prompting. Despite SAM finding applications and adaptations in various
domains, its primary limitation lies in the inability to grasp object
semantics. In this paper, we present Sambor to seamlessly integrate SAM with
the open-vocabulary object detector in an end-to-end framework. While retaining
all the remarkable capabilities inherent to SAM, we enhance it with the
capacity to detect arbitrary objects based on human inputs like category names
or reference expressions. To accomplish this, we introduce a novel SideFormer
module that extracts SAM features to facilitate zero-shot object localization
and inject comprehensive semantic information for open-vocabulary recognition.
In addition, we devise an open-set region proposal network (Open-set RPN),
enabling the detector to acquire the open-set proposals generated by SAM.
Sambor demonstrates superior zero-shot performance across benchmarks, including
COCO and LVIS, proving highly competitive against previous SoTA methods. We
aspire for this work to serve as a meaningful endeavor in endowing SAM to
recognize diverse object categories and advancing open-vocabulary learning with
the support of vision foundation models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03631">MOCHa: Multi-Objective Reinforcement Mitigating Caption Hallucinations. (arXiv:2312.03631v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ben_Kish_A/0/1/0/all/0/1">Assaf Ben-Kish</a>, <a href="http://arxiv.org/find/cs/1/au:+Yanuka_M/0/1/0/all/0/1">Moran Yanuka</a>, <a href="http://arxiv.org/find/cs/1/au:+Alper_M/0/1/0/all/0/1">Morris Alper</a>, <a href="http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1">Raja Giryes</a>, <a href="http://arxiv.org/find/cs/1/au:+Averbuch_Elor_H/0/1/0/all/0/1">Hadar Averbuch-Elor</a></p>
<p>While recent years have seen rapid progress in image-conditioned text
generation, image captioning still suffers from the fundamental issue of
hallucinations, the generation of spurious details that cannot be inferred from
the given image. Dedicated methods for reducing hallucinations in image
captioning largely focus on closed-vocabulary object tokens, ignoring most
types of hallucinations that occur in practice. In this work, we propose MOCHa,
an approach that harnesses advancements in reinforcement learning (RL) to
address the sequence-level nature of hallucinations in an open-world setup. To
optimize for caption fidelity to the input image, we leverage ground-truth
reference captions as proxies to measure the logical consistency of generated
captions. However, optimizing for caption fidelity alone fails to preserve the
semantic adequacy of generations; therefore, we propose a multi-objective
reward function that jointly targets these qualities, without requiring any
strong supervision. We demonstrate that these goals can be simultaneously
optimized with our framework, enhancing performance for various captioning
models of different scales. Our qualitative and quantitative results
demonstrate MOCHa's superior performance across various established metrics. We
also demonstrate the benefit of our method in the open-vocabulary setting. To
this end, we contribute OpenCHAIR, a new benchmark for quantifying
open-vocabulary hallucinations in image captioning models, constructed using
generative foundation models. We will release our code, benchmark, and trained
models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03640">Training Neural Networks on RAW and HDR Images for Restoration Tasks. (arXiv:2312.03640v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Luo_L/0/1/0/all/0/1">Lei Luo</a>, <a href="http://arxiv.org/find/eess/1/au:+Chapiro_A/0/1/0/all/0/1">Alexandre Chapiro</a>, <a href="http://arxiv.org/find/eess/1/au:+Xiang_X/0/1/0/all/0/1">Xiaoyu Xiang</a>, <a href="http://arxiv.org/find/eess/1/au:+Fan_Y/0/1/0/all/0/1">Yuchen Fan</a>, <a href="http://arxiv.org/find/eess/1/au:+Ranjan_R/0/1/0/all/0/1">Rakesh Ranjan</a>, <a href="http://arxiv.org/find/eess/1/au:+Mantiuk_R/0/1/0/all/0/1">Rafal Mantiuk</a></p>
<p>The vast majority of standard image and video content available online is
represented in display-encoded color spaces, in which pixel values are
conveniently scaled to a limited range (0-1) and the color distribution is
approximately perceptually uniform. In contrast, both camera RAW and high
dynamic range (HDR) images are often represented in linear color spaces, in
which color values are linearly related to colorimetric quantities of light.
While training on commonly available display-encoded images is a
well-established practice, there is no consensus on how neural networks should
be trained for tasks on RAW and HDR images in linear color spaces. In this
work, we test several approaches on three popular image restoration
applications: denoising, deblurring, and single-image super-resolution. We
examine whether HDR/RAW images need to be display-encoded using popular
transfer functions (PQ, PU21, mu-law), or whether it is better to train in
linear color spaces, but use loss functions that correct for perceptual
non-uniformity. Our results indicate that neural networks train significantly
better on HDR and RAW images represented in display-encoded color spaces, which
offer better perceptual uniformity than linear spaces. This small change to the
training strategy can bring a very substantial gain in performance, up to 10-15
dB.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03641">MotionCtrl: A Unified and Flexible Motion Controller for Video Generation. (arXiv:2312.03641v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhouxia Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1">Ziyang Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xintao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tianshui Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1">Menghan Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1">Ping Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1">Ying Shan</a></p>
<p>Motions in a video primarily consist of camera motion, induced by camera
movement, and object motion, resulting from object movement. Accurate control
of both camera and object motion is essential for video generation. However,
existing works either mainly focus on one type of motion or do not clearly
distinguish between the two, limiting their control capabilities and diversity.
Therefore, this paper presents MotionCtrl, a unified and flexible motion
controller for video generation designed to effectively and independently
control camera and object motion. The architecture and training strategy of
MotionCtrl are carefully devised, taking into account the inherent properties
of camera motion, object motion, and imperfect training data. Compared to
previous methods, MotionCtrl offers three main advantages: 1) It effectively
and independently controls camera motion and object motion, enabling more
fine-grained motion control and facilitating flexible and diverse combinations
of both types of motion. 2) Its motion conditions are determined by camera
poses and trajectories, which are appearance-free and minimally impact the
appearance or shape of objects in generated videos. 3) It is a relatively
generalizable model that can adapt to a wide array of camera poses and
trajectories once trained. Extensive qualitative and quantitative experiments
have been conducted to demonstrate the superiority of MotionCtrl over existing
methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03647">Editable Stain Transformation Of Histological Images Using Unpaired GANs. (arXiv:2312.03647v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Sloboda_T/0/1/0/all/0/1">Tibor Sloboda</a>, <a href="http://arxiv.org/find/eess/1/au:+Hudec_L/0/1/0/all/0/1">Luk&#xe1;&#x161; Hudec</a>, <a href="http://arxiv.org/find/eess/1/au:+Benesova_W/0/1/0/all/0/1">Wanda Bene&#x161;ov&#xe1;</a></p>
<p>Double staining in histopathology, particularly for metaplastic breast
cancer, typically employs H&amp;E and P63 dyes. However, P63's tissue damage and
high cost necessitate alternative methods. This study introduces xAI-CycleGAN,
an advanced architecture combining Mask CycleGAN with explainability features
and structure-preserving capabilities for transforming H&amp;E stained breast
tissue images into P63-like images. The architecture allows for output editing,
enhancing resemblance to actual images and enabling further model refinement.
We showcase xAI-CycleGAN's efficacy in maintaining structural integrity and
generating high-quality images. Additionally, a histopathologist survey
indicates the generated images' realism is often comparable to actual images,
validating our model's high-quality output.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03661">Reason2Drive: Towards Interpretable and Chain-based Reasoning for Autonomous Driving. (arXiv:2312.03661v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nie_M/0/1/0/all/0/1">Ming Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_R/0/1/0/all/0/1">Renyuan Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chunwei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1">Xinyue Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Jianhua Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Li Zhang</a></p>
<p>Large vision-language models (VLMs) have garnered increasing interest in
autonomous driving areas, due to their advanced capabilities in complex
reasoning tasks essential for highly autonomous vehicle behavior. Despite their
potential, research in autonomous systems is hindered by the lack of datasets
with annotated reasoning chains that explain the decision-making processes in
driving. To bridge this gap, we present Reason2Drive, a benchmark dataset with
over 600K video-text pairs, aimed at facilitating the study of interpretable
reasoning in complex driving environments. We distinctly characterize the
autonomous driving process as a sequential combination of perception,
prediction, and reasoning steps, and the question-answer pairs are
automatically collected from a diverse range of open-source outdoor driving
datasets, including nuScenes, Waymo and ONCE. Moreover, we introduce a novel
aggregated evaluation metric to assess chain-based reasoning performance in
autonomous systems, addressing the semantic ambiguities of existing metrics
such as BLEU and CIDEr. Based on the proposed benchmark, we conduct experiments
to assess various existing VLMs, revealing insights into their reasoning
capabilities. Additionally, we develop an efficient approach to empower VLMs to
leverage object-level perceptual elements in both feature extraction and
prediction, further enhancing their reasoning accuracy. The code and dataset
will be released.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03667">WarpDiffusion: Efficient Diffusion Model for High-Fidelity Virtual Try-on. (arXiv:2312.03667v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+zhang_x/0/1/0/all/0/1">xujie zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Kampffmeyer_M/0/1/0/all/0/1">Michael Kampffmeyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1">Xin Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1">Zhenyu Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1">Feida Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1">Haoye Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1">Xiaodan Liang</a></p>
<p>Image-based Virtual Try-On (VITON) aims to transfer an in-shop garment image
onto a target person. While existing methods focus on warping the garment to
fit the body pose, they often overlook the synthesis quality around the
garment-skin boundary and realistic effects like wrinkles and shadows on the
warped garments. These limitations greatly reduce the realism of the generated
results and hinder the practical application of VITON techniques. Leveraging
the notable success of diffusion-based models in cross-modal image synthesis,
some recent diffusion-based methods have ventured to tackle this issue.
However, they tend to either consume a significant amount of training resources
or struggle to achieve realistic try-on effects and retain garment details. For
efficient and high-fidelity VITON, we propose WarpDiffusion, which bridges the
warping-based and diffusion-based paradigms via a novel informative and local
garment feature attention mechanism. Specifically, WarpDiffusion incorporates
local texture attention to reduce resource consumption and uses a novel
auto-mask module that effectively retains only the critical areas of the warped
garment while disregarding unrealistic or erroneous portions. Notably,
WarpDiffusion can be integrated as a plug-and-play component into existing
VITON methodologies, elevating their synthesis quality. Extensive experiments
on high-resolution VITON benchmarks and an in-the-wild test set demonstrate the
superiority of WarpDiffusion, surpassing state-of-the-art methods both
qualitatively and quantitatively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03678">Hybrid Functional Maps for Crease-Aware Non-Isometric Shape Matching. (arXiv:2312.03678v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bastian_L/0/1/0/all/0/1">Lennart Bastian</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1">Yizheng Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1">Nassir Navab</a>, <a href="http://arxiv.org/find/cs/1/au:+Lahner_Z/0/1/0/all/0/1">Zorah L&#xe4;hner</a></p>
<p>Non-isometric shape correspondence remains a fundamental challenge in
computer vision. Traditional methods using Laplace-Beltrami operator (LBO)
eigenmodes face limitations in characterizing high-frequency extrinsic shape
changes like bending and creases. We propose a novel approach of combining the
non-orthogonal extrinsic basis of eigenfunctions of the elastic thin-shell
hessian with the intrinsic ones of the LBO, creating a hybrid spectral space in
which we construct functional maps. To this end, we present a theoretical
framework to effectively integrate non-orthogonal basis functions into
descriptor- and learning-based functional map methods. Our approach can be
incorporated easily into existing functional map pipelines across varying
applications and is able to handle complex deformations beyond isometries. We
show extensive evaluations across various supervised and unsupervised settings
and demonstrate significant improvements. Notably, our approach achieves up to
15% better mean geodesic error for non-isometric correspondence settings and up
to 45% improvement in scenarios with topological noise.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03692">Memory Triggers: Unveiling Memorization in Text-To-Image Generative Models through Word-Level Duplication. (arXiv:2312.03692v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Naseh_A/0/1/0/all/0/1">Ali Naseh</a>, <a href="http://arxiv.org/find/cs/1/au:+Roh_J/0/1/0/all/0/1">Jaechul Roh</a>, <a href="http://arxiv.org/find/cs/1/au:+Houmansadr_A/0/1/0/all/0/1">Amir Houmansadr</a></p>
<p>Diffusion-based models, such as the Stable Diffusion model, have
revolutionized text-to-image synthesis with their ability to produce
high-quality, high-resolution images. These advancements have prompted
significant progress in image generation and editing tasks. However, these
models also raise concerns due to their tendency to memorize and potentially
replicate exact training samples, posing privacy risks and enabling adversarial
attacks. Duplication in training datasets is recognized as a major factor
contributing to memorization, and various forms of memorization have been
studied so far. This paper focuses on two distinct and underexplored types of
duplication that lead to replication during inference in diffusion-based
models, particularly in the Stable Diffusion model. We delve into these
lesser-studied duplication phenomena and their implications through two case
studies, aiming to contribute to the safer and more responsible use of
generative models in various applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03698">Intrinsic Harmonization for Illumination-Aware Compositing. (arXiv:2312.03698v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Careaga_C/0/1/0/all/0/1">Chris Careaga</a>, <a href="http://arxiv.org/find/cs/1/au:+Aksoy_Y/0/1/0/all/0/1">Ya&#x11f;&#x131;z Aksoy</a>, <a href="http://arxiv.org/find/cs/1/au:+Miangoleh_S/0/1/0/all/0/1">S. Mahdi H. Miangoleh</a></p>
<p>Despite significant advancements in network-based image harmonization
techniques, there still exists a domain disparity between typical training
pairs and real-world composites encountered during inference. Most existing
methods are trained to reverse global edits made on segmented image regions,
which fail to accurately capture the lighting inconsistencies between the
foreground and background found in composited images. In this work, we
introduce a self-supervised illumination harmonization approach formulated in
the intrinsic image domain. First, we estimate a simple global lighting model
from mid-level vision representations to generate a rough shading for the
foreground region. A network then refines this inferred shading to generate a
harmonious re-shading that aligns with the background scene. In order to match
the color appearance of the foreground and background, we utilize ideas from
prior harmonization approaches to perform parameterized image edits in the
albedo domain. To validate the effectiveness of our approach, we present
results from challenging real-world composites and conduct a user study to
objectively measure the enhanced realism achieved compared to state-of-the-art
harmonization methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03700">OneLLM: One Framework to Align All Modalities with Language. (arXiv:2312.03700v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Jiaming Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_K/0/1/0/all/0/1">Kaixiong Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yiyuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiaqi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kaipeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1">Dahua Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1">Peng Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1">Xiangyu Yue</a></p>
<p>Multimodal large language models (MLLMs) have gained significant attention
due to their strong multimodal understanding capability. However, existing
works rely heavily on modality-specific encoders, which usually differ in
architecture and are limited to common modalities. In this paper, we present
OneLLM, an MLLM that aligns eight modalities to language using a unified
framework. We achieve this through a unified multimodal encoder and a
progressive multimodal alignment pipeline. In detail, we first train an image
projection module to connect a vision encoder with LLM. Then, we build a
universal projection module (UPM) by mixing multiple image projection modules
and dynamic routing. Finally, we progressively align more modalities to LLM
with the UPM. To fully leverage the potential of OneLLM in following
instructions, we also curated a comprehensive multimodal instruction dataset,
including 2M items from image, audio, video, point cloud, depth/normal map, IMU
and fMRI brain activity. OneLLM is evaluated on 25 diverse benchmarks,
encompassing tasks such as multimodal captioning, question answering and
reasoning, where it delivers excellent performance. Code, data, model and
online demo are available at https://github.com/csuhan/OneLLM
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03701">Self-conditioned Image Generation via Generating Representations. (arXiv:2312.03701v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1">Tianhong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Katabi_D/0/1/0/all/0/1">Dina Katabi</a>, <a href="http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1">Kaiming He</a></p>
<p>This paper presents $\textbf{R}$epresentation-$\textbf{C}$onditioned image
$\textbf{G}$eneration (RCG), a simple yet effective image generation framework
which sets a new benchmark in class-unconditional image generation. RCG does
not condition on any human annotations. Instead, it conditions on a
self-supervised representation distribution which is mapped from the image
distribution using a pre-trained encoder. During generation, RCG samples from
such representation distribution using a representation diffusion model (RDM),
and employs a pixel generator to craft image pixels conditioned on the sampled
representation. Such a design provides substantial guidance during the
generative process, resulting in high-quality image generation. Tested on
ImageNet 256$\times$256, RCG achieves a Frechet Inception Distance (FID) of
3.31 and an Inception Score (IS) of 253.4. These results not only significantly
improve the state-of-the-art of class-unconditional image generation but also
rival the current leading methods in class-conditional image generation,
bridging the long-standing performance gap between these two tasks. Code is
available at https://github.com/LTH14/rcg.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03703">Skeleton-in-Context: Unified Skeleton Sequence Modeling with In-Context Learning. (arXiv:2312.03703v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinshun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1">Zhongbin Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xia Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiangtai Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Mengyuan Liu</a></p>
<p>In-context learning provides a new perspective for multi-task modeling for
vision and NLP. Under this setting, the model can perceive tasks from prompts
and accomplish them without any extra task-specific head predictions or model
fine-tuning. However, Skeleton sequence modeling via in-context learning
remains unexplored. Directly applying existing in-context models from other
areas onto skeleton sequences fails due to the inter-frame and cross-task pose
similarity that makes it outstandingly hard to perceive the task correctly from
a subtle context. To address this challenge, we propose Skeleton-in-Context
(SiC), an effective framework for in-context skeleton sequence modeling. Our
SiC is able to handle multiple skeleton-based tasks simultaneously after a
single training process and accomplish each task from context according to the
given prompt. It can further generalize to new, unseen tasks according to
customized prompts. To facilitate context perception, we additionally propose a
task-unified prompt, which adaptively learns tasks of different natures, such
as partial joint-level generation, sequence-level prediction, or 2D-to-3D
motion prediction. We conduct extensive experiments to evaluate the
effectiveness of our SiC on multiple tasks, including motion prediction, pose
estimation, joint completion, and future pose estimation. We also evaluate its
generalization capability on unseen tasks such as motion-in-between. These
experiments show that our model achieves state-of-the-art multi-task
performance and even outperforms single-task methods on certain tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03704">Relightable Gaussian Codec Avatars. (arXiv:2312.03704v1 [cs.GR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Saito_S/0/1/0/all/0/1">Shunsuke Saito</a>, <a href="http://arxiv.org/find/cs/1/au:+Schwartz_G/0/1/0/all/0/1">Gabriel Schwartz</a>, <a href="http://arxiv.org/find/cs/1/au:+Simon_T/0/1/0/all/0/1">Tomas Simon</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Junxuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Nam_G/0/1/0/all/0/1">Giljoo Nam</a></p>
<p>The fidelity of relighting is bounded by both geometry and appearance
representations. For geometry, both mesh and volumetric approaches have
difficulty modeling intricate structures like 3D hair geometry. For appearance,
existing relighting models are limited in fidelity and often too slow to render
in real-time with high-resolution continuous environments. In this work, we
present Relightable Gaussian Codec Avatars, a method to build high-fidelity
relightable head avatars that can be animated to generate novel expressions.
Our geometry model based on 3D Gaussians can capture 3D-consistent
sub-millimeter details such as hair strands and pores on dynamic face
sequences. To support diverse materials of human heads such as the eyes, skin,
and hair in a unified manner, we present a novel relightable appearance model
based on learnable radiance transfer. Together with global illumination-aware
spherical harmonics for the diffuse components, we achieve real-time relighting
with spatially all-frequency reflections using spherical Gaussians. This
appearance model can be efficiently relit under both point light and continuous
illumination. We further improve the fidelity of eye reflections and enable
explicit gaze control by introducing relightable explicit eye models. Our
method outperforms existing approaches without compromising real-time
performance. We also demonstrate real-time relighting of avatars on a tethered
consumer VR headset, showcasing the efficiency and fidelity of our avatars.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2201.07394">KappaFace: Adaptive Additive Angular Margin Loss for Deep Face Recognition. (arXiv:2201.07394v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oinar_C/0/1/0/all/0/1">Chingis Oinar</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_B/0/1/0/all/0/1">Binh M. Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1">Simon S. Woo</a></p>
<p>Feature learning is a widely used method employed for large-scale face
recognition. Recently, large-margin softmax loss methods have demonstrated
significant enhancements on deep face recognition. These methods propose fixed
positive margins in order to enforce intra-class compactness and inter-class
diversity. However, the majority of the proposed methods do not consider the
class imbalance issue, which is a major challenge in practice for developing
deep face recognition models. We hypothesize that it significantly affects the
generalization ability of the deep face models. Inspired by this observation,
we introduce a novel adaptive strategy, called KappaFace, to modulate the
relative importance based on class difficultness and imbalance. With the
support of the von Mises-Fisher distribution, our proposed KappaFace loss can
intensify the margin's magnitude for hard learning or low concentration classes
while relaxing it for counter classes. Experiments conducted on popular facial
benchmarks demonstrate that our proposed method achieves superior performance
to the state-of-the-art.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2204.01278">Technical Report on Subspace Pyramid Fusion Network for Semantic Segmentation. (arXiv:2204.01278v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Elhassan_M/0/1/0/all/0/1">Mohammed A. M. Elhassan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Chenhui Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1">Chenxi Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Munea_T/0/1/0/all/0/1">Tewodros Legesse Munea</a></p>
<p>The following is a technical report to test the validity of the proposed
Subspace Pyramid Fusion Module (SPFM) to capture multi-scale feature
representations, which is more useful for semantic segmentation. In this
investigation, we have proposed the Efficient Shuffle Attention Module(ESAM) to
reconstruct the skip-connections paths by fusing multi-level global context
features. Experimental results on two well-known semantic segmentation
datasets, including Camvid and Cityscapes, show the effectiveness of our
proposed method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.04425">Multiple Instance Learning for Digital Pathology: A Review on the State-of-the-Art, Limitations &amp; Future Potential. (arXiv:2206.04425v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gadermayr_M/0/1/0/all/0/1">Michael Gadermayr</a>, <a href="http://arxiv.org/find/cs/1/au:+Tschuchnig_M/0/1/0/all/0/1">Maximilian Tschuchnig</a></p>
<p>Digital whole slides images contain an enormous amount of information
providing a strong motivation for the development of automated image analysis
tools. Particularly deep neural networks show high potential with respect to
various tasks in the field of digital pathology. However, a limitation is given
by the fact that typical deep learning algorithms require (manual) annotations
in addition to the large amounts of image data, to enable effective training.
Multiple instance learning exhibits a powerful tool for learning deep neural
networks in a scenario without fully annotated data. These methods are
particularly effective in this domain, due to the fact that labels for a
complete whole slide image are often captured routinely, whereas labels for
patches, regions or pixels are not. This potential already resulted in a
considerable number of publications, with the majority published in the last
three years. Besides the availability of data and a high motivation from the
medical perspective, the availability of powerful graphics processing units
exhibits an accelerator in this field. In this paper, we provide an overview of
widely and effectively used concepts of used deep multiple instance learning
approaches, recent advances and also critically discuss remaining challenges
and future potential.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.04979">Convolutional layers are equivariant to discrete shifts but not continuous translations. (arXiv:2206.04979v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+McGreivy_N/0/1/0/all/0/1">Nick McGreivy</a>, <a href="http://arxiv.org/find/cs/1/au:+Hakim_A/0/1/0/all/0/1">Ammar Hakim</a></p>
<p>The purpose of this short and simple note is to clarify a common
misconception about convolutional neural networks (CNNs). CNNs are made up of
convolutional layers which are shift equivariant due to weight sharing.
However, convolutional layers are not translation equivariant, even when
boundary effects are ignored and when pooling and subsampling are absent. This
is because shift equivariance is a discrete symmetry while translation
equivariance is a continuous symmetry. This fact is well known among
researchers in equivariant machine learning, but is usually overlooked among
non-experts. To minimize confusion, we suggest using the term `shift
equivariance' to refer to discrete shifts in pixels and `translation
equivariance' to refer to continuous translations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.03934">Inflating 2D Convolution Weights for Efficient Generation of 3D Medical Images. (arXiv:2208.03934v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1">Yanbin Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Dwivedi_G/0/1/0/all/0/1">Girish Dwivedi</a>, <a href="http://arxiv.org/find/eess/1/au:+Boussaid_F/0/1/0/all/0/1">Farid Boussaid</a>, <a href="http://arxiv.org/find/eess/1/au:+Sanfilippo_F/0/1/0/all/0/1">Frank Sanfilippo</a>, <a href="http://arxiv.org/find/eess/1/au:+Yamada_M/0/1/0/all/0/1">Makoto Yamada</a>, <a href="http://arxiv.org/find/eess/1/au:+Bennamoun_M/0/1/0/all/0/1">Mohammed Bennamoun</a></p>
<p>The generation of three-dimensional (3D) medical images has great application
potential since it takes into account the 3D anatomical structure. Two problems
prevent effective training of a 3D medical generative model: (1) 3D medical
images are expensive to acquire and annotate, resulting in an insufficient
number of training images, and (2) a large number of parameters are involved in
3D convolution.
</p>
<p>Methods: We propose a novel GAN model called 3D Split&amp;Shuffle-GAN. To address
the 3D data scarcity issue, we first pre-train a two-dimensional (2D) GAN model
using abundant image slices and inflate the 2D convolution weights to improve
the initialization of the 3D GAN. Novel 3D network architectures are proposed
for both the generator and discriminator of the GAN model to significantly
reduce the number of parameters while maintaining the quality of image
generation. Several weight inflation strategies and parameter-efficient 3D
architectures are investigated.
</p>
<p>Results: Experiments on both heart (Stanford AIMI Coronary Calcium) and brain
(Alzheimer's Disease Neuroimaging Initiative) datasets show that our method
leads to improved 3D image generation quality (14.7 improvements on Fr\'echet
inception distance) with significantly fewer parameters (only 48.5% of the
baseline method).
</p>
<p>Conclusions: We built a parameter-efficient 3D medical image generation
model. Due to the efficiency and effectiveness, it has the potential to
generate high-quality 3D brain and heart images for real use cases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.14085">Evaluating Point Cloud from Moving Camera Videos: A No-Reference Metric. (arXiv:2208.14085v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zicheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1">Wei Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yucheng Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1">Xiongkuo Min</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1">Wei Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Ying Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1">Guangtao Zhai</a></p>
<p>Point cloud is one of the most widely used digital representation formats for
three-dimensional (3D) contents, the visual quality of which may suffer from
noise and geometric shift distortions during the production procedure as well
as compression and downsampling distortions during the transmission process. To
tackle the challenge of point cloud quality assessment (PCQA), many PCQA
methods have been proposed to evaluate the visual quality levels of point
clouds by assessing the rendered static 2D projections. Although such
projection-based PCQA methods achieve competitive performance with the
assistance of mature image quality assessment (IQA) methods, they neglect that
the 3D model is also perceived in a dynamic viewing manner, where the viewpoint
is continually changed according to the feedback of the rendering device.
Therefore, in this paper, we evaluate the point clouds from moving camera
videos and explore the way of dealing with PCQA tasks via using video quality
assessment (VQA) methods. First, we generate the captured videos by rotating
the camera around the point clouds through several circular pathways. Then we
extract both spatial and temporal quality-aware features from the selected key
frames and the video clips through using trainable 2D-CNN and pre-trained
3D-CNN models respectively. Finally, the visual quality of point clouds is
represented by the video quality values. The experimental results reveal that
the proposed method is effective for predicting the visual quality levels of
the point clouds and even competitive with full-reference (FR) PCQA methods.
The ablation studies further verify the rationality of the proposed framework
and confirm the contributions made by the quality-aware features extracted via
the dynamic viewing manner. The code is available at
https://github.com/zzc-1998/VQA_PC.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.02890">Data-Driven Target Localization Using Adaptive Radar Processing and Convolutional Neural Networks. (arXiv:2209.02890v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Venkatasubramanian_S/0/1/0/all/0/1">Shyam Venkatasubramanian</a>, <a href="http://arxiv.org/find/cs/1/au:+Gogineni_S/0/1/0/all/0/1">Sandeep Gogineni</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_B/0/1/0/all/0/1">Bosung Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pezeshki_A/0/1/0/all/0/1">Ali Pezeshki</a>, <a href="http://arxiv.org/find/cs/1/au:+Rangaswamy_M/0/1/0/all/0/1">Muralidhar Rangaswamy</a>, <a href="http://arxiv.org/find/cs/1/au:+Tarokh_V/0/1/0/all/0/1">Vahid Tarokh</a></p>
<p>Leveraging the advanced functionalities of modern radio frequency (RF)
modeling and simulation tools, specifically designed for adaptive radar
processing applications, this paper presents a data-driven approach to improve
accuracy in radar target localization post adaptive radar detection. To this
end, we generate a large number of radar returns by randomly placing targets of
variable strengths in a predefined area, using RFView, a high-fidelity,
site-specific, RF modeling &amp; simulation tool. We produce heatmap tensors from
the radar returns, in range, azimuth [and Doppler], of the normalized adaptive
matched filter (NAMF) test statistic. We then train a regression convolutional
neural network (CNN) to estimate target locations from these heatmap tensors,
and we compare the target localization accuracy of this approach with that of
peak-finding and local search methods. This empirical study shows that our
regression CNN achieves a considerable improvement in target location
estimation accuracy. The regression CNN offers significant gains and reasonable
accuracy even at signal-to-clutter-plus-noise ratio (SCNR) regimes that are
close to the breakdown threshold SCNR of the NAMF. We also study the robustness
of our trained CNN to mismatches in the radar data, where the CNN is tested on
heatmap tensors collected from areas that it was not trained on. We show that
our CNN can be made robust to mismatches in the radar data through few-shot
learning, using a relatively small number of new training samples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.05952">3D Brain and Heart Volume Generative Models: A Survey. (arXiv:2210.05952v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1">Yanbin Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Dwivedi_G/0/1/0/all/0/1">Girish Dwivedi</a>, <a href="http://arxiv.org/find/eess/1/au:+Boussaid_F/0/1/0/all/0/1">Farid Boussaid</a>, <a href="http://arxiv.org/find/eess/1/au:+Bennamoun_M/0/1/0/all/0/1">Mohammed Bennamoun</a></p>
<p>Generative models such as generative adversarial networks and autoencoders
have gained a great deal of attention in the medical field due to their
excellent data generation capability. This paper provides a comprehensive
survey of generative models for three-dimensional (3D) volumes, focusing on the
brain and heart. A new and elaborate taxonomy of unconditional and conditional
generative models is proposed to cover diverse medical tasks for the brain and
heart: unconditional synthesis, classification, conditional synthesis,
segmentation, denoising, detection, and registration. We provide relevant
background, examine each task and also suggest potential future directions. A
list of the latest publications will be updated on Github to keep up with the
rapid influx of papers at
https://github.com/csyanbin/3D-Medical-Generative-Survey.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.16380">Incorporating Crowdsourced Annotator Distributions into Ensemble Modeling to Improve Classification Trustworthiness for Ancient Greek Papyri. (arXiv:2210.16380v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+West_G/0/1/0/all/0/1">Graham West</a>, <a href="http://arxiv.org/find/cs/1/au:+Swindall_M/0/1/0/all/0/1">Matthew I. Swindall</a>, <a href="http://arxiv.org/find/cs/1/au:+Keener_B/0/1/0/all/0/1">Ben Keener</a>, <a href="http://arxiv.org/find/cs/1/au:+Player_T/0/1/0/all/0/1">Timothy Player</a>, <a href="http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1">Alex C. Williams</a>, <a href="http://arxiv.org/find/cs/1/au:+Brusuelas_J/0/1/0/all/0/1">James H. Brusuelas</a>, <a href="http://arxiv.org/find/cs/1/au:+Wallin_J/0/1/0/all/0/1">John F. Wallin</a></p>
<p>Performing classification on noisy, crowdsourced image datasets can prove
challenging even for the best neural networks. Two issues which complicate the
problem on such datasets are class imbalance and ground-truth uncertainty in
labeling. The AL-ALL and AL-PUB datasets - consisting of tightly cropped,
individual characters from images of ancient Greek papyri - are strongly
affected by both issues. The application of ensemble modeling to such datasets
can help identify images where the ground-truth is questionable and quantify
the trustworthiness of those samples. As such, we apply stacked generalization
consisting of nearly identical ResNets with different loss functions: one
utilizing sparse cross-entropy (CXE) and the other Kullback-Liebler Divergence
(KLD). Both networks use labels drawn from a crowd-sourced consensus. This
consensus is derived from a Normalized Distribution of Annotations (NDA) based
on all annotations for a given character in the dataset. For the second
network, the KLD is calculated with respect to the NDA. For our ensemble model,
we apply a k-nearest neighbors model to the outputs of the CXE and KLD
networks. Individually, the ResNet models have approximately 93% accuracy,
while the ensemble model achieves an accuracy of &gt; 95%, increasing the
classification trustworthiness. We also perform an analysis of the Shannon
entropy of the various models' output distributions to measure classification
uncertainty. Our results suggest that entropy is useful for predicting model
misclassifications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.06930">PaintNet: Unstructured Multi-Path Learning from 3D Point Clouds for Robotic Spray Painting. (arXiv:2211.06930v3 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tiboni_G/0/1/0/all/0/1">Gabriele Tiboni</a>, <a href="http://arxiv.org/find/cs/1/au:+Camoriano_R/0/1/0/all/0/1">Raffaello Camoriano</a>, <a href="http://arxiv.org/find/cs/1/au:+Tommasi_T/0/1/0/all/0/1">Tatiana Tommasi</a></p>
<p>Popular industrial robotic problems such as spray painting and welding
require (i) conditioning on free-shape 3D objects and (ii) planning of multiple
trajectories to solve the task. Yet, existing solutions make strong assumptions
on the form of input surfaces and the nature of output paths, resulting in
limited approaches unable to cope with real-data variability. By leveraging on
recent advances in 3D deep learning, we introduce a novel framework capable of
dealing with arbitrary 3D surfaces, and handling a variable number of unordered
output paths (i.e. unstructured). Our approach predicts local path segments,
which can be later concatenated to reconstruct long-horizon paths. We
extensively validate the proposed method in the context of robotic spray
painting by releasing PaintNet, the first public dataset of expert
demonstrations on free-shape 3D objects collected in a real industrial
scenario. A thorough experimental analysis demonstrates the capabilities of our
model to promptly predict smooth output paths that cover up to 95% of
previously unseen object surfaces, even without explicitly optimizing for paint
coverage.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.00564">Leveraging Single-View Images for Unsupervised 3D Point Cloud Completion. (arXiv:2212.00564v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1">Lintai Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qijian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1">Junhui Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yong Xu</a></p>
<p>Point clouds captured by scanning devices are often incomplete due to
occlusion. To overcome this limitation, point cloud completion methods have
been developed to predict the complete shape of an object based on its partial
input. These methods can be broadly classified as supervised or unsupervised.
However, both categories require a large number of 3D complete point clouds,
which may be difficult to capture. In this paper, we propose Cross-PCC, an
unsupervised point cloud completion method without requiring any 3D complete
point clouds. We only utilize 2D images of the complete objects, which are
easier to capture than 3D complete and clean point clouds. Specifically, to
take advantage of the complementary information from 2D images, we use a
single-view RGB image to extract 2D features and design a fusion module to fuse
the 2D and 3D features extracted from the partial point cloud. To guide the
shape of predicted point clouds, we project the predicted points of the object
to the 2D plane and use the foreground pixels of its silhouette maps to
constrain the position of the projected points. To reduce the outliers of the
predicted point clouds, we propose a view calibrator to move the points
projected to the background into the foreground by the single-view silhouette
image. To the best of our knowledge, our approach is the first point cloud
completion method that does not require any 3D supervision. The experimental
results of our method are superior to those of the state-of-the-art
unsupervised methods by a large margin. Moreover, our method even achieves
comparable performance to some supervised methods. We will make the source code
publicly available at https://github.com/ltwu6/cross-pcc.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.07495">SAIF: Sparse Adversarial and Imperceptible Attack Framework. (arXiv:2212.07495v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Imtiaz_T/0/1/0/all/0/1">Tooba Imtiaz</a>, <a href="http://arxiv.org/find/cs/1/au:+Kohler_M/0/1/0/all/0/1">Morgan Kohler</a>, <a href="http://arxiv.org/find/cs/1/au:+Miller_J/0/1/0/all/0/1">Jared Miller</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zifeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sznaier_M/0/1/0/all/0/1">Mario Sznaier</a>, <a href="http://arxiv.org/find/cs/1/au:+Camps_O/0/1/0/all/0/1">Octavia Camps</a>, <a href="http://arxiv.org/find/cs/1/au:+Dy_J/0/1/0/all/0/1">Jennifer Dy</a></p>
<p>Adversarial attacks hamper the decision-making ability of neural networks by
perturbing the input signal. The addition of calculated small distortion to
images, for instance, can deceive a well-trained image classification network.
In this work, we propose a novel attack technique called Sparse Adversarial and
Interpretable Attack Framework (SAIF). Specifically, we design imperceptible
attacks that contain low-magnitude perturbations at a small number of pixels
and leverage these sparse attacks to reveal the vulnerability of classifiers.
We use the Frank-Wolfe (conditional gradient) algorithm to simultaneously
optimize the attack perturbations for bounded magnitude and sparsity with
$O(1/\sqrt{T})$ convergence. Empirical results show that SAIF computes highly
imperceptible and interpretable adversarial examples, and outperforms
state-of-the-art sparse attack methods on the ImageNet dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.02367">FastPillars: A Deployment-friendly Pillar-based 3D Detector. (arXiv:2302.02367v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1">Sifan Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1">Zhi Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1">Xiangxiang Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xinyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Bo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1">Xiaobo Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1">Chengjian Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Jie_Z/0/1/0/all/0/1">Zequn Jie</a>, <a href="http://arxiv.org/find/cs/1/au:+Chiang_P/0/1/0/all/0/1">Patrick Yin Chiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1">Lin Ma</a></p>
<p>The deployment of 3D detectors strikes one of the major challenges in
real-world self-driving scenarios. Existing BEV-based (i.e., Bird Eye View)
detectors favor sparse convolutions (known as SPConv) to speed up training and
inference, which puts a hard barrier for deployment, especially for on-device
applications. In this paper, to tackle the challenge of efficient 3D object
detection from an industry perspective, we devise a deployment-friendly
pillar-based 3D detector, termed FastPillars. First, we introduce a novel
lightweight Max-and-Attention Pillar Encoding (MAPE) module specially for
enhancing small 3D objects. Second, we propose a simple yet effective principle
for designing a backbone in pillar-based 3D detection. We construct FastPillars
based on these designs, achieving high performance and low latency without
SPConv. Extensive experiments on two large-scale datasets demonstrate the
effectiveness and efficiency of FastPillars for on-device 3D detection
regarding both performance and speed. Specifically, FastPillars delivers
state-of-the-art accuracy on Waymo Open Dataset with 1.8X speed up and 3.8
mAPH/L2 improvement over CenterPoint (SPConv-based). Our code is publicly
available at: https://github.com/StiphyJay/FastPillars.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.07444">The Art of Camouflage: Few-shot Learning for Animal Detection and Segmentation. (arXiv:2304.07444v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Thanh-Danh Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Vu_A/0/1/0/all/0/1">Anh-Khoa Nguyen Vu</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1">Nhat-Duy Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1">Vinh-Tiep Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ngo_T/0/1/0/all/0/1">Thanh Duc Ngo</a>, <a href="http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1">Thanh-Toan Do</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1">Minh-Triet Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Tam V. Nguyen</a></p>
<p>Camouflaged object detection and segmentation is a new and challenging
research topic in computer vision. There is a serious issue of lacking data of
camouflaged objects such as camouflaged animals in natural scenes. In this
paper, we address the problem of few-shot learning for camouflaged object
detection and segmentation. To this end, we first collect a new dataset,
CAMO-FS, for the benchmark. We then propose a novel method to efficiently
detect and segment the camouflaged objects in the images. In particular, we
introduce the instance triplet loss and the instance memory storage. The
extensive experiments demonstrated that our proposed method achieves
state-of-the-art performance on the newly collected dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.12507">Learning Task-Specific Strategies for Accelerated MRI. (arXiv:2304.12507v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Wu_Z/0/1/0/all/0/1">Zihui Wu</a>, <a href="http://arxiv.org/find/eess/1/au:+Yin_T/0/1/0/all/0/1">Tianwei Yin</a>, <a href="http://arxiv.org/find/eess/1/au:+Sun_Y/0/1/0/all/0/1">Yu Sun</a>, <a href="http://arxiv.org/find/eess/1/au:+Frost_R/0/1/0/all/0/1">Robert Frost</a>, <a href="http://arxiv.org/find/eess/1/au:+Kouwe_A/0/1/0/all/0/1">Andre van der Kouwe</a>, <a href="http://arxiv.org/find/eess/1/au:+Dalca_A/0/1/0/all/0/1">Adrian V. Dalca</a>, <a href="http://arxiv.org/find/eess/1/au:+Bouman_K/0/1/0/all/0/1">Katherine L. Bouman</a></p>
<p>Compressed sensing magnetic resonance imaging (CS-MRI) seeks to recover
visual information from subsampled measurements for diagnostic tasks.
Traditional CS-MRI methods often separately address measurement subsampling,
image reconstruction, and task prediction, resulting in a suboptimal end-to-end
performance. In this work, we propose TACKLE as a unified co-design framework
for jointly optimizing subsampling, reconstruction, and prediction strategies
for the performance on downstream tasks. The na\"ive approach of simply
appending a task prediction module and training with a task-specific loss leads
to suboptimal downstream performance. Instead, we develop a training procedure
where a backbone architecture is first trained for a generic pre-training task
(image reconstruction in our case), and then fine-tuned for different
downstream tasks with a prediction head. Experimental results on multiple
public MRI datasets show that TACKLE achieves an improved performance on
various tasks over traditional CS-MRI methods. We also demonstrate that TACKLE
is robust to distribution shifts by showing that it generalizes to a new
dataset we experimentally collected using different acquisition setups from the
training data. Without additional fine-tuning, TACKLE leads to both numerical
and visual improvements compared to existing baselines. We have further
implemented a learned 4$\times$-accelerated sequence on a Siemens 3T MRI Skyra
scanner. Compared to the fully-sampling scan that takes 335 seconds, our
optimized sequence only takes 84 seconds, achieving a four-fold time reduction
as desired, while maintaining high performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14269">Source-Free Domain Adaptation for RGB-D Semantic Segmentation with Vision Transformers. (arXiv:2305.14269v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rizzoli_G/0/1/0/all/0/1">Giulia Rizzoli</a>, <a href="http://arxiv.org/find/cs/1/au:+Shenaj_D/0/1/0/all/0/1">Donald Shenaj</a>, <a href="http://arxiv.org/find/cs/1/au:+Zanuttigh_P/0/1/0/all/0/1">Pietro Zanuttigh</a></p>
<p>With the increasing availability of depth sensors, multimodal frameworks that
combine color information with depth data are gaining interest. However, ground
truth data for semantic segmentation is burdensome to provide, thus making
domain adaptation a significant research area. Yet most domain adaptation
methods are not able to effectively handle multimodal data. Specifically, we
address the challenging source-free domain adaptation setting where the
adaptation is performed without reusing source data. We propose MISFIT:
MultImodal Source-Free Information fusion Transformer, a depth-aware framework
which injects depth data into a segmentation module based on vision
transformers at multiple stages, namely at the input, feature and output
levels. Color and depth style transfer helps early-stage domain alignment while
re-wiring self-attention between modalities creates mixed features, allowing
the extraction of better semantic content. Furthermore, a depth-based entropy
minimization strategy is also proposed to adaptively weight regions at
different distances. Our framework, which is also the first approach using
RGB-D vision transformers for source-free semantic segmentation, shows
noticeable performance improvements with respect to standard strategies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.17903">Deeply Coupled Cross-Modal Prompt Learning. (arXiv:2305.17903v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xuejing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1">Wei Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Jinghui Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1">Rui Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Zhaojun Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_F/0/1/0/all/0/1">Fei Tan</a></p>
<p>Recent advancements in multimodal foundation models (e.g., CLIP) have
excelled in zero-shot generalization. Prompt tuning involved in the knowledge
transfer from foundation models to downstream tasks has gained significant
attention recently. Existing prompt-tuning methods in cross-modal learning,
however, either solely focus on language branch, or learn vision-language
interaction in a shallow mechanism. In this context, we propose a Deeply
coupled Cross-modal Prompt learning (DCP) method based on CLIP. DCP flexibly
accommodates the interplay between vision and language with a Cross-Modal
Prompt Attention (CMPA) mechanism, which enables the mutual exchange of
respective representation through a well-connected multi-head attention module
progressively and strongly. We then conduct comprehensive few-shot learning
experiments on 11 image classification datasets and analyze the robustness to
domain shift as well. Thorough experimental analysis evidently demonstrates the
superb few-shot generalization and compelling domain adaption capacity of a
well-executed DCP. The code can be found at https://github.com/GingL/CMPA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.18731">Epistemic Graph: A Plug-And-Play Module For Hybrid Representation Learning. (arXiv:2305.18731v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1">Jin Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1">Yangzhou Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1">Zhongchao Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1">Xin Geng</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1">Jianping Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Rui_Y/0/1/0/all/0/1">Yong Rui</a></p>
<p>In recent years, deep models have achieved remarkable success in various
vision tasks. However, their performance heavily relies on large training
datasets. In contrast, humans exhibit hybrid learning, seamlessly integrating
structured knowledge for cross-domain recognition or relying on a smaller
amount of data samples for few-shot learning. Motivated by this human-like
epistemic process, we aim to extend hybrid learning to computer vision tasks by
integrating structured knowledge with data samples for more effective
representation learning. Nevertheless, this extension faces significant
challenges due to the substantial gap between structured knowledge and deep
features learned from data samples, encompassing both dimensions and knowledge
granularity. In this paper, a novel Epistemic Graph Layer (EGLayer) is
introduced to enable hybrid learning, enhancing the exchange of information
between deep features and a structured knowledge graph. Our EGLayer is composed
of three major parts, including a local graph module, a query aggregation
model, and a novel correlation alignment loss function to emulate human
epistemic ability. Serving as a plug-and-play module that can replace the
standard linear classifier, EGLayer significantly improves the performance of
deep models. Extensive experiments demonstrates that EGLayer can greatly
enhance representation learning for the tasks of cross-domain recognition and
few-shot learning, and the visualization of knowledge graphs can aid in model
interpretation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.01531">PanoGRF: Generalizable Spherical Radiance Fields for Wide-baseline Panoramas. (arXiv:2306.01531v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yan-Pei Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yuan-Chen Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1">Ying Shan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Song-Hai Zhang</a></p>
<p>Achieving an immersive experience enabling users to explore virtual
environments with six degrees of freedom (6DoF) is essential for various
applications such as virtual reality (VR). Wide-baseline panoramas are commonly
used in these applications to reduce network bandwidth and storage
requirements. However, synthesizing novel views from these panoramas remains a
key challenge. Although existing neural radiance field methods can produce
photorealistic views under narrow-baseline and dense image captures, they tend
to overfit the training views when dealing with \emph{wide-baseline} panoramas
due to the difficulty in learning accurate geometry from sparse $360^{\circ}$
views. To address this problem, we propose PanoGRF, Generalizable Spherical
Radiance Fields for Wide-baseline Panoramas, which construct spherical radiance
fields incorporating $360^{\circ}$ scene priors. Unlike generalizable radiance
fields trained on perspective images, PanoGRF avoids the information loss from
panorama-to-perspective conversion and directly aggregates geometry and
appearance features of 3D sample points from each panoramic view based on
spherical projection. Moreover, as some regions of the panorama are only
visible from one view while invisible from others under wide baseline settings,
PanoGRF incorporates $360^{\circ}$ monocular depth priors into spherical depth
estimation to improve the geometry features. Experimental results on multiple
panoramic datasets demonstrate that PanoGRF significantly outperforms
state-of-the-art generalizable view synthesis methods for wide-baseline
panoramas (e.g., OmniSyn) and perspective images (e.g., IBRNet, NeuRay).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.01546">Publicly available datasets of breast histopathology H&amp;E whole-slide images: A scoping review. (arXiv:2306.01546v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Tafavvoghi_M/0/1/0/all/0/1">Masoud Tafavvoghi</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Bongo_L/0/1/0/all/0/1">Lars Ailo Bongo</a> (2), <a href="http://arxiv.org/find/eess/1/au:+Shvetsov_N/0/1/0/all/0/1">Nikita Shvetsov</a> (2), <a href="http://arxiv.org/find/eess/1/au:+Busund_L/0/1/0/all/0/1">Lill-Tove Rasmussen Busund</a> (3), <a href="http://arxiv.org/find/eess/1/au:+Mollersen_K/0/1/0/all/0/1">Kajsa M&#xf8;llersen</a> (1) ((1) Department of Community Medicine, UiT The Arctic University of Norway, Troms&#xf8;, Norway, (2) Department of Computer Science, UiT The Arctic University of Norway, Troms&#xf8;, Norway, (3) Department of Medical Biology, UiT The Arctic University of Norway, Troms&#xf8;, Norway)</p>
<p>Advancements in digital pathology and computing resources have made a
significant impact in the field of computational pathology for breast cancer
diagnosis and treatment. However, access to high-quality labeled
histopathological images of breast cancer is a big challenge that limits the
development of accurate and robust deep learning models. In this scoping
review, we identified the publicly available datasets of breast H&amp;E stained
whole-slide images (WSI) that can be used to develop deep learning algorithms.
We systematically searched nine scientific literature databases and nine
research data repositories and found 17 publicly available datasets containing
10385 H&amp;E WSIs of breast cancer. Moreover, we reported image metadata and
characteristics for each dataset to assist researchers in selecting proper
datasets for specific tasks in breast cancer computational pathology. In
addition, we compiled two lists of breast H&amp;E patches and private datasets as
supplementary resources for researchers. Notably, only 28% of the included
articles utilized multiple datasets, and only 14% used an external validation
set, suggesting that the performance of other developed models may be
susceptible to overestimation. The TCGA-BRCA was used in 52% of the selected
studies. This dataset has a considerable selection bias that can impact the
robustness and generalizability of the trained algorithms. There is also a lack
of consistent metadata reporting of breast WSI datasets that can be an issue in
developing accurate deep learning models, indicating the necessity of
establishing explicit guidelines for documenting breast WSI dataset
characteristics and metadata.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.01923">The Surprising Effectiveness of Diffusion Models for Optical Flow and Monocular Depth Estimation. (arXiv:2306.01923v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Saxena_S/0/1/0/all/0/1">Saurabh Saxena</a>, <a href="http://arxiv.org/find/cs/1/au:+Herrmann_C/0/1/0/all/0/1">Charles Herrmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Hur_J/0/1/0/all/0/1">Junhwa Hur</a>, <a href="http://arxiv.org/find/cs/1/au:+Kar_A/0/1/0/all/0/1">Abhishek Kar</a>, <a href="http://arxiv.org/find/cs/1/au:+Norouzi_M/0/1/0/all/0/1">Mohammad Norouzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1">Deqing Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Fleet_D/0/1/0/all/0/1">David J. Fleet</a></p>
<p>Denoising diffusion probabilistic models have transformed image generation
with their impressive fidelity and diversity. We show that they also excel in
estimating optical flow and monocular depth, surprisingly, without
task-specific architectures and loss functions that are predominant for these
tasks. Compared to the point estimates of conventional regression-based
methods, diffusion models also enable Monte Carlo inference, e.g., capturing
uncertainty and ambiguity in flow and depth. With self-supervised pre-training,
the combined use of synthetic and real data for supervised training, and
technical innovations (infilling and step-unrolled denoising diffusion
training) to handle noisy-incomplete training data, and a simple form of
coarse-to-fine refinement, one can train state-of-the-art diffusion models for
depth and optical flow estimation. Extensive experiments focus on quantitative
performance against benchmarks, ablations, and the model's ability to capture
uncertainty and multimodality, and impute missing values. Our model, DDVM
(Denoising Diffusion Vision Model), obtains a state-of-the-art relative depth
error of 0.074 on the indoor NYU benchmark and an Fl-all outlier rate of 3.26\%
on the KITTI optical flow benchmark, about 25\% better than the best published
method. For an overview see https://diffusion-vision.github.io.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.02000">Context-PIPs: Persistent Independent Particles Demands Spatial Context Features. (arXiv:2306.02000v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bian_W/0/1/0/all/0/1">Weikang Bian</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zhaoyang Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1">Xiaoyu Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1">Yitong Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yijin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hongsheng Li</a></p>
<p>We tackle the problem of Persistent Independent Particles (PIPs), also called
Tracking Any Point (TAP), in videos, which specifically aims at estimating
persistent long-term trajectories of query points in videos. Previous methods
attempted to estimate these trajectories independently to incorporate longer
image sequences, therefore, ignoring the potential benefits of incorporating
spatial context features. We argue that independent video point tracking also
demands spatial context features. To this end, we propose a novel framework
Context-PIPs, which effectively improves point trajectory accuracy by
aggregating spatial context features in videos. Context-PIPs contains two main
modules: 1) a SOurse Feature Enhancement (SOFE) module, and 2) a TArget Feature
Aggregation (TAFA) module. Context-PIPs significantly improves PIPs all-sided,
reducing 11.4% Average Trajectory Error of Occluded Points (ATE-Occ) on CroHD
and increasing 11.8% Average Percentage of Correct Keypoint (A-PCK) on
TAP-Vid-Kinectics. Demos are available at
https://wkbian.github.io/Projects/Context-PIPs/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.07096">Global and Local Semantic Completion Learning for Vision-Language Pre-training. (arXiv:2306.07096v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tu_R/0/1/0/all/0/1">Rong-Cheng Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1">Yatai Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1">Jie Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kong_W/0/1/0/all/0/1">Weijie Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1">Chengfei Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Wenzhe Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hongfa Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yujiu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wei Liu</a></p>
<p>Cross-modal alignment plays a crucial role in vision-language pre-training
(VLP) models, enabling them to capture meaningful associations across different
modalities. For this purpose, numerous masked modeling tasks have been proposed
for VLP to further promote cross-modal interactions. The core idea of previous
masked modeling tasks is to focus on reconstructing the masked tokens based on
visible context for learning local-local alignment. However, most of them pay
little attention to the global semantic features generated for the masked data,
resulting in a limited cross-modal alignment ability of global representations
to local features of the other modality. Therefore, in this paper, we propose a
novel Global and Local Semantic Completion Learning (GLSCL) task to facilitate
global-local alignment and local-local alignment simultaneously. Specifically,
the GLSCL task complements the missing semantics of masked data and recovers
global and local features by cross-modal interactions. Our GLSCL consists of
masked global semantic completion (MGSC) and masked local token completion
(MLTC). MGSC promotes learning more representative global features, which have
a great impact on the performance of downstream tasks, while MLTC reconstructs
modal-fusion local tokens, further enhancing accurate comprehension of
multimodal data. To evaluate the proposed approaches on cross-modal alignment,
we develop a validation benchmark called ALIGN-BENCH. Moreover, we present a
flexible vision encoder, enabling our model to simultaneously perform
image-text and video-text multimodal tasks. Experimental results show that our
proposed method obtains state-of-the-art performance on various vision-language
benchmarks, such as visual question answering, image-text retrieval, and
video-text retrieval.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.13394">MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models. (arXiv:2306.13394v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1">Chaoyou Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1">Peixian Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yunhang Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1">Yulei Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Mengdan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1">Xu Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jinrui Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1">Xiawu Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Ke Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xing Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yunsheng Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1">Rongrong Ji</a></p>
<p>Multimodal Large Language Model (MLLM) relies on the powerful LLM to perform
multimodal tasks, showing amazing emergent abilities in recent studies, such as
writing poems based on an image. However, it is difficult for these case
studies to fully reflect the performance of MLLM, lacking a comprehensive
evaluation. In this paper, we fill in this blank, presenting the first
comprehensive MLLM Evaluation benchmark MME. It measures both perception and
cognition abilities on a total of 14 subtasks. In order to avoid data leakage
that may arise from direct use of public datasets for evaluation, the
annotations of instruction-answer pairs are all manually designed. The concise
instruction design allows us to fairly compare MLLMs, instead of struggling in
prompt engineering. Besides, with such an instruction, we can also easily carry
out quantitative statistics. A total of 30 advanced MLLMs are comprehensively
evaluated on our MME, which not only suggests that existing MLLMs still have a
large room for improvement, but also reveals the potential directions for the
subsequent model optimization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04749">Divide, Evaluate, and Refine: Evaluating and Improving Text-to-Image Alignment with Iterative VQA Feedback. (arXiv:2307.04749v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1">Jaskirat Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1">Liang Zheng</a></p>
<p>The field of text-conditioned image generation has made unparalleled progress
with the recent advent of latent diffusion models. While remarkable, as the
complexity of given text input increases, the state-of-the-art diffusion models
may still fail in generating images which accurately convey the semantics of
the given prompt. Furthermore, it has been observed that such misalignments are
often left undetected by pretrained multi-modal models such as CLIP. To address
these problems, in this paper we explore a simple yet effective decompositional
approach towards both evaluation and improvement of text-to-image alignment. In
particular, we first introduce a Decompositional-Alignment-Score which given a
complex prompt decomposes it into a set of disjoint assertions. The alignment
of each assertion with generated images is then measured using a VQA model.
Finally, alignment scores for different assertions are combined aposteriori to
give the final text-to-image alignment score. Experimental analysis reveals
that the proposed alignment metric shows significantly higher correlation with
human ratings as opposed to traditional CLIP, BLIP scores. Furthermore, we also
find that the assertion level alignment scores provide a useful feedback which
can then be used in a simple iterative procedure to gradually increase the
expression of different assertions in the final image outputs. Human user
studies indicate that the proposed approach surpasses previous state-of-the-art
by 8.7% in overall text-to-image alignment accuracy. Project page for our paper
is available at https://1jsingh.github.io/divide-evaluate-and-refine
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06472">Early Autism Diagnosis based on Path Signature and Siamese Unsupervised Feature Compressor. (arXiv:2307.06472v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1">Zhuowen Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1">Xinyao Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhengwang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Li Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1">Gang Li</a></p>
<p>Autism Spectrum Disorder (ASD) has been emerging as a growing public health
threat. Early diagnosis of ASD is crucial for timely, effective intervention
and treatment. However, conventional diagnosis methods based on communications
and behavioral patterns are unreliable for children younger than 2 years of
age. Given evidences of neurodevelopmental abnormalities in ASD infants, we
resort to a novel deep learning-based method to extract key features from the
inherently scarce, class-imbalanced, and heterogeneous structural MR images for
early autism diagnosis. Specifically, we propose a Siamese verification
framework to extend the scarce data, and an unsupervised compressor to
alleviate data imbalance by extracting key features. We also proposed weight
constraints to cope with sample heterogeneity by giving different samples
different voting weights during validation, and we used Path Signature to
unravel meaningful developmental features from the two-time point data
longitudinally. We further extracted machine learning focused brain regions for
autism diagnosis. Extensive experiments have shown that our method performed
well under practical scenarios, transcending existing machine learning methods
and providing anatomical insights for autism early diagnosis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10518">Interactive Segmentation for Diverse Gesture Types Without Context. (arXiv:2307.10518v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Myers_Dean_J/0/1/0/all/0/1">Josh Myers-Dean</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1">Yifei Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Price_B/0/1/0/all/0/1">Brian Price</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_W/0/1/0/all/0/1">Wilson Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Gurari_D/0/1/0/all/0/1">Danna Gurari</a></p>
<p>Interactive segmentation entails a human marking an image to guide how a
model either creates or edits a segmentation. Our work addresses limitations of
existing methods: they either only support one gesture type for marking an
image (e.g., either clicks or scribbles) or require knowledge of the gesture
type being employed, and require specifying whether marked regions should be
included versus excluded in the final segmentation. We instead propose a
simplified interactive segmentation task where a user only must mark an image,
where the input can be of any gesture type without specifying the gesture type.
We support this new task by introducing the first interactive segmentation
dataset with multiple gesture types as well as a new evaluation metric capable
of holistically evaluating interactive segmentation algorithms. We then analyze
numerous interactive segmentation algorithms, including ones adapted for our
novel task. While we observe promising performance overall, we also highlight
areas for future improvement. To facilitate further extensions of this work, we
publicly share our new dataset at https://github.com/joshmyersdean/dig.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.06444">TongueSAM: An Universal Tongue Segmentation Model Based on SAM with Zero-Shot. (arXiv:2308.06444v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1">Shan Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruan_Q/0/1/0/all/0/1">Qunsheng Ruan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1">Linjian Ma</a></p>
<p>Tongue segmentation serves as the primary step in automated TCM tongue
diagnosis, which plays a significant role in the diagnostic results. Currently,
numerous deep learning based methods have achieved promising results. However,
when confronted with tongue images that differ from the training set or possess
challenging backgrounds, these methods demonstrate limited performance. To
address this issue, this paper proposes a universal tongue segmentation model
named TongueSAM based on SAM (Segment Anything Model). SAM is a large-scale
pretrained interactive segmentation model known for its powerful zero-shot
generalization capability. Applying SAM to tongue segmentation leverages its
learned prior knowledge from natural images, enabling the achievement of
zero-shot segmentation for various types of tongue images. In this study, a
Prompt Generator based on object detection is integrated into SAM to enable an
end-to-end automated tongue segmentation method. Experiments demonstrate that
TongueSAM achieves exceptional performance across various of tongue
segmentation datasets, particularly under zero-shot. Even when dealing with
challenging background tongue images, TongueSAM achieves a mIoU of 95.23\%
under zero-shot conditions, surpassing other segmentation methods. As far as we
know, this is the first application of large-scale pretrained model for tongue
segmentation. The project mentioned in this paper is currently publicly
available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.08333">Improving Depth Gradient Continuity in Transformers: A Comparative Study on Monocular Depth Estimation with CNN. (arXiv:2308.08333v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1">Jiawei Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1">Tong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaofeng Zhang</a></p>
<p>Monocular depth estimation is an ongoing challenge in computer vision. Recent
progress with Transformer models has demonstrated notable advantages over
conventional CNNs in this area. However, there's still a gap in understanding
how these models prioritize different regions in 2D images and how these
regions affect depth estimation performance. To explore the differences between
Transformers and CNNs, we employ a sparse pixel approach to contrastively
analyze the distinctions between the two. Our findings suggest that while
Transformers excel in handling global context and intricate textures, they lag
behind CNNs in preserving depth gradient continuity. To further enhance the
performance of Transformer models in monocular depth estimation, we propose the
Depth Gradient Refinement (DGR) module that refines depth estimation through
high-order differentiation, feature fusion, and recalibration. Additionally, we
leverage optimal transport theory, treating depth maps as spatial probability
distributions, and employ the optimal transport distance as a loss function to
optimize our model. Experimental results demonstrate that models integrated
with the plug-and-play Depth Gradient Refinement (DGR) module and the proposed
loss function enhance performance without increasing complexity and
computational costs on both outdoor KITTI and indoor NYU-Depth-v2 datasets.
This research not only offers fresh insights into the distinctions between
Transformers and CNNs in depth estimation but also paves the way for novel
depth estimation methodologies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12462">Overcoming Generic Knowledge Loss with Selective Parameter Update. (arXiv:2308.12462v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenxuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Janson_P/0/1/0/all/0/1">Paul Janson</a>, <a href="http://arxiv.org/find/cs/1/au:+Aljundi_R/0/1/0/all/0/1">Rahaf Aljundi</a>, <a href="http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1">Mohamed Elhoseiny</a></p>
<p>Foundation models encompass an extensive knowledge base and offer remarkable
transferability. However, this knowledge becomes outdated or insufficient over
time. The challenge lies in continuously updating foundation models to
accommodate novel information while retaining their original capabilities.
Leveraging the fact that foundation models have initial knowledge on various
tasks and domains, we propose a novel approach that, instead of updating all
parameters equally, localizes the updates to a sparse set of parameters
relevant to the task being learned. We strike a balance between efficiency and
new task performance, while maintaining the transferability and
generalizability of foundation models. We extensively evaluate our method on
foundational vision-language models with a diverse spectrum of continual
learning tasks. Our method achieves improvements on the accuracy of the newly
learned tasks up to 7% while preserving the pretraining knowledge with a
negligible decrease of 0.9% on a representative control set accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.02959">A Non-Invasive Interpretable NAFLD Diagnostic Method Combining TCM Tongue Features. (arXiv:2309.02959v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Cao_S/0/1/0/all/0/1">Shan Cao</a>, <a href="http://arxiv.org/find/eess/1/au:+Ruan_Q/0/1/0/all/0/1">Qunsheng Ruan</a>, <a href="http://arxiv.org/find/eess/1/au:+Wu_Q/0/1/0/all/0/1">Qingfeng Wu</a>, <a href="http://arxiv.org/find/eess/1/au:+Lin_W/0/1/0/all/0/1">Weiqiang Lin</a></p>
<p>Non-alcoholic fatty liver disease (NAFLD) is a clinicopathological syndrome
characterized by hepatic steatosis resulting from the exclusion of alcohol and
other identifiable liver-damaging factors. It has emerged as a leading cause of
chronic liver disease worldwide. Currently, the conventional methods for NAFLD
detection are expensive and not suitable for users to perform daily
diagnostics. To address this issue, this study proposes a non-invasive and
interpretable NAFLD diagnostic method, the required user-provided indicators
are only Gender, Age, Height, Weight, Waist Circumference, Hip Circumference,
and tongue image. This method involves merging patients' physiological
indicators with tongue features, which are then input into a fusion network
named SelectorNet. SelectorNet combines attention mechanisms with feature
selection mechanisms, enabling it to autonomously learn the ability to select
important features. The experimental results show that the proposed method
achieves an accuracy of 77.22\% using only non-invasive data, and it also
provides compelling interpretability matrices. This study contributes to the
early diagnosis of NAFLD and the intelligent advancement of TCM tongue
diagnosis. The project mentioned in this paper is currently publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.08769">The Use of Multi-Scale Fiducial Markers To Aid Takeoff and Landing Navigation by Rotorcraft. (arXiv:2309.08769v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jongwon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1">Su Yeon Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Bretl_T/0/1/0/all/0/1">Timothy Bretl</a></p>
<p>This paper quantifies the performance of visual SLAM that leverages
multi-scale fiducial markers (i.e., artificial landmarks that can be detected
at a wide range of distances) to show its potential for reliable takeoff and
landing navigation in rotorcraft. Prior work has shown that square markers with
a black-and-white pattern of grid cells can be used to improve the performance
of visual SLAM with color cameras. We extend this prior work to allow nested
marker layouts. We evaluate performance during semi-autonomous takeoff and
landing operations in a variety of environmental conditions by a DJI Matrice
300 RTK rotorcraft with two FLIR Blackfly color cameras, using RTK GNSS to
obtain ground truth pose estimates. Performance measures include absolute
trajectory error and the fraction of the number of estimated poses to the total
frame. We release all of our results -- our dataset and the code of the
implementation of the visual SLAM with fiducial markers -- to the public as
open-source.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00500">Self-Supervised Open-Ended Classification with Small Visual Language Models. (arXiv:2310.00500v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Derakhshani_M/0/1/0/all/0/1">Mohammad Mahdi Derakhshani</a>, <a href="http://arxiv.org/find/cs/1/au:+Najdenkoska_I/0/1/0/all/0/1">Ivona Najdenkoska</a>, <a href="http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1">Cees G. M. Snoek</a>, <a href="http://arxiv.org/find/cs/1/au:+Worring_M/0/1/0/all/0/1">Marcel Worring</a>, <a href="http://arxiv.org/find/cs/1/au:+Asano_Y/0/1/0/all/0/1">Yuki M. Asano</a></p>
<p>We present Self-Context Adaptation (SeCAt), a self-supervised approach that
unlocks few-shot abilities for open-ended classification with small visual
language models. Our approach imitates image captions in a self-supervised way
based on clustering a large pool of images followed by assigning
semantically-unrelated names to clusters. By doing so, we construct a training
signal consisting of interleaved sequences of image and pseudocaption pairs and
a query image, which we denote as the 'self-context' sequence. Based on this
signal the model is trained to produce the right pseudo-caption. We demonstrate
the performance and flexibility of SeCAt on several multimodal few-shot
datasets, spanning various granularities. By using models with approximately 1B
parameters we outperform the few-shot abilities of much larger models, such as
Frozen and FROMAGe. SeCAt opens new possibilities for research and applications
in open-ended few-shot learning that otherwise requires access to large or
proprietary models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03059">Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models. (arXiv:2310.03059v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_I/0/1/0/all/0/1">Ivan Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Ray Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Zoey Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xianzheng Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhigang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1">Bin Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xuelong Li</a></p>
<p>The popularity of pre-trained large models has revolutionized downstream
tasks across diverse fields, such as language, vision, and multi-modality. To
minimize the adaption cost for downstream tasks, many Parameter-Efficient
Fine-Tuning (PEFT) techniques are proposed for language and 2D image
pre-trained models. However, the specialized PEFT method for 3D pre-trained
models is still under-explored. To this end, we introduce Point-PEFT, a novel
framework for adapting point cloud pre-trained models with minimal learnable
parameters. Specifically, for a pre-trained 3D model, we freeze most of its
parameters, and only tune the newly added PEFT modules on downstream tasks,
which consist of a Point-prior Prompt and a Geometry-aware Adapter. The
Point-prior Prompt adopts a set of learnable prompt tokens, for which we
propose to construct a memory bank with domain-specific knowledge, and utilize
a parameter-free attention to enhance the prompt tokens. The Geometry-aware
Adapter aims to aggregate point cloud features within spatial neighborhoods to
capture fine-grained geometric information through local interactions.
Extensive experiments indicate that our Point-PEFT can achieve better
performance than the full fine-tuning on various downstream tasks, while using
only 5% of the trainable parameters, demonstrating the efficiency and
effectiveness of our approach. Code will be released at
https://github.com/Even-JK/PEFT-3D.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08577">Visual Data-Type Understanding does not emerge from Scaling Vision-Language Models. (arXiv:2310.08577v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Udandarao_V/0/1/0/all/0/1">Vishaal Udandarao</a>, <a href="http://arxiv.org/find/cs/1/au:+Burg_M/0/1/0/all/0/1">Max F. Burg</a>, <a href="http://arxiv.org/find/cs/1/au:+Albanie_S/0/1/0/all/0/1">Samuel Albanie</a>, <a href="http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1">Matthias Bethge</a></p>
<p>Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of Visual Data-Type Identification, a basic perceptual skill
with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual data-types, spanning four broad categories. An extensive zero-shot
evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a nuanced
performance landscape. While VLMs are reasonably good at identifying certain
stylistic \textit{data-types}, such as cartoons and sketches, they struggle
with simpler data-types arising from basic manipulations like image rotations
or additive noise. Our findings reveal that (i) model scaling alone yields
marginal gains for contrastively-trained models like CLIP, and (ii) there is a
pronounced drop in performance for the largest auto-regressively trained VLMs
like OpenFlamingo. This finding points to a blind spot in current frontier
VLMs: they excel in recognizing semantic content but fail to acquire an
understanding of visual data-types through scaling. By analyzing the
pre-training distributions of these models and incorporating data-type
information into the captions during fine-tuning, we achieve a significant
enhancement in performance. By exploring this previously uncharted task, we aim
to set the stage for further advancing VLMs to equip them with visual data-type
understanding. Code and datasets are released at
https://github.com/bethgelab/DataTypeIdentification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19378">Few-shot Hybrid Domain Adaptation of Image Generators. (arXiv:2310.19378v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hengjia Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_L/0/1/0/all/0/1">Linxuan Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yuqi Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1">Tu Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zheng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenxiao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_X/0/1/0/all/0/1">Xiaohui Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1">Xiaobo Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xiaofei He</a></p>
<p>Can a pre-trained generator be adapted to the hybrid of multiple target
domains and generate images with integrated attributes of them? In this work,
we introduce a new task -- Few-shot Hybrid Domain Adaptation (HDA). Given a
source generator and several target domains, HDA aims to acquire an adapted
generator that preserves the integrated attributes of all target domains,
without overriding the source domain's characteristics. Compared with Domain
Adaptation (DA), HDA offers greater flexibility and versatility to adapt
generators to more composite and expansive domains. Simultaneously, HDA also
presents more challenges than DA as we have access only to images from
individual target domains and lack authentic images from the hybrid domain. To
address this issue, we introduce a discriminator-free framework that directly
encodes different domains' images into well-separable subspaces. To achieve
HDA, we propose a novel directional subspace loss comprised of a distance loss
and a direction loss. Concretely, the distance loss blends the attributes of
all target domains by reducing the distances from generated images to all
target subspaces. The direction loss preserves the characteristics from the
source domain by guiding the adaptation along the perpendicular to subspaces.
Experiments show that our method can obtain numerous domain-specific attributes
in a single adapted generator, which surpasses the baseline methods in semantic
similarity, image fidelity, and cross-domain consistency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06791">InfMLLM: A Unified Framework for Visual-Language Tasks. (arXiv:2311.06791v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1">Qiang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhibin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1">Wei Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yinghui Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1">Yuan Qi</a></p>
<p>Large language models (LLMs) have proven their remarkable versatility in
handling a comprehensive range of language-centric applications. To expand
LLMs' capabilities to a broader spectrum of modal inputs, multimodal large
language models (MLLMs) have attracted growing interest. This work delves into
enabling LLMs to tackle more vision-language-related tasks, particularly image
captioning, visual question answering (VQA,) and visual grounding. To this end,
we implemented a three-stage training scheme: starting with lightweight
alignment pretraining, then moderate-weight multitask hybrid training, and
finally, LLM fine-tuning to improve instruction following capability.
Throughout the training process, the requirements on GPU memory gradually
increase. To effectively manage the number of visual embeddings passed to the
LLM while preserving their positional information, we introduce a
straightforward visual adapter module dubbed pool-adapter. Our experiments
demonstrate that preserving the positional information of visual embeddings
through the pool-adapter is particularly beneficial for tasks like visual
grounding. We name our proposed approach InfMLLM and have evaluated it
extensively on various benchmark datasets. Our results demonstrate that InfMLLM
achieves either state-of-the-art (SOTA) performance or performance comparable
to recent MLLMs. The code and model will be made open-source at:
\url{https://github.com/mightyzau/InfMLLM}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10121">Slide-SAM: Medical SAM Meets Sliding Window. (arXiv:2311.10121v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Quan_Q/0/1/0/all/0/1">Quan Quan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_F/0/1/0/all/0/1">Fenghe Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zikang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1">Heqin Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1">S.Kevin Zhou</a></p>
<p>The Segment Anything Model (SAM) has achieved a notable success in
two-dimensional image segmentation in natural images. However, the substantial
gap between medical and natural images hinders its direct application to
medical image segmentation tasks. Particularly in 3D medical images, SAM
struggles to learn contextual relationships between slices, limiting its
practical applicability. Moreover, applying 2D SAM to 3D images requires
prompting the entire volume, which is time- and label-consuming. To address
these problems, we propose Slide-SAM, which treats a stack of three adjacent
slices as a prediction window. It firstly takes three slices from a 3D volume
and point- or bounding box prompts on the central slice as inputs to predict
segmentation masks for all three slices. Subsequently, the masks of the top and
bottom slices are then used to generate new prompts for adjacent slices.
Finally, step-wise prediction can be achieved by sliding the prediction window
forward or backward through the entire volume. Our model is trained on multiple
public and private medical datasets and demonstrates its effectiveness through
extensive 3D segmetnation experiments, with the help of minimal prompts. Code
is available at \url{https://github.com/Curli-quan/Slide-SAM}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.11312">Optimizing rgb-d semantic segmentation through multi-modal interaction and pooling attention. (arXiv:2311.11312v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shuai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1">Minghong Xie</a></p>
<p>Semantic segmentation of RGB-D images involves understanding the appearance
and spatial relationships of objects within a scene, which requires careful
consideration of various factors. However, in indoor environments, the simple
input of RGB and depth images often results in a relatively limited acquisition
of semantic and spatial information, leading to suboptimal segmentation
outcomes. To address this, we propose the Multi-modal Interaction and Pooling
Attention Network (MIPANet), a novel approach designed to harness the
interactive synergy between RGB and depth modalities, optimizing the
utilization of complementary information. Specifically, we incorporate a
Multi-modal Interaction Fusion Module (MIM) into the deepest layers of the
network. This module is engineered to facilitate the fusion of RGB and depth
information, allowing for mutual enhancement and correction. Additionally, we
introduce a Pooling Attention Module (PAM) at various stages of the encoder.
This module serves to amplify the features extracted by the network and
integrates the module's output into the decoder in a targeted manner,
significantly improving semantic segmentation performance. Our experimental
results demonstrate that MIPANet outperforms existing methods on two indoor
scene datasets, NYUDv2 and SUN-RGBD, underscoring its effectiveness in
enhancing RGB-D semantic segmentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13750">Towards Transferable Multi-modal Perception Representation Learning for Autonomy: NeRF-Supervised Masked AutoEncoder. (arXiv:2311.13750v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiaohao Xu</a></p>
<p>This work proposes a unified self-supervised pre-training framework for
transferable multi-modal perception representation learning via masked
multi-modal reconstruction in Neural Radiance Field (NeRF), namely
NeRF-Supervised Masked AutoEncoder (NS-MAE). Specifically, conditioned on
certain view directions and locations, multi-modal embeddings extracted from
corrupted multi-modal input signals, i.e., Lidar point clouds and images, are
rendered into projected multi-modal feature maps via neural rendering. Then,
original multi-modal signals serve as reconstruction targets for the rendered
multi-modal feature maps to enable self-supervised representation learning.
Extensive experiments show that the representation learned via NS-MAE shows
promising transferability for diverse multi-modal and single-modal (camera-only
and Lidar-only) perception models on diverse 3D perception downstream tasks (3D
object detection and BEV map segmentation) with diverse amounts of fine-tuning
labeled data. Moreover, we empirically find that NS-MAE enjoys the synergy of
both the mechanism of masked autoencoder and neural radiance field. We hope
this study can inspire exploration of more general multi-modal representation
learning for autonomous agents.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14948">Effective Backdoor Mitigation Depends on the Pre-training Objective. (arXiv:2311.14948v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Verma_S/0/1/0/all/0/1">Sahil Verma</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhatt_G/0/1/0/all/0/1">Gantavya Bhatt</a>, <a href="http://arxiv.org/find/cs/1/au:+Schwarzschild_A/0/1/0/all/0/1">Avi Schwarzschild</a>, <a href="http://arxiv.org/find/cs/1/au:+Singhal_S/0/1/0/all/0/1">Soumye Singhal</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1">Arnav Mohanty Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_C/0/1/0/all/0/1">Chirag Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Dickerson_J/0/1/0/all/0/1">John P Dickerson</a>, <a href="http://arxiv.org/find/cs/1/au:+Bilmes_J/0/1/0/all/0/1">Jeff Bilmes</a></p>
<p>Despite the advanced capabilities of contemporary machine learning (ML)
models, they remain vulnerable to adversarial and backdoor attacks. This
vulnerability is particularly concerning in real-world deployments, where
compromised models may exhibit unpredictable behavior in critical scenarios.
Such risks are heightened by the prevalent practice of collecting massive,
internet-sourced datasets for pre-training multimodal models, as these datasets
may harbor backdoors. Various techniques have been proposed to mitigate the
effects of backdooring in these models such as CleanCLIP which is the current
state-of-the-art approach. In this work, we demonstrate that the efficacy of
CleanCLIP in mitigating backdoors is highly dependent on the particular
objective used during model pre-training. We observe that stronger pre-training
objectives correlate with harder to remove backdoors behaviors. We show this by
training multimodal models on two large datasets consisting of 3 million (CC3M)
and 6 million (CC6M) datapoints, under various pre-training objectives,
followed by poison removal using CleanCLIP. We find that CleanCLIP is
ineffective when stronger pre-training objectives are used, even with extensive
hyperparameter tuning. Our findings underscore critical considerations for ML
practitioners who pre-train models using large-scale web-curated data and are
concerned about potential backdoor threats. Notably, our results suggest that
simpler pre-training objectives are more amenable to effective backdoor
removal. This insight is pivotal for practitioners seeking to balance the
trade-offs between using stronger pre-training objectives and security against
backdoor attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17082">DreamPropeller: Supercharge Text-to-3D Generation with Parallel Sampling. (arXiv:2311.17082v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1">Linqi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Shih_A/0/1/0/all/0/1">Andy Shih</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1">Chenlin Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1">Stefano Ermon</a></p>
<p>Recent methods such as Score Distillation Sampling (SDS) and Variational
Score Distillation (VSD) using 2D diffusion models for text-to-3D generation
have demonstrated impressive generation quality. However, the long generation
time of such algorithms significantly degrades the user experience. To tackle
this problem, we propose DreamPropeller, a drop-in acceleration algorithm that
can be wrapped around any existing text-to-3D generation pipeline based on
score distillation. Our framework generalizes Picard iterations, a classical
algorithm for parallel sampling an ODE path, and can account for non-ODE paths
such as momentum-based gradient updates and changes in dimensions during the
optimization process as in many cases of 3D generation. We show that our
algorithm trades parallel compute for wallclock time and empirically achieves
up to 4.7x speedup with a negligible drop in generation quality for all tested
frameworks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18260">Consensus, dissensus and synergy between clinicians and specialist foundation models in radiology report generation. (arXiv:2311.18260v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Tanno_R/0/1/0/all/0/1">Ryutaro Tanno</a>, <a href="http://arxiv.org/find/eess/1/au:+Barrett_D/0/1/0/all/0/1">David G.T. Barrett</a>, <a href="http://arxiv.org/find/eess/1/au:+Sellergren_A/0/1/0/all/0/1">Andrew Sellergren</a>, <a href="http://arxiv.org/find/eess/1/au:+Ghaisas_S/0/1/0/all/0/1">Sumedh Ghaisas</a>, <a href="http://arxiv.org/find/eess/1/au:+Dathathri_S/0/1/0/all/0/1">Sumanth Dathathri</a>, <a href="http://arxiv.org/find/eess/1/au:+See_A/0/1/0/all/0/1">Abigail See</a>, <a href="http://arxiv.org/find/eess/1/au:+Welbl_J/0/1/0/all/0/1">Johannes Welbl</a>, <a href="http://arxiv.org/find/eess/1/au:+Singhal_K/0/1/0/all/0/1">Karan Singhal</a>, <a href="http://arxiv.org/find/eess/1/au:+Azizi_S/0/1/0/all/0/1">Shekoofeh Azizi</a>, <a href="http://arxiv.org/find/eess/1/au:+Tu_T/0/1/0/all/0/1">Tao Tu</a>, <a href="http://arxiv.org/find/eess/1/au:+Schaekermann_M/0/1/0/all/0/1">Mike Schaekermann</a>, <a href="http://arxiv.org/find/eess/1/au:+May_R/0/1/0/all/0/1">Rhys May</a>, <a href="http://arxiv.org/find/eess/1/au:+Lee_R/0/1/0/all/0/1">Roy Lee</a>, <a href="http://arxiv.org/find/eess/1/au:+Man_S/0/1/0/all/0/1">SiWai Man</a>, <a href="http://arxiv.org/find/eess/1/au:+Ahmed_Z/0/1/0/all/0/1">Zahra Ahmed</a>, <a href="http://arxiv.org/find/eess/1/au:+Mahdavi_S/0/1/0/all/0/1">Sara Mahdavi</a>, <a href="http://arxiv.org/find/eess/1/au:+Belgrave_D/0/1/0/all/0/1">Danielle Belgrave</a>, <a href="http://arxiv.org/find/eess/1/au:+Natarajan_V/0/1/0/all/0/1">Vivek Natarajan</a>, <a href="http://arxiv.org/find/eess/1/au:+Shetty_S/0/1/0/all/0/1">Shravya Shetty</a>, <a href="http://arxiv.org/find/eess/1/au:+Kohli_P/0/1/0/all/0/1">Pushmeet Kohli</a>, <a href="http://arxiv.org/find/eess/1/au:+Huang_P/0/1/0/all/0/1">Po-Sen Huang</a>, <a href="http://arxiv.org/find/eess/1/au:+Karthikesalingam_A/0/1/0/all/0/1">Alan Karthikesalingam</a>, <a href="http://arxiv.org/find/eess/1/au:+Ktena_I/0/1/0/all/0/1">Ira Ktena</a></p>
<p>Radiology reports are an instrumental part of modern medicine, informing key
clinical decisions such as diagnosis and treatment. The worldwide shortage of
radiologists, however, restricts access to expert care and imposes heavy
workloads, contributing to avoidable errors and delays in report delivery.
While recent progress in automated report generation with vision-language
models offer clear potential in ameliorating the situation, the path to
real-world adoption has been stymied by the challenge of evaluating the
clinical quality of AI-generated reports. In this study, we build a
state-of-the-art report generation system for chest radiographs,
\textit{Flamingo-CXR}, by fine-tuning a well-known vision-language foundation
model on radiology data. To evaluate the quality of the AI-generated reports, a
group of 16 certified radiologists provide detailed evaluations of AI-generated
and human written reports for chest X-rays from an intensive care setting in
the United States and an inpatient setting in India. At least one radiologist
(out of two per case) preferred the AI report to the ground truth report in
over 60$\%$ of cases for both datasets. Amongst the subset of AI-generated
reports that contain errors, the most frequently cited reasons were related to
the location and finding, whereas for human written reports, most mistakes were
related to severity and finding. This disparity suggested potential
complementarity between our AI system and human experts, prompting us to
develop an assistive scenario in which \textit{Flamingo-CXR} generates a
first-draft report, which is subsequently revised by a clinician. This is the
first demonstration of clinician-AI collaboration for report writing, and the
resultant reports are assessed to be equivalent or preferred by at least one
radiologist to reports written by experts alone in 80$\%$ of in-patient cases
and 60$\%$ of intensive care cases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01623">Universal Segmentation at Arbitrary Granularity with Language Instruction. (arXiv:2312.01623v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Cairong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yitong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiahao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yujiu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1">Yansong Tang</a></p>
<p>This paper aims to achieve universal segmentation of arbitrary semantic
level. Despite significant progress in recent years, specialist segmentation
approaches are limited to specific tasks and data distribution. Retraining a
new model for adaptation to new scenarios or settings takes expensive
computation and time cost, which raises the demand for versatile and universal
segmentation model that can cater to various granularity. Although some
attempts have been made for unifying different segmentation tasks or
generalization to various scenarios, limitations in the definition of paradigms
and input-output spaces make it difficult for them to achieve accurate
understanding of content at arbitrary granularity. To this end, we present
UniLSeg, a universal segmentation model that can perform segmentation at any
semantic level with the guidance of language instructions. For training
UniLSeg, we reorganize a group of tasks from original diverse distributions
into a unified data format, where images with texts describing segmentation
targets as input and corresponding masks are output. Combined with a automatic
annotation engine for utilizing numerous unlabeled data, UniLSeg achieves
excellent performance on various tasks and settings, surpassing both specialist
and unified segmentation models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01650">Adaptive Confidence Threshold for ByteTrack in Multi-Object Tracking. (arXiv:2312.01650v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1">Linh Van Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Hussain_M/0/1/0/all/0/1">Muhammad Ishfaq Hussain</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">JongHyun Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jeongbae Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeon_M/0/1/0/all/0/1">Moongu Jeon</a></p>
<p>We investigate the application of ByteTrack in the realm of multiple object
tracking. ByteTrack, a simple tracking algorithm, enables the simultaneous
tracking of multiple objects by strategically incorporating detections with a
low confidence threshold. Conventionally, objects are initially associated with
high confidence threshold detections. When the association between objects and
detections becomes ambiguous, ByteTrack extends the association to lower
confidence threshold detections. One notable drawback of the existing ByteTrack
approach is its reliance on a fixed threshold to differentiate between high and
low-confidence detections. In response to this limitation, we introduce a novel
and adaptive approach. Our proposed method entails a dynamic adjustment of the
confidence threshold, leveraging insights derived from overall detections.
Through experimentation, we demonstrate the effectiveness of our adaptive
confidence threshold technique while maintaining running time compared to
ByteTrack.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02010">Towards Learning a Generalist Model for Embodied Navigation. (arXiv:2312.02010v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1">Duo Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Shijia Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1">Lin Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1">Yiwu Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Liwei Wang</a></p>
<p>Building a generalist agent that can interact with the world is the
intriguing target of AI systems, thus spurring the research for embodied
navigation, where an agent is required to navigate according to instructions or
respond to queries. Despite the major progress attained, previous works
primarily focus on task-specific agents and lack generalizability to unseen
scenarios. Recently, LLMs have presented remarkable capabilities across various
fields, and provided a promising opportunity for embodied navigation. Drawing
on this, we propose the first generalist model for embodied navigation,
NaviLLM. It adapts LLMs to embodied navigation by introducing schema-based
instruction. The schema-based instruction flexibly casts various tasks into
generation problems, thereby unifying a wide range of tasks. This approach
allows us to integrate diverse data sources from various datasets into the
training, equipping NaviLLM with a wide range of capabilities required by
embodied navigation. We conduct extensive experiments to evaluate the
performance and generalizability of our model. The experimental results
demonstrate that our unified model achieves state-of-the-art performance on
CVDN, SOON, and ScanQA. Specifically, it surpasses the previous
stats-of-the-art method by a significant margin of 29% in goal progress on
CVDN. Moreover, our model also demonstrates strong generalizability and
presents impressive results on unseen tasks, e.g., embodied question answering
and 3D captioning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02168">The SVHN Dataset Is Deceptive for Probabilistic Generative Models Due to a Distribution Mismatch. (arXiv:2312.02168v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1">Tim Z. Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zenn_J/0/1/0/all/0/1">Johannes Zenn</a>, <a href="http://arxiv.org/find/cs/1/au:+Bamler_R/0/1/0/all/0/1">Robert Bamler</a></p>
<p>The Street View House Numbers (SVHN) dataset is a popular benchmark dataset
in deep learning. Originally designed for digit classification tasks, the SVHN
dataset has been widely used as a benchmark for various other tasks including
generative modeling. However, with this work, we aim to warn the community
about an issue of the SVHN dataset as a benchmark for generative modeling
tasks: we discover that the official split into training set and test set of
the SVHN dataset are not drawn from the same distribution. We empirically show
that this distribution mismatch has little impact on the classification task
(which may explain why this issue has not been detected before), but it
severely affects the evaluation of probabilistic generative models, such as
Variational Autoencoders and diffusion models. As a workaround, we propose to
mix and re-split the official training and test set when SVHN is used for tasks
other than classification. We publish a new split and the indices we used to
create it at https://jzenn.github.io/svhn-remix/ .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02439">Let&#x27;s Think Outside the Box: Exploring Leap-of-Thought in Large Language Models with Creative Humor Generation. (arXiv:2312.02439v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhong_S/0/1/0/all/0/1">Shanshan Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zhongzhan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1">Shanghua Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_W/0/1/0/all/0/1">Wushao Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1">Liang Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zitnik_M/0/1/0/all/0/1">Marinka Zitnik</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1">Pan Zhou</a></p>
<p>Chain-of-Thought (CoT) guides large language models (LLMs) to reason
step-by-step, and can motivate their logical reasoning ability. While effective
for logical tasks, CoT is not conducive to creative problem-solving which often
requires out-of-box thoughts and is crucial for innovation advancements. In
this paper, we explore the Leap-of-Thought (LoT) abilities within LLMs -- a
non-sequential, creative paradigm involving strong associations and knowledge
leaps. To this end, we study LLMs on the popular Oogiri game which needs
participants to have good creativity and strong associative thinking for
responding unexpectedly and humorously to the given image, text, or both, and
thus is suitable for LoT study. Then to investigate LLMs' LoT ability in the
Oogiri game, we first build a multimodal and multilingual Oogiri-GO dataset
which contains over 130,000 samples from the Oogiri game, and observe the
insufficient LoT ability or failures of most existing LLMs on the Oogiri game.
Accordingly, we introduce a creative Leap-of-Thought (CLoT) paradigm to improve
LLM's LoT ability. CLoT first formulates the Oogiri-GO dataset into
LoT-oriented instruction tuning data to train pretrained LLM for achieving
certain LoT humor generation and discrimination abilities. Then CLoT designs an
explorative self-refinement that encourages the LLM to generate more creative
LoT data via exploring parallels between seemingly unrelated concepts and
selects high-quality data to train itself for self-refinement. CLoT not only
excels in humor generation in the Oogiri game but also boosts creative
abilities in various tasks like cloud guessing game and divergent association
task. These findings advance our understanding and offer a pathway to improve
LLMs' creative capacities for innovative applications across domains. The
dataset, code, and models will be released online.
https://zhongshsh.github.io/CLoT/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02663">FaceStudio: Put Your Face Everywhere in Seconds. (arXiv:2312.02663v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1">Yuxuan Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Rui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yichao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Gege Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1">Pei Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1">Gang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_B/0/1/0/all/0/1">Bin Fu</a></p>
<p>This study investigates identity-preserving image synthesis, an intriguing
task in image generation that seeks to maintain a subject's identity while
adding a personalized, stylistic touch. Traditional methods, such as Textual
Inversion and DreamBooth, have made strides in custom image creation, but they
come with significant drawbacks. These include the need for extensive resources
and time for fine-tuning, as well as the requirement for multiple reference
images. To overcome these challenges, our research introduces a novel approach
to identity-preserving synthesis, with a particular focus on human images. Our
model leverages a direct feed-forward mechanism, circumventing the need for
intensive fine-tuning, thereby facilitating quick and efficient image
generation. Central to our innovation is a hybrid guidance framework, which
combines stylized images, facial images, and textual prompts to guide the image
generation process. This unique combination enables our model to produce a
variety of applications, such as artistic portraits and identity-blended
images. Our experimental results, including both qualitative and quantitative
evaluations, demonstrate the superiority of our method over existing baseline
models and previous works, particularly in its remarkable efficiency and
ability to preserve the subject's identity with high fidelity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02896">BenchLMM: Benchmarking Cross-style Visual Capability of Large Multimodal Models. (arXiv:2312.02896v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1">Rizhao Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1">Zirui Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_D/0/1/0/all/0/1">Dayan Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhenhao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1">Xing Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_C/0/1/0/all/0/1">Chenyu Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kot_A/0/1/0/all/0/1">Alex Kot</a></p>
<p>Large Multimodal Models (LMMs) such as GPT-4V and LLaVA have shown remarkable
capabilities in visual reasoning with common image styles. However, their
robustness against diverse style shifts, crucial for practical applications,
remains largely unexplored. In this paper, we propose a new benchmark,
BenchLMM, to assess the robustness of LMMs against three different styles:
artistic image style, imaging sensor style, and application style, where each
style has five sub-styles. Utilizing BenchLMM, we comprehensively evaluate
state-of-the-art LMMs and reveal: 1) LMMs generally suffer performance
degradation when working with other styles; 2) An LMM performs better than
another model in common style does not guarantee its superior performance in
other styles; 3) LMMs' reasoning capability can be enhanced by prompting LMMs
to predict the style first, based on which we propose a versatile and
training-free method for improving LMMs; 4) An intelligent LMM is expected to
interpret the causes of its errors when facing stylistic variations. We hope
that our benchmark and analysis can shed new light on developing more
intelligent and versatile LMMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02934">WoVoGen: World Volume-aware Diffusion for Controllable Multi-camera Driving Scene Generation. (arXiv:2312.02934v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Jiachen Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Ze Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiahui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zeyu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Li Zhang</a></p>
<p>Generating multi-camera street-view videos is critical for augmenting
autonomous driving datasets, addressing the urgent demand for extensive and
varied data. Due to the limitations in diversity and challenges in handling
lighting conditions, traditional rendering-based methods are increasingly being
supplanted by diffusion-based methods. However, a significant challenge in
diffusion-based methods is ensuring that the generated sensor data preserve
both intra-world consistency and inter-sensor coherence. To address these
challenges, we combine an additional explicit world volume and propose the
World Volume-aware Multi-camera Driving Scene Generator (WoVoGen). This system
is specifically designed to leverage 4D world volume as a foundational element
for video generation. Our model operates in two distinct phases: (i)
envisioning the future 4D temporal world volume based on vehicle control
sequences, and (ii) generating multi-camera videos, informed by this envisioned
4D temporal world volume and sensor interconnectivity. The incorporation of the
4D world volume empowers WoVoGen not only to generate high-quality street-view
videos in response to vehicle control inputs but also to facilitate scene
editing tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02617">DreaMo: Articulated 3D Reconstruction From A Single Casual Video. (arXiv:2312.02617v1 [cs.CV] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tu_T/0/1/0/all/0/1">Tao Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Ming-Feng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1">Chieh Hubert Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1">Yen-Chi Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1">Min Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Ming-Hsuan Yang</a></p>
<p>Articulated 3D reconstruction has valuable applications in various domains,
yet it remains costly and demands intensive work from domain experts. Recent
advancements in template-free learning methods show promising results with
monocular videos. Nevertheless, these approaches necessitate a comprehensive
coverage of all viewpoints of the subject in the input video, thus limiting
their applicability to casually captured videos from online sources. In this
work, we study articulated 3D shape reconstruction from a single and casually
captured internet video, where the subject's view coverage is incomplete. We
propose DreaMo that jointly performs shape reconstruction while solving the
challenging low-coverage regions with view-conditioned diffusion prior and
several tailored regularizations. In addition, we introduce a skeleton
generation strategy to create human-interpretable skeletons from the learned
neural bones and skinning weights. We conduct our study on a self-collected
internet video collection characterized by incomplete view coverage. DreaMo
shows promising quality in novel-view rendering, detailed articulated shape
reconstruction, and skeleton generation. Extensive qualitative and quantitative
studies validate the efficacy of each proposed component, and show existing
methods are unable to solve correct geometry due to the incomplete view
coverage.
</p>
</p>
</div>

    </div>
    </body>
    