<!DOCTYPE html>
<html>
<head>
<title>2023-11-16-cs-cl</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2311.07582">Evaluating the Potential of Leading Large Language Models in Reasoning Biology Questions. (arXiv:2311.07582v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1">Xinyu Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Holmes_J/0/1/0/all/0/1">Jason Holmes</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yiwei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhengliang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gan_Q/0/1/0/all/0/1">Qi Gan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zihao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jianli Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1">Yusong Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Teng_Y/0/1/0/all/0/1">Yuxi Teng</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1">Tian Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1">Hongtu Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tianming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1">Yajun Yan</a></p>
<p>Recent advances in Large Language Models (LLMs) have presented new
opportunities for integrating Artificial General Intelligence (AGI) into
biological research and education. This study evaluated the capabilities of
leading LLMs, including GPT-4, GPT-3.5, PaLM2, Claude2, and SenseNova, in
answering conceptual biology questions. The models were tested on a
108-question multiple-choice exam covering biology topics in molecular biology,
biological techniques, metabolic engineering, and synthetic biology. Among the
models, GPT-4 achieved the highest average score of 90 and demonstrated the
greatest consistency across trials with different prompts. The results
indicated GPT-4's proficiency in logical reasoning and its potential to aid
biology research through capabilities like data analysis, hypothesis
generation, and knowledge integration. However, further development and
validation are still required before the promise of LLMs in accelerating
biological discovery can be realized.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07583">Cross-Dialect Sentence Transformation: A Comparative Analysis of Language Models for Adapting Sentences to British English. (arXiv:2311.07583v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1">Shruti Dutta</a>, <a href="http://arxiv.org/find/cs/1/au:+Mookherjee_S/0/1/0/all/0/1">Shashwat Mookherjee</a></p>
<p>This study explores linguistic distinctions among American, Indian, and Irish
English dialects and assesses various Language Models (LLMs) in their ability
to generate British English translations from these dialects. Using cosine
similarity analysis, the study measures the linguistic proximity between
original British English translations and those produced by LLMs for each
dialect. The findings reveal that Indian and Irish English translations
maintain notably high similarity scores, suggesting strong linguistic alignment
with British English. In contrast, American English exhibits slightly lower
similarity, reflecting its distinct linguistic traits. Additionally, the choice
of LLM significantly impacts translation quality, with Llama-2-70b consistently
demonstrating superior performance. The study underscores the importance of
selecting the right model for dialect translation, emphasizing the role of
linguistic expertise and contextual understanding in achieving accurate
translations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07584">Performance Prediction of Data-Driven Knowledge summarization of High Entropy Alloys (HEAs) literature implementing Natural Language Processing algorithms. (arXiv:2311.07584v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1">Akshansh Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Jatti_V/0/1/0/all/0/1">Vijaykumar S Jatti</a>, <a href="http://arxiv.org/find/cs/1/au:+More_V/0/1/0/all/0/1">Vaishnavi More</a>, <a href="http://arxiv.org/find/cs/1/au:+Dasgupta_A/0/1/0/all/0/1">Anish Dasgupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Dixit_D/0/1/0/all/0/1">Devarrishi Dixit</a>, <a href="http://arxiv.org/find/cs/1/au:+Sefene_E/0/1/0/all/0/1">Eyob Messele Sefene</a></p>
<p>The ability to interpret spoken language is connected to natural language
processing. It involves teaching the AI how words relate to one another, how
they are meant to be used, and in what settings. The goal of natural language
processing (NLP) is to get a machine intelligence to process words the same way
a human brain does. This enables machine intelligence to interpret, arrange,
and comprehend textual data by processing the natural language. The technology
can comprehend what is communicated, whether it be through speech or writing
because AI pro-cesses language more quickly than humans can. In the present
study, five NLP algorithms, namely, Geneism, Sumy, Luhn, Latent Semantic
Analysis (LSA), and Kull-back-Liebler (KL) al-gorithm, are implemented for the
first time for the knowledge summarization purpose of the High Entropy Alloys
(HEAs). The performance prediction of these algorithms is made by using the
BLEU score and ROUGE score. The results showed that the Luhn algorithm has the
highest accuracy score for the knowledge summarization tasks compared to the
other used algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07585">Input Reconstruction Attack against Vertical Federated Large Language Models. (arXiv:2311.07585v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1">Fei Zheng</a></p>
<p>Recently, large language models (LLMs) have drawn extensive attention from
academia and the public, due to the advent of the ChatGPT. While LLMs show
their astonishing ability in text generation for various tasks, privacy
concerns limit their usage in real-life businesses. More specifically, either
the user's inputs (the user sends the query to the model-hosting server) or the
model (the user downloads the complete model) itself will be revealed during
the usage. Vertical federated learning (VFL) is a promising solution to this
kind of problem. It protects both the user's input and the knowledge of the
model by splitting the model into a bottom part and a top part, which is
maintained by the user and the model provider, respectively. However, in this
paper, we demonstrate that in LLMs, VFL fails to protect the user input since
it is simple and cheap to reconstruct the input from the intermediate
embeddings. Experiments show that even with a commercial GPU, the input
sentence can be reconstructed in only one second. We also discuss several
possible solutions to enhance the privacy of vertical federated LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07587">Frontier Language Models are not Robust to Adversarial Arithmetic, or &quot;What do I need to say so you agree 2+2=5?. (arXiv:2311.07587v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Freeman_C/0/1/0/all/0/1">C. Daniel Freeman</a>, <a href="http://arxiv.org/find/cs/1/au:+Culp_L/0/1/0/all/0/1">Laura Culp</a>, <a href="http://arxiv.org/find/cs/1/au:+Parisi_A/0/1/0/all/0/1">Aaron Parisi</a>, <a href="http://arxiv.org/find/cs/1/au:+Bileschi_M/0/1/0/all/0/1">Maxwell L Bileschi</a>, <a href="http://arxiv.org/find/cs/1/au:+Elsayed_G/0/1/0/all/0/1">Gamaleldin F Elsayed</a>, <a href="http://arxiv.org/find/cs/1/au:+Rizkowsky_A/0/1/0/all/0/1">Alex Rizkowsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Simpson_I/0/1/0/all/0/1">Isabelle Simpson</a>, <a href="http://arxiv.org/find/cs/1/au:+Alemi_A/0/1/0/all/0/1">Alex Alemi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nova_A/0/1/0/all/0/1">Azade Nova</a>, <a href="http://arxiv.org/find/cs/1/au:+Adlam_B/0/1/0/all/0/1">Ben Adlam</a>, <a href="http://arxiv.org/find/cs/1/au:+Bohnet_B/0/1/0/all/0/1">Bernd Bohnet</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishra_G/0/1/0/all/0/1">Gaurav Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Sedghi_H/0/1/0/all/0/1">Hanie Sedghi</a>, <a href="http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1">Igor Mordatch</a>, <a href="http://arxiv.org/find/cs/1/au:+Gur_I/0/1/0/all/0/1">Izzeddin Gur</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jaehoon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Co_Reyes_J/0/1/0/all/0/1">JD Co-Reyes</a>, <a href="http://arxiv.org/find/cs/1/au:+Pennington_J/0/1/0/all/0/1">Jeffrey Pennington</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Kelvin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Swersky_K/0/1/0/all/0/1">Kevin Swersky</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahajan_K/0/1/0/all/0/1">Kshiteej Mahajan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1">Lechao Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1">Rosanne Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kornblith_S/0/1/0/all/0/1">Simon Kornblith</a>, <a href="http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1">Noah Constant</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1">Peter J. Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Novak_R/0/1/0/all/0/1">Roman Novak</a>, <a href="http://arxiv.org/find/cs/1/au:+Vikram_S/0/1/0/all/0/1">Sharad Vikram</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1">Yundi Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Fiedel_N/0/1/0/all/0/1">Noah Fiedel</a>, <a href="http://arxiv.org/find/cs/1/au:+Sohl_Dickstein_J/0/1/0/all/0/1">Jascha Sohl-Dickstein</a></p>
<p>We introduce and study the problem of adversarial arithmetic, which provides
a simple yet challenging testbed for language model alignment. This problem is
comprised of arithmetic questions posed in natural language, with an arbitrary
adversarial string inserted before the question is complete. Even in the simple
setting of 1-digit addition problems, it is easy to find adversarial prompts
that make all tested models (including PaLM2, GPT4, Claude2) misbehave, and
even to steer models to a particular wrong answer. We additionally provide a
simple algorithm for finding successful attacks by querying those same models,
which we name "prompt inversion rejection sampling" (PIRS). We finally show
that models can be partially hardened against these attacks via reinforcement
learning and via agentic constitutional loops. However, we were not able to
make a language model fully robust against adversarial arithmetic attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07588">NLQxform: A Language Model-based Question to SPARQL Transformer. (arXiv:2311.07588v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Ruijie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhiruo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Rossetto_L/0/1/0/all/0/1">Luca Rossetto</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruosch_F/0/1/0/all/0/1">Florian Ruosch</a>, <a href="http://arxiv.org/find/cs/1/au:+Bernstein_A/0/1/0/all/0/1">Abraham Bernstein</a></p>
<p>In recent years, scholarly data has grown dramatically in terms of both scale
and complexity. It becomes increasingly challenging to retrieve information
from scholarly knowledge graphs that include large-scale heterogeneous
relationships, such as authorship, affiliation, and citation, between various
types of entities, e.g., scholars, papers, and organizations. As part of the
Scholarly QALD Challenge, this paper presents a question-answering (QA) system
called NLQxform, which provides an easy-to-use natural language interface to
facilitate accessing scholarly knowledge graphs. NLQxform allows users to
express their complex query intentions in natural language questions. A
transformer-based language model, i.e., BART, is employed to translate
questions into standard SPARQL queries, which can be evaluated to retrieve the
required information. According to the public leaderboard of the Scholarly QALD
Challenge at ISWC 2023 (Task 1: DBLP-QUAD - Knowledge Graph Question Answering
over DBLP), NLQxform achieved an F1 score of 0.85 and ranked first on the QA
task, demonstrating the competitiveness of the system.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07589">Dialogizer: Context-aware Conversational-QA Dataset Generation from Textual Sources. (arXiv:2311.07589v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hwang_Y/0/1/0/all/0/1">Yerin Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Yongil Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Bae_H/0/1/0/all/0/1">Hyunkyung Bae</a>, <a href="http://arxiv.org/find/cs/1/au:+Bang_J/0/1/0/all/0/1">Jeesoo Bang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hwanhee Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_K/0/1/0/all/0/1">Kyomin Jung</a></p>
<p>To address the data scarcity issue in Conversational question answering
(ConvQA), a dialog inpainting method, which utilizes documents to generate
ConvQA datasets, has been proposed. However, the original dialog inpainting
model is trained solely on the dialog reconstruction task, resulting in the
generation of questions with low contextual relevance due to insufficient
learning of question-answer alignment. To overcome this limitation, we propose
a novel framework called Dialogizer, which has the capability to automatically
generate ConvQA datasets with high contextual relevance from textual sources.
The framework incorporates two training tasks: question-answer matching (QAM)
and topic-aware dialog generation (TDG). Moreover, re-ranking is conducted
during the inference phase based on the contextual relevance of the generated
questions. Using our framework, we produce four ConvQA datasets by utilizing
documents from multiple domains as the primary source. Through automatic
evaluation using diverse metrics, as well as human evaluation, we validate that
our proposed framework exhibits the ability to generate datasets of higher
quality compared to the baseline dialog inpainting model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07590">Technical Report: Large Language Models can Strategically Deceive their Users when Put Under Pressure. (arXiv:2311.07590v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Scheurer_J/0/1/0/all/0/1">J&#xe9;r&#xe9;my Scheurer</a>, <a href="http://arxiv.org/find/cs/1/au:+Balesni_M/0/1/0/all/0/1">Mikita Balesni</a>, <a href="http://arxiv.org/find/cs/1/au:+Hobbhahn_M/0/1/0/all/0/1">Marius Hobbhahn</a></p>
<p>We demonstrate a situation in which Large Language Models, trained to be
helpful, harmless, and honest, can display misaligned behavior and
strategically deceive their users about this behavior without being instructed
to do so. Concretely, we deploy GPT-4 as an agent in a realistic, simulated
environment, where it assumes the role of an autonomous stock trading agent.
Within this environment, the model obtains an insider tip about a lucrative
stock trade and acts upon it despite knowing that insider trading is
disapproved of by company management. When reporting to its manager, the model
consistently hides the genuine reasons behind its trading decision. We perform
a brief investigation of how this behavior varies under changes to the setting,
such as removing model access to a reasoning scratchpad, attempting to prevent
the misaligned behavior by changing system instructions, changing the amount of
pressure the model is under, varying the perceived risk of getting caught, and
making other simple changes to the environment. To our knowledge, this is the
first demonstration of Large Language Models trained to be helpful, harmless,
and honest, strategically deceiving their users in a realistic situation
without direct instructions or training for deception.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07591">Identification of Books That are Suitable for Middle School Students Using Artificial Neural Networks. (arXiv:2311.07591v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Niksarli_A/0/1/0/all/0/1">Alp Niksarli</a>, <a href="http://arxiv.org/find/cs/1/au:+Gorgu_S/0/1/0/all/0/1">Sadik Ozan Gorgu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gencer_E/0/1/0/all/0/1">Ege Gencer</a></p>
<p>Reading right books contributes to children's imagination and brain
development, enhances their language and emotional comprehension abilities, and
strengthens their relationships with others. Building upon the critical role of
reading books in individual development, this paper aims to develop an
algorithm that determines the suitability of books for middle school students
by analyzing their structural and semantic features. Using methods described,
an algorithm will be created that can be utilized by institutions and
individuals responsible for children's education, such as the Ministry of
National Education officials and schools. This algorithm will facilitate the
selection of books to be taught at the middle school level. With the algorithm,
the book selection process for the middle school curriculum can be expedited,
and it will serve as a preliminary reference source for those who evaluate
books by reading them. In this paper, the Python programming language was
employed, utilizing natural language processing methods. Additionally, an
artificial neural network (ANN) was trained using the data which had been
preprocessed to construct an original dataset. To train this network, suitable
books for middle school students were provided by the MEB, Oxford and Cambridge
and with content assessed based on the "R" criterion, and inappropriate books
for middle school students in terms of content were included. This trained
neural network achieved a 90.06% consistency rate in determining the
appropriateness of the test-provided books. Considering the obtained findings,
it can be concluded that the developed software has achieved the desired
objective.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07592">Hallucination-minimized Data-to-answer Framework for Financial Decision-makers. (arXiv:2311.07592v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Roychowdhury_S/0/1/0/all/0/1">Sohini Roychowdhury</a>, <a href="http://arxiv.org/find/cs/1/au:+Alvarez_A/0/1/0/all/0/1">Andres Alvarez</a>, <a href="http://arxiv.org/find/cs/1/au:+Moore_B/0/1/0/all/0/1">Brian Moore</a>, <a href="http://arxiv.org/find/cs/1/au:+Krema_M/0/1/0/all/0/1">Marko Krema</a>, <a href="http://arxiv.org/find/cs/1/au:+Gelpi_M/0/1/0/all/0/1">Maria Paz Gelpi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_F/0/1/0/all/0/1">Federico Martin Rodriguez</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_A/0/1/0/all/0/1">Angel Rodriguez</a>, <a href="http://arxiv.org/find/cs/1/au:+Cabrejas_J/0/1/0/all/0/1">Jose Ramon Cabrejas</a>, <a href="http://arxiv.org/find/cs/1/au:+Serrano_P/0/1/0/all/0/1">Pablo Martinez Serrano</a>, <a href="http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1">Punit Agrawal</a>, <a href="http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1">Arijit Mukherjee</a></p>
<p>Large Language Models (LLMs) have been applied to build several automation
and personalized question-answering prototypes so far. However, scaling such
prototypes to robust products with minimized hallucinations or fake responses
still remains an open challenge, especially in niche data-table heavy domains
such as financial decision making. In this work, we present a novel
Langchain-based framework that transforms data tables into hierarchical textual
data chunks to enable a wide variety of actionable question answering. First,
the user-queries are classified by intention followed by automated retrieval of
the most relevant data chunks to generate customized LLM prompts per query.
Next, the custom prompts and their responses undergo multi-metric scoring to
assess for hallucinations and response confidence. The proposed system is
optimized with user-query intention classification, advanced prompting, data
scaling capabilities and it achieves over 90% confidence scores for a variety
of user-queries responses ranging from {What, Where, Why, How, predict, trend,
anomalies, exceptions} that are crucial for financial decision making
applications. The proposed data to answers framework can be extended to other
analytical domains such as sales and payroll to ensure optimal hallucination
control guardrails.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07593">Follow-Up Differential Descriptions: Language Models Resolve Ambiguities for Image Classification. (arXiv:2311.07593v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Esfandiarpoor_R/0/1/0/all/0/1">Reza Esfandiarpoor</a>, <a href="http://arxiv.org/find/cs/1/au:+Bach_S/0/1/0/all/0/1">Stephen H. Bach</a></p>
<p>A promising approach for improving the performance of vision-language models
like CLIP for image classification is to extend the class descriptions (i.e.,
prompts) with related attributes, e.g., using brown sparrow instead of sparrow.
However, current zero-shot methods select a subset of attributes regardless of
commonalities between the target classes, potentially providing no useful
information that would have helped to distinguish between them. For instance,
they may use color instead of bill shape to distinguish between sparrows and
wrens, which are both brown. We propose Follow-up Differential Descriptions
(FuDD), a zero-shot approach that tailors the class descriptions to each
dataset and leads to additional attributes that better differentiate the target
classes. FuDD first identifies the ambiguous classes for each image, and then
uses a Large Language Model (LLM) to generate new class descriptions that
differentiate between them. The new class descriptions resolve the initial
ambiguity and help predict the correct label. In our experiments, FuDD
consistently outperforms generic description ensembles and naive LLM-generated
descriptions on 12 datasets. We show that differential descriptions are an
effective tool to resolve class ambiguities, which otherwise significantly
degrade the performance. We also show that high quality natural language class
descriptions produced by FuDD result in comparable performance to few-shot
adaptation methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07594">How to Bridge the Gap between Modalities: A Comprehensive Survey on Multimodal Large Language Model. (arXiv:2311.07594v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1">Shezheng Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaopeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shasha Li</a></p>
<p>This review paper explores Multimodal Large Language Models (MLLMs), which
integrate Large Language Models (LLMs) like GPT-4 to handle multimodal data
such as text and vision. MLLMs demonstrate capabilities like generating image
narratives and answering image-based questions, bridging the gap towards
real-world human-computer interactions and hinting at a potential pathway to
artificial general intelligence. However, MLLMs still face challenges in
processing the semantic gap in multimodality, which may lead to erroneous
generation, posing potential risks to society. Choosing the appropriate
modality alignment method is crucial, as improper methods might require more
parameters with limited performance improvement. This paper aims to explore
modality alignment methods for LLMs and their existing capabilities.
Implementing modality alignment allows LLMs to address environmental issues and
enhance accessibility. The study surveys existing modal alignment methods in
MLLMs into four groups: (1) Multimodal Converters that change data into
something LLMs can understand; (2) Multimodal Perceivers to improve how LLMs
perceive different types of data; (3) Tools Assistance for changing data into
one common format, usually text; and (4) Data-Driven methods that teach LLMs to
understand specific types of data in a dataset. This field is still in a phase
of exploration and experimentation, and we will organize and update various
existing research methods for multimodal information alignment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07598">Multi-Label Topic Model for Financial Textual Data. (arXiv:2311.07598v1 [q-fin.ST])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+Scherrmann_M/0/1/0/all/0/1">Moritz Scherrmann</a></p>
<p>This paper presents a multi-label topic model for financial texts like ad-hoc
announcements, 8-K filings, finance related news or annual reports. I train the
model on a new financial multi-label database consisting of 3,044 German ad-hoc
announcements that are labeled manually using 20 predefined, economically
motivated topics. The best model achieves a macro F1 score of more than 85%.
Translating the data results in an English version of the model with similar
performance. As application of the model, I investigate differences in stock
market reactions across topics. I find evidence for strong positive or negative
market reactions for some topics, like announcements of new Large Scale
Projects or Bankruptcy Filings, while I do not observe significant price
effects for some other topics. Furthermore, in contrast to previous studies,
the multi-label structure of the model allows to analyze the effects of
co-occurring topics on stock market reactions. For many cases, the reaction to
a specific topic depends heavily on the co-occurrence with other topics. For
example, if allocated capital from a Seasoned Equity Offering (SEO) is used for
restructuring a company in the course of a Bankruptcy Proceeding, the market
reacts positively on average. However, if that capital is used for covering
unexpected, additional costs from the development of new drugs, the SEO implies
negative reactions on average.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07611">Intentional Biases in LLM Responses. (arXiv:2311.07611v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Badyal_N/0/1/0/all/0/1">Nicklaus Badyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Jacoby_D/0/1/0/all/0/1">Derek Jacoby</a>, <a href="http://arxiv.org/find/cs/1/au:+Coady_Y/0/1/0/all/0/1">Yvonne Coady</a></p>
<p>In this study we intentionally introduce biases into large language model
responses in an attempt to create specific personas for interactive media
purposes. We explore the differences between open source models such as
Falcon-7b and the GPT-4 model from Open AI, and we quantify some differences in
responses afforded by the two systems. We find that the guardrails in the GPT-4
mixture of experts models with a supervisor, while useful in assuring AI
alignment in general, are detrimental in trying to construct personas with a
variety of uncommon viewpoints. This study aims to set the groundwork for
future exploration in intentional biases of large language models such that
these practices can be applied in the creative field, and new forms of media.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07617">CLAMP: A Contrastive Language And Molecule Pre-training Network. (arXiv:2311.07617v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Redkar_N/0/1/0/all/0/1">Neel Redkar</a></p>
<p>This paper highlights a shift in how to approach material generation. Instead
of material-to-material, we propose a language-to-material generation
architecture that utilizes millions of untapped data points. Using a web
scraper to collect crystal text pairs from open-source research papers, a
contrastive model can be trained using a convolutional graph neural network
encoder and a language encoder. This would allow unsupervised zero-shot
classification which can be trained by taking advantage of linguistic
structure. Without any specific training data, an ~82\% accuracy was achieved
and ~75\% accuracy for photocatalyst prediction with an extremely small
dataset. This novel network could ideally be cross-applied to any reaction that
can be described via text, opening completely new methods to think about 3D
chemical framework generation. In the full experiment diffusion models would
likely be incorporated to fully exploit the latent space.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07618">Large Language Models&#x27; Understanding of Math: Source Criticism and Extrapolation. (arXiv:2311.07618v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yousefzadeh_R/0/1/0/all/0/1">Roozbeh Yousefzadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1">Xuenan Cao</a></p>
<p>It has been suggested that large language models such as GPT-4 have acquired
some form of understanding beyond the correlations among the words in text
including some understanding of mathematics as well. Here, we perform a
critical inquiry into this claim by evaluating the mathematical understanding
of the GPT-4 model. Considering that GPT-4's training set is a secret, it is
not straightforward to evaluate whether the model's correct answers are based
on a mathematical understanding or based on replication of proofs that the
model has seen before. We specifically craft mathematical questions which their
formal proofs are not readily available on the web, proofs that are more likely
not seen by the GPT-4. We see that GPT-4 is unable to solve those problems
despite their simplicity. It is hard to find scientific evidence suggesting
that GPT-4 has acquired an understanding of even basic mathematical concepts. A
straightforward way to find failure modes of GPT-4 in theorem proving is to
craft questions where their formal proofs are not available on the web. Our
finding suggests that GPT-4's ability is to reproduce, rephrase, and polish the
mathematical proofs that it has seen before, and not in grasping mathematical
concepts. We also see that GPT-4's ability to prove mathematical theorems is
continuously expanding over time despite the claim that it is a fixed model. We
suggest that the task of proving mathematical theorems in formal language is
comparable to the methods used in search engines such as Google while
predicting the next word in a sentence may be a misguided approach, a recipe
that often leads to excessive extrapolation and eventual failures. Prompting
the GPT-4 over and over may benefit the GPT-4 and the OpenAI, but we question
whether it is valuable for machine learning or for theorem proving.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07635">Past as a Guide: Leveraging Retrospective Learning for Python Code Completion. (arXiv:2311.07635v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1">Seunggyoon Shin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1">Seunggyu Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1">Sungjoon Choi</a></p>
<p>This work presents Past as a Guide (PaG), a simple approach for Large
Language Models (LLMs) to improve the coding capabilities by integrating the
past history with interactive and iterative code refinements. To be specific,
inspired by human cognitive processes, the proposed method enables LLMs to
utilize previous programming and debugging experiences to enhance the Python
code completion tasks. The framework facilitates LLMs to iteratively refine the
Python code based on previous execution and debugging results and optimize
learning and reasoning capabilities. The proposed methodology achieved a 92\%
pass@1 on HumanEval, demonstrating the potential to advance the field by
leveraging retrospection from past experiences and interactive and iterative
refinement processes without external correctness indicators.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07682">Fuse to Forget: Bias Reduction and Selective Memorization through Model Fusion. (arXiv:2311.07682v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zaman_K/0/1/0/all/0/1">Kerem Zaman</a>, <a href="http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1">Leshem Choshen</a>, <a href="http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1">Shashank Srivastava</a></p>
<p>Model fusion research aims to aggregate the knowledge of multiple models to
enhance performance by combining their weights. In this work, we study the
inverse, investigating whether and how can model fusion interfere and reduce
unwanted knowledge. We delve into the effects of model fusion on the evolution
of learned shortcuts, social biases, and memorization capabilities in
fine-tuned language models. Through several experiments covering text
classification and generation tasks, our analysis highlights that shared
knowledge among models is usually enhanced during model fusion, while unshared
knowledge is usually lost or forgotten. Based on this observation, we
demonstrate the potential of model fusion as a debiasing tool and showcase its
efficacy in addressing privacy concerns associated with language models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07687">Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games. (arXiv:2311.07687v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sudhakar_A/0/1/0/all/0/1">Arjun Vaithilingam Sudhakar</a>, <a href="http://arxiv.org/find/cs/1/au:+Parthasarathi_P/0/1/0/all/0/1">Prasanna Parthasarathi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajendran_J/0/1/0/all/0/1">Janarthanan Rajendran</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1">Sarath Chandar</a></p>
<p>Large Language Models (LLMs) have demonstrated superior performance in
language understanding benchmarks. CALM, a popular approach, leverages
linguistic priors of LLMs -- GPT-2 -- for action candidate recommendations to
improve the performance in text games in Jericho without environment-provided
actions. However, CALM adapts GPT-2 with annotated human gameplays and keeps
the LLM fixed during the learning of the text based games. In this work, we
explore and evaluate updating LLM used for candidate recommendation during the
learning of the text based game as well to mitigate the reliance on the human
annotated gameplays, which are costly to acquire. We observe that by updating
the LLM during learning using carefully selected in-game transitions, we can
reduce the dependency on using human annotated game plays for fine-tuning the
LLMs. We conducted further analysis to study the transferability of the updated
LLMs and observed that transferring in-game trained models to other games did
not result in a consistent transfer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07689">MART: Improving LLM Safety with Multi-round Automatic Red-Teaming. (arXiv:2311.07689v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1">Suyu Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1">Chunting Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_R/0/1/0/all/0/1">Rui Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Khabsa_M/0/1/0/all/0/1">Madian Khabsa</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yi-Chia Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qifan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Jiawei Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1">Yuning Mao</a></p>
<p>Red-teaming is a common practice for mitigating unsafe behaviors in Large
Language Models (LLMs), which involves thoroughly assessing LLMs to identify
potential flaws and addressing them with responsible and accurate responses.
While effective, manual red-teaming is costly, and existing automatic
red-teaming typically discovers safety risks without addressing them. In this
paper, we propose a Multi-round Automatic Red-Teaming (MART) method, which
incorporates both automatic adversarial prompt writing and safe response
generation, significantly increasing red-teaming scalability and the safety of
the target LLM. Specifically, an adversarial LLM and a target LLM interplay
with each other in an iterative manner, where the adversarial LLM aims to
generate challenging prompts that elicit unsafe responses from the target LLM,
while the target LLM is fine-tuned with safety aligned data on these
adversarial prompts. In each round, the adversarial LLM crafts better attacks
on the updated target LLM, while the target LLM also improves itself through
safety fine-tuning. On adversarial prompt benchmarks, the violation rate of an
LLM with limited safety alignment reduces up to 84.7% after 4 rounds of MART,
achieving comparable performance to LLMs with extensive adversarial prompt
writing. Notably, model helpfulness on non-adversarial prompts remains stable
throughout iterations, indicating the target LLM maintains strong performance
on instruction following.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07692">On The Truthfulness of &#x27;Surprisingly Likely&#x27; Responses of Large Language Models. (arXiv:2311.07692v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Goel_N/0/1/0/all/0/1">Naman Goel</a></p>
<p>The surprisingly likely criterion in the seminal work of Prelec (the Bayesian
Truth Serum) guarantees truthfulness in a game-theoretic multi-agent setting,
by rewarding rational agents to maximise the expected information gain with
their answers w.r.t. their probabilistic beliefs. We investigate the relevance
of a similar criterion for responses of LLMs. We hypothesize that if the
surprisingly likely criterion works in LLMs, under certain conditions, the
responses that maximize the reward under this criterion should be more accurate
than the responses that only maximize the posterior probability. Using
benchmarks including the TruthfulQA benchmark and using openly available LLMs:
GPT-2 and LLaMA-2, we show that the method indeed improves the accuracy
significantly (for example, upto 24 percentage points aggregate improvement on
TruthfulQA and upto 70 percentage points improvement on individual categories
of questions).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07700">AuthentiGPT: Detecting Machine-Generated Text via Black-Box Language Models Denoising. (arXiv:2311.07700v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Zhen Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1">Shangdi Yu</a></p>
<p>Large language models (LLMs) have opened up enormous opportunities while
simultaneously posing ethical dilemmas. One of the major concerns is their
ability to create text that closely mimics human writing, which can lead to
potential misuse, such as academic misconduct, disinformation, and fraud. To
address this problem, we present AuthentiGPT, an efficient classifier that
distinguishes between machine-generated and human-written texts. Under the
assumption that human-written text resides outside the distribution of
machine-generated text, AuthentiGPT leverages a black-box LLM to denoise input
text with artificially added noise, and then semantically compares the denoised
text with the original to determine if the content is machine-generated. With
only one trainable parameter, AuthentiGPT eliminates the need for a large
training dataset, watermarking the LLM's output, or computing the
log-likelihood. Importantly, the detection capability of AuthentiGPT can be
easily adapted to any generative language model. With a 0.918 AUROC score on a
domain-specific dataset, AuthentiGPT demonstrates its effectiveness over other
commercial algorithms, highlighting its potential for detecting
machine-generated text in academic settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07703">Measuring Entrainment in Spontaneous Code-switched Speech. (arXiv:2311.07703v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_D/0/1/0/all/0/1">Debasmita Bhattacharya</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1">Siying Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1">Alayna Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hirschberg_J/0/1/0/all/0/1">Julia Hirschberg</a></p>
<p>It is well-known that interlocutors who entrain to one another have more
successful conversations than those who do not. Previous research has shown
that interlocutors entrain on linguistic features in both written and spoken
monolingual domains. More recent work on code-switched communication has also
shown preliminary evidence of entrainment on certain aspects of code-switching
(CSW). However, such studies of entrainment in code-switched domains have been
extremely few and restricted to human-machine textual interactions. Our work
studies code-switched spontaneous speech between humans by answering the
following questions: 1) Do patterns of written and spoken entrainment in
monolingual settings generalize to code-switched settings? 2) Do patterns of
entrainment on code-switching in generated text generalize to spontaneous
code-switched speech? We find evidence of affirmative answers to both of these
questions, with important implications for the potentially "universal" nature
of entrainment as a communication phenomenon, and potential applications in
inclusive and interactive speech technology.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07715">PolyIE: A Dataset of Information Extraction from Polymer Material Scientific Literature. (arXiv:2311.07715v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheung_J/0/1/0/all/0/1">Jerry Junyang Cheung</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1">Yuchen Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yinghao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Shetty_P/0/1/0/all/0/1">Pranav Shetty</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Wantian Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Grampurohit_S/0/1/0/all/0/1">Sanjeev Grampurohit</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramprasad_R/0/1/0/all/0/1">Rampi Ramprasad</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chao Zhang</a></p>
<p>Scientific information extraction (SciIE), which aims to automatically
extract information from scientific literature, is becoming more important than
ever. However, there are no existing SciIE datasets for polymer materials,
which is an important class of materials used ubiquitously in our daily lives.
To bridge this gap, we introduce POLYIE, a new SciIE dataset for polymer
materials. POLYIE is curated from 146 full-length polymer scholarly articles,
which are annotated with different named entities (i.e., materials, properties,
values, conditions) as well as their N-ary relations by domain experts. POLYIE
presents several unique challenges due to diverse lexical formats of entities,
ambiguity between entities, and variable-length relations. We evaluate
state-of-the-art named entity extraction and relation extraction models on
POLYIE, analyze their strengths and weaknesses, and highlight some difficult
cases for these models. To the best of our knowledge, POLYIE is the first SciIE
benchmark for polymer materials, and we hope it will lead to more research
efforts from the community on this challenging task. Our code and data are
available on: https://github.com/jerry3027/PolyIE.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07723">Generalization Analogies (GENIES): A Testbed for Generalizing AI Oversight to Hard-To-Measure Domains. (arXiv:2311.07723v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Clymer_J/0/1/0/all/0/1">Joshua Clymer</a>, <a href="http://arxiv.org/find/cs/1/au:+Baker_G/0/1/0/all/0/1">Garrett Baker</a>, <a href="http://arxiv.org/find/cs/1/au:+Subramani_R/0/1/0/all/0/1">Rohan Subramani</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Sam Wang</a></p>
<p>As AI systems become more intelligent and their behavior becomes more
challenging to assess, they may learn to game the flaws of human feedback
instead of genuinely striving to follow instructions; however, this risk can be
mitigated by controlling how LLMs generalize human feedback to situations where
it is unreliable. To better understand how reward models generalize, we craft
69 distribution shifts spanning 8 categories. We find that reward models do not
learn to evaluate `instruction-following' by default and instead favor personas
that resemble internet text. Techniques for interpreting reward models'
internal representations achieve better generalization than standard
fine-tuning, but still frequently fail to distinguish instruction-following
from conflated behaviors. We consolidate the 15 most challenging distribution
shifts into the GENaralization analogIES (GENIES) benchmark, which we hope will
enable progress toward controlling reward model generalization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07766">Vision-Language Integration in Multimodal Video Transformers (Partially) Aligns with the Brain. (arXiv:2311.07766v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dong_D/0/1/0/all/0/1">Dota Tianai Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Toneva_M/0/1/0/all/0/1">Mariya Toneva</a></p>
<p>Integrating information from multiple modalities is arguably one of the
essential prerequisites for grounding artificial intelligence systems with an
understanding of the real world. Recent advances in video transformers that
jointly learn from vision, text, and sound over time have made some progress
toward this goal, but the degree to which these models integrate information
from modalities still remains unclear. In this work, we present a promising
approach for probing a pre-trained multimodal video transformer model by
leveraging neuroscientific evidence of multimodal information processing in the
brain. Using brain recordings of participants watching a popular TV show, we
analyze the effects of multi-modal connections and interactions in a
pre-trained multi-modal video transformer on the alignment with uni- and
multi-modal brain regions. We find evidence that vision enhances masked
prediction performance during language processing, providing support that
cross-modal representations in models can benefit individual modalities.
However, we don't find evidence of brain-relevant information captured by the
joint multi-modal transformer representations beyond that captured by all of
the individual modalities. We finally show that the brain alignment of the
pre-trained joint representation can be improved by fine-tuning using a task
that requires vision-language inferences. Overall, our results paint an
optimistic picture of the ability of multi-modal transformers to integrate
vision and language in partially brain-relevant ways but also show that
improving the brain alignment of these models may require new approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07767">GreekT5: A Series of Greek Sequence-to-Sequence Models for News Summarization. (arXiv:2311.07767v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Giarelis_N/0/1/0/all/0/1">Nikolaos Giarelis</a>, <a href="http://arxiv.org/find/cs/1/au:+Mastrokostas_C/0/1/0/all/0/1">Charalampos Mastrokostas</a>, <a href="http://arxiv.org/find/cs/1/au:+Karacapilidis_N/0/1/0/all/0/1">Nikos Karacapilidis</a></p>
<p>Text summarization (TS) is a natural language processing (NLP) subtask
pertaining to the automatic formulation of a concise and coherent summary that
covers the major concepts and topics from one or multiple documents. Recent
advancements in deep learning have led to the development of abstractive
summarization transformer-based models, which outperform classical approaches.
In any case, research in this field focuses on high resource languages such as
English, while the corresponding work for low resource languages is still
underdeveloped. Taking the above into account, this paper proposes a series of
novel TS models for Greek news articles. The proposed models were thoroughly
evaluated on the same dataset against GreekBART, which is the state-of-the-art
model in Greek abstractive news summarization. Our evaluation results reveal
that most of the proposed models significantly outperform GreekBART on various
evaluation metrics. We make our evaluation code public, aiming to increase the
reproducibility of this work and facilitate future research in the field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07772">In-context Learning and Gradient Descent Revisited. (arXiv:2311.07772v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nathan_T/0/1/0/all/0/1">Tomer Bar Nathan</a>, <a href="http://arxiv.org/find/cs/1/au:+Deutch_G/0/1/0/all/0/1">Gilad Deutch</a>, <a href="http://arxiv.org/find/cs/1/au:+Magar_N/0/1/0/all/0/1">Nadav Magar</a>, <a href="http://arxiv.org/find/cs/1/au:+Dar_G/0/1/0/all/0/1">Guy Dar</a></p>
<p>In-context learning (ICL) has shown impressive results in few-shot learning
tasks, yet its underlying mechanism is still not fully understood. Recent works
suggest that ICL can be thought of as a gradient descent (GD) based
optimization process. While promising, these results mainly focus on simplified
settings of ICL and provide only a preliminary evaluation of the similarities
between the two methods. In this work, we revisit the comparison between ICL
and GD-based finetuning and study what properties of ICL an equivalent process
must follow. We highlight a major difference in the flow of information between
ICL and standard finetuning. Namely, ICL can only rely on information from
lower layers at every point, while finetuning depends on loss gradients from
deeper layers. We refer to this discrepancy as Layer Causality and show that a
layer causal variant of the finetuning process aligns with ICL on par with
vanilla finetuning and is even better in most cases across relevant metrics. To
the best of our knowledge, this is the first work to discuss this discrepancy
explicitly and suggest a solution that tackles this problem with minimal
changes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07804">IruMozhi: Automatically classifying diglossia in Tamil. (arXiv:2311.07804v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Prasanna_K/0/1/0/all/0/1">Kabilan Prasanna</a>, <a href="http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1">Aryaman Arora</a></p>
<p>Tamil, a Dravidian language of South Asia, is a highly diglossic language
with two very different registers in everyday use: Literary Tamil (preferred in
writing and formal communication) and Spoken Tamil (confined to speech and
informal media). Spoken Tamil is under-supported in modern NLP systems. In this
paper, we release IruMozhi, a human-annotated dataset of parallel text in
Literary and Spoken Tamil. We train classifiers on the task of identifying
which variety a text belongs to. We use these models to gauge the availability
of pretraining data in Spoken Tamil, to audit the composition of existing
labelled datasets for Tamil, and to encourage future work on the variety.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07811">In-context Learning Generalizes, But Not Always Robustly: The Case of Syntax. (arXiv:2311.07811v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mueller_A/0/1/0/all/0/1">Aaron Mueller</a>, <a href="http://arxiv.org/find/cs/1/au:+Webson_A/0/1/0/all/0/1">Albert Webson</a>, <a href="http://arxiv.org/find/cs/1/au:+Petty_J/0/1/0/all/0/1">Jackson Petty</a>, <a href="http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1">Tal Linzen</a></p>
<p>In-context learning (ICL) is now a common method for supervising large
language models (LLMs): given labeled examples in the input context, the LLM
learns to perform the task without weight updates. Despite ICL's prevalence and
utility, we understand little about whether models supervised in this manner
represent the underlying structure of their tasks, rather than superficial
heuristics that only generalize to identically distributed examples. In this
study, we investigate the robustness of LLMs supervised via ICL using the test
case of sensitivity to syntax, which is a prerequisite for robust language
understanding. Our experiments are based on two simple and well-controlled
syntactic transformations tasks, where correct out-of-distribution
generalization requires an accurate syntactic analysis of the input. We further
investigate whether out-of-distribution generalization can be improved via
chain-of-thought prompting, where the model is provided with a sequence of
intermediate computation steps that illustrate how the task ought to be
performed. In experiments with models from the GPT, PaLM, and Llama 2 families,
we find large variance across LMs on this fundamental linguistic phenomenon,
and that the variance is explained more by the composition of the pre-training
corpus and supervision methods than by model size. In particular, we find
evidence that models pre-trained on code generalize better, and benefit to a
greater extent from chain-of-thought prompting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07820">On the Analysis of Cross-Lingual Prompt Tuning for Decoder-based Multilingual Model. (arXiv:2311.07820v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1">Nohil Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Joonsuk Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoo_K/0/1/0/all/0/1">Kang Min Yoo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1">Sungroh Yoon</a></p>
<p>An exciting advancement in the field of multilingual models is the emergence
of autoregressive models with zero- and few-shot capabilities, a phenomenon
widely reported in large-scale language models. To further improve model
adaptation to cross-lingual tasks, another trend is to further fine-tune the
language models with either full fine-tuning or parameter-efficient tuning.
However, the interaction between parameter-efficient fine-tuning (PEFT) and
cross-lingual tasks in multilingual autoregressive models has yet to be
studied. Specifically, we lack an understanding of the role of linguistic
distributions in multilingual models in the effectiveness of token-based prompt
tuning. To address this question, we conduct experiments comparing prompt
tuning and fine-tuning on the decoder-based multilingual model, XGLM, with four
cross-lingual tasks (XNLI, PAWS-X, POS, NER). According to our study, prompt
tuning achieves on par or better performance over fine-tuning across all
languages while updating at most 0.13\% of the model parameters. Moreover, we
empirically show that prompt tuning is more effective in enhancing the
performance of low-resource languages than fine-tuning. Our further analysis
shows that the phenomenon is related to the tokenization scheme of the
multilingual model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07838">LLatrieval: LLM-Verified Retrieval for Verifiable Generation. (arXiv:2311.07838v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaonan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1">Changtai Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Linyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1">Zhangyue Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1">Tianxiang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1">Xipeng Qiu</a></p>
<p>Verifiable generation aims to let the large language model (LLM) generate
text with corresponding supporting documents, which enables the user to
flexibly verify the answer and makes it more trustworthy. Its evaluation not
only measures the correctness of the answer, but also the answer's
verifiability, i.e., how well the answer is supported by the corresponding
documents. In typical, verifiable generation adopts the retrieval-read
pipeline, which is divided into two stages: 1) retrieve relevant documents of
the question. 2) according to the documents, generate the corresponding answer.
Since the retrieved documents can supplement knowledge for the LLM to generate
the answer and serve as evidence, the retrieval stage is essential for the
correctness and verifiability of the answer. However, the widely used
retrievers become the bottleneck of the entire pipeline and limit the overall
performance. They often have fewer parameters than the large language model and
have not been proven to scale well to the size of LLMs. Since the LLM passively
receives the retrieval result, if the retriever does not correctly find the
supporting documents, the LLM can not generate the correct and verifiable
answer, which overshadows the LLM's remarkable abilities. In this paper, we
propose LLatrieval (Large Language Model Verified Retrieval), where the LLM
updates the retrieval result until it verifies that the retrieved documents can
support answering the question. Thus, the LLM can iteratively provide feedback
to retrieval and facilitate the retrieval result to sufficiently support
verifiable generation. Experimental results show that our method significantly
outperforms extensive baselines and achieves new state-of-the-art results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07850">Bring Your Own KG: Self-Supervised Program Synthesis for Zero-Shot KGQA. (arXiv:2311.07850v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Agarwal_D/0/1/0/all/0/1">Dhruv Agarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1">Rajarshi Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Khosla_S/0/1/0/all/0/1">Sopan Khosla</a>, <a href="http://arxiv.org/find/cs/1/au:+Gangadharaiah_R/0/1/0/all/0/1">Rashmi Gangadharaiah</a></p>
<p>We present BYOKG, a universal question-answering (QA) system that can operate
on any knowledge graph (KG), requires no human-annotated training data, and can
be ready to use within a day -- attributes that are out-of-scope for current
KGQA systems. BYOKG draws inspiration from the remarkable ability of humans to
comprehend information present in an unseen KG through exploration -- starting
at random nodes, inspecting the labels of adjacent nodes and edges, and
combining them with their prior world knowledge. In BYOKG, exploration
leverages an LLM-backed symbolic agent that generates a diverse set of
query-program exemplars, which are then used to ground a retrieval-augmented
reasoning procedure to predict programs for arbitrary questions. BYOKG is
effective over both small- and large-scale graphs, showing dramatic gains in QA
accuracy over a zero-shot baseline of 27.89 and 58.02 F1 on GrailQA and MetaQA,
respectively. On GrailQA, we further show that our unsupervised BYOKG
outperforms a supervised in-context learning method, demonstrating the
effectiveness of exploration. Lastly, we find that performance of BYOKG
reliably improves with continued exploration as well as improvements in the
base LLM, notably outperforming a state-of-the-art fine-tuned model by 7.08 F1
on a sub-sampled zero-shot split of GrailQA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07853">Learning Mutually Informed Representations for Characters and Subwords. (arXiv:2311.07853v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yilin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xinyi Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gormley_M/0/1/0/all/0/1">Matthew R. Gormley</a></p>
<p>Most pretrained language models rely on subword tokenization, which processes
text as a sequence of subword tokens. However, different granularities of text,
such as characters, subwords, and words, can contain different kinds of
information. Previous studies have shown that incorporating multiple input
granularities improves model generalization, yet very few of them outputs
useful representations for each granularity. In this paper, we introduce the
entanglement model, aiming to combine character and subword language models.
Inspired by vision-language models, our model treats characters and subwords as
separate modalities, and it generates mutually informed representations for
both granularities as output. We evaluate our model on text classification,
named entity recognition, and POS-tagging tasks. Notably, the entanglement
model outperforms its backbone language models, particularly in the presence of
noisy texts and low-resource languages. Furthermore, the entanglement model
even outperforms larger pre-trained models on all English sequence labeling
tasks and classification tasks. Our anonymized code is available at
https://anonymous.4open.science/r/noisy-IE-A673
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07879">Toxicity Detection is NOT all you Need: Measuring the Gaps to Supporting Volunteer Content Moderators. (arXiv:2311.07879v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yang Trista Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Domingo_L/0/1/0/all/0/1">Lovely-Frances Domingo</a>, <a href="http://arxiv.org/find/cs/1/au:+Gilbert_S/0/1/0/all/0/1">Sarah Ann Gilbert</a>, <a href="http://arxiv.org/find/cs/1/au:+Mazurek_M/0/1/0/all/0/1">Michelle Mazurek</a>, <a href="http://arxiv.org/find/cs/1/au:+Shilton_K/0/1/0/all/0/1">Katie Shilton</a>, <a href="http://arxiv.org/find/cs/1/au:+Daume_H/0/1/0/all/0/1">Hal Daum&#xe9; III</a></p>
<p>Extensive efforts in automated approaches for content moderation have been
focused on developing models to identify toxic, offensive, and hateful content
-- with the aim of lightening the load for moderators. Yet, it remains
uncertain whether improvements on those tasks truly address the needs that
moderators have in accomplishing their work. In this paper, we surface the gaps
between past research efforts that have aimed to provide automation for aspects
of the content moderation task, and the needs of volunteer content moderators.
To do so, we conduct a model review on Hugging Face to reveal the availability
of models to cover various moderation rules and guidelines. We further put
state-of-the-art LLMs to the test (GPT-4 and Llama-2), evaluating how well
these models perform in flagging violations of platform rules. Overall, we
observe a non-trivial gap, as missing developed models and LLMs exhibit low
recall on a significant portion of the rules.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07884">Fair Abstractive Summarization of Diverse Perspectives. (arXiv:2311.07884v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yusen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Nan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yixin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fabbri_A/0/1/0/all/0/1">Alexander Fabbri</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Junru Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kamoi_R/0/1/0/all/0/1">Ryo Kamoi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1">Xiaoxin Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1">Caiming Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jieyu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1">Dragomir Radev</a>, <a href="http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1">Kathleen McKeown</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Rui Zhang</a></p>
<p>People from different social and demographic groups express diverse
perspectives and conflicting opinions on a broad set of topics such as product
reviews, healthcare, law, and politics. A fair summary should provide a
comprehensive coverage of diverse perspectives without underrepresenting
certain groups. However, current work in summarization metrics and Large
Language Models (LLMs) evaluation has not explored fair abstractive
summarization. In this paper, we systematically investigate fair abstractive
summarization for user-generated data. We first formally define fairness in
abstractive summarization as not underrepresenting perspectives of any groups
of people and propose four reference-free automatic metrics measuring the
differences between target and source perspectives. We evaluate five LLMs,
including three GPT models, Alpaca, and Claude, on six datasets collected from
social media, online reviews, and recorded transcripts. Experiments show that
both the model-generated and the human-written reference summaries suffer from
low fairness. We conduct a comprehensive analysis of the common factors
influencing fairness and propose three simple but effective methods to
alleviate unfair summarization. Our dataset and code are available at
https://github.com/psunlpgroup/FairSumm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07897">CPopQA: Ranking Cultural Concept Popularity by LLMs. (arXiv:2311.07897v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1">Ming Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Joshi_M/0/1/0/all/0/1">Mansi Joshi</a></p>
<p>Prior work has demonstrated large language models' (LLMs) potential to
discern statistical tendencies within their pre-training corpora. Despite that,
many examinations of LLMs' knowledge capacity focus on knowledge explicitly
appearing in the training data or implicitly inferable from similar contexts.
How well an LLM captures the corpus-level statistical trends of concepts for
reasoning, especially long-tail ones, is still underexplored. In this study, we
introduce a novel few-shot question-answering task (CPopQA) that examines LLMs'
statistical ranking abilities for long-tail cultural concepts (e.g., holidays),
with a specific focus on these concepts' popularity in the United States and
the United Kingdom, respectively. We curate a dataset containing 459 holidays
across 58 countries, generating a total of 6,000 QA testing pairs. Experiments
on four strong LLMs show that large models are capable of ranking long-tail
cultural concepts regarding their statistical tendency. Notably, GPT-3.5
displayed superior performance and exhibited its potential to identify
geo-cultural proximity across continents.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07911">Instruction-Following Evaluation for Large Language Models. (arXiv:2311.07911v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jeffrey Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1">Tianjian Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1">Swaroop Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Brahma_S/0/1/0/all/0/1">Siddhartha Brahma</a>, <a href="http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1">Sujoy Basu</a>, <a href="http://arxiv.org/find/cs/1/au:+Luan_Y/0/1/0/all/0/1">Yi Luan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1">Denny Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1">Le Hou</a></p>
<p>One core capability of Large Language Models (LLMs) is to follow natural
language instructions. However, the evaluation of such abilities is not
standardized: Human evaluations are expensive, slow, and not objectively
reproducible, while LLM-based auto-evaluation is potentially biased or limited
by the ability of the evaluator LLM. To overcome these issues, we introduce
Instruction-Following Eval (IFEval) for large language models. IFEval is a
straightforward and easy-to-reproduce evaluation benchmark. It focuses on a set
of "verifiable instructions" such as "write in more than 400 words" and
"mention the keyword of AI at least 3 times". We identified 25 types of those
verifiable instructions and constructed around 500 prompts, with each prompt
containing one or more verifiable instructions. We show evaluation results of
two widely available LLMs on the market. Our code and data can be found at
https://github.com/google-research/google-research/tree/master/instruction_following_eval
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07914">Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey. (arXiv:2311.07914v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Agrawal_G/0/1/0/all/0/1">Garima Agrawal</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumarage_T/0/1/0/all/0/1">Tharindu Kumarage</a>, <a href="http://arxiv.org/find/cs/1/au:+Alghami_Z/0/1/0/all/0/1">Zeyad Alghami</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Huan Liu</a></p>
<p>The contemporary LLMs are prone to producing hallucinations, stemming mainly
from the knowledge gaps within the models. To address this critical limitation,
researchers employ diverse strategies to augment the LLMs by incorporating
external knowledge, aiming to reduce hallucinations and enhance reasoning
accuracy. Among these strategies, leveraging knowledge graphs as a source of
external information has demonstrated promising results. In this survey, we
conduct a comprehensive review of these knowledge-graph-based knowledge
augmentation techniques in LLMs, focusing on their efficacy in mitigating
hallucinations. We systematically categorize these methods into three
overarching groups, offering both methodological comparisons and empirical
evaluations of their performance. Lastly, the paper explores the challenges
associated with these techniques and outlines potential avenues for future
research in this emerging field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07918">Automated title and abstract screening for scoping reviews using the GPT-4 Large Language Model. (arXiv:2311.07918v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wilkins_D/0/1/0/all/0/1">David Wilkins</a></p>
<p>Scoping reviews, a type of literature review, require intensive human effort
to screen large numbers of scholarly sources for their relevance to the review
objectives. This manuscript introduces GPTscreenR, a package for the R
statistical programming language that uses the GPT-4 Large Language Model (LLM)
to automatically screen sources. The package makes use of the chain-of-thought
technique with the goal of maximising performance on complex screening tasks.
In validation against consensus human reviewer decisions, GPTscreenR performed
similarly to an alternative zero-shot technique, with a sensitivity of 71%,
specificity of 89%, and overall accuracy of 84%. Neither method achieved
perfect accuracy nor human levels of intraobserver agreement. GPTscreenR
demonstrates the potential for LLMs to support scholarly work and provides a
user-friendly software framework that can be integrated into existing review
processes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07919">Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models. (arXiv:2311.07919v1 [eess.AS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Chu_Y/0/1/0/all/0/1">Yunfei Chu</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1">Jin Xu</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1">Xiaohuan Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_Q/0/1/0/all/0/1">Qian Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1">Shiliang Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Yan_Z/0/1/0/all/0/1">Zhijie Yan</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_C/0/1/0/all/0/1">Chang Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_J/0/1/0/all/0/1">Jingren Zhou</a></p>
<p>Recently, instruction-following audio-language models have received broad
attention for audio interaction with humans. However, the absence of
pre-trained audio models capable of handling diverse audio types and tasks has
hindered progress in this field. Consequently, most existing works have only
been able to support a limited range of interaction capabilities. In this
paper, we develop the Qwen-Audio model and address this limitation by scaling
up audio-language pre-training to cover over 30 tasks and various audio types,
such as human speech, natural sounds, music, and songs, to facilitate universal
audio understanding abilities. However, directly co-training all tasks and
datasets can lead to interference issues, as the textual labels associated with
different datasets exhibit considerable variations due to differences in task
focus, language, granularity of annotation, and text structure. To overcome the
one-to-many interference, we carefully design a multi-task training framework
by conditioning on a sequence of hierarchical tags to the decoder for
encouraging knowledge sharing and avoiding interference through shared and
specified tags respectively. Remarkably, Qwen-Audio achieves impressive
performance across diverse benchmark tasks without requiring any task-specific
fine-tuning, surpassing its counterparts. Building upon the capabilities of
Qwen-Audio, we further develop Qwen-Audio-Chat, which allows for input from
various audios and text inputs, enabling multi-turn dialogues and supporting
various audio-central scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07925">Brain-Driven Representation Learning Based on Diffusion Model. (arXiv:2311.07925v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Soowon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Seo-Hyun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1">Young-Eun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Ji-Won Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Ji-Ha Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Seong-Whan Lee</a></p>
<p>Interpreting EEG signals linked to spoken language presents a complex
challenge, given the data's intricate temporal and spatial attributes, as well
as the various noise factors. Denoising diffusion probabilistic models (DDPMs),
which have recently gained prominence in diverse areas for their capabilities
in representation learning, are explored in our research as a means to address
this issue. Using DDPMs in conjunction with a conditional autoencoder, our new
approach considerably outperforms traditional machine learning algorithms and
established baseline models in accuracy. Our results highlight the potential of
DDPMs as a sophisticated computational method for the analysis of
speech-related EEG signals. This could lead to significant advances in
brain-computer interfaces tailored for spoken communication.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07930">It&#x27;s All Relative! -- A Synthetic Query Generation Approach for Improving Zero-Shot Relevance Prediction. (arXiv:2311.07930v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chaudhary_A/0/1/0/all/0/1">Aditi Chaudhary</a>, <a href="http://arxiv.org/find/cs/1/au:+Raman_K/0/1/0/all/0/1">Karthik Raman</a>, <a href="http://arxiv.org/find/cs/1/au:+Bendersky_M/0/1/0/all/0/1">Michael Bendersky</a></p>
<p>Recent developments in large language models (LLMs) have shown promise in
their ability to generate synthetic query-document pairs by prompting with as
few as 8 demonstrations. This has enabled building better IR models, especially
for tasks with no training data readily available. Typically, such synthetic
query generation (QGen) approaches condition on an input context (e.g. a text
document) and generate a query relevant to that context, or condition the QGen
model additionally on the relevance label (e.g. relevant vs irrelevant) to
generate queries across relevance buckets. However, we find that such QGen
approaches are sub-optimal as they require the model to reason about the
desired label and the input from a handful of examples. In this work, we
propose to reduce this burden of LLMs by generating queries simultaneously for
different labels. We hypothesize that instead of asking the model to generate,
say, an irrelevant query given an input context, asking the model to generate
an irrelevant query relative to a relevant query is a much simpler task setup
for the model to reason about. Extensive experimentation across seven IR
datasets shows that synthetic queries generated in such a fashion translates to
a better downstream performance, suggesting that the generated queries are
indeed of higher quality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07941">Non-autoregressive Machine Translation with Probabilistic Context-free Grammar. (arXiv:2311.07941v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gui_S/0/1/0/all/0/1">Shangtong Gui</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_C/0/1/0/all/0/1">Chenze Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1">Zhengrui Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xishan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yunji Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yang Feng</a></p>
<p>Non-autoregressive Transformer(NAT) significantly accelerates the inference
of neural machine translation. However, conventional NAT models suffer from
limited expression power and performance degradation compared to autoregressive
(AT) models due to the assumption of conditional independence among target
tokens. To address these limitations, we propose a novel approach called
PCFG-NAT, which leverages a specially designed Probabilistic Context-Free
Grammar (PCFG) to enhance the ability of NAT models to capture complex
dependencies among output tokens. Experimental results on major machine
translation benchmarks demonstrate that PCFG-NAT further narrows the gap in
translation quality between NAT and AT models. Moreover, PCFG-NAT facilitates a
deeper understanding of the generated sentences, addressing the lack of
satisfactory explainability in neural machine translation.Code is publicly
available at https://github.com/ictnlp/PCFG-NAT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07945">First Step Advantage: Importance of Starting Right in Multi-Step Reasoning. (arXiv:2311.07945v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jain_K/0/1/0/all/0/1">Kushal Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Shridhar_K/0/1/0/all/0/1">Kumar Shridhar</a></p>
<p>Large Language Models (LLMs) can solve complex reasoning tasks by generating
rationales for their predictions. Distilling these capabilities into a smaller,
compact model can facilitate the creation of specialized, cost-effective models
tailored for specific tasks. However, smaller models often face challenges in
complex reasoning tasks and often deviate from the correct reasoning path. We
show that LLMs can guide smaller models and bring them back to the correct
reasoning path only if they intervene at the right time. We show that smaller
models fail to reason primarily due to their difficulty in initiating the
process, and that guiding them in the right direction can lead to a performance
gain of over 100%. We explore different model sizes and evaluate the benefits
of providing guidance to improve reasoning in smaller models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07954">A Closer Look at the Self-Verification Abilities of Large Language Models in Logical Reasoning. (arXiv:2311.07954v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hong_R/0/1/0/all/0/1">Ruixin Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pang_X/0/1/0/all/0/1">Xinyu Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1">Dong Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Changshui Zhang</a></p>
<p>Logical reasoning has been an ongoing pursuit in the field of AI. Despite
significant advancements made by large language models (LLMs), they still
struggle with complex logical reasoning problems. To enhance reasoning
performance, one promising direction is scalable oversight, which requires LLMs
to identify their own errors and then improve by themselves. Various
self-verification methods have been proposed in pursuit of this goal.
Nevertheless, whether existing models understand their own errors well is still
under investigation. In this paper, we take a closer look at the
self-verification abilities of LLMs in the context of logical reasoning,
focusing on their ability to identify logical fallacies accurately. We
introduce a dataset, FALLACIES, containing 232 types of reasoning fallacies
categorized in a hierarchical taxonomy. By conducting exhaustive experiments on
FALLACIES, we obtain comprehensive and detailed analyses of a series of models
on their verification abilities. Our main findings suggest that existing LLMs
could struggle to identify fallacious reasoning steps accurately and may fall
short of guaranteeing the validity of self-verification methods. Drawing from
these observations, we offer suggestions for future research and practical
applications of self-verification methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07961">The ART of LLM Refinement: Ask, Refine, and Trust. (arXiv:2311.07961v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shridhar_K/0/1/0/all/0/1">Kumar Shridhar</a>, <a href="http://arxiv.org/find/cs/1/au:+Sinha_K/0/1/0/all/0/1">Koustuv Sinha</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_A/0/1/0/all/0/1">Andrew Cohen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tianlu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1">Ping Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pasunuru_R/0/1/0/all/0/1">Ram Pasunuru</a>, <a href="http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1">Mrinmaya Sachan</a>, <a href="http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1">Jason Weston</a>, <a href="http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1">Asli Celikyilmaz</a></p>
<p>In recent years, Large Language Models (LLMs) have demonstrated remarkable
generative abilities, but can they judge the quality of their own generations?
A popular concept, referred to as self-refinement, postulates that LLMs can
detect and correct the errors in their generations when asked to do so.
However, recent empirical evidence points in the opposite direction, suggesting
that LLMs often struggle to accurately identify errors when reasoning is
involved. To address this, we propose a reasoning with refinement objective
called ART: Ask, Refine, and Trust, which asks necessary questions to decide
when an LLM should refine its output, and either affirm or withhold trust in
its refinement by ranking the refinement and the initial prediction. On two
multistep reasoning tasks of mathematical word problems (GSM8K) and question
answering (StrategyQA), ART achieves a performance gain of +5 points over
self-refinement baselines, while using a much smaller model as the decision
maker. We also demonstrate the benefit of using smaller models to make
refinement decisions as a cost-effective alternative to fine-tuning a larger
model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07978">How good are Large Language Models on African Languages?. (arXiv:2311.07978v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ojo_J/0/1/0/all/0/1">Jessica Ojo</a>, <a href="http://arxiv.org/find/cs/1/au:+Ogueji_K/0/1/0/all/0/1">Kelechi Ogueji</a>, <a href="http://arxiv.org/find/cs/1/au:+Stenetorp_P/0/1/0/all/0/1">Pontus Stenetorp</a>, <a href="http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1">David I. Adelani</a></p>
<p>Recent advancements in natural language processing have led to the
proliferation of large language models (LLMs). These models have been shown to
yield good performance, using in-context learning, even on unseen tasks and
languages. Additionally, they have been widely adopted as
language-model-as-a-service commercial APIs like GPT-4 API. However, their
performance on African languages is largely unknown. We present an analysis of
three popular large language models (mT0, LLaMa 2, and GPT-4) on five tasks
(news topic classification, sentiment classification, machine translation,
question answering, and named entity recognition) across 30 African languages,
spanning different language families and geographical regions. Our results
suggest that all LLMs produce below-par performance on African languages, and
there is a large gap in performance compared to high-resource languages like
English most tasks. We find that GPT-4 has an average or impressive performance
on classification tasks but very poor results on generative tasks like machine
translation. Surprisingly, we find that mT0 had the best overall on
cross-lingual QA, better than the state-of-the-art supervised model (i.e.
fine-tuned mT5) and GPT-4 on African languages. Overall, LLaMa 2 records the
worst performance due to its limited multilingual capabilities and
English-centric pre-training corpus. In general, our findings present a
call-to-action to ensure African languages are well represented in large
language models, given their growing popularity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07989">A Survey on Language Models for Code. (arXiv:2311.07989v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Ziyin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chaoyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bingchang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_C/0/1/0/all/0/1">Cong Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1">Zi Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Hang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jianguo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Rui Wang</a></p>
<p>In this work we systematically review the recent advancements in code
processing with language models, covering 50+ models, 30+ evaluation tasks, and
500 related works. We break down code processing models into general language
models represented by the GPT family and specialized models that are
specifically pretrained on code, often with tailored objectives. We discuss the
relations and differences between these models, and highlight the historical
transition of code modeling from statistical models and RNNs to pretrained
Transformers and LLMs, which is exactly the same course that had been taken by
NLP. We also discuss code-specific features such as AST, CFG, and unit tests,
along with their application in training code language models, and identify key
challenges and potential future directions in this domain. We keep the survey
open and updated on github repository at
https://github.com/codefuse-ai/Awesome-Code-LLM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07996">How Well Do Text Embedding Models Understand Syntax?. (arXiv:2311.07996v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1">Zhaopeng Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Teng_Z/0/1/0/all/0/1">Zhiyang Teng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zuozhu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Haizhou Li</a></p>
<p>Text embedding models have significantly contributed to advancements in
natural language processing by adeptly capturing semantic properties of textual
data. However, the ability of these models to generalize across a wide range of
syntactic contexts remains under-explored. In this paper, we first develop an
evaluation set, named \textbf{SR}, to scrutinize the capability for syntax
understanding of text embedding models from two crucial syntactic aspects:
Structural heuristics, and Relational understanding among concepts, as revealed
by the performance gaps in previous studies. Our findings reveal that existing
text embedding models have not sufficiently addressed these syntactic
understanding challenges, and such ineffectiveness becomes even more apparent
when evaluated against existing benchmark datasets. Furthermore, we conduct
rigorous analysis to unearth factors that lead to such limitations and examine
why previous evaluations fail to detect such ineffectiveness. Lastly, we
propose strategies to augment the generalization ability of text embedding
models in diverse syntactic scenarios. This study serves to highlight the
hurdles associated with syntactic generalization and provides pragmatic
guidance for boosting model performance across varied syntactic contexts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08001">A Comparative Analysis of the COVID-19 Infodemic in English and Chinese: Insights from Social Media Textual Data. (arXiv:2311.08001v1 [cs.SI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1">Jia Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1">Daiyun Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1">Lei Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Baz_D/0/1/0/all/0/1">Didier El Baz</a> (LAAS-SARA), <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xinran Liu</a></p>
<p>The COVID-19 infodemic, characterized by the rapid spread of misinformation
and unverified claims related to the pandemic, presents a significant
challenge. This paper presents a comparative analysis of the COVID-19 infodemic
in the English and Chinese languages, utilizing textual data extracted from
social media platforms. To ensure a balanced representation, two infodemic
datasets were created by augmenting previously collected social media textual
data. Through word frequency analysis, the thirty-five most frequently
occurring infodemic words are identified, shedding light on prevalent
discussions surrounding the infodemic. Moreover, topic clustering analysis
uncovers thematic structures and provides a deeper understanding of primary
topics within each language context. Additionally, sentiment analysis enables
comprehension of the emotional tone associated with COVID-19 information on
social media platforms in English and Chinese. This research contributes to a
better understanding of the COVID-19 infodemic phenomenon and can guide the
development of strategies to combat misinformation during public health crises
across different languages.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08002">TempTabQA: Temporal Question Answering for Semi-Structured Tables. (arXiv:2311.08002v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1">Vivek Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Kandoi_P/0/1/0/all/0/1">Pranshu Kandoi</a>, <a href="http://arxiv.org/find/cs/1/au:+Vora_M/0/1/0/all/0/1">Mahek Bhavesh Vora</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shuo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yujie He</a>, <a href="http://arxiv.org/find/cs/1/au:+Reinanda_R/0/1/0/all/0/1">Ridho Reinanda</a>, <a href="http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1">Vivek Srikumar</a></p>
<p>Semi-structured data, such as Infobox tables, often include temporal
information about entities, either implicitly or explicitly. Can current NLP
systems reason about such information in semi-structured tables? To tackle this
question, we introduce the task of temporal question answering on
semi-structured tables. We present a dataset, TempTabQA, which comprises 11,454
question-answer pairs extracted from 1,208 Wikipedia Infobox tables spanning
more than 90 distinct domains. Using this dataset, we evaluate several
state-of-the-art models for temporal reasoning. We observe that even the
top-performing LLMs lag behind human performance by more than 13.5 F1 points.
Given these results, our dataset has the potential to serve as a challenging
benchmark to improve the temporal reasoning capabilities of NLP models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08010">Distantly-Supervised Named Entity Recognition with Uncertainty-aware Teacher Learning and Student-student Collaborative Learning. (arXiv:2311.08010v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1">Helan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1">Shuzheng Si</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Haozhe Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1">Shuang Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+An_K/0/1/0/all/0/1">Kaikai An</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1">Zefan Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1">Baobao Chang</a></p>
<p>Distantly-Supervised Named Entity Recognition (DS-NER) effectively alleviates
the burden of annotation, but meanwhile suffers from the label noise. Recent
works attempt to adopt the teacher-student framework to gradually refine the
training labels and improve the overall robustness. However, we argue that
these teacher-student methods achieve limited performance because poor network
calibration produces incorrectly pseudo-labeled samples, leading to error
propagation. Therefore, we attempt to mitigate this issue by proposing: (1)
Uncertainty-aware Teacher Learning that leverages the prediction uncertainty to
guide the selection of pseudo-labels, avoiding the number of incorrect
pseudo-labels in the self-training stage. (2) Student-student Collaborative
Learning that allows the transfer of reliable labels between two student
networks instead of completely relying on all pseudo-labels from its teacher.
Meanwhile, this approach allows a full exploration of mislabeled samples rather
than simply filtering unreliable pseudo-labeled samples. Extensive experimental
results on five DS-NER datasets demonstrate that our method is superior to
state-of-the-art teacher-student methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08011">Forgetting before Learning: Utilizing Parametric Arithmetic for Knowledge Updating in Large Language Models. (arXiv:2311.08011v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ni_S/0/1/0/all/0/1">Shiwen Ni</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Dingwei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chengming Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xiping Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">Ruifeng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Min Yang</a></p>
<p>Recently Large Language Models (LLMs) have demonstrated their amazing text
understanding and generation capabilities. However, even stronger LLMs may
still learn incorrect knowledge from the training corpus, as well as some
knowledge that is outdated over time. Direct secondary fine-tuning with data
containing new knowledge may be ineffective in updating knowledge due to the
conflict between old and new knowledge. In this paper, we propose a new
paradigm for fine-tuning called F-Learning (Forgetting before Learning), which
is based on parametric arithmetic to achieve forgetting of old knowledge and
learning of new knowledge. Experimental results on two publicly available
datasets demonstrate that our proposed F-Learning can obviously improve the
knowledge updating performance of both full fine-tuning and LoRA fine-tuning.
Moreover, we have also discovered that forgetting old knowledge by subtracting
the parameters of LoRA can achieve a similar effect to subtracting the
parameters of full fine-tuning, and sometimes even surpass it significantly.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08045">Adversarial Preference Optimization. (arXiv:2311.08045v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1">Pengyu Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yifan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jian Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1">Yong Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_N/0/1/0/all/0/1">Nan Du</a></p>
<p>Human preference alignment is a crucial training step to improve the
interaction quality of large language models (LLMs). Existing aligning methods
depend on manually annotated preference data to guide the LLM optimization
directions. However, in practice, continuously updating LLMs raises a
distribution gap between model-generated samples and human-preferred responses,
which hinders model fine-tuning efficiency. To mitigate this issue, previous
methods require additional preference annotation on generated samples to adapt
the shifted distribution, which consumes a large amount of annotation
resources. Targeting more efficient human preference optimization, we propose
an adversarial preference optimization (APO) framework, where the LLM agent and
the preference model update alternatively via a min-max game. Without
additional annotation, our APO method can make a self-adaption to the
generation distribution gap through the adversarial learning process. In
experiments, we empirically verify the effectiveness of APO in improving LLM's
helpfulness and harmlessness compared with rejection sampling baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08057">Data and models for stance and premise detection in COVID-19 tweets: insights from the Social Media Mining for Health (SMM4H) 2022 shared task. (arXiv:2311.08057v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Davydova_V/0/1/0/all/0/1">Vera Davydova</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Huabin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tutubalina_E/0/1/0/all/0/1">Elena Tutubalina</a></p>
<p>The COVID-19 pandemic has sparked numerous discussions on social media
platforms, with users sharing their views on topics such as mask-wearing and
vaccination. To facilitate the evaluation of neural models for stance detection
and premise classification, we organized the Social Media Mining for Health
(SMM4H) 2022 Shared Task 2. This competition utilized manually annotated posts
on three COVID-19-related topics: school closures, stay-at-home orders, and
wearing masks. In this paper, we extend the previous work and present newly
collected data on vaccination from Twitter to assess the performance of models
on a different topic. To enhance the accuracy and effectiveness of our
evaluation, we employed various strategies to aggregate tweet texts with
claims, including models with feature-level (early) fusion and dual-view
architectures from SMM4H 2022 leaderboard. Our primary objective was to create
a valuable dataset and perform an extensive experimental evaluation to support
future research in argument mining in the health domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08089">Align after Pre-train: Improving Multilingual Generative Models with Cross-lingual Alignment. (arXiv:2311.08089v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shaonan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiajun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zong_C/0/1/0/all/0/1">Chengqing Zong</a></p>
<p>Multilingual generative models obtain remarkable cross-lingual capabilities
through pre-training on large-scale corpora. However, they still exhibit a
performance bias toward high-resource languages, and learn isolated
distributions of sentence representations across languages. To bridge this gap,
we propose a simple yet effective alignment framework exploiting pairs of
translation sentences. It aligns the internal sentence representations across
different languages via multilingual contrastive learning and aligns model
outputs by answering prompts in different languages. Experimental results
demonstrate that even with less than 0.1 {\textperthousand} of pre-training
tokens, our alignment framework significantly boosts the cross-lingual
abilities of generative models and mitigates the performance gap. Further
analysis reveals that it results in a better internal multilingual
representation distribution of multilingual models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08093">Spot: A Natural Language Interface for Geospatial Searches in OSM. (arXiv:2311.08093v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khellaf_L/0/1/0/all/0/1">Lynn Khellaf</a>, <a href="http://arxiv.org/find/cs/1/au:+Schlicht_I/0/1/0/all/0/1">Ipek Baris Schlicht</a>, <a href="http://arxiv.org/find/cs/1/au:+Bayer_J/0/1/0/all/0/1">Julia Bayer</a>, <a href="http://arxiv.org/find/cs/1/au:+Bouwmeester_R/0/1/0/all/0/1">Ruben Bouwmeester</a>, <a href="http://arxiv.org/find/cs/1/au:+Mirass_T/0/1/0/all/0/1">Tilman Mira&#xdf;</a>, <a href="http://arxiv.org/find/cs/1/au:+Wagner_T/0/1/0/all/0/1">Tilman Wagner</a></p>
<p>Investigative journalists and fact-checkers have found OpenStreetMap (OSM) to
be an invaluable resource for their work due to its extensive coverage and
intricate details of various locations, which play a crucial role in
investigating news scenes. Despite its value, OSM's complexity presents
considerable accessibility and usability challenges, especially for those
without a technical background. To address this, we introduce 'Spot', a
user-friendly natural language interface for querying OSM data. Spot utilizes a
semantic mapping from natural language to OSM tags, leveraging artificially
generated sentence queries and a T5 transformer. This approach enables Spot to
extract relevant information from user-input sentences and display candidate
locations matching the descriptions on a map. To foster collaboration and
future advancement, all code and generated data is available as an open-source
repository.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08097">Empowering Multi-step Reasoning across Languages via Tree-of-Thoughts. (arXiv:2311.08097v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ranaldi_L/0/1/0/all/0/1">Leonardo Ranaldi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zanzotto_F/0/1/0/all/0/1">Fabio Massimo Zanzotto</a></p>
<p>Chain-of-Thought (CoT) prompting empowers the reasoning abilities of Large
Language Models (LLMs), eliciting them to solve complex reasoning tasks
step-by-step. However, with the success of CoT methods, the ability to deliver
multi-step reasoning remains limited to English due to the imbalance in the
distribution of the pre-training data, making the other languages a barrier.
</p>
<p>In this work, we propose a Cross-lingual multi-step reasoning approach,
aiming to align reasoning processes across different languages. In particular,
our method, through a Self-consistent Cross-lingual prompting mechanism
inspired by the Tree-of-Thoughts approach, delivers multi-step reasoning paths
in different languages that, during the steps, lead to the final solution. Our
experimental evaluations show that our method significantly outperforms
existing prompting methods, reducing the number of interactions and achieving
state-of-the-art performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08103">Exploring Semi-supervised Hierarchical Stacked Encoder for Legal Judgement Prediction. (arXiv:2311.08103v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Prasad_N/0/1/0/all/0/1">Nishchal Prasad</a>, <a href="http://arxiv.org/find/cs/1/au:+Boughanem_M/0/1/0/all/0/1">Mohand Boughanem</a>, <a href="http://arxiv.org/find/cs/1/au:+Dkaki_T/0/1/0/all/0/1">Taoufiq Dkaki</a></p>
<p>Predicting the judgment of a legal case from its unannotated case facts is a
challenging task. The lengthy and non-uniform document structure poses an even
greater challenge in extracting information for decision prediction. In this
work, we explore and propose a two-level classification mechanism; both
supervised and unsupervised; by using domain-specific pre-trained BERT to
extract information from long documents in terms of sentence embeddings further
processing with transformer encoder layer and use unsupervised clustering to
extract hidden labels from these embeddings to better predict a judgment of a
legal case. We conduct several experiments with this mechanism and see higher
performance gains than the previously proposed methods on the ILDC dataset. Our
experimental results also show the importance of domain-specific pre-training
of Transformer Encoders in legal information processing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08105">DiLoCo: Distributed Low-Communication Training of Language Models. (arXiv:2311.08105v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Douillard_A/0/1/0/all/0/1">Arthur Douillard</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1">Qixuan Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Rusu_A/0/1/0/all/0/1">Andrei A. Rusu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chhaparia_R/0/1/0/all/0/1">Rachita Chhaparia</a>, <a href="http://arxiv.org/find/cs/1/au:+Donchev_Y/0/1/0/all/0/1">Yani Donchev</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuncoro_A/0/1/0/all/0/1">Adhiguna Kuncoro</a>, <a href="http://arxiv.org/find/cs/1/au:+Ranzato_M/0/1/0/all/0/1">Marc&#x27;Aurelio Ranzato</a>, <a href="http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1">Arthur Szlam</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1">Jiajun Shen</a></p>
<p>Large language models (LLM) have become a critical component in many
applications of machine learning. However, standard approaches to training LLM
require a large number of tightly interconnected accelerators, with devices
exchanging gradients and other intermediate states at each optimization step.
While it is difficult to build and maintain a single computing cluster hosting
many accelerators, it might be easier to find several computing clusters each
hosting a smaller number of devices. In this work, we propose a distributed
optimization algorithm, Distributed Low-Communication (DiLoCo), that enables
training of language models on islands of devices that are poorly connected.
The approach is a variant of federated averaging, where the number of inner
steps is large, the inner optimizer is AdamW, and the outer optimizer is
Nesterov momentum. On the widely used C4 dataset, we show that DiLoCo on 8
workers performs as well as fully synchronous optimization while communicating
500 times less. DiLoCo exhibits great robustness to the data distribution of
each worker. It is also robust to resources becoming unavailable over time, and
vice versa, it can seamlessly leverage resources that become available during
training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08106">Carpe Diem: On the Evaluation of World Knowledge in Lifelong Language Models. (arXiv:2311.08106v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Yujin Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1">Jaehong Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1">Seonghyeon Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1">Sung Ju Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1">Se-young Yun</a></p>
<p>In an ever-evolving world, the dynamic nature of knowledge presents
challenges for language models that are trained on static data, leading to
outdated encoded information. However, real-world scenarios require models not
only to acquire new knowledge but also to overwrite outdated information into
updated ones. To address this under-explored issue, we introduce the temporally
evolving question answering benchmark, EvolvingQA - a novel benchmark designed
for training and evaluating LMs on an evolving Wikipedia database, where the
construction of our benchmark is automated with our pipeline using large
language models. Our benchmark incorporates question-answering as a downstream
task to emulate real-world applications. Through EvolvingQA, we uncover that
existing continual learning baselines have difficulty in updating and
forgetting outdated knowledge. Our findings suggest that the models fail to
learn updated knowledge due to the small weight gradient. Furthermore, we
elucidate that the models struggle mostly on providing numerical or temporal
answers to questions asking for updated knowledge. Our work aims to model the
dynamic nature of real-world information, offering a robust measure for the
evolution-adaptability of language models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08107">SAIE Framework: Support Alone Isn&#x27;t Enough -- Advancing LLM Training with Adversarial Remarks. (arXiv:2311.08107v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Loem_M/0/1/0/all/0/1">Mengsay Loem</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaneko_M/0/1/0/all/0/1">Masahiro Kaneko</a>, <a href="http://arxiv.org/find/cs/1/au:+Okazaki_N/0/1/0/all/0/1">Naoaki Okazaki</a></p>
<p>Large Language Models (LLMs) can justify or criticize their predictions
through discussion with other models or humans, thereby enhancing their
intrinsic understanding of instances. While proactive discussions enhance
performance, this approach is currently limited to the inference phase. In this
context, we posit a hypothesis: learning interactive discussions during
training can improve understanding for the instances in the training step and
proficiency in logical/critical thinking ability and verbalized expression of
the model in the inference step. Our proposed SAIE training method involves
both supportive and adversarial discussions between the learner and partner
models. The learner model receives a remark from the partner through the
discussion, and the parameters of the learner model are then updated based on
this remark. That is, the teacher signal dynamically adjusts in response to the
evolving model output throughout the training step. By bolstering the capacity
for discussion and comprehension of instances, our experiments across datasets,
including GSM8K, CommonsenseQA, and MMLU, reveal that models fine-tuned with
our method consistently surpass those trained with standard fine-tuning
techniques. Moreover, our approach demonstrates superior performance in
multi-agent inference scenarios, boosting the models' reasoning abilities at
the inference step.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08110">Improving hateful memes detection via learning hatefulness-aware embedding space through retrieval-guided contrastive learning. (arXiv:2311.08110v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mei_J/0/1/0/all/0/1">Jingbiao Mei</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jinghong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1">Weizhe Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Byrne_B/0/1/0/all/0/1">Bill Byrne</a>, <a href="http://arxiv.org/find/cs/1/au:+Tomalin_M/0/1/0/all/0/1">Marcus Tomalin</a></p>
<p>Hateful memes have emerged as a significant concern on the Internet. These
memes, which are a combination of image and text, often convey messages vastly
different from their individual meanings. Thus, detecting hateful memes
requires the system to jointly understand the visual and textual modalities.
However, our investigation reveals that the embedding space of existing
CLIP-based systems lacks sensitivity to subtle differences in memes that are
vital for correct hatefulness classification. To address this issue, we propose
constructing a hatefulness-aware embedding space through retrieval-guided
contrastive training. Specifically, we add an auxiliary loss that utilizes hard
negative and pseudo-gold samples to train the embedding space. Our approach
achieves state-of-the-art performance on the HatefulMemes dataset with an AUROC
of 86.7. Notably, our approach outperforms much larger fine-tuned Large
Multimodal Models like Flamingo and LLaVA. Finally, we demonstrate a
retrieval-based hateful memes detection system, which is capable of making
hatefulness classification based on data unseen in training from a database.
This allows developers to update the hateful memes detection system by simply
adding new data without retraining, a desirable feature for real services in
the constantly-evolving landscape of hateful memes on the Internet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08117">Insights into Classifying and Mitigating LLMs&#x27; Hallucinations. (arXiv:2311.08117v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bruno_A/0/1/0/all/0/1">Alessandro Bruno</a>, <a href="http://arxiv.org/find/cs/1/au:+Mazzeo_P/0/1/0/all/0/1">Pier Luigi Mazzeo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chetouani_A/0/1/0/all/0/1">Aladine Chetouani</a>, <a href="http://arxiv.org/find/cs/1/au:+Tliba_M/0/1/0/all/0/1">Marouane Tliba</a>, <a href="http://arxiv.org/find/cs/1/au:+Kerkouri_M/0/1/0/all/0/1">Mohamed Amine Kerkouri</a></p>
<p>The widespread adoption of large language models (LLMs) across diverse AI
applications is proof of the outstanding achievements obtained in several
tasks, such as text mining, text generation, and question answering. However,
LLMs are not exempt from drawbacks. One of the most concerning aspects regards
the emerging problematic phenomena known as "Hallucinations". They manifest in
text generation systems, particularly in question-answering systems reliant on
LLMs, potentially resulting in false or misleading information propagation.
This paper delves into the underlying causes of AI hallucination and elucidates
its significance in artificial intelligence. In particular, Hallucination
classification is tackled over several tasks (Machine Translation, Question and
Answer, Dialog Systems, Summarisation Systems, Knowledge Graph with LLMs, and
Visual Question Answer). Additionally, we explore potential strategies to
mitigate hallucinations, aiming to enhance the overall reliability of LLMs. Our
research addresses this critical issue within the HeReFaNMi (Health-Related
Fake News Mitigation) project, generously supported by NGI Search, dedicated to
combating Health-Related Fake News dissemination on the Internet. This
endeavour represents a concerted effort to safeguard the integrity of
information dissemination in an age of evolving AI technologies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08123">Memory-efficient Stochastic methods for Memory-based Transformers. (arXiv:2311.08123v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vishnu_V/0/1/0/all/0/1">Vishwajit Kumar Vishnu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sekhar_C/0/1/0/all/0/1">C. Chandra Sekhar</a></p>
<p>Training Memory-based transformers can require a large amount of memory and
can be quite inefficient. We propose a novel two-phase training mechanism and a
novel regularization technique to improve the training efficiency of
memory-based transformers, which are often used for long-range context
problems. For our experiments, we consider transformer-XL as our baseline model
which is one of memorybased transformer models. We show that our resultant
model, Skip Cross-head TransformerXL, outperforms the baseline on character
level language modeling task with similar parameters and outperforms the
baseline on word level language modelling task with almost 20% fewer
parameters. Our proposed methods do not require any additional memory. We also
demonstrate the effectiveness of our regularization mechanism on BERT which
shows similar performance with reduction in standard deviation of scores of
around 30% on multiple GLUE tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08143">Sinkhorn Transformations for Single-Query Postprocessing in Text-Video Retrieval. (arXiv:2311.08143v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yakovlev_K/0/1/0/all/0/1">Konstantin Yakovlev</a>, <a href="http://arxiv.org/find/cs/1/au:+Polyakov_G/0/1/0/all/0/1">Gregory Polyakov</a>, <a href="http://arxiv.org/find/cs/1/au:+Alimova_I/0/1/0/all/0/1">Ilseyar Alimova</a>, <a href="http://arxiv.org/find/cs/1/au:+Podolskiy_A/0/1/0/all/0/1">Alexander Podolskiy</a>, <a href="http://arxiv.org/find/cs/1/au:+Bout_A/0/1/0/all/0/1">Andrey Bout</a>, <a href="http://arxiv.org/find/cs/1/au:+Nikolenko_S/0/1/0/all/0/1">Sergey Nikolenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Piontkovskaya_I/0/1/0/all/0/1">Irina Piontkovskaya</a></p>
<p>A recent trend in multimodal retrieval is related to postprocessing test set
results via the dual-softmax loss (DSL). While this approach can bring
significant improvements, it usually presumes that an entire matrix of test
samples is available as DSL input. This work introduces a new postprocessing
approach based on Sinkhorn transformations that outperforms DSL. Further, we
propose a new postprocessing setting that does not require access to multiple
test queries. We show that our approach can significantly improve the results
of state of the art models such as CLIP4Clip, BLIP, X-CLIP, and DRL, thus
achieving a new state-of-the-art on several standard text-video retrieval
datasets both with access to the entire test set and in the single-query
setting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08147">RECALL: A Benchmark for LLMs Robustness against External Counterfactual Knowledge. (arXiv:2311.08147v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1">Lianzhe Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shicheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Sishuo Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Hao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1">Fandong Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jie Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xu Sun</a></p>
<p>LLMs and AI chatbots have improved people's efficiency in various fields.
However, the necessary knowledge for answering the question may be beyond the
models' knowledge boundaries. To mitigate this issue, many researchers try to
introduce external knowledge, such as knowledge graphs and Internet contents,
into LLMs for up-to-date information. However, the external information from
the Internet may include counterfactual information that will confuse the model
and lead to an incorrect response. Thus there is a pressing need for LLMs to
possess the ability to distinguish reliable information from external
knowledge. Therefore, to evaluate the ability of LLMs to discern the
reliability of external knowledge, we create a benchmark from existing
knowledge bases. Our benchmark consists of two tasks, Question Answering and
Text Generation, and for each task, we provide models with a context containing
counterfactual information. Evaluation results show that existing LLMs are
susceptible to interference from unreliable external knowledge with
counterfactual information, and simple intervention methods make limited
contributions to the alleviation of this issue.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08152">Towards Reasoning in Large Language Models via Multi-Agent Peer Review Collaboration. (arXiv:2311.08152v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zhenran Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1">Senbao Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1">Baotian Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Jindi Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dongfang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Min Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yuxiang Wu</a></p>
<p>Large Language Models (LLMs) have shown remarkable capabilities in general
natural language processing tasks but often fall short in complex reasoning
tasks. Recent studies have explored human-like problem-solving strategies, such
as self-correct, to push further the boundary of single-model reasoning
ability. In this work, we let a single model "step outside the box" by engaging
multiple models to correct each other. We introduce a multi-agent collaboration
strategy that emulates the academic peer review process. Each agent
independently constructs its own solution, provides reviews on the solutions of
others, and assigns confidence levels to its reviews. Upon receiving peer
reviews, agents revise their initial solutions. Extensive experiments on three
different types of reasoning tasks show that our collaboration approach
delivers superior accuracy across all ten datasets compared to existing
methods. Further study demonstrates the effectiveness of integrating confidence
in the reviews for math reasoning, and suggests a promising direction for
human-mimicking multi-agent collaboration process.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08154">Ask One More Time: Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios. (arXiv:2311.08154v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1">Lei Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1">Jiayi Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1">Pengli Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1">Junchen Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1">Fuzheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhongyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Di Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gai_K/0/1/0/all/0/1">Kun Gai</a></p>
<p>Although chain-of-thought (CoT) prompting combined with language models has
achieved encouraging results on complex reasoning tasks, the naive greedy
decoding used in CoT prompting usually causes the repetitiveness and local
optimality. To address this shortcoming, ensemble-optimization tries to obtain
multiple reasoning paths to get the final answer assembly. However, current
ensemble-optimization methods either simply employ rule-based post-processing
such as \textit{self-consistency}, or train an additional model based on
several task-related human annotations to select the best one among multiple
reasoning paths, yet fail to generalize to realistic settings where the type of
input questions is unknown or the answer format of reasoning paths is unknown.
To avoid their limitations, we propose \textbf{self-agreement}, a generalizable
ensemble-optimization method applying in almost all scenarios where the type of
input questions and the answer format of reasoning paths may be known or
unknown. Self-agreement firstly samples from language model's decoder to
generate a \textit{diverse} set of reasoning paths, and subsequently prompts
the language model \textit{one more time} to determine the optimal answer by
selecting the most \textit{agreed} answer among the sampled reasoning paths.
Self-agreement simultaneously achieves remarkable performance on six public
reasoning benchmarks and superior generalization capabilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08166">MechAgents: Large language model multi-agent collaborations can solve mechanics problems, generate new data, and integrate knowledge. (arXiv:2311.08166v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1">Bo Ni</a>, <a href="http://arxiv.org/find/cs/1/au:+Buehler_M/0/1/0/all/0/1">Markus J. Buehler</a></p>
<p>Solving mechanics problems using numerical methods requires comprehensive
intelligent capability of retrieving relevant knowledge and theory,
constructing and executing codes, analyzing the results, a task that has thus
far mainly been reserved for humans. While emerging AI methods can provide
effective approaches to solve end-to-end problems, for instance via the use of
deep surrogate models or various data analytics strategies, they often lack
physical intuition since knowledge is baked into the parametric complement
through training, offering less flexibility when it comes to incorporating
mathematical or physical insights. By leveraging diverse capabilities of
multiple dynamically interacting large language models (LLMs), we can overcome
the limitations of conventional approaches and develop a new class of
physics-inspired generative machine learning platform, here referred to as
MechAgents. A set of AI agents can solve mechanics tasks, here demonstrated for
elasticity problems, via autonomous collaborations. A two-agent team can
effectively write, execute and self-correct code, in order to apply finite
element methods to solve classical elasticity problems in various flavors
(different boundary conditions, domain geometries, meshes, small/finite
deformation and linear/hyper-elastic constitutive laws, and others). For more
complex tasks, we construct a larger group of agents with enhanced division of
labor among planning, formulating, coding, executing and criticizing the
process and results. The agents mutually correct each other to improve the
overall team-work performance in understanding, formulating and validating the
solution. Our framework shows the potential of synergizing the intelligence of
language models, the reliability of physics-based modeling, and the dynamic
collaborations among diverse agents, opening novel avenues for automation of
solving engineering problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08182">Self-Evolved Diverse Data Sampling for Efficient Instruction Tuning. (arXiv:2311.08182v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Shengguang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1">Keming Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1">Benfeng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Junyang Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1">Qi Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1">Chang Zhou</a></p>
<p>Enhancing the instruction-following ability of Large Language Models (LLMs)
primarily demands substantial instruction-tuning datasets. However, the sheer
volume of these imposes a considerable computational burden and annotation
cost. To investigate a label-efficient instruction tuning method that allows
the model itself to actively sample subsets that are equally or even more
effective, we introduce a self-evolving mechanism DiverseEvol. In this process,
a model iteratively augments its training subset to refine its own performance,
without requiring any intervention from humans or more advanced LLMs. The key
to our data sampling technique lies in the enhancement of diversity in the
chosen subsets, as the model selects new data points most distinct from any
existing ones according to its current embedding space. Extensive experiments
across three datasets and benchmarks demonstrate the effectiveness of
DiverseEvol. Our models, trained on less than 8% of the original dataset,
maintain or improve performance compared with finetuning on full data. We also
provide empirical evidence to analyze the importance of diversity in
instruction data and the iterative scheme as opposed to one-time sampling. Our
code is publicly available at https://github.com/OFA-Sys/DiverseEvol.git.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08189">Unlocking Science: Novel Dataset and Benchmark for Cross-Modality Scientific Information Extraction. (arXiv:2311.08189v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuhan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jian Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Zhiwei Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Karlsso_B/0/1/0/all/0/1">B&#xf6;rje F. Karlsso</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1">Wei Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Okumura_M/0/1/0/all/0/1">Manabu Okumura</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1">Chin-Yew Lin</a></p>
<p>Extracting key information from scientific papers has the potential to help
researchers work more efficiently and accelerate the pace of scientific
progress. Over the last few years, research on Scientific Information
Extraction (SciIE) witnessed the release of several new systems and benchmarks.
However, existing paper-focused datasets mostly focus only on specific parts of
a manuscript (e.g., abstracts) and are single-modality (i.e., text- or
table-only), due to complex processing and expensive annotations. Moreover,
core information can be present in either text or tables or across both. To
close this gap in data availability and enable cross-modality IE, while
alleviating labeling costs, we propose a semi-supervised pipeline for
annotating entities in text, as well as entities and relations in tables, in an
iterative procedure. Based on this pipeline, we release novel resources for the
scientific community, including a high-quality benchmark, a large-scale corpus,
and a semi-supervised annotation pipeline. We further report the performance of
state-of-the-art IE models on the proposed benchmark dataset, as a baseline.
Lastly, we explore the potential capability of large language models such as
ChatGPT for the current task. Our new dataset, results, and analysis validate
the effectiveness and efficiency of our semi-supervised pipeline, and we
discuss its remaining limitations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08191">GEC-DePenD: Non-Autoregressive Grammatical Error Correction with Decoupled Permutation and Decoding. (arXiv:2311.08191v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yakovlev_K/0/1/0/all/0/1">Konstantin Yakovlev</a>, <a href="http://arxiv.org/find/cs/1/au:+Podolskiy_A/0/1/0/all/0/1">Alexander Podolskiy</a>, <a href="http://arxiv.org/find/cs/1/au:+Bout_A/0/1/0/all/0/1">Andrey Bout</a>, <a href="http://arxiv.org/find/cs/1/au:+Nikolenko_S/0/1/0/all/0/1">Sergey Nikolenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Piontkovskaya_I/0/1/0/all/0/1">Irina Piontkovskaya</a></p>
<p>Grammatical error correction (GEC) is an important NLP task that is currently
usually solved with autoregressive sequence-to-sequence models. However,
approaches of this class are inherently slow due to one-by-one token
generation, so non-autoregressive alternatives are needed. In this work, we
propose a novel non-autoregressive approach to GEC that decouples the
architecture into a permutation network that outputs a self-attention weight
matrix that can be used in beam search to find the best permutation of input
tokens (with auxiliary {ins} tokens) and a decoder network based on a
step-unrolled denoising autoencoder that fills in specific tokens. This allows
us to find the token permutation after only one forward pass of the permutation
network, avoiding autoregressive constructions. We show that the resulting
network improves over previously known non-autoregressive methods for GEC and
reaches the level of autoregressive methods that do not use language-specific
synthetic data generation methods. Our results are supported by a comprehensive
experimental validation on the ConLL-2014 and Write&amp;Improve+LOCNESS datasets
and an extensive ablation study that supports our architectural and algorithmic
choices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08195">Automated Fact-Checking in Dialogue: Are Specialized Models Needed?. (arXiv:2311.08195v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chamoun_E/0/1/0/all/0/1">Eric Chamoun</a>, <a href="http://arxiv.org/find/cs/1/au:+Saeidi_M/0/1/0/all/0/1">Marzieh Saeidi</a>, <a href="http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1">Andreas Vlachos</a></p>
<p>Prior research has shown that typical fact-checking models for stand-alone
claims struggle with claims made in dialogues. As a solution, fine-tuning these
models on labelled dialogue data has been proposed. However, creating separate
models for each use case is impractical, and we show that fine-tuning models
for dialogue results in poor performance on typical fact-checking. To overcome
this challenge, we present techniques that allow us to use the same models for
both dialogue and typical fact-checking. These mainly focus on retrieval
adaptation and transforming conversational inputs so that they can be
accurately predicted by models trained on stand-alone claims. We demonstrate
that a typical fact-checking model incorporating these techniques is
competitive with state-of-the-art models fine-tuned for dialogue, while
maintaining its accuracy on stand-alone claims.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08206">Human-Centric Autonomous Systems With LLMs for User Command Reasoning. (arXiv:2311.08206v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qingwen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Ci Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Marta_D/0/1/0/all/0/1">Daniel Sim&#xf5;es Marta</a>, <a href="http://arxiv.org/find/cs/1/au:+Batool_N/0/1/0/all/0/1">Nazre Batool</a>, <a href="http://arxiv.org/find/cs/1/au:+Folkesson_J/0/1/0/all/0/1">John Folkesson</a></p>
<p>The evolution of autonomous driving has made remarkable advancements in
recent years, evolving into a tangible reality. However, a human-centric
large-scale adoption hinges on meeting a variety of multifaceted requirements.
To ensure that the autonomous system meets the user's intent, it is essential
to accurately discern and interpret user commands, especially in complex or
emergency situations. To this end, we propose to leverage the reasoning
capabilities of Large Language Models (LLMs) to infer system requirements from
in-cabin users' commands. Through a series of experiments that include
different LLM models and prompt designs, we explore the few-shot multivariate
binary classification accuracy of system requirements from natural language
textual commands. We confirm the general ability of LLMs to understand and
reason about prompts but underline that their effectiveness is conditioned on
the quality of both the LLM model and the design of appropriate sequential
prompts. Code and models are public with the link
\url{https://github.com/KTH-RPL/DriveCmd_LLM}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08213">Unlock the Power: Competitive Distillation for Multi-Modal Large Language Models. (arXiv:2311.08213v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xinwei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1">Li Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1">Chen Qian</a></p>
<p>Recently, multi-modal content generation has attracted lots of attention from
researchers by investigating the utilization of visual instruction tuning based
on large language models (LLMs). To enhance the performance and generalization
ability of such LLMs, the practice of distilling knowledge from pretrained
multi-modal models (a.k.a. teachers) to more compact multi-modal LLMs
(students) has gained considerable interest. However, the prevailing paradigm
of instructiontuning in multi-modal LLMs knowledge distillation is
resource-intensive and unidirectional, neglecting the potential for mutual
feedback between the student and teacher models. Thus, we propose an innovative
Competitive Multi-modal Distillation framework (CoMD), which captures
bidirectional feedback between teacher and student models and continually
updates the multi-modal capabilities that the student model has learned. It
comprises two stages: multi-modal pre-training and multi-modal competitive
distillation. The first stage pre-trains the student model on a large number of
filtered multi-modal datasets. The second stage facilitates a bidirectional
knowledge transfer between the student and teacher models. Our experimental
analysis of diverse datasets shows that our knowledge transfer method
consistently improves the capabilities of the student model. Finally, the
7B-sized student model after four distillations surpassed the current
state-of-the-art model LLaVA-13B on the ScienceQA and LLaVA Test dataset, also
outperforms other strong baselines in the zero-shot setting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08219">Eval-GCSC: A New Metric for Evaluating ChatGPT&#x27;s Performance in Chinese Spelling Correction. (arXiv:2311.08219v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Kunting Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yong Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shaolei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1">Hanhan Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1">Liang He</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1">Fandong Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jie Zhou</a></p>
<p>ChatGPT has demonstrated impressive performance in various downstream tasks.
However, in the Chinese Spelling Correction (CSC) task, we observe a
discrepancy: while ChatGPT performs well under human evaluation, it scores
poorly according to traditional metrics. We believe this inconsistency arises
because the traditional metrics are not well-suited for evaluating generative
models. Their overly strict length and phonics constraints may lead to
underestimating ChatGPT's correction capabilities. To better evaluate
generative models in the CSC task, this paper proposes a new evaluation metric:
Eval-GCSC. By incorporating word-level and semantic similarity judgments, it
relaxes the stringent length and phonics constraints. Experimental results show
that Eval-GCSC closely aligns with human evaluations. Under this metric,
ChatGPT's performance is comparable to traditional token-level classification
models (TCM), demonstrating its potential as a CSC tool. The source code and
scripts can be accessed at https://github.com/ktlKTL/Eval-GCSC.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08240">Investigating the Encoding of Words in BERT&#x27;s Neurons using Feature Textualization. (arXiv:2311.08240v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Baeumel_T/0/1/0/all/0/1">Tanja Baeumel</a>, <a href="http://arxiv.org/find/cs/1/au:+Vijayakumar_S/0/1/0/all/0/1">Soniya Vijayakumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Genabith_J/0/1/0/all/0/1">Josef van Genabith</a>, <a href="http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1">Guenter Neumann</a>, <a href="http://arxiv.org/find/cs/1/au:+Ostermann_S/0/1/0/all/0/1">Simon Ostermann</a></p>
<p>Pretrained language models (PLMs) form the basis of most state-of-the-art NLP
technologies. Nevertheless, they are essentially black boxes: Humans do not
have a clear understanding of what knowledge is encoded in different parts of
the models, especially in individual neurons. The situation is different in
computer vision, where feature visualization provides a decompositional
interpretability technique for neurons of vision models. Activation
maximization is used to synthesize inherently interpretable visual
representations of the information encoded in individual neurons. Our work is
inspired by this but presents a cautionary tale on the interpretability of
single neurons, based on the first large-scale attempt to adapt activation
maximization to NLP, and, more specifically, large PLMs. We propose feature
textualization, a technique to produce dense representations of neurons in the
PLM word embedding space. We apply feature textualization to the BERT model
(Devlin et al., 2019) to investigate whether the knowledge encoded in
individual neurons can be interpreted and symbolized. We find that the produced
representations can provide insights about the knowledge encoded in individual
neurons, but that individual neurons do not represent clearcut symbolic units
of language such as words. Additionally, we use feature textualization to
investigate how many neurons are needed to encode words in BERT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08249">On Using Distribution-Based Compositionality Assessment to Evaluate Compositional Generalisation in Machine Translation. (arXiv:2311.08249v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Moisio_A/0/1/0/all/0/1">Anssi Moisio</a>, <a href="http://arxiv.org/find/cs/1/au:+Creutz_M/0/1/0/all/0/1">Mathias Creutz</a>, <a href="http://arxiv.org/find/cs/1/au:+Kurimo_M/0/1/0/all/0/1">Mikko Kurimo</a></p>
<p>Compositional generalisation (CG), in NLP and in machine learning more
generally, has been assessed mostly using artificial datasets. It is important
to develop benchmarks to assess CG also in real-world natural language tasks in
order to understand the abilities and limitations of systems deployed in the
wild. To this end, our GenBench Collaborative Benchmarking Task submission
utilises the distribution-based compositionality assessment (DBCA) framework to
split the Europarl translation corpus into a training and a test set in such a
way that the test set requires compositional generalisation capacity.
Specifically, the training and test sets have divergent distributions of
dependency relations, testing NMT systems' capability of translating
dependencies that they have not been trained on. This is a fully-automated
procedure to create natural language compositionality benchmarks, making it
simple and inexpensive to apply it further to other datasets and languages. The
code and data for the experiments is available at
https://github.com/aalto-speech/dbca.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08252">REST: Retrieval-Based Speculative Decoding. (arXiv:2311.08252v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1">Zhenyu He</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1">Zexuan Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1">Tianle Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jason D Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1">Di He</a></p>
<p>We introduce Retrieval-Based Speculative Decoding (REST), a novel algorithm
designed to speed up language model generation. The key insight driving the
development of REST is the observation that the process of text generation
often includes certain common phases and patterns. Unlike previous methods that
rely on a draft language model for speculative decoding, REST harnesses the
power of retrieval to generate draft tokens. This method draws from the
reservoir of existing knowledge, retrieving and employing relevant tokens based
on the current context. Its plug-and-play nature allows for seamless
integration and acceleration of any language models, all without necessitating
additional training. When benchmarked on 7B and 13B language models in a
single-batch setting, REST achieves a significant speedup of 1.62X to 2.36X on
code or text generation. The code of REST is available at
https://github.com/FasterDecoding/REST.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08263">Fast Chain-of-Thought: A Glance of Future from Parallel Decoding Leads to Answers Faster. (arXiv:2311.08263v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongxuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhining Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1">Jiaqi Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_C/0/1/0/all/0/1">Chenyi Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jinjie Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1">Guihai Chen</a></p>
<p>In this work, we propose FastCoT, a model-agnostic framework based on
parallel decoding without any further training of an auxiliary model or
modification to the LLM itself. FastCoT uses a size-varying context window
whose size changes with position to conduct parallel decoding and
auto-regressive decoding simultaneously, thus fully utilizing GPU computation
resources. In FastCoT, the parallel decoding part provides the LLM with a quick
glance of the future composed of approximate tokens, which could lead to faster
answers compared to regular autoregressive decoding used by causal
transformers. We also provide an implementation of parallel decoding within
LLM, which supports KV-cache generation and batch processing. Through extensive
experiments, we demonstrate that FastCoT saves inference time by nearly 20%
with only a negligible performance drop compared to the regular approach.
Additionally, we show that the context window size exhibits considerable
robustness for different tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08268">A Wolf in Sheep&#x27;s Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily. (arXiv:2311.08268v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ding_P/0/1/0/all/0/1">Peng Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuang_J/0/1/0/all/0/1">Jun Kuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_D/0/1/0/all/0/1">Dan Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1">Xuezhi Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xian_Y/0/1/0/all/0/1">Yunsen Xian</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiajun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Shujian Huang</a></p>
<p>Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to
provide useful and safe responses. However, adversarial prompts known as
'jailbreaks' can circumvent safeguards, leading LLMs to generate harmful
content. Exploring jailbreak prompts can help to better reveal the weaknesses
of LLMs and further steer us to secure them. Unfortunately, existing jailbreak
methods either suffer from intricate manual design or require optimization on
another white-box model, compromising generalization or jailbreak efficiency.
In this paper, we generalize jailbreak prompt attacks into two aspects: (1)
Prompt Rewriting and (2) Scenario Nesting. Based on this, we propose ReNeLLM,
an automatic framework that leverages LLMs themselves to generate effective
jailbreak prompts. Extensive experiments demonstrate that ReNeLLM significantly
improves the attack success rate while greatly reducing the time cost compared
to existing baselines. Our study also reveals the inadequacy of current defense
methods in safeguarding LLMs. Finally, we offer detailed analysis and
discussion from the perspective of prompt execution priority on the failure of
LLMs' defense. We hope that our research can catalyze both the academic
community and LLMs vendors towards the provision of safer and more regulated
Large Language Models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08273">Examining Modularity in Multilingual LMs via Language-Specialized Subnetworks. (arXiv:2311.08273v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Choenni_R/0/1/0/all/0/1">Rochelle Choenni</a>, <a href="http://arxiv.org/find/cs/1/au:+Shutova_E/0/1/0/all/0/1">Ekaterina Shutova</a>, <a href="http://arxiv.org/find/cs/1/au:+Garrette_D/0/1/0/all/0/1">Dan Garrette</a></p>
<p>Recent work has proposed explicitly inducing language-wise modularity in
multilingual LMs via sparse fine-tuning (SFT) on per-language subnetworks as a
means of better guiding cross-lingual sharing. In this work, we investigate (1)
the degree to which language-wise modularity naturally arises within models
with no special modularity interventions, and (2) how cross-lingual sharing and
interference differ between such models and those with explicit SFT-guided
subnetwork modularity. To quantify language specialization and cross-lingual
interaction, we use a Training Data Attribution method that estimates the
degree to which a model's predictions are influenced by in-language or
cross-language training examples. Our results show that language-specialized
subnetworks do naturally arise, and that SFT, rather than always increasing
modularity, can decrease language specialization of subnetworks in favor of
more cross-lingual sharing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2106.05544">CogAlign: Learning to Align Textual Neural Representations to Cognitive Language Processing Signals. (arXiv:2106.05544v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1">Yuqi Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_D/0/1/0/all/0/1">Deyi Xiong</a></p>
<p>Most previous studies integrate cognitive language processing signals (e.g.,
eye-tracking or EEG data) into neural models of natural language processing
(NLP) just by directly concatenating word embeddings with cognitive features,
ignoring the gap between the two modalities (i.e., textual vs. cognitive) and
noise in cognitive features. In this paper, we propose a CogAlign approach to
these issues, which learns to align textual neural representations to cognitive
features. In CogAlign, we use a shared encoder equipped with a modality
discriminator to alternatively encode textual and cognitive inputs to capture
their differences and commonalities. Additionally, a text-aware attention
mechanism is proposed to detect task-related information and to avoid using
noise in cognitive features. Experimental results on three NLP tasks, namely
named entity recognition, sentiment analysis and relation extraction, show that
CogAlign achieves significant improvements with multiple cognitive features
over state-of-the-art models on public datasets. Moreover, our model is able to
transfer cognitive information to other datasets that do not have any cognitive
processing signals.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.13854">ComCLIP: Training-Free Compositional Image and Text Matching. (arXiv:2211.13854v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1">Kenan Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xuehai He</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">Ruize Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xin Eric Wang</a></p>
<p>Contrastive Language-Image Pretraining (CLIP) has demonstrated great
zero-shot performance for matching images and text. However, it is still
challenging to adapt vision-lanaguage pretrained models like CLIP to
compositional image and text matching -- a more challenging image and text
matching task requiring the model understanding of compositional word concepts
and visual components. Towards better compositional generalization in zero-shot
image and text matching, in this paper, we study the problem from a causal
perspective: the erroneous semantics of individual entities are essentially
confounders that cause the matching failure. Therefore, we propose a novel
\textbf{\textit{training-free}} compositional CLIP model (ComCLIP). ComCLIP
disentangles input images into subjects, objects, and action sub-images and
composes CLIP's vision encoder and text encoder to perform evolving matching
over compositional text embedding and sub-image embeddings. In this way,
ComCLIP can mitigate spurious correlations introduced by the pretrained CLIP
models and dynamically evaluate the importance of each component. Experiments
on four compositional image-text matching datasets: SVO, ComVG, Winoground, and
VL-checklist, and two general image-text retrieval datasets: Flick30K, and
MSCOCO demonstrate the effectiveness of our plug-and-play method, which boosts
the \textbf{\textit{zero-shot}} inference ability of CLIP, SLIP, and BLIP2 even
without further training or fine-tuning. Our codes can be found at
https://github.com/eric-ai-lab/ComCLIP.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.00768">Simplifying and Understanding State Space Models with Diagonal Linear RNNs. (arXiv:2212.00768v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1">Ankit Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Mehta_H/0/1/0/all/0/1">Harsh Mehta</a>, <a href="http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1">Jonathan Berant</a></p>
<p>Sequence models based on linear state spaces (SSMs) have recently emerged as
a promising choice of architecture for modeling long range dependencies across
various modalities. However, they invariably rely on discretization of a
continuous state space, which complicates their presentation and understanding.
In this work, we dispose of the discretization step, and propose a model based
on vanilla Diagonal Linear RNNs ($\mathrm{DLR}$). We empirically show that,
despite being conceptually much simpler, $\mathrm{DLR}$ is as performant as
previously-proposed SSMs on a variety of tasks and benchmarks including Long
Range Arena and raw speech classification. Moreover, we characterize the
expressivity of SSMs (including $\mathrm{DLR}$) and attention-based models via
a suite of $13$ synthetic sequence-to-sequence tasks involving interactions
over tens of thousands of tokens, ranging from simple operations, such as
shifting an input sequence, to detecting co-dependent visual features over long
spatial ranges in flattened images. We find that while SSMs report near-perfect
performance on tasks that can be modeled via $\textit{few}$ convolutional
kernels, they struggle on tasks requiring $\textit{many}$ such kernels and
especially when the desired sequence manipulation is
$\textit{context-dependent}$. Despite these limitations, $\mathrm{DLR}$ reaches
high performance on two higher-order reasoning tasks $\mathrm{ListOpsSubTrees}$
and $\mathrm{PathfinderSegmentation}\text{-}\mathrm{256}$ with input lengths
$8K$ and $65K$ respectively, and gives encouraging performance on
$\mathrm{PathfinderSegmentation}\text{-}\mathrm{512}$ with input length $262K$
for which attention is not a viable choice.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.02712">Improved Beam Search for Hallucination Mitigation in Abstractive Summarization. (arXiv:2212.02712v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sridhar_A/0/1/0/all/0/1">Arvind Krishna Sridhar</a>, <a href="http://arxiv.org/find/cs/1/au:+Visser_E/0/1/0/all/0/1">Erik Visser</a></p>
<p>Advancement in large pretrained language models has significantly improved
their performance for conditional language generation tasks including
summarization albeit with hallucinations. To reduce hallucinations,
conventional methods proposed improving beam search or using a fact checker as
a postprocessing step. In this paper, we investigate the use of the Natural
Language Inference (NLI) entailment metric to detect and prevent hallucinations
in summary generation. We propose an NLI-assisted beam re-ranking mechanism by
computing entailment probability scores between the input context and
summarization model-generated beams during saliency-enhanced greedy decoding.
Moreover, a diversity metric is introduced to compare its effectiveness against
vanilla beam search. Our proposed algorithm significantly outperforms vanilla
beam decoding on XSum and CNN/DM datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.04972">MOPRD: A multidisciplinary open peer review dataset. (arXiv:2212.04972v2 [cs.DL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Jialiang Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1">Jiaxin Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhangping Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yidong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1">Xiaodong Shi</a></p>
<p>Open peer review is a growing trend in academic publications. Public access
to peer review data can benefit both the academic and publishing communities.
It also serves as a great support to studies on review comment generation and
further to the realization of automated scholarly paper review. However, most
of the existing peer review datasets do not provide data that cover the whole
peer review process. Apart from this, their data are not diversified enough as
the data are mainly collected from the field of computer science. These two
drawbacks of the currently available peer review datasets need to be addressed
to unlock more opportunities for related studies. In response, we construct
MOPRD, a multidisciplinary open peer review dataset. This dataset consists of
paper metadata, multiple version manuscripts, review comments, meta-reviews,
author's rebuttal letters, and editorial decisions. Moreover, we propose a
modular guided review comment generation method based on MOPRD. Experiments
show that our method delivers better performance as indicated by both automatic
metrics and human evaluation. We also explore other potential applications of
MOPRD, including meta-review generation, editorial decision prediction, author
rebuttal generation, and scientometric analysis. MOPRD is a strong endorsement
for further studies in peer review-related research and other applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.06273">Consistency Analysis of ChatGPT. (arXiv:2303.06273v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jang_M/0/1/0/all/0/1">Myeongjun Erik Jang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1">Thomas Lukasiewicz</a></p>
<p>ChatGPT has gained a huge popularity since its introduction. Its positive
aspects have been reported through many media platforms, and some analyses even
showed that ChatGPT achieved a decent grade in professional exams, adding extra
support to the claim that AI can now assist and even replace humans in
industrial fields. Others, however, doubt its reliability and trustworthiness.
This paper investigates the trustworthiness of ChatGPT and GPT-4 regarding
logically consistent behaviour, focusing specifically on semantic consistency
and the properties of negation, symmetric, and transitive consistency. Our
findings suggest that while both models appear to show an enhanced language
understanding and reasoning ability, they still frequently fall short of
generating logically consistent predictions. We also ascertain via experiments
that prompt designing, few-shot learning and employing larger large language
models (LLMs) are unlikely to be the ultimate solution to resolve the
inconsistency issue of LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.16445">Larger Probes Tell a Different Story: Extending Psycholinguistic Datasets Via In-Context Learning. (arXiv:2303.16445v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shivagunde_N/0/1/0/all/0/1">Namrata Shivagunde</a>, <a href="http://arxiv.org/find/cs/1/au:+Lialin_V/0/1/0/all/0/1">Vladislav Lialin</a>, <a href="http://arxiv.org/find/cs/1/au:+Rumshisky_A/0/1/0/all/0/1">Anna Rumshisky</a></p>
<p>Language model probing is often used to test specific capabilities of models.
However, conclusions from such studies may be limited when the probing
benchmarks are small and lack statistical power. In this work, we introduce
new, larger datasets for negation (NEG-1500-SIMP) and role reversal (ROLE-1500)
inspired by psycholinguistic studies. We dramatically extend existing NEG-136
and ROLE-88 benchmarks using GPT3, increasing their size from 18 and 44
sentence pairs to 750 each. We also create another version of extended negation
dataset (NEG-1500-SIMP-TEMP), created using template-based generation. It
consists of 770 sentence pairs. We evaluate 22 models on the extended datasets,
seeing model performance dip 20-57% compared to the original smaller
benchmarks. We observe high levels of negation sensitivity in models like BERT
and ALBERT demonstrating that previous findings might have been skewed due to
smaller test sets. Finally, we observe that while GPT3 has generated all the
examples in ROLE-1500 is only able to solve 24.6% of them during probing. The
datasets and code are available on
$\href{https://github.com/text-machine-lab/extending_psycholinguistic_dataset}{Github}$.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.03520">Context-Aware Semantic Similarity Measurement for Unsupervised Word Sense Disambiguation. (arXiv:2305.03520v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Martinez_Gil_J/0/1/0/all/0/1">Jorge Martinez-Gil</a></p>
<p>The issue of word sense ambiguity poses a significant challenge in natural
language processing due to the scarcity of annotated data to feed machine
learning models to face the challenge. Therefore, unsupervised word sense
disambiguation methods have been developed to overcome that challenge without
relying on annotated data. This research proposes a new context-aware approach
to unsupervised word sense disambiguation, which provides a flexible mechanism
for incorporating contextual information into the similarity measurement
process. We experiment with a popular benchmark dataset to evaluate the
proposed strategy and compare its performance with state-of-the-art
unsupervised word sense disambiguation techniques. The experimental results
indicate that our approach substantially enhances disambiguation accuracy and
surpasses the performance of several existing techniques. Our findings
underscore the significance of integrating contextual information in semantic
similarity measurements to manage word sense ambiguity in unsupervised
scenarios effectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14647">Scientific Opinion Summarization: Meta-review Generation with Checklist-guided Iterative Introspection. (arXiv:2305.14647v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zeng_Q/0/1/0/all/0/1">Qi Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Sidhu_M/0/1/0/all/0/1">Mankeerat Sidhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_H/0/1/0/all/0/1">Hou Pong Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1">Heng Ji</a></p>
<p>Opinions in the scientific domain can be divergent, leading to controversy or
consensus among reviewers. However, current opinion summarization datasets
mostly focus on product review domains, which do not account for this
variability under the assumption that the input opinions are non-controversial.
To address this gap, we propose the task of scientific opinion summarization,
where research paper reviews are synthesized into meta-reviews. To facilitate
this task, we introduce a new ORSUM dataset covering 10,989 paper meta-reviews
and 40,903 paper reviews from 39 conferences. Furthermore, we propose the
Checklist-guided Iterative Introspection (CGI$^2$) approach, which breaks down
the task into several stages and iteratively refines the summary under the
guidance of questions from a checklist. We conclude that (1) human-written
summaries are not always reliable since many do not follow the guidelines, and
(2) the combination of task decomposition and iterative self-refinement shows
promising discussion involvement ability and can be applied to other complex
text generation using black-box LLM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14770">Using Natural Language Explanations to Rescale Human Judgments. (arXiv:2305.14770v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wadhwa_M/0/1/0/all/0/1">Manya Wadhwa</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jifan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Junyi Jessy Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1">Greg Durrett</a></p>
<p>The rise of large language models (LLMs) has brought a critical need for
high-quality human-labeled data, particularly for processes like human feedback
and evaluation. A common practice is to label data via consensus annotation
over crowdworker judgments. However, annotators' judgments for subjective tasks
can differ in many ways: they may have different qualitative judgments about an
example, and they may map those to a labeling scheme in different ways. We show
that these nuances can be captured by natural language explanations, and
propose a method to rescale ordinal annotations and explanations using LLMs.
Specifically, we feed annotators' Likert ratings and corresponding explanations
into an LLM and prompt it to produce a numeric score anchored in a scoring
rubric. These scores should reflect the annotators' underlying assessments of
the example. The rubric can be designed or modified after annotation, and
include distinctions that may not have been known when the original error
taxonomy was devised. We explore our technique in the context of rating system
outputs for a document-grounded question answering task, where LLMs achieve
near-human performance. Our method rescales the raw judgments without impacting
agreement and brings the scores closer to human judgments grounded in the same
scoring rubric.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14827">Pre-training Intent-Aware Encoders for Zero- and Few-Shot Intent Classification. (arXiv:2305.14827v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sung_M/0/1/0/all/0/1">Mujeen Sung</a>, <a href="http://arxiv.org/find/cs/1/au:+Gung_J/0/1/0/all/0/1">James Gung</a>, <a href="http://arxiv.org/find/cs/1/au:+Mansimov_E/0/1/0/all/0/1">Elman Mansimov</a>, <a href="http://arxiv.org/find/cs/1/au:+Pappas_N/0/1/0/all/0/1">Nikolaos Pappas</a>, <a href="http://arxiv.org/find/cs/1/au:+Shu_R/0/1/0/all/0/1">Raphael Shu</a>, <a href="http://arxiv.org/find/cs/1/au:+Romeo_S/0/1/0/all/0/1">Salvatore Romeo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Castelli_V/0/1/0/all/0/1">Vittorio Castelli</a></p>
<p>Intent classification (IC) plays an important role in task-oriented dialogue
systems. However, IC models often generalize poorly when training without
sufficient annotated examples for each user intent. We propose a novel
pre-training method for text encoders that uses contrastive learning with
intent psuedo-labels to produce embeddings that are well-suited for IC tasks,
reducing the need for manual annotations. By applying this pre-training
strategy, we also introduce Pre-trained Intent-aware Encoder (PIE), which is
designed to align encodings of utterances with their intent names.
Specifically, we first train a tagger to identify key phrases within utterances
that are crucial for interpreting intents. We then use these extracted phrases
to create examples for pre-training a text encoder in a contrastive manner. As
a result, our PIE model achieves up to 5.4% and 4.0% higher accuracy than the
previous state-of-the-art text encoder for the N-way zero- and one-shot
settings on four IC datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.17353">Complementary and Integrative Health Lexicon (CIHLex) and Entity Recognition in the Literature. (arXiv:2305.17353v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Huixue Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Austin_R/0/1/0/all/0/1">Robin Austin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1">Sheng-Chieh Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Silverman_G/0/1/0/all/0/1">Greg Silverman</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yuqi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Kilicoglu_H/0/1/0/all/0/1">Halil Kilicoglu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hua Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Rui Zhang</a></p>
<p>Objective: Our study aimed to construct an exhaustive Complementary and
Integrative Health (CIH) Lexicon (CIHLex) to better represent the often
underrepresented physical and psychological CIH approaches in standard
terminologies. We also intended to apply advanced Natural Language Processing
(NLP) models such as Bidirectional Encoder Representations from Transformers
(BERT) and GPT-3.5 Turbo for CIH named entity recognition, evaluating their
performance against established models like MetaMap and CLAMP. Materials and
Methods: We constructed the CIHLex by integrating various resources, compiling
and integrating data from biomedical literature and relevant knowledge bases.
The Lexicon encompasses 198 unique concepts with 1090 corresponding unique
terms. We matched these concepts to the Unified Medical Language System (UMLS).
Additionally, we developed and utilized BERT models and compared their
efficiency in CIH named entity recognition to that of other models such as
MetaMap, CLAMP, and GPT3.5-turbo. Results: From the 198 unique concepts in
CIHLex, 62.1% could be matched to at least one term in the UMLS. Moreover,
75.7% of the mapped UMLS Concept Unique Identifiers (CUIs) were categorized as
"Therapeutic or Preventive Procedure." Among the models applied to CIH named
entity recognition, BLUEBERT delivered the highest macro average F1-score of
0.90, surpassing other models. Conclusion: Our CIHLex significantly augments
representation of CIH approaches in biomedical literature. Demonstrating the
utility of advanced NLP models, BERT notably excelled in CIH entity
recognition. These results highlight promising strategies for enhancing
standardization and recognition of CIH terminology in biomedical contexts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.09299">Can Language Models Teach Weaker Agents? Teacher Explanations Improve Students via Personalization. (arXiv:2306.09299v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1">Swarnadeep Saha</a>, <a href="http://arxiv.org/find/cs/1/au:+Hase_P/0/1/0/all/0/1">Peter Hase</a>, <a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1">Mohit Bansal</a></p>
<p>A hallmark property of explainable AI models is the ability to teach other
agents, communicating knowledge of how to perform a task. While Large Language
Models perform complex reasoning by generating explanations for their
predictions, it is unclear whether they also make good teachers for weaker
agents. To address this, we consider a student-teacher framework between two
LLM agents and study if, when, and how the teacher should intervene with
natural language explanations to improve the student's performance. Since
communication is expensive, we define a budget such that the teacher only
communicates explanations for a fraction of the data, after which the student
should perform well on its own. We decompose the teaching problem along four
axes: (1) if teacher's test time intervention improve student predictions, (2)
when it is worth explaining a data point, (3) how the teacher should
personalize explanations to better teach the student, and (4) if teacher
explanations also improve students on future unexplained data. We first show
that teacher LLMs can indeed intervene on student reasoning to improve their
performance. Next, inspired by the Theory of Mind abilities of effective
teachers, we propose building two few-shot mental models of the student. The
first model defines an Intervention Function that simulates the utility of an
intervention, allowing the teacher to intervene when this utility is the
highest and improving student performance at lower budgets. The second model
enables the teacher to personalize explanations for a particular student and
outperform unpersonalized teachers. We also demonstrate that in multi-turn
interactions, teacher explanations generalize and learning from explained data
improves student performance on future unexplained data. Finally, we verify
that misaligned teachers can lower student performance to random chance by
intentionally misleading them.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06440">No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models. (arXiv:2307.06440v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kaddour_J/0/1/0/all/0/1">Jean Kaddour</a>, <a href="http://arxiv.org/find/cs/1/au:+Key_O/0/1/0/all/0/1">Oscar Key</a>, <a href="http://arxiv.org/find/cs/1/au:+Nawrot_P/0/1/0/all/0/1">Piotr Nawrot</a>, <a href="http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1">Pasquale Minervini</a>, <a href="http://arxiv.org/find/cs/1/au:+Kusner_M/0/1/0/all/0/1">Matt J. Kusner</a></p>
<p>The computation necessary for training Transformer-based language models has
skyrocketed in recent years. This trend has motivated research on efficient
training algorithms designed to improve training, validation, and downstream
performance faster than standard training. In this work, we revisit three
categories of such algorithms: dynamic architectures (layer stacking, layer
dropping), batch selection (selective backprop, RHO loss), and efficient
optimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed
computation budget using such methods, we find that their training, validation,
and downstream gains vanish compared to a baseline with a fully-decayed
learning rate. We define an evaluation protocol that enables computation to be
done on arbitrary machines by mapping all computation time to a reference
machine which we call reference system time. We discuss the limitations of our
proposed protocol and release our code to encourage rigorous research in
efficient training procedures: https://github.com/JeanKaddour/NoTrainNoGain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10485">FinGPT: Democratizing Internet-scale Data for Financial Large Language Models. (arXiv:2307.10485v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiao-Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guoxuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Hongyang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zha_D/0/1/0/all/0/1">Daochen Zha</a></p>
<p>Large language models (LLMs) have demonstrated remarkable proficiency in
understanding and generating human-like texts, which may potentially
revolutionize the finance industry. However, existing LLMs often fall short in
the financial field, which is mainly attributed to the disparities between
general text data and financial text data. Unfortunately, there is only a
limited number of financial text datasets available, and BloombergGPT, the
first financial LLM (FinLLM), is close-sourced (only the training logs were
released). In light of this, we aim to democratize Internet-scale financial
data for LLMs, which is an open challenge due to diverse data sources, low
signal-to-noise ratio, and high time-validity. To address the challenges, we
introduce an open-sourced and data-centric framework, Financial Generative
Pre-trained Transformer (FinGPT), that automates the collection and curation of
real-time financial data from 34 diverse sources on the Internet, providing
researchers and practitioners with accessible and transparent resources to
develop their FinLLMs. Additionally, we propose a simple yet effective strategy
for fine-tuning FinLLM using the inherent feedback from the market, dubbed
Reinforcement Learning with Stock Prices (RLSP). We also adopt the Low-rank
Adaptation (LoRA, QLoRA) method that enables users to customize their own
FinLLMs from general-purpose LLMs at a low cost. Finally, we showcase several
FinGPT applications, including robo-advisor, sentiment analysis for algorithmic
trading, and low-code development. FinGPT aims to democratize FinLLMs,
stimulate innovation, and unlock new opportunities in open finance. The codes
have been open-sourced.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.07336">Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic. (arXiv:2308.07336v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Morishita_T/0/1/0/all/0/1">Terufumi Morishita</a>, <a href="http://arxiv.org/find/cs/1/au:+Morio_G/0/1/0/all/0/1">Gaku Morio</a>, <a href="http://arxiv.org/find/cs/1/au:+Yamaguchi_A/0/1/0/all/0/1">Atsuki Yamaguchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Sogawa_Y/0/1/0/all/0/1">Yasuhiro Sogawa</a></p>
<p>We study a synthetic corpus based approach for language models (LMs) to
acquire logical deductive reasoning ability. The previous studies generated
deduction examples using specific sets of deduction rules. However, these rules
were limited or otherwise arbitrary, limiting the generalizability of acquired
reasoning ability. We rethink this and adopt a well-grounded set of deduction
rules based on formal logic theory, which can derive any other deduction rules
when combined in a multistep way. Then, using the proposed corpora, which we
name FLD (Formal Logic Deduction), we first evaluate and analyze the logical
reasoning ability of the latest LLMs. Even GPT-4 can solve only half of the
problems, suggesting that pure logical reasoning isolated from knowledge is
still challenging for the LLMs, and additional training specialized in logical
reasoning is indeed essential. We next empirically verify that LMs trained on
FLD corpora acquire more generalizable reasoning ability. Furthermore, we
identify the aspects of reasoning ability on which deduction corpora can
enhance LMs and those on which they cannot, and discuss future directions on
each aspect. The released corpora serve both as learning resources and as
challenging benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.10874">Analyzing Transformer Dynamics as Movement through Embedding Space. (arXiv:2308.10874v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Sumeet S. Singh</a></p>
<p>Transformer based language models exhibit intelligent behaviors such as
understanding natural language, recognizing patterns, acquiring knowledge,
reasoning, planning, reflecting and using tools. This paper explores how their
underlying mechanics give rise to intelligent behaviors. Towards that end, we
propose framing Transformer dynamics as movement through embedding space.
Examining Transformers through this perspective reveals key insights,
establishing a Theory of Transformers: 1) Intelligent behaviours map to paths
in Embedding Space which, the Transformer random-walks through during
inferencing. 2) LM training learns a probability distribution over all possible
paths. `Intelligence' is learnt by assigning higher probabilities to paths
representing intelligent behaviors. No learning can take place in-context;
context only narrows the subset of paths sampled during decoding. 5) The
Transformer is a self-mapping composition function, folding a context sequence
into a context-vector such that it's proximity to a token-vector reflects its
co-occurrence and conditioned probability. Thus, the physical arrangement of
vectors in Embedding Space determines path probabilities. 6) Context vectors
are composed by aggregating features of the sequence's tokens via a process we
call the encoding walk. Attention contributes a - potentially redundant -
association-bias to this process. 7) This process is comprised of two principal
operation types: filtering (data independent) and aggregation (data dependent).
This generalization unifies Transformers with other sequence models. Building
upon this foundation, we formalize a popular semantic interpretation of
embeddings into a ``concept-space theory'' and find some evidence of it's
validity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16039">Effective Long-Context Scaling of Foundation Models. (arXiv:2309.16039v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1">Wenhan Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jingyu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Molybog_I/0/1/0/all/0/1">Igor Molybog</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hejia Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhargava_P/0/1/0/all/0/1">Prajjwal Bhargava</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_R/0/1/0/all/0/1">Rui Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Martin_L/0/1/0/all/0/1">Louis Martin</a>, <a href="http://arxiv.org/find/cs/1/au:+Rungta_R/0/1/0/all/0/1">Rashi Rungta</a>, <a href="http://arxiv.org/find/cs/1/au:+Sankararaman_K/0/1/0/all/0/1">Karthik Abinav Sankararaman</a>, <a href="http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1">Barlas Oguz</a>, <a href="http://arxiv.org/find/cs/1/au:+Khabsa_M/0/1/0/all/0/1">Madian Khabsa</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1">Han Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mehdad_Y/0/1/0/all/0/1">Yashar Mehdad</a>, <a href="http://arxiv.org/find/cs/1/au:+Narang_S/0/1/0/all/0/1">Sharan Narang</a>, <a href="http://arxiv.org/find/cs/1/au:+Malik_K/0/1/0/all/0/1">Kshitiz Malik</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_A/0/1/0/all/0/1">Angela Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhosale_S/0/1/0/all/0/1">Shruti Bhosale</a>, <a href="http://arxiv.org/find/cs/1/au:+Edunov_S/0/1/0/all/0/1">Sergey Edunov</a>, <a href="http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1">Mike Lewis</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Sinong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1">Hao Ma</a></p>
<p>We present a series of long-context LLMs that support effective context
windows of up to 32,768 tokens. Our model series are built through continual
pretraining from Llama 2 with longer training sequences and on a dataset where
long texts are upsampled. We perform extensive evaluation on language modeling,
synthetic context probing tasks, and a wide range of research benchmarks. On
research benchmarks, our models achieve consistent improvements on most regular
tasks and significant improvements on long-context tasks over Llama 2. Notably,
with a cost-effective instruction tuning procedure that does not require
human-annotated long instruction data, the 70B variant can already surpass
gpt-3.5-turbo-16k's overall performance on a suite of long-context tasks.
Alongside these results, we provide an in-depth analysis on the individual
components of our method. We delve into Llama's position encodings and discuss
its limitation in modeling long dependencies. We also examine the impact of
various design choices in the pretraining process, including the data mix and
the training curriculum of sequence lengths -- our ablation experiments suggest
that having abundant long texts in the pretrain dataset is not the key to
achieving strong performance, and we empirically verify that long context
continual pretraining is more efficient and similarly effective compared to
pretraining from scratch with long sequences.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.17157">LatticeGen: A Cooperative Framework which Hides Generated Text in a Lattice for Privacy-Aware Generation on Cloud. (arXiv:2309.17157v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Mengke Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1">Tianxing He</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tianle Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mi_L/0/1/0/all/0/1">Lu Mi</a>, <a href="http://arxiv.org/find/cs/1/au:+Mireshghallah_F/0/1/0/all/0/1">Fatemehsadat Mireshghallah</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Binyi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1">Yulia Tsvetkov</a></p>
<p>In the current user-server interaction paradigm of prompted generation with
large language models (LLM) on cloud, the server fully controls the generation
process, which leaves zero options for users who want to keep the generated
text to themselves. We propose LatticeGen, a cooperative framework in which the
server still handles most of the computation while the user controls the
sampling operation. The key idea is that the true generated sequence is mixed
with noise tokens by the user and hidden in a noised lattice. Considering
potential attacks from a hypothetically malicious server and how the user can
defend against it, we propose the repeated beam-search attack and the mixing
noise scheme. In our experiments we apply LatticeGen to protect both prompt and
generation. It is shown that while the noised lattice degrades generation
quality, LatticeGen successfully protects the true generation to a remarkable
degree under strong attacks (more than 50% of the semantic remains hidden as
measured by BERTScore).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00566">Empowering Many, Biasing a Few: Generalist Credit Scoring through Large Language Models. (arXiv:2310.00566v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1">Duanyu Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1">Yongfu Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jimin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yifang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1">Qianqian Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1">Weiguang Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Lopez_Lira_A/0/1/0/all/0/1">Alejandro Lopez-Lira</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hao Wang</a></p>
<p>In the financial industry, credit scoring is a fundamental element, shaping
access to credit and determining the terms of loans for individuals and
businesses alike. Traditional credit scoring methods, however, often grapple
with challenges such as narrow knowledge scope and isolated evaluation of
credit tasks. Our work posits that Large Language Models (LLMs) have great
potential for credit scoring tasks, with strong generalization ability across
multiple tasks. To systematically explore LLMs for credit scoring, we propose
the first open-source comprehensive framework. We curate a novel benchmark
covering 9 datasets with 14K samples, tailored for credit assessment and a
critical examination of potential biases within LLMs, and the novel instruction
tuning data with over 45k samples. We then propose the first Credit and Risk
Assessment Large Language Model (CALM) by instruction tuning, tailored to the
nuanced demands of various financial risk assessment tasks. We evaluate CALM,
and existing state-of-art (SOTA) open source and close source LLMs on the build
benchmark. Our empirical results illuminate the capability of LLMs to not only
match but surpass conventional models, pointing towards a future where credit
scoring can be more inclusive, comprehensive, and unbiased. We contribute to
the industry's transformation by sharing our pioneering instruction-tuning
datasets, credit and risk assessment LLM, and benchmarks with the research
community and the financial industry.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01420">Ruffle&amp;Riley: Towards the Automated Induction of Conversational Tutoring Systems. (arXiv:2310.01420v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schmucker_R/0/1/0/all/0/1">Robin Schmucker</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1">Meng Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Azaria_A/0/1/0/all/0/1">Amos Azaria</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitchell_T/0/1/0/all/0/1">Tom Mitchell</a></p>
<p>Conversational tutoring systems (CTSs) offer learning experiences driven by
natural language interaction. They are known to promote high levels of
cognitive engagement and benefit learning outcomes, particularly in reasoning
tasks. Nonetheless, the time and cost required to author CTS content is a major
obstacle to widespread adoption. In this paper, we introduce a novel type of
CTS that leverages the recent advances in large language models (LLMs) in two
ways: First, the system induces a tutoring script automatically from a lesson
text. Second, the system automates the script orchestration via two LLM-based
agents (Ruffle&amp;Riley) with the roles of a student and a professor in a
learning-by-teaching format. The system allows a free-form conversation that
follows the ITS-typical inner and outer loop structure. In an initial
between-subject online user study (N = 100) comparing Ruffle&amp;Riley to simpler
QA chatbots and reading activity, we found no significant differences in
post-test scores. Nonetheless, in the learning experience survey, Ruffle&amp;Riley
users expressed higher ratings of understanding and remembering and further
perceived the offered support as more helpful and the conversation as coherent.
Our study provides insights for a new generation of scalable CTS technologies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07793">GenTKG: Generative Forecasting on Temporal Knowledge Graph. (arXiv:2310.07793v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1">Ruotong Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1">Xu Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yunpu Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1">Volker Tresp</a></p>
<p>The rapid advancements in large language models (LLMs) have ignited interest
in the temporal knowledge graph (tKG) domain, where conventional carefully
designed embedding-based and rule-based models dominate. The question remains
open of whether pre-trained LLMs can understand structured temporal relational
data and replace them as the foundation model for temporal relational
forecasting. Therefore, we bring temporal knowledge forecasting into the
generative setting. However, challenges occur in the huge chasms between
complex temporal graph data structure and sequential natural expressions LLMs
can handle, and between the enormous data sizes of tKGs and heavy computation
costs of finetuning LLMs. To address these challenges, we propose a novel
retrieval augmented generation framework that performs generative forecasting
on tKGs named GenTKG, which combines a temporal logical rule-based retrieval
strategy and lightweight parameter-efficient instruction tuning. Extensive
experiments have shown that GenTKG outperforms conventional methods of temporal
relational forecasting under low computation resources. GenTKG also highlights
remarkable transferability with exceeding performance on unseen datasets
without re-training. Our work reveals the huge potential of LLMs in the tKG
domain and opens a new frontier for generative forecasting on tKGs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16147">PreWoMe: Exploiting Presuppositions as Working Memory for Long Form Question Answering. (arXiv:2310.16147v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1">Wookje Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Jinsol Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kyungjae Lee</a></p>
<p>Information-seeking questions in long-form question answering (LFQA) often
prove misleading due to ambiguity or false presupposition in the question.
While many existing approaches handle misleading questions, they are tailored
to limited questions, which are insufficient in a real-world setting with
unpredictable input characteristics. In this work, we propose PreWoMe, a
unified approach capable of handling any type of information-seeking question.
The key idea of PreWoMe involves extracting presuppositions in the question and
exploiting them as working memory to generate feedback and action about the
question. Our experiment shows that PreWoMe is effective not only in tackling
misleading questions but also in handling normal ones, thereby demonstrating
the effectiveness of leveraging presuppositions, feedback, and action for
real-world QA settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17490">Improving Zero-shot Reader by Reducing Distractions from Irrelevant Documents in Open-Domain Question Answering. (arXiv:2310.17490v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1">Sukmin Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1">Jeongyeon Seo</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeong_S/0/1/0/all/0/1">Soyeong Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Jong C. Park</a></p>
<p>Large language models (LLMs) enable zero-shot approaches in open-domain
question answering (ODQA), yet with limited advancements as the reader is
compared to the retriever. This study aims at the feasibility of a zero-shot
reader that addresses the challenges of computational cost and the need for
labeled data. We find that LLMs are distracted due to irrelevant documents in
the retrieved set and the overconfidence of the generated answers when they are
exploited as zero-shot readers. To tackle these problems, we mitigate the
impact of such documents via Distraction-aware Answer Selection (DAS) with a
negation-based instruction and score adjustment for proper answer selection.
Experimental results show that our approach successfully handles distraction
across diverse scenarios, enhancing the performance of zero-shot readers.
Furthermore, unlike supervised readers struggling with unseen data, zero-shot
readers demonstrate outstanding transferability without any training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17784">Data-Centric Financial Large Language Models. (arXiv:2310.17784v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1">Zhixuan Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1">Huaiyu Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xinyuan Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yijia Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1">Fei Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Wanqing Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1">Xin Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_Q/0/1/0/all/0/1">Qing Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Longfei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jun Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Sheng Li</a></p>
<p>Large language models (LLMs) show promise for natural language tasks but
struggle when applied directly to complex domains like finance. LLMs have
difficulty reasoning about and integrating all relevant information. We propose
a data-centric approach to enable LLMs to better handle financial tasks. Our
key insight is that rather than overloading the LLM with everything at once, it
is more effective to preprocess and pre-understand the data. We create a
financial LLM (FLLM) using multitask prompt-based finetuning to achieve data
pre-processing and pre-understanding. However, labeled data is scarce for each
task. To overcome manual annotation costs, we employ abductive augmentation
reasoning (AAR) to automatically generate training data by modifying the pseudo
labels from FLLM's own outputs. Experiments show our data-centric FLLM with AAR
substantially outperforms baseline financial LLMs designed for raw text,
achieving state-of-the-art on financial analysis and interpretation tasks. We
also open source a new benchmark for financial analysis and interpretation. Our
methodology provides a promising path to unlock LLMs' potential for complex
real-world domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17940">Unified Segment-to-Segment Framework for Simultaneous Sequence Generation. (arXiv:2310.17940v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shaolei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yang Feng</a></p>
<p>Simultaneous sequence generation is a pivotal task for real-time scenarios,
such as streaming speech recognition, simultaneous machine translation and
simultaneous speech translation, where the target sequence is generated while
receiving the source sequence. The crux of achieving high-quality generation
with low latency lies in identifying the optimal moments for generating,
accomplished by learning a mapping between the source and target sequences.
However, existing methods often rely on task-specific heuristics for different
sequence types, limiting the model's capacity to adaptively learn the
source-target mapping and hindering the exploration of multi-task learning for
various simultaneous tasks. In this paper, we propose a unified
segment-to-segment framework (Seg2Seg) for simultaneous sequence generation,
which learns the mapping in an adaptive and unified manner. During the process
of simultaneous generation, the model alternates between waiting for a source
segment and generating a target segment, making the segment serve as the
natural bridge between the source and target. To accomplish this, Seg2Seg
introduces a latent segment as the pivot between source to target and explores
all potential source-target mappings via the proposed expectation training,
thereby learning the optimal moments for generating. Experiments on multiple
simultaneous generation tasks demonstrate that Seg2Seg achieves
state-of-the-art performance and exhibits better generality across various
tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19347">Improving Factual Consistency of Text Summarization by Adversarially Decoupling Comprehension and Embellishment Abilities of LLMs. (arXiv:2310.19347v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1">Huawen Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1">Yan Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1">Ting-En Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1">Zekun Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yuchuan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1">Fei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yongbin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1">Qianli Ma</a></p>
<p>Despite the recent progress in text summarization made by large language
models (LLMs), they often generate summaries that are factually inconsistent
with original articles, known as "hallucinations" in text generation. Unlike
previous small models (e.g., BART, T5), current LLMs make fewer silly mistakes
but more sophisticated ones, such as imposing cause and effect, adding false
details, overgeneralizing, etc. These hallucinations are challenging to detect
through traditional methods, which poses great challenges for improving the
factual consistency of text summarization. In this paper, we propose an
adversarially DEcoupling method to disentangle the Comprehension and
EmbellishmeNT abilities of LLMs (DECENT). Furthermore, we adopt a probing-based
efficient training to cover the shortage of sensitivity for true and false in
the training process of LLMs. In this way, LLMs are less confused about
embellishing and understanding; thus, they can execute the instructions more
accurately and have enhanced abilities to distinguish hallucinations.
Experimental results show that DECENT significantly improves the reliability of
text summarization based on LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.20410">FollowBench: A Multi-level Fine-grained Constraints Following Benchmark for Large Language Models. (arXiv:2310.20410v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yuxin Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yufei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1">Xingshan Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1">Wanjun Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Liangyou Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1">Fei Mi</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1">Lifeng Shang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1">Xin Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wei Wang</a></p>
<p>The ability to follow instructions is crucial for Large Language Models
(LLMs) to handle various real-world applications. Existing benchmarks primarily
focus on evaluating pure response quality, rather than assessing whether the
response follows constraints stated in the instruction. To fill this research
gap, in this paper, we propose FollowBench, a Multi-level Fine-grained
Constraints Following Benchmark for LLMs. FollowBench comprehensively includes
five different types (i.e., Content, Situation, Style, Format, and Example) of
fine-grained constraints. To enable a precise constraint following estimation
on diverse difficulties, we introduce a Multi-level mechanism that
incrementally adds a single constraint to the initial instruction at each
increased level. To assess whether LLMs' outputs have satisfied every
individual constraint, we propose to prompt strong LLMs with
constraint-evolution paths to handle challenging open-ended instructions. By
evaluating ten closed-source and open-source popular LLMs on FollowBench, we
highlight the weaknesses of LLMs in instruction following and point towards
potential avenues for future work. The data and code are publicly available at
https://github.com/YJiangcm/FollowBench.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.20689">Learning From Mistakes Makes LLM Better Reasoner. (arXiv:2310.20689v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+An_S/0/1/0/all/0/1">Shengnan An</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1">Zexiong Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zeqi Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1">Nanning Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1">Jian-Guang Lou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Weizhu Chen</a></p>
<p>Large language models (LLMs) recently exhibited remarkable reasoning
capabilities on solving math problems. To further improve this capability, this
work proposes Learning from Mistakes (LeMa), akin to human learning processes.
Consider a human student who failed to solve a math problem, he will learn from
what mistake he has made and how to correct it. Mimicking this error-driven
learning process, LeMa fine-tunes LLMs on mistake-correction data pairs
generated by GPT-4. Specifically, we first collect inaccurate reasoning paths
from various LLMs and then employ GPT-4 as a "corrector" to (1) identify the
mistake step, (2) explain the reason for the mistake, and (3) correct the
mistake and generate the final answer. Experimental results demonstrate the
effectiveness of LeMa: across five backbone LLMs and two mathematical reasoning
tasks, LeMa consistently improves the performance compared with fine-tuning on
CoT data alone. Impressively, LeMa can also benefit specialized LLMs such as
WizardMath and MetaMath, achieving 85.4% pass@1 accuracy on GSM8K and 27.1% on
MATH. This surpasses the SOTA performance achieved by non-execution open-source
models on these challenging tasks. Our code, data and models will be publicly
available at https://github.com/microsoft/LEMA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00176">ChipNeMo: Domain-Adapted LLMs for Chip Design. (arXiv:2311.00176v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Mingjie Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ene_T/0/1/0/all/0/1">Teodor-Dumitru Ene</a>, <a href="http://arxiv.org/find/cs/1/au:+Kirby_R/0/1/0/all/0/1">Robert Kirby</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1">Chris Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Pinckney_N/0/1/0/all/0/1">Nathaniel Pinckney</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_R/0/1/0/all/0/1">Rongjian Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Alben_J/0/1/0/all/0/1">Jonah Alben</a>, <a href="http://arxiv.org/find/cs/1/au:+Anand_H/0/1/0/all/0/1">Himyanshu Anand</a>, <a href="http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1">Sanmitra Banerjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Bayraktaroglu_I/0/1/0/all/0/1">Ismet Bayraktaroglu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhaskaran_B/0/1/0/all/0/1">Bonita Bhaskaran</a>, <a href="http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1">Bryan Catanzaro</a>, <a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_A/0/1/0/all/0/1">Arjun Chaudhuri</a>, <a href="http://arxiv.org/find/cs/1/au:+Clay_S/0/1/0/all/0/1">Sharon Clay</a>, <a href="http://arxiv.org/find/cs/1/au:+Dally_B/0/1/0/all/0/1">Bill Dally</a>, <a href="http://arxiv.org/find/cs/1/au:+Dang_L/0/1/0/all/0/1">Laura Dang</a>, <a href="http://arxiv.org/find/cs/1/au:+Deshpande_P/0/1/0/all/0/1">Parikshit Deshpande</a>, <a href="http://arxiv.org/find/cs/1/au:+Dhodhi_S/0/1/0/all/0/1">Siddhanth Dhodhi</a>, <a href="http://arxiv.org/find/cs/1/au:+Halepete_S/0/1/0/all/0/1">Sameer Halepete</a>, <a href="http://arxiv.org/find/cs/1/au:+Hill_E/0/1/0/all/0/1">Eric Hill</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1">Jiashang Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1">Sumit Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Khailany_B/0/1/0/all/0/1">Brucek Khailany</a>, <a href="http://arxiv.org/find/cs/1/au:+Kunal_K/0/1/0/all/0/1">Kishor Kunal</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaowei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Oberman_S/0/1/0/all/0/1">Stuart Oberman</a>, <a href="http://arxiv.org/find/cs/1/au:+Omar_S/0/1/0/all/0/1">Sujeet Omar</a>, <a href="http://arxiv.org/find/cs/1/au:+Pratty_S/0/1/0/all/0/1">Sreedhar Pratty</a>, <a href="http://arxiv.org/find/cs/1/au:+Raiman_J/0/1/0/all/0/1">Jonathan Raiman</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1">Ambar Sarkar</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1">Zhengjiang Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Hanfei Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Suthar_P/0/1/0/all/0/1">Pratik P Suthar</a>, <a href="http://arxiv.org/find/cs/1/au:+Tej_V/0/1/0/all/0/1">Varun Tej</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Kaizhe Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1">Haoxing Ren</a></p>
<p>ChipNeMo aims to explore the applications of large language models (LLMs) for
industrial chip design. Instead of directly deploying off-the-shelf commercial
or open-source LLMs, we instead adopt the following domain adaptation
techniques: custom tokenizers, domain-adaptive continued pretraining,
supervised fine-tuning (SFT) with domain-specific instructions, and
domain-adapted retrieval models. We evaluate these methods on three selected
LLM applications for chip design: an engineering assistant chatbot, EDA script
generation, and bug summarization and analysis. Our results show that these
domain adaptation techniques enable significant LLM performance improvements
over general-purpose base models across the three evaluated applications,
enabling up to 5x model size reduction with similar or better performance on a
range of design tasks. Our findings also indicate that there's still room for
improvement between our current results and ideal outcomes. We believe that
further investigation of domain-adapted LLM approaches will help close this gap
in the future.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04589">TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models. (arXiv:2311.04589v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhen Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yingxue Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1">Fandong Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jie Zhou</a></p>
<p>Despite Multi-modal Large Language Models (MM-LLMs) have made exciting
strides recently, they are still struggling to efficiently model the
interactions among multi-modal inputs and the generation in non-textual
modalities. In this work, we propose TEAL (Tokenize and Embed ALl)}, an
approach to treat the input from any modality as a token sequence and learn a
joint embedding space for all modalities. Specifically, for the input from any
modality, TEAL first discretizes it into a token sequence with the
off-the-shelf tokenizer and embeds the token sequence into a joint embedding
space with a learnable embedding matrix. MM-LLMs just need to predict the
multi-modal tokens autoregressively as the textual LLMs do. Finally, the
corresponding de-tokenizer is applied to generate the output in each modality
based on the predicted token sequence. With the joint embedding space, TEAL
enables the frozen LLMs to perform both understanding and generation tasks
involving non-textual modalities, such as image and audio. Thus, the textual
LLM can just work as an interface and maintain its high performance in textual
understanding and generation. Experiments show that TEAL achieves substantial
improvements in multi-modal understanding, and implements a simple scheme for
multi-modal generations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05915">Fake Alignment: Are LLMs Really Aligned Well?. (arXiv:2311.05915v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yixu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Teng_Y/0/1/0/all/0/1">Yan Teng</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1">Kexin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1">Chengqi Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Songyang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xingjun Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yu-Gang Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yingchun Wang</a></p>
<p>The growing awareness of safety concerns in large language models (LLMs) has
sparked considerable interest in the evaluation of safety within current
research endeavors. This study investigates an interesting issue pertaining to
the evaluation of LLMs, namely the substantial discrepancy in performance
between multiple-choice questions and open-ended questions. Inspired by
research on jailbreak attack patterns, we argue this is caused by mismatched
generalization. That is, the LLM does not have a comprehensive understanding of
the complex concept of safety. Instead, it only remembers what to answer for
open-ended safety questions, which makes it unable to solve other forms of
safety tests. We refer to this phenomenon as fake alignment and construct a
comparative benchmark to empirically verify its existence in LLMs. Such fake
alignment renders previous evaluation protocols unreliable. To address this, we
introduce the Fake alIgNment Evaluation (FINE) framework and two novel
metrics--Consistency Score (CS) and Consistent Safety Score (CSS), which
jointly assess two complementary forms of evaluation to quantify fake alignment
and obtain corrected performance estimates. Applying FINE to 14 widely-used
LLMs reveals several models with purported safety are poorly aligned in
practice. Our work highlights potential limitations in prevailing alignment
methodologies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06401">Autoregressive Language Models For Estimating the Entropy of Epic EHR Audit Logs. (arXiv:2311.06401v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Warner_B/0/1/0/all/0/1">Benjamin C. Warner</a>, <a href="http://arxiv.org/find/cs/1/au:+Kannampallil_T/0/1/0/all/0/1">Thomas Kannampallil</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Seunghwan Kim</a></p>
<p>EHR audit logs are a highly granular stream of events that capture clinician
activities, and is a significant area of interest for research in
characterizing clinician workflow on the electronic health record (EHR).
Existing techniques to measure the complexity of workflow through EHR audit
logs (audit logs) involve time- or frequency-based cross-sectional aggregations
that are unable to capture the full complexity of a EHR session. We briefly
evaluate the usage of transformer-based tabular language model (tabular LM) in
measuring the entropy or disorderedness of action sequences within workflow and
release the evaluated models publicly.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06453">DocGen: Generating Detailed Parameter Docstrings in Python. (arXiv:2311.06453v2 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Venkatkrishna_V/0/1/0/all/0/1">Vatsal Venkatkrishna</a>, <a href="http://arxiv.org/find/cs/1/au:+Nagabushanam_D/0/1/0/all/0/1">Durga Shree Nagabushanam</a>, <a href="http://arxiv.org/find/cs/1/au:+Simon_E/0/1/0/all/0/1">Emmanuel Iko-Ojo Simon</a>, <a href="http://arxiv.org/find/cs/1/au:+Vidoni_M/0/1/0/all/0/1">Melina Vidoni</a></p>
<p>Documentation debt hinders the effective utilization of open-source software.
Although code summarization tools have been helpful for developers, most would
prefer a detailed account of each parameter in a function rather than a
high-level summary. However, generating such a summary is too intricate for a
single generative model to produce reliably due to the lack of high-quality
training data. Thus, we propose a multi-step approach that combines multiple
task-specific models, each adept at producing a specific section of a
docstring. The combination of these models ensures the inclusion of each
section in the final docstring. We compared the results from our approach with
existing generative models using both automatic metrics and a human-centred
evaluation with 17 participating developers, which proves the superiority of
our approach over existing methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06513">Step by Step to Fairness: Attributing Societal Bias in Task-oriented Dialogue Systems. (arXiv:2311.06513v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1">Hsuan Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1">Rebecca Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Sankar_C/0/1/0/all/0/1">Chinnadhurai Sankar</a>, <a href="http://arxiv.org/find/cs/1/au:+Shayandeh_S/0/1/0/all/0/1">Shahin Shayandeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shang-Tse Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hung-yi Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Bikel_D/0/1/0/all/0/1">Daniel M. Bikel</a></p>
<p>Recent works have shown considerable improvements in task-oriented dialogue
(TOD) systems by utilizing pretrained large language models (LLMs) in an
end-to-end manner. However, the biased behavior of each component in a TOD
system and the error propagation issue in the end-to-end framework can lead to
seriously biased TOD responses. Existing works of fairness only focus on the
total bias of a system. In this paper, we propose a diagnosis method to
attribute bias to each component of a TOD system. With the proposed attribution
method, we can gain a deeper understanding of the sources of bias.
Additionally, researchers can mitigate biased model behavior at a more granular
level. We conduct experiments to attribute the TOD system's bias toward three
demographic axes: gender, age, and race. Experimental results show that the
bias of a TOD system usually comes from the response generation model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06595">From Classification to Generation: Insights into Crosslingual Retrieval Augmented ICL. (arXiv:2311.06595v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaoqian Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_E/0/1/0/all/0/1">Ercong Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1">Sheng Liang</a></p>
<p>The remarkable ability of Large Language Models (LLMs) to understand and
follow instructions has sometimes been limited by their in-context learning
(ICL) performance in low-resource languages. To address this, we introduce a
novel approach that leverages cross-lingual retrieval-augmented in-context
learning (CREA-ICL). By extracting semantically similar prompts from
high-resource languages, we aim to improve the zero-shot performance of
multilingual pre-trained language models (MPLMs) across diverse tasks. Though
our approach yields steady improvements in classification tasks, it faces
challenges in generation tasks. Our evaluation offers insights into the
performance dynamics of retrieval-augmented in-context learning across both
classification and generation domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07093">On the Effectiveness of ASR Representations in Real-world Noisy Speech Emotion Recognition. (arXiv:2311.07093v2 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1">Xiaohan Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Jiajun He</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xingfeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1">Tomoki Toda</a></p>
<p>This paper proposes an efficient attempt to noisy speech emotion recognition
(NSER). Conventional NSER approaches have proven effective in mitigating the
impact of artificial noise sources, such as white Gaussian noise, but are
limited to non-stationary noises in real-world environments due to their
complexity and uncertainty. To overcome this limitation, we introduce a new
method for NSER by adopting the automatic speech recognition (ASR) model as a
noise-robust feature extractor to eliminate non-vocal information in noisy
speech. We first obtain intermediate layer information from the ASR model as a
feature representation for emotional speech and then apply this representation
for the downstream NSER task. Our experimental results show that 1) the
proposed method achieves better NSER performance compared with the conventional
noise reduction method, 2) outperforms self-supervised learning approaches, and
3) even outperforms text-based approaches using ASR transcription or the ground
truth transcription of noisy speech.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07362">Volcano: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision. (arXiv:2311.07362v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Seongyun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1">Sue Hyun Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Jo_Y/0/1/0/all/0/1">Yongrae Jo</a>, <a href="http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1">Minjoon Seo</a></p>
<p>Large multimodal models (LMMs) suffer from multimodal hallucination, where
they provide incorrect responses misaligned with the given visual information.
Recent works have conjectured that one of the reasons behind multimodal
hallucination might be due to the vision encoder failing to ground on the image
properly. To mitigate this issue, we propose a novel approach that leverages
self-feedback as visual cues. Building on this approach, we introduce Volcano,
a multimodal self-feedback guided revision model. Volcano generates natural
language feedback to its initial response based on the provided visual
information and utilizes this feedback to self-revise its initial response.
Volcano effectively reduces multimodal hallucination and achieves
state-of-the-art on MMHal-Bench, POPE, and GAVIE. It also improves on general
multimodal abilities and outperforms previous models on MM-Vet and MMBench.
Through a qualitative analysis, we show that Volcano's feedback is properly
grounded on the image than the initial response. This indicates that Volcano
can provide itself with richer visual information, helping alleviate multimodal
hallucination. We publicly release Volcano models of 7B and 13B sizes along
with the data and code at https://github.com/kaistAI/Volcano.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07439">Investigating Multi-Pivot Ensembling with Massively Multilingual Machine Translation Models. (arXiv:2311.07439v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mohammadshahi_A/0/1/0/all/0/1">Alireza Mohammadshahi</a>, <a href="http://arxiv.org/find/cs/1/au:+Vamvas_J/0/1/0/all/0/1">Jannis Vamvas</a>, <a href="http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1">Rico Sennrich</a></p>
<p>Massively multilingual machine translation models allow for the translation
of a large number of languages with a single model, but have limited
performance on low- and very-low-resource translation directions. Pivoting via
high-resource languages remains a strong strategy for low-resource directions,
and in this paper we revisit ways of pivoting through multiple languages.
Previous work has used a simple averaging of probability distributions from
multiple paths, but we find that this performs worse than using a single pivot,
and exacerbates the hallucination problem because the same hallucinations can
be probable across different paths. As an alternative, we propose MaxEns, a
combination strategy that is biased towards the most confident predictions,
hypothesising that confident predictions are less prone to be hallucinations.
We evaluate different strategies on the FLORES benchmark for 20 low-resource
language directions, demonstrating that MaxEns improves translation quality for
low-resource languages while reducing hallucination in translations, compared
to both direct translation and an averaging approach. On average, multi-pivot
strategies still lag behind using English as a single pivot language, raising
the question of how to identify the best pivoting strategy for a given
translation direction.
</p>
</p>
</div>

    </div>
    </body>
    