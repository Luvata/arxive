<!DOCTYPE html>
<html>
<head>
<title>2023-11-15-cs-ai</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2311.06251">AI for Investment: A Platform Disruption. (arXiv:2311.06251v1 [q-fin.GN])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+Rasouli_M/0/1/0/all/0/1">Mohammad Rasouli</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Chiruvolu_R/0/1/0/all/0/1">Ravi Chiruvolu</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Risheh_A/0/1/0/all/0/1">Ali Risheh</a></p>
<p>With the investment landscape becoming more competitive, efficiently scaling
deal sourcing and improving deal insights have become a dominant strategy for
funds. While funds are already spending significant efforts on these two tasks,
they cannot be scaled with traditional approaches; hence, there is a surge in
automating them. Many third party software providers have emerged recently to
address this need with productivity solutions, but they fail due to a lack of
personalization for the fund, privacy constraints, and natural limits of
software use cases. Therefore, most major funds and many smaller funds have
started developing their in-house AI platforms: a game changer for the
industry. These platforms grow smarter by direct interactions with the fund and
can be used to provide personalized use cases. Recent developments in large
language models, e.g. ChatGPT, have provided an opportunity for other funds to
also develop their own AI platforms. While not having an AI platform now is not
a competitive disadvantage, it will be in two years. Funds require a practical
plan and corresponding risk assessments for such AI platforms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06255">Privacy-Engineered Value Decomposition Networks for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2311.06255v1 [cs.MA])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gohari_P/0/1/0/all/0/1">Parham Gohari</a>, <a href="http://arxiv.org/find/cs/1/au:+Hale_M/0/1/0/all/0/1">Matthew Hale</a>, <a href="http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1">Ufuk Topcu</a></p>
<p>In cooperative multi-agent reinforcement learning (Co-MARL), a team of agents
must jointly optimize the team's long-term rewards to learn a designated task.
Optimizing rewards as a team often requires inter-agent communication and data
sharing, leading to potential privacy implications. We assume privacy
considerations prohibit the agents from sharing their environment interaction
data. Accordingly, we propose Privacy-Engineered Value Decomposition Networks
(PE-VDN), a Co-MARL algorithm that models multi-agent coordination while
provably safeguarding the confidentiality of the agents' environment
interaction data. We integrate three privacy-engineering techniques to redesign
the data flows of the VDN algorithm, an existing Co-MARL algorithm that
consolidates the agents' environment interaction data to train a central
controller that models multi-agent coordination, and develop PE-VDN. In the
first technique, we design a distributed computation scheme that eliminates
Vanilla VDN's dependency on sharing environment interaction data. Then, we
utilize a privacy-preserving multi-party computation protocol to guarantee that
the data flows of the distributed computation scheme do not pose new privacy
risks. Finally, we enforce differential privacy to preempt inference threats
against the agents' training data, past environment interactions, when they
take actions based on their neural network predictions. We implement PE-VDN in
StarCraft Multi-Agent Competition (SMAC) and show that it achieves 80% of
Vanilla VDN's win rate while maintaining differential privacy levels that
provide meaningful privacy guarantees. The results demonstrate that PE-VDN can
safeguard the confidentiality of agents' environment interaction data without
sacrificing multi-agent coordination.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06256">From Deep Filtering to Deep Econometrics. (arXiv:2311.06256v1 [q-fin.ST])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+Stok_R/0/1/0/all/0/1">Robert Stok</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Bilokon_P/0/1/0/all/0/1">Paul Bilokon</a></p>
<p>Calculating true volatility is an essential task for option pricing and risk
management. However, it is made difficult by market microstructure noise.
Particle filtering has been proposed to solve this problem as it favorable
statistical properties, but relies on assumptions about underlying market
dynamics. Machine learning methods have also been proposed but lack
interpretability, and often lag in performance. In this paper we implement the
SV-PF-RNN: a hybrid neural network and particle filter architecture. Our
SV-PF-RNN is designed specifically with stochastic volatility estimation in
mind. We then show that it can improve on the performance of a basic particle
filter.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06258">Post-COVID Highlights: Challenges and Solutions of AI Techniques for Swift Identification of COVID-19. (arXiv:2311.06258v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1">Yingying Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_X/0/1/0/all/0/1">Xiaodan Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shiyi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Walsh_S/0/1/0/all/0/1">Simon Walsh</a></p>
<p>Since the onset of the COVID-19 pandemic in 2019, there has been a concerted
effort to develop cost-effective, non-invasive, and rapid AI-based tools. These
tools were intended to alleviate the burden on healthcare systems, control the
rapid spread of the virus, and enhance intervention outcomes, all in response
to this unprecedented global crisis. As we transition into a post-COVID era, we
retrospectively evaluate these proposed studies and offer a review of the
techniques employed in AI diagnostic models, with a focus on the solutions
proposed for different challenges. This review endeavors to provide insights
into the diverse solutions designed to address the multifaceted challenges that
arose during the pandemic. By doing so, we aim to prepare the AI community for
the development of AI tools tailored to address public health emergencies
effectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06261">With ChatGPT, do we have to rewrite our learning objectives -- CASE study in Cybersecurity. (arXiv:2311.06261v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jamieson_P/0/1/0/all/0/1">Peter Jamieson</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhunia_S/0/1/0/all/0/1">Suman Bhunia</a>, <a href="http://arxiv.org/find/cs/1/au:+Rao_D/0/1/0/all/0/1">Dhananjai M. Rao</a></p>
<p>With the emergence of Artificial Intelligent chatbot tools such as ChatGPT
and code writing AI tools such as GitHub Copilot, educators need to question
what and how we should teach our courses and curricula in the future. In
reality, automated tools may result in certain academic fields being deeply
reduced in the number of employable people. In this work, we make a case study
of cybersecurity undergrad education by using the lens of ``Understanding by
Design'' (UbD). First, we provide a broad understanding of learning objectives
(LOs) in cybersecurity from a computer science perspective. Next, we dig a
little deeper into a curriculum with an undergraduate emphasis on cybersecurity
and examine the major courses and their LOs for our cybersecurity program at
Miami University. With these details, we perform a thought experiment on how
attainable the LOs are with the above-described tools, asking the key question
``what needs to be enduring concepts?'' learned in this process. If an LO
becomes something that the existence of automation tools might be able to do,
we then ask ``what level is attainable for the LO that is not a simple query to
the tools?''. With this exercise, we hope to establish an example of how to
prompt ChatGPT to accelerate students in their achievements of LOs given the
existence of these new AI tools, and our goal is to push all of us to leverage
and teach these tools as powerful allies in our quest to improve human
existence and knowledge.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06263">No Trust without regulation!. (arXiv:2311.06263v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Terrier_F/0/1/0/all/0/1">Fran&#xe7;ois Terrier</a> (CEA List)</p>
<p>The explosion in the performance of Machine Learning (ML) and the potential
of its applications are strongly encouraging us to consider its use in
industrial systems, including for critical functions such as decision-making in
autonomous systems. While the AI community is well aware of the need to ensure
the trustworthiness of AI-based applications, it is still leaving too much to
one side the issue of safety and its corollary, regulation and standards,
without which it is not possible to certify any level of safety, whether the
systems are slightly or very critical.The process of developing and qualifying
safety-critical software and systems in regulated industries such as aerospace,
nuclear power stations, railways or automotive industry has long been well
rationalized and mastered. They use well-defined standards, regulatory
frameworks and processes, as well as formal techniques to assess and
demonstrate the quality and safety of the systems and software they develop.
However, the low level of formalization of specifications and the uncertainties
and opacity of machine learning-based components make it difficult to validate
and verify them using most traditional critical systems engineering methods.
This raises the question of qualification standards, and therefore of
regulations adapted to AI. With the AI Act, the European Commission has laid
the foundations for moving forward and building solid approaches to the
integration of AI-based applications that are safe, trustworthy and respect
European ethical values. The question then becomes "How can we rise to the
challenge of certification and propose methods and tools for trusted artificial
intelligence?"
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06264">&quot;ChatGPT, a Friend or Foe for Education?&quot; Analyzing the User&#x27;s Perspectives on the Latest AI Chatbot Via Reddit. (arXiv:2311.06264v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Emdad_F/0/1/0/all/0/1">Forhan Bin Emdad</a>, <a href="http://arxiv.org/find/cs/1/au:+Ravuri_B/0/1/0/all/0/1">Benhur Ravuri</a>, <a href="http://arxiv.org/find/cs/1/au:+Ayinde_L/0/1/0/all/0/1">Lateef Ayinde</a>, <a href="http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1">Mohammad Ishtiaque Rahman</a></p>
<p>Latest developments in Artificial Intelligence (AI) and big data gave rise to
Artificial Intelligent agents like Open AI's ChatGPT, which has recently become
the fastest growing application since Facebook and WhatsApp. ChatGPT has
demonstrated its ability to impact students' classroom learning experience and
exam outcomes. However, there is evidence that ChatGPT provides biased and
erroneous information, yet students use ChatGPT in academic tasks. Therefore,
an accurate understanding of ChatGPT user perception is crucial. This study has
analyzed 247 Reddit top posts related to the educational use of ChatGPT from a
prominent subreddit called "ChatGPT" for user perception analysis. Descriptive
statistics, sentiment analysis using NLP techniques, and LDA topic modeling
were used for analysis to gather a contextual understanding of the data.
Results show that the majority of the users took a neutral viewpoint. However,
there was more positive perception than negative regarding the usefulness of
ChatGPT in education.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06275">Algorithmic Robustness. (arXiv:2311.06275v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jensen_D/0/1/0/all/0/1">David Jensen</a>, <a href="http://arxiv.org/find/cs/1/au:+LaMacchia_B/0/1/0/all/0/1">Brian LaMacchia</a>, <a href="http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1">Ufuk Topcu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wisniewski_P/0/1/0/all/0/1">Pamela Wisniewski</a></p>
<p>Algorithmic robustness refers to the sustained performance of a computational
system in the face of change in the nature of the environment in which that
system operates or in the task that the system is meant to perform. Below, we
motivate the importance of algorithmic robustness, present a conceptual
framework, and highlight the relevant areas of research for which algorithmic
robustness is relevant. Why robustness? Robustness is an important enabler of
other goals that are frequently cited in the context of public policy decisions
about computational systems, including trustworthiness, accountability,
fairness, and safety. Despite this dependence, it tends to be under-recognized
compared to these other concepts. This is unfortunate, because robustness is
often more immediately achievable than these other ultimate goals, which can be
more subjective and exacting. Thus, we highlight robustness as an important
goal for researchers, engineers, regulators, and policymakers when considering
the design, implementation, and deployment of computational systems. We urge
researchers and practitioners to elevate the attention paid to robustness when
designing and evaluating computational systems. For many key systems, the
immediate question after any demonstration of high performance should be: "How
robust is that performance to realistic changes in the task or environment?"
Greater robustness will set the stage for systems that are more trustworthy,
accountable, fair, and safe. Toward that end, this document provides a brief
roadmap to some of the concepts and existing research around the idea of
algorithmic robustness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06278">Boosting Stock Price Prediction with Anticipated Macro Policy Changes. (arXiv:2311.06278v1 [q-fin.ST])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+Haque_M/0/1/0/all/0/1">Md Sabbirul Haque</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Amin_M/0/1/0/all/0/1">Md Shahedul Amin</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Miah_J/0/1/0/all/0/1">Jonayet Miah</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Cao_D/0/1/0/all/0/1">Duc Minh Cao</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Ahmed_A/0/1/0/all/0/1">Ashiqul Haque Ahmed</a></p>
<p>Prediction of stock prices plays a significant role in aiding the
decision-making of investors. Considering its importance, a growing literature
has emerged trying to forecast stock prices with improved accuracy. In this
study, we introduce an innovative approach for forecasting stock prices with
greater accuracy. We incorporate external economic environment-related
information along with stock prices. In our novel approach, we improve the
performance of stock price prediction by taking into account variations due to
future expected macroeconomic policy changes as investors adjust their current
behavior ahead of time based on expected future macroeconomic policy changes.
Furthermore, we incorporate macroeconomic variables along with historical stock
prices to make predictions. Results from this strongly support the inclusion of
future economic policy changes along with current macroeconomic information. We
confirm the supremacy of our method over the conventional approach using
several tree-based machine-learning algorithms. Results are strongly conclusive
across various machine learning models. Our preferred model outperforms the
conventional approach with an RMSE value of 1.61 compared to an RMSE value of
1.75 from the conventional approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06280">A Data-driven Deep Learning Approach for Bitcoin Price Forecasting. (arXiv:2311.06280v1 [q-fin.ST])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+Modi_P/0/1/0/all/0/1">Parth Daxesh Modi</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Arshi_K/0/1/0/all/0/1">Kamyar Arshi</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Kunz_P/0/1/0/all/0/1">Pertami J. Kunz</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Zoubir_A/0/1/0/all/0/1">Abdelhak M. Zoubir</a></p>
<p>Bitcoin as a cryptocurrency has been one of the most important digital coins
and the first decentralized digital currency. Deep neural networks, on the
other hand, has shown promising results recently; however, we require huge
amount of high-quality data to leverage their power. There are some techniques
such as augmentation that can help us with increasing the dataset size, but we
cannot exploit them on historical bitcoin data. As a result, we propose a
shallow Bidirectional-LSTM (Bi-LSTM) model, fed with feature engineered data
using our proposed method to forecast bitcoin closing prices in a daily time
frame. We compare the performance with that of other forecasting methods, and
show that with the help of the proposed feature engineering method, a shallow
deep neural network outperforms other popular price forecasting models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06300">AI Chatbot for Generating Episodic Future Thinking (EFT) Cue Texts for Health. (arXiv:2311.06300v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ahmadi_S/0/1/0/all/0/1">Sareh Ahmadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Fox_E/0/1/0/all/0/1">Edward A. Fox</a></p>
<p>We describe an AI-powered chatbot to aid with health improvement by
generating Episodic Future Thinking (EFT) cue texts that should reduce delay
discounting. In prior studies, EFT has been shown to address maladaptive health
behaviors. Those studies involved participants, working with researchers,
vividly imagining future events, and writing a description that they
subsequently will frequently review, to ensure a shift from an inclination
towards immediate rewards. That should promote behavior change, aiding in
health tasks such as treatment adherence and lifestyle modifications. The AI
chatbot is designed to guide users in generating personalized EFTs, automating
the current labor-intensive interview-based process. This can enhance the
efficiency of EFT interventions and make them more accessible, targeting
specifically those with limited educational backgrounds or communication
challenges. By leveraging AI for EFT intervention, we anticipate broadened
access and improved health outcomes across diverse populations
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06302">Knowledge-Based Support for Adhesive Selection: Will it Stick?. (arXiv:2311.06302v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vandevelde_S/0/1/0/all/0/1">Simon Vandevelde</a>, <a href="http://arxiv.org/find/cs/1/au:+Jordens_J/0/1/0/all/0/1">Jeroen Jordens</a>, <a href="http://arxiv.org/find/cs/1/au:+Doninck_B/0/1/0/all/0/1">Bart Van Doninck</a>, <a href="http://arxiv.org/find/cs/1/au:+Witters_M/0/1/0/all/0/1">Maarten Witters</a>, <a href="http://arxiv.org/find/cs/1/au:+Vennekens_J/0/1/0/all/0/1">Joost Vennekens</a></p>
<p>As the popularity of adhesive joints in industry increases, so does the need
for tools to support the process of selecting a suitable adhesive. While some
such tools already exist, they are either too limited in scope, or offer too
little flexibility in use. This work presents a more advanced tool, that was
developed together with a team of adhesive experts. We first extract the
experts' knowledge about this domain and formalize it in a Knowledge Base (KB).
The IDP-Z3 reasoning system can then be used to derive the necessary
functionality from this KB. Together with a user-friendly interactive
interface, this creates an easy-to-use tool capable of assisting the adhesive
experts. To validate our approach, we performed user testing in the form of
qualitative interviews. The experts are very positive about the tool, stating
that, among others, it will help save time and find more suitable adhesives.
Under consideration in Theory and Practice of Logic Programming (TPLP).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06304">Retro-BLEU: Quantifying Chemical Plausibility of Retrosynthesis Routes through Reaction Template Sequence Analysis. (arXiv:2311.06304v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Junren Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1">Lei Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1">Jian-Guang Lou</a></p>
<p>Computer-assisted methods have emerged as valuable tools for retrosynthesis
analysis. However, quantifying the plausibility of generated retrosynthesis
routes remains a challenging task. We introduce Retro-BLEU, a statistical
metric adapted from the well-established BLEU score in machine translation, to
evaluate the plausibility of retrosynthesis routes based on reaction template
sequences analysis. We demonstrate the effectiveness of Retro-BLEU by applying
it to a diverse set of retrosynthesis routes generated by state-of-the-art
algorithms and compare the performance with other evaluation metrics. The
results show that Retro-BLEU is capable of differentiating between plausible
and implausible routes. Furthermore, we provide insights into the strengths and
weaknesses of Retro-BLEU, paving the way for future developments and
improvements in this field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06305">A Systematic Review on Fostering Appropriate Trust in Human-AI Interaction. (arXiv:2311.06305v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mehrotra_S/0/1/0/all/0/1">Siddharth Mehrotra</a>, <a href="http://arxiv.org/find/cs/1/au:+Degachi_C/0/1/0/all/0/1">Chadha Degachi</a>, <a href="http://arxiv.org/find/cs/1/au:+Vereschak_O/0/1/0/all/0/1">Oleksandra Vereschak</a>, <a href="http://arxiv.org/find/cs/1/au:+Jonker_C/0/1/0/all/0/1">Catholijn M. Jonker</a>, <a href="http://arxiv.org/find/cs/1/au:+Tielman_M/0/1/0/all/0/1">Myrthe L. Tielman</a></p>
<p>Appropriate Trust in Artificial Intelligence (AI) systems has rapidly become
an important area of focus for both researchers and practitioners. Various
approaches have been used to achieve it, such as confidence scores,
explanations, trustworthiness cues, or uncertainty communication. However, a
comprehensive understanding of the field is lacking due to the diversity of
perspectives arising from various backgrounds that influence it and the lack of
a single definition for appropriate trust. To investigate this topic, this
paper presents a systematic review to identify current practices in building
appropriate trust, different ways to measure it, types of tasks used, and
potential challenges associated with it. We also propose a Belief, Intentions,
and Actions (BIA) mapping to study commonalities and differences in the
concepts related to appropriate trust by (a) describing the existing
disagreements on defining appropriate trust, and (b) providing an overview of
the concepts and definitions related to appropriate trust in AI from the
existing literature. Finally, the challenges identified in studying appropriate
trust are discussed, and observations are summarized as current trends,
potential gaps, and research opportunities for future work. Overall, the paper
provides insights into the complex concept of appropriate trust in human-AI
interaction and presents research opportunities to advance our understanding on
this topic.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06307">Synthetic Speaking Children -- Why We Need Them and How to Make Them. (arXiv:2311.06307v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Farooq_M/0/1/0/all/0/1">Muhammad Ali Farooq</a>, <a href="http://arxiv.org/find/cs/1/au:+Bigioi_D/0/1/0/all/0/1">Dan Bigioi</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1">Rishabh Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1">Wang Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yiwere_M/0/1/0/all/0/1">Mariam Yiwere</a>, <a href="http://arxiv.org/find/cs/1/au:+Corcoran_P/0/1/0/all/0/1">Peter Corcoran</a></p>
<p>Contemporary Human Computer Interaction (HCI) research relies primarily on
neural network models for machine vision and speech understanding of a system
user. Such models require extensively annotated training datasets for optimal
performance and when building interfaces for users from a vulnerable population
such as young children, GDPR introduces significant complexities in data
collection, management, and processing. Motivated by the training needs of an
Edge AI smart toy platform this research explores the latest advances in
generative neural technologies and provides a working proof of concept of a
controllable data generation pipeline for speech driven facial training data at
scale. In this context, we demonstrate how StyleGAN2 can be finetuned to create
a gender balanced dataset of children's faces. This dataset includes a variety
of controllable factors such as facial expressions, age variations, facial
poses, and even speech-driven animations with realistic lip synchronization. By
combining generative text to speech models for child voice synthesis and a 3D
landmark based talking heads pipeline, we can generate highly realistic,
entirely synthetic, talking child video clips. These video clips can provide
valuable, and controllable, synthetic training data for neural network models,
bridging the gap when real data is scarce or restricted due to privacy
regulations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06310">$\textit{Labor Space}$: A Unifying Representation of the Labor Market via Large Language Models. (arXiv:2311.06310v1 [physics.soc-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Kim_S/0/1/0/all/0/1">Seongwoon Kim</a>, <a href="http://arxiv.org/find/physics/1/au:+Ahn_Y/0/1/0/all/0/1">Yong-Yeol Ahn</a>, <a href="http://arxiv.org/find/physics/1/au:+Park_J/0/1/0/all/0/1">Jaehyuk Park</a></p>
<p>The labor market is a complex ecosystem comprising diverse, interconnected
entities, such as industries, occupations, skills, and firms. Due to the lack
of a systematic method to map these heterogeneous entities together, each
entity has been analyzed in isolation or only through pairwise relationships,
inhibiting comprehensive understanding of the whole ecosystem. Here, we
introduce $\textit{Labor Space}$, a vector-space embedding of heterogeneous
labor market entities, derived through applying a large language model with
fine-tuning. Labor Space exposes the complex relational fabric of various labor
market constituents, facilitating coherent integrative analysis of industries,
occupations, skills, and firms, while retaining type-specific clustering. We
demonstrate its unprecedented analytical capacities, including positioning
heterogeneous entities on an economic axes, such as
`Manufacturing--Healthcare'. Furthermore, by allowing vector arithmetic of
these entities, Labor Space enables the exploration of complex inter-unit
relations, and subsequently the estimation of the ramifications of economic
shocks on individual units and their ripple effect across the labor market. We
posit that Labor Space provides policymakers and business leaders with a
comprehensive unifying framework for labor market analysis and simulation,
fostering more nuanced and effective strategic decision-making.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06311">Game Theory Solutions in Sensor-Based Human Activity Recognition: A Review. (arXiv:2311.06311v1 [cs.GT])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shayesteh_M/0/1/0/all/0/1">Mohammad Hossein Shayesteh</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharokhzadeh_B/0/1/0/all/0/1">Behrooz Sharokhzadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Masoumi_B/0/1/0/all/0/1">Behrooz Masoumi</a></p>
<p>The Human Activity Recognition (HAR) tasks automatically identify human
activities using the sensor data, which has numerous applications in
healthcare, sports, security, and human-computer interaction. Despite
significant advances in HAR, critical challenges still exist. Game theory has
emerged as a promising solution to address these challenges in machine learning
problems including HAR. However, there is a lack of research work on applying
game theory solutions to the HAR problems. This review paper explores the
potential of game theory as a solution for HAR tasks, and bridges the gap
between game theory and HAR research work by suggesting novel game-theoretic
approaches for HAR problems. The contributions of this work include exploring
how game theory can improve the accuracy and robustness of HAR models,
investigating how game-theoretic concepts can optimize recognition algorithms,
and discussing the game-theoretic approaches against the existing HAR methods.
The objective is to provide insights into the potential of game theory as a
solution for sensor-based HAR, and contribute to develop a more accurate and
efficient recognition system in the future research directions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06315">ShipGen: A Diffusion Model for Parametric Ship Hull Generation with Multiple Objectives and Constraints. (arXiv:2311.06315v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bagazinski_N/0/1/0/all/0/1">Noah J. Bagazinski</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_F/0/1/0/all/0/1">Faez Ahmed</a></p>
<p>Ship design is a years-long process that requires balancing complex design
trade-offs to create a ship that is efficient and effective. Finding new ways
to improve the ship design process can lead to significant cost savings for
ship building and operation. One promising technology is generative artificial
intelligence, which has been shown to reduce design cycle time and create
novel, high-performing designs. In literature review, generative artificial
intelligence has been shown to generate ship hulls; however, ship design is
particularly difficult as the hull of a ship requires the consideration of many
objectives. This paper presents a study on the generation of parametric ship
hull designs using a parametric diffusion model that considers multiple
objectives and constraints for the hulls. This denoising diffusion
probabilistic model (DDPM) generates the tabular parametric design vectors of a
ship hull for evaluation. In addition to a tabular DDPM, this paper details
adding guidance to improve the quality of generated ship hull designs. By
leveraging classifier guidance, the DDPM produced feasible parametric ship
hulls that maintain the coverage of the initial training dataset of ship hulls
with a 99.5% rate, a 149x improvement over random sampling of the design vector
parameters across the design space. Parametric ship hulls produced with
performance guidance saw an average of 91.4% reduction in wave drag
coefficients and an average of a 47.9x relative increase in the total displaced
volume of the hulls compared to the mean performance of the hulls in the
training dataset. The use of a DDPM to generate parametric ship hulls can
reduce design time by generating high-performing hull designs for future
analysis. These generated hulls have low drag and high volume, which can reduce
the cost of operating a ship and increase its potential to generate revenue.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06318">Knowledge-Augmented Large Language Models for Personalized Contextual Query Suggestion. (arXiv:2311.06318v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Baek_J/0/1/0/all/0/1">Jinheon Baek</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandrasekaran_N/0/1/0/all/0/1">Nirupama Chandrasekaran</a>, <a href="http://arxiv.org/find/cs/1/au:+Cucerzan_S/0/1/0/all/0/1">Silviu Cucerzan</a>, <a href="http://arxiv.org/find/cs/1/au:+herring_A/0/1/0/all/0/1">Allen herring</a>, <a href="http://arxiv.org/find/cs/1/au:+Jauhar_S/0/1/0/all/0/1">Sujay Kumar Jauhar</a></p>
<p>Large Language Models (LLMs) excel at tackling various natural language
tasks. However, due to the significant costs involved in re-training or
fine-tuning them, they remain largely static and difficult to personalize.
Nevertheless, a variety of applications could benefit from generations that are
tailored to users' preferences, goals, and knowledge. Among them is web search,
where knowing what a user is trying to accomplish, what they care about, and
what they know can lead to improved search experiences. In this work, we
propose a novel and general approach that augments an LLM with relevant context
from users' interaction histories with a search engine in order to personalize
its outputs. Specifically, we construct an entity-centric knowledge store for
each user based on their search and browsing activities on the web, which is
then leveraged to provide contextually relevant LLM prompt augmentations. This
knowledge store is light-weight, since it only produces user-specific aggregate
projections of interests and knowledge onto public knowledge graphs, and
leverages existing search log infrastructure, thereby mitigating the privacy,
compliance, and scalability concerns associated with building deep user
profiles for personalization. We then validate our approach on the task of
contextual query suggestion, which requires understanding not only the user's
current search context but also what they historically know and care about.
Through a number of experiments based on human evaluation, we show that our
approach is significantly better than several other LLM-powered baselines,
generating query suggestions that are contextually more relevant, personalized,
and useful.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06323">Reviewing Developments of Graph Convolutional Network Techniques for Recommendation Systems. (arXiv:2311.06323v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1">Haojun Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kapoor_V/0/1/0/all/0/1">Vikram Kapoor</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1">Priya Sharma</a></p>
<p>The Recommender system is a vital information service on today's Internet.
Recently, graph neural networks have emerged as the leading approach for
recommender systems. We try to review recent literature on graph neural
network-based recommender systems, covering the background and development of
both recommender systems and graph neural networks. Then categorizing
recommender systems by their settings and graph neural networks by spectral and
spatial models, we explore the motivation behind incorporating graph neural
networks into recommender systems. We also analyze challenges and open problems
in graph construction, embedding propagation and aggregation, and computation
efficiency. This guides us to better explore the future directions and
developments in this domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06329">A Survey of AI Text-to-Image and AI Text-to-Video Generators. (arXiv:2311.06329v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1">Aditi Singh</a></p>
<p>Text-to-Image and Text-to-Video AI generation models are revolutionary
technologies that use deep learning and natural language processing (NLP)
techniques to create images and videos from textual descriptions. This paper
investigates cutting-edge approaches in the discipline of Text-to-Image and
Text-to-Video AI generations. The survey provides an overview of the existing
literature as well as an analysis of the approaches used in various studies. It
covers data preprocessing techniques, neural network types, and evaluation
metrics used in the field. In addition, the paper discusses the challenges and
limitations of Text-to-Image and Text-to-Video AI generations, as well as
future research directions. Overall, these models have promising potential for
a wide range of applications such as video production, content creation, and
digital marketing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06330">Smart Agent-Based Modeling: On the Use of Large Language Models in Computer Simulations. (arXiv:2311.06330v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zengqing Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_R/0/1/0/all/0/1">Run Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1">Xu Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1">Shuyuan Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yixin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Chuan Xiao</a></p>
<p>Computer simulations offer a robust toolset for exploring complex systems
across various disciplines. A particularly impactful approach within this realm
is Agent-Based Modeling (ABM), which harnesses the interactions of individual
agents to emulate intricate system dynamics. ABM's strength lies in its
bottom-up methodology, illuminating emergent phenomena by modeling the
behaviors of individual components of a system. Yet, ABM has its own set of
challenges, notably its struggle with modeling natural language instructions
and common sense in mathematical equations or rules. This paper seeks to
transcend these boundaries by integrating Large Language Models (LLMs) like GPT
into ABM. This amalgamation gives birth to a novel framework, Smart Agent-Based
Modeling (SABM). Building upon the concept of smart agents -- entities
characterized by their intelligence, adaptability, and computation ability --
we explore in the direction of utilizing LLM-powered agents to simulate
real-world scenarios with increased nuance and realism. In this comprehensive
exploration, we elucidate the state of the art of ABM, introduce SABM's
potential and methodology, and present three case studies (source codes
available at https://github.com/Roihn/SABM), demonstrating the SABM methodology
and validating its effectiveness in modeling real-world systems. Furthermore,
we cast a vision towards several aspects of the future of SABM, anticipating a
broader horizon for its applications. Through this endeavor, we aspire to
redefine the boundaries of computer simulations, enabling a more profound
understanding of complex systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06390">ChatGPT in the context of precision agriculture data analytics. (arXiv:2311.06390v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Potamitis_I/0/1/0/all/0/1">Ilyas Potamitis</a></p>
<p>In this study we argue that integrating ChatGPT into the data processing
pipeline of automated sensors in precision agriculture has the potential to
bring several benefits and enhance various aspects of modern farming practices.
Policy makers often face a barrier when they need to get informed about the
situation in vast agricultural fields to reach to decisions. They depend on the
close collaboration between agricultural experts in the field, data analysts,
and technology providers to create interdisciplinary teams that cannot always
be secured on demand or establish effective communication across these diverse
domains to respond in real-time. In this work we argue that the speech
recognition input modality of ChatGPT provides a more intuitive and natural way
for policy makers to interact with the database of the server of an
agricultural data processing system to which a large, dispersed network of
automated insect traps and sensors probes reports. The large language models
map the speech input to text, allowing the user to form its own version of
unconstrained verbal query, raising the barrier of having to learn and adapt
oneself to a specific data analytics software. The output of the language model
can interact through Python code and Pandas with the entire database, visualize
the results and use speech synthesis to engage the user in an iterative and
refining discussion related to the data. We show three ways of how ChatGPT can
interact with the database of the remote server to which a dispersed network of
different modalities (optical counters, vibration recordings, pictures, and
video), report. We examine the potential and the validity of the response of
ChatGPT in analyzing, and interpreting agricultural data, providing real time
insights and recommendations to stakeholders
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06413">Forte: An Interactive Visual Analytic Tool for Trust-Augmented Net Load Forecasting. (arXiv:2311.06413v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bhattacharjee_K/0/1/0/all/0/1">Kaustav Bhattacharjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kundu_S/0/1/0/all/0/1">Soumya Kundu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chakraborty_I/0/1/0/all/0/1">Indrasis Chakraborty</a>, <a href="http://arxiv.org/find/cs/1/au:+Dasgupta_A/0/1/0/all/0/1">Aritra Dasgupta</a></p>
<p>Accurate net load forecasting is vital for energy planning, aiding decisions
on trade and load distribution. However, assessing the performance of
forecasting models across diverse input variables, like temperature and
humidity, remains challenging, particularly for eliciting a high degree of
trust in the model outcomes. In this context, there is a growing need for
data-driven technological interventions to aid scientists in comprehending how
models react to both noisy and clean input variables, thus shedding light on
complex behaviors and fostering confidence in the outcomes. In this paper, we
present Forte, a visual analytics-based application to explore deep
probabilistic net load forecasting models across various input variables and
understand the error rates for different scenarios. With carefully designed
visual interventions, this web-based interface empowers scientists to derive
insights about model performance by simulating diverse scenarios, facilitating
an informed decision-making process. We discuss observations made using Forte
and demonstrate the effectiveness of visualization techniques to provide
valuable insights into the correlation between weather inputs and net load
forecasts, ultimately advancing grid capabilities by improving trust in
forecasting models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06417">Resolving uncertainty on the fly: Modeling adaptive driving behavior as active inference. (arXiv:2311.06417v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Engstrom_J/0/1/0/all/0/1">Johan Engstr&#xf6;m</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_R/0/1/0/all/0/1">Ran Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+McDonald_A/0/1/0/all/0/1">Anthony McDonald</a>, <a href="http://arxiv.org/find/cs/1/au:+Garcia_A/0/1/0/all/0/1">Alfredo Garcia</a>, <a href="http://arxiv.org/find/cs/1/au:+OKelly_M/0/1/0/all/0/1">Matt O&#x27;Kelly</a>, <a href="http://arxiv.org/find/cs/1/au:+Johnson_L/0/1/0/all/0/1">Leif Johnson</a></p>
<p>Understanding adaptive human driving behavior, in particular how drivers
manage uncertainty, is of key importance for developing simulated human driver
models that can be used in the evaluation and development of autonomous
vehicles. However, existing traffic psychology models of adaptive driving
behavior either lack computational rigor or only address specific scenarios
and/or behavioral phenomena. While models developed in the fields of machine
learning and robotics can effectively learn adaptive driving behavior from
data, due to their black box nature, they offer little or no explanation of the
mechanisms underlying the adaptive behavior. Thus, a generalizable,
interpretable, computational model of adaptive human driving behavior is still
lacking. This paper proposes such a model based on active inference, a
behavioral modeling framework originating in computational neuroscience. The
model offers a principled solution to how humans trade progress against caution
through policy selection based on the single mandate to minimize expected free
energy. This casts goal-seeking and information-seeking (uncertainty-resolving)
behavior under a single objective function, allowing the model to seamlessly
resolve uncertainty as a means to obtain its goals. We apply the model in two
apparently disparate driving scenarios that require managing uncertainty, (1)
driving past an occluding object and (2) visual time sharing between driving
and a secondary task, and show how human-like adaptive driving behavior emerges
from the single principle of expected free energy minimization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06438">Controllability-Constrained Deep Network Models for Enhanced Control of Dynamical Systems. (arXiv:2311.06438v1 [eess.SY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Sharma_S/0/1/0/all/0/1">Suruchi Sharma</a>, <a href="http://arxiv.org/find/eess/1/au:+Makarenko_V/0/1/0/all/0/1">Volodymyr Makarenko</a>, <a href="http://arxiv.org/find/eess/1/au:+Kumar_G/0/1/0/all/0/1">Gautam Kumar</a>, <a href="http://arxiv.org/find/eess/1/au:+Tiomkin_S/0/1/0/all/0/1">Stas Tiomkin</a></p>
<p>Control of a dynamical system without the knowledge of dynamics is an
important and challenging task. Modern machine learning approaches, such as
deep neural networks (DNNs), allow for the estimation of a dynamics model from
control inputs and corresponding state observation outputs. Such data-driven
models are often utilized for the derivation of model-based controllers.
However, in general, there are no guarantees that a model represented by DNNs
will be controllable according to the formal control-theoretical meaning of
controllability, which is crucial for the design of effective controllers. This
often precludes the use of DNN-estimated models in applications, where formal
controllability guarantees are required. In this proof-of-the-concept work, we
propose a control-theoretical method that explicitly enhances models estimated
from data with controllability. That is achieved by augmenting the model
estimation objective with a controllability constraint, which penalizes models
with a low degree of controllability. As a result, the models estimated with
the proposed controllability constraint allow for the derivation of more
efficient controllers, they are interpretable by the control-theoretical
quantities and have a lower long-term prediction error. The proposed method
provides new insights on the connection between the DNN-based estimation of
unknown dynamics and the control-theoretical guarantees of the solution
properties. We demonstrate the superiority of the proposed method in two
standard classical control systems with state observation given by low
resolution high-dimensional images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06446">THOS: A Benchmark Dataset for Targeted Hate and Offensive Speech. (arXiv:2311.06446v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Almohaimeed_S/0/1/0/all/0/1">Saad Almohaimeed</a>, <a href="http://arxiv.org/find/cs/1/au:+Almohaimeed_S/0/1/0/all/0/1">Saleh Almohaimeed</a>, <a href="http://arxiv.org/find/cs/1/au:+Shafin_A/0/1/0/all/0/1">Ashfaq Ali Shafin</a>, <a href="http://arxiv.org/find/cs/1/au:+Carbunar_B/0/1/0/all/0/1">Bogdan Carbunar</a>, <a href="http://arxiv.org/find/cs/1/au:+Boloni_L/0/1/0/all/0/1">Ladislau B&#xf6;l&#xf6;ni</a></p>
<p>Detecting harmful content on social media, such as Twitter, is made difficult
by the fact that the seemingly simple yes/no classification conceals a
significant amount of complexity. Unfortunately, while several datasets have
been collected for training classifiers in hate and offensive speech, there is
a scarcity of datasets labeled with a finer granularity of target classes and
specific targets. In this paper, we introduce THOS, a dataset of 8.3k tweets
manually labeled with fine-grained annotations about the target of the message.
We demonstrate that this dataset makes it feasible to train classifiers, based
on Large Language Models, to perform classification at this level of
granularity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06455">Aria-NeRF: Multimodal Egocentric View Synthesis. (arXiv:2311.06455v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jiankai Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1">Jianing Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1">Chuanyang Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Tucker_J/0/1/0/all/0/1">John Tucker</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Javier Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Schwager_M/0/1/0/all/0/1">Mac Schwager</a></p>
<p>We seek to accelerate research in developing rich, multimodal scene models
trained from egocentric data, based on differentiable volumetric ray-tracing
inspired by Neural Radiance Fields (NeRFs). The construction of a NeRF-like
model from an egocentric image sequence plays a pivotal role in understanding
human behavior and holds diverse applications within the realms of VR/AR. Such
egocentric NeRF-like models may be used as realistic simulations, contributing
significantly to the advancement of intelligent agents capable of executing
tasks in the real-world. The future of egocentric view synthesis may lead to
novel environment representations going beyond today's NeRFs by augmenting
visual data with multimodal sensors such as IMU for egomotion tracking, audio
sensors to capture surface texture and human language context, and eye-gaze
trackers to infer human attention patterns in the scene. To support and
facilitate the development and evaluation of egocentric multimodal scene
modeling, we present a comprehensive multimodal egocentric video dataset. This
dataset offers a comprehensive collection of sensory data, featuring RGB
images, eye-tracking camera footage, audio recordings from a microphone,
atmospheric pressure readings from a barometer, positional coordinates from
GPS, connectivity details from Wi-Fi and Bluetooth, and information from
dual-frequency IMU datasets (1kHz and 800Hz) paired with a magnetometer. The
dataset was collected with the Meta Aria Glasses wearable device platform. The
diverse data modalities and the real-world context captured within this dataset
serve as a robust foundation for furthering our understanding of human behavior
and enabling more immersive and intelligent experiences in the realms of VR,
AR, and robotics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06462">Electronic Communication Data Link Encryption Simulation Based on Wireless Communication. (arXiv:2311.06462v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bai_R/0/1/0/all/0/1">Rulin Bai</a></p>
<p>In order to improve the simulation effect of electronic communication data
link encryption, the author proposes a solution based on wireless
communication. The main content of this technology is based on the research of
wireless communication, improve the elliptic curve cryptographic algorithm to
build a system encryption model, obtain legal and valid node private keys,
evaluate and analyze the relevant security attributes of the system, verify the
security of the keys, and realize the encryption optimization of wireless
network communication. Experimental results show that: Using the improved
elliptic curve to simulate the system data chain encryption under the
certificateless public key cryptosystem in network communication, the time is
only 2.31 milliseconds, which is lower than other algorithms. Conclusion: It is
proved that the technology research based on wireless communication can
effectively improve the encryption simulation effect of electronic
communication data link.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06467">Adaptive Language-based Mental Health Assessment with Item-Response Theory. (arXiv:2311.06467v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Varadarajan_V/0/1/0/all/0/1">Vasudha Varadarajan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sikstrom_S/0/1/0/all/0/1">Sverker Sikstr&#xf6;m</a>, <a href="http://arxiv.org/find/cs/1/au:+Kjell_O/0/1/0/all/0/1">Oscar N.E. Kjell</a>, <a href="http://arxiv.org/find/cs/1/au:+Schwartz_H/0/1/0/all/0/1">H. Andrew Schwartz</a></p>
<p>Mental health issues widely vary across individuals - the manifestations of
signs and symptoms can be fairly heterogeneous. Recently, language-based
depression and anxiety assessments have shown promise for capturing this
heterogeneous nature by evaluating a patient's own language, but such
approaches require a large sample of words per person to be accurate. In this
work, we introduce adaptive language-based assessment - the task of iteratively
estimating an individual's psychological score based on limited language
responses to questions that the model also decides to ask. To this end, we
explore two statistical learning-based approaches for measurement/scoring:
classical test theory (CTT) and item response theory (IRT). We find that using
adaptive testing in general can significantly reduce the number of questions
required to achieve high validity (r ~ 0.7) with standardized tests, bringing
down from 11 total questions down to 3 for depression and 5 for anxiety. Given
the combinatorial nature of the problem, we empirically evaluate multiple
strategies for both the ordering and scoring objectives, introducing two new
methods: a semi-supervised item response theory based method (ALIRT), and a
supervised actor-critic based model. While both of the models achieve
significant improvements over random and fixed orderings, we find ALIRT to be a
scalable model that achieves the highest accuracy with lower numbers of
questions (e.g. achieves Pearson r ~ 0.93 after only 3 questions versus asking
all 11 questions). Overall, ALIRT allows prompting a reduced number of
questions without compromising accuracy or overhead computational costs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06497">DRUformer: Enhancing the driving scene Important object detection with driving relationship self-understanding. (arXiv:2311.06497v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1">Yingjie Niu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1">Ming Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Fujii_K/0/1/0/all/0/1">Keisuke Fujii</a>, <a href="http://arxiv.org/find/cs/1/au:+Ohtani_K/0/1/0/all/0/1">Kento Ohtani</a>, <a href="http://arxiv.org/find/cs/1/au:+Carballo_A/0/1/0/all/0/1">Alexander Carballo</a>, <a href="http://arxiv.org/find/cs/1/au:+Takeda_K/0/1/0/all/0/1">Kazuya Takeda</a></p>
<p>Traffic accidents frequently lead to fatal injuries, contributing to over 50
million deaths until 2023. To mitigate driving hazards and ensure personal
safety, it is crucial to assist vehicles in anticipating important objects
during travel. Previous research on important object detection primarily
assessed the importance of individual participants, treating them as
independent entities and frequently overlooking the connections between these
participants. Unfortunately, this approach has proven less effective in
detecting important objects in complex scenarios. In response, we introduce
Driving scene Relationship self-Understanding transformer (DRUformer), designed
to enhance the important object detection task. The DRUformer is a
transformer-based multi-modal important object detection model that takes into
account the relationships between all the participants in the driving scenario.
Recognizing that driving intention also significantly affects the detection of
important objects during driving, we have incorporated a module for embedding
driving intention. To assess the performance of our approach, we conducted a
comparative experiment on the DRAMA dataset, pitting our model against other
state-of-the-art (SOTA) models. The results demonstrated a noteworthy 16.2\%
improvement in mIoU and a substantial 12.3\% boost in ACC compared to SOTA
methods. Furthermore, we conducted a qualitative analysis of our model's
ability to detect important objects across different road scenarios and
classes, highlighting its effectiveness in diverse contexts. Finally, we
conducted various ablation studies to assess the efficiency of the proposed
modules in our DRUformer model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06503">Knowledgeable Preference Alignment for LLMs in Domain-specific Question Answering. (arXiv:2311.06503v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yichi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhuo Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1">Yin Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1">Lei Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yanxi Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1">Fangming Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huajun Chen</a></p>
<p>Recently, the development of large language models (LLMs) has attracted wide
attention in academia and industry. Deploying LLMs to real scenarios is one of
the key directions in the current Internet industry. In this paper, we present
a novel pipeline to apply LLMs for domain-specific question answering (QA) that
incorporates domain knowledge graphs (KGs), addressing an important direction
of LLM application. As a real-world application, the content generated by LLMs
should be user-friendly to serve the customers. Additionally, the model needs
to utilize domain knowledge properly to generate reliable answers. These two
issues are the two major difficulties in the LLM application as vanilla
fine-tuning can not adequately address them. We think both requirements can be
unified as the model preference problem that needs to align with humans to
achieve practical application. Thus, we introduce Knowledgeable Preference
AlignmenT (KnowPAT), which constructs two kinds of preference set called style
preference set and knowledge preference set respectively to tackle the two
issues. Besides, we design a new alignment objective to align the LLM
preference with human preference, aiming to train a better LLM for
real-scenario domain-specific QA to generate reliable and user-friendly
answers. Adequate experiments and comprehensive with 15 baseline methods
demonstrate that our KnowPAT is an outperforming pipeline for real-scenario
domain-specific QA with LLMs. Our code is open-source at
https://github.com/zjukg/KnowPAT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06513">Step by Step to Fairness: Attributing Societal Bias in Task-oriented Dialogue Systems. (arXiv:2311.06513v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1">Hsuan Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1">Rebecca Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Sankar_C/0/1/0/all/0/1">Chinnadhurai Sankar</a>, <a href="http://arxiv.org/find/cs/1/au:+Shayandeh_S/0/1/0/all/0/1">Shahin Shayandeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shang-Tse Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hung-yi Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Bikel_D/0/1/0/all/0/1">Daniel M. Bikel</a></p>
<p>Recent works have shown considerable improvements in task-oriented dialogue
(TOD) systems by utilizing pretrained large language models (LLMs) in an
end-to-end manner. However, the biased behavior of each component in a TOD
system and the error propagation issue in the end-to-end framework can lead to
seriously biased TOD responses. Existing works of fairness only focus on the
total bias of a system. In this paper, we propose a diagnosis method to
attribute bias to each component of a TOD system. With the proposed attribution
method, we can gain a deeper understanding of the sources of bias.
Additionally, researchers can mitigate biased model behavior at a more granular
level. We conduct experiments to attribute the TOD system's bias toward three
demographic axes: gender, age, and race. Experimental results show that the
bias of a TOD system usually comes from the response generation model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06517">BClean: A Bayesian Data Cleaning System. (arXiv:2311.06517v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1">Jianbin Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Sifan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yaoshu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jing Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yifan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Miao_Y/0/1/0/all/0/1">Yukai Miao</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_R/0/1/0/all/0/1">Rui Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Onizuka_M/0/1/0/all/0/1">Makoto Onizuka</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Chuan Xiao</a></p>
<p>There is a considerable body of work on data cleaning which employs various
principles to rectify erroneous data and transform a dirty dataset into a
cleaner one. One of prevalent approaches is probabilistic methods, including
Bayesian methods. However, existing probabilistic methods often assume a
simplistic distribution (e.g., Gaussian distribution), which is frequently
underfitted in practice, or they necessitate experts to provide a complex prior
distribution (e.g., via a programming language). This requirement is both
labor-intensive and costly, rendering these methods less suitable for
real-world applications. In this paper, we propose BClean, a Bayesian Cleaning
system that features automatic Bayesian network construction and user
interaction. We recast the data cleaning problem as a Bayesian inference that
fully exploits the relationships between attributes in the observed dataset and
any prior information provided by users. To this end, we present an automatic
Bayesian network construction method that extends a structure learning-based
functional dependency discovery method with similarity functions to capture the
relationships between attributes. Furthermore, our system allows users to
modify the generated Bayesian network in order to specify prior information or
correct inaccuracies identified by the automatic generation process. We also
design an effective scoring model (called the compensative scoring model)
necessary for the Bayesian inference. To enhance the efficiency of data
cleaning, we propose several approximation strategies for the Bayesian
inference, including graph partitioning, domain pruning, and pre-detection. By
evaluating on both real-world and synthetic datasets, we demonstrate that
BClean is capable of achieving an F-measure of up to 0.9 in data cleaning,
outperforming existing Bayesian methods by 2% and other data cleaning methods
by 15%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06530">How ChatGPT is Solving Vulnerability Management Problem. (arXiv:2311.06530v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1">Peiyu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Junming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_L/0/1/0/all/0/1">Lirong Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1">Kangjie Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1">Yifan Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xuhong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wenzhi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Weng_H/0/1/0/all/0/1">Haiqin Weng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1">Shouling Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenhai Wang</a></p>
<p>Recently, ChatGPT has attracted great attention from the code analysis
domain. Prior works show that ChatGPT has the capabilities of processing
foundational code analysis tasks, such as abstract syntax tree generation,
which indicates the potential of using ChatGPT to comprehend code syntax and
static behaviors. However, it is unclear whether ChatGPT can complete more
complicated real-world vulnerability management tasks, such as the prediction
of security relevance and patch correctness, which require an all-encompassing
understanding of various aspects, including code syntax, program semantics, and
related manual comments.
</p>
<p>In this paper, we explore ChatGPT's capabilities on 6 tasks involving the
complete vulnerability management process with a large-scale dataset containing
78,445 samples. For each task, we compare ChatGPT against SOTA approaches,
investigate the impact of different prompts, and explore the difficulties. The
results suggest promising potential in leveraging ChatGPT to assist
vulnerability management. One notable example is ChatGPT's proficiency in tasks
like generating titles for software bug reports. Furthermore, our findings
reveal the difficulties encountered by ChatGPT and shed light on promising
future directions. For instance, directly providing random demonstration
examples in the prompt cannot consistently guarantee good performance in
vulnerability management. By contrast, leveraging ChatGPT in a self-heuristic
way -- extracting expertise from demonstration examples itself and integrating
the extracted expertise in the prompt is a promising research direction.
Besides, ChatGPT may misunderstand and misuse the information in the prompt.
Consequently, effectively guiding ChatGPT to focus on helpful information
rather than the irrelevant content is still an open problem.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06537">Is Machine Learning Unsafe and Irresponsible in Social Sciences? Paradoxes and Reconsidering from Recidivism Prediction Tasks. (arXiv:2311.06537v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jianhong Liu</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dianshi Li</a> (1) ((1) Faculty of Law, University of Macau, Macau, China)</p>
<p>The paper addresses some fundamental and hotly debated issues for high-stakes
event predictions underpinning the computational approach to social sciences.
We question several prevalent views against machine learning and outline a new
paradigm that highlights the promises and promotes the infusion of
computational methods and conventional social science approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06555">Heuristics-Driven Link-of-Analogy Prompting: Enhancing Large Language Models for Document-Level Event Argument Extraction. (arXiv:2311.06555v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Hanzhang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1">Junlang Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1">Zijian Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1">Hui Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Zixiao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_K/0/1/0/all/0/1">Kezhi Mao</a></p>
<p>In this study, we investigate in-context learning (ICL) in document-level
event argument extraction (EAE). The paper identifies key challenges in this
problem, including example selection, context length limitation, abundance of
event types, and the limitation of Chain-of-Thought (CoT) prompting in
non-reasoning tasks. To address these challenges, we introduce the
Heuristic-Driven Link-of-Analogy (HD-LoA) prompting method. Specifically, we
hypothesize and validate that LLMs learn task-specific heuristics from
demonstrations via ICL. Building upon this hypothesis, we introduce an explicit
heuristic-driven demonstration construction approach, which transforms the
haphazard example selection process into a methodical method that emphasizes
task heuristics. Additionally, inspired by the analogical reasoning of human,
we propose the link-of-analogy prompting, which enables LLMs to process new
situations by drawing analogies to known situations, enhancing their
adaptability. Extensive experiments show that our method outperforms the
existing prompting methods and few-shot supervised learning methods, exhibiting
F1 score improvements of 4.53% and 9.38% on the document-level EAE dataset.
Furthermore, when applied to sentiment analysis and natural language inference
tasks, the HD-LoA prompting achieves accuracy gains of 2.87% and 2.63%,
indicating its effectiveness across different tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06567">SCADI: Self-supervised Causal Disentanglement in Latent Variable Models. (arXiv:2311.06567v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nam_H/0/1/0/all/0/1">Heejeong Nam</a></p>
<p>Causal disentanglement has great potential for capturing complex situations.
However, there is a lack of practical and efficient approaches. It is already
known that most unsupervised disentangling methods are unable to produce
identifiable results without additional information, often leading to randomly
disentangled output. Therefore, most existing models for disentangling are
weakly supervised, providing information about intrinsic factors, which incurs
excessive costs. Therefore, we propose a novel model, SCADI(SElf-supervised
CAusal DIsentanglement), that enables the model to discover semantic factors
and learn their causal relationships without any supervision. This model
combines a masked structural causal model (SCM) with a pseudo-label generator
for causal disentanglement, aiming to provide a new direction for
self-supervised causal disentanglement models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06576">An Intelligent Social Learning-based Optimization Strategy for Black-box Robotic Control with Reinforcement Learning. (arXiv:2311.06576v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xubo Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jian Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Ting Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yaozhen He</a></p>
<p>Implementing intelligent control of robots is a difficult task, especially
when dealing with complex black-box systems, because of the lack of visibility
and understanding of how these robots work internally. This paper proposes an
Intelligent Social Learning (ISL) algorithm to enable intelligent control of
black-box robotic systems. Inspired by mutual learning among individuals in
human social groups, ISL includes learning, imitation, and self-study styles.
Individuals in the learning style use the Levy flight search strategy to learn
from the best performer and form the closest relationships. In the imitation
style, individuals mimic the best performer with a second-level rapport by
employing a random perturbation strategy. In the self-study style, individuals
learn independently using a normal distribution sampling method while
maintaining a distant relationship with the best performer. Individuals in the
population are regarded as autonomous intelligent agents in each style. Neural
networks perform strategic actions in three styles to interact with the
environment and the robot and iteratively optimize the network policy. Overall,
ISL builds on the principles of intelligent optimization, incorporating ideas
from reinforcement learning, and possesses strong search capabilities, fast
computation speed, fewer hyperparameters, and insensitivity to sparse rewards.
The proposed ISL algorithm is compared with four state-of-the-art methods on
six continuous control benchmark cases in MuJoCo to verify its effectiveness
and advantages. Furthermore, ISL is adopted in the simulation and experimental
grasping tasks of the UR3 robot for validations, and satisfactory solutions are
yielded.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06597">Understanding Grokking Through A Robustness Viewpoint. (arXiv:2311.06597v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1">Zhiquan Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1">Weiran Huang</a></p>
<p>Recently, an unusual phenomenon called grokking has gained much attention,
where sometimes a neural network generalizes long after it perfectly fits the
training data. We try to understand this seemingly strange phenomenon using the
robustness of the neural network. Using a robustness viewpoint, we show that
the popular $l_2$ weight norm (metric) of the neural network is actually a
sufficient condition for grokking. As we also empirically find that $l_2$ norm
correlates with grokking on the test data not in a timely way, we propose new
metrics based on robustness and information theory and find that our new
metrics correlate well with the grokking phenomenon. Based on the previous
observations, we propose methods to speed up the generalization process. In
addition, we examine the standard training process on modulo addition dataset
and find that it hardly learns other basic group operations before grokking,
including the commutative law. Interestingly, the speed up of generalization
when using our proposed method can be partially explained by learning the
commutative law, a necessary condition when the model groks on test dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06607">Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models. (arXiv:2311.06607v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1">Biao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qiang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1">Zhiyin Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shuo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jingxu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yabo Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuliang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1">Xiang Bai</a></p>
<p>Large Multimodal Models have demonstrated impressive capabilities in
understanding general vision-language tasks. However, due to the limitation of
supported input resolution (e.g., 448 x 448) as well as the inexhaustive
description of the training image-text pair, these models often encounter
challenges when dealing with intricate scene understandings and narratives.
Here we address the problem by proposing the Monkey. Our contributions are
two-fold: 1) without pretraining from the start, our method can be built upon
an existing vision encoder (e.g., vit-BigHuge) to effectively improve the input
resolution capacity up to 896 x 1344 pixels; 2) we propose a multi-level
description generation method, which automatically provides rich information
that can guide model to learn contextual association between scenes and
objects. Our extensive testing across more than 16 distinct datasets reveals
that Monkey achieves consistently competitive performance over the existing
LMMs on fundamental tasks, such as Image Captioning, General Visual Question
Answering (VQA), and Document-oriented VQA. Models, interactive demo, and the
source code are provided at the following
https://github.com/Yuliang-Liu/Monkey.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06622">TrainerAgent: Customizable and Efficient Model Training through LLM-Powered Multi-Agent System. (arXiv:2311.06622v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Haoyuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1">Hao Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tianke Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Zhelun Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_A/0/1/0/all/0/1">Aoxiong Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1">Hao Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1">Siming Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuhao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1">Wanggui He</a></p>
<p>Training AI models has always been challenging, especially when there is a
need for custom models to provide personalized services. Algorithm engineers
often face a lengthy process to iteratively develop models tailored to specific
business requirements, making it even more difficult for non-experts. The quest
for high-quality and efficient model development, along with the emergence of
Large Language Model (LLM) Agents, has become a key focus in the industry.
Leveraging the powerful analytical, planning, and decision-making capabilities
of LLM, we propose a TrainerAgent system comprising a multi-agent framework
including Task, Data, Model and Server agents. These agents analyze
user-defined tasks, input data, and requirements (e.g., accuracy, speed),
optimizing them comprehensively from both data and model perspectives to obtain
satisfactory models, and finally deploy these models as online service.
Experimental evaluations on classical discriminative and generative tasks in
computer vision and natural language processing domains demonstrate that our
system consistently produces models that meet the desired criteria.
Furthermore, the system exhibits the ability to critically identify and reject
unattainable tasks, such as fantastical scenarios or unethical requests,
ensuring robustness and safety. This research presents a significant
advancement in achieving desired models with increased efficiency and quality
as compared to traditional model development, facilitated by the integration of
LLM-powered analysis, decision-making, and execution capabilities, as well as
the collaboration among four agents. We anticipate that our work will
contribute to the advancement of research on TrainerAgent in both academic and
industry communities, potentially establishing it as a new paradigm for model
development in the field of AI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06623">VT-Former: A Transformer-based Vehicle Trajectory Prediction Approach For Intelligent Highway Transportation Systems. (arXiv:2311.06623v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pazho_A/0/1/0/all/0/1">Armin Danesh Pazho</a>, <a href="http://arxiv.org/find/cs/1/au:+Katariya_V/0/1/0/all/0/1">Vinit Katariya</a>, <a href="http://arxiv.org/find/cs/1/au:+Noghre_G/0/1/0/all/0/1">Ghazal Alinezhad Noghre</a>, <a href="http://arxiv.org/find/cs/1/au:+Tabkhi_H/0/1/0/all/0/1">Hamed Tabkhi</a></p>
<p>Enhancing roadway safety and traffic management has become an essential focus
area for a broad range of modern cyber-physical systems and intelligent
transportation systems. Vehicle Trajectory Prediction is a pivotal element
within numerous applications for highway and road safety. These applications
encompass a wide range of use cases, spanning from traffic management and
accident prevention to enhancing work-zone safety and optimizing energy
conservation. The ability to implement intelligent management in this context
has been greatly advanced by the developments in the field of Artificial
Intelligence (AI), alongside the increasing deployment of surveillance cameras
across road networks. In this paper, we introduce a novel transformer-based
approach for vehicle trajectory prediction for highway safety and surveillance,
denoted as VT-Former. In addition to utilizing transformers to capture
long-range temporal patterns, a new Graph Attentive Tokenization (GAT) module
has been proposed to capture intricate social interactions among vehicles.
Combining these two core components culminates in a precise approach for
vehicle trajectory prediction. Our study on three benchmark datasets with three
different viewpoints demonstrates the State-of-The-Art (SoTA) performance of
VT-Former in vehicle trajectory prediction and its generalizability and
robustness. We also evaluate VT-Former's efficiency on embedded boards and
explore its potential for vehicle anomaly detection as a sample application,
showcasing its broad applicability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06633">The Pros and Cons of Using Machine Learning and Interpretable Machine Learning Methods in psychiatry detection applications, specifically depression disorder: A Brief Review. (arXiv:2311.06633v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Simchi_H/0/1/0/all/0/1">Hossein Simchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Tajik_S/0/1/0/all/0/1">Samira Tajik</a></p>
<p>The COVID-19 pandemic has forced many people to limit their social
activities, which has resulted in a rise in mental illnesses, particularly
depression. To diagnose these illnesses with accuracy and speed, and prevent
severe outcomes such as suicide, the use of machine learning has become
increasingly important. Additionally, to provide precise and understandable
diagnoses for better treatment, AI scientists and researchers must develop
interpretable AI-based solutions. This article provides an overview of relevant
articles in the field of machine learning and interpretable AI, which helps to
understand the advantages and disadvantages of using AI in psychiatry disorder
detection applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/1902.11122">Deep Learning in Cardiology. (arXiv:1902.11122v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bizopoulos_P/0/1/0/all/0/1">Paschalis Bizopoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Koutsouris_D/0/1/0/all/0/1">Dimitrios Koutsouris</a></p>
<p>The medical field is creating large amount of data that physicians are unable
to decipher and use efficiently. Moreover, rule-based expert systems are
inefficient in solving complicated medical tasks or for creating insights using
big data. Deep learning has emerged as a more accurate and effective technology
in a wide range of medical problems such as diagnosis, prediction and
intervention. Deep learning is a representation learning method that consists
of layers that transform the data non-linearly, thus, revealing hierarchical
relationships and structures. In this review we survey deep learning
application papers that use structured data, signal and imaging modalities from
cardiology. We discuss the advantages and limitations of applying deep learning
in cardiology that also apply in medicine in general, while proposing certain
directions as the most viable for clinical use.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2012.08026">Classification of Smoking and Calling using Deep Learning. (arXiv:2012.08026v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Miaowei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohacey_A/0/1/0/all/0/1">Alexander William Mohacey</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hongyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Apfel_J/0/1/0/all/0/1">James Apfel</a></p>
<p>Since 2014, very deep convolutional neural networks have been proposed and
become the must-have weapon for champions in all kinds of competition. In this
report, a pipeline is introduced to perform the classification of smoking and
calling by modifying the pretrained inception V3. Brightness enhancing based on
deep learning is implemented to improve the classification of this
classification task along with other useful training tricks. Based on the
quality and quantity results, it can be concluded that this pipeline with small
biased samples is practical and useful with high accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2201.11239">Diagnosing AI Explanation Methods with Folk Concepts of Behavior. (arXiv:2201.11239v4 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jacovi_A/0/1/0/all/0/1">Alon Jacovi</a>, <a href="http://arxiv.org/find/cs/1/au:+Bastings_J/0/1/0/all/0/1">Jasmijn Bastings</a>, <a href="http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1">Sebastian Gehrmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1">Yoav Goldberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Filippova_K/0/1/0/all/0/1">Katja Filippova</a></p>
<p>We investigate a formalism for the conditions of a successful explanation of
AI. We consider "success" to depend not only on what information the
explanation contains, but also on what information the human explainee
understands from it. Theory of mind literature discusses the folk concepts that
humans use to understand and generalize behavior. We posit that folk concepts
of behavior provide us with a "language" that humans understand behavior with.
We use these folk concepts as a framework of *social attribution* by the human
explainee -- the information constructs that humans are likely to comprehend
from explanations -- by introducing a blueprint for an explanatory narrative
(Figure 1) that explains AI behavior with these constructs. We then demonstrate
that many XAI methods today can be mapped to folk concepts of behavior in a
qualitative evaluation. This allows us to uncover their failure modes that
prevent current methods from explaining successfully -- i.e., the information
constructs that are missing for any given XAI method, and whose inclusion can
decrease the likelihood of misunderstanding AI behavior.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.03691">HyperMixer: An MLP-based Low Cost Alternative to Transformers. (arXiv:2203.03691v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mai_F/0/1/0/all/0/1">Florian Mai</a>, <a href="http://arxiv.org/find/cs/1/au:+Pannatier_A/0/1/0/all/0/1">Arnaud Pannatier</a>, <a href="http://arxiv.org/find/cs/1/au:+Fehr_F/0/1/0/all/0/1">Fabio Fehr</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Haolin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Marelli_F/0/1/0/all/0/1">Francois Marelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Fleuret_F/0/1/0/all/0/1">Francois Fleuret</a>, <a href="http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1">James Henderson</a></p>
<p>Transformer-based architectures are the model of choice for natural language
understanding, but they come at a significant cost, as they have quadratic
complexity in the input length, require a lot of training data, and can be
difficult to tune. In the pursuit of lower costs, we investigate simple
MLP-based architectures. We find that existing architectures such as MLPMixer,
which achieves token mixing through a static MLP applied to each feature
independently, are too detached from the inductive biases required for natural
language understanding. In this paper, we propose a simple variant, HyperMixer,
which forms the token mixing MLP dynamically using hypernetworks. Empirically,
we demonstrate that our model performs better than alternative MLP-based
models, and on par with Transformers. In contrast to Transformers, HyperMixer
achieves these results at substantially lower costs in terms of processing
time, training data, and hyperparameter tuning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2204.00281">i-Razor: A Differentiable Neural Input Razor for Feature Selection and Dimension Search in DNN-Based Recommender Systems. (arXiv:2204.00281v2 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1">Yao Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1">Haoxun He</a>, <a href="http://arxiv.org/find/cs/1/au:+Sheng_D/0/1/0/all/0/1">Dakui Sheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Ke Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1">Li Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1">Huanhuan Cao</a></p>
<p>Input features play a crucial role in DNN-based recommender systems with
thousands of categorical and continuous fields from users, items, contexts, and
interactions. Noisy features and inappropriate embedding dimension assignments
can deteriorate the performance of recommender systems and introduce
unnecessary complexity in model training and online serving. Optimizing the
input configuration of DNN models, including feature selection and embedding
dimension assignment, has become one of the essential topics in feature
engineering. However, in existing industrial practices, feature selection and
dimension search are optimized sequentially, i.e., feature selection is
performed first, followed by dimension search to determine the optimal
dimension size for each selected feature. Such a sequential optimization
mechanism increases training costs and risks generating suboptimal input
configurations. To address this problem, we propose a differentiable neural
input razor (i-Razor) that enables joint optimization of feature selection and
dimension search. Concretely, we introduce an end-to-end differentiable model
to learn the relative importance of different embedding regions of each
feature. Furthermore, a flexible pruning algorithm is proposed to achieve
feature filtering and dimension derivation simultaneously. Extensive
experiments on two large-scale public datasets in the Click-Through-Rate (CTR)
prediction task demonstrate the efficacy and superiority of i-Razor in
balancing model complexity and performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.01269">MP and MT properties of fuzzy inference with aggregation function. (arXiv:2205.01269v2 [math.LO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Li_D/0/1/0/all/0/1">Dechao Li</a>, <a href="http://arxiv.org/find/math/1/au:+He_M/0/1/0/all/0/1">Mengying He</a></p>
<p>As the two basic fuzzy inference models, fuzzy modus ponens (FMP) and fuzzy
modus tollens (FMT) have the important application in artificial intelligence.
In order to solve FMP and FMT problems, Zadeh proposed a compositional rule of
inference (CRI) method. This paper aims mainly to investigate the validity of
A-compositional rule of inference (ACRI) method, as a generalized CRI method
based on aggregation functions, from a logical view and an interpolative view,
respectively. Specifically, the modus ponens (MP) and modus tollens (MT)
properties of ACRI method are discussed in detail. It is shown that the
aggregation functions to implement FMP and FMT problems provide more generality
than the t-norms, uninorms and overlap functions as well-known the laws of
T-conditionality, U-conditionality and O-conditionality, respectively.
Moreover, two examples are also given to illustrate our theoretical results.
Especially, Example 6.2 shows that the output B' in FMP(FMT) problem is close
to B(DC) with our proposed inference method when the fuzzy input and the
antecedent of fuzzy rule are near (the fuzzy input near with the negation of
the seccedent in fuzzy rule).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.02536">The impact of spatio-temporal travel distance on epidemics using an interpretable attention-based sequence-to-sequence model. (arXiv:2206.02536v2 [physics.soc-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Jiang_Y/0/1/0/all/0/1">Yukang Jiang</a>, <a href="http://arxiv.org/find/physics/1/au:+Tian_T/0/1/0/all/0/1">Ting Tian</a>, <a href="http://arxiv.org/find/physics/1/au:+Xie_H/0/1/0/all/0/1">Huajun Xie</a>, <a href="http://arxiv.org/find/physics/1/au:+Guo_H/0/1/0/all/0/1">Hailiang Guo</a>, <a href="http://arxiv.org/find/physics/1/au:+Wang_X/0/1/0/all/0/1">Xueqin Wang</a></p>
<p>Amidst the COVID-19 pandemic, travel restrictions have emerged as crucial
interventions for mitigating the spread of the virus. In this study, we enhance
the predictive capabilities of our model, Sequence-to-Sequence Epidemic
Attention Network (S2SEA-Net), by incorporating an attention module, allowing
us to assess the impact of distinct classes of travel distances on epidemic
dynamics. Furthermore, our model provides forecasts for new confirmed cases and
deaths. To achieve this, we leverage daily data on population movement across
various travel distance categories, coupled with county-level epidemic data in
the United States. Our findings illuminate a compelling relationship between
the volume of travelers at different distance ranges and the trajectories of
COVID-19. Notably, a discernible spatial pattern emerges with respect to these
travel distance categories on a national scale. We unveil the geographical
variations in the influence of population movement at different travel
distances on the dynamics of epidemic spread. This will contribute to the
formulation of strategies for future epidemic prevention and public health
policies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.14750">Non-contrastive representation learning for intervals from well logs. (arXiv:2209.14750v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Marusov_A/0/1/0/all/0/1">Alexander Marusov</a>, <a href="http://arxiv.org/find/cs/1/au:+Zaytsev_A/0/1/0/all/0/1">Alexey Zaytsev</a></p>
<p>The representation learning problem in the oil &amp; gas industry aims to
construct a model that provides a representation based on logging data for a
well interval. Previous attempts are mainly supervised and focus on similarity
task, which estimates closeness between intervals. We desire to build
informative representations without using supervised (labelled) data. One of
the possible approaches is self-supervised learning (SSL). In contrast to the
supervised paradigm, this one requires little or no labels for the data.
Nowadays, most SSL approaches are either contrastive or non-contrastive.
Contrastive methods make representations of similar (positive) objects closer
and distancing different (negative) ones. Due to possible wrong marking of
positive and negative pairs, these methods can provide an inferior performance.
Non-contrastive methods don't rely on such labelling and are widespread in
computer vision. They learn using only pairs of similar objects that are easier
to identify in logging data.
</p>
<p>We are the first to introduce non-contrastive SSL for well-logging data. In
particular, we exploit Bootstrap Your Own Latent (BYOL) and Barlow Twins
methods that avoid using negative pairs and focus only on matching positive
pairs. The crucial part of these methods is an augmentation strategy. Our
augmentation strategies and adaption of BYOL and Barlow Twins together allow us
to achieve superior quality on clusterization and mostly the best performance
on different classification tasks. Our results prove the usefulness of the
proposed non-contrastive self-supervised approaches for representation learning
and interval similarity in particular.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.03461">FastCLIPstyler: Optimisation-free Text-based Image Style Transfer Using Style Representations. (arXiv:2210.03461v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Suresh_A/0/1/0/all/0/1">Ananda Padhmanabhan Suresh</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1">Sanjana Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Noinongyao_P/0/1/0/all/0/1">Pavit Noinongyao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganguly_A/0/1/0/all/0/1">Ankush Ganguly</a></p>
<p>In recent years, language-driven artistic style transfer has emerged as a new
type of style transfer technique, eliminating the need for a reference style
image by using natural language descriptions of the style. The first model to
achieve this, called CLIPstyler, has demonstrated impressive stylisation
results. However, its lengthy optimisation procedure at runtime for each query
limits its suitability for many practical applications. In this work, we
present FastCLIPstyler, a generalised text-based image style transfer model
capable of stylising images in a single forward pass for arbitrary text inputs.
Furthermore, we introduce EdgeCLIPstyler, a lightweight model designed for
compatibility with resource-constrained devices. Through quantitative and
qualitative comparisons with state-of-the-art approaches, we demonstrate that
our models achieve superior stylisation quality based on measurable metrics
while offering significantly improved runtime efficiency, particularly on edge
devices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.06650">Interpreting Neural Policies with Disentangled Tree Representations. (arXiv:2210.06650v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tsun-Hsuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1">Wei Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Seyde_T/0/1/0/all/0/1">Tim Seyde</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasani_R/0/1/0/all/0/1">Ramin Hasani</a>, <a href="http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1">Daniela Rus</a></p>
<p>The advancement of robots, particularly those functioning in complex
human-centric environments, relies on control solutions that are driven by
machine learning. Understanding how learning-based controllers make decisions
is crucial since robots are often safety-critical systems. This urges a formal
and quantitative understanding of the explanatory factors in the
interpretability of robot learning. In this paper, we aim to study
interpretability of compact neural policies through the lens of disentangled
representation. We leverage decision trees to obtain factors of variation [1]
for disentanglement in robot learning; these encapsulate skills, behaviors, or
strategies toward solving tasks. To assess how well networks uncover the
underlying task dynamics, we introduce interpretability metrics that measure
disentanglement of learned neural dynamics from a concentration of decisions,
mutual information and modularity perspective. We showcase the effectiveness of
the connection between interpretability and disentanglement consistently across
extensive experimental analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.03831">Multi-Head Adapter Routing for Cross-Task Generalization. (arXiv:2211.03831v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Caccia_L/0/1/0/all/0/1">Lucas Caccia</a>, <a href="http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1">Edoardo Ponti</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1">Zhan Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Pereira_M/0/1/0/all/0/1">Matheus Pereira</a>, <a href="http://arxiv.org/find/cs/1/au:+Roux_N/0/1/0/all/0/1">Nicolas Le Roux</a>, <a href="http://arxiv.org/find/cs/1/au:+Sordoni_A/0/1/0/all/0/1">Alessandro Sordoni</a></p>
<p>Parameter-efficient fine-tuning (PEFT) for cross-task generalization consists
in pre-training adapters on a multi-task training set before few-shot
adaptation to test tasks. Polytropon [Ponti et al., 2023] ($\texttt{Poly}$)
jointly learns an inventory of adapters and a routing function that selects a
(variable-size) subset of adapters for each task during both pre-training and
few-shot adaptation. In this paper, we investigate the role that adapter
routing plays in its success and design new variants based on our findings.
First, we build on the intuition that finer-grained routing provides more
expressivity. Hence, we propose $\texttt{MHR}$ (Multi-Head Routing) which
combines subsets of adapter parameters and outperforms $\texttt{Poly}$ under a
comparable parameter budget; by only fine-tuning the routing function and not
the adapters ($\texttt{MHR}$-$z$) we achieve competitive performance with
extreme parameter efficiency. Second, we find that
$\texttt{Poly}$/$\texttt{MHR}$ performance is a result of better multi-task
optimization, rather than modular inductive biases that facilitate adapter
recombination and local adaptation, as previously hypothesized. In fact, we
find that $\texttt{MHR}$ exhibits high gradient alignment between training
tasks. We find that routing is most beneficial during multi-task pre-training
rather than during few-shot adaptation and propose $\texttt{MHR}$-$\mu$, which
discards routing and fine-tunes the average of the pre-trained adapters on each
downstream tasks. This establishes $\texttt{MHR}$-$\mu$ as an effective method
for single-adapter fine-tuning. We also show that $\texttt{MHR}$-$\mu$ can be
used as an effective zero-shot transfer method by training the average of the
pre-trained adapters for a few additional steps on the multi-task training set:
this yields gains up to 3% on absolute accuracy w.r.t. the baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.13308">SciRepEval: A Multi-Format Benchmark for Scientific Document Representations. (arXiv:2211.13308v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1">Amanpreet Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+DArcy_M/0/1/0/all/0/1">Mike D&#x27;Arcy</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1">Arman Cohan</a>, <a href="http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1">Doug Downey</a>, <a href="http://arxiv.org/find/cs/1/au:+Feldman_S/0/1/0/all/0/1">Sergey Feldman</a></p>
<p>Learned representations of scientific documents can serve as valuable input
features for downstream tasks without further fine-tuning. However, existing
benchmarks for evaluating these representations fail to capture the diversity
of relevant tasks. In response, we introduce SciRepEval, the first
comprehensive benchmark for training and evaluating scientific document
representations. It includes 24 challenging and realistic tasks, 8 of which are
new, across four formats: classification, regression, ranking and search. We
then use this benchmark to study and improve the generalization ability of
scientific document representation models. We show how state-of-the-art models
like SPECTER and SciNCL struggle to generalize across the task formats, and
that simple multi-task training fails to improve them. However, a new approach
that learns multiple embeddings per document, each tailored to a different
format, can improve performance. We experiment with task-format-specific
control codes and adapters and find they outperform the existing
single-embedding state-of-the-art by over 2 points absolute. We release the
resulting family of multi-format models, called SPECTER2, for the community to
use and build on.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.14468">Similarity-based cooperative equilibrium. (arXiv:2211.14468v2 [cs.GT] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oesterheld_C/0/1/0/all/0/1">Caspar Oesterheld</a>, <a href="http://arxiv.org/find/cs/1/au:+Treutlein_J/0/1/0/all/0/1">Johannes Treutlein</a>, <a href="http://arxiv.org/find/cs/1/au:+Grosse_R/0/1/0/all/0/1">Roger Grosse</a>, <a href="http://arxiv.org/find/cs/1/au:+Conitzer_V/0/1/0/all/0/1">Vincent Conitzer</a>, <a href="http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1">Jakob Foerster</a></p>
<p>As machine learning agents act more autonomously in the world, they will
increasingly interact with each other. Unfortunately, in many social dilemmas
like the one-shot Prisoner's Dilemma, standard game theory predicts that ML
agents will fail to cooperate with each other. Prior work has shown that one
way to enable cooperative outcomes in the one-shot Prisoner's Dilemma is to
make the agents mutually transparent to each other, i.e., to allow them to
access one another's source code (Rubinstein 1998, Tennenholtz 2004) -- or
weights in the case of ML agents. However, full transparency is often
unrealistic, whereas partial transparency is commonplace. Moreover, it is
challenging for agents to learn their way to cooperation in the full
transparency setting. In this paper, we introduce a more realistic setting in
which agents only observe a single number indicating how similar they are to
each other. We prove that this allows for the same set of cooperative outcomes
as the full transparency setting. We also demonstrate experimentally that
cooperation can be learned using simple ML methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.01382">Welfare and Fairness in Multi-objective Reinforcement Learning. (arXiv:2212.01382v5 [cs.GT] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1">Zimeng Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1">Nianli Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_M/0/1/0/all/0/1">Muhang Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Fain_B/0/1/0/all/0/1">Brandon Fain</a></p>
<p>We study fair multi-objective reinforcement learning in which an agent must
learn a policy that simultaneously achieves high reward on multiple dimensions
of a vector-valued reward. Motivated by the fair resource allocation
literature, we model this as an expected welfare maximization problem, for some
nonlinear fair welfare function of the vector of long-term cumulative rewards.
One canonical example of such a function is the Nash Social Welfare, or
geometric mean, the log transform of which is also known as the Proportional
Fairness objective. We show that even approximately optimal optimization of the
expected Nash Social Welfare is computationally intractable even in the tabular
case. Nevertheless, we provide a novel adaptation of Q-learning that combines
nonlinear scalarized learning updates and non-stationary action selection to
learn effective policies for optimizing nonlinear welfare functions. We show
that our algorithm is provably convergent, and we demonstrate experimentally
that our approach outperforms techniques based on linear scalarization,
mixtures of optimal linear scalarizations, or stationary action selection for
the Nash Social Welfare Objective.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.08487">Semantics-Empowered Communication: A Tutorial-cum-Survey. (arXiv:2212.08487v5 [cs.HC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Zhilin Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1">Rongpeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1">Kun Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xianfu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hossain_E/0/1/0/all/0/1">Ekram Hossain</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zhifeng Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Honggang Zhang</a></p>
<p>Along with the springing up of the semantics-empowered communication (SemCom)
research, it is now witnessing an unprecedentedly growing interest towards a
wide range of aspects (e.g., theories, applications, metrics and
implementations) in both academia and industry. In this work, we primarily aim
to provide a comprehensive survey on both the background and research taxonomy,
as well as a detailed technical tutorial. Specifically, we start by reviewing
the literature and answering the "what" and "why" questions in semantic
transmissions. Afterwards, we present the ecosystems of SemCom, including
history, theories, metrics, datasets and toolkits, on top of which the taxonomy
for research directions is presented. Furthermore, we propose to categorize the
critical enabling techniques by explicit and implicit reasoning-based methods,
and elaborate on how they evolve and contribute to modern content &amp; channel
semantics-empowered communications. Besides reviewing and summarizing the
latest efforts in SemCom, we discuss the relations with other communication
levels (e.g., conventional communications) from a holistic and unified
viewpoint. Subsequently, in order to facilitate future developments and
industrial applications, we also highlight advanced practical techniques for
boosting semantic accuracy, robustness, and large-scale scalability, just to
mention a few. Finally, we discuss the technical challenges that shed light on
future research opportunities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.13020">Track Before Detect of Low SNR Objects in a Sequence of Image Frames Using Particle Filter. (arXiv:2212.13020v4 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Rezaie_R/0/1/0/all/0/1">Reza Rezaie</a></p>
<p>A multiple model track-before-detect (TBD) particle filter-based approach for
detection and tracking of low signal to noise ratio (SNR) objects based on a
sequence of image frames in the presence of noise and clutter is briefly
studied in this letter. At each time instance after receiving a frame of image,
first, some preprocessing approaches are applied to the image. Then, it is sent
to the multiple model TBD particle filter for detection and tracking of an
object. Performance of the approach is evaluated for detection and tracking of
an object in different scenarios including noise and clutter.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.01590">FATE in AI: Towards Algorithmic Inclusivity and Accessibility. (arXiv:2301.01590v2 [cs.CY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Inuwa_Dutse_I/0/1/0/all/0/1">Isa Inuwa-Dutse</a></p>
<p>Artificial Intelligence (AI) is at the forefront of modern technology, and
its effects are felt in many areas of society. To prevent algorithmic
disparities, fairness, accountability, transparency, and ethics (FATE) in AI
are being implemented. However, the current discourse on these issues is
largely dominated by more economically developed countries (MEDC), leaving out
local knowledge, cultural pluralism, and global fairness. This study aims to
address this gap by examining FATE-related desiderata, particularly
transparency and ethics, in areas of the global South that are underserved by
AI. A user study (n=43) and a participatory session (n=30) were conducted to
achieve this goal. The results showed that AI models can encode bias and
amplify stereotypes. To promote inclusivity, a community-led strategy is
proposed to collect and curate representative data for responsible AI design.
This will enable the affected community or individuals to monitor the
increasing use of AI-powered systems. Additionally, recommendations based on
public input are provided to ensure that AI adheres to social values and
context-specific FATE needs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.07085">Are Language Models Worse than Humans at Following Prompts? It&#x27;s Complicated. (arXiv:2301.07085v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Webson_A/0/1/0/all/0/1">Albert Webson</a>, <a href="http://arxiv.org/find/cs/1/au:+Loo_A/0/1/0/all/0/1">Alyssa Marie Loo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1">Qinan Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1">Ellie Pavlick</a></p>
<p>Prompts have been the center of progress in advancing language models'
zero-shot and few-shot performance. However, recent work finds that models can
perform surprisingly well when given intentionally irrelevant or misleading
prompts. Such results may be interpreted as evidence that model behavior is not
"human like". In this study, we challenge a central assumption in such work:
that humans would perform badly when given pathological instructions. We find
that humans are able to reliably ignore irrelevant instructions and thus, like
models, perform well on the underlying task despite an apparent lack of signal
regarding the task they are being asked to do. However, when given deliberately
misleading instructions, humans follow the instructions faithfully, whereas
models do not. Our findings caution that future research should not idealize
human behaviors as a monolith and should not train or evaluate models to mimic
assumptions about these behaviors without first validating humans' behaviors
empirically.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.11975">Byte Pair Encoding for Symbolic Music. (arXiv:2301.11975v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fradet_N/0/1/0/all/0/1">Nathan Fradet</a>, <a href="http://arxiv.org/find/cs/1/au:+Gutowski_N/0/1/0/all/0/1">Nicolas Gutowski</a>, <a href="http://arxiv.org/find/cs/1/au:+Chhel_F/0/1/0/all/0/1">Fabien Chhel</a>, <a href="http://arxiv.org/find/cs/1/au:+Briot_J/0/1/0/all/0/1">Jean-Pierre Briot</a></p>
<p>When used with deep learning, the symbolic music modality is often coupled
with language model architectures. To do so, the music needs to be tokenized,
i.e. converted into a sequence of discrete tokens. This can be achieved by
different approaches, as music can be composed of simultaneous tracks, of
simultaneous notes with several attributes. Until now, the proposed
tokenizations rely on small vocabularies of tokens describing the note
attributes and time events, resulting in fairly long token sequences, and a
sub-optimal use of the embedding space of language models. Recent research has
put efforts on reducing the overall sequence length by merging embeddings or
combining tokens. In this paper, we show that Byte Pair Encoding, a compression
technique widely used for natural language, significantly decreases the
sequence length while increasing the vocabulary size. By doing so, we leverage
the embedding capabilities of such models with more expressive tokens,
resulting in both better results and faster inference in generation and
classification tasks. The source code is shared on Github, along with a
companion website. Finally, BPE is directly implemented in MidiTok, allowing
the reader to easily benefit from this method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.13439">Navigating the Grey Area: How Expressions of Uncertainty and Overconfidence Affect Language Models. (arXiv:2302.13439v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1">Kaitlyn Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1">Dan Jurafsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1">Tatsunori Hashimoto</a></p>
<p>The increased deployment of LMs for real-world tasks involving knowledge and
facts makes it important to understand model epistemology: what LMs think they
know, and how their attitudes toward that knowledge are affected by language
use in their inputs. Here, we study an aspect of model epistemology: how
epistemic markers of certainty, uncertainty, or evidentiality like "I'm sure
it's", "I think it's", or "Wikipedia says it's" affect models, and whether they
contribute to model failures. We develop a typology of epistemic markers and
inject 50 markers into prompts for question answering. We find that LMs are
highly sensitive to epistemic markers in prompts, with accuracies varying more
than 80%. Surprisingly, we find that expressions of high certainty result in a
7% decrease in accuracy as compared to low certainty expressions; similarly,
factive verbs hurt performance, while evidentials benefit performance. Our
analysis of a popular pretraining dataset shows that these markers of
uncertainty are associated with answers on question-answering websites, while
markers of certainty are associated with questions. These associations may
suggest that the behavior of LMs is based on mimicking observed language use,
rather than truly reflecting epistemic uncertainty.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.13608">DeepSeq: Deep Sequential Circuit Learning. (arXiv:2302.13608v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1">Sadaf Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1">Zhengyuan Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Min Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1">Qiang Xu</a></p>
<p>Circuit representation learning is a promising research direction in the
electronic design automation (EDA) field. With sufficient data for
pre-training, the learned general yet effective representation can help to
solve multiple downstream EDA tasks by fine-tuning it on a small set of
task-related data. However, existing solutions only target combinational
circuits, significantly limiting their applications. In this work, we propose
DeepSeq, a novel representation learning framework for sequential netlists.
Specifically, we introduce a dedicated graph neural network (GNN) with a
customized propagation scheme to exploit the temporal correlations between
gates in sequential circuits. To ensure effective learning, we propose to use a
multi-task training objective with two sets of strongly related supervision:
logic probability and transition probability at each node. A novel dual
attention aggregation mechanism is introduced to facilitate learning both tasks
efficiently. Experimental results on various benchmark circuits show that
DeepSeq outperforms other GNN models for sequential circuit learning. We
evaluate the generalization capability of DeepSeq on a downstream power
estimation task. After fine-tuning, DeepSeq can accurately estimate power
across various circuits under different workloads.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.09447">Steering Prototypes with Prompt-tuning for Rehearsal-free Continual Learning. (arXiv:2303.09447v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhuowei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1">Long Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zizhao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Han Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1">Di Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Ting Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1">Dimitris N. Metaxas</a></p>
<p>In the context of continual learning, prototypes-as representative class
embeddings-offer advantages in memory conservation and the mitigation of
catastrophic forgetting. However, challenges related to semantic drift and
prototype interference persist. In this study, we introduce the Contrastive
Prototypical Prompt (CPP) approach. Through task-specific prompt-tuning,
underpinned by a contrastive learning objective, we effectively address both
aforementioned challenges. Our evaluations on four challenging
class-incremental benchmarks reveal that CPP achieves a significant 4% to 6%
improvement over state-of-the-art methods. Importantly, CPP operates without a
rehearsal buffer and narrows the performance divergence between continual and
offline joint-learning, suggesting an innovative scheme for Transformer-based
continual learning systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.10180">Towards Real-World Applications of Personalized Anesthesia Using Policy Constraint Q Learning for Propofol Infusion Control. (arXiv:2303.10180v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1">Xiuding Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yaoyao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Beimin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1">Yu Yao</a></p>
<p>Automated anesthesia promises to enable more precise and personalized
anesthetic administration and free anesthesiologists from repetitive tasks,
allowing them to focus on the most critical aspects of a patient's surgical
care. Current research has typically focused on creating simulated environments
from which agents can learn. These approaches have demonstrated good
experimental results, but are still far from clinical application. In this
paper, Policy Constraint Q-Learning (PCQL), a data-driven reinforcement
learning algorithm for solving the problem of learning anesthesia strategies on
real clinical datasets, is proposed. Conservative Q-Learning was first
introduced to alleviate the problem of Q function overestimation in an offline
context. A policy constraint term is added to agent training to keep the policy
distribution of the agent and the anesthesiologist consistent to ensure safer
decisions made by the agent in anesthesia scenarios. The effectiveness of PCQL
was validated by extensive experiments on a real clinical anesthesia dataset.
Experimental results show that PCQL is predicted to achieve higher gains than
the baseline approach while maintaining good agreement with the reference dose
given by the anesthesiologist, using less total dose, and being more responsive
to the patient's vital signs. In addition, the confidence intervals of the
agent were investigated, which were able to cover most of the clinical
decisions of the anesthesiologist. Finally, an interpretable method, SHAP, was
used to analyze the contributing components of the model predictions to
increase the transparency of the model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.12816">From Wide to Deep: Dimension Lifting Network for Parameter-efficient Knowledge Graph Embedding. (arXiv:2303.12816v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cai_B/0/1/0/all/0/1">Borui Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1">Yong Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1">Longxiang Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1">Di Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">He Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1">Jiong Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Luan_T/0/1/0/all/0/1">Tom Luan</a></p>
<p>Knowledge graph embedding (KGE) that maps entities and relations into vector
representations is essential for downstream applications. Conventional KGE
methods require high-dimensional representations to learn the complex structure
of knowledge graph, but lead to oversized model parameters. Recent advances
reduce parameters by low-dimensional entity representations, while developing
techniques (e.g., knowledge distillation or reinvented representation forms) to
compensate for reduced dimension. However, such operations introduce
complicated computations and model designs that may not benefit large knowledge
graphs. To seek a simple strategy to improve the parameter efficiency of
conventional KGE models, we take inspiration from that deeper neural networks
require exponentially fewer parameters to achieve expressiveness comparable to
wider networks for compositional structures. We view all entity representations
as a single-layer embedding network, and conventional KGE methods that adopt
high-dimensional entity representations equal widening the embedding network to
gain expressiveness. To achieve parameter efficiency, we instead propose a
deeper embedding network for entity representations, i.e., a narrow entity
embedding layer plus a multi-layer dimension lifting network (LiftNet).
Experiments on three public datasets show that by integrating LiftNet, four
conventional KGE methods with 16-dimensional representations achieve comparable
link prediction accuracy as original models that adopt 512-dimensional
representations, saving 68.4% to 96.9% parameters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.13525">Forecasting Workload in Cloud Computing: Towards Uncertainty-Aware Predictions and Transfer Learning. (arXiv:2303.13525v2 [cs.DC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rossi_A/0/1/0/all/0/1">Andrea Rossi</a>, <a href="http://arxiv.org/find/cs/1/au:+Visentin_A/0/1/0/all/0/1">Andrea Visentin</a>, <a href="http://arxiv.org/find/cs/1/au:+Carraro_D/0/1/0/all/0/1">Diego Carraro</a>, <a href="http://arxiv.org/find/cs/1/au:+Prestwich_S/0/1/0/all/0/1">Steven Prestwich</a>, <a href="http://arxiv.org/find/cs/1/au:+Brown_K/0/1/0/all/0/1">Kenneth N. Brown</a></p>
<p>Predicting future resource demand in Cloud Computing is essential for
optimizing the trade-off between serving customers' requests efficiently and
minimizing the provisioning cost. Modelling prediction uncertainty is also
desirable to better inform the resource decision-making process, but research
in this field is under-investigated. In this paper, we propose univariate and
bivariate Bayesian deep learning models that provide predictions of future
workload demand and its uncertainty. We run extensive experiments on Google and
Alibaba clusters, where we first train our models with datasets from different
cloud providers and compare them with LSTM-based baselines. Results show that
modelling the uncertainty of predictions has a positive impact on performance,
especially on service level metrics, because uncertainty quantification can be
tailored to desired target service levels that are critical in cloud
applications. Moreover, we investigate whether our models benefit transfer
learning capabilities across different domains, i.e. dataset distributions.
Experiments on the same workload datasets reveal that acceptable transfer
learning performance can be achieved within the same provider (because
distributions are more similar). Also, domain knowledge does not transfer when
the source and target domains are very different (e.g. from different
providers), but this performance degradation can be mitigated by increasing the
training set size of the source domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.00477">Demonstration of InsightPilot: An LLM-Empowered Automated Data Exploration System. (arXiv:2304.00477v2 [cs.DB] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_P/0/1/0/all/0/1">Pingchuan Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1">Rui Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1">Shi Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dongmei Zhang</a></p>
<p>Exploring data is crucial in data analysis, as it helps users understand and
interpret the data more effectively. However, performing effective data
exploration requires in-depth knowledge of the dataset and expertise in data
analysis techniques. Not being familiar with either can create obstacles that
make the process time-consuming and overwhelming for data analysts. To address
this issue, we introduce InsightPilot, an LLM (Large Language Model)-based,
automated data exploration system designed to simplify the data exploration
process. InsightPilot automatically selects appropriate analysis intents, such
as understanding, summarizing, and explaining. Then, these analysis intents are
concretized by issuing corresponding intentional queries (IQueries) to create a
meaningful and coherent exploration sequence. In brief, an IQuery is an
abstraction and automation of data analysis operations, which mimics the
approach of data analysts and simplifies the exploration process for users. By
employing an LLM to iteratively collaborate with a state-of-the-art insight
engine via IQueries, InsightPilot is effective in analyzing real-world
datasets, enabling users to gain valuable insights through natural language
inquiries. We demonstrate the effectiveness of InsightPilot in a case study,
showing how it can help users gain valuable insights from their datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.11755">Visualization for Recommendation Explainability: A Survey and New Perspectives. (arXiv:2305.11755v2 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chatti_M/0/1/0/all/0/1">Mohamed Amine Chatti</a>, <a href="http://arxiv.org/find/cs/1/au:+Guesmi_M/0/1/0/all/0/1">Mouadh Guesmi</a>, <a href="http://arxiv.org/find/cs/1/au:+Muslim_A/0/1/0/all/0/1">Arham Muslim</a></p>
<p>Providing system-generated explanations for recommendations represents an
important step towards transparent and trustworthy recommender systems.
Explainable recommender systems provide a human-understandable rationale for
their outputs. Over the last two decades, explainable recommendation has
attracted much attention in the recommender systems research community. This
paper aims to provide a comprehensive review of research efforts on visual
explanation in recommender systems. More concretely, we systematically review
the literature on explanations in recommender systems based on four dimensions,
namely explanation goal, explanation scope, explanation style, and explanation
format. Recognizing the importance of visualization, we approach the
recommender system literature from the angle of explanatory visualizations,
that is using visualizations as a display style of explanation. As a result, we
derive a set of guidelines that might be constructive for designing explanatory
visualizations in recommender systems and identify perspectives for future work
in this field. The aim of this review is to help recommendation researchers and
practitioners better understand the potential of visually explainable
recommendation research and to support them in the systematic design of visual
explanations in current and future recommender systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.12751">Testing of Deep Reinforcement Learning Agents with Surrogate Models. (arXiv:2305.12751v2 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Biagiola_M/0/1/0/all/0/1">Matteo Biagiola</a>, <a href="http://arxiv.org/find/cs/1/au:+Tonella_P/0/1/0/all/0/1">Paolo Tonella</a></p>
<p>Deep Reinforcement Learning (DRL) has received a lot of attention from the
research community in recent years. As the technology moves away from game
playing to practical contexts, such as autonomous vehicles and robotics, it is
crucial to evaluate the quality of DRL agents. In this paper, we propose a
search-based approach to test such agents. Our approach, implemented in a tool
called Indago, trains a classifier on failure and non-failure environment
(i.e., pass) configurations resulting from the DRL training process. The
classifier is used at testing time as a surrogate model for the DRL agent
execution in the environment, predicting the extent to which a given
environment configuration induces a failure of the DRL agent under test. The
failure prediction acts as a fitness function, guiding the generation towards
failure environment configurations, while saving computation time by deferring
the execution of the DRL agent in the environment to those configurations that
are more likely to expose failures. Experimental results show that our
search-based approach finds 50% more failures of the DRL agent than
state-of-the-art techniques. Moreover, such failures are, on average, 78% more
diverse; similarly, the behaviors of the DRL agent induced by failure
configurations are 74% more diverse.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15047">Ghostbuster: Detecting Text Ghostwritten by Large Language Models. (arXiv:2305.15047v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Verma_V/0/1/0/all/0/1">Vivek Verma</a>, <a href="http://arxiv.org/find/cs/1/au:+Fleisig_E/0/1/0/all/0/1">Eve Fleisig</a>, <a href="http://arxiv.org/find/cs/1/au:+Tomlin_N/0/1/0/all/0/1">Nicholas Tomlin</a>, <a href="http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1">Dan Klein</a></p>
<p>We introduce Ghostbuster, a state-of-the-art system for detecting
AI-generated text. Our method works by passing documents through a series of
weaker language models, running a structured search over possible combinations
of their features, and then training a classifier on the selected features to
predict whether documents are AI-generated. Crucially, Ghostbuster does not
require access to token probabilities from the target model, making it useful
for detecting text generated by black-box models or unknown model versions. In
conjunction with our model, we release three new datasets of human- and
AI-generated text as detection benchmarks in the domains of student essays,
creative writing, and news articles. We compare Ghostbuster to a variety of
existing detectors, including DetectGPT and GPTZero, as well as a new RoBERTa
baseline. Ghostbuster achieves 99.0 F1 when evaluated across domains, which is
5.9 F1 higher than the best preexisting model. It also outperforms all previous
approaches in generalization across writing domains (+7.5 F1), prompting
strategies (+2.1 F1), and language models (+4.4 F1). We also analyze the
robustness of our system to a variety of perturbations and paraphrasing attacks
and evaluate its performance on documents written by non-native English
speakers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19452">Bigger, Better, Faster: Human-level Atari with human-level efficiency. (arXiv:2305.19452v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schwarzer_M/0/1/0/all/0/1">Max Schwarzer</a>, <a href="http://arxiv.org/find/cs/1/au:+Obando_Ceron_J/0/1/0/all/0/1">Johan Obando-Ceron</a>, <a href="http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1">Aaron Courville</a>, <a href="http://arxiv.org/find/cs/1/au:+Bellemare_M/0/1/0/all/0/1">Marc Bellemare</a>, <a href="http://arxiv.org/find/cs/1/au:+Agarwal_R/0/1/0/all/0/1">Rishabh Agarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Castro_P/0/1/0/all/0/1">Pablo Samuel Castro</a></p>
<p>We introduce a value-based RL agent, which we call BBF, that achieves
super-human performance in the Atari 100K benchmark. BBF relies on scaling the
neural networks used for value estimation, as well as a number of other design
choices that enable this scaling in a sample-efficient manner. We conduct
extensive analyses of these design choices and provide insights for future
work. We end with a discussion about updating the goalposts for
sample-efficient RL research on the ALE. We make our code and data publicly
available at
https://github.com/google-research/google-research/tree/master/bigger_better_faster.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19915">Source Code Data Augmentation for Deep Learning: A Survey. (arXiv:2305.19915v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhuo_T/0/1/0/all/0/1">Terry Yue Zhuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhou Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1">Zhensu Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yufei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Li Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1">Xiaoning Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1">Zhenchang Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Lo_D/0/1/0/all/0/1">David Lo</a></p>
<p>The increasingly popular adoption of deep learning models in many critical
source code tasks motivates the development of data augmentation (DA)
techniques to enhance training data and improve various capabilities (e.g.,
robustness and generalizability) of these models. Although a series of DA
methods have been proposed and tailored for source code models, there lacks a
comprehensive survey and examination to understand their effectiveness and
implications. This paper fills this gap by conducting a comprehensive and
integrative survey of data augmentation for source code, wherein we
systematically compile and encapsulate existing literature to provide a
comprehensive overview of the field. We start with an introduction of data
augmentation in source code and then provide a discussion on major
representative approaches. Next, we highlight the general strategies and
techniques to optimize the DA quality. Subsequently, we underscore techniques
useful in real-world source code scenarios and downstream tasks. Finally, we
outline the prevailing challenges and potential opportunities for future
research. In essence, we aim to demystify the corpus of existing literature on
source code DA for deep learning, and foster further exploration in this
sphere. Complementing this, we present a continually updated GitHub repository
that hosts a list of update-to-date papers on DA for source code modeling,
accessible at \url{https://github.com/terryyz/DataAug4Code}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.04220">Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL. (arXiv:2306.04220v6 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1">Peng Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1">Xianyuan Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhihao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenjia Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1">Shoucheng Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Han Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Youfang Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1">Li Jiang</a></p>
<p>Offline reinforcement learning (RL) offers an appealing approach to
real-world tasks by learning policies from pre-collected datasets without
interacting with the environment. However, the performance of existing offline
RL algorithms heavily depends on the scale and state-action space coverage of
datasets. Real-world data collection is often expensive and uncontrollable,
leading to small and narrowly covered datasets and posing significant
challenges for practical deployments of offline RL. In this paper, we provide a
new insight that leveraging the fundamental symmetry of system dynamics can
substantially enhance offline RL performance under small datasets.
Specifically, we propose a Time-reversal symmetry (T-symmetry) enforced
Dynamics Model (TDM), which establishes consistency between a pair of forward
and reverse latent dynamics. TDM provides both well-behaved representations for
small datasets and a new reliability measure for OOD samples based on
compliance with the T-symmetry. These can be readily used to construct a new
offline RL algorithm (TSRL) with less conservative policy constraints and a
reliable latent space data augmentation procedure. Based on extensive
experiments, we find TSRL achieves great performance on small benchmark
datasets with as few as 1% of the original samples, which significantly
outperforms the recent offline RL algorithms in terms of data efficiency and
generalizability.Code is available at: https://github.com/pcheng2/TSRL
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.10065">Taming Diffusion Models for Music-driven Conducting Motion Generation. (arXiv:2306.10065v2 [eess.AS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Zhao_Z/0/1/0/all/0/1">Zhuoran Zhao</a>, <a href="http://arxiv.org/find/eess/1/au:+Bai_J/0/1/0/all/0/1">Jinbin Bai</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_D/0/1/0/all/0/1">Delong Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1">Debang Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Pan_Y/0/1/0/all/0/1">Yubo Pan</a></p>
<p>Generating the motion of orchestral conductors from a given piece of symphony
music is a challenging task since it requires a model to learn semantic music
features and capture the underlying distribution of real conducting motion.
Prior works have applied Generative Adversarial Networks (GAN) to this task,
but the promising diffusion model, which recently showed its advantages in
terms of both training stability and output quality, has not been exploited in
this context. This paper presents Diffusion-Conductor, a novel DDIM-based
approach for music-driven conducting motion generation, which integrates the
diffusion model to a two-stage learning framework. We further propose a random
masking strategy to improve the feature robustness, and use a pair of geometric
loss functions to impose additional regularizations and increase motion
diversity. We also design several novel metrics, including Frechet Gesture
Distance (FGD) and Beat Consistency Score (BC) for a more comprehensive
evaluation of the generated motion. Experimental results demonstrate the
advantages of our model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.11816">Learning to Generate Better Than Your LLM. (arXiv:2306.11816v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1">Jonathan D. Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Brantley_K/0/1/0/all/0/1">Kiante Brantley</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramamurthy_R/0/1/0/all/0/1">Rajkumar Ramamurthy</a>, <a href="http://arxiv.org/find/cs/1/au:+Misra_D/0/1/0/all/0/1">Dipendra Misra</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1">Wen Sun</a></p>
<p>Reinforcement learning (RL) has emerged as a powerful paradigm for
fine-tuning Large Language Models (LLMs) for text generation. In particular,
recent LLMs such as ChatGPT and GPT-4 can engage in fluent conversations with
users after finetuning with RL. Capitalizing on key properties of text
generation, we seek to investigate RL algorithms beyond general purpose
algorithms like Proximal Policy Optimization (PPO). In particular, we extend RL
algorithms to allow them to interact with a dynamic black-box guide LLM and
propose RL with guided feedback (RLGF), a suite of RL algorithms for LLM
fine-tuning. We provide two ways for the guide LLM to interact with the LLM to
be optimized for maximizing rewards. The guide LLM can generate text which
serves as additional starting states for the RL optimization procedure. The
guide LLM can also be used to complete the partial sentences generated by the
LLM that is being optimized, treating the guide LLM as an expert to imitate and
surpass eventually. We experiment on the IMDB positive sentiment, CommonGen,
and TL;DR summarization tasks. We show that our RL algorithms achieve higher
performance than supervised learning (SL) and the RL baseline PPO,
demonstrating the benefit of interaction with the guide LLM. On both CommonGen
and TL;DR, we not only outperform our SL baselines but also improve upon PPO
across a variety of metrics beyond the one we optimized for. Our code can be
found at https://github.com/Cornell-RL/tril.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.13761">CeBed: A Benchmark for Deep Data-Driven OFDM Channel Estimation. (arXiv:2306.13761v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feriani_A/0/1/0/all/0/1">Amal Feriani</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1">Di Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Steve Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dudek_G/0/1/0/all/0/1">Greg Dudek</a></p>
<p>Deep learning has been extensively used in wireless communication problems,
including channel estimation. Although several data-driven approaches exist, a
fair and realistic comparison between them is difficult due to inconsistencies
in the experimental conditions and the lack of a standardized experimental
design. In addition, the performance of data-driven approaches is often
compared based on empirical analysis. The lack of reproducibility and
availability of standardized evaluation tools (e.g., datasets, codebases)
hinder the development and progress of data-driven methods for channel
estimation and wireless communication in general. In this work, we introduce an
initiative to build benchmarks that unify several data-driven OFDM channel
estimation approaches. Specifically, we present CeBed (a testbed for channel
estimation) including different datasets covering various systems models and
propagation conditions along with the implementation of ten deep and
traditional baselines. This benchmark considers different practical aspects
such as the robustness of the data-driven models, the number and the
arrangement of pilots, and the number of receive antennas. This work offers a
comprehensive and unified framework to help researchers evaluate and design
data-driven channel estimation algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.13948">Unleashing Realistic Air Quality Forecasting: Introducing the Ready-to-Use PurpleAirSF Dataset. (arXiv:2306.13948v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zuo_J/0/1/0/all/0/1">Jingwei Zuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wenbin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Baldo_M/0/1/0/all/0/1">Michele Baldo</a>, <a href="http://arxiv.org/find/cs/1/au:+Hacid_H/0/1/0/all/0/1">Hakim Hacid</a></p>
<p>Air quality forecasting has garnered significant attention recently, with
data-driven models taking center stage due to advancements in machine learning
and deep learning models. However, researchers face challenges with complex
data acquisition and the lack of open-sourced datasets, hindering efficient
model validation. This paper introduces PurpleAirSF, a comprehensive and easily
accessible dataset collected from the PurpleAir network. With its high temporal
resolution, various air quality measures, and diverse geographical coverage,
this dataset serves as a useful tool for researchers aiming to develop novel
forecasting models, study air pollution patterns, and investigate their impacts
on health and the environment. We present a detailed account of the data
collection and processing methods employed to build PurpleAirSF. Furthermore,
we conduct preliminary experiments using both classic and modern
spatio-temporal forecasting models, thereby establishing a benchmark for future
air quality forecasting tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.16045">OpenNDD: Open Set Recognition for Neurodevelopmental Disorders Detection. (arXiv:2306.16045v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Jiaming Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_Z/0/1/0/all/0/1">Zihao Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1">Xinyue Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shujie Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1">Zhenshan Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiumei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Changcai Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1">Riqing Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1">Lanyan Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1">Lifang Wei</a></p>
<p>Since the strong comorbid similarity in NDDs, such as attention-deficit
hyperactivity disorder, can interfere with the accurate diagnosis of autism
spectrum disorder (ASD), identifying unknown classes is extremely crucial and
challenging from NDDs. We design a novel open set recognition framework for
ASD-aided diagnosis (OpenNDD), which trains a model by combining autoencoder
and adversarial reciprocal points learning to distinguish in-distribution and
out-of-distribution categories as well as identify ASD accurately. Considering
the strong similarities between NDDs, we present a joint scaling method by
Min-Max scaling combined with Standardization (MMS) to increase the differences
between classes for better distinguishing unknown NDDs. We conduct the
experiments in the hybrid datasets from Autism Brain Imaging Data Exchange I
(ABIDE I) and THE ADHD-200 SAMPLE (ADHD-200) with 791 samples from four sites
and the results demonstrate the superiority on various metrics. Our OpenNDD
achieves promising performance, where the accuracy is 77.38%, AUROC is 75.53%
and the open set classification rate is as high as 59.43%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.17010">milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing. (arXiv:2306.17010v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1">Fangqiang Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1">Zhen Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1">Peijun Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1">Chris Xiaoxuan Lu</a></p>
<p>Approaching the era of ubiquitous computing, human motion sensing plays a
crucial role in smart systems for decision making, user interaction, and
personalized services. Extensive research has been conducted on human tracking,
pose estimation, gesture recognition, and activity recognition, which are
predominantly based on cameras in traditional methods. However, the intrusive
nature of cameras limits their use in smart home applications. To address this,
mmWave radars have gained popularity due to their privacy-friendly features. In
this work, we propose milliFlow, a novel deep learning method for scene flow
estimation as a complementary motion information for mmWave point cloud,
serving as an intermediate level of features and directly benefiting downstream
human motion sensing tasks. Experimental results demonstrate the superior
performance of our method with an average 3D endpoint error of 4.6cm,
significantly surpassing the competing approaches. Furthermore, by
incorporating scene flow information, we achieve remarkable improvements in
human activity recognition, human parsing, and human body part tracking. To
foster further research in this area, we will provide our codebase and dataset
for open access upon acceptance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01231">A Critical Re-evaluation of Benchmark Datasets for (Deep) Learning-Based Matching Algorithms. (arXiv:2307.01231v2 [cs.DB] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Papadakis_G/0/1/0/all/0/1">George Papadakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Kirielle_N/0/1/0/all/0/1">Nishadi Kirielle</a>, <a href="http://arxiv.org/find/cs/1/au:+Christen_P/0/1/0/all/0/1">Peter Christen</a>, <a href="http://arxiv.org/find/cs/1/au:+Palpanas_T/0/1/0/all/0/1">Themis Palpanas</a></p>
<p>Entity resolution (ER) is the process of identifying records that refer to
the same entities within one or across multiple databases. Numerous techniques
have been developed to tackle ER challenges over the years, with recent
emphasis placed on machine and deep learning methods for the matching phase.
However, the quality of the benchmark datasets typically used in the
experimental evaluations of learning-based matching algorithms has not been
examined in the literature. To cover this gap, we propose four different
approaches to assessing the difficulty and appropriateness of 13 established
datasets: two theoretical approaches, which involve new measures of linearity
and existing measures of complexity, and two practical approaches: the
difference between the best non-linear and linear matchers, as well as the
difference between the best learning-based matcher and the perfect oracle. Our
analysis demonstrates that most of the popular datasets pose rather easy
classification tasks. As a result, they are not suitable for properly
evaluating learning-based matching algorithms. To address this issue, we
propose a new methodology for yielding benchmark datasets. We put it into
practice by creating four new matching tasks, and we verify that these new
benchmarks are more challenging and therefore more suitable for further
advancements in the field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04349">RLTF: Reinforcement Learning from Unit Test Feedback. (arXiv:2307.04349v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiate Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yiqin Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_K/0/1/0/all/0/1">Kaiwen Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1">Qiang Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1">Xiao Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1">Wei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_D/0/1/0/all/0/1">Deheng Ye</a></p>
<p>The goal of program synthesis, or code generation, is to generate executable
code based on given descriptions. Recently, there has been an increasing number
of studies employing reinforcement learning (RL) to improve the performance of
large language models (LLMs) for code. However, current representative works
either rely solely on offline frameworks, limiting the exploration of new
sample spaces, or fall short in the utilization of unit test signals, not
accounting for specific error locations within the code. To address these
issues, we propose RLTF, i.e., Reinforcement Learning from Unit Test Feedback,
a novel online RL framework with unit test feedback of multi-granularity for
refining code LLMs. Our approach generates data in real-time during training
and simultaneously utilizes fine-grained feedback signals to guide the model
towards producing higher-quality code. Extensive experiments show that RLTF
achieves state-of-the-art performance on the APPS and the MBPP benchmarks. Our
code is available at: https://github.com/Zyq-scut/RLTF.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06440">No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models. (arXiv:2307.06440v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kaddour_J/0/1/0/all/0/1">Jean Kaddour</a>, <a href="http://arxiv.org/find/cs/1/au:+Key_O/0/1/0/all/0/1">Oscar Key</a>, <a href="http://arxiv.org/find/cs/1/au:+Nawrot_P/0/1/0/all/0/1">Piotr Nawrot</a>, <a href="http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1">Pasquale Minervini</a>, <a href="http://arxiv.org/find/cs/1/au:+Kusner_M/0/1/0/all/0/1">Matt J. Kusner</a></p>
<p>The computation necessary for training Transformer-based language models has
skyrocketed in recent years. This trend has motivated research on efficient
training algorithms designed to improve training, validation, and downstream
performance faster than standard training. In this work, we revisit three
categories of such algorithms: dynamic architectures (layer stacking, layer
dropping), batch selection (selective backprop, RHO loss), and efficient
optimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed
computation budget using such methods, we find that their training, validation,
and downstream gains vanish compared to a baseline with a fully-decayed
learning rate. We define an evaluation protocol that enables computation to be
done on arbitrary machines by mapping all computation time to a reference
machine which we call reference system time. We discuss the limitations of our
proposed protocol and release our code to encourage rigorous research in
efficient training procedures: https://github.com/JeanKaddour/NoTrainNoGain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08286">Going Beyond Linear Mode Connectivity: The Layerwise Linear Feature Connectivity. (arXiv:2307.08286v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhanpeng Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yongyi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xiaojiang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1">Junchi Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1">Wei Hu</a></p>
<p>Recent work has revealed many intriguing empirical phenomena in neural
network training, despite the poorly understood and highly complex loss
landscapes and training dynamics. One of these phenomena, Linear Mode
Connectivity (LMC), has gained considerable attention due to the intriguing
observation that different solutions can be connected by a linear path in the
parameter space while maintaining near-constant training and test losses. In
this work, we introduce a stronger notion of linear connectivity, Layerwise
Linear Feature Connectivity (LLFC), which says that the feature maps of every
layer in different trained networks are also linearly connected. We provide
comprehensive empirical evidence for LLFC across a wide range of settings,
demonstrating that whenever two trained networks satisfy LMC (via either
spawning or permutation methods), they also satisfy LLFC in nearly all the
layers. Furthermore, we delve deeper into the underlying factors contributing
to LLFC, which reveal new insights into the spawning and permutation
approaches. The study of LLFC transcends and advances our understanding of LMC
by adopting a feature-learning perspective.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10317">FedBug: A Bottom-Up Gradual Unfreezing Framework for Federated Learning. (arXiv:2307.10317v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kao_C/0/1/0/all/0/1">Chia-Hsiang Kao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu-Chiang Frank Wang</a></p>
<p>Federated Learning (FL) offers a collaborative training framework, allowing
multiple clients to contribute to a shared model without compromising data
privacy. Due to the heterogeneous nature of local datasets, updated client
models may overfit and diverge from one another, commonly known as the problem
of client drift. In this paper, we propose FedBug (Federated Learning with
Bottom-Up Gradual Unfreezing), a novel FL framework designed to effectively
mitigate client drift. FedBug adaptively leverages the client model parameters,
distributed by the server at each global round, as the reference points for
cross-client alignment. Specifically, on the client side, FedBug begins by
freezing the entire model, then gradually unfreezes the layers, from the input
layer to the output layer. This bottom-up approach allows models to train the
newly thawed layers to project data into a latent space, wherein the separating
hyperplanes remain consistent across all clients. We theoretically analyze
FedBug in a novel over-parameterization FL setup, revealing its superior
convergence rate compared to FedAvg. Through comprehensive experiments,
spanning various datasets, training conditions, and network architectures, we
validate the efficacy of FedBug. Our contributions encompass a novel FL
framework, theoretical analysis, and empirical validation, demonstrating the
wide potential and applicability of FedBug.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10455">A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset. (arXiv:2307.10455v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gharaee_Z/0/1/0/all/0/1">Zahra Gharaee</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1">ZeMing Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Pellegrino_N/0/1/0/all/0/1">Nicholas Pellegrino</a>, <a href="http://arxiv.org/find/cs/1/au:+Zarubiieva_I/0/1/0/all/0/1">Iuliia Zarubiieva</a>, <a href="http://arxiv.org/find/cs/1/au:+Haurum_J/0/1/0/all/0/1">Joakim Bruslund Haurum</a>, <a href="http://arxiv.org/find/cs/1/au:+Lowe_S/0/1/0/all/0/1">Scott C. Lowe</a>, <a href="http://arxiv.org/find/cs/1/au:+McKeown_J/0/1/0/all/0/1">Jaclyn T.A. McKeown</a>, <a href="http://arxiv.org/find/cs/1/au:+Ho_C/0/1/0/all/0/1">Chris C.Y. Ho</a>, <a href="http://arxiv.org/find/cs/1/au:+McLeod_J/0/1/0/all/0/1">Joschka McLeod</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1">Yi-Yun C Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Agda_J/0/1/0/all/0/1">Jireh Agda</a>, <a href="http://arxiv.org/find/cs/1/au:+Ratnasingham_S/0/1/0/all/0/1">Sujeevan Ratnasingham</a>, <a href="http://arxiv.org/find/cs/1/au:+Steinke_D/0/1/0/all/0/1">Dirk Steinke</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_A/0/1/0/all/0/1">Angel X. Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Taylor_G/0/1/0/all/0/1">Graham W. Taylor</a>, <a href="http://arxiv.org/find/cs/1/au:+Fieguth_P/0/1/0/all/0/1">Paul Fieguth</a></p>
<p>In an effort to catalog insect biodiversity, we propose a new large dataset
of hand-labelled insect images, the BIOSCAN-Insect Dataset. Each record is
taxonomically classified by an expert, and also has associated genetic
information including raw nucleotide barcode sequences and assigned barcode
index numbers, which are genetically-based proxies for species classification.
This paper presents a curated million-image dataset, primarily to train
computer-vision models capable of providing image-based taxonomic assessment,
however, the dataset also presents compelling characteristics, the study of
which would be of interest to the broader machine learning community. Driven by
the biological nature inherent to the dataset, a characteristic long-tailed
class-imbalance distribution is exhibited. Furthermore, taxonomic labelling is
a hierarchical classification scheme, presenting a highly fine-grained
classification problem at lower levels. Beyond spurring interest in
biodiversity research within the machine learning community, progress on
creating an image-based taxonomic classifier will also further the ultimate
goal of all BIOSCAN research: to lay the foundation for a comprehensive survey
of global biodiversity. This paper introduces the dataset and explores the
classification task through the implementation and analysis of a baseline
classifier.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.11760">Large Language Models Understand and Can be Enhanced by Emotional Stimuli. (arXiv:2307.11760v7 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Cheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jindong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yixuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1">Kaijie Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_W/0/1/0/all/0/1">Wenxin Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Lian_J/0/1/0/all/0/1">Jianxun Lian</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1">Fang Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1">Qiang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xing Xie</a></p>
<p>Emotional intelligence significantly impacts our daily behaviors and
interactions. Although Large Language Models (LLMs) are increasingly viewed as
a stride toward artificial general intelligence, exhibiting impressive
performance in numerous tasks, it is still uncertain if LLMs can genuinely
grasp psychological emotional stimuli. Understanding and responding to
emotional cues gives humans a distinct advantage in problem-solving. In this
paper, we take the first step towards exploring the ability of LLMs to
understand emotional stimuli. To this end, we first conduct automatic
experiments on 45 tasks using various LLMs, including Flan-T5-Large, Vicuna,
Llama 2, BLOOM, ChatGPT, and GPT-4. Our tasks span deterministic and generative
applications that represent comprehensive evaluation scenarios. Our automatic
experiments show that LLMs have a grasp of emotional intelligence, and their
performance can be improved with emotional prompts (which we call
"EmotionPrompt" that combines the original prompt with emotional stimuli),
e.g., 8.00% relative performance improvement in Instruction Induction and 115%
in BIG-Bench. In addition to those deterministic tasks that can be
automatically evaluated using existing metrics, we conducted a human study with
106 participants to assess the quality of generative tasks using both vanilla
and emotional prompts. Our human study results demonstrate that EmotionPrompt
significantly boosts the performance of generative tasks (10.9% average
improvement in terms of performance, truthfulness, and responsibility metrics).
We provide an in-depth discussion regarding why EmotionPrompt works for LLMs
and the factors that may influence its performance. We posit that EmotionPrompt
heralds a novel avenue for exploring interdisciplinary knowledge for human-LLMs
interaction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.12166">The Imitation Game: Detecting Human and AI-Generated Texts in the Era of ChatGPT and BARD. (arXiv:2307.12166v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hayawi_K/0/1/0/all/0/1">Kadhim Hayawi</a>, <a href="http://arxiv.org/find/cs/1/au:+Shahriar_S/0/1/0/all/0/1">Sakib Shahriar</a>, <a href="http://arxiv.org/find/cs/1/au:+Mathew_S/0/1/0/all/0/1">Sujith Samuel Mathew</a></p>
<p>The potential of artificial intelligence (AI)-based large language models
(LLMs) holds considerable promise in revolutionizing education, research, and
practice. However, distinguishing between human-written and AI-generated text
has become a significant task. This paper presents a comparative study,
introducing a novel dataset of human-written and LLM-generated texts in
different genres: essays, stories, poetry, and Python code. We employ several
machine learning models to classify the texts. Results demonstrate the efficacy
of these models in discerning between human and AI-generated text, despite the
dataset's limited sample size. However, the task becomes more challenging when
classifying GPT-generated text, particularly in story writing. The results
indicate that the models exhibit superior performance in binary classification
tasks, such as distinguishing human-generated text from a specific LLM,
compared to the more complex multiclass tasks that involve discerning among
human-generated and multiple LLMs. Our findings provide insightful implications
for AI text detection while our dataset paves the way for future research in
this evolving area.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.12306">Tackling the Curse of Dimensionality with Physics-Informed Neural Networks. (arXiv:2307.12306v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1">Zheyuan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shukla_K/0/1/0/all/0/1">Khemraj Shukla</a>, <a href="http://arxiv.org/find/cs/1/au:+Karniadakis_G/0/1/0/all/0/1">George Em Karniadakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1">Kenji Kawaguchi</a></p>
<p>The curse-of-dimensionality taxes computational resources heavily with
exponentially increasing computational cost as the dimension increases. This
poses great challenges in solving high-dimensional PDEs, as Richard E. Bellman
first pointed out over 60 years ago. While there has been some recent success
in solving numerically partial differential equations (PDEs) in high
dimensions, such computations are prohibitively expensive, and true scaling of
general nonlinear PDEs to high dimensions has never been achieved. We develop a
new method of scaling up physics-informed neural networks (PINNs) to solve
arbitrary high-dimensional PDEs. The new method, called Stochastic Dimension
Gradient Descent (SDGD), decomposes a gradient of PDEs into pieces
corresponding to different dimensions and randomly samples a subset of these
dimensional pieces in each iteration of training PINNs. We prove theoretically
the convergence and other desired properties of the proposed method. We
demonstrate in various diverse tests that the proposed method can solve many
notoriously hard high-dimensional PDEs, including the Hamilton-Jacobi-Bellman
(HJB) and the Schr\"{o}dinger equations in tens of thousands of dimensions very
fast on a single GPU using the PINNs mesh-free approach. Notably, we solve
nonlinear PDEs with nontrivial, anisotropic, and inseparable solutions in
100,000 effective dimensions in 12 hours on a single GPU using SDGD with PINNs.
Since SDGD is a general training methodology of PINNs, it can be applied to any
current and future variants of PINNs to scale them up for arbitrary
high-dimensional PDEs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.12897">Anytime Model Selection in Linear Bandits. (arXiv:2307.12897v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Kassraie_P/0/1/0/all/0/1">Parnian Kassraie</a>, <a href="http://arxiv.org/find/stat/1/au:+Emmenegger_N/0/1/0/all/0/1">Nicolas Emmenegger</a>, <a href="http://arxiv.org/find/stat/1/au:+Krause_A/0/1/0/all/0/1">Andreas Krause</a>, <a href="http://arxiv.org/find/stat/1/au:+Pacchiano_A/0/1/0/all/0/1">Aldo Pacchiano</a></p>
<p>Model selection in the context of bandit optimization is a challenging
problem, as it requires balancing exploration and exploitation not only for
action selection, but also for model selection. One natural approach is to rely
on online learning algorithms that treat different models as experts. Existing
methods, however, scale poorly ($\text{poly}M$) with the number of models $M$
in terms of their regret. Our key insight is that, for model selection in
linear bandits, we can emulate full-information feedback to the online learner
with a favorable bias-variance trade-off. This allows us to develop ALEXP,
which has an exponentially improved ($\log M$) dependence on $M$ for its
regret. ALEXP has anytime guarantees on its regret, and neither requires
knowledge of the horizon $n$, nor relies on an initial purely exploratory
stage. Our approach utilizes a novel time-uniform analysis of the Lasso,
establishing a new connection between online learning and high-dimensional
statistics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.16833">Data Augmentation for Neural Machine Translation using Generative Language Model. (arXiv:2307.16833v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1">Seokjin Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Su Ah Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_W/0/1/0/all/0/1">Woohwan Jung</a></p>
<p>Despite the rapid growth in model architecture, the scarcity of large
parallel corpora remains the main bottleneck in Neural Machine Translation.
Data augmentation is a technique that enhances the performance of data-hungry
models by generating synthetic data instead of collecting new ones. We explore
prompt-based data augmentation approaches that leverage large-scale language
models such as ChatGPT. To create a synthetic parallel corpus, we compare 3
methods using different prompts. We employ two assessment metrics to measure
the diversity of the generated synthetic data. This approach requires no
further model training cost, which is mandatory in other augmentation methods
like back-translation. The proposed method improves the unaugmented baseline by
0.68 BLEU score.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04586">Bootstrapping Developmental AIs: From Simple Competences to Intelligent Human-Compatible AIs. (arXiv:2308.04586v12 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stefik_M/0/1/0/all/0/1">Mark Stefik</a>, <a href="http://arxiv.org/find/cs/1/au:+Price_R/0/1/0/all/0/1">Robert Price</a></p>
<p>The mainstream AI approaches include the generative and deep learning
approaches with large language models (LLMs) and the manually constructed
symbolic approach. These approaches have led to valuable AI systems and
impressive feats. However, manually constructed AIs are brittle even in
circumscribed domains. Generative AIs make strange mistakes and do not notice
them. In these approaches AIs cannot be instructed easily, fail to use common
sense, and lack curiosity. They have abstract knowledge but lack social
alignment. Developmental AIs start with innate competences, interact with their
environment, and learn from their interactions. They interact, learn from
people, and establish perceptual, cognitive, and common grounding. The
bootstrapping approach tracks a competence trajectory where the competences of
intelligence are developed in small steps in parallel by embodied AIs.
Developmental AIs have capabilities for multimodal perception, object
recognition, and manipulation. Powerful computational models for hierarchical
planning, abstraction discovery, curiosity, and language acquisition exist but
need to be adapted to a developmental learning based approach. The promise is
that this research will ultimately produce AIs that learn to communicate, read
critically, consider the provenance of information, test hypotheses, and
collaborate. However, developmental AI projects have not yet passed the
abilities of young children. The research needs to bridge competence gaps
involving nonverbal communication, speech, reading, and writing. This position
paper lays out prospects, gaps, and challenges for AI and a developmental
approach. The goal is to create resilient, intelligent, and human-compatible
AIs. They would learn, share what they learn, and collaborate to achieve high
standards.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.07336">Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic. (arXiv:2308.07336v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Morishita_T/0/1/0/all/0/1">Terufumi Morishita</a>, <a href="http://arxiv.org/find/cs/1/au:+Morio_G/0/1/0/all/0/1">Gaku Morio</a>, <a href="http://arxiv.org/find/cs/1/au:+Yamaguchi_A/0/1/0/all/0/1">Atsuki Yamaguchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Sogawa_Y/0/1/0/all/0/1">Yasuhiro Sogawa</a></p>
<p>We study a synthetic corpus based approach for language models (LMs) to
acquire logical deductive reasoning ability. The previous studies generated
deduction examples using specific sets of deduction rules. However, these rules
were limited or otherwise arbitrary, limiting the generalizability of acquired
reasoning ability. We rethink this and adopt a well-grounded set of deduction
rules based on formal logic theory, which can derive any other deduction rules
when combined in a multistep way. Then, using the proposed corpora, which we
name FLD (Formal Logic Deduction), we first evaluate and analyze the logical
reasoning ability of the latest LLMs. Even GPT-4 can solve only half of the
problems, suggesting that pure logical reasoning isolated from knowledge is
still challenging for the LLMs, and additional training specialized in logical
reasoning is indeed essential. We next empirically verify that LMs trained on
FLD corpora acquire more generalizable reasoning ability. Furthermore, we
identify the aspects of reasoning ability on which deduction corpora can
enhance LMs and those on which they cannot, and discuss future directions on
each aspect. The released corpora serve both as learning resources and as
challenging benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.09289">Preference-conditioned Pixel-based AI Agent For Game Testing. (arXiv:2308.09289v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abdelfattah_S/0/1/0/all/0/1">Sherif Abdelfattah</a>, <a href="http://arxiv.org/find/cs/1/au:+Brown_A/0/1/0/all/0/1">Adrian Brown</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Pushi Zhang</a></p>
<p>The game industry is challenged to cope with increasing growth in demand and
game complexity while maintaining acceptable quality standards for released
games. Classic approaches solely depending on human efforts for quality
assurance and game testing do not scale effectively in terms of time and cost.
Game-testing AI agents that learn by interaction with the environment have the
potential to mitigate these challenges with good scalability properties on time
and costs. However, most recent work in this direction depends on game state
information for the agent's state representation, which limits generalization
across different game scenarios. Moreover, game test engineers usually prefer
exploring a game in a specific style, such as exploring the golden path.
However, current game testing AI agents do not provide an explicit way to
satisfy such a preference. This paper addresses these limitations by proposing
an agent design that mainly depends on pixel-based state observations while
exploring the environment conditioned on a user's preference specified by
demonstration trajectories. In addition, we propose an imitation learning
method that couples self-supervised and supervised learning objectives to
enhance the quality of imitation behaviors. Our agent significantly outperforms
state-of-the-art pixel-based game testing agents over exploration coverage and
test execution quality when evaluated on a complex open-world environment
resembling many aspects of real AAA games.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.13488">Temporal Uncertainty Localization to Enable Human-in-the-loop Analysis of Dynamic Contrast-enhanced Cardiac MRI Datasets. (arXiv:2308.13488v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Yalcinkaya_D/0/1/0/all/0/1">Dilek M. Yalcinkaya</a>, <a href="http://arxiv.org/find/eess/1/au:+Youssef_K/0/1/0/all/0/1">Khalid Youssef</a>, <a href="http://arxiv.org/find/eess/1/au:+Heydari_B/0/1/0/all/0/1">Bobak Heydari</a>, <a href="http://arxiv.org/find/eess/1/au:+Simonetti_O/0/1/0/all/0/1">Orlando Simonetti</a>, <a href="http://arxiv.org/find/eess/1/au:+Dharmakumar_R/0/1/0/all/0/1">Rohan Dharmakumar</a>, <a href="http://arxiv.org/find/eess/1/au:+Raman_S/0/1/0/all/0/1">Subha Raman</a>, <a href="http://arxiv.org/find/eess/1/au:+Sharif_B/0/1/0/all/0/1">Behzad Sharif</a></p>
<p>Dynamic contrast-enhanced (DCE) cardiac magnetic resonance imaging (CMRI) is
a widely used modality for diagnosing myocardial blood flow (perfusion)
abnormalities. During a typical free-breathing DCE-CMRI scan, close to 300
time-resolved images of myocardial perfusion are acquired at various contrast
"wash in/out" phases. Manual segmentation of myocardial contours in each
time-frame of a DCE image series can be tedious and time-consuming,
particularly when non-rigid motion correction has failed or is unavailable.
While deep neural networks (DNNs) have shown promise for analyzing DCE-CMRI
datasets, a "dynamic quality control" (dQC) technique for reliably detecting
failed segmentations is lacking. Here we propose a new space-time uncertainty
metric as a dQC tool for DNN-based segmentation of free-breathing DCE-CMRI
datasets by validating the proposed metric on an external dataset and
establishing a human-in-the-loop framework to improve the segmentation results.
In the proposed approach, we referred the top 10% most uncertain segmentations
as detected by our dQC tool to the human expert for refinement. This approach
resulted in a significant increase in the Dice score (p&lt;0.001) and a notable
decrease in the number of images with failed segmentation (16.2% to 11.3%)
whereas the alternative approach of randomly selecting the same number of
segmentations for human referral did not achieve any significant improvement.
Our results suggest that the proposed dQC framework has the potential to
accurately identify poor-quality segmentations and may enable efficient
DNN-based analysis of DCE-CMRI in a human-in-the-loop pipeline for clinical
interpretation and reporting of dynamic CMRI datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.14522">Graph Meets LLMs: Towards Large Graph Models. (arXiv:2308.14522v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Ziwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Haoyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zeyang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1">Yijian Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1">Wenwu Zhu</a></p>
<p>Large models have emerged as the most recent groundbreaking achievements in
artificial intelligence, and particularly machine learning. However, when it
comes to graphs, large models have not achieved the same level of success as in
other fields, such as natural language processing and computer vision. In order
to promote applying large models for graphs forward, we present a perspective
paper to discuss the challenges and opportunities associated with developing
large graph models. First, we discuss the desired characteristics of large
graph models. Then, we present detailed discussions from three key
perspectives: representation basis, graph data, and graph models. In each
category, we provide a brief overview of recent advances and highlight the
remaining challenges together with our visions. Finally, we discuss valuable
applications of large graph models. We believe this perspective can encourage
further investigations into large graph models, ultimately pushing us one step
closer towards artificial general intelligence (AGI). We are the first to
comprehensively study large graph models, to the best of our knowledge.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.15991">DRL-Based Trajectory Tracking for Motion-Related Modules in Autonomous Driving. (arXiv:2308.15991v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yinda Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1">Lidong Yu</a></p>
<p>Autonomous driving systems are always built on motion-related modules such as
the planner and the controller. An accurate and robust trajectory tracking
method is indispensable for these motion-related modules as a primitive
routine. Current methods often make strong assumptions about the model such as
the context and the dynamics, which are not robust enough to deal with the
changing scenarios in a real-world system. In this paper, we propose a Deep
Reinforcement Learning (DRL)-based trajectory tracking method for the
motion-related modules in autonomous driving systems. The representation
learning ability of DL and the exploration nature of RL bring strong robustness
and improve accuracy. Meanwhile, it enhances versatility by running the
trajectory tracking in a model-free and data-driven manner. Through extensive
experiments, we demonstrate both the efficiency and effectiveness of our method
compared to current methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.10987">SpikingNeRF: Making Bio-inspired Neural Networks See through the Real World. (arXiv:2309.10987v3 [cs.NE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1">Xingting Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1">Qinghao Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tielong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mo_Z/0/1/0/all/0/1">Zitao Mo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Zeyu Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuge_Z/0/1/0/all/0/1">Zhengyang Zhuge</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1">Jian Cheng</a></p>
<p>Spiking neural networks (SNNs) have been thriving on numerous tasks to
leverage their promising energy efficiency and exploit their potentialities as
biologically plausible intelligence. Meanwhile, the Neural Radiance Fields
(NeRF) render high-quality 3D scenes with massive energy consumption, but few
works delve into the energy-saving solution with a bio-inspired approach. In
this paper, we propose SpikingNeRF, which aligns the radiance ray with the
temporal dimension of SNN, to naturally accommodate the SNN to the
reconstruction of Radiance Fields. Thus, the computation turns into a
spike-based, multiplication-free manner, reducing the energy consumption. In
SpikingNeRF, each sampled point on the ray is matched onto a particular time
step, and represented in a hybrid manner where the voxel grids are maintained
as well. Based on the voxel grids, sampled points are determined whether to be
masked for better training and inference. However, this operation also incurs
irregular temporal length. We propose the temporal padding strategy to tackle
the masked samples to maintain regular temporal length, i.e., regular tensors,
and the temporal condensing strategy to form a denser data structure for
hardware-friendly computation. Extensive experiments on various datasets
demonstrate that our method reduces the 70.79% energy consumption on average
and obtains comparable synthesis quality with the ANN baseline.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00023">De-SaTE: Denoising Self-attention Transformer Encoders for Li-ion Battery Health Prognostics. (arXiv:2310.00023v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shinde_G/0/1/0/all/0/1">Gaurav Shinde</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohapatra_R/0/1/0/all/0/1">Rohan Mohapatra</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishan_P/0/1/0/all/0/1">Pooja Krishan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1">Saptarshi Sengupta</a></p>
<p>The usage of Lithium-ion (Li-ion) batteries has gained widespread popularity
across various industries, from powering portable electronic devices to
propelling electric vehicles and supporting energy storage systems. A central
challenge in Li-ion battery reliability lies in accurately predicting their
Remaining Useful Life (RUL), which is a critical measure for proactive
maintenance and predictive analytics. This study presents a novel approach that
harnesses the power of multiple denoising modules, each trained to address
specific types of noise commonly encountered in battery data. Specifically, a
denoising auto-encoder and a wavelet denoiser are used to generate
encoded/decomposed representations, which are subsequently processed through
dedicated self-attention transformer encoders. After extensive experimentation
on NASA and CALCE data, a broad spectrum of health indicator values are
estimated under a set of diverse noise patterns. The reported error metrics on
these data are on par with or better than the state-of-the-art reported in
recent literature.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03059">Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models. (arXiv:2310.03059v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_I/0/1/0/all/0/1">Ivan Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Ray Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Zoey Guo</a></p>
<p>The popularity of pre-trained large models has revolutionized downstream
tasks across diverse fields, such as language, vision, and multi-modality. To
minimize the adaption cost for downstream tasks, many Parameter-Efficient
Fine-Tuning (PEFT) techniques are proposed for language and 2D image
pre-trained models. However, the specialized PEFT method for 3D pre-trained
models is still under-explored. To this end, we introduce Point-PEFT, a novel
framework for adapting point cloud pre-trained models with minimal learnable
parameters. Specifically, for a pre-trained 3D model, we freeze most of its
parameters, and only tune the newly added PEFT modules on downstream tasks,
which consist of a Point-prior Prompt and a Geometry-aware Adapter. The
Point-prior Prompt adopts a set of learnable prompt tokens, for which we
propose to construct a memory bank with domain-specific knowledge, and utilize
a parameter-free attention to enhance the prompt tokens. The Geometry-aware
Adapter aims to aggregate point cloud features within spatial neighborhoods to
capture fine-grained geometric information through local interactions.
Extensive experiments indicate that our Point-PEFT can achieve better
performance than the full fine-tuning on various downstream tasks, while using
only 5% of the trainable parameters, demonstrating the efficiency and
effectiveness of our approach. Code will be released at
https://github.com/Even-JK/PEFT-3D.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04816">Hacking Generative Models with Differentiable Network Bending. (arXiv:2310.04816v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aldegheri_G/0/1/0/all/0/1">Giacomo Aldegheri</a>, <a href="http://arxiv.org/find/cs/1/au:+Rogalska_A/0/1/0/all/0/1">Alina Rogalska</a>, <a href="http://arxiv.org/find/cs/1/au:+Youssef_A/0/1/0/all/0/1">Ahmed Youssef</a>, <a href="http://arxiv.org/find/cs/1/au:+Iofinova_E/0/1/0/all/0/1">Eugenia Iofinova</a></p>
<p>In this work, we propose a method to 'hack' generative models, pushing their
outputs away from the original training distribution towards a new objective.
We inject a small-scale trainable module between the intermediate layers of the
model and train it for a low number of iterations, keeping the rest of the
network frozen. The resulting output images display an uncanny quality, given
by the tension between the original and new objectives that can be exploited
for artistic purposes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05317">Task-Adaptive Tokenization: Enhancing Long-Form Text Generation Efficacy in Mental Health and Beyond. (arXiv:2310.05317v5 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Siyang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_N/0/1/0/all/0/1">Naihao Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Sabour_S/0/1/0/all/0/1">Sahand Sabour</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1">Yilin Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1">Minlie Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1">Rada Mihalcea</a></p>
<p>We propose task-adaptive tokenization as a way to adapt the generation
pipeline to the specifics of a downstream task and enhance long-form generation
in mental health. Inspired by insights from cognitive science, our
task-adaptive tokenizer samples variable segmentations from multiple outcomes,
with sampling probabilities optimized based on task-specific data. We introduce
a strategy for building a specialized vocabulary and introduce a vocabulary
merging protocol that allows for the integration of task-specific tokens into
the pre-trained model's tokenization step. Through extensive experiments on
psychological question-answering tasks in both Chinese and English, we find
that our task-adaptive tokenization approach brings a significant improvement
in generation performance while using up to 60% fewer tokens. Preliminary
experiments point to promising results when using our tokenization approach
with very large language models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.06552">Automated clinical coding using off-the-shelf large language models. (arXiv:2310.06552v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Boyle_J/0/1/0/all/0/1">Joseph S. Boyle</a>, <a href="http://arxiv.org/find/cs/1/au:+Kascenas_A/0/1/0/all/0/1">Antanas Kascenas</a>, <a href="http://arxiv.org/find/cs/1/au:+Lok_P/0/1/0/all/0/1">Pat Lok</a>, <a href="http://arxiv.org/find/cs/1/au:+Liakata_M/0/1/0/all/0/1">Maria Liakata</a>, <a href="http://arxiv.org/find/cs/1/au:+ONeil_A/0/1/0/all/0/1">Alison Q. O&#x27;Neil</a></p>
<p>The task of assigning diagnostic ICD codes to patient hospital admissions is
typically performed by expert human coders. Efforts towards automated ICD
coding are dominated by supervised deep learning models. However, difficulties
in learning to predict the large number of rare codes remain a barrier to
adoption in clinical practice. In this work, we leverage off-the-shelf
pre-trained generative large language models (LLMs) to develop a practical
solution that is suitable for zero-shot and few-shot code assignment, with no
need for further task-specific training. Unsupervised pre-training alone does
not guarantee precise knowledge of the ICD ontology and specialist clinical
coding task, therefore we frame the task as information extraction, providing a
description of each coded concept and asking the model to retrieve related
mentions. For efficiency, rather than iterating over all codes, we leverage the
hierarchical nature of the ICD ontology to sparsely search for relevant codes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07838">Towards the Fundamental Limits of Knowledge Transfer over Finite Domains. (arXiv:2310.07838v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1">Qingyue Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1">Banghua Zhu</a></p>
<p>We characterize the statistical efficiency of knowledge transfer through $n$
samples from a teacher to a probabilistic student classifier with input space
$\mathcal S$ over labels $\mathcal A$. We show that privileged information at
three progressive levels accelerates the transfer. At the first level, only
samples with hard labels are known, via which the maximum likelihood estimator
attains the minimax rate $\sqrt{{|{\mathcal S}||{\mathcal A}|}/{n}}$. The
second level has the teacher probabilities of sampled labels available in
addition, which turns out to boost the convergence rate lower bound to
${{|{\mathcal S}||{\mathcal A}|}/{n}}$. However, under this second data
acquisition protocol, minimizing a naive adaptation of the cross-entropy loss
results in an asymptotically biased student. We overcome this limitation and
achieve the fundamental limit by using a novel empirical variant of the squared
error logit loss. The third level further equips the student with the soft
labels (complete logits) on ${\mathcal A}$ given every sampled input, thereby
provably enables the student to enjoy a rate ${|{\mathcal S}|}/{n}$ free of
$|{\mathcal A}|$. We find any Kullback-Leibler divergence minimizer to be
optimal in the last case. Numerical simulations distinguish the four learners
and corroborate our theory.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08660">Learning RL-Policies for Joint Beamforming Without Exploration: A Batch Constrained Off-Policy Approach. (arXiv:2310.08660v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">Heasung Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Ankireddy_S/0/1/0/all/0/1">Sravan Kumar Ankireddy</a></p>
<p>In this work, we consider the problem of network parameter optimization for
rate maximization. We frame this as a joint optimization problem of power
control, beam forming, and interference cancellation. We consider the setting
where multiple Base Stations (BSs) communicate with multiple user equipment
(UEs). Because of the exponential computational complexity of brute force
search, we instead solve this nonconvex optimization problem using deep
reinforcement learning (RL) techniques. Modern communication systems are
notorious for their difficulty in exactly modeling their behavior. This limits
us in using RL-based algorithms as interaction with the environment is needed
for the agent to explore and learn efficiently. Further, it is ill-advised to
deploy the algorithm in the real world for exploration and learning because of
the high cost of failure. In contrast to the previous RL-based solutions
proposed, such as deep-Q network (DQN) based control, we suggest an offline
model-based approach. We specifically consider discrete batch-constrained deep
Q-learning (BCQ) and show that performance similar to DQN can be achieved with
only a fraction of the data without exploring. This maximizes sample efficiency
and minimizes risk in deploying a new algorithm to commercial networks. We
provide the entire project resource, including code and data, at the following
link: https://github.com/Heasung-Kim/ safe-rl-deployment-for-5g.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.09624">ASSERT: Automated Safety Scenario Red Teaming for Evaluating the Robustness of Large Language Models. (arXiv:2310.09624v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mei_A/0/1/0/all/0/1">Alex Mei</a>, <a href="http://arxiv.org/find/cs/1/au:+Levy_S/0/1/0/all/0/1">Sharon Levy</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">William Yang Wang</a></p>
<p>As large language models are integrated into society, robustness toward a
suite of prompts is increasingly important to maintain reliability in a
high-variance environment.Robustness evaluations must comprehensively
encapsulate the various settings in which a user may invoke an intelligent
system. This paper proposes ASSERT, Automated Safety Scenario Red Teaming,
consisting of three methods -- semantically aligned augmentation, target
bootstrapping, and adversarial knowledge injection. For robust safety
evaluation, we apply these methods in the critical domain of AI safety to
algorithmically generate a test suite of prompts covering diverse robustness
settings -- semantic equivalence, related scenarios, and adversarial. We
partition our prompts into four safety domains for a fine-grained analysis of
how the domain affects model performance. Despite dedicated safeguards in
existing state-of-the-art models, we find statistically significant performance
differences of up to 11% in absolute classification accuracy among semantically
related scenarios and error rates of up to 19% absolute error in zero-shot
adversarial settings, raising concerns for users' physical safety.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.10072">Fine-tuning ChatGPT for Automatic Scoring. (arXiv:2310.10072v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Latif_E/0/1/0/all/0/1">Ehsan Latif</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1">Xiaoming Zhai</a></p>
<p>This study highlights the potential of fine-tuned ChatGPT (GPT-3.5) for
automatically scoring student written constructed responses using example
assessment tasks in science education. Recent studies on OpenAI's generative
model GPT-3.5 proved its superiority in predicting the natural language with
high accuracy and human-like responses. GPT-3.5 has been trained over enormous
online language materials such as journals and Wikipedia; therefore, more than
direct usage of pre-trained GPT-3.5 is required for automatic scoring as
students utilize a different language than trained material. These imply that a
domain-specific model, fine-tuned over data for specific tasks, can enhance
model performance. In this study, we fine-tuned GPT-3.5 on six assessment tasks
with a diverse dataset of middle-school and high-school student responses and
expert scoring. The six tasks comprise two multi-label and four multi-class
assessment tasks. We compare the performance of fine-tuned GPT-3.5 with the
fine-tuned state-of-the-art Google's generated language model, BERT. The
results show that in-domain training corpora constructed from science questions
and responses for BERT achieved average accuracy = 0.838, SD = 0.069. GPT-3.5
shows a remarkable average increase (9.1%) in automatic scoring accuracy (mean
= 9.15, SD = 0.042) for the six tasks, p =0.001 &lt; 0.05. Specifically, for
multi-label tasks (item 1 with 5 labels; item 2 with 10 labels), GPT-3.5
achieved significantly higher scoring accuracy than BERT across all the labels,
with the second item achieving a 7.1% increase. The average scoring increase
for the four multi-class items for GPT-3.5 was 10.6% compared to BERT. Our
study confirmed the effectiveness of fine-tuned GPT-3.5 for automatic scoring
of student responses on domain-specific data in education with high accuracy.
We have released fine-tuned models for public use and community engagement.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.10418">Reading Books is Great, But Not if You Are Driving! Visually Grounded Reasoning about Defeasible Commonsense Norms. (arXiv:2310.10418v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1">Seungju Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Junhyeok Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1">Jack Hessel</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1">Liwei Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1">Jiwan Chung</a>, <a href="http://arxiv.org/find/cs/1/au:+Son_Y/0/1/0/all/0/1">Yejin Son</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1">Yejin Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Youngjae Yu</a></p>
<p>Commonsense norms are defeasible by context: reading books is usually great,
but not when driving a car. While contexts can be explicitly described in
language, in embodied scenarios, contexts are often provided visually. This
type of visually grounded reasoning about defeasible commonsense norms is
generally easy for humans, but (as we show) poses a challenge for machines, as
it necessitates both visual understanding and reasoning about commonsense
norms. We construct a new multimodal benchmark for studying visual-grounded
commonsense norms: NORMLENS. NORMLENS consists of 10K human judgments
accompanied by free-form explanations covering 2K multimodal situations, and
serves as a probe to address two questions: (1) to what extent can models align
with average human judgment? and (2) how well can models explain their
predicted judgments? We find that state-of-the-art model judgments and
explanations are not well-aligned with human annotation. Additionally, we
present a new approach to better align models with humans by distilling social
commonsense knowledge from large language models. The data and code are
released at https://seungjuhan.me/normlens.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.10705">Machine Learning Classification Techniques for Identifying the Defective Patterns in Semiconductor Wafer Maps: A Survey, Empirical, and Experimental Evaluations. (arXiv:2310.10705v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Taha_K/0/1/0/all/0/1">Kamal Taha</a></p>
<p>This survey paper offers a comprehensive review of methodologies utilizing
machine learning (ML) classification techniques for identifying wafer defects
in semiconductor manufacturing. Despite the growing body of research
demonstrating the effectiveness of ML in wafer defect identification, there is
a noticeable absence of comprehensive reviews on this subject. This survey
attempts to fill this void by amalgamating available literature and providing
an in-depth analysis of the advantages, limitations, and potential applications
of various ML classification algorithms in the realm of wafer defect detection.
An innovative taxonomy of methodologies that we present provides a detailed
classification of algorithms into more refined categories and techniques. This
taxonomy follows a four-tier structure, starting from broad methodology
categories and ending with specific sub-techniques. It aids researchers in
comprehending the complex relationships between different algorithms and their
techniques. We employ a rigorous empirical and experimental evaluation to rank
these varying techniques. For the empirical evaluation, we assess techniques
based on a set of four criteria. The experimental evaluation ranks the
algorithms employing the same sub-techniques, techniques, sub-categories, and
categories. This integration of a multi-layered taxonomy, empirical
evaluations, and comparative experiments provides a detailed and holistic
understanding of ML techniques and algorithms for identifying wafer defects.
Additionally, the paper illuminates the future prospects of ML classification
techniques for wafer defect identification, underscoring potential advancements
and opportunities for further research in this field
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.11670">Prototype-based HyperAdapter for Sample-Efficient Multi-task Tuning. (arXiv:2310.11670v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Hao Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1">Jie Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1">Zhaofeng He</a></p>
<p>Parameter-efficient fine-tuning (PEFT) has shown its effectiveness in
adapting the pre-trained language models to downstream tasks while only
updating a small number of parameters. Despite the success, most existing
methods independently adapt to each task without considering knowledge transfer
between tasks and are limited to low-data regimes. To overcome this issue, we
propose Prototype-based HyperAdapter (PHA), a novel framework built on the
adapter-tuning and hypernetwork. It introduces an instance-dense retriever and
a prototypical hypernetwork to generate the conditional modules in a
sample-efficient manner. This leads to comparable performance improvements
against existing PEFT methods on multi-task learning and few-shot transfer
learning. More importantly, when the available data size gets smaller, our
method outperforms other strong baselines by a large margin. Based on our
extensive empirical experiments across various datasets, we demonstrate that
PHA strikes a better trade-off between trainable parameters, accuracy on stream
tasks, and sample efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.11676">PREM: A Simple Yet Effective Approach for Node-Level Graph Anomaly Detection. (arXiv:2310.11676v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1">Junjun Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yixin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1">Yizhen Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1">Shirui Pan</a></p>
<p>Node-level graph anomaly detection (GAD) plays a critical role in identifying
anomalous nodes from graph-structured data in various domains such as medicine,
social networks, and e-commerce. However, challenges have arisen due to the
diversity of anomalies and the dearth of labeled data. Existing methodologies -
reconstruction-based and contrastive learning - while effective, often suffer
from efficiency issues, stemming from their complex objectives and elaborate
modules. To improve the efficiency of GAD, we introduce a simple method termed
PREprocessing and Matching (PREM for short). Our approach streamlines GAD,
reducing time and memory consumption while maintaining powerful anomaly
detection capabilities. Comprising two modules - a pre-processing module and an
ego-neighbor matching module - PREM eliminates the necessity for
message-passing propagation during training, and employs a simple contrastive
loss, leading to considerable reductions in training time and memory usage.
Moreover, through rigorous evaluations of five real-world datasets, our method
demonstrated robustness and effectiveness. Notably, when validated on the ACM
dataset, PREM achieved a 5% improvement in AUC, a 9-fold increase in training
speed, and sharply reduce memory usage compared to the most efficient baseline.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.11715">Enhancing Low-resource Fine-grained Named Entity Recognition by Leveraging Coarse-grained Datasets. (arXiv:2310.11715v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Su Ah Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1">Seokjin Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_W/0/1/0/all/0/1">Woohwan Jung</a></p>
<p>Named Entity Recognition (NER) frequently suffers from the problem of
insufficient labeled data, particularly in fine-grained NER scenarios. Although
$K$-shot learning techniques can be applied, their performance tends to
saturate when the number of annotations exceeds several tens of labels. To
overcome this problem, we utilize existing coarse-grained datasets that offer a
large number of annotations. A straightforward approach to address this problem
is pre-finetuning, which employs coarse-grained data for representation
learning. However, it cannot directly utilize the relationships between
fine-grained and coarse-grained entities, although a fine-grained entity type
is likely to be a subcategory of a coarse-grained entity type. We propose a
fine-grained NER model with a Fine-to-Coarse(F2C) mapping matrix to leverage
the hierarchical structure explicitly. In addition, we present an inconsistency
filtering method to eliminate coarse-grained entities that are inconsistent
with fine-grained entity types to avoid performance degradation. Our
experimental results show that our method outperforms both $K$-shot learning
and supervised learning methods when dealing with a small number of
fine-grained annotations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.14338">From Chaos to Clarity: Claim Normalization to Empower Fact-Checking. (arXiv:2310.14338v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sundriyal_M/0/1/0/all/0/1">Megha Sundriyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1">Tanmoy Chakraborty</a>, <a href="http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1">Preslav Nakov</a></p>
<p>With the rise of social media, users are exposed to many misleading claims.
However, the pervasive noise inherent in these posts presents a challenge in
identifying precise and prominent claims that require verification. Extracting
the important claims from such posts is arduous and time-consuming, yet it is
an underexplored problem. Here, we aim to bridge this gap. We introduce a novel
task, Claim Normalization (aka ClaimNorm), which aims to decompose complex and
noisy social media posts into more straightforward and understandable forms,
termed normalized claims. We propose CACN, a pioneering approach that leverages
chain-of-thought and claim check-worthiness estimation, mimicking human
reasoning processes, to comprehend intricate claims. Moreover, we capitalize on
the in-context learning capabilities of large language models to provide
guidance and to improve claim normalization. To evaluate the effectiveness of
our proposed model, we meticulously compile a comprehensive real-world dataset,
CLAN, comprising more than 6k instances of social media posts alongside their
respective normalized claims. Our experiments demonstrate that CACN outperforms
several baselines across various evaluation measures. Finally, our rigorous
error analysis validates CACN's capabilities and pitfalls.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16452">Faithful Path Language Modelling for Explainable Recommendation over Knowledge Graph. (arXiv:2310.16452v2 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Balloccu_G/0/1/0/all/0/1">Giacomo Balloccu</a>, <a href="http://arxiv.org/find/cs/1/au:+Boratto_L/0/1/0/all/0/1">Ludovico Boratto</a>, <a href="http://arxiv.org/find/cs/1/au:+Cancedda_C/0/1/0/all/0/1">Christian Cancedda</a>, <a href="http://arxiv.org/find/cs/1/au:+Fenu_G/0/1/0/all/0/1">Gianni Fenu</a>, <a href="http://arxiv.org/find/cs/1/au:+Marras_M/0/1/0/all/0/1">Mirko Marras</a></p>
<p>Path reasoning methods over knowledge graphs have gained popularity for their
potential to improve transparency in recommender systems. However, the
resulting models still rely on pre-trained knowledge graph embeddings, fail to
fully exploit the interdependence between entities and relations in the KG for
recommendation, and may generate inaccurate explanations. In this paper, we
introduce PEARLM, a novel approach that efficiently captures user behaviour and
product-side knowledge through language modelling. With our approach, knowledge
graph embeddings are directly learned from paths over the KG by the language
model, which also unifies entities and relations in the same optimisation
space. Constraints on the sequence decoding additionally guarantee path
faithfulness with respect to the KG. Experiments on two datasets show the
effectiveness of our approach compared to state-of-the-art baselines. Source
code and datasets: AVAILABLE AFTER GETTING ACCEPTED.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17688">Managing AI Risks in an Era of Rapid Progress. (arXiv:2310.17688v2 [cs.CY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1">Yoshua Bengio</a>, <a href="http://arxiv.org/find/cs/1/au:+Hinton_G/0/1/0/all/0/1">Geoffrey Hinton</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1">Andrew Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1">Dawn Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1">Pieter Abbeel</a>, <a href="http://arxiv.org/find/cs/1/au:+Harari_Y/0/1/0/all/0/1">Yuval Noah Harari</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Ya-Qin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1">Lan Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Shalev_Shwartz_S/0/1/0/all/0/1">Shai Shalev-Shwartz</a>, <a href="http://arxiv.org/find/cs/1/au:+Hadfield_G/0/1/0/all/0/1">Gillian Hadfield</a>, <a href="http://arxiv.org/find/cs/1/au:+Clune_J/0/1/0/all/0/1">Jeff Clune</a>, <a href="http://arxiv.org/find/cs/1/au:+Maharaj_T/0/1/0/all/0/1">Tegan Maharaj</a>, <a href="http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1">Frank Hutter</a>, <a href="http://arxiv.org/find/cs/1/au:+Baydin_A/0/1/0/all/0/1">At&#x131;l&#x131;m G&#xfc;ne&#x15f; Baydin</a>, <a href="http://arxiv.org/find/cs/1/au:+McIlraith_S/0/1/0/all/0/1">Sheila McIlraith</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1">Qiqi Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1">Ashwin Acharya</a>, <a href="http://arxiv.org/find/cs/1/au:+Krueger_D/0/1/0/all/0/1">David Krueger</a>, <a href="http://arxiv.org/find/cs/1/au:+Dragan_A/0/1/0/all/0/1">Anca Dragan</a>, <a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1">Philip Torr</a>, <a href="http://arxiv.org/find/cs/1/au:+Russell_S/0/1/0/all/0/1">Stuart Russell</a>, <a href="http://arxiv.org/find/cs/1/au:+Kahneman_D/0/1/0/all/0/1">Daniel Kahneman</a>, <a href="http://arxiv.org/find/cs/1/au:+Brauner_J/0/1/0/all/0/1">Jan Brauner</a>, <a href="http://arxiv.org/find/cs/1/au:+Mindermann_S/0/1/0/all/0/1">S&#xf6;ren Mindermann</a></p>
<p>In this short consensus paper, we outline risks from upcoming, advanced AI
systems. We examine large-scale social harms and malicious uses, as well as an
irreversible loss of human control over autonomous AI systems. In light of
rapid and continuing AI progress, we propose urgent priorities for AI R&amp;D and
governance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.18122">OpinSummEval: Revisiting Automated Evaluation for Opinion Summarization. (arXiv:2310.18122v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yuchen Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1">Xiaojun Wan</a></p>
<p>Opinion summarization sets itself apart from other types of summarization
tasks due to its distinctive focus on aspects and sentiments. Although certain
automated evaluation methods like ROUGE have gained popularity, we have found
them to be unreliable measures for assessing the quality of opinion summaries.
In this paper, we present OpinSummEval, a dataset comprising human judgments
and outputs from 14 opinion summarization models. We further explore the
correlation between 24 automatic metrics and human ratings across four
dimensions. Our findings indicate that metrics based on neural networks
generally outperform non-neural ones. However, even metrics built on powerful
backbones, such as BART and GPT-3/3.5, do not consistently correlate well
across all dimensions, highlighting the need for advancements in automated
evaluation methods for opinion summarization. The code and data are publicly
available at https://github.com/A-Chicharito-S/OpinSummEval/tree/main.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19056">MILL: Mutual Verification with Large Language Models for Zero-Shot Query Expansion. (arXiv:2310.19056v2 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jia_P/0/1/0/all/0/1">Pengyue Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yiding Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xiangyu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaopeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hao_C/0/1/0/all/0/1">Changying Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuaiqiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1">Dawei Yin</a></p>
<p>Query expansion is a commonly-used technique in many search systems to better
represent users' information needs with additional query terms. Existing
studies for this task usually propose to expand a query with retrieved or
generated contextual documents. However, both types of methods have clear
limitations. For retrieval-based methods, the documents retrieved with the
original query might not be accurate enough to reveal the search intent,
especially when the query is brief or ambiguous. For generation-based methods,
existing models can hardly be trained or aligned on a particular corpus, due to
the lack of corpus-specific labeled data. In this paper, we propose a novel
Large Language Model (LLM) based mutual verification framework for query
expansion, which alleviates the aforementioned limitations. Specifically, we
first design a query-query-document generation pipeline, which can effectively
leverage the contextual knowledge encoded in LLMs to generate sub-queries and
corresponding documents from multiple perspectives. Next, we employ a mutual
verification method for both generated and retrieved contextual documents,
where 1) retrieved documents are filtered with the external contextual
knowledge in generated documents, and 2) generated documents are filtered with
the corpus-specific knowledge in retrieved documents. Overall, the proposed
method allows retrieved and generated documents to complement each other to
finalize a better query expansion. We conduct extensive experiments on three
information retrieval datasets, i.e., TREC-DL-2020, TREC-COVID, and MSMARCO.
The results demonstrate that our method outperforms other baselines
significantly.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19680">Integrating Pre-trained Language Model into Neural Machine Translation. (arXiv:2310.19680v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1">Soon-Jae Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeong_C/0/1/0/all/0/1">Chang-Sung Jeong</a></p>
<p>Neural Machine Translation (NMT) has become a significant technology in
natural language processing through extensive research and development.
However, the deficiency of high-quality bilingual language pair data still
poses a major challenge to improving NMT performance. Recent studies are
exploring the use of contextual information from pre-trained language model
(PLM) to address this problem. Yet, the issue of incompatibility between PLM
and NMT model remains unresolved. This study proposes a PLM-integrated NMT
(PiNMT) model to overcome the identified problems. The PiNMT model consists of
three critical components, PLM Multi Layer Converter, Embedding Fusion, and
Cosine Alignment, each playing a vital role in providing effective PLM
information to NMT. Furthermore, two training strategies, Separate Learning
Rates and Dual Step Training, are also introduced in this paper. By
implementing the proposed PiNMT model and training strategy, we achieved
state-of-the-art performance on the IWSLT'14 En$\leftrightarrow$De dataset.
This study's outcomes are noteworthy as they demonstrate a novel approach for
efficiently integrating PLM with NMT to overcome incompatibility and enhance
performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00334">MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows. (arXiv:2311.00334v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stripelis_D/0/1/0/all/0/1">Dimitris Stripelis</a>, <a href="http://arxiv.org/find/cs/1/au:+Anastasiou_C/0/1/0/all/0/1">Chrysovalantis Anastasiou</a>, <a href="http://arxiv.org/find/cs/1/au:+Toral_P/0/1/0/all/0/1">Patrick Toral</a>, <a href="http://arxiv.org/find/cs/1/au:+Asghar_A/0/1/0/all/0/1">Armaghan Asghar</a>, <a href="http://arxiv.org/find/cs/1/au:+Ambite_J/0/1/0/all/0/1">Jose Luis Ambite</a></p>
<p>A Federated Learning (FL) system typically consists of two core processing
entities: the federation controller and the learners. The controller is
responsible for managing the execution of FL workflows across learners and the
learners for training and evaluating federated models over their private
datasets. While executing an FL workflow, the FL system has no control over the
computational resources or data of the participating learners. Still, it is
responsible for other operations, such as model aggregation, task dispatching,
and scheduling. These computationally heavy operations generally need to be
handled by the federation controller. Even though many FL systems have been
recently proposed to facilitate the development of FL workflows, most of these
systems overlook the scalability of the controller. To meet this need, we
designed and developed a novel FL system called MetisFL, where the federation
controller is the first-class citizen. MetisFL re-engineers all the operations
conducted by the federation controller to accelerate the training of
large-scale FL workflows. By quantitatively comparing MetisFL against other
state-of-the-art FL systems, we empirically demonstrate that MetisFL leads to a
10-fold wall-clock time execution boost across a wide range of challenging FL
workflows with increasing model sizes and federation sites.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00797">Tipping Points of Evolving Epidemiological Networks: Machine Learning-Assisted, Data-Driven Effective Modeling. (arXiv:2311.00797v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Evangelou_N/0/1/0/all/0/1">Nikolaos Evangelou</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_T/0/1/0/all/0/1">Tianqi Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Bello_Rivas_J/0/1/0/all/0/1">Juan M. Bello-Rivas</a>, <a href="http://arxiv.org/find/cs/1/au:+Makeev_A/0/1/0/all/0/1">Alexei Makeev</a>, <a href="http://arxiv.org/find/cs/1/au:+Kevrekidis_I/0/1/0/all/0/1">Ioannis G. Kevrekidis</a></p>
<p>We study the tipping point collective dynamics of an adaptive
susceptible-infected-susceptible (SIS) epidemiological network in a
data-driven, machine learning-assisted manner. We identify a
parameter-dependent effective stochastic differential equation (eSDE) in terms
of physically meaningful coarse mean-field variables through a deep-learning
ResNet architecture inspired by numerical stochastic integrators. We construct
an approximate effective bifurcation diagram based on the identified drift term
of the eSDE and contrast it with the mean-field SIS model bifurcation diagram.
We observe a subcritical Hopf bifurcation in the evolving network's effective
SIS dynamics, that causes the tipping point behavior; this takes the form of
large amplitude collective oscillations that spontaneously -- yet rarely --
arise from the neighborhood of a (noisy) stationary state. We study the
statistics of these rare events both through repeated brute force simulations
and by using established mathematical/computational tools exploiting the
right-hand-side of the identified SDE. We demonstrate that such a collective
SDE can also be identified (and the rare events computations also performed) in
terms of data-driven coarse observables, obtained here via manifold learning
techniques, in particular Diffusion Maps. The workflow of our study is
straightforwardly applicable to other complex dynamics problems exhibiting
tipping point dynamics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01305">AWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language Models. (arXiv:2311.01305v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Baisong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xingwang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Haixiao Xu</a></p>
<p>Large language models(LLMs) exhibit excellent performance across a variety of
tasks, but they come with significant computational and storage costs.
Quantizing these models is an effective way to alleviate this issue. However,
existing methods struggle to strike a balance between model accuracy and
hardware efficiency. This is where we introduce AWEQ, a post-training method
that requires no additional training overhead. AWEQ excels in both
ultra-low-bit quantization and 8-bit weight and activation (W8A8) quantization.
There is an observation that weight quantization is less challenging than
activation quantization. AWEQ transfers the difficulty of activation
quantization to weights using channel equalization, achieving a balance between
the quantization difficulties of both, and thereby maximizing performance. We
have further refined the equalization method to mitigate quantization bias
error, ensuring the robustness of the model. Extensive experiments on popular
models such as LLaMA and OPT demonstrate that AWEQ outperforms all existing
post-training quantization methods for large models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01351">Simplicial Models for the Epistemic Logic of Faulty Agents. (arXiv:2311.01351v2 [cs.LO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Goubault_E/0/1/0/all/0/1">Eric Goubault</a>, <a href="http://arxiv.org/find/cs/1/au:+Kniazev_R/0/1/0/all/0/1">Roman Kniazev</a>, <a href="http://arxiv.org/find/cs/1/au:+Ledent_J/0/1/0/all/0/1">Jeremy Ledent</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajsbaum_S/0/1/0/all/0/1">Sergio Rajsbaum</a></p>
<p>In recent years, several authors have been investigating simplicial models, a
model of epistemic logic based on higher-dimensional structures called
simplicial complexes. In the original formulation, simplicial models were
always assumed to be pure, meaning that all worlds have the same dimension.
This is equivalent to the standard S5n semantics of epistemic logic, based on
Kripke models. By removing the assumption that models must be pure, we can go
beyond the usual Kripke semantics and study epistemic logics where the number
of agents participating in a world can vary. This approach has been developed
in a number of papers, with applications in fault-tolerant distributed
computing where processes may crash during the execution of a system. A
difficulty that arises is that subtle design choices in the definition of
impure simplicial models can result in different axioms of the resulting logic.
In this paper, we classify those design choices systematically, and axiomatize
the corresponding logics. We illustrate them via distributed computing examples
of synchronous systems where processes may crash.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01455">RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation. (arXiv:2311.01455v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yufei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xian_Z/0/1/0/all/0/1">Zhou Xian</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1">Feng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tsun-Hsuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Erickson_Z/0/1/0/all/0/1">Zackory Erickson</a>, <a href="http://arxiv.org/find/cs/1/au:+Held_D/0/1/0/all/0/1">David Held</a>, <a href="http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1">Chuang Gan</a></p>
<p>We present RoboGen, a generative robotic agent that automatically learns
diverse robotic skills at scale via generative simulation. RoboGen leverages
the latest advancements in foundation and generative models. Instead of
directly using or adapting these models to produce policies or low-level
actions, we advocate for a generative scheme, which uses these models to
automatically generate diversified tasks, scenes, and training supervisions,
thereby scaling up robotic skill learning with minimal human supervision. Our
approach equips a robotic agent with a self-guided propose-generate-learn
cycle: the agent first proposes interesting tasks and skills to develop, and
then generates corresponding simulation environments by populating pertinent
objects and assets with proper spatial configurations. Afterwards, the agent
decomposes the proposed high-level task into sub-tasks, selects the optimal
learning approach (reinforcement learning, motion planning, or trajectory
optimization), generates required training supervision, and then learns
policies to acquire the proposed skill. Our work attempts to extract the
extensive and versatile knowledge embedded in large-scale models and transfer
them to the field of robotics. Our fully generative pipeline can be queried
repeatedly, producing an endless stream of skill demonstrations associated with
diverse tasks and environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02651">Compute at Scale -- A Broad Investigation into the Data Center Industry. (arXiv:2311.02651v2 [cs.CY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pilz_K/0/1/0/all/0/1">Konstantin Pilz</a>, <a href="http://arxiv.org/find/cs/1/au:+Heim_L/0/1/0/all/0/1">Lennart Heim</a></p>
<p>This report characterizes the data center industry and its importance for AI
development. Data centers are industrial facilities that efficiently provide
compute at scale and thus constitute the engine rooms of today's digital
economy. As large-scale AI training and inference become increasingly
computationally expensive, they are dominantly executed from this designated
infrastructure. Key features of data centers include large-scale compute
clusters that require extensive cooling and consume large amounts of power, the
need for fast connectivity both within the data center and to the internet, and
an emphasis on security and reliability. The global industry is valued at
approximately $250B and is expected to double over the next seven years. There
are likely about 500 large (above 10 MW) data centers globally, with the US,
Europe, and China constituting the most important markets. The report further
covers important actors, business models, main inputs, and typical locations of
data centers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02775">ChaTA: Towards an Intelligent Question-Answer Teaching Assistant using Open-Source LLMs. (arXiv:2311.02775v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hicke_Y/0/1/0/all/0/1">Yann Hicke</a>, <a href="http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1">Anmol Agarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1">Qianou Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Denny_P/0/1/0/all/0/1">Paul Denny</a></p>
<p>Responding to the thousands of student questions on online QA platforms each
semester has a considerable human cost, particularly in computing courses with
rapidly growing enrollments. To address the challenges of scalable and
intelligent question-answering (QA), we introduce an innovative solution that
leverages open-source Large Language Models (LLMs) from the LLaMA-2 family to
ensure data privacy. Our approach combines augmentation techniques such as
retrieval augmented generation (RAG), supervised fine-tuning (SFT), and
learning from human preferences data using Direct Preference Optimization
(DPO). Through extensive experimentation on a Piazza dataset from an
introductory CS course, comprising 10,000 QA pairs and 1,500 pairs of
preference data, we demonstrate a significant 30% improvement in the quality of
answers, with RAG being a particularly impactful addition. Our contributions
include the development of a novel architecture for educational QA, extensive
evaluations of LLM performance utilizing both human assessments and LLM-based
metrics, and insights into the challenges and future directions of educational
data processing. This work paves the way for the development of CHATA, an
intelligent QA assistant customizable for courses with an online QA platform
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02782">Towards Generic Anomaly Detection and Understanding: Large-scale Visual-linguistic Model (GPT-4V) Takes the Lead. (arXiv:2311.02782v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yunkang Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiaohao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1">Chen Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xiaonan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1">Weiming Shen</a></p>
<p>Anomaly detection is a crucial task across different domains and data types.
However, existing anomaly detection models are often designed for specific
domains and modalities. This study explores the use of GPT-4V(ision), a
powerful visual-linguistic model, to address anomaly detection tasks in a
generic manner. We investigate the application of GPT-4V in multi-modality,
multi-domain anomaly detection tasks, including image, video, point cloud, and
time series data, across multiple application areas, such as industrial,
medical, logical, video, 3D anomaly detection, and localization tasks. To
enhance GPT-4V's performance, we incorporate different kinds of additional cues
such as class information, human expertise, and reference images as
prompts.Based on our experiments, GPT-4V proves to be highly effective in
detecting and explaining global and fine-grained semantic patterns in
zero/one-shot anomaly detection. This enables accurate differentiation between
normal and abnormal instances. Although we conducted extensive evaluations in
this study, there is still room for future evaluation to further exploit
GPT-4V's generic anomaly detection capacity from different aspects. These
include exploring quantitative metrics, expanding evaluation benchmarks,
incorporating multi-round interactions, and incorporating human feedback loops.
Nevertheless, GPT-4V exhibits promising performance in generic anomaly
detection and understanding, thus opening up a new avenue for anomaly
detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04254">Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation. (arXiv:2311.04254v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1">Ruomeng Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chaoyun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yong Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1">Minghua Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_S/0/1/0/all/0/1">Si Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajmohan_S/0/1/0/all/0/1">Saravan Rajmohan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1">Qingwei Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dongmei Zhang</a></p>
<p>Recent advancements in Large Language Models (LLMs) have revolutionized
decision-making by breaking down complex problems into more manageable language
sequences referred to as ``thoughts''. An effective thought design should
consider three key perspectives: performance, efficiency, and flexibility.
However, existing thought can at most exhibit two of these attributes. To
address these limitations, we introduce a novel thought prompting approach
called ``Everything of Thoughts'' (XoT) to defy the law of ``Penrose triangle
of existing thought paradigms. XoT leverages pretrained reinforcement learning
and Monte Carlo Tree Search (MCTS) to incorporate external domain knowledge
into thoughts, thereby enhancing LLMs' capabilities and enabling them to
generalize to unseen problems efficiently. Through the utilization of the
MCTS-LLM collaborative thought revision framework, this approach autonomously
produces high-quality comprehensive cognitive mappings with minimal LLM
interactions. Additionally, XoT empowers LLMs to engage in unconstrained
thinking, allowing for flexible cognitive mappings for problems with multiple
solutions. We evaluate XoT on several challenging multi-solution
problem-solving tasks, including Game of 24, 8-Puzzle, and Pocket Cube. Our
results demonstrate that XoT significantly outperforms existing approaches.
Notably, XoT can yield multiple solutions with just one LLM call, showcasing
its remarkable proficiency in addressing complex problems across diverse
domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04418">AI-accelerated Discovery of Altermagnetic Materials. (arXiv:2311.04418v2 [cond-mat.mtrl-sci] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Gao_Z/0/1/0/all/0/1">Ze-Feng Gao</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Qu_S/0/1/0/all/0/1">Shuai Qu</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Zeng_B/0/1/0/all/0/1">Bocheng Zeng</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Wen_J/0/1/0/all/0/1">Ji-Rong Wen</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Sun_H/0/1/0/all/0/1">Hao Sun</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Guo_P/0/1/0/all/0/1">Peng-Jie Guo</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Lu_Z/0/1/0/all/0/1">Zhong-Yi Lu</a></p>
<p>Altermagnetism, a new magnetic phase, has been theoretically proposed and
experimentally verified to be distinct from ferromagnetism and
antiferromagnetism. Although altermagnets have been found to possess many
exotic physical properties, the very limited availability of known
altermagnetic materials (e.g., 14 confirmed materials) hinders the study of
such properties. Hence, discovering more types of altermagnetic materials is
crucial for a comprehensive understanding of altermagnetism and thus
facilitating new applications in the next-generation information technologies,
e.g., storage devices and high-sensitivity sensors. Here, we report 25 new
altermagnetic materials that cover metals, semiconductors, and insulators,
discovered by an AI search engine unifying symmetry analysis, graph neural
network pre-training, optimal transport theory, and first-principles electronic
structure calculation. The wide range of electronic structural characteristics
reveals that various novel physical properties manifest in these newly
discovered altermagnetic materials, e.g., anomalous Hall effect, anomalous Kerr
effect, and topological property. Noteworthy, we discovered 8 i-wave
altermagnetic materials for the first time. Overall, the AI search engine
performs much better than human experts and suggests a set of new altermagnetic
materials with unique properties, outlining its potential for accelerated
discovery of the materials with targeting properties.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04498">NExT-Chat: An LMM for Chat, Detection and Segmentation. (arXiv:2311.04498v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1">Ao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1">Wei Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1">Tat-Seng Chua</a></p>
<p>The development of large language models (LLMs) has greatly advanced the
field of multimodal understanding, leading to the emergence of large multimodal
models (LMMs). In order to enhance the level of visual comprehension, recent
studies have equipped LMMs with region-level understanding capabilities by
representing object bounding box coordinates as a series of text sequences
(pixel2seq). In this paper, we introduce a novel paradigm for object location
modeling called pixel2emb method, where we ask the LMM to output the location
embeddings and then decoded by different decoders. This paradigm allows for
different location formats (such as bounding boxes and masks) to be used in
multimodal conversations Furthermore, this kind of embedding based location
modeling enables the utilization of existing practices in localization tasks,
such as detection and segmentation. In scenarios with limited resources, our
pixel2emb demonstrates superior performance compared to existing
state-of-the-art (SOTA) approaches in both the location input and output tasks
under fair comparison. Leveraging the proposed pixel2emb method, we train an
LMM named NExT-Chat and demonstrate its capability of handling multiple tasks
like visual grounding, region caption, and grounded reasoning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04850">Rethinking Benchmark and Contamination for Language Models with Rephrased Samples. (arXiv:2311.04850v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Shuo Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chiang_W/0/1/0/all/0/1">Wei-Lin Chiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1">Lianmin Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1">Joseph E. Gonzalez</a>, <a href="http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1">Ion Stoica</a></p>
<p>Large language models are increasingly trained on all the data ever produced
by humans. Many have raised concerns about the trustworthiness of public
benchmarks due to potential contamination in pre-training or fine-tuning
datasets. While most data decontamination efforts apply string matching (e.g.,
n-gram overlap) to remove benchmark data, we show that these methods are
insufficient, and simple variations of test data (e.g., paraphrasing,
translation) can easily bypass these decontamination measures. Furthermore, we
demonstrate that if such variation of test data is not eliminated, a 13B model
can easily overfit a test benchmark and achieve drastically high performance,
on par with GPT-4. We validate such observations in widely used benchmarks such
as MMLU, GSK8k, and HumanEval. To address this growing risk, we propose a
stronger LLM-based decontamination method and apply it to widely used
pre-training and fine-tuning datasets, revealing significant previously unknown
test overlap. For example, in pre-training sets such as RedPajama-Data-1T and
StarCoder-Data, we identified that 8-18\% of the HumanEval benchmark overlaps.
Interestingly, we also find such contamination in synthetic dataset generated
by GPT-3.5/4, suggesting a potential risk of unintentional contamination. We
urge the community to adopt stronger decontamination approaches when using
public benchmarks. Moreover, we call for the community to actively develop
fresh one-time exams to evaluate models accurately. Our decontamination tool is
publicly available at https://github.com/lm-sys/llm-decontaminator.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04913">An Improved Transformer-based Model for Detecting Phishing, Spam, and Ham: A Large Language Model Approach. (arXiv:2311.04913v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jamal_S/0/1/0/all/0/1">Suhaima Jamal</a>, <a href="http://arxiv.org/find/cs/1/au:+Wimmer_H/0/1/0/all/0/1">Hayden Wimmer</a></p>
<p>Phishing and spam detection is long standing challenge that has been the
subject of much academic research. Large Language Models (LLM) have vast
potential to transform society and provide new and innovative approaches to
solve well-established challenges. Phishing and spam have caused financial
hardships and lost time and resources to email users all over the world and
frequently serve as an entry point for ransomware threat actors. While
detection approaches exist, especially heuristic-based approaches, LLMs offer
the potential to venture into a new unexplored area for understanding and
solving this challenge. LLMs have rapidly altered the landscape from business,
consumers, and throughout academia and demonstrate transformational potential
for the potential of society. Based on this, applying these new and innovative
approaches to email detection is a rational next step in academic research. In
this work, we present IPSDM, our model based on fine-tuning the BERT family of
models to specifically detect phishing and spam email. We demonstrate our
fine-tuned version, IPSDM, is able to better classify emails in both unbalanced
and balanced datasets. This work serves as an important first step towards
employing LLMs to improve the security of our information systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04943">MathNAS: If Blocks Have a Role in Mathematical Architecture Design. (arXiv:2311.04943v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qinsi_W/0/1/0/all/0/1">Wang Qinsi</a>, <a href="http://arxiv.org/find/cs/1/au:+Jinghan_K/0/1/0/all/0/1">Ke Jinghan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhi_L/0/1/0/all/0/1">Liang Zhi</a>, <a href="http://arxiv.org/find/cs/1/au:+Sihai_Z/0/1/0/all/0/1">Zhang Sihai</a></p>
<p>Neural Architecture Search (NAS) has emerged as a favoured method for
unearthing effective neural architectures. Recent development of large models
has intensified the demand for faster search speeds and more accurate search
results. However, designing large models by NAS is challenging due to the
dramatical increase of search space and the associated huge performance
evaluation cost. Consider a typical modular search space widely used in NAS, in
which a neural architecture consists of $m$ block nodes and a block node has
$n$ alternative blocks. Facing the space containing $n^m$ candidate networks,
existing NAS methods attempt to find the best one by searching and evaluating
candidate networks directly.Different from the general strategy that takes
architecture search as a whole problem, we propose a novel divide-and-conquer
strategy by making use of the modular nature of the search space.Here, we
introduce MathNAS, a general NAS framework based on mathematical programming.In
MathNAS, the performances of the $m*n$ possible building blocks in the search
space are calculated first, and then the performance of a network is directly
predicted based on the performances of its building blocks. Although estimating
block performances involves network training, just as what happens for network
performance evaluation in existing NAS methods, predicting network performance
is completely training-free and thus extremely fast. In contrast to the $n^m$
candidate networks to evaluate in existing NAS methods, which require training
and a formidable computational burden, there are only $m*n$ possible blocks to
handle in MathNAS. Therefore, our approach effectively reduces the complexity
of network performance evaluation.Our code is available at
https://github.com/wangqinsi1/MathNAS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05304">Data Valuation and Detections in Federated Learning. (arXiv:2311.05304v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wenqian Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1">Shuran Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1">Fengrui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1">Yan Pang</a></p>
<p>Federated Learning (FL) enables collaborative model training while preserving
the privacy of raw data. A challenge in this framework is the fair and
efficient valuation of data, which is crucial for incentivizing clients to
contribute high-quality data in the FL task. In scenarios involving numerous
data clients within FL, it is often the case that only a subset of clients and
datasets are pertinent to a specific learning task, while others might have
either a negative or negligible impact on the model training process. This
paper introduces a novel privacy-preserving method for evaluating client
contributions and selecting relevant datasets without a pre-specified training
algorithm in an FL task. Our proposed approach FedBary, utilizes Wasserstein
distance within the federated context, offering a new solution for data
valuation in the FL framework. This method ensures transparent data valuation
and efficient computation of the Wasserstein barycenter and reduces the
dependence on validation datasets. Through extensive empirical experiments and
theoretical analyses, we demonstrate the potential of this data valuation
method as a promising avenue for FL research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05553">Removing RLHF Protections in GPT-4 via Fine-Tuning. (arXiv:2311.05553v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhan_Q/0/1/0/all/0/1">Qiusi Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_R/0/1/0/all/0/1">Richard Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bindu_R/0/1/0/all/0/1">Rohan Bindu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1">Akul Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1">Tatsunori Hashimoto</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1">Daniel Kang</a></p>
<p>As large language models (LLMs) have increased in their capabilities, so does
their potential for dual use. To reduce harmful outputs, produces and vendors
of LLMs have used reinforcement learning with human feedback (RLHF). In tandem,
LLM vendors have been increasingly enabling fine-tuning of their most powerful
models. However, concurrent work has shown that fine-tuning can remove RLHF
protections. We may expect that the most powerful models currently available
(GPT-4) are less susceptible to fine-tuning attacks.
</p>
<p>In this work, we show the contrary: fine-tuning allows attackers to remove
RLHF protections with as few as 340 examples and a 95% success rate. These
training examples can be automatically generated with weaker models. We further
show that removing RLHF protections does not decrease usefulness on
non-censored outputs, providing evidence that our fine-tuning strategy does not
decrease usefulness despite using weaker models to generate training data. Our
results show the need for further research on protections on LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.08493">Time Travel in LLMs: Tracing Data Contamination in Large Language Models. (arXiv:2308.08493v2 [cs.CL] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Golchin_S/0/1/0/all/0/1">Shahriar Golchin</a>, <a href="http://arxiv.org/find/cs/1/au:+Surdeanu_M/0/1/0/all/0/1">Mihai Surdeanu</a></p>
<p>Data contamination, i.e., the presence of test data from downstream tasks in
the training data of large language models (LLMs), is a potential major issue
in measuring LLMs' real effectiveness on other tasks. We propose a
straightforward yet effective method for identifying data contamination within
LLMs. At its core, our approach starts by identifying potential contamination
at the instance level; using this information, our approach then assesses wider
contamination at the partition level. To estimate contamination of individual
instances, we employ "guided instruction:" a prompt consisting of the dataset
name, partition type, and the random-length initial segment of a reference
instance, asking the LLM to complete it. An instance is flagged as contaminated
if the LLM's output either exactly or nearly matches the latter segment of the
reference. To understand if an entire partition is contaminated, we propose two
ideas. The first idea marks a dataset partition as contaminated if the average
overlap score with the reference instances (as measured by ROUGE-L or BLEURT)
is statistically significantly better with the completions from guided
instruction compared to a "general instruction" that does not include the
dataset and partition name. The second idea marks a dataset partition as
contaminated if a classifier based on GPT-4 with few-shot in-context learning
prompt marks multiple generated completions as exact/near-exact matches of the
corresponding reference instances. Our best method achieves an accuracy between
92% and 100% in detecting if an LLM is contaminated with seven datasets,
containing train and test/validation partitions, when contrasted with manual
evaluation by human experts. Further, our findings indicate that GPT-4 is
contaminated with AG News, WNLI, and XSum datasets.
</p>
</p>
</div>

    </div>
    </body>
    