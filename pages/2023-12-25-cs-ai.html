<!DOCTYPE html>
<html>
<head>
<title>2023-12-25-cs-ai</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2312.13301">SimQ-NAS: Simultaneous Quantization Policy and Neural Architecture Search. (arXiv:2312.13301v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sridhar_S/0/1/0/all/0/1">Sharath Nittur Sridhar</a>, <a href="http://arxiv.org/find/cs/1/au:+Szankin_M/0/1/0/all/0/1">Maciej Szankin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1">Fang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sundaresan_S/0/1/0/all/0/1">Sairam Sundaresan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarah_A/0/1/0/all/0/1">Anthony Sarah</a></p>
<p>Recent one-shot Neural Architecture Search algorithms rely on training a
hardware-agnostic super-network tailored to a specific task and then extracting
efficient sub-networks for different hardware platforms. Popular approaches
separate the training of super-networks from the search for sub-networks, often
employing predictors to alleviate the computational overhead associated with
search. Additionally, certain methods also incorporate the quantization policy
within the search space. However, while the quantization policy search for
convolutional neural networks is well studied, the extension of these methods
to transformers and especially foundation models remains under-explored. In
this paper, we demonstrate that by using multi-objective search algorithms
paired with lightly trained predictors, we can efficiently search for both the
sub-network architecture and the corresponding quantization policy and
outperform their respective baselines across different performance objectives
such as accuracy, model size, and latency. Specifically, we demonstrate that
our approach performs well across both uni-modal (ViT and BERT) and multi-modal
(BEiT-3) transformer-based architectures as well as convolutional architectures
(ResNet). For certain networks, we demonstrate an improvement of up to $4.80x$
and $3.44x$ for latency and model size respectively, without degradation in
accuracy compared to the fully quantized INT8 baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13303">RealGen: Retrieval Augmented Generation for Controllable Traffic Scenarios. (arXiv:2312.13303v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1">Wenhao Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yulong Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1">Ding Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Chaowei Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1">Marco Pavone</a></p>
<p>Simulation plays a crucial role in the development of autonomous vehicles
(AVs) due to the potential risks associated with real-world testing. Although
significant progress has been made in the visual aspects of simulators,
generating complex behavior among agents remains a formidable challenge. It is
not only imperative to ensure realism in the scenarios generated but also
essential to incorporate preferences and conditions to facilitate controllable
generation for AV training and evaluation. Traditional methods, mainly relying
on memorizing the distribution of training datasets, often fall short in
generating unseen scenarios. Inspired by the success of retrieval augmented
generation in large language models, we present RealGen, a novel
retrieval-based in-context learning framework for traffic scenario generation.
RealGen synthesizes new scenarios by combining behaviors from multiple
retrieved examples in a gradient-free way, which may originate from templates
or tagged scenarios. This in-context learning framework endows versatile
generative capabilities, including the ability to edit scenarios, compose
various behaviors, and produce critical scenarios. Evaluations show that
RealGen offers considerable flexibility and controllability, marking a new
direction in the field of controllable traffic scenario generation. Check our
project website for more information: https://realgen.github.io.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13306">Towards Fair Graph Federated Learning via Incentive Mechanisms. (arXiv:2312.13306v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1">Chenglu Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jiarong Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yue Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Ziqi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1">Qingbiao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chunping Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yang Yang</a></p>
<p>Graph federated learning (FL) has emerged as a pivotal paradigm enabling
multiple agents to collaboratively train a graph model while preserving local
data privacy. Yet, current efforts overlook a key issue: agents are
self-interested and would hesitant to share data without fair and satisfactory
incentives. This paper is the first endeavor to address this issue by studying
the incentive mechanism for graph federated learning. We identify a unique
phenomenon in graph federated learning: the presence of agents posing potential
harm to the federation and agents contributing with delays. This stands in
contrast to previous FL incentive mechanisms that assume all agents contribute
positively and in a timely manner. In view of this, this paper presents a novel
incentive mechanism tailored for fair graph federated learning, integrating
incentives derived from both model gradient and payoff. To achieve this, we
first introduce an agent valuation function aimed at quantifying agent
contributions through the introduction of two criteria: gradient alignment and
graph diversity. Moreover, due to the high heterogeneity in graph federated
learning, striking a balance between accuracy and fairness becomes particularly
crucial. We introduce motif prototypes to enhance accuracy, communicated
between the server and agents, enhancing global model aggregation and aiding
agents in local model optimization. Extensive experiments show that our model
achieves the best trade-off between accuracy and the fairness of model
gradient, as well as superior payoff fairness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13307">Not All Steps are Equal: Efficient Generation with Progressive Diffusion Models. (arXiv:2312.13307v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wenhao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1">Xiu Su</a>, <a href="http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1">Shan You</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1">Tao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Fei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1">Chen Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Chang Xu</a></p>
<p>Diffusion models have demonstrated remarkable efficacy in various generative
tasks with the predictive prowess of denoising model. Currently, these models
employ a uniform denoising approach across all timesteps. However, the inherent
variations in noisy latents at each timestep lead to conflicts during training,
constraining the potential of diffusion models. To address this challenge, we
propose a novel two-stage training strategy termed Step-Adaptive Training. In
the initial stage, a base denoising model is trained to encompass all
timesteps. Subsequently, we partition the timesteps into distinct groups,
fine-tuning the model within each group to achieve specialized denoising
capabilities. Recognizing that the difficulties of predicting noise at
different timesteps vary, we introduce a diverse model size requirement. We
dynamically adjust the model size for each timestep by estimating task
difficulty based on its signal-to-noise ratio before fine-tuning. This
adjustment is facilitated by a proxy-based structural importance assessment
mechanism, enabling precise and efficient pruning of the base denoising model.
Our experiments validate the effectiveness of the proposed training strategy,
demonstrating an improvement in the FID score on CIFAR10 by over 0.3 while
utilizing only 80\% of the computational resources. This innovative approach
not only enhances model performance but also significantly reduces
computational costs, opening new avenues for the development and application of
diffusion models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13309">Generate E-commerce Product Background by Integrating Category Commonality and Personalized Style. (arXiv:2312.13309v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haohan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1">Wei Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yang Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yaoyu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1">Jingjing Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xin Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1">Junjie Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zhangang Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Bo_L/0/1/0/all/0/1">Lixing Bo</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1">Jingping Shao</a></p>
<p>The state-of-the-art methods for e-commerce product background generation
suffer from the inefficiency of designing product-wise prompts when scaling up
the production, as well as the ineffectiveness of describing fine-grained
styles when customizing personalized backgrounds for some specific brands. To
address these obstacles, we integrate the category commonality and personalized
style into diffusion models. Concretely, we propose a Category-Wise Generator
to enable large-scale background generation for the first time. A unique
identifier in the prompt is assigned to each category, whose attention is
located on the background by a mask-guided cross attention layer to learn the
category-wise style. Furthermore, for products with specific and fine-grained
requirements in layout, elements, etc, a Personality-Wise Generator is devised
to learn such personalized style directly from a reference image to resolve
textual ambiguities, and is trained in a self-supervised manner for more
efficient training data usage. To advance research in this field, the first
large-scale e-commerce product background generation dataset BG60k is
constructed, which covers more than 60k product images from over 2k categories.
Experiments demonstrate that our method could generate high-quality backgrounds
for different categories, and maintain the personalized background style of
reference images. The link to BG60k and codes will be available soon.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13314">Unlocking Pre-trained Image Backbones for Semantic Image Synthesis. (arXiv:2312.13314v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Berrada_T/0/1/0/all/0/1">Tariq Berrada</a>, <a href="http://arxiv.org/find/cs/1/au:+Verbeek_J/0/1/0/all/0/1">Jakob Verbeek</a>, <a href="http://arxiv.org/find/cs/1/au:+Couprie_C/0/1/0/all/0/1">Camille Couprie</a>, <a href="http://arxiv.org/find/cs/1/au:+Alahari_K/0/1/0/all/0/1">Karteek Alahari</a></p>
<p>Semantic image synthesis, i.e., generating images from user-provided semantic
label maps, is an important conditional image generation task as it allows to
control both the content as well as the spatial layout of generated images.
Although diffusion models have pushed the state of the art in generative image
modeling, the iterative nature of their inference process makes them
computationally demanding. Other approaches such as GANs are more efficient as
they only need a single feed-forward pass for generation, but the image quality
tends to suffer on large and diverse datasets. In this work, we propose a new
class of GAN discriminators for semantic image synthesis that generates highly
realistic images by exploiting feature backbone networks pre-trained for tasks
such as image classification. We also introduce a new generator architecture
with better context modeling and using cross-attention to inject noise into
latent variables, leading to more diverse generated images. Our model, which we
dub DP-SIMS, achieves state-of-the-art results in terms of image quality and
consistency with the input label maps on ADE-20K, COCO-Stuff, and Cityscapes,
surpassing recent diffusion models while requiring two orders of magnitude less
compute for inference.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13322">Domain-Specific Code Language Models: Unraveling the Potential for HPC Codes and Tasks. (arXiv:2312.13322v1 [cs.PL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kadosh_T/0/1/0/all/0/1">Tal Kadosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasabnis_N/0/1/0/all/0/1">Niranjan Hasabnis</a>, <a href="http://arxiv.org/find/cs/1/au:+Vo_V/0/1/0/all/0/1">Vy A. Vo</a>, <a href="http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1">Nadav Schneider</a>, <a href="http://arxiv.org/find/cs/1/au:+Krien_N/0/1/0/all/0/1">Neva Krien</a>, <a href="http://arxiv.org/find/cs/1/au:+Capota_M/0/1/0/all/0/1">Mihai Capota</a>, <a href="http://arxiv.org/find/cs/1/au:+Wasay_A/0/1/0/all/0/1">Abdul Wasay</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_N/0/1/0/all/0/1">Nesreen Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Willke_T/0/1/0/all/0/1">Ted Willke</a>, <a href="http://arxiv.org/find/cs/1/au:+Tamir_G/0/1/0/all/0/1">Guy Tamir</a>, <a href="http://arxiv.org/find/cs/1/au:+Pinter_Y/0/1/0/all/0/1">Yuval Pinter</a>, <a href="http://arxiv.org/find/cs/1/au:+Mattson_T/0/1/0/all/0/1">Timothy Mattson</a>, <a href="http://arxiv.org/find/cs/1/au:+Oren_G/0/1/0/all/0/1">Gal Oren</a></p>
<p>With easier access to powerful compute resources, there is a growing trend in
AI for software development to develop larger language models (LLMs) to address
a variety of programming tasks. Even LLMs applied to tasks from the
high-performance computing (HPC) domain are huge in size and demand expensive
compute resources for training. This is partly because these LLMs for HPC tasks
are obtained by finetuning existing LLMs that support several natural and/or
programming languages. We found this design choice confusing - why do we need
large LMs trained on natural languages and programming languages unrelated to
HPC for HPC-specific tasks?
</p>
<p>In this line of work, we aim to question choices made by existing LLMs by
developing smaller LMs for specific domains - we call them domain-specific LMs.
Specifically, we start off with HPC as a domain and build an HPC-specific LM,
named MonoCoder, that is orders of magnitude smaller than existing LMs but
delivers similar, if not better performance, on non-HPC and HPC tasks.
Specifically, we pre-trained MonoCoder on an HPC-specific dataset (named
HPCorpus) of C and C++ programs mined from GitHub. We evaluated the performance
of MonoCoder against conventional multi-lingual LLMs. Results demonstrate that
MonoCoder, although much smaller than existing LMs, achieves similar results on
normalized-perplexity tests and much better ones in CodeBLEU competence for
high-performance and parallel code generations. Furthermore, fine-tuning the
base model for the specific task of parallel code generation (OpenMP parallel
for pragmas) demonstrates outstanding results compared to GPT, especially when
local misleading semantics are removed by our novel pre-processor Tokompiler,
showcasing the ability of domain-specific models to assist in HPC-relevant
tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13327">In-Context Reinforcement Learning for Variable Action Spaces. (arXiv:2312.13327v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sinii_V/0/1/0/all/0/1">Viacheslav Sinii</a>, <a href="http://arxiv.org/find/cs/1/au:+Nikulin_A/0/1/0/all/0/1">Alexander Nikulin</a>, <a href="http://arxiv.org/find/cs/1/au:+Kurenkov_V/0/1/0/all/0/1">Vladislav Kurenkov</a>, <a href="http://arxiv.org/find/cs/1/au:+Zisman_I/0/1/0/all/0/1">Ilya Zisman</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolesnikov_S/0/1/0/all/0/1">Sergey Kolesnikov</a></p>
<p>Recent work has shown that supervised pre-training on learning histories of
RL algorithms results in a model that captures the learning process and is able
to improve in-context on novel tasks through interactions with an environment.
Despite the progress in this area, there is still a gap in the existing
literature, particularly in the in-context generalization to new action spaces.
While existing methods show high performance on new tasks created by different
reward distributions, their architectural design and training process are not
suited for the introduction of new actions during evaluation. We aim to bridge
this gap by developing an architecture and training methodology specifically
for the task of generalizing to new action spaces. Inspired by Headless LLM, we
remove the dependence on the number of actions by directly predicting the
action embeddings. Furthermore, we use random embeddings to force the semantic
inference of actions from context and to prepare for the new unseen embeddings
during test time. Using multi-armed bandit environments with a variable number
of arms, we show that our model achieves the performance of the data generation
algorithm without requiring retraining for each new environment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13334">Transparency and Privacy: The Role of Explainable AI and Federated Learning in Financial Fraud Detection. (arXiv:2312.13334v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Awosika_T/0/1/0/all/0/1">Tomisin Awosika</a>, <a href="http://arxiv.org/find/cs/1/au:+Shukla_R/0/1/0/all/0/1">Raj Mani Shukla</a>, <a href="http://arxiv.org/find/cs/1/au:+Pranggono_B/0/1/0/all/0/1">Bernardi Pranggono</a></p>
<p>Fraudulent transactions and how to detect them remain a significant problem
for financial institutions around the world. The need for advanced fraud
detection systems to safeguard assets and maintain customer trust is paramount
for financial institutions, but some factors make the development of effective
and efficient fraud detection systems a challenge. One of such factors is the
fact that fraudulent transactions are rare and that many transaction datasets
are imbalanced; that is, there are fewer significant samples of fraudulent
transactions than legitimate ones. This data imbalance can affect the
performance or reliability of the fraud detection model. Moreover, due to the
data privacy laws that all financial institutions are subject to follow,
sharing customer data to facilitate a higher-performing centralized model is
impossible. Furthermore, the fraud detection technique should be transparent so
that it does not affect the user experience. Hence, this research introduces a
novel approach using Federated Learning (FL) and Explainable AI (XAI) to
address these challenges. FL enables financial institutions to collaboratively
train a model to detect fraudulent transactions without directly sharing
customer data, thereby preserving data privacy and confidentiality. Meanwhile,
the integration of XAI ensures that the predictions made by the model can be
understood and interpreted by human experts, adding a layer of transparency and
trust to the system. Experimental results, based on realistic transaction
datasets, reveal that the FL-based fraud detection system consistently
demonstrates high performance metrics. This study grounds FL's potential as an
effective and privacy-preserving tool in the fight against fraud.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13382">DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines. (arXiv:2312.13382v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singhvi_A/0/1/0/all/0/1">Arnav Singhvi</a>, <a href="http://arxiv.org/find/cs/1/au:+Shetty_M/0/1/0/all/0/1">Manish Shetty</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1">Shangyin Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1">Christopher Potts</a>, <a href="http://arxiv.org/find/cs/1/au:+Sen_K/0/1/0/all/0/1">Koushik Sen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1">Matei Zaharia</a>, <a href="http://arxiv.org/find/cs/1/au:+Khattab_O/0/1/0/all/0/1">Omar Khattab</a></p>
<p>Chaining language model (LM) calls as composable modules is fueling a new
powerful way of programming. However, ensuring that LMs adhere to important
constraints remains a key challenge, one often addressed with heuristic "prompt
engineering". We introduce LM Assertions, a new programming construct for
expressing computational constraints that LMs should satisfy. We integrate our
constructs into the recent DSPy programming model for LMs, and present new
strategies that allow DSPy to compile programs with arbitrary LM Assertions
into systems that are more reliable and more accurate. In DSPy, LM Assertions
can be integrated at compile time, via automatic prompt optimization, and/or at
inference time, via automatic selfrefinement and backtracking. We report on two
early case studies for complex question answering (QA), in which the LM program
must iteratively retrieve information in multiple hops and synthesize a
long-form answer with citations. We find that LM Assertions improve not only
compliance with imposed rules and guidelines but also enhance downstream task
performance, delivering intrinsic and extrinsic gains up to 35.7% and 13.3%,
respectively. Our reference implementation of LM Assertions is integrated into
DSPy at https://github.com/stanfordnlp/dspy
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13434">Zero-1-to-3: Domain-level Zero-shot Cognitive Diagnosis via One Batch of Early-bird Students towards Three Diagnostic Objectives. (arXiv:2312.13434v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1">Weibo Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yue_L/0/1/0/all/0/1">Linan Yue</a>, <a href="http://arxiv.org/find/cs/1/au:+Bi_H/0/1/0/all/0/1">Haoyang Bi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1">Yin Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_F/0/1/0/all/0/1">Fangzhou Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zheng Zhangm Xin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yuanjing He</a></p>
<p>Cognitive diagnosis seeks to estimate the cognitive states of students by
exploring their logged practice quiz data. It plays a pivotal role in
personalized learning guidance within intelligent education systems. In this
paper, we focus on an important, practical, yet often underexplored task:
domain-level zero-shot cognitive diagnosis (DZCD), which arises due to the
absence of student practice logs in newly launched domains. Recent cross-domain
diagnostic models have been demonstrated to be a promising strategy for DZCD.
These methods primarily focus on how to transfer student states across domains.
However, they might inadvertently incorporate non-transferable information into
student representations, thereby limiting the efficacy of knowledge transfer.
To tackle this, we propose Zero-1-to-3, a domain-level zero-shot cognitive
diagnosis framework via one batch of early-bird students towards three
diagnostic objectives. Our approach initiates with pre-training a diagnosis
model with dual regularizers, which decouples student states into domain-shared
and domain-specific parts. The shared cognitive signals can be transferred to
the target domain, enriching the cognitive priors for the new domain, which
ensures the cognitive state propagation objective. Subsequently, we devise a
strategy to generate simulated practice logs for cold-start students through
analyzing the behavioral patterns from early-bird students, fulfilling the
domain-adaption goal. Consequently, we refine the cognitive states of
cold-start students as diagnostic outcomes via virtual data, aligning with the
diagnosis-oriented goal. Finally, extensive experiments on six real-world
datasets highlight the efficacy of our model for DZCD and its practical
application in question recommendation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13435">Adversarial Markov Games: On Adaptive Decision-Based Attacks and Defenses. (arXiv:2312.13435v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tsingenopoulos_I/0/1/0/all/0/1">Ilias Tsingenopoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Rimmer_V/0/1/0/all/0/1">Vera Rimmer</a>, <a href="http://arxiv.org/find/cs/1/au:+Preuveneers_D/0/1/0/all/0/1">Davy Preuveneers</a>, <a href="http://arxiv.org/find/cs/1/au:+Pierazzi_F/0/1/0/all/0/1">Fabio Pierazzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Cavallaro_L/0/1/0/all/0/1">Lorenzo Cavallaro</a>, <a href="http://arxiv.org/find/cs/1/au:+Joosen_W/0/1/0/all/0/1">Wouter Joosen</a></p>
<p>Despite considerable efforts on making them robust, real-world ML-based
systems remain vulnerable to decision based attacks, as definitive proofs of
their operational robustness have so far proven intractable. The canonical
approach in robustness evaluation calls for adaptive attacks, that is with
complete knowledge of the defense and tailored to bypass it. In this study, we
introduce a more expansive notion of being adaptive and show how attacks but
also defenses can benefit by it and by learning from each other through
interaction. We propose and evaluate a framework for adaptively optimizing
black-box attacks and defenses against each other through the competitive game
they form. To reliably measure robustness, it is important to evaluate against
realistic and worst-case attacks. We thus augment both attacks and the evasive
arsenal at their disposal through adaptive control, and observe that the same
can be done for defenses, before we evaluate them first apart and then jointly
under a multi-agent perspective. We demonstrate that active defenses, which
control how the system responds, are a necessary complement to model hardening
when facing decision-based attacks; then how these defenses can be circumvented
by adaptive attacks, only to finally elicit active and adaptive defenses. We
validate our observations through a wide theoretical and empirical
investigation to confirm that AI-enabled adversaries pose a considerable threat
to black-box ML-based systems, rekindling the proverbial arms race where
defenses have to be AI-enabled too. Succinctly, we address the challenges posed
by adaptive adversaries and develop adaptive defenses, thereby laying out
effective strategies in ensuring the robustness of ML-based systems deployed in
the real-world.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13476">Fortify Your Defenses: Strategic Budget Allocation to Enhance Power Grid Cybersecurity. (arXiv:2312.13476v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Meyur_R/0/1/0/all/0/1">Rounak Meyur</a>, <a href="http://arxiv.org/find/cs/1/au:+Purohit_S/0/1/0/all/0/1">Sumit Purohit</a>, <a href="http://arxiv.org/find/cs/1/au:+Webb_B/0/1/0/all/0/1">Braden K. Webb</a></p>
<p>The abundance of cyber-physical components in modern day power grid with
their diverse hardware and software vulnerabilities has made it difficult to
protect them from advanced persistent threats (APTs). An attack graph depicting
the propagation of potential cyber-attack sequences from the initial access
point to the end objective is vital to identify critical weaknesses of any
cyber-physical system. A cyber security personnel can accordingly plan
preventive mitigation measures for the identified weaknesses addressing the
cyber-attack sequences. However, limitations on available cybersecurity budget
restrict the choice of mitigation measures. We address this aspect through our
framework, which solves the following problem: given potential cyber-attack
sequences for a cyber-physical component in the power grid, find the optimal
manner to allocate an available budget to implement necessary preventive
mitigation measures. We formulate the problem as a mixed integer linear program
(MILP) to identify the optimal budget partition and set of mitigation measures
which minimize the vulnerability of cyber-physical components to potential
attack sequences. We assume that the allocation of budget affects the efficacy
of the mitigation measures. We show how altering the budget allocation for
tasks such as asset management, cybersecurity infrastructure improvement,
incident response planning and employee training affects the choice of the
optimal set of preventive mitigation measures and modifies the associated
cybersecurity risk. The proposed framework can be used by cyber policymakers
and system owners to allocate optimal budgets for various tasks required to
improve the overall security of a cyber-physical system.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13487">Understanding and Estimating Domain Complexity Across Domains. (arXiv:2312.13487v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Doctor_K/0/1/0/all/0/1">Katarina Doctor</a>, <a href="http://arxiv.org/find/cs/1/au:+Kejriwal_M/0/1/0/all/0/1">Mayank Kejriwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Holder_L/0/1/0/all/0/1">Lawrence Holder</a>, <a href="http://arxiv.org/find/cs/1/au:+Kildebeck_E/0/1/0/all/0/1">Eric Kildebeck</a>, <a href="http://arxiv.org/find/cs/1/au:+Resmini_E/0/1/0/all/0/1">Emma Resmini</a>, <a href="http://arxiv.org/find/cs/1/au:+Pereyda_C/0/1/0/all/0/1">Christopher Pereyda</a>, <a href="http://arxiv.org/find/cs/1/au:+Steininger_R/0/1/0/all/0/1">Robert J. Steininger</a>, <a href="http://arxiv.org/find/cs/1/au:+Olivenca_D/0/1/0/all/0/1">Daniel V. Oliven&#xe7;a</a></p>
<p>Artificial Intelligence (AI) systems, trained in controlled environments,
often struggle in real-world complexities. We propose a general framework for
estimating domain complexity across diverse environments, like open-world
learning and real-world applications. This framework distinguishes between
intrinsic complexity (inherent to the domain) and extrinsic complexity
(dependent on the AI agent). By analyzing dimensionality, sparsity, and
diversity within these categories, we offer a comprehensive view of domain
challenges. This approach enables quantitative predictions of AI difficulty
during environment transitions, avoids bias in novel situations, and helps
navigate the vast search spaces of open-world domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13495">Decoupling Representation and Knowledge for Few-Shot Intent Classification and Slot Filling. (arXiv:2312.13495v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Jie Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1">Yixiong Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haozhao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1">Ruixuan Li</a></p>
<p>Few-shot intent classification and slot filling are important but challenging
tasks due to the scarcity of finely labeled data. Therefore, current works
first train a model on source domains with sufficiently labeled data, and then
transfer the model to target domains where only rarely labeled data is
available. However, experience transferring as a whole usually suffers from
gaps that exist among source domains and target domains. For instance,
transferring domain-specific-knowledge-related experience is difficult. To
tackle this problem, we propose a new method that explicitly decouples the
transferring of general-semantic-representation-related experience and the
domain-specific-knowledge-related experience. Specifically, for
domain-specific-knowledge-related experience, we design two modules to capture
intent-slot relation and slot-slot relation respectively. Extensive experiments
on Snips and FewJoint datasets show that our method achieves state-of-the-art
performance. The method improves the joint accuracy metric from 27.72% to
42.20% in the 1-shot setting, and from 46.54% to 60.79% in the 5-shot setting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13503">InfoVisDial: An Informative Visual Dialogue Dataset by Bridging Large Multimodal and Language Models. (arXiv:2312.13503v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1">Bingbing Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhengyuan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jianfeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1">Zhe Gan</a>, <a href="http://arxiv.org/find/cs/1/au:+Howe_B/0/1/0/all/0/1">Bill Howe</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lijuan Wang</a></p>
<p>In this paper, we build a visual dialogue dataset, named InfoVisDial, which
provides rich informative answers in each round even with external knowledge
related to the visual content. Different from existing datasets where the
answer is compact and short, InfoVisDial contains long free-form answers with
rich information in each round of dialogue. For effective data collection, the
key idea is to bridge the large-scale multimodal model (e.g., GIT) and the
language models (e.g., GPT-3). GIT can describe the image content even with
scene text, while GPT-3 can generate informative dialogue based on the image
description and appropriate prompting techniques. With such automatic pipeline,
we can readily generate informative visual dialogue data at scale. Then, we ask
human annotators to rate the generated dialogues to filter the low-quality
conversations.Human analyses show that InfoVisDial covers informative and
diverse dialogue topics: $54.4\%$ of the dialogue rounds are related to image
scene texts, and $36.7\%$ require external knowledge. Each round's answer is
also long and open-ended: $87.3\%$ of answers are unique with an average length
of $8.9$, compared with $27.37\%$ and $2.9$ in VisDial. Last, we propose a
strong baseline by adapting the GIT model for the visual dialogue task and
fine-tune the model on InfoVisDial. Hopefully, our work can motivate more
effort on this direction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13506">SPDGAN: A Generative Adversarial Network based on SPD Manifold Learning for Automatic Image Colorization. (arXiv:2312.13506v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mourchid_Y/0/1/0/all/0/1">Youssef Mourchid</a>, <a href="http://arxiv.org/find/cs/1/au:+Donias_M/0/1/0/all/0/1">Marc Donias</a>, <a href="http://arxiv.org/find/cs/1/au:+Berthoumieu_Y/0/1/0/all/0/1">Yannick Berthoumieu</a>, <a href="http://arxiv.org/find/cs/1/au:+Najim_M/0/1/0/all/0/1">Mohamed Najim</a></p>
<p>This paper addresses the automatic colorization problem, which converts a
gray-scale image to a colorized one. Recent deep-learning approaches can
colorize automatically grayscale images. However, when it comes to different
scenes which contain distinct color styles, it is difficult to accurately
capture the color characteristics. In this work, we propose a fully automatic
colorization approach based on Symmetric Positive Definite (SPD) Manifold
Learning with a generative adversarial network (SPDGAN) that improves the
quality of the colorization results. Our SPDGAN model establishes an
adversarial game between two discriminators and a generator. The latter is
based on ResNet architecture with few alterations. Its goal is to generate fake
colorized images without losing color information across layers through
residual connections. Then, we employ two discriminators from different
domains. The first one is devoted to the image pixel domain, while the second
one is to the Riemann manifold domain which helps to avoid color misalignment.
Extensive experiments are conducted on the Places365 and COCO-stuff databases
to test the effect of each component of our SPDGAN. In addition, quantitative
and qualitative comparisons with state-of-the-art methods demonstrate the
effectiveness of our model by achieving more realistic colorized images with
less artifacts visually, and good results of PSNR, SSIM, and FID values.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13508">Multimodal Federated Learning with Missing Modality via Prototype Mask and Contrast. (arXiv:2312.13508v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bao_G/0/1/0/all/0/1">Guangyin Bao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Miao_D/0/1/0/all/0/1">Duoqian Miao</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1">Zixuan Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1">Liang Hu</a></p>
<p>In real-world scenarios, multimodal federated learning often faces the
practical challenge of intricate modality missing, which poses constraints on
building federated frameworks and significantly degrades model inference
accuracy. Existing solutions for addressing missing modalities generally
involve developing modality-specific encoders on clients and training modality
fusion modules on servers. However, these methods are primarily constrained to
specific scenarios with either unimodal clients or complete multimodal clients,
struggling to generalize effectively in the intricate modality missing
scenarios. In this paper, we introduce a prototype library into the
FedAvg-based Federated Learning framework, thereby empowering the framework
with the capability to alleviate the global model performance degradation
resulting from modality missing during both training and testing. The proposed
method utilizes prototypes as masks representing missing modalities to
formulate a task-calibrated training loss and a model-agnostic uni-modality
inference strategy. In addition, a proximal term based on prototypes is
constructed to enhance local training. Experimental results demonstrate the
state-of-the-art performance of our approach. Compared to the baselines, our
method improved inference accuracy by 3.7\% with 50\% modality missing during
training and by 23.8\% during uni-modality inference. Code is available at
https://github.com/BaoGuangYin/PmcmFL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13521">Using GPT-4 Prompts to Determine Whether Articles Contain Functional Evidence Supporting or Refuting Variant Pathogenicity. (arXiv:2312.13521v1 [q-bio.GN])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Aronson_S/0/1/0/all/0/1">Samuel J. Aronson</a> (1,2), <a href="http://arxiv.org/find/q-bio/1/au:+Machini_K/0/1/0/all/0/1">Kalotina Machini</a> (1,3), <a href="http://arxiv.org/find/q-bio/1/au:+Sriraman_P/0/1/0/all/0/1">Pranav Sriraman</a> (1), <a href="http://arxiv.org/find/q-bio/1/au:+Shin_J/0/1/0/all/0/1">Jiyeon Shin</a> (2), <a href="http://arxiv.org/find/q-bio/1/au:+Henricks_E/0/1/0/all/0/1">Emma R. Henricks</a> (1), <a href="http://arxiv.org/find/q-bio/1/au:+Mailly_C/0/1/0/all/0/1">Charlotte Mailly</a> (1,2), <a href="http://arxiv.org/find/q-bio/1/au:+Nottage_A/0/1/0/all/0/1">Angie J. Nottage</a> (1), <a href="http://arxiv.org/find/q-bio/1/au:+Oates_M/0/1/0/all/0/1">Michael Oates</a> (1,2), <a href="http://arxiv.org/find/q-bio/1/au:+Lebo_M/0/1/0/all/0/1">Matthew S. Lebo</a> (1,3) ((1) Mass Gneral Brigham Personalized Medicine, (2) Accelerator for Clinical Transformation, Mass General Brigham, (3) Department of Pathology, Brigham and Women&#x27;s Hospital)</p>
<p>Purpose: To assess Generative Pre-trained Transformer version 4's (GPT-4)
ability to classify articles containing functional evidence relevant to
assessments of variant pathogenicity.
</p>
<p>Results: GPT-4 settings and prompts were trained on a set of 45 articles and
genetic variants. A final test set of 72 manually classified articles and
genetic variants were then processed using two prompts. The prompts asked GPT-4
to supply all functional evidence present in an article for a variant or
indicate that no functional evidence is present. For articles with having
functional evidence, a second prompt asked GPT-4 to classify the evidence into
pathogenic, benign, intermediate, and inconclusive categories. The first prompt
identified articles with variant-level functional evidence with 87% sensitivity
and 89% positive predictive value (PPV). Five of 26 articles with no functional
data were indicated as having functional evidence by GPT-4. For variants with
functional assays present as determined by both manual review and GPT-4, the
sensitivity and PPV of GPT-4 prompt concordance was: Pathogenic (92% sensitive
and 73% PPV), Intermediate or Inconclusive (67% sensitive and 93% PPV), Benign
(100% sensitive and 73% PPV).
</p>
<p>Conclusion: The GPT-4 prompts detected the presence or absence of a
functional assay with high sensitivity and PPV, and articles with unambiguous
evidence supporting a benign or pathogenic classification with high sensitivity
and reasonable PPV. Our prompts detected papers with intermediate or
inconclusive evidence with lower sensitivity but high PPV. Our results support
that GPT-4 may be useful in variant classification workflows by enabling
prioritization of articles for review that are likely to have functional
evidence supporting or refuting pathogenicity, but not that GPT-4 is capable of
fully automating the genetics literature review component of variant
classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13530">HW-V2W-Map: Hardware Vulnerability to Weakness Mapping Framework for Root Cause Analysis with GPT-assisted Mitigation Suggestion. (arXiv:2312.13530v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yu-Zheng Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Mamun_M/0/1/0/all/0/1">Muntasir Mamun</a>, <a href="http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1">Muhtasim Alam Chowdhury</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1">Shuyu Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1">Mingyu Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Latibari_B/0/1/0/all/0/1">Banafsheh Saber Latibari</a>, <a href="http://arxiv.org/find/cs/1/au:+Gubbi_K/0/1/0/all/0/1">Kevin Immanuel Gubbi</a>, <a href="http://arxiv.org/find/cs/1/au:+Bavarsad_N/0/1/0/all/0/1">Najmeh Nazari Bavarsad</a>, <a href="http://arxiv.org/find/cs/1/au:+Caputo_A/0/1/0/all/0/1">Arjun Caputo</a>, <a href="http://arxiv.org/find/cs/1/au:+Sasan_A/0/1/0/all/0/1">Avesta Sasan</a>, <a href="http://arxiv.org/find/cs/1/au:+Homayoun_H/0/1/0/all/0/1">Houman Homayoun</a>, <a href="http://arxiv.org/find/cs/1/au:+Rafatirad_S/0/1/0/all/0/1">Setareh Rafatirad</a>, <a href="http://arxiv.org/find/cs/1/au:+Satam_P/0/1/0/all/0/1">Pratik Satam</a>, <a href="http://arxiv.org/find/cs/1/au:+Salehi_S/0/1/0/all/0/1">Soheil Salehi</a></p>
<p>The escalating complexity of modern computing frameworks has resulted in a
surge in the cybersecurity vulnerabilities reported to the National
Vulnerability Database (NVD) by practitioners. Despite the fact that the
stature of NVD is one of the most significant databases for the latest insights
into vulnerabilities, extracting meaningful trends from such a large amount of
unstructured data is still challenging without the application of suitable
technological methodologies. Previous efforts have mostly concentrated on
software vulnerabilities; however, a holistic strategy incorporates approaches
for mitigating vulnerabilities, score prediction, and a knowledge-generating
system that may extract relevant insights from the Common Weakness Enumeration
(CWE) and Common Vulnerability Exchange (CVE) databases is notably absent. As
the number of hardware attacks on Internet of Things (IoT) devices continues to
rapidly increase, we present the Hardware Vulnerability to Weakness Mapping
(HW-V2W-Map) Framework, which is a Machine Learning (ML) framework focusing on
hardware vulnerabilities and IoT security. The architecture that we have
proposed incorporates an Ontology-driven Storytelling framework, which
automates the process of updating the ontology in order to recognize patterns
and evolution of vulnerabilities over time and provides approaches for
mitigating the vulnerabilities. The repercussions of vulnerabilities can be
mitigated as a result of this, and conversely, future exposures can be
predicted and prevented. Furthermore, our proposed framework utilized
Generative Pre-trained Transformer (GPT) Large Language Models (LLMs) to
provide mitigation suggestions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13536">Domain Adaptive Graph Classification. (arXiv:2312.13536v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1">Siyang Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1">Ziyi Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhenghan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1">Xiaoxuan Liang</a></p>
<p>Despite the remarkable accomplishments of graph neural networks (GNNs), they
typically rely on task-specific labels, posing potential challenges in terms of
their acquisition. Existing work have been made to address this issue through
the lens of unsupervised domain adaptation, wherein labeled source graphs are
utilized to enhance the learning process for target data. However, the
simultaneous exploration of graph topology and reduction of domain disparities
remains a substantial hurdle. In this paper, we introduce the Dual Adversarial
Graph Representation Learning (DAGRL), which explore the graph topology from
dual branches and mitigate domain discrepancies via dual adversarial learning.
Our method encompasses a dual-pronged structure, consisting of a graph
convolutional network branch and a graph kernel branch, which enables us to
capture graph semantics from both implicit and explicit perspectives. Moreover,
our approach incorporates adaptive perturbations into the dual branches, which
align the source and target distribution to address domain discrepancies.
Extensive experiments on a wild range graph classification datasets demonstrate
the effectiveness of our proposed method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13557">Empowering Few-Shot Recommender Systems with Large Language Models -- Enhanced Representations. (arXiv:2312.13557v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhoumeng Wang</a></p>
<p>Recommender systems utilizing explicit feedback have witnessed significant
advancements and widespread applications over the past years. However,
generating recommendations in few-shot scenarios remains a persistent
challenge. Recently, large language models (LLMs) have emerged as a promising
solution for addressing natural language processing (NLP) tasks, thereby
offering novel insights into tackling the few-shot scenarios encountered by
explicit feedback-based recommender systems. To bridge recommender systems and
LLMs, we devise a prompting template that generates user and item
representations based on explicit feedback. Subsequently, we integrate these
LLM-processed representations into various recommendation models to evaluate
their significance across diverse recommendation tasks. Our ablation
experiments and case study analysis collectively demonstrate the effectiveness
of LLMs in processing explicit feedback, highlighting that LLMs equipped with
generative and logical reasoning capabilities can effectively serve as a
component of recommender systems to enhance their performance in few-shot
scenarios. Furthermore, the broad adaptability of LLMs augments the
generalization potential of recommender models, despite certain inherent
constraints. We anticipate that our study can inspire researchers to delve
deeper into the multifaceted dimensions of LLMs's involvement in recommender
systems and contribute to the advancement of the explicit feedback-based
recommender systems field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13558">The Truth is in There: Improving Reasoning in Language Models with Layer-Selective Rank Reduction. (arXiv:2312.13558v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1">Pratyusha Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Ash_J/0/1/0/all/0/1">Jordan T. Ash</a>, <a href="http://arxiv.org/find/cs/1/au:+Misra_D/0/1/0/all/0/1">Dipendra Misra</a></p>
<p>Transformer-based Large Language Models (LLMs) have become a fixture in
modern machine learning. Correspondingly, significant resources are allocated
towards research that aims to further advance this technology, typically
resulting in models of increasing size that are trained on increasing amounts
of data. This work, however, demonstrates the surprising result that it is
often possible to significantly improve the performance of LLMs by selectively
removing higher-order components of their weight matrices. This simple
intervention, which we call LAyer-SElective Rank reduction (LASER), can be done
on a model after training has completed, and requires no additional parameters
or data. We show extensive experiments demonstrating the generality of this
finding across language models and datasets, and provide in-depth analyses
offering insights into both when LASER is effective and the mechanism by which
it operates.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13565">Automatic Curriculum Learning with Gradient Reward Signals. (arXiv:2312.13565v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Campbell_R/0/1/0/all/0/1">Ryan Campbell</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1">Junsang Yoon</a></p>
<p>This paper investigates the impact of using gradient norm reward signals in
the context of Automatic Curriculum Learning (ACL) for deep reinforcement
learning (DRL). We introduce a framework where the teacher model, utilizing the
gradient norm information of a student model, dynamically adapts the learning
curriculum. This approach is based on the hypothesis that gradient norms can
provide a nuanced and effective measure of learning progress. Our experimental
setup involves several reinforcement learning environments (PointMaze, AntMaze,
and AdroitHandRelocate), to assess the efficacy of our method. We analyze how
gradient norm rewards influence the teacher's ability to craft challenging yet
achievable learning sequences, ultimately enhancing the student's performance.
Our results show that this approach not only accelerates the learning process
but also leads to improved generalization and adaptability in complex tasks.
The findings underscore the potential of gradient norm signals in creating more
efficient and robust ACL systems, opening new avenues for research in
curriculum learning and reinforcement learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13583">Fine-tuning Graph Neural Networks by Preserving Graph Generative Patterns. (arXiv:2312.13583v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yifei Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1">Qi Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chunping Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_T/0/1/0/all/0/1">Tianyu Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jiajun Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lei Chen</a></p>
<p>Recently, the paradigm of pre-training and fine-tuning graph neural networks
has been intensively studied and applied in a wide range of graph mining tasks.
Its success is generally attributed to the structural consistency between
pre-training and downstream datasets, which, however, does not hold in many
real-world scenarios. Existing works have shown that the structural divergence
between pre-training and downstream graphs significantly limits the
transferability when using the vanilla fine-tuning strategy. This divergence
leads to model overfitting on pre-training graphs and causes difficulties in
capturing the structural properties of the downstream graphs. In this paper, we
identify the fundamental cause of structural divergence as the discrepancy of
generative patterns between the pre-training and downstream graphs.
Furthermore, we propose G-Tuning to preserve the generative patterns of
downstream graphs. Given a downstream graph G, the core idea is to tune the
pre-trained GNN so that it can reconstruct the generative patterns of G, the
graphon W. However, the exact reconstruction of a graphon is known to be
computationally expensive. To overcome this challenge, we provide a theoretical
analysis that establishes the existence of a set of alternative graphons called
graphon bases for any given graphon. By utilizing a linear combination of these
graphon bases, we can efficiently approximate W. This theoretical finding forms
the basis of our proposed model, as it enables effective learning of the
graphon bases and their associated coefficients. Compared with existing
algorithms, G-Tuning demonstrates an average improvement of 0.5% and 2.6% on
in-domain and out-of-domain transfer learning experiments, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13594">Towards More Faithful Natural Language Explanation Using Multi-Level Contrastive Learning in VQA. (arXiv:2312.13594v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1">Chengen Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1">Shengli Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_S/0/1/0/all/0/1">Shiqi Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jingyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1">Sitong Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_G/0/1/0/all/0/1">Guangneng Hu</a></p>
<p>Natural language explanation in visual question answer (VQA-NLE) aims to
explain the decision-making process of models by generating natural language
sentences to increase users' trust in the black-box systems. Existing post-hoc
methods have achieved significant progress in obtaining a plausible
explanation. However, such post-hoc explanations are not always aligned with
human logical inference, suffering from the issues on: 1) Deductive
unsatisfiability, the generated explanations do not logically lead to the
answer; 2) Factual inconsistency, the model falsifies its counterfactual
explanation for answers without considering the facts in images; and 3)
Semantic perturbation insensitivity, the model can not recognize the semantic
changes caused by small perturbations. These problems reduce the faithfulness
of explanations generated by models. To address the above issues, we propose a
novel self-supervised \textbf{M}ulti-level \textbf{C}ontrastive
\textbf{L}earning based natural language \textbf{E}xplanation model (MCLE) for
VQA with semantic-level, image-level, and instance-level factual and
counterfactual samples. MCLE extracts discriminative features and aligns the
feature spaces from explanations with visual question and answer to generate
more consistent explanations. We conduct extensive experiments, ablation
analysis, and case study to demonstrate the effectiveness of our method on two
VQA-NLE benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13596">Anchoring Path for Inductive Relation Prediction in Knowledge Graphs. (arXiv:2312.13596v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1">Zhixiang Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Di Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1">Chunyan Miao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1">Lizhen Cui</a></p>
<p>Aiming to accurately predict missing edges representing relations between
entities, which are pervasive in real-world Knowledge Graphs (KGs), relation
prediction plays a critical role in enhancing the comprehensiveness and utility
of KGs. Recent research focuses on path-based methods due to their inductive
and explainable properties. However, these methods face a great challenge when
lots of reasoning paths do not form Closed Paths (CPs) in the KG. To address
this challenge, we propose Anchoring Path Sentence Transformer (APST) by
introducing Anchoring Paths (APs) to alleviate the reliance of CPs.
Specifically, we develop a search-based description retrieval method to enrich
entity descriptions and an assessment mechanism to evaluate the rationality of
APs. APST takes both APs and CPs as the inputs of a unified Sentence
Transformer architecture, enabling comprehensive predictions and high-quality
explanations. We evaluate APST on three public datasets and achieve
state-of-the-art (SOTA) performance in 30 of 36 transductive, inductive, and
few-shot experimental settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13616">Navigating the Structured What-If Spaces: Counterfactual Generation via Structured Diffusion. (arXiv:2312.13616v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Madaan_N/0/1/0/all/0/1">Nishtha Madaan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bedathur_S/0/1/0/all/0/1">Srikanta Bedathur</a></p>
<p>Generating counterfactual explanations is one of the most effective
approaches for uncovering the inner workings of black-box neural network models
and building user trust. While remarkable strides have been made in generative
modeling using diffusion models in domains like vision, their utility in
generating counterfactual explanations in structured modalities remains
unexplored. In this paper, we introduce Structured Counterfactual Diffuser or
SCD, the first plug-and-play framework leveraging diffusion for generating
counterfactual explanations in structured data. SCD learns the underlying data
distribution via a diffusion model which is then guided at test time to
generate counterfactuals for any arbitrary black-box model, input, and desired
prediction. Our experiments show that our counterfactuals not only exhibit high
plausibility compared to the existing state-of-the-art but also show
significantly better proximity and diversity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13632">ProvFL: Client-Driven Interpretability of Global Model Predictions in Federated Learning. (arXiv:2312.13632v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gill_W/0/1/0/all/0/1">Waris Gill</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Anwar_A/0/1/0/all/0/1">Ali Anwar</a> (2), <a href="http://arxiv.org/find/cs/1/au:+Gulzar_M/0/1/0/all/0/1">Muhammad Ali Gulzar</a> (1) ((1) Virginia Tech, (2) University of Minnesota Twin Cities)</p>
<p>Federated Learning (FL) trains a collaborative machine learning model by
aggregating multiple privately trained clients' models over several training
rounds. Such a long, continuous action of model aggregations poses significant
challenges in reasoning about the origin and composition of such a global
model. Regardless of the quality of the global model or if it has a fault,
understanding the model's origin is equally important for debugging,
interpretability, and explainability in federated learning. FL application
developers often question: (1) what clients contributed towards a global model
and (2) if a global model predicts a label, which clients are responsible for
it?
</p>
<p>We introduce, neuron provenance, a fine-grained lineage capturing mechanism
that tracks the flow of information between the individual participating
clients in FL and the final global model. We operationalize this concept in
ProvFL that functions on two key principles. First, recognizing that monitoring
every neuron of every client's model statically is ineffective and noisy due to
the uninterpretable nature of individual neurons, ProvFL dynamically isolates
influential and sensitive neurons in the global model, significantly reducing
the search space. Second, as multiple clients' models are fused in each round
to form a global model, tracking each client's contribution becomes
challenging. ProvFL leverages the invertible nature of fusion algorithms to
precisely isolate each client's contribution derived from selected neurons.
When asked to localize the clients responsible for the given behavior (i.e.,
prediction) of the global model, ProvFL successfully localizes them with an
average provenance accuracy of 97%. Additionally, ProvFL outperforms the
state-of-the-art FL fault localization approach by an average margin of 50%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13655">Compositional Zero-Shot Learning for Attribute-Based Object Reference in Human-Robot Interaction. (arXiv:2312.13655v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1">Peng Gao</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Jaafar_A/0/1/0/all/0/1">Ahmed Jaafar</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Reily_B/0/1/0/all/0/1">Brian Reily</a> (2), <a href="http://arxiv.org/find/cs/1/au:+Reardon_C/0/1/0/all/0/1">Christopher Reardon</a> (3), <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hao Zhang</a> (1) ((1) University of Massachusetts Amherst, (2) DEVCOM Army Research Laboratory, (3) University of Denver)</p>
<p>Language-enabled robots have been widely studied over the past years to
enable natural human-robot interaction and teaming in various real-world
applications. Language-enabled robots must be able to comprehend referring
expressions to identify a particular object from visual perception using a set
of referring attributes extracted from natural language. However, visual
observations of an object may not be available when it is referred to, and the
number of objects and attributes may also be unbounded in open worlds. To
address the challenges, we implement an attribute-based compositional zero-shot
learning method that uses a list of attributes to perform referring expression
comprehension in open worlds. We evaluate the approach on two datasets
including the MIT-States and the Clothing 16K. The preliminary experimental
results show that our implemented approach allows a robot to correctly identify
the objects referred to by human commands.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13680">HGE: Embedding Temporal Knowledge Graphs in a Product Space of Heterogeneous Geometric Subspaces. (arXiv:2312.13680v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1">Jiaxin Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Nayyeri_M/0/1/0/all/0/1">Mojtaba Nayyeri</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yinan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Staab_S/0/1/0/all/0/1">Steffen Staab</a></p>
<p>Temporal knowledge graphs represent temporal facts $(s,p,o,\tau)$ relating a
subject $s$ and an object $o$ via a relation label $p$ at time $\tau$, where
$\tau$ could be a time point or time interval. Temporal knowledge graphs may
exhibit static temporal patterns at distinct points in time and dynamic
temporal patterns between different timestamps. In order to learn a rich set of
static and dynamic temporal patterns and apply them for inference, several
embedding approaches have been suggested in the literature. However, as most of
them resort to single underlying embedding spaces, their capability to model
all kinds of temporal patterns was severely limited by having to adhere to the
geometric property of their one embedding space. We lift this limitation by an
embedding approach that maps temporal facts into a product space of several
heterogeneous geometric subspaces with distinct geometric properties, i.e.\
Complex, Dual, and Split-complex spaces. In addition, we propose a
temporal-geometric attention mechanism to integrate information from different
geometric subspaces conveniently according to the captured relational and
temporal information. Experimental results on standard temporal benchmark
datasets favorably evaluate our approach against state-of-the-art models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13682">A Constraint Programming Model for Scheduling the Unloading of Trains in Ports: Extended. (arXiv:2312.13682v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Perez_G/0/1/0/all/0/1">Guillaume Perez</a>, <a href="http://arxiv.org/find/cs/1/au:+Glorian_G/0/1/0/all/0/1">Gael Glorian</a>, <a href="http://arxiv.org/find/cs/1/au:+Suijlen_W/0/1/0/all/0/1">Wijnand Suijlen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lallouet_A/0/1/0/all/0/1">Arnaud Lallouet</a></p>
<p>In this paper, we propose a model to schedule the next 24 hours of operations
in a bulk cargo port to unload bulk cargo trains onto stockpiles. It is a
problem that includes multiple parts such as splitting long trains into shorter
ones and the routing of bulk material through a configurable network of
conveyors to the stockpiles. Managing such trains (up to three kilometers long)
also requires specialized equipment. The real world nature of the problem
specification implies the necessity to manage heterogeneous data. Indeed, when
new equipment is added (e.g. dumpers) or a new type of wagon comes in use,
older or different equipment will still be in use as well. All these details
need to be accounted for. In fact, avoiding a full deadlock of the facility
after a new but ineffective schedule is produced. In this paper, we provide a
detailed presentation of this real world problem and its associated data. This
allows us to propose an effective constraint programming model to solve this
problem. We also discuss the model design and the different implementations of
the propagators that we used in practice. Finally, we show how this model,
coupled with a large neighborhood search, was able to find 24 hour schedules
efficiently.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13716">Critic-Guided Decision Transformer for Offline Reinforcement Learning. (arXiv:2312.13716v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuanfu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Chao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1">Ying Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a></p>
<p>Recent advancements in offline reinforcement learning (RL) have underscored
the capabilities of Return-Conditioned Supervised Learning (RCSL), a paradigm
that learns the action distribution based on target returns for each state in a
supervised manner. However, prevailing RCSL methods largely focus on
deterministic trajectory modeling, disregarding stochastic state transitions
and the diversity of future trajectory distributions. A fundamental challenge
arises from the inconsistency between the sampled returns within individual
trajectories and the expected returns across multiple trajectories.
Fortunately, value-based methods offer a solution by leveraging a value
function to approximate the expected returns, thereby addressing the
inconsistency effectively. Building upon these insights, we propose a novel
approach, termed the Critic-Guided Decision Transformer (CGDT), which combines
the predictability of long-term returns from value-based methods with the
trajectory modeling capability of the Decision Transformer. By incorporating a
learned value function, known as the critic, CGDT ensures a direct alignment
between the specified target returns and the expected returns of actions. This
integration bridges the gap between the deterministic nature of RCSL and the
probabilistic characteristics of value-based methods. Empirical evaluations on
stochastic environments and D4RL benchmark datasets demonstrate the superiority
of CGDT over traditional RCSL methods. These results highlight the potential of
CGDT to advance the state of the art in offline RL and extend the applicability
of RCSL to a wide range of RL tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13752">Hunting imaging biomarkers in pulmonary fibrosis: Benchmarks of the AIIB23 challenge. (arXiv:2312.13752v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Nan_Y/0/1/0/all/0/1">Yang Nan</a>, <a href="http://arxiv.org/find/eess/1/au:+Xing_X/0/1/0/all/0/1">Xiaodan Xing</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1">Shiyi Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Tang_Z/0/1/0/all/0/1">Zeyu Tang</a>, <a href="http://arxiv.org/find/eess/1/au:+Felder_F/0/1/0/all/0/1">Federico N Felder</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1">Sheng Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Ledda_R/0/1/0/all/0/1">Roberta Eufrasia Ledda</a>, <a href="http://arxiv.org/find/eess/1/au:+Ding_X/0/1/0/all/0/1">Xiaoliu Ding</a>, <a href="http://arxiv.org/find/eess/1/au:+Yu_R/0/1/0/all/0/1">Ruiqi Yu</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_W/0/1/0/all/0/1">Weiping Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Shi_F/0/1/0/all/0/1">Feng Shi</a>, <a href="http://arxiv.org/find/eess/1/au:+Sun_T/0/1/0/all/0/1">Tianyang Sun</a>, <a href="http://arxiv.org/find/eess/1/au:+Cao_Z/0/1/0/all/0/1">Zehong Cao</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_M/0/1/0/all/0/1">Minghui Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Gu_Y/0/1/0/all/0/1">Yun Gu</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1">Hanxiao Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Gao_J/0/1/0/all/0/1">Jian Gao</a>, <a href="http://arxiv.org/find/eess/1/au:+Tang_W/0/1/0/all/0/1">Wen Tang</a>, <a href="http://arxiv.org/find/eess/1/au:+Yu_P/0/1/0/all/0/1">Pengxin Yu</a>, <a href="http://arxiv.org/find/eess/1/au:+Kang_H/0/1/0/all/0/1">Han Kang</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1">Junqiang Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Lu_X/0/1/0/all/0/1">Xing Lu</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_B/0/1/0/all/0/1">Boyu Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Mamalakis_M/0/1/0/all/0/1">Michail Mamalakis</a>, <a href="http://arxiv.org/find/eess/1/au:+Prinzi_F/0/1/0/all/0/1">Francesco Prinzi</a>, <a href="http://arxiv.org/find/eess/1/au:+Carlini_G/0/1/0/all/0/1">Gianluca Carlini</a>, <a href="http://arxiv.org/find/eess/1/au:+Cuneo_L/0/1/0/all/0/1">Lisa Cuneo</a>, <a href="http://arxiv.org/find/eess/1/au:+Banerjee_A/0/1/0/all/0/1">Abhirup Banerjee</a>, <a href="http://arxiv.org/find/eess/1/au:+Xing_Z/0/1/0/all/0/1">Zhaohu Xing</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhu_L/0/1/0/all/0/1">Lei Zhu</a>, <a href="http://arxiv.org/find/eess/1/au:+Mesbah_Z/0/1/0/all/0/1">Zacharia Mesbah</a>, <a href="http://arxiv.org/find/eess/1/au:+Jain_D/0/1/0/all/0/1">Dhruv Jain</a>, <a href="http://arxiv.org/find/eess/1/au:+Mayet_T/0/1/0/all/0/1">Tsiry Mayet</a>, <a href="http://arxiv.org/find/eess/1/au:+Yuan_H/0/1/0/all/0/1">Hongyu Yuan</a>, <a href="http://arxiv.org/find/eess/1/au:+Lyu_Q/0/1/0/all/0/1">Qing Lyu</a>, <a href="http://arxiv.org/find/eess/1/au:+Wells_A/0/1/0/all/0/1">Athol Wells</a>, <a href="http://arxiv.org/find/eess/1/au:+Walsh_S/0/1/0/all/0/1">Simon LF Walsh</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1">Guang Yang</a></p>
<p>Airway-related quantitative imaging biomarkers are crucial for examination,
diagnosis, and prognosis in pulmonary diseases. However, the manual delineation
of airway trees remains prohibitively time-consuming. While significant efforts
have been made towards enhancing airway modelling, current public-available
datasets concentrate on lung diseases with moderate morphological variations.
The intricate honeycombing patterns present in the lung tissues of fibrotic
lung disease patients exacerbate the challenges, often leading to various
prediction errors. To address this issue, the 'Airway-Informed Quantitative CT
Imaging Biomarker for Fibrotic Lung Disease 2023' (AIIB23) competition was
organized in conjunction with the official 2023 International Conference on
Medical Image Computing and Computer Assisted Intervention (MICCAI). The airway
structures were meticulously annotated by three experienced radiologists.
Competitors were encouraged to develop automatic airway segmentation models
with high robustness and generalization abilities, followed by exploring the
most correlated QIB of mortality prediction. A training set of 120
high-resolution computerised tomography (HRCT) scans were publicly released
with expert annotations and mortality status. The online validation set
incorporated 52 HRCT scans from patients with fibrotic lung disease and the
offline test set included 140 cases from fibrosis and COVID-19 patients. The
results have shown that the capacity of extracting airway trees from patients
with fibrotic lung disease could be enhanced by introducing voxel-wise weighted
general union loss and continuity loss. In addition to the competitive image
biomarkers for prognosis, a strong airway-derived biomarker (Hazard ratio&gt;1.5,
p&lt;0.0001) was revealed for survival prognostication compared with existing
clinical measurements, clinician assessment and AI-based biomarkers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13754">Cross-Layer Optimization for Fault-Tolerant Deep Learning. (arXiv:2312.13754v1 [cs.AR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Cheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Haitong Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Ying Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Huawei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaowei Li</a></p>
<p>Fault-tolerant deep learning accelerator is the basis for highly reliable
deep learning processing and critical to deploy deep learning in
safety-critical applications such as avionics and robotics. Since deep learning
is known to be computing- and memory-intensive, traditional fault-tolerant
approaches based on redundant computing will incur substantial overhead
including power consumption and chip area. To this end, we propose to
characterize deep learning vulnerability difference across both neurons and
bits of each neuron, and leverage the vulnerability difference to enable
selective protection of the deep learning processing components from the
perspective of architecture layer and circuit layer respectively. At the same
time, we observe the correlation between model quantization and bit protection
overhead of the underlying processing elements of deep learning accelerators,
and propose to reduce the bit protection overhead by adding additional
quantization constrain without compromising the model accuracy. Finally, we
employ Bayesian optimization strategy to co-optimize the correlated cross-layer
design parameters at algorithm layer, architecture layer, and circuit layer to
minimize the hardware resource consumption while fulfilling multiple user
constraints including reliability, accuracy, and performance of the deep
learning processing at the same time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13765">Team Irisapu Project Description for DRC2023. (arXiv:2312.13765v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ohashi_R/0/1/0/all/0/1">Reon Ohashi</a>, <a href="http://arxiv.org/find/cs/1/au:+Agatsuma_S/0/1/0/all/0/1">Shinjitsu Agatsuma</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsubokura_K/0/1/0/all/0/1">Kazuya Tsubokura</a>, <a href="http://arxiv.org/find/cs/1/au:+Iribe_Y/0/1/0/all/0/1">Yurie Iribe</a></p>
<p>This paper describes the dialog robot system designed by Team Irisapu for the
preliminary round of the Dialogue Robot Competition 2023 (DRC2023). In order to
generate dialogue responses flexibly while adhering to predetermined scenarios,
we attempted to generate dialogue response sentences using OpenAI's GPT-3. We
aimed to create a system that can appropriately respond to users by dividing
the dialogue scenario into five sub-scenarios, and creating prompts for each
sub-scenario. Also, we incorporated a recovery strategy that can handle
dialogue breakdowns flexibly. Our research group has been working on research
related to dialogue breakdown detection, and we incorporated our findings to
date in this competition. As a result of the preliminary round, a bug in our
system affected the outcome and we were not able to achieve a satisfactory
result. However, in the evaluation category of "reliability of provided
information", we ranked third among all teams.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13772">On Task Performance and Model Calibration with Supervised and Self-Ensembled In-Context Learning. (arXiv:2312.13772v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chengzu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Han Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1">Goran Glava&#x161;</a>, <a href="http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1">Anna Korhonen</a>, <a href="http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1">Ivan Vuli&#x107;</a></p>
<p>Following the standard supervised fine-tuning (SFT) paradigm, in-context
learning (ICL) has become an efficient approach propelled by the recent
advancements in large language models (LLMs), yielding promising performance
across various tasks in few-shot data setups. However, both paradigms are prone
to suffer from the critical problem of overconfidence (i.e., miscalibration),
especially in such limited data setups. In this work, we deliver an in-depth
analysis of the behavior across different choices of learning methods from the
perspective of both performance and calibration, as well as their interplay.
Through extensive controlled experiments, we find that simultaneous gains for
both task performance and calibration are difficult to achieve, and the problem
of miscalibration exists across all learning methods in low-resource
scenarios.To address this challenging trade-off between performance and
calibration, we then investigate the potential of self-ensembling techniques
applied at different modeling stages (e.g., variations of in-context examples
or variations in prompts or different ensembling strategies). We justify the
feasibility of self-ensembling on SFT in addition to ICL, to make the
predictions more calibrated and have comparable or even better performance. Our
work sheds light on which learning paradigm to choose and how to enhance both
task performance and calibration of LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13783">Few Shot Part Segmentation Reveals Compositional Logic for Industrial Anomaly Detection. (arXiv:2312.13783v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Soopil Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+An_S/0/1/0/all/0/1">Sion An</a>, <a href="http://arxiv.org/find/cs/1/au:+Chikontwe_P/0/1/0/all/0/1">Philip Chikontwe</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1">Myeongkyun Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1">Ehsan Adeli</a>, <a href="http://arxiv.org/find/cs/1/au:+Pohl_K/0/1/0/all/0/1">Kilian M. Pohl</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1">Sanghyun Park</a></p>
<p>Logical anomalies (LA) refer to data violating underlying logical constraints
e.g., the quantity, arrangement, or composition of components within an image.
Detecting accurately such anomalies requires models to reason about various
component types through segmentation. However, curation of pixel-level
annotations for semantic segmentation is both time-consuming and expensive.
Although there are some prior few-shot or unsupervised co-part segmentation
algorithms, they often fail on images with industrial object. These images have
components with similar textures and shapes, and a precise differentiation
proves challenging. In this study, we introduce a novel component segmentation
model for LA detection that leverages a few labeled samples and unlabeled
images sharing logical constraints. To ensure consistent segmentation across
unlabeled images, we employ a histogram matching loss in conjunction with an
entropy loss. As segmentation predictions play a crucial role, we propose to
enhance both local and global sample validity detection by capturing key
aspects from visual semantics via three memory banks: class histograms,
component composition embeddings and patch-level representations. For effective
LA detection, we propose an adaptive scaling strategy to standardize anomaly
scores from different memory banks in inference. Extensive experiments on the
public benchmark MVTec LOCO AD reveal our method achieves 98.1% AUROC in LA
detection vs. 89.6% from competing methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13795">Sparse Training for Federated Learning with Regularized Error Correction. (arXiv:2312.13795v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Greidi_R/0/1/0/all/0/1">Ran Greidi</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_K/0/1/0/all/0/1">Kobi Cohen</a></p>
<p>Federated Learning (FL) has attracted much interest due to the significant
advantages it brings to training deep neural network (DNN) models. However,
since communications and computation resources are limited, training DNN models
in FL systems face challenges such as elevated computational and communication
costs in complex tasks. Sparse training schemes gain increasing attention in
order to scale down the dimensionality of each client (i.e., node)
transmission. Specifically, sparsification with error correction methods is a
promising technique, where only important updates are sent to the parameter
server (PS) and the rest are accumulated locally. While error correction
methods have shown to achieve a significant sparsification level of the
client-to-PS message without harming convergence, pushing sparsity further
remains unresolved due to the staleness effect. In this paper, we propose a
novel algorithm, dubbed Federated Learning with Accumulated Regularized
Embeddings (FLARE), to overcome this challenge. FLARE presents a novel sparse
training approach via accumulated pulling of the updated models with
regularization on the embeddings in the FL process, providing a powerful
solution to the staleness effect, and pushing sparsity to an exceptional level.
The performance of FLARE is validated through extensive experiments on diverse
and complex models, achieving a remarkable sparsity level (10 times and more
beyond the current state-of-the-art) along with significantly improved
accuracy. Additionally, an open-source software package has been developed for
the benefit of researchers and developers in related fields.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13816">Team Flow at DRC2023: Building Common Ground and Text-based Turn-taking in a Travel Agent Spoken Dialogue System. (arXiv:2312.13816v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hirai_R/0/1/0/all/0/1">Ryu Hirai</a>, <a href="http://arxiv.org/find/cs/1/au:+Iizuka_S/0/1/0/all/0/1">Shinya Iizuka</a>, <a href="http://arxiv.org/find/cs/1/au:+Iseno_H/0/1/0/all/0/1">Haruhisa Iseno</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_A/0/1/0/all/0/1">Ao Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1">Jingjing Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ohashi_A/0/1/0/all/0/1">Atsumoto Ohashi</a>, <a href="http://arxiv.org/find/cs/1/au:+Higashinaka_R/0/1/0/all/0/1">Ryuichiro Higashinaka</a></p>
<p>At the Dialogue Robot Competition 2023 (DRC2023), which was held to improve
the capability of dialogue robots, our team developed a system that could build
common ground and take more natural turns based on user utterance texts. Our
system generated queries for sightseeing spot searches using the common ground
and engaged in dialogue while waiting for user comprehension.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13845">Image Clustering using Restricted Boltzman Machine. (arXiv:2312.13845v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Woubie_A/0/1/0/all/0/1">Abraham Woubie</a>, <a href="http://arxiv.org/find/cs/1/au:+Solomon_E/0/1/0/all/0/1">Enoch Solomon</a>, <a href="http://arxiv.org/find/cs/1/au:+Emiru_E/0/1/0/all/0/1">Eyael Solomon Emiru</a></p>
<p>In various verification systems, Restricted Boltzmann Machines (RBMs) have
demonstrated their efficacy in both front-end and back-end processes. In this
work, we propose the use of RBMs to the image clustering tasks. RBMs are
trained to convert images into image embeddings. We employ the conventional
bottom-up Agglomerative Hierarchical Clustering (AHC) technique. To address the
challenge of limited test face image data, we introduce Agglomerative
Hierarchical Clustering based Method for Image Clustering using Restricted
Boltzmann Machine (AHC-RBM) with two major steps. Initially, a universal RBM
model is trained using all available training dataset. Subsequently, we train
an adapted RBM model using the data from each test image. Finally, RBM vectors
which is the embedding vector is generated by concatenating the
visible-to-hidden weight matrices of these adapted models, and the bias
vectors. These vectors effectively preserve class-specific information and are
utilized in image clustering tasks. Our experimental results, conducted on two
benchmark image datasets (MS-Celeb-1M and DeepFashion), demonstrate that our
proposed approach surpasses well-known clustering algorithms such as k-means,
spectral clustering, and approximate Rank-order.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13866">Understanding Inter-Session Intentions via Complex Logical Reasoning. (arXiv:2312.13866v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1">Jiaxin Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1">Chen Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_Q/0/1/0/all/0/1">Qingyu Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Yangqiu Song</a></p>
<p>Understanding user intentions is crucial for enhancing product
recommendations, navigation suggestions, and query reformulations. However,
user intentions can be complex, involving multiple sessions and attribute
requirements connected by logical operators such as And, Or, and Not. For
example, a user may search for Nike or Adidas running shoes across various
sessions, with a preference for the color purple. In another case, a user may
have purchased a mattress in a previous session and is now seeking a
corresponding bed frame without intending to buy another mattress. Prior
research on session understanding has not sufficiently addressed how to make
product or attribute recommendations for such complex intentions. In this
paper, we introduce the task of logical session complex query answering, where
sessions are treated as hyperedges of items, and we formulate the problem of
complex intention understanding as a task of logical session complex queries
answering (LS-CQA) on an aggregated hypergraph of sessions, items, and
attributes. The proposed task is a special type of complex query answering task
with sessions as ordered hyperedges. We also propose a new model, the Logical
Session Graph Transformer (LSGT), which captures interactions among items
across different sessions and their logical connections using a transformer
structure. We analyze the expressiveness of LSGT and prove the permutation
invariance of the inputs for the logical operators. We evaluate LSGT on three
datasets and demonstrate that it achieves state-of-the-art results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13905">Domain-Specific Fine-Tuning of Large Language Models for Interactive Robot Programming. (arXiv:2312.13905v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alt_B/0/1/0/all/0/1">Benjamin Alt</a>, <a href="http://arxiv.org/find/cs/1/au:+Kessner_U/0/1/0/all/0/1">Urs Ke&#xdf;ner</a>, <a href="http://arxiv.org/find/cs/1/au:+Taranovic_A/0/1/0/all/0/1">Aleksandar Taranovic</a>, <a href="http://arxiv.org/find/cs/1/au:+Katic_D/0/1/0/all/0/1">Darko Katic</a>, <a href="http://arxiv.org/find/cs/1/au:+Hermann_A/0/1/0/all/0/1">Andreas Hermann</a>, <a href="http://arxiv.org/find/cs/1/au:+Jakel_R/0/1/0/all/0/1">Rainer J&#xe4;kel</a>, <a href="http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1">Gerhard Neumann</a></p>
<p>Industrial robots are applied in a widening range of industries, but robot
programming mostly remains a task limited to programming experts. We propose a
natural language-based assistant for programming of advanced, industrial
robotic applications and investigate strategies for domain-specific fine-tuning
of foundation models with limited data and compute.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13906">EfficientPPS: Part-aware Panoptic Segmentation of Transparent Objects for Robotic Manipulation. (arXiv:2312.13906v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alt_B/0/1/0/all/0/1">Benjamin Alt</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1">Minh Dang Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hermann_A/0/1/0/all/0/1">Andreas Hermann</a>, <a href="http://arxiv.org/find/cs/1/au:+Katic_D/0/1/0/all/0/1">Darko Katic</a>, <a href="http://arxiv.org/find/cs/1/au:+Jakel_R/0/1/0/all/0/1">Rainer J&#xe4;kel</a>, <a href="http://arxiv.org/find/cs/1/au:+Dillmann_R/0/1/0/all/0/1">R&#xfc;diger Dillmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Sax_E/0/1/0/all/0/1">Eric Sax</a></p>
<p>The use of autonomous robots for assistance tasks in hospitals has the
potential to free up qualified staff and im-prove patient care. However, the
ubiquity of deformable and transparent objects in hospital settings poses
signif-icant challenges to vision-based perception systems. We present
EfficientPPS, a neural architecture for part-aware panoptic segmentation that
provides robots with semantically rich visual information for grasping and
ma-nipulation tasks. We also present an unsupervised data collection and
labelling method to reduce the need for human involvement in the training
process. EfficientPPS is evaluated on a dataset containing real-world hospital
objects and demonstrated to be robust and efficient in grasping transparent
transfusion bags with a collaborative robot arm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13912">Solving Long-run Average Reward Robust MDPs via Stochastic Games. (arXiv:2312.13912v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chatterjee_K/0/1/0/all/0/1">Krishnendu Chatterjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Goharshady_E/0/1/0/all/0/1">Ehsan Kafshdar Goharshady</a>, <a href="http://arxiv.org/find/cs/1/au:+Karrabi_M/0/1/0/all/0/1">Mehrdad Karrabi</a>, <a href="http://arxiv.org/find/cs/1/au:+Novotny_P/0/1/0/all/0/1">Petr Novotn&#xfd;</a>, <a href="http://arxiv.org/find/cs/1/au:+Zikelic_%7B/0/1/0/all/0/1">&#x110;or&#x111;e &#x17d;ikeli&#x107;</a></p>
<p>Markov decision processes (MDPs) provide a standard framework for sequential
decision making under uncertainty. However, transition probabilities in MDPs
are often estimated from data and MDPs do not take data uncertainty into
account. Robust Markov decision processes (RMDPs) address this shortcoming of
MDPs by assigning to each transition an uncertainty set rather than a single
probability value. The goal of solving RMDPs is then to find a policy which
maximizes the worst-case performance over the uncertainty sets. In this work,
we consider polytopic RMDPs in which all uncertainty sets are polytopes and
study the problem of solving long-run average reward polytopic RMDPs. Our focus
is on computational complexity aspects and efficient algorithms. We present a
novel perspective on this problem and show that it can be reduced to solving
long-run average reward turn-based stochastic games with finite state and
action spaces. This reduction allows us to derive several important
consequences that were hitherto not known to hold for polytopic RMDPs. First,
we derive new computational complexity bounds for solving long-run average
reward polytopic RMDPs, showing for the first time that the threshold decision
problem for them is in NP coNP and that they admit a randomized algorithm with
sub-exponential expected runtime. Second, we present Robust Polytopic Policy
Iteration (RPPI), a novel policy iteration algorithm for solving long-run
average reward polytopic RMDPs. Our experimental evaluation shows that RPPI is
much more efficient in solving long-run average reward polytopic RMDPs compared
to state-of-the-art methods based on value iteration.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13923">Fed-CO$_{2}$: Cooperation of Online and Offline Models for Severe Data Heterogeneity in Federated Learning. (arXiv:2312.13923v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1">Zhongyi Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Ye Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1">Wei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jingya Wang</a></p>
<p>Federated Learning (FL) has emerged as a promising distributed learning
paradigm that enables multiple clients to learn a global model collaboratively
without sharing their private data. However, the effectiveness of FL is highly
dependent on the quality of the data that is being used for training. In
particular, data heterogeneity issues, such as label distribution skew and
feature skew, can significantly impact the performance of FL. Previous studies
in FL have primarily focused on addressing label distribution skew data
heterogeneity, while only a few recent works have made initial progress in
tackling feature skew issues. Notably, these two forms of data heterogeneity
have been studied separately and have not been well explored within a unified
FL framework. To address this gap, we propose Fed-CO$_{2}$, a universal FL
framework that handles both label distribution skew and feature skew within a
\textbf{C}ooperation mechanism between the \textbf{O}nline and \textbf{O}ffline
models. Specifically, the online model learns general knowledge that is shared
among all clients, while the offline model is trained locally to learn the
specialized knowledge of each individual client. To further enhance model
cooperation in the presence of feature shifts, we design an intra-client
knowledge transfer mechanism that reinforces mutual learning between the online
and offline models, and an inter-client knowledge transfer mechanism to
increase the models' domain generalization ability. Extensive experiments show
that our Fed-CO$_{2}$ outperforms a wide range of existing personalized
federated learning algorithms in terms of handling label distribution skew and
feature skew, both individually and collectively. The empirical results are
supported by our convergence analyses in a simplified setting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13925">AsyncMLD: Asynchronous Multi-LLM Framework for Dialogue Recommendation System. (arXiv:2312.13925v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yoshimaru_N/0/1/0/all/0/1">Naoki Yoshimaru</a>, <a href="http://arxiv.org/find/cs/1/au:+Okuma_M/0/1/0/all/0/1">Motoharu Okuma</a>, <a href="http://arxiv.org/find/cs/1/au:+Iio_T/0/1/0/all/0/1">Takamasa Iio</a>, <a href="http://arxiv.org/find/cs/1/au:+Hatano_K/0/1/0/all/0/1">Kenji Hatano</a></p>
<p>We have reached a practical and realistic phase in human-support dialogue
agents by developing a large language model (LLM). However, when requiring
expert knowledge or anticipating the utterance content using the massive size
of the dialogue database, we still need help with the utterance content's
effectiveness and the efficiency of its output speed, even if using LLM.
Therefore, we propose a framework that uses LLM asynchronously in the part of
the system that returns an appropriate response and in the part that
understands the user's intention and searches the database. In particular,
noting that it takes time for the robot to speak, threading related to database
searches is performed while the robot is speaking.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13927">On the convergence of loss and uncertainty-based active learning algorithms. (arXiv:2312.13927v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Haimovich_D/0/1/0/all/0/1">Daniel Haimovich</a>, <a href="http://arxiv.org/find/cs/1/au:+Karamshuk_D/0/1/0/all/0/1">Dima Karamshuk</a>, <a href="http://arxiv.org/find/cs/1/au:+Linder_F/0/1/0/all/0/1">Fridolin Linder</a>, <a href="http://arxiv.org/find/cs/1/au:+Tax_N/0/1/0/all/0/1">Niek Tax</a>, <a href="http://arxiv.org/find/cs/1/au:+Vojnovic_M/0/1/0/all/0/1">Milan Vojnovic</a></p>
<p>We study convergence rates of loss and uncertainty-based active learning
algorithms under various assumptions. First, we provide a set of conditions
under which a convergence rate guarantee holds, and use this for linear
classifiers and linearly separable datasets to show convergence rate guarantees
for loss-based sampling and different loss functions. Second, we provide a
framework that allows us to derive convergence rate bounds for loss-based
sampling by deploying known convergence rate bounds for stochastic gradient
descent algorithms. Third, and last, we propose an active learning algorithm
that combines sampling of points and stochastic Polyak's step size. We show a
condition on the sampling that ensures a convergence rate guarantee for this
algorithm for smooth convex loss functions. Our numerical results demonstrate
efficiency of our proposed algorithm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13931">Joint Sensing and Task-Oriented Communications with Image and Wireless Data Modalities for Dynamic Spectrum Access. (arXiv:2312.13931v1 [cs.NI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sagduyu_Y/0/1/0/all/0/1">Yalin E. Sagduyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Erpek_T/0/1/0/all/0/1">Tugba Erpek</a>, <a href="http://arxiv.org/find/cs/1/au:+Yener_A/0/1/0/all/0/1">Aylin Yener</a>, <a href="http://arxiv.org/find/cs/1/au:+Ulukus_S/0/1/0/all/0/1">Sennur Ulukus</a></p>
<p>This paper introduces a deep learning approach to dynamic spectrum access,
leveraging the synergy of multi-modal image and spectrum data for the
identification of potential transmitters. We consider an edge device equipped
with a camera that is taking images of potential objects such as vehicles that
may harbor transmitters. Recognizing the computational constraints and trust
issues associated with on-device computation, we propose a collaborative system
wherein the edge device communicates selectively processed information to a
trusted receiver acting as a fusion center, where a decision is made to
identify whether a potential transmitter is present, or not. To achieve this,
we employ task-oriented communications, utilizing an encoder at the transmitter
for joint source coding, channel coding, and modulation. This architecture
efficiently transmits essential information of reduced dimension for object
classification. Simultaneously, the transmitted signals may reflect off objects
and return to the transmitter, allowing for the collection of target sensing
data. Then the collected sensing data undergoes a second round of encoding at
the transmitter, with the reduced-dimensional information communicated back to
the fusion center through task-oriented communications. On the receiver side, a
decoder performs the task of identifying a transmitter by fusing data received
through joint sensing and task-oriented communications. The two encoders at the
transmitter and the decoder at the receiver are jointly trained, enabling a
seamless integration of image classification and wireless signal detection.
Using AWGN and Rayleigh channel models, we demonstrate the effectiveness of the
proposed approach, showcasing high accuracy in transmitter identification
across diverse channel conditions while sustaining low latency in decision
making.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13944">Docking-based generative approaches in the search for new drug candidates. (arXiv:2312.13944v1 [q-bio.BM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Danel_T/0/1/0/all/0/1">Tomasz Danel</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Leski_J/0/1/0/all/0/1">Jan &#x141;&#x119;ski</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Podlewska_S/0/1/0/all/0/1">Sabina Podlewska</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Podolak_I/0/1/0/all/0/1">Igor T. Podolak</a></p>
<p>Despite the great popularity of virtual screening of existing compound
libraries, the search for new potential drug candidates also takes advantage of
generative protocols, where new compound suggestions are enumerated using
various algorithms. To increase the activity potency of generative approaches,
they have recently been coupled with molecular docking, a leading methodology
of structure-based drug design. In this review, we summarize progress since
docking-based generative models emerged. We propose a new taxonomy for these
methods and discuss their importance for the field of computer-aided drug
design. In addition, we discuss the most promising directions for further
development of generative protocols coupled with docking.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13951">Typhoon: Thai Large Language Models. (arXiv:2312.13951v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pipatanakul_K/0/1/0/all/0/1">Kunat Pipatanakul</a>, <a href="http://arxiv.org/find/cs/1/au:+Jirabovonvisut_P/0/1/0/all/0/1">Phatrasek Jirabovonvisut</a>, <a href="http://arxiv.org/find/cs/1/au:+Manakul_P/0/1/0/all/0/1">Potsawee Manakul</a>, <a href="http://arxiv.org/find/cs/1/au:+Sripaisarnmongkol_S/0/1/0/all/0/1">Sittipong Sripaisarnmongkol</a>, <a href="http://arxiv.org/find/cs/1/au:+Patomwong_R/0/1/0/all/0/1">Ruangsak Patomwong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chokchainant_P/0/1/0/all/0/1">Pathomporn Chokchainant</a>, <a href="http://arxiv.org/find/cs/1/au:+Tharnpipitchai_K/0/1/0/all/0/1">Kasima Tharnpipitchai</a></p>
<p>Typhoon is a series of Thai large language models (LLMs) developed
specifically for the Thai language. This technical report presents challenges
and insights in developing Thai LLMs, including data preparation, pretraining,
instruction-tuning, and evaluation. As one of the challenges of low-resource
languages is the amount of pretraining data, we apply continual training to
transfer existing world knowledge from a strong LLM. To evaluate the Thai
knowledge encapsulated in each model from the pretraining stage, we develop
ThaiExam, a benchmark based on examinations for high-school students and
investment professionals in Thailand. In addition, we fine-tune Typhoon to
follow Thai instructions, and we evaluate instruction-tuned models on Thai
instruction datasets as well as translation, summarization, and
question-answering tasks. Experimental results on a suite of Thai benchmarks
show that Typhoon outperforms all open-source Thai language models, and its
performance is on par with GPT-3.5 in Thai while having only 7 billion
parameters and being 2.62 times more efficient in tokenizing Thai text.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13964">PIA: Your Personalized Image Animator via Plug-and-Play Modules in Text-to-Image Models. (arXiv:2312.13964v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yiming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1">Zhening Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1">Yanhong Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1">Youqing Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Kai Chen</a></p>
<p>Recent advancements in personalized text-to-image (T2I) models have
revolutionized content creation, empowering non-experts to generate stunning
images with unique styles. While promising, adding realistic motions into these
personalized images by text poses significant challenges in preserving distinct
styles, high-fidelity details, and achieving motion controllability by text. In
this paper, we present PIA, a Personalized Image Animator that excels in
aligning with condition images, achieving motion controllability by text, and
the compatibility with various personalized T2I models without specific tuning.
To achieve these goals, PIA builds upon a base T2I model with well-trained
temporal alignment layers, allowing for the seamless transformation of any
personalized T2I model into an image animation model. A key component of PIA is
the introduction of the condition module, which utilizes the condition frame
and inter-frame affinity as input to transfer appearance information guided by
the affinity hint for individual frame synthesis in the latent space. This
design mitigates the challenges of appearance-related image alignment within
and allows for a stronger focus on aligning with motion-related guidance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13970">On Partial Optimal Transport: Revising the Infeasibility of Sinkhorn and Efficient Gradient Methods. (arXiv:2312.13970v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1">Anh Duc Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Tuan Dung Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1">Quang Minh Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1">Hoang H. Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Toh_K/0/1/0/all/0/1">Kim-Chuan Toh</a></p>
<p>This paper studies the Partial Optimal Transport (POT) problem between two
unbalanced measures with at most $n$ supports and its applications in various
AI tasks such as color transfer or domain adaptation. There is hence the need
for fast approximations of POT with increasingly large problem sizes in arising
applications. We first theoretically and experimentally investigate the
infeasibility of the state-of-the-art Sinkhorn algorithm for POT due to its
incompatible rounding procedure, which consequently degrades its qualitative
performance in real world applications like point-cloud registration. To this
end, we propose a novel rounding algorithm for POT, and then provide a feasible
Sinkhorn procedure with a revised computation complexity of
$\mathcal{\widetilde O}(n^2/\varepsilon^4)$. Our rounding algorithm also
permits the development of two first-order methods to approximate the POT
problem. The first algorithm, Adaptive Primal-Dual Accelerated Gradient Descent
(APDAGD), finds an $\varepsilon$-approximate solution to the POT problem in
$\mathcal{\widetilde O}(n^{2.5}/\varepsilon)$, which is better in $\varepsilon$
than revised Sinkhorn. The second method, Dual Extrapolation, achieves the
computation complexity of $\mathcal{\widetilde O}(n^2/\varepsilon)$, thereby
being the best in the literature. We further demonstrate the flexibility of POT
compared to standard OT as well as the practicality of our algorithms on real
applications where two marginal distributions are unbalanced.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13976">Anatomical basis of sex differences in human post-myocardial infarction ECG phenotypes identified by novel automated torso-cardiac 3D reconstruction. (arXiv:2312.13976v1 [physics.med-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Smith_H/0/1/0/all/0/1">Hannah J. Smith</a>, <a href="http://arxiv.org/find/physics/1/au:+Rodriguez_B/0/1/0/all/0/1">Blanca Rodriguez</a>, <a href="http://arxiv.org/find/physics/1/au:+Sang_Y/0/1/0/all/0/1">Yuling Sang</a>, <a href="http://arxiv.org/find/physics/1/au:+Beetz_M/0/1/0/all/0/1">Marcel Beetz</a>, <a href="http://arxiv.org/find/physics/1/au:+Choudhury_R/0/1/0/all/0/1">Robin Choudhury</a>, <a href="http://arxiv.org/find/physics/1/au:+Grau_V/0/1/0/all/0/1">Vicente Grau</a>, <a href="http://arxiv.org/find/physics/1/au:+Banerjee_A/0/1/0/all/0/1">Abhirup Banerjee</a></p>
<p>The electrocardiogram (ECG) is routinely used in cardiology, though its
interpretation is confounded by anatomical variability. A novel, automated
computational pipeline enables quantification of torso-ventricular anatomy
metrics from magnetic resonance imaging, and comparison to ECG characteristics.
Sex and myocardial infarction differences are investigated based on 1051
healthy and 425 post-MI subjects from UK Biobank. Smaller ventricles in females
explain ~50% of shorter QRS durations than in males, and contribute to lower
STJ amplitudes in females (also due to more superior and posterior position).
In females, torso-ventricular anatomy, particularly from larger BMI, is a
stronger modulator of T wave amplitude reductions and left-deviated R axis
angles in post-MI than in males. Thus, female MI phenotype is less reflective
of pathology, and baseline STJ amplitudes and QRS durations are further from
clinical thresholds. Therefore, quantification of anatomical sex-differences
and impact on ECG in health and disease is critical to avoid clinical sex-bias.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13987">Modular Neural Network Policies for Learning In-Flight Object Catching with a Robot Hand-Arm System. (arXiv:2312.13987v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1">Wenbin Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Acero_F/0/1/0/all/0/1">Fernando Acero</a>, <a href="http://arxiv.org/find/cs/1/au:+Triantafyllidis_E/0/1/0/all/0/1">Eleftherios Triantafyllidis</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhaocheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhibin Li</a></p>
<p>We present a modular framework designed to enable a robot hand-arm system to
learn how to catch flying objects, a task that requires fast, reactive, and
accurately-timed robot motions. Our framework consists of five core modules:
(i) an object state estimator that learns object trajectory prediction, (ii) a
catching pose quality network that learns to score and rank object poses for
catching, (iii) a reaching control policy trained to move the robot hand to
pre-catch poses, (iv) a grasping control policy trained to perform soft
catching motions for safe and robust grasping, and (v) a gating network trained
to synthesize the actions given by the reaching and grasping policy. The former
two modules are trained via supervised learning and the latter three use deep
reinforcement learning in a simulated environment. We conduct extensive
evaluations of our framework in simulation for each module and the integrated
system, to demonstrate high success rates of in-flight catching and robustness
to perturbations and sensory noise. Whilst only simple cylindrical and
spherical objects are used for training, the integrated system shows successful
generalization to a variety of household objects that are not used in training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14001">Deep Learning Based Face Recognition Method using Siamese Network. (arXiv:2312.14001v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Solomon_E/0/1/0/all/0/1">Enoch Solomon</a>, <a href="http://arxiv.org/find/cs/1/au:+Woubie_A/0/1/0/all/0/1">Abraham Woubie</a>, <a href="http://arxiv.org/find/cs/1/au:+Emiru_E/0/1/0/all/0/1">Eyael Solomon Emiru</a></p>
<p>Achieving state-of-the-art results in face verification systems typically
hinges on the availability of labeled face training data, a resource that often
proves challenging to acquire in substantial quantities. In this research
endeavor, we proposed employing Siamese networks for face recognition,
eliminating the need for labeled face images. We achieve this by strategically
leveraging negative samples alongside nearest neighbor counterparts, thereby
establishing positive and negative pairs through an unsupervised methodology.
The architectural framework adopts a VGG encoder, trained as a double branch
siamese network. Our primary aim is to circumvent the necessity for labeled
face image data, thus proposing the generation of training pairs in an entirely
unsupervised manner. Positive training data are selected within a dataset based
on their highest cosine similarity scores with a designated anchor, while
negative training data are culled in a parallel fashion, though drawn from an
alternate dataset. During training, the proposed siamese network conducts
binary classification via cross-entropy loss. Subsequently, during the testing
phase, we directly extract face verification scores from the network's output
layer. Experimental results reveal that the proposed unsupervised system
delivers a performance on par with a similar but fully supervised baseline.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14005">On the choice of the optimal temporal support for audio classification with Pre-trained embeddings. (arXiv:2312.14005v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Quelennec_A/0/1/0/all/0/1">Aurian Quelennec</a>, <a href="http://arxiv.org/find/cs/1/au:+Olvera_M/0/1/0/all/0/1">Michel Olvera</a>, <a href="http://arxiv.org/find/cs/1/au:+Peeters_G/0/1/0/all/0/1">Geoffroy Peeters</a>, <a href="http://arxiv.org/find/cs/1/au:+Essid_S/0/1/0/all/0/1">Slim Essid</a></p>
<p>Current state-of-the-art audio analysis systems rely on pre-trained embedding
models, often used off-the-shelf as (frozen) feature extractors. Choosing the
best one for a set of tasks is the subject of many recent publications.
However, one aspect often overlooked in these works is the influence of the
duration of audio input considered to extract an embedding, which we refer to
as Temporal Support (TS). In this work, we study the influence of the TS for
well-established or emerging pre-trained embeddings, chosen to represent
different types of architectures and learning paradigms. We conduct this
evaluation using both musical instrument and environmental sound datasets,
namely OpenMIC, TAU Urban Acoustic Scenes 2020 Mobile, and ESC-50. We
especially highlight that Audio Spectrogram Transformer-based systems (PaSST
and BEATs) remain effective with smaller TS, which therefore allows for a
drastic reduction in memory and computational cost. Moreover, we show that by
choosing the optimal TS we reach competitive results across all tasks. In
particular, we improve the state-of-the-art results on OpenMIC, using BEATs and
PaSST without any fine-tuning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14037">Neural Contextual Bandits for Personalized Recommendation. (arXiv:2312.14037v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ban_Y/0/1/0/all/0/1">Yikun Ban</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1">Yunzhe Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Jingrui He</a></p>
<p>In the dynamic landscape of online businesses, recommender systems are
pivotal in enhancing user experiences. While traditional approaches have relied
on static supervised learning, the quest for adaptive, user-centric
recommendations has led to the emergence of the formulation of contextual
bandits. This tutorial investigates the contextual bandits as a powerful
framework for personalized recommendations. We delve into the challenges,
advanced algorithms and theories, collaborative strategies, and open challenges
and future prospects within this field. Different from existing related
tutorials, (1) we focus on the exploration perspective of contextual bandits to
alleviate the ``Matthew Effect'' in the recommender systems, i.e., the rich get
richer and the poor get poorer, concerning the popularity of items; (2) in
addition to the conventional linear contextual bandits, we will also dedicated
to neural contextual bandits which have emerged as an important branch in
recent years, to investigate how neural networks benefit contextual bandits for
personalized recommendation both empirically and theoretically; (3) we will
cover the latest topic, collaborative neural contextual bandits, to incorporate
both user heterogeneity and user correlations customized for recommender
system; (4) we will provide and discuss the new emerging challenges and open
questions for neural contextual bandits with applications in the personalized
recommendation, especially for large neural models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14095">RetailSynth: Synthetic Data Generation for Retail AI Systems Evaluation. (arXiv:2312.14095v1 [stat.AP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Xia_Y/0/1/0/all/0/1">Yu Xia</a>, <a href="http://arxiv.org/find/stat/1/au:+Arian_A/0/1/0/all/0/1">Ali Arian</a>, <a href="http://arxiv.org/find/stat/1/au:+Narayanamoorthy_S/0/1/0/all/0/1">Sriram Narayanamoorthy</a>, <a href="http://arxiv.org/find/stat/1/au:+Mabry_J/0/1/0/all/0/1">Joshua Mabry</a></p>
<p>Significant research effort has been devoted in recent years to developing
personalized pricing, promotions, and product recommendation algorithms that
can leverage rich customer data to learn and earn. Systematic benchmarking and
evaluation of these causal learning systems remains a critical challenge, due
to the lack of suitable datasets and simulation environments. In this work, we
propose a multi-stage model for simulating customer shopping behavior that
captures important sources of heterogeneity, including price sensitivity and
past experiences. We embedded this model into a working simulation environment
-- RetailSynth. RetailSynth was carefully calibrated on publicly available
grocery data to create realistic synthetic shopping transactions. Multiple
pricing policies were implemented within the simulator and analyzed for impact
on revenue, category penetration, and customer retention. Applied researchers
can use RetailSynth to validate causal demand models for multi-category retail
and to incorporate realistic price sensitivity into emerging benchmarking
suites for personalized pricing, promotions, and product recommendations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14106">Learning Human-like Representations to Enable Learning Human Values. (arXiv:2312.14106v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wynn_A/0/1/0/all/0/1">Andrea Wynn</a>, <a href="http://arxiv.org/find/cs/1/au:+Sucholutsky_I/0/1/0/all/0/1">Ilia Sucholutsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1">Thomas L. Griffiths</a></p>
<p>How can we build AI systems that are aligned with human values and objectives
in order to avoid causing harm or violating societal standards for acceptable
behavior? Making AI systems learn human-like representations of the world has
many known benefits, including improving generalization, robustness to domain
shifts, and few-shot learning performance, among others. We propose that this
kind of representational alignment between machine learning (ML) models and
humans is also a necessary condition for value alignment, where ML systems
conform to human values and societal norms. We focus on ethics as one aspect of
value alignment and train multiple ML agents (support vector regression and
kernel regression) in a multi-armed bandit setting, where rewards are sampled
from a distribution that reflects the morality of the chosen action. We then
study the relationship between each agent's degree of representational
alignment with humans and their performance when learning to take the most
ethical actions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14115">LingoQA: Video Question Answering for Autonomous Driving. (arXiv:2312.14115v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Marcu_A/0/1/0/all/0/1">Ana-Maria Marcu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Long Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hunermann_J/0/1/0/all/0/1">Jan H&#xfc;nermann</a>, <a href="http://arxiv.org/find/cs/1/au:+Karnsund_A/0/1/0/all/0/1">Alice Karnsund</a>, <a href="http://arxiv.org/find/cs/1/au:+Hanotte_B/0/1/0/all/0/1">Benoit Hanotte</a>, <a href="http://arxiv.org/find/cs/1/au:+Chidananda_P/0/1/0/all/0/1">Prajwal Chidananda</a>, <a href="http://arxiv.org/find/cs/1/au:+Nair_S/0/1/0/all/0/1">Saurabh Nair</a>, <a href="http://arxiv.org/find/cs/1/au:+Badrinarayanan_V/0/1/0/all/0/1">Vijay Badrinarayanan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kendall_A/0/1/0/all/0/1">Alex Kendall</a>, <a href="http://arxiv.org/find/cs/1/au:+Shotton_J/0/1/0/all/0/1">Jamie Shotton</a>, <a href="http://arxiv.org/find/cs/1/au:+Sinavski_O/0/1/0/all/0/1">Oleg Sinavski</a></p>
<p>Autonomous driving has long faced a challenge with public acceptance due to
the lack of explainability in the decision-making process. Video
question-answering (QA) in natural language provides the opportunity for
bridging this gap. Nonetheless, evaluating the performance of Video QA models
has proved particularly tough due to the absence of comprehensive benchmarks.
To fill this gap, we introduce LingoQA, a benchmark specifically for autonomous
driving Video QA. The LingoQA trainable metric demonstrates a 0.95 Spearman
correlation coefficient with human evaluations. We introduce a Video QA dataset
of central London consisting of 419k samples that we release with the paper. We
establish a baseline vision-language model and run extensive ablation studies
to understand its performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14121">Fast and Knowledge-Free Deep Learning for General Game Playing (Student Abstract). (arXiv:2312.14121v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maras_M/0/1/0/all/0/1">Micha&#x142; Maras</a>, <a href="http://arxiv.org/find/cs/1/au:+Kepa_M/0/1/0/all/0/1">Micha&#x142; K&#x119;pa</a>, <a href="http://arxiv.org/find/cs/1/au:+Kowalski_J/0/1/0/all/0/1">Jakub Kowalski</a>, <a href="http://arxiv.org/find/cs/1/au:+Szykula_M/0/1/0/all/0/1">Marek Szyku&#x142;a</a></p>
<p>We develop a method of adapting the AlphaZero model to General Game Playing
(GGP) that focuses on faster model generation and requires less knowledge to be
extracted from the game rules. The dataset generation uses MCTS playing instead
of self-play; only the value network is used, and attention layers replace the
convolutional ones. This allows us to abandon any assumptions about the action
space and board topology. We implement the method within the Regular Boardgames
GGP system and show that we can build models outperforming the UCT baseline for
most games efficiently.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14125">VideoPoet: A Large Language Model for Zero-Shot Video Generation. (arXiv:2312.14125v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kondratyuk_D/0/1/0/all/0/1">Dan Kondratyuk</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1">Lijun Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1">Xiuye Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lezama_J/0/1/0/all/0/1">Jos&#xe9; Lezama</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jonathan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hornung_R/0/1/0/all/0/1">Rachel Hornung</a>, <a href="http://arxiv.org/find/cs/1/au:+Adam_H/0/1/0/all/0/1">Hartwig Adam</a>, <a href="http://arxiv.org/find/cs/1/au:+Akbari_H/0/1/0/all/0/1">Hassan Akbari</a>, <a href="http://arxiv.org/find/cs/1/au:+Alon_Y/0/1/0/all/0/1">Yair Alon</a>, <a href="http://arxiv.org/find/cs/1/au:+Birodkar_V/0/1/0/all/0/1">Vighnesh Birodkar</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1">Yong Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chiu_M/0/1/0/all/0/1">Ming-Chang Chiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dillon_J/0/1/0/all/0/1">Josh Dillon</a>, <a href="http://arxiv.org/find/cs/1/au:+Essa_I/0/1/0/all/0/1">Irfan Essa</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1">Agrim Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Hahn_M/0/1/0/all/0/1">Meera Hahn</a>, <a href="http://arxiv.org/find/cs/1/au:+Hauth_A/0/1/0/all/0/1">Anja Hauth</a>, <a href="http://arxiv.org/find/cs/1/au:+Hendon_D/0/1/0/all/0/1">David Hendon</a>, <a href="http://arxiv.org/find/cs/1/au:+Martinez_A/0/1/0/all/0/1">Alonso Martinez</a>, <a href="http://arxiv.org/find/cs/1/au:+Minnen_D/0/1/0/all/0/1">David Minnen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ross_D/0/1/0/all/0/1">David Ross</a>, <a href="http://arxiv.org/find/cs/1/au:+Schindler_G/0/1/0/all/0/1">Grant Schindler</a>, <a href="http://arxiv.org/find/cs/1/au:+Sirotenko_M/0/1/0/all/0/1">Mikhail Sirotenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1">Kihyuk Sohn</a>, <a href="http://arxiv.org/find/cs/1/au:+Somandepalli_K/0/1/0/all/0/1">Krishna Somandepalli</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Huisheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1">Jimmy Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Ming-Hsuan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xuan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Seybold_B/0/1/0/all/0/1">Bryan Seybold</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1">Lu Jiang</a></p>
<p>We present VideoPoet, a language model capable of synthesizing high-quality
video, with matching audio, from a large variety of conditioning signals.
VideoPoet employs a decoder-only transformer architecture that processes
multimodal inputs -- including images, videos, text, and audio. The training
protocol follows that of Large Language Models (LLMs), consisting of two
stages: pretraining and task-specific adaptation. During pretraining, VideoPoet
incorporates a mixture of multimodal generative objectives within an
autoregressive Transformer framework. The pretrained LLM serves as a foundation
that can be adapted for a range of video generation tasks. We present empirical
results demonstrating the model's state-of-the-art capabilities in zero-shot
video generation, specifically highlighting VideoPoet's ability to generate
high-fidelity motions. Project page: <a href="http://sites.research.google/videopoet/">this http URL</a>
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14129">WellFactor: Patient Profiling using Integrative Embedding of Healthcare Data. (arXiv:2312.14129v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1">Dongjin Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_A/0/1/0/all/0/1">Andy Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ozturk_O/0/1/0/all/0/1">Ozgur Ozturk</a>, <a href="http://arxiv.org/find/cs/1/au:+Shrestha_D/0/1/0/all/0/1">Deep Shrestha</a>, <a href="http://arxiv.org/find/cs/1/au:+Drake_B/0/1/0/all/0/1">Barry Drake</a>, <a href="http://arxiv.org/find/cs/1/au:+Haidarian_H/0/1/0/all/0/1">Hamid Haidarian</a>, <a href="http://arxiv.org/find/cs/1/au:+Javed_F/0/1/0/all/0/1">Faizan Javed</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1">Haesun Park</a></p>
<p>In the rapidly evolving healthcare industry, platforms now have access to not
only traditional medical records, but also diverse data sets encompassing
various patient interactions, such as those from healthcare web portals. To
address this rich diversity of data, we introduce WellFactor: a method that
derives patient profiles by integrating information from these sources. Central
to our approach is the utilization of constrained low-rank approximation.
WellFactor is optimized to handle the sparsity that is often inherent in
healthcare data. Moreover, by incorporating task-specific label information,
our method refines the embedding results, offering a more informed perspective
on patients. One important feature of WellFactor is its ability to compute
embeddings for new, previously unobserved patient data instantaneously,
eliminating the need to revisit the entire data set or recomputing the
embedding. Comprehensive evaluations on real-world healthcare data demonstrate
WellFactor's effectiveness. It produces better results compared to other
existing methods in classification performance, yields meaningful clustering of
patients, and delivers consistent results in patient similarity searches and
predictions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14149">TagAlign: Improving Vision-Language Alignment with Multi-Tag Classification. (arXiv:2312.14149v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qinying Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1">Kecheng Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1">Wu Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Tong_Z/0/1/0/all/0/1">Zhan Tong</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zilei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yujun Shen</a></p>
<p>The crux of learning vision-language models is to extract semantically
aligned information from visual and linguistic data. Existing attempts usually
face the problem of coarse alignment, \textit{e.g.}, the vision encoder
struggles in localizing an attribute-specified object. In this work, we propose
an embarrassingly simple approach to better align image and text features with
no need of additional data formats other than image-text pairs. Concretely,
given an image and its paired text, we manage to parse objects (\textit{e.g.},
cat) and attributes (\textit{e.g.}, black) from the description, which are
highly likely to exist in the image. It is noteworthy that the parsing pipeline
is fully automatic and thus enjoys good scalability. With these parsed
semantics as supervision signals, we can complement the commonly used
image-text contrastive loss with the multi-tag classification loss. Extensive
experimental results on a broad suite of semantic segmentation datasets
substantiate the average 3.65\% improvement of our framework over existing
alternatives. Furthermore, the visualization results indicate that attribute
supervision makes vision-language models accurately localize
attribute-specified objects. Project page can be found at
https://qinying-liu.github.io/Tag-Align/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.08965">KitBit: A New AI Model for Solving Intelligence Tests and Numerical Series. (arXiv:2206.08965v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Corsino_V/0/1/0/all/0/1">V&#xed;ctor Corsino</a>, <a href="http://arxiv.org/find/cs/1/au:+Gilperez_J/0/1/0/all/0/1">Jos&#xe9; Manuel Gilp&#xe9;rez</a>, <a href="http://arxiv.org/find/cs/1/au:+Herrera_L/0/1/0/all/0/1">Luis Herrera</a></p>
<p>The resolution of intelligence tests, in particular numerical sequences, has
been of great interest in the evaluation of AI systems. We present a new
computational model called KitBit that uses a reduced set of algorithms and
their combinations to build a predictive model that finds the underlying
pattern in numerical sequences, such as those included in IQ tests and others
of much greater complexity. We present the fundamentals of the model and its
application in different cases. First, the system is tested on a set of number
series used in IQ tests collected from various sources. Next, our model is
successfully applied on the sequences used to evaluate the models reported in
the literature. In both cases, the system is capable of solving these types of
problems in less than a second using standard computing power. Finally,
KitBit's algorithms have been applied for the first time to the complete set of
entire sequences of the well-known OEIS database. We find a pattern in the form
of a list of algorithms and predict the following terms in the largest number
of series to date. These results demonstrate the potential of KitBit to solve
complex problems that could be represented numerically.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.14203">Latent Combinational Game Design. (arXiv:2206.14203v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1">Anurag Sarkar</a>, <a href="http://arxiv.org/find/cs/1/au:+Cooper_S/0/1/0/all/0/1">Seth Cooper</a></p>
<p>We present latent combinational game design -- an approach for generating
playable games that blend a given set of games in a desired combination using
deep generative latent variable models. We use Gaussian Mixture Variational
Autoencoders (GMVAEs) which model the VAE latent space via a mixture of
Gaussian components. Through supervised training, each component encodes levels
from one game and lets us define blended games as linear combinations of these
components. This enables generating new games that blend the input games as
well as controlling the relative proportions of each game in the blend. We also
extend prior blending work using conditional VAEs and compare against the GMVAE
and additionally introduce a hybrid conditional GMVAE (CGMVAE) architecture
which lets us generate whole blended levels and layouts. Results show that
these approaches can generate playable games that blend the input games in
specified combinations. We use both platformers and dungeon-based games to
demonstrate our results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.10619">Restricted Bernoulli Matrix Factorization: Balancing the trade-off between prediction accuracy and coverage in classification based collaborative filtering. (arXiv:2210.10619v2 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gonzalez_Prieto_A/0/1/0/all/0/1">&#xc1;ngel Gonz&#xe1;lez-Prieto</a>, <a href="http://arxiv.org/find/cs/1/au:+Gutierrez_A/0/1/0/all/0/1">Abraham Guti&#xe9;rrez</a>, <a href="http://arxiv.org/find/cs/1/au:+Ortega_F/0/1/0/all/0/1">Fernando Ortega</a>, <a href="http://arxiv.org/find/cs/1/au:+Lara_Cabrera_R/0/1/0/all/0/1">Ra&#xfa;l Lara-Cabrera</a></p>
<p>Reliability measures associated with the prediction of the machine learning
models are critical to strengthening user confidence in artificial
intelligence. Therefore, those models that are able to provide not only
predictions, but also reliability, enjoy greater popularity. In the field of
recommender systems, reliability is crucial, since users tend to prefer those
recommendations that are sure to interest them, that is, high predictions with
high reliabilities. In this paper, we propose Restricted Bernoulli Matrix
Factorization (ResBeMF), a new algorithm aimed at enhancing the performance of
classification-based collaborative filtering. The proposed model has been
compared to other existing solutions in the literature in terms of prediction
quality (Mean Absolute Error and accuracy scores), prediction quantity
(coverage score) and recommendation quality (Mean Average Precision score). The
experimental results demonstrate that the proposed model provides a good
balance in terms of the quality measures used compared to other recommendation
models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.13495">Few-shot Object Detection with Refined Contrastive Learning. (arXiv:2211.13495v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shangguan_Z/0/1/0/all/0/1">Zeyu Shangguan</a>, <a href="http://arxiv.org/find/cs/1/au:+Huai_L/0/1/0/all/0/1">Lian Huai</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1">Xingqun Jiang</a></p>
<p>Due to the scarcity of sampling data in reality, few-shot object detection
(FSOD) has drawn more and more attention because of its ability to quickly
train new detection concepts with less data. However, there are still failure
identifications due to the difficulty in distinguishing confusable classes. We
also notice that the high standard deviation of average precision reveals the
inconsistent detection performance. To this end, we propose a novel FSOD method
with Refined Contrastive Learning (FSRC). A pre-determination component is
introduced to find out the Resemblance Group from novel classes which contains
confusable classes. Afterwards, Refined Contrastive Learning (RCL) is pointedly
performed on this group of classes in order to increase the inter-class
distances among them. In the meantime, the detection results distribute more
uniformly which further improve the performance. Experimental results based on
PASCAL VOC and COCO datasets demonstrate our proposed method outperforms the
current state-of-the-art research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.03616">Can gamification reduce the burden of self-reporting in mHealth applications? A feasibility study using machine learning from smartwatch data to estimate cognitive load. (arXiv:2302.03616v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Grzeszczyk_M/0/1/0/all/0/1">Michal K. Grzeszczyk</a>, <a href="http://arxiv.org/find/cs/1/au:+Adamczyk_P/0/1/0/all/0/1">Paulina Adamczyk</a>, <a href="http://arxiv.org/find/cs/1/au:+Marek_S/0/1/0/all/0/1">Sylwia Marek</a>, <a href="http://arxiv.org/find/cs/1/au:+Precikowski_R/0/1/0/all/0/1">Ryszard Pr&#x119;cikowski</a>, <a href="http://arxiv.org/find/cs/1/au:+Kus_M/0/1/0/all/0/1">Maciej Ku&#x15b;</a>, <a href="http://arxiv.org/find/cs/1/au:+Lelujko_M/0/1/0/all/0/1">M. Patrycja Lelujko</a>, <a href="http://arxiv.org/find/cs/1/au:+Blanco_R/0/1/0/all/0/1">Rosmary Blanco</a>, <a href="http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1">Tomasz Trzci&#x144;ski</a>, <a href="http://arxiv.org/find/cs/1/au:+Sitek_A/0/1/0/all/0/1">Arkadiusz Sitek</a>, <a href="http://arxiv.org/find/cs/1/au:+Malawski_M/0/1/0/all/0/1">Maciej Malawski</a>, <a href="http://arxiv.org/find/cs/1/au:+Lisowska_A/0/1/0/all/0/1">Aneta Lisowska</a></p>
<p>The effectiveness of digital treatments can be measured by requiring patients
to self-report their state through applications, however, it can be
overwhelming and causes disengagement. We conduct a study to explore the impact
of gamification on self-reporting. Our approach involves the creation of a
system to assess cognitive load (CL) through the analysis of
photoplethysmography (PPG) signals. The data from 11 participants is utilized
to train a machine learning model to detect CL. Subsequently, we create two
versions of surveys: a gamified and a traditional one. We estimate the CL
experienced by other participants (13) while completing surveys. We find that
CL detector performance can be enhanced via pre-training on stress detection
tasks. For 10 out of 13 participants, a personalized CL detector can achieve an
F1 score above 0.7. We find no difference between the gamified and non-gamified
surveys in terms of CL but participants prefer the gamified version.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.11883">PIFON-EPT: MR-Based Electrical Property Tomography Using Physics-Informed Fourier Networks. (arXiv:2302.11883v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1">Xinling Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Serralles_J/0/1/0/all/0/1">Jos&#xe9; E. C. Serrall&#xe9;s</a>, <a href="http://arxiv.org/find/cs/1/au:+Giannakopoulos_I/0/1/0/all/0/1">Ilias I. Giannakopoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Ziyue Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Daniel_L/0/1/0/all/0/1">Luca Daniel</a>, <a href="http://arxiv.org/find/cs/1/au:+Lattanzi_R/0/1/0/all/0/1">Riccardo Lattanzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zheng Zhang</a></p>
<p>We propose Physics-Informed Fourier Networks for Electrical Properties (EP)
Tomography (PIFON-EPT), a novel deep learning-based method for EP
reconstruction using noisy and/or incomplete magnetic resonance (MR)
measurements. Our approach leverages the Helmholtz equation to constrain two
networks, responsible for the denoising and completion of the transmit fields,
and the estimation of the object's EP, respectively. We embed a random Fourier
features mapping into our networks to enable efficient learning of
high-frequency details encoded in the transmit fields. We demonstrated the
efficacy of PIFON-EPT through several simulated experiments at 3 and 7 tesla
(T) MR imaging, and showed that our method can reconstruct physically
consistent EP and transmit fields. Specifically, when only $20\%$ of the noisy
measured fields were used as inputs, PIFON-EPT reconstructed the EP of a
phantom with $\leq 5\%$ error, and denoised and completed the measurements with
$\leq 1\%$ error. Additionally, we adapted PIFON-EPT to solve the generalized
Helmholtz equation that accounts for gradients of EP between inhomogeneities.
This yielded improved results at interfaces between different materials without
explicit knowledge of boundary conditions. PIFON-EPT is the first method that
can simultaneously reconstruct EP and transmit fields from incomplete noisy MR
measurements, providing new opportunities for EPT research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.00586">FAIR-Ensemble: When Fairness Naturally Emerges From Deep Ensembling. (arXiv:2303.00586v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Ko_W/0/1/0/all/0/1">Wei-Yin Ko</a>, <a href="http://arxiv.org/find/stat/1/au:+Dsouza_D/0/1/0/all/0/1">Daniel D&#x27;souza</a>, <a href="http://arxiv.org/find/stat/1/au:+Nguyen_K/0/1/0/all/0/1">Karina Nguyen</a>, <a href="http://arxiv.org/find/stat/1/au:+Balestriero_R/0/1/0/all/0/1">Randall Balestriero</a>, <a href="http://arxiv.org/find/stat/1/au:+Hooker_S/0/1/0/all/0/1">Sara Hooker</a></p>
<p>Ensembling multiple Deep Neural Networks (DNNs) is a simple and effective way
to improve top-line metrics and to outperform a larger single model. In this
work, we go beyond top-line metrics and instead explore the impact of
ensembling on subgroup performances. Surprisingly, we observe that even with a
simple homogeneous ensemble -- all the individual DNNs share the same training
set, architecture, and design choices -- the minority group performance
disproportionately improves with the number of models compared to the majority
group, i.e. fairness naturally emerges from ensembling. Even more surprising,
we find that this gain keeps occurring even when a large number of models is
considered, e.g. $20$, despite the fact that the average performance of the
ensemble plateaus with fewer models. Our work establishes that simple DNN
ensembles can be a powerful tool for alleviating disparate impact from DNN
classifiers, thus curbing algorithmic harm. We also explore why this is the
case. We find that even in homogeneous ensembles, varying the sources of
stochasticity through parameter initialization, mini-batch sampling, and
data-augmentation realizations, results in different fairness outcomes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.02846">Contrastive variational information bottleneck for aspect-based sentiment analysis. (arXiv:2303.02846v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1">Mingshan Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Min Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1">Qingshan Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">Ruifeng Xu</a></p>
<p>Deep learning techniques have dominated the literature on aspect-based
sentiment analysis (ABSA), achieving state-of-the-art performance. However,
deep models generally suffer from spurious correlations between input features
and output labels, which hurts the robustness and generalization capability by
a large margin. In this paper, we propose to reduce spurious correlations for
ABSA, via a novel Contrastive Variational Information Bottleneck framework
(called CVIB). The proposed CVIB framework is composed of an original network
and a self-pruned network, and these two networks are optimized simultaneously
via contrastive learning. Concretely, we employ the Variational Information
Bottleneck (VIB) principle to learn an informative and compressed network
(self-pruned network) from the original network, which discards the superfluous
patterns or spurious correlations between input features and prediction labels.
Then, self-pruning contrastive learning is devised to pull together
semantically similar positive pairs and push away dissimilar pairs, where the
representations of the anchor learned by the original and self-pruned networks
respectively are regarded as a positive pair while the representations of two
different sentences within a mini-batch are treated as a negative pair. To
verify the effectiveness of our CVIB method, we conduct extensive experiments
on five benchmark ABSA datasets and the experimental results show that our
approach achieves better performance than the strong competitors in terms of
overall prediction performance, robustness, and generalization. Code and data
to reproduce the results in this paper is available at:
https://github.com/shesshan/CVIB.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.09449">Proof Number Based Monte-Carlo Tree Search. (arXiv:2303.09449v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kowalski_J/0/1/0/all/0/1">Jakub Kowalski</a>, <a href="http://arxiv.org/find/cs/1/au:+Doe_E/0/1/0/all/0/1">Elliot Doe</a>, <a href="http://arxiv.org/find/cs/1/au:+Winands_M/0/1/0/all/0/1">Mark H. M. Winands</a>, <a href="http://arxiv.org/find/cs/1/au:+Gorski_D/0/1/0/all/0/1">Daniel G&#xf3;rski</a>, <a href="http://arxiv.org/find/cs/1/au:+Soemers_D/0/1/0/all/0/1">Dennis J. N. J. Soemers</a></p>
<p>This paper proposes a new game-search algorithm, PN-MCTS, which combines
Monte-Carlo Tree Search (MCTS) and Proof-Number Search (PNS). These two
algorithms have been successfully applied for decision making in a range of
domains. We define three areas where the additional knowledge provided by the
proof and disproof numbers gathered in MCTS trees might be used: final move
selection, solving subtrees, and the UCB1 selection mechanism. We test all
possible combinations on different time settings, playing against vanilla UCT
on several games: Lines of Action ($7$$\times$$7$ and $8$$\times$$8$ board
sizes), MiniShogi, Knightthrough, and Awari. Furthermore, we extend this new
algorithm to properly address games with draws, like Awari, by adding an
additional layer of PNS on top of the MCTS tree. The experiments show that
PN-MCTS confidently outperforms MCTS in all tested game domains, achieving win
rates up to 96.2\% for Lines of Action.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.14496">Learning with Explanation Constraints. (arXiv:2303.14496v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pukdee_R/0/1/0/all/0/1">Rattana Pukdee</a>, <a href="http://arxiv.org/find/cs/1/au:+Sam_D/0/1/0/all/0/1">Dylan Sam</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1">J. Zico Kolter</a>, <a href="http://arxiv.org/find/cs/1/au:+Balcan_M/0/1/0/all/0/1">Maria-Florina Balcan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ravikumar_P/0/1/0/all/0/1">Pradeep Ravikumar</a></p>
<p>As larger deep learning models are hard to interpret, there has been a recent
focus on generating explanations of these black-box models. In contrast, we may
have apriori explanations of how models should behave. In this paper, we
formalize this notion as learning from explanation constraints and provide a
learning theoretic framework to analyze how such explanations can improve the
learning of our models. One may naturally ask, "When would these explanations
be helpful?" Our first key contribution addresses this question via a class of
models that satisfies these explanation constraints in expectation over new
data. We provide a characterization of the benefits of these models (in terms
of the reduction of their Rademacher complexities) for a canonical class of
explanations given by gradient information in the settings of both linear
models and two layer neural networks. In addition, we provide an algorithmic
solution for our framework, via a variational approximation that achieves
better performance and satisfies these constraints more frequently, when
compared to simpler augmented Lagrangian methods to incorporate these
explanations. We demonstrate the benefits of our approach over a large array of
synthetic and real-world experiments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.17564">BloombergGPT: A Large Language Model for Finance. (arXiv:2303.17564v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Shijie Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Irsoy_O/0/1/0/all/0/1">Ozan Irsoy</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1">Steven Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dabravolski_V/0/1/0/all/0/1">Vadim Dabravolski</a>, <a href="http://arxiv.org/find/cs/1/au:+Dredze_M/0/1/0/all/0/1">Mark Dredze</a>, <a href="http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1">Sebastian Gehrmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Kambadur_P/0/1/0/all/0/1">Prabhanjan Kambadur</a>, <a href="http://arxiv.org/find/cs/1/au:+Rosenberg_D/0/1/0/all/0/1">David Rosenberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Mann_G/0/1/0/all/0/1">Gideon Mann</a></p>
<p>The use of NLP in the realm of financial technology is broad and complex,
with applications ranging from sentiment analysis and named entity recognition
to question answering. Large Language Models (LLMs) have been shown to be
effective on a variety of tasks; however, no LLM specialized for the financial
domain has been reported in literature. In this work, we present BloombergGPT,
a 50 billion parameter language model that is trained on a wide range of
financial data. We construct a 363 billion token dataset based on Bloomberg's
extensive data sources, perhaps the largest domain-specific dataset yet,
augmented with 345 billion tokens from general purpose datasets. We validate
BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite
of internal benchmarks that most accurately reflect our intended usage. Our
mixed dataset training leads to a model that outperforms existing models on
financial tasks by significant margins without sacrificing performance on
general LLM benchmarks. Additionally, we explain our modeling choices, training
process, and evaluation methodology. We release Training Chronicles (Appendix
C) detailing our experience in training BloombergGPT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.06762">Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study. (arXiv:2304.06762v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Boxin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1">Wei Ping</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1">Peng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+McAfee_L/0/1/0/all/0/1">Lawrence McAfee</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zihan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1">Mohammad Shoeybi</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1">Yi Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuchaiev_O/0/1/0/all/0/1">Oleksii Kuchaiev</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Chaowei Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1">Anima Anandkumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1">Bryan Catanzaro</a></p>
<p>Large decoder-only language models (LMs) can be largely improved in terms of
perplexity by retrieval (e.g., RETRO), but its impact on text generation
quality and downstream task accuracy is unclear. Thus, it is still an open
question: shall we pretrain large autoregressive LMs with retrieval? To answer
it, we perform a comprehensive study on a scalable pre-trained
retrieval-augmented LM (i.e., RETRO) compared with standard GPT and
retrieval-augmented GPT incorporated at fine-tuning or inference stages. We
first provide the recipe to reproduce RETRO up to 9.5B parameters while
retrieving a text corpus with 330B tokens. Based on that, we have the following
novel findings: i) RETRO outperforms GPT on text generation with much less
degeneration (i.e., repetition), moderately higher factual accuracy, and
slightly lower toxicity with a nontoxic retrieval database. ii) On the LM
Evaluation Harness benchmark, RETRO largely outperforms GPT on
knowledge-intensive tasks, but is on par with GPT on other tasks. Furthermore,
we introduce a simple variant of the model, RETRO++, which largely improves
open-domain QA results of original RETRO (e.g., EM score +8.6 on Natural
Question) and significantly outperforms retrieval-augmented GPT in both
fine-tuning and zero-shot evaluation settings. Our findings highlight the
promising direction of pretraining autoregressive LMs with retrieval as future
foundation models. We release our code and model at:
https://github.com/NVIDIA/Megatron-LM/blob/main/tools/retro/README.md
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.10549">A note on the connectedness property of union-free generic sets of partial orders. (arXiv:2304.10549v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schollmeyer_G/0/1/0/all/0/1">Georg Schollmeyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Blocher_H/0/1/0/all/0/1">Hannah Blocher</a></p>
<p>This short note describes and proves a connectedness property which was
introduced in Blocher et al. [2023] in the context of data depth functions for
partial orders. The connectedness property gives a structural insight into
union-free generic sets. These sets, presented in Blocher et al. [2023], are
defined by using a closure operator on the set of all partial orders which
naturally appears within the theory of formal concept analysis. In the language
of formal concept analysis, the property of connectedness can be vividly
proven. However, since within Blocher et al. [2023] we did not discuss formal
concept analysis, we outsourced the proof to this note.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.05807">Even Small Correlation and Diversity Shifts Pose Dataset-Bias Issues. (arXiv:2305.05807v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bissoto_A/0/1/0/all/0/1">Alceu Bissoto</a>, <a href="http://arxiv.org/find/cs/1/au:+Barata_C/0/1/0/all/0/1">Catarina Barata</a>, <a href="http://arxiv.org/find/cs/1/au:+Valle_E/0/1/0/all/0/1">Eduardo Valle</a>, <a href="http://arxiv.org/find/cs/1/au:+Avila_S/0/1/0/all/0/1">Sandra Avila</a></p>
<p>Distribution shifts are common in real-world datasets and can affect the
performance and reliability of deep learning models. In this paper, we study
two types of distribution shifts: diversity shifts, which occur when test
samples exhibit patterns unseen during training, and correlation shifts, which
occur when test data present a different correlation between seen invariant and
spurious features. We propose an integrated protocol to analyze both types of
shifts using datasets where they co-exist in a controllable manner. Finally, we
apply our approach to a real-world classification problem of skin cancer
analysis, using out-of-distribution datasets and specialized bias annotations.
Our protocol reveals three findings: 1) Models learn and propagate correlation
shifts even with low-bias training; this poses a risk of accumulating and
combining unaccountable weak biases; 2) Models learn robust features in high-
and low-bias scenarios but use spurious ones if test samples have them; this
suggests that spurious correlations do not impair the learning of robust
features; 3) Diversity shift can reduce the reliance on spurious correlations;
this is counter intuitive since we expect biased models to depend more on
biases when invariant features are missing. Our work has implications for
distribution shift research and practice, providing new insights into how
models learn and rely on spurious correlations under different types of shifts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15194">DiffBlender: Scalable and Composable Multimodal Text-to-Image Diffusion Models. (arXiv:2305.15194v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Sungnyun Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Junsoo Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_K/0/1/0/all/0/1">Kibeom Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1">Daesik Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahn_N/0/1/0/all/0/1">Namhyuk Ahn</a></p>
<p>In this study, we aim to extend the capabilities of diffusion-based
text-to-image (T2I) generation models by incorporating diverse modalities
beyond textual description, such as sketch, box, color palette, and style
embedding, within a single model. We thus design a multimodal T2I diffusion
model, coined as DiffBlender, by separating the channels of conditions into
three types, i.e., image forms, spatial tokens, and non-spatial tokens. The
unique architecture of DiffBlender facilitates adding new input modalities,
pioneering a scalable framework for conditional image generation. Notably, we
achieve this without altering the parameters of the existing generative model,
Stable Diffusion, only with updating partial components. Our study establishes
new benchmarks in multimodal generation through quantitative and qualitative
comparisons with existing conditional generation methods. We demonstrate that
DiffBlender faithfully blends all the provided information and showcase its
various applications in the detailed image synthesis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.01665">SourceP: Detecting Ponzi Schemes on Ethereum with Source Code. (arXiv:2306.01665v7 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1">Pengcheng Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1">Liang Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1">Keting Yin</a></p>
<p>As blockchain technology becomes more and more popular, a typical financial
scam, the Ponzi scheme, has also emerged in the blockchain platform Ethereum.
This Ponzi scheme deployed through smart contracts, also known as the smart
Ponzi scheme, has caused a lot of economic losses and negative impacts.
Existing methods for detecting smart Ponzi schemes on Ethereum mainly rely on
bytecode features, opcode features, account features, and transaction behavior
features of smart contracts, which are unable to truly characterize the
behavioral features of Ponzi schemes, and thus generally perform poorly in
terms of detection accuracy and false alarm rates. In this paper, we propose
SourceP, a method to detect smart Ponzi schemes on the Ethereum platform using
pre-trained models and data flow, which only requires using the source code of
smart contracts as features. SourceP reduces the difficulty of data acquisition
and feature extraction of existing detection methods. Specifically, we first
convert the source code of a smart contract into a data flow graph and then
introduce a pre-trained model based on learning code representations to build a
classification model to identify Ponzi schemes in smart contracts. The
experimental results show that SourceP achieves 87.2\% recall and 90.7\%
F-score for detecting smart Ponzi schemes within Ethereum's smart contract
dataset, outperforming state-of-the-art methods in terms of performance and
sustainability. We also demonstrate through additional experiments that
pre-trained models and data flow play an important contribution to SourceP, as
well as proving that SourceP has a good generalization ability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.09200">ChessGPT: Bridging Policy Learning and Language Modeling. (arXiv:2306.09200v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1">Xidong Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1">Yicheng Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Ziyan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1">Hongrui Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Mengyue Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_K/0/1/0/all/0/1">Kun Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Mguni_D/0/1/0/all/0/1">David Mguni</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1">Yali Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jun Wang</a></p>
<p>When solving decision-making tasks, humans typically depend on information
from two key sources: (1) Historical policy data, which provides interaction
replay from the environment, and (2) Analytical insights in natural language
form, exposing the invaluable thought process or strategic considerations.
Despite this, the majority of preceding research focuses on only one source:
they either use historical replay exclusively to directly learn policy or value
functions, or engaged in language model training utilizing mere language
corpus. In this paper, we argue that a powerful autonomous agent should cover
both sources. Thus, we propose ChessGPT, a GPT model bridging policy learning
and language modeling by integrating data from these two sources in Chess
games. Specifically, we build a large-scale game and language dataset related
to chess. Leveraging the dataset, we showcase two model examples ChessCLIP and
ChessGPT, integrating policy learning and language modeling. Finally, we
propose a full evaluation framework for evaluating language model's chess
ability. Experimental results validate our model and dataset's effectiveness.
We open source our code, model, and dataset at
https://github.com/waterhorse1/ChessGPT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.00764">Hierarchical Open-vocabulary Universal Image Segmentation. (arXiv:2307.00764v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xudong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shufan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Kallidromitis_K/0/1/0/all/0/1">Konstantinos Kallidromitis</a>, <a href="http://arxiv.org/find/cs/1/au:+Kato_Y/0/1/0/all/0/1">Yusuke Kato</a>, <a href="http://arxiv.org/find/cs/1/au:+Kozuka_K/0/1/0/all/0/1">Kazuki Kozuka</a>, <a href="http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1">Trevor Darrell</a></p>
<p>Open-vocabulary image segmentation aims to partition an image into semantic
regions according to arbitrary text descriptions. However, complex visual
scenes can be naturally decomposed into simpler parts and abstracted at
multiple levels of granularity, introducing inherent segmentation ambiguity.
Unlike existing methods that typically sidestep this ambiguity and treat it as
an external factor, our approach actively incorporates a hierarchical
representation encompassing different semantic-levels into the learning
process. We propose a decoupled text-image fusion mechanism and representation
learning modules for both "things" and "stuff". Additionally, we systematically
examine the differences that exist in the textual and visual features between
these types of categories. Our resulting model, named HIPIE, tackles
HIerarchical, oPen-vocabulary, and unIvErsal segmentation tasks within a
unified framework. Benchmarked on over 40 datasets, e.g., ADE20K, COCO,
Pascal-VOC Part, RefCOCO/RefCOCOg, ODinW and SeginW, HIPIE achieves the
state-of-the-art results at various levels of image comprehension, including
semantic-level (e.g., semantic segmentation), instance-level (e.g.,
panoptic/referring segmentation and object detection), as well as part-level
(e.g., part/subpart segmentation) tasks. Our code is released at
https://github.com/berkeley-hipie/HIPIE.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05722">Exploring Large Language Model for Graph Data Understanding in Online Job Recommendations. (arXiv:2307.05722v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1">Likang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1">Zhaopeng Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Zhi Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1">Hengshu Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1">Enhong Chen</a></p>
<p>Large Language Models (LLMs) have revolutionized natural language processing
tasks, demonstrating their exceptional capabilities in various domains.
However, their potential for behavior graph understanding in job
recommendations remains largely unexplored. This paper focuses on unveiling the
capability of large language models in understanding behavior graphs and
leveraging this understanding to enhance recommendations in online recruitment,
including the promotion of out-of-distribution (OOD) application. We present a
novel framework that harnesses the rich contextual information and semantic
representations provided by large language models to analyze behavior graphs
and uncover underlying patterns and relationships. Specifically, we propose a
meta-path prompt constructor that leverages LLM recommender to understand
behavior graphs for the first time and design a corresponding path augmentation
module to alleviate the prompt bias introduced by path-based sequence input. By
leveraging this capability, our framework enables personalized and accurate job
recommendations for individual users. We evaluate the effectiveness of our
approach on a comprehensive dataset and demonstrate its ability to improve the
relevance and quality of recommended quality. This research not only sheds
light on the untapped potential of large language models but also provides
valuable insights for developing advanced recommendation systems in the
recruitment market. The findings contribute to the growing field of natural
language processing and offer practical implications for enhancing job search
experiences. We release the code at https://github.com/WLiK/GLRec.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06971">Short Boolean Formulas as Explanations in Practice. (arXiv:2307.06971v2 [cs.LO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jaakkola_R/0/1/0/all/0/1">Reijo Jaakkola</a>, <a href="http://arxiv.org/find/cs/1/au:+Janhunen_T/0/1/0/all/0/1">Tomi Janhunen</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuusisto_A/0/1/0/all/0/1">Antti Kuusisto</a>, <a href="http://arxiv.org/find/cs/1/au:+Rankooh_M/0/1/0/all/0/1">Masood Feyzbakhsh Rankooh</a>, <a href="http://arxiv.org/find/cs/1/au:+Vilander_M/0/1/0/all/0/1">Miikka Vilander</a></p>
<p>We investigate explainability via short Boolean formulas in the data model
based on unary relations. As an explanation of length k, we take a Boolean
formula of length k that minimizes the error with respect to the target
attribute to be explained. We first provide novel quantitative bounds for the
expected error in this scenario. We then also demonstrate how the setting works
in practice by studying three concrete data sets. In each case, we calculate
explanation formulas of different lengths using an encoding in Answer Set
Programming. The most accurate formulas we obtain achieve errors similar to
other methods on the same data sets. However, due to overfitting, these
formulas are not necessarily ideal explanations, so we use cross validation to
identify a suitable length for explanations. By limiting to shorter formulas,
we obtain explanations that avoid overfitting but are still reasonably accurate
and also, importantly, human interpretable.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.15043">Universal and Transferable Adversarial Attacks on Aligned Language Models. (arXiv:2307.15043v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zou_A/0/1/0/all/0/1">Andy Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zifan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1">Nicholas Carlini</a>, <a href="http://arxiv.org/find/cs/1/au:+Nasr_M/0/1/0/all/0/1">Milad Nasr</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1">J. Zico Kolter</a>, <a href="http://arxiv.org/find/cs/1/au:+Fredrikson_M/0/1/0/all/0/1">Matt Fredrikson</a></p>
<p>Because "out-of-the-box" large language models are capable of generating a
great deal of objectionable content, recent work has focused on aligning these
models in an attempt to prevent undesirable generation. While there has been
some success at circumventing these measures -- so-called "jailbreaks" against
LLMs -- these attacks have required significant human ingenuity and are brittle
in practice. In this paper, we propose a simple and effective attack method
that causes aligned language models to generate objectionable behaviors.
Specifically, our approach finds a suffix that, when attached to a wide range
of queries for an LLM to produce objectionable content, aims to maximize the
probability that the model produces an affirmative response (rather than
refusing to answer). However, instead of relying on manual engineering, our
approach automatically produces these adversarial suffixes by a combination of
greedy and gradient-based search techniques, and also improves over past
automatic prompt generation methods.
</p>
<p>Surprisingly, we find that the adversarial prompts generated by our approach
are quite transferable, including to black-box, publicly released LLMs.
Specifically, we train an adversarial attack suffix on multiple prompts (i.e.,
queries asking for many different types of objectionable content), as well as
multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting
attack suffix is able to induce objectionable content in the public interfaces
to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat,
Pythia, Falcon, and others. In total, this work significantly advances the
state-of-the-art in adversarial attacks against aligned language models,
raising important questions about how such systems can be prevented from
producing objectionable information. Code is available at
github.com/llm-attacks/llm-attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.15254">Multiple Instance Learning Framework with Masked Hard Instance Mining for Whole Slide Image Classification. (arXiv:2307.15254v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1">Wenhao Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Sheng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaoxian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1">Fengtao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bo Liu</a></p>
<p>The whole slide image (WSI) classification is often formulated as a multiple
instance learning (MIL) problem. Since the positive tissue is only a small
fraction of the gigapixel WSI, existing MIL methods intuitively focus on
identifying salient instances via attention mechanisms. However, this leads to
a bias towards easy-to-classify instances while neglecting hard-to-classify
instances. Some literature has revealed that hard examples are beneficial for
modeling a discriminative boundary accurately. By applying such an idea at the
instance level, we elaborate a novel MIL framework with masked hard instance
mining (MHIM-MIL), which uses a Siamese structure (Teacher-Student) with a
consistency constraint to explore the potential hard instances. With several
instance masking strategies based on attention scores, MHIM-MIL employs a
momentum teacher to implicitly mine hard instances for training the student
model, which can be any attention-based MIL model. This counter-intuitive
strategy essentially enables the student to learn a better discriminating
boundary. Moreover, the student is used to update the teacher with an
exponential moving average (EMA), which in turn identifies new hard instances
for subsequent training iterations and stabilizes the optimization.
Experimental results on the CAMELYON-16 and TCGA Lung Cancer datasets
demonstrate that MHIM-MIL outperforms other latest methods in terms of
performance and training cost. The code is available at:
https://github.com/DearCaat/MHIM-MIL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.08309">A Framework for Data-Driven Explainability in Mathematical Optimization. (arXiv:2308.08309v2 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Aigner_K/0/1/0/all/0/1">Kevin-Martin Aigner</a>, <a href="http://arxiv.org/find/math/1/au:+Goerigk_M/0/1/0/all/0/1">Marc Goerigk</a>, <a href="http://arxiv.org/find/math/1/au:+Hartisch_M/0/1/0/all/0/1">Michael Hartisch</a>, <a href="http://arxiv.org/find/math/1/au:+Liers_F/0/1/0/all/0/1">Frauke Liers</a>, <a href="http://arxiv.org/find/math/1/au:+Miehlich_A/0/1/0/all/0/1">Arthur Miehlich</a></p>
<p>Advancements in mathematical programming have made it possible to efficiently
tackle large-scale real-world problems that were deemed intractable just a few
decades ago. However, provably optimal solutions may not be accepted due to the
perception of optimization software as a black box. Although well understood by
scientists, this lacks easy accessibility for practitioners. Hence, we advocate
for introducing the explainability of a solution as another evaluation
criterion, next to its objective value, which enables us to find trade-off
solutions between these two criteria. Explainability is attained by comparing
against (not necessarily optimal) solutions that were implemented in similar
situations in the past. Thus, solutions are preferred that exhibit similar
features. Although we prove that already in simple cases the explainable model
is NP-hard, we characterize relevant polynomially solvable cases such as the
explainable shortest path problem. Our numerical experiments on both artificial
as well as real-world road networks show the resulting Pareto front. It turns
out that the cost of enforcing explainability can be very small.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.08746">SurgicalSAM: Efficient Class Promptable Surgical Instrument Segmentation. (arXiv:2308.08746v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yue_W/0/1/0/all/0/1">Wenxi Yue</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1">Kun Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1">Yong Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1">Jiebo Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhiyong Wang</a></p>
<p>The Segment Anything Model (SAM) is a powerful foundation model that has
revolutionised image segmentation. To apply SAM to surgical instrument
segmentation, a common approach is to locate precise points or boxes of
instruments and then use them as prompts for SAM in a zero-shot manner.
However, we observe two problems with this naive pipeline: (1) the domain gap
between natural objects and surgical instruments leads to inferior
generalisation of SAM; and (2) SAM relies on precise point or box locations for
accurate segmentation, requiring either extensive manual guidance or a
well-performing specialist detector for prompt preparation, which leads to a
complex multi-stage pipeline. To address these problems, we introduce
SurgicalSAM, a novel end-to-end efficient-tuning approach for SAM to
effectively integrate surgical-specific information with SAM's pre-trained
knowledge for improved generalisation. Specifically, we propose a lightweight
prototype-based class prompt encoder for tuning, which directly generates
prompt embeddings from class prototypes and eliminates the use of explicit
prompts for improved robustness and a simpler pipeline. In addition, to address
the low inter-class variance among surgical instrument categories, we propose
contrastive prototype learning, further enhancing the discrimination of the
class prototypes for more accurate class prompting. The results of extensive
experiments on both EndoVis2018 and EndoVis2017 datasets demonstrate that
SurgicalSAM achieves state-of-the-art performance while only requiring a small
number of tunable parameters. The source code is available at
https://github.com/wenxi-yue/SurgicalSAM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12466">Are ChatGPT and GPT-4 Good Poker Players? -- A Pre-Flop Analysis. (arXiv:2308.12466v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1">Akshat Gupta</a></p>
<p>Since the introduction of ChatGPT and GPT-4, these models have been tested
across a large number of tasks. Their adeptness across domains is evident, but
their aptitude in playing games, and specifically their aptitude in the realm
of poker has remained unexplored. Poker is a game that requires decision making
under uncertainty and incomplete information. In this paper, we put ChatGPT and
GPT-4 through the poker test and evaluate their poker skills. Our findings
reveal that while both models display an advanced understanding of poker,
encompassing concepts like the valuation of starting hands, playing positions
and other intricacies of game theory optimal (GTO) poker, both ChatGPT and
GPT-4 are NOT game theory optimal poker players.
</p>
<p>Profitable strategies in poker are evaluated in expectations over large
samples. Through a series of experiments, we first discover the characteristics
of optimal prompts and model parameters for playing poker with these models.
Our observations then unveil the distinct playing personas of the two models.
We first conclude that GPT-4 is a more advanced poker player than ChatGPT. This
exploration then sheds light on the divergent poker tactics of the two models:
ChatGPT's conservativeness juxtaposed against GPT-4's aggression. In poker
vernacular, when tasked to play GTO poker, ChatGPT plays like a nit, which
means that it has a propensity to only engage with premium hands and folds a
majority of hands. When subjected to the same directive, GPT-4 plays like a
maniac, showcasing a loose and aggressive style of play. Both strategies,
although relatively advanced, are not game theory optimal.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.14034">Confucius: Iterative Tool Learning from Introspection Feedback by Easy-to-Difficult Curriculum. (arXiv:2308.14034v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1">Shen Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1">Zhengliang Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1">Minghang Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_B/0/1/0/all/0/1">Bowen Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xin_X/0/1/0/all/0/1">Xin Xin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1">Pengjie Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhumin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1">Jun Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1">Zhaochun Ren</a></p>
<p>Augmenting large language models (LLMs) with external tools has emerged as a
promising approach to extending the capability of LLMs. Although some works
employ open-source LLMs for the tool learning task, most of them are trained in
a controlled environment in which LLMs only learn to execute the human-provided
tools. However, selecting proper tools from the large toolset is also a crucial
ability for the tool learning model to be applied in real-world applications.
Existing methods usually directly employ self-instruction methods to train the
model, which ignores differences in tool complexity. In this paper, we propose
the Confucius, a novel tool learning framework to train LLM to use complicated
tools in real-world scenarios, which contains two main phases: (1) We first
propose a multi-stage learning method to teach the LLM to use various tools
from an easy-to-difficult curriculum; (2) thenceforth, we propose the Iterative
Self-instruct from Introspective Feedback (ISIF) to dynamically construct the
dataset to improve the ability to use the complicated tool. Extensive
experiments conducted on both controlled and real-world settings demonstrate
the superiority of our tool learning framework in the real-world application
scenarios compared to both tuning-free (e.g. ChatGPT, Claude) and tuning-based
baselines (e.g. GPT4Tools).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12559">Invariant Learning via Probability of Sufficient and Necessary Causes. (arXiv:2309.12559v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Mengyue Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1">Zhen Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yonggang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1">Yali Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1">Furui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ton_J/0/1/0/all/0/1">Jean-Francois Ton</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jianhong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jun Wang</a></p>
<p>Out-of-distribution (OOD) generalization is indispensable for learning models
in the wild, where testing distribution typically unknown and different from
the training. Recent methods derived from causality have shown great potential
in achieving OOD generalization. However, existing methods mainly focus on the
invariance property of causes, while largely overlooking the property of
\textit{sufficiency} and \textit{necessity} conditions. Namely, a necessary but
insufficient cause (feature) is invariant to distribution shift, yet it may not
have required accuracy. By contrast, a sufficient yet unnecessary cause
(feature) tends to fit specific data well but may have a risk of adapting to a
new domain. To capture the information of sufficient and necessary causes, we
employ a classical concept, the probability of sufficiency and necessary causes
(PNS), which indicates the probability of whether one is the necessary and
sufficient cause. To associate PNS with OOD generalization, we propose PNS risk
and formulate an algorithm to learn representation with a high PNS value. We
theoretically analyze and prove the generalizability of the PNS risk.
Experiments on both synthetic and real-world benchmarks demonstrate the
effectiveness of the proposed method. The details of the implementation can be
found at the GitHub repository: https://github.com/ymy4323460/CaSN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.13439">Finding Order in Chaos: A Novel Data Augmentation Method for Time Series in Contrastive Learning. (arXiv:2309.13439v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Demirel_B/0/1/0/all/0/1">Berken Utku Demirel</a>, <a href="http://arxiv.org/find/cs/1/au:+Holz_C/0/1/0/all/0/1">Christian Holz</a></p>
<p>The success of contrastive learning is well known to be dependent on data
augmentation. Although the degree of data augmentations has been well
controlled by utilizing pre-defined techniques in some domains like vision,
time-series data augmentation is less explored and remains a challenging
problem due to the complexity of the data generation mechanism, such as the
intricate mechanism involved in the cardiovascular system. Moreover, there is
no widely recognized and general time-series augmentation method that can be
applied across different tasks. In this paper, we propose a novel data
augmentation method for quasi-periodic time-series tasks that aims to connect
intra-class samples together, and thereby find order in the latent space. Our
method builds upon the well-known mixup technique by incorporating a novel
approach that accounts for the periodic nature of non-stationary time-series.
Also, by controlling the degree of chaos created by data augmentation, our
method leads to improved feature representations and performance on downstream
tasks. We evaluate our proposed method on three time-series tasks, including
heart rate estimation, human activity recognition, and cardiovascular disease
detection. Extensive experiments against state-of-the-art methods show that the
proposed approach outperforms prior works on optimal data generation and known
data augmentation techniques in the three tasks, reflecting the effectiveness
of the presented method. Source code:
https://github.com/eth-siplab/Finding_Order_in_Chaos
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00526">Are Graph Neural Networks Optimal Approximation Algorithms?. (arXiv:2310.00526v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yau_M/0/1/0/all/0/1">Morris Yau</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_E/0/1/0/all/0/1">Eric Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Karalias_N/0/1/0/all/0/1">Nikolaos Karalias</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jessica Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jegelka_S/0/1/0/all/0/1">Stefanie Jegelka</a></p>
<p>In this work we design graph neural network architectures that can be used to
obtain optimal approximation algorithms for a large class of combinatorial
optimization problems using powerful algorithmic tools from semidefinite
programming (SDP). Concretely, we prove that polynomial-sized message passing
algorithms can represent the most powerful polynomial time algorithms for Max
Constraint Satisfaction Problems assuming the Unique Games Conjecture. We
leverage this result to construct efficient graph neural network architectures,
OptGNN, that obtain high-quality approximate solutions on landmark
combinatorial optimization problems such as Max Cut and maximum independent
set. Our approach achieves strong empirical results across a wide range of
real-world and synthetic datasets against both neural baselines and classical
algorithms. Finally, we take advantage of OptGNN's ability to capture convex
relaxations to design an algorithm for producing dual certificates of
optimality (bounds on the optimal solution) from the learned embeddings of
OptGNN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02679">Diffusion Generative Flow Samplers: Improving learning signals through partial trajectory optimization. (arXiv:2310.02679v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dinghuai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1">Ricky T. Q. Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Cheng-Hao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1">Aaron Courville</a>, <a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1">Yoshua Bengio</a></p>
<p>We tackle the problem of sampling from intractable high-dimensional density
functions, a fundamental task that often appears in machine learning and
statistics. We extend recent sampling-based approaches that leverage controlled
stochastic processes to model approximate samples from these target densities.
The main drawback of these approaches is that the training objective requires
full trajectories to compute, resulting in sluggish credit assignment issues
due to use of entire trajectories and a learning signal present only at the
terminal time. In this work, we present Diffusion Generative Flow Samplers
(DGFS), a sampling-based framework where the learning process can be tractably
broken down into short partial trajectory segments, via parameterizing an
additional "flow function". Our method takes inspiration from the theory
developed for generative flow networks (GFlowNets), allowing us to make use of
intermediate learning signals. Through various challenging experiments, we
demonstrate that DGFS achieves more accurate estimates of the normalization
constant than closely-related prior methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03780">Automating Human Tutor-Style Programming Feedback: Leveraging GPT-4 Tutor Model for Hint Generation and GPT-3.5 Student Model for Hint Validation. (arXiv:2310.03780v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Phung_T/0/1/0/all/0/1">Tung Phung</a>, <a href="http://arxiv.org/find/cs/1/au:+Padurean_V/0/1/0/all/0/1">Victor-Alexandru P&#x103;durean</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1">Anjali Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Brooks_C/0/1/0/all/0/1">Christopher Brooks</a>, <a href="http://arxiv.org/find/cs/1/au:+Cambronero_J/0/1/0/all/0/1">Jos&#xe9; Cambronero</a>, <a href="http://arxiv.org/find/cs/1/au:+Gulwani_S/0/1/0/all/0/1">Sumit Gulwani</a>, <a href="http://arxiv.org/find/cs/1/au:+Singla_A/0/1/0/all/0/1">Adish Singla</a>, <a href="http://arxiv.org/find/cs/1/au:+Soares_G/0/1/0/all/0/1">Gustavo Soares</a></p>
<p>Generative AI and large language models hold great promise in enhancing
programming education by automatically generating individualized feedback for
students. We investigate the role of generative AI models in providing human
tutor-style programming hints to help students resolve errors in their buggy
programs. Recent works have benchmarked state-of-the-art models for various
feedback generation scenarios; however, their overall quality is still inferior
to human tutors and not yet ready for real-world deployment. In this paper, we
seek to push the limits of generative AI models toward providing high-quality
programming hints and develop a novel technique, GPT4Hints-GPT3.5Val. As a
first step, our technique leverages GPT-4 as a ``tutor'' model to generate
hints -- it boosts the generative quality by using symbolic information of
failing test cases and fixes in prompts. As a next step, our technique
leverages GPT-3.5, a weaker model, as a ``student'' model to further validate
the hint quality -- it performs an automatic quality validation by simulating
the potential utility of providing this feedback. We show the efficacy of our
technique via extensive evaluation using three real-world datasets of Python
programs covering a variety of concepts ranging from basic algorithms to
regular expressions and data analysis using pandas library.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.18608">Embedding in Recommender Systems: A Survey. (arXiv:2310.18608v2 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xiangyu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Maolin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xinjian Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiansheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1">Shucheng Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1">Dawei Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qing Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jiliang Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1">Ruocheng Guo</a></p>
<p>Recommender systems have become an essential component of many online
platforms, providing personalized recommendations to users. A crucial aspect is
embedding techniques that coverts the high-dimensional discrete features, such
as user and item IDs, into low-dimensional continuous vectors and can enhance
the recommendation performance. Applying embedding techniques captures complex
entity relationships and has spurred substantial research. In this survey, we
provide an overview of the recent literature on embedding techniques in
recommender systems. This survey covers embedding methods like collaborative
filtering, self-supervised learning, and graph-based techniques. Collaborative
filtering generates embeddings capturing user-item preferences, excelling in
sparse data. Self-supervised methods leverage contrastive or generative
learning for various tasks. Graph-based techniques like node2vec exploit
complex relationships in network-rich environments. Addressing the scalability
challenges inherent to embedding methods, our survey delves into innovative
directions within the field of recommendation systems. These directions aim to
enhance performance and reduce computational complexity, paving the way for
improved recommender systems. Among these innovative approaches, we will
introduce Auto Machine Learning (AutoML), hash techniques, and quantization
techniques in this survey. We discuss various architectures and techniques and
highlight the challenges and future directions in these aspects. This survey
aims to provide a comprehensive overview of the state-of-the-art in this
rapidly evolving field and serve as a useful resource for researchers and
practitioners working in the area of recommender systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03830">Reducing Spatial Fitting Error in Distillation of Denoising Diffusion Models. (arXiv:2311.03830v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1">Shengzhe Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_Z/0/1/0/all/0/1">Zejian Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shengyuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1">Lefan Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Changyuan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1">Guang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhiyuan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Lingyun Sun</a></p>
<p>Denoising Diffusion models have exhibited remarkable capabilities in image
generation. However, generating high-quality samples requires a large number of
iterations. Knowledge distillation for diffusion models is an effective method
to address this limitation with a shortened sampling process but causes
degraded generative quality. Based on our analysis with bias-variance
decomposition and experimental observations, we attribute the degradation to
the spatial fitting error occurring in the training of both the teacher and
student model. Accordingly, we propose $\textbf{S}$patial
$\textbf{F}$itting-$\textbf{E}$rror $\textbf{R}$eduction
$\textbf{D}$istillation model ($\textbf{SFERD}$). SFERD utilizes attention
guidance from the teacher model and a designed semantic gradient predictor to
reduce the student's fitting error. Empirically, our proposed model facilitates
high-quality sample generation in a few function evaluations. We achieve an FID
of 5.31 on CIFAR-10 and 9.39 on ImageNet 64$\times$64 with only one step,
outperforming existing diffusion methods. Our study provides a new perspective
on diffusion distillation by highlighting the intrinsic denoising ability of
models. Project link: \url{https://github.com/Sainzerjj/SFERD}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05152">Cross-modal Prompts: Adapting Large Pre-trained Models for Audio-Visual Downstream Tasks. (arXiv:2311.05152v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Duan_H/0/1/0/all/0/1">Haoyi Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1">Yan Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1">Mingze Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1">Li Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jieming Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zhou Zhao</a></p>
<p>In recent years, the deployment of large-scale pre-trained models in
audio-visual downstream tasks has yielded remarkable outcomes. However, these
models, primarily trained on single-modality unconstrained datasets, still
encounter challenges in feature extraction for multi-modal tasks, leading to
suboptimal performance. This limitation arises due to the introduction of
irrelevant modality-specific information during encoding, which adversely
affects the performance of downstream tasks. To address this challenge, this
paper proposes a novel Dual-Guided Spatial-Channel-Temporal (DG-SCT) attention
mechanism. This mechanism leverages audio and visual modalities as soft prompts
to dynamically adjust the parameters of pre-trained models based on the current
multi-modal input features. Specifically, the DG-SCT module incorporates
trainable cross-modal interaction layers into pre-trained audio-visual
encoders, allowing adaptive extraction of crucial information from the current
modality across spatial, channel, and temporal dimensions, while preserving the
frozen parameters of large-scale pre-trained models. Experimental evaluations
demonstrate that our proposed model achieves state-of-the-art results across
multiple downstream tasks, including AVE, AVVP, AVS, and AVQA. Furthermore, our
model exhibits promising performance in challenging few-shot and zero-shot
scenarios. The source code and pre-trained models are available at
https://github.com/haoyi-duan/DG-SCT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10940">Unsupervised Estimation of Ensemble Accuracy. (arXiv:2311.10940v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Haber_S/0/1/0/all/0/1">Simi Haber</a>, <a href="http://arxiv.org/find/cs/1/au:+Wexler_Y/0/1/0/all/0/1">Yonatan Wexler</a></p>
<p>Ensemble learning combines several individual models to obtain a better
generalization performance. In this work we present a practical method for
estimating the joint power of several classifiers. It differs from existing
approaches which focus on "diversity" measures by not relying on labels. This
makes it both accurate and practical in the modern setting of unsupervised
learning with huge datasets.
</p>
<p>The heart of the method is a combinatorial bound on the number of mistakes
the ensemble is likely to make. The bound can be efficiently approximated in
time linear in the number of samples. We relate the bound to actual
misclassifications, hence its usefulness as a predictor of performance.
</p>
<p>We demonstrate the method on popular large-scale face recognition datasets
which provide a useful playground for fine-grain classification tasks using
noisy data over many classes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16512">CoSeR: Bridging Image and Language for Cognitive Super-Resolution. (arXiv:2311.16512v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Haoze Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wenbo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jianzhuang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Haoyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pei_R/0/1/0/all/0/1">Renjing Pei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1">Xueyi Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1">Youliang Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yujiu Yang</a></p>
<p>Existing super-resolution (SR) models primarily focus on restoring local
texture details, often neglecting the global semantic information within the
scene. This oversight can lead to the omission of crucial semantic details or
the introduction of inaccurate textures during the recovery process. In our
work, we introduce the Cognitive Super-Resolution (CoSeR) framework, empowering
SR models with the capacity to comprehend low-resolution images. We achieve
this by marrying image appearance and language understanding to generate a
cognitive embedding, which not only activates prior information from large
text-to-image diffusion models but also facilitates the generation of
high-quality reference images to optimize the SR process. To further improve
image fidelity, we propose a novel condition injection scheme called
"All-in-Attention", consolidating all conditional information into a single
module. Consequently, our method successfully restores semantically correct and
photorealistic details, demonstrating state-of-the-art performance across
multiple benchmarks. Code: https://github.com/VINHYU/CoSeR
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17458">Quantum Neural Networks under Depolarization Noise: Exploring White-Box Attacks and Defenses. (arXiv:2311.17458v2 [quant-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Winderl_D/0/1/0/all/0/1">David Winderl</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Franco_N/0/1/0/all/0/1">Nicola Franco</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Lorenz_J/0/1/0/all/0/1">Jeanette Miriam Lorenz</a></p>
<p>Leveraging the unique properties of quantum mechanics, Quantum Machine
Learning (QML) promises computational breakthroughs and enriched perspectives
where traditional systems reach their boundaries. However, similarly to
classical machine learning, QML is not immune to adversarial attacks. Quantum
adversarial machine learning has become instrumental in highlighting the weak
points of QML models when faced with adversarial crafted feature vectors.
Diving deep into this domain, our exploration shines light on the interplay
between depolarization noise and adversarial robustness. While previous results
enhanced robustness from adversarial threats through depolarization noise, our
findings paint a different picture. Interestingly, adding depolarization noise
discontinued the effect of providing further robustness for a multi-class
classification scenario. Consolidating our findings, we conducted experiments
with a multi-class classifier adversarially trained on gate-based quantum
simulators, further elucidating this unexpected behavior.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01057">RLHF and IIA: Perverse Incentives. (arXiv:2312.01057v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Wanqiao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1">Shi Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1">Xiuyuan Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lam_G/0/1/0/all/0/1">Grace Lam</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1">Zheng Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1">Benjamin Van Roy</a></p>
<p>Existing algorithms for reinforcement learning from human feedback (RLHF) can
incentivize responses at odds with preferences because they are based on models
that assume independence of irrelevant alternatives (IIA). The perverse
incentives induced by IIA give rise to egregious behavior when innovating on
query formats or learning algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05964">ConSequence: Synthesizing Logically Constrained Sequences for Electronic Health Record Generation. (arXiv:2312.05964v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Theodorou_B/0/1/0/all/0/1">Brandon Theodorou</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1">Shrusti Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Cao Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jimeng Sun</a></p>
<p>Generative models can produce synthetic patient records for analytical tasks
when real data is unavailable or limited. However, current methods struggle
with adhering to domain-specific knowledge and removing invalid data. We
present ConSequence, an effective approach to integrating domain knowledge into
sequential generative neural network outputs. Our rule-based formulation
includes temporal aggregation and antecedent evaluation modules, ensured by an
efficient matrix multiplication formulation, to satisfy hard and soft logical
constraints across time steps. Existing constraint methods often fail to
guarantee constraint satisfaction, lack the ability to handle temporal
constraints, and hinder the learning and computational efficiency of the model.
In contrast, our approach efficiently handles all types of constraints with
guaranteed logical coherence. We demonstrate ConSequence's effectiveness in
generating electronic health records, outperforming competitors in achieving
complete temporal and spatial constraint satisfaction without compromising
runtime performance or generative quality. Specifically, ConSequence
successfully prevents all rule violations while improving the model quality in
reducing its test perplexity by 5% and incurring less than a 13% slowdown in
generation speed compared to an unconstrained model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07069">Context Matters: Data-Efficient Augmentation of Large Language Models for Scientific Applications. (arXiv:2312.07069v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1">Haoran Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Siyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Ziwei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Maravi_A/0/1/0/all/0/1">Anurag Maravi</a>, <a href="http://arxiv.org/find/cs/1/au:+Abram_M/0/1/0/all/0/1">Marcin Abram</a></p>
<p>In this paper, we explore the challenges inherent to Large Language Models
(LLMs) like GPT-4, particularly their propensity for hallucinations, logic
mistakes, and incorrect conclusions when tasked with answering complex
questions. The capacity of LLMs to present erroneous answers in a coherent and
semantically rigorous manner further complicates the detection of factual
inaccuracies. This issue is especially pronounced in fields that require
specialized expertise. Our work delves into these challenges, aiming to enhance
the understanding and mitigation of such errors, thereby contributing to the
improvement of LLM accuracy and reliability in scientific and other specialized
domains. Our findings reveal a non-linear relationship between the context's
relevancy and the answers' measured quality. In addition, we demonstrate that
with the correct calibration, it is possible to automate the grading procedure
-- a finding suggesting that, at least to some degree, the LLMs can be used to
self-examine the quality of their own performance. Finally, we describe an
experimental platform that can be seen as a proof-of-concept of the techniques
described in this work.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07488">LMDrive: Closed-Loop End-to-End Driving with Large Language Models. (arXiv:2312.07488v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shao_H/0/1/0/all/0/1">Hao Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yuxuan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Letian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Waslander_S/0/1/0/all/0/1">Steven L. Waslander</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hongsheng Li</a></p>
<p>Despite significant recent progress in the field of autonomous driving,
modern methods still struggle and can incur serious accidents when encountering
long-tail unforeseen events and challenging urban scenarios. On the one hand,
large language models (LLM) have shown impressive reasoning capabilities that
approach "Artificial General Intelligence". On the other hand, previous
autonomous driving methods tend to rely on limited-format inputs (e.g. sensor
data and navigation waypoints), restricting the vehicle's ability to understand
language information and interact with humans. To this end, this paper
introduces LMDrive, a novel language-guided, end-to-end, closed-loop autonomous
driving framework. LMDrive uniquely processes and integrates multi-modal sensor
data with natural language instructions, enabling interaction with humans and
navigation software in realistic instructional settings. To facilitate further
research in language-based closed-loop autonomous driving, we also publicly
release the corresponding dataset which includes approximately 64K
instruction-following data clips, and the LangAuto benchmark that tests the
system's ability to handle complex instructions and challenging driving
scenarios. Extensive closed-loop experiments are conducted to demonstrate
LMDrive's effectiveness. To the best of our knowledge, we're the very first
work to leverage LLMs for closed-loop end-to-end autonomous driving. Codes,
models, and datasets can be found at https://github.com/opendilab/LMDrive
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10423">Stochastic Bayesian Optimization with Unknown Continuous Context Distribution via Kernel Density Estimation. (arXiv:2312.10423v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xiaobin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1">Lei Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_K/0/1/0/all/0/1">Ke Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1">Chao Qian</a></p>
<p>Bayesian optimization (BO) is a sample-efficient method and has been widely
used for optimizing expensive black-box functions. Recently, there has been a
considerable interest in BO literature in optimizing functions that are
affected by context variable in the environment, which is uncontrollable by
decision makers. In this paper, we focus on the optimization of functions'
expectations over continuous context variable, subject to an unknown
distribution. To address this problem, we propose two algorithms that employ
kernel density estimation to learn the probability density function (PDF) of
continuous context variable online. The first algorithm is simpler, which
directly optimizes the expectation under the estimated PDF. Considering that
the estimated PDF may have high estimation error when the true distribution is
complicated, we further propose the second algorithm that optimizes the
distributionally robust objective. Theoretical results demonstrate that both
algorithms have sub-linear Bayesian cumulative regret on the expectation
objective. Furthermore, we conduct numerical experiments to empirically
demonstrate the effectiveness of our algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11396">MAG-Edit: Localized Image Editing in Complex Scenarios via Mask-Based Attention-Adjusted Guidance. (arXiv:2312.11396v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mao_Q/0/1/0/all/0/1">Qi Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1">Yuchao Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1">Zhen Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1">Mike Zheng Shou</a></p>
<p>Recent diffusion-based image editing approaches have exhibited impressive
editing capabilities in images with simple compositions. However, localized
editing in complex scenarios has not been well-studied in the literature,
despite its growing real-world demands. Existing mask-based inpainting methods
fall short of retaining the underlying structure within the edit region.
Meanwhile, mask-free attention-based methods often exhibit editing leakage and
misalignment in more complex compositions. In this work, we develop MAG-Edit, a
training-free, inference-stage optimization method, which enables localized
image editing in complex scenarios. In particular, MAG-Edit optimizes the noise
latent feature in diffusion models by maximizing two mask-based cross-attention
constraints of the edit token, which in turn gradually enhances the local
alignment with the desired prompt. Extensive quantitative and qualitative
experiments demonstrate the effectiveness of our method in achieving both text
alignment and structure preservation for localized editing within complex
scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11460">Hybrid Internal Model: A Simple and Efficient Learner for Agile Legged Locomotion. (arXiv:2312.11460v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Long_J/0/1/0/all/0/1">Junfeng Long</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zirui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Quanyi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jiawei Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1">Liu Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1">Jiangmiao Pang</a></p>
<p>Robust locomotion control depends on accurate state estimations. However, the
sensors of most legged robots can only provide partial and noisy observations,
making the estimation particularly challenging, especially for external states
like terrain frictions and elevation maps. Inspired by the classical Internal
Model Control principle, we consider these external states as disturbances and
introduce Hybrid Internal Model (HIM) to estimate them according to the
response of the robot. The response, which we refer to as the hybrid internal
embedding, contains the robot's explicit velocity and implicit stability
representation, corresponding to two primary goals for locomotion tasks:
explicitly tracking velocity and implicitly maintaining stability. We use
contrastive learning to optimize the embedding to be close to the robot's
successor state, in which the response is naturally embedded. HIM has several
appealing benefits: It only needs the robot's proprioceptions, i.e., those from
joint encoders and IMU as observations. It innovatively maintains consistent
observations between simulation reference and reality that avoids information
loss in mimicking learning. It exploits batch-level information that is more
robust to noises and keeps better sample efficiency. It only requires 1 hour of
training on an RTX 4090 to enable a quadruped robot to traverse any terrain
under any disturbances. A wealth of real-world experiments demonstrates its
agility, even in high-difficulty tasks and cases never occurred during the
training process, revealing remarkable open-world generalizability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11562">A Survey of Reasoning with Foundation Models: Concepts, Methodologies, and Outlook. (arXiv:2312.11562v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jiankai Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1">Chuanyang Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1">Enze Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhengying Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_R/0/1/0/all/0/1">Ruihang Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1">Jianing Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jiaqi Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1">Mingyu Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hongyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Geng_M/0/1/0/all/0/1">Mengzhe Geng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yue Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenhai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Junsong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1">Zhangyue Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1">Xiaozhe Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1">Jie Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Junxian He</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1">Wu Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xihui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1">Hao Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1">Yu Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Ming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1">Pheng Ann Heng</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1">Jifeng Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1">Ping Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jingdong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1">Ji-Rong Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1">Xipeng Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yike Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1">Hui Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhenguo Li</a></p>
<p>Reasoning, a crucial ability for complex problem-solving, plays a pivotal
role in various real-world settings such as negotiation, medical diagnosis, and
criminal investigation. It serves as a fundamental methodology in the field of
Artificial General Intelligence (AGI). With the ongoing development of
foundation models, there is a growing interest in exploring their abilities in
reasoning tasks. In this paper, we introduce seminal foundation models proposed
or adaptable for reasoning, highlighting the latest advancements in various
reasoning tasks, methods, and benchmarks. We then delve into the potential
future directions behind the emergence of reasoning abilities within foundation
models. We also discuss the relevance of multimodal learning, autonomous
agents, and super alignment in the context of reasoning. By discussing these
future research directions, we hope to inspire researchers in their exploration
of this field, stimulate further advancements in reasoning with foundation
models, and contribute to the development of AGI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11779">Are you talking to [&#x27;xem&#x27;] or [&#x27;x&#x27;, &#x27;em&#x27;]? On Tokenization and Addressing Misgendering in LLMs with Pronoun Tokenization Parity. (arXiv:2312.11779v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ovalle_A/0/1/0/all/0/1">Anaelia Ovalle</a>, <a href="http://arxiv.org/find/cs/1/au:+Mehrabi_N/0/1/0/all/0/1">Ninareh Mehrabi</a>, <a href="http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1">Palash Goyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Dhamala_J/0/1/0/all/0/1">Jwala Dhamala</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1">Kai-Wei Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1">Richard Zemel</a>, <a href="http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1">Aram Galstyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1">Rahul Gupta</a></p>
<p>A large body of NLP research has documented the ways gender biases manifest
and amplify within large language models (LLMs), though this research has
predominantly operated within a gender binary-centric context. A growing body
of work has identified the harmful limitations of this gender-exclusive
framing; many LLMs cannot correctly and consistently refer to persons outside
the gender binary, especially if they use neopronouns. While data scarcity has
been identified as a possible culprit, the precise mechanisms through which it
influences LLM misgendering remain underexplored. Our work addresses this gap
by studying data scarcity's role in subword tokenization and, consequently, the
formation of LLM word representations. We uncover how the Byte-Pair Encoding
(BPE) tokenizer, a backbone for many popular LLMs, contributes to neopronoun
misgendering through out-of-vocabulary behavior. We introduce pronoun
tokenization parity (PTP), a novel approach to reduce LLM neopronoun
misgendering by preserving a token's functional structure. We evaluate PTP's
efficacy using pronoun consistency-based metrics and a novel syntax-based
metric. Through several controlled experiments, finetuning LLMs with PTP
improves neopronoun consistency from 14.5% to 58.4%, highlighting the
significant role tokenization plays in LLM pronoun consistency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11836">AiDAC: A Low-Cost In-Memory Computing Architecture with All-Analog Multi-Bit Compute and Interconnect. (arXiv:2312.11836v2 [cs.AR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xuan_Z/0/1/0/all/0/1">Zihao Xuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Song Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1">Yi Kang</a></p>
<p>Analog in-memory computing (AiMC) is an emerging technology that shows
fantastic performance superiority for neural network acceleration. However, as
the computational bit-width and scale increase, high-precision data conversion
and long-distance data routing will result in unacceptable energy and latency
overheads in the AiMC system. In this work, we focus on the potential of
in-charge computing and in-time interconnection and show an innovative AiMC
architecture, named AiDAC, with three key contributions: (1) AiDAC enhances
multibit computing efficiency and reduces data conversion times by grouping
capacitors technology; (2) AiDAC first adopts row drivers and column time
accumulators to achieve large-scale AiMC arrays integration while minimizing
the energy cost of data movements. (3) AiDAC is the first work to support
large-scale all-analog multibit vector-matrix multiplication (VMM) operations.
The evaluation shows that AiDAC maintains high-precision calculation (less than
0.79% total computing error) while also possessing excellent performance
features, such as high parallelism (up to 26.2TOPS), low latency (&lt;20ns/VMM),
and high energy efficiency (123.8TOPS/W), for 8bits VMM with 1024 input
channels.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12450">Can It Edit? Evaluating the Ability of Large Language Models to Follow Code Editing Instructions. (arXiv:2312.12450v2 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cassano_F/0/1/0/all/0/1">Federico Cassano</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Luisa Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sethi_A/0/1/0/all/0/1">Akul Sethi</a>, <a href="http://arxiv.org/find/cs/1/au:+Shinn_N/0/1/0/all/0/1">Noah Shinn</a>, <a href="http://arxiv.org/find/cs/1/au:+Brennan_Jones_A/0/1/0/all/0/1">Abby Brennan-Jones</a>, <a href="http://arxiv.org/find/cs/1/au:+Lozhkov_A/0/1/0/all/0/1">Anton Lozhkov</a>, <a href="http://arxiv.org/find/cs/1/au:+Anderson_C/0/1/0/all/0/1">Carolyn Jane Anderson</a>, <a href="http://arxiv.org/find/cs/1/au:+Guha_A/0/1/0/all/0/1">Arjun Guha</a></p>
<p>A significant amount of research is focused on developing and evaluating
large language models for a variety of code synthesis tasks. These include
synthesizing code from natural language instructions, synthesizing tests from
code, and synthesizing explanations of code. In contrast, the behavior of
instructional code editing with LLMs is understudied. These are tasks in which
the model is instructed to update a block of code provided in a prompt. The
editing instruction may ask for a feature to added or removed, describe a bug
and ask for a fix, ask for a different kind of solution, or many other common
code editing tasks.
</p>
<p>We introduce a carefully crafted benchmark of code editing tasks and use it
evaluate several cutting edge LLMs. Our evaluation exposes a significant gap
between the capabilities of state-of-the-art open and closed models. For
example, even GPT-3.5-Turbo is 8.8% better than the best open model at editing
code.
</p>
<p>We also introduce a new, carefully curated, permissively licensed training
set of code edits coupled with natural language instructions. Using this
training set, we show that we can fine-tune open Code LLMs to significantly
improve their code editing capabilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12464">Towards Better Serialization of Tabular Data for Few-shot Classification with Large Language Models. (arXiv:2312.12464v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jaitly_S/0/1/0/all/0/1">Sukriti Jaitly</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_T/0/1/0/all/0/1">Tanay Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Shugani_A/0/1/0/all/0/1">Ashish Shugani</a>, <a href="http://arxiv.org/find/cs/1/au:+Grewal_R/0/1/0/all/0/1">Razik Singh Grewal</a></p>
<p>We present a study on the integration of Large Language Models (LLMs) in
tabular data classification, emphasizing an efficient framework. Building upon
existing work done in TabLLM (<a href="/abs/2210.10723">arXiv:2210.10723</a>), we introduce three novel
serialization techniques, including the standout LaTeX serialization method.
This method significantly boosts the performance of LLMs in processing
domain-specific datasets, Our method stands out for its memory efficiency and
ability to fully utilize complex data structures. Through extensive
experimentation, including various serialization approaches like feature
combination and importance, we demonstrate our work's superiority in accuracy
and efficiency over traditional models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12473">A Study on Social Robot Behavior in Group Conversation. (arXiv:2312.12473v2 [cs.HC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Tung Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nichols_E/0/1/0/all/0/1">Eric Nichols</a>, <a href="http://arxiv.org/find/cs/1/au:+Gomez_R/0/1/0/all/0/1">Randy Gomez</a></p>
<p>Recently, research in human-robot interaction began to consider a robot's
influence at the group level. Despite the recent growth in research
investigating the effects of robots within groups of people, our overall
understanding of what happens when robots are placed within groups or teams of
people is still limited. This paper investigates several key problems for
social robots that manage conversations in a group setting, where the number of
participants is more than two. In a group setting, the conversation dynamics
are a lot more complicated than the conventional one-to-one conversation, thus,
there are more challenges need to be solved.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12655">Can Transformers Learn Sequential Function Classes In Context?. (arXiv:2312.12655v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Campbell_R/0/1/0/all/0/1">Ryan Campbell</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_E/0/1/0/all/0/1">Emma Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_E/0/1/0/all/0/1">Evan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Vir_R/0/1/0/all/0/1">Reya Vir</a>, <a href="http://arxiv.org/find/cs/1/au:+Hsiao_E/0/1/0/all/0/1">Ethan Hsiao</a></p>
<p>In-context learning (ICL) has revolutionized the capabilities of transformer
models in NLP. In our project, we extend the understanding of the mechanisms
underpinning ICL by exploring whether transformers can learn from sequential,
non-textual function class data distributions. We introduce a novel sliding
window sequential function class and employ toy-sized transformers with a GPT-2
architecture to conduct our experiments. Our analysis indicates that these
models can indeed leverage ICL when trained on non-textual sequential function
classes. Additionally, our experiments with randomized y-label sequences
highlights that transformers retain some ICL capabilities even when the label
associations are obfuscated. We provide evidence that transformers can reason
with and understand sequentiality encoded within function classes, as reflected
by the effective learning of our proposed tasks. Our results also show that the
performance deteriorated with increasing randomness in the labels, though not
to the extent one might expect, implying a potential robustness of learned
sequentiality against label noise. Future research may want to look into how
previous explanations of transformers, such as induction heads and task
vectors, relate to sequentiality in ICL in these toy examples. Our
investigation lays the groundwork for further research into how transformers
process and perceive sequential data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13032">NodeMixup: Tackling Under-Reaching for Graph Neural Networks. (arXiv:2312.13032v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1">Weigang Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_Z/0/1/0/all/0/1">Ziyu Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Wei Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yaming Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1">Long Jin</a></p>
<p>Graph Neural Networks (GNNs) have become mainstream methods for solving the
semi-supervised node classification problem. However, due to the uneven
location distribution of labeled nodes in the graph, labeled nodes are only
accessible to a small portion of unlabeled nodes, leading to the
\emph{under-reaching} issue. In this study, we firstly reveal under-reaching by
conducting an empirical investigation on various well-known graphs. Then, we
demonstrate that under-reaching results in unsatisfactory distribution
alignment between labeled and unlabeled nodes through systematic experimental
analysis, significantly degrading GNNs' performance. To tackle under-reaching
for GNNs, we propose an architecture-agnostic method dubbed NodeMixup. The
fundamental idea is to (1) increase the reachability of labeled nodes by
labeled-unlabeled pairs mixup, (2) leverage graph structures via fusing the
neighbor connections of intra-class node pairs to improve performance gains of
mixup, and (3) use neighbor label distribution similarity incorporating node
degrees to determine sampling weights for node mixup. Extensive experiments
demonstrate the efficacy of NodeMixup in assisting GNNs in handling
under-reaching. The source code is available at
\url{https://github.com/WeigangLu/NodeMixup}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19459">Security Challenges for Cloud or Fog Computing-Based AI Applications. (arXiv:2310.19459v3 [cs.CR] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pakmehr_A/0/1/0/all/0/1">Amir Pakmehr</a>, <a href="http://arxiv.org/find/cs/1/au:+Assmuth_A/0/1/0/all/0/1">Andreas A&#xdf;muth</a>, <a href="http://arxiv.org/find/cs/1/au:+Neumann_C/0/1/0/all/0/1">Christoph P. Neumann</a>, <a href="http://arxiv.org/find/cs/1/au:+Pirkl_G/0/1/0/all/0/1">Gerald Pirkl</a></p>
<p>Security challenges for Cloud or Fog-based machine learning services pose
several concerns. Securing the underlying Cloud or Fog services is essential,
as successful attacks against these services, on which machine learning
applications rely, can lead to significant impairments of these applications.
Because the requirements for AI applications can also be different, we
differentiate according to whether they are used in the Cloud or in a Fog
Computing network. This then also results in different threats or attack
possibilities. For Cloud platforms, the responsibility for security can be
divided between different parties. Security deficiencies at a lower level can
have a direct impact on the higher level where user data is stored. While
responsibilities are simpler for Fog Computing networks, by moving services to
the edge of the network, we have to secure them against physical access to the
devices. We conclude by outlining specific information security requirements
for AI applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10868">From Google Gemini to OpenAI Q* (Q-Star): A Survey of Reshaping the Generative Artificial Intelligence (AI) Research Landscape. (arXiv:2312.10868v1 [cs.AI] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+McIntosh_T/0/1/0/all/0/1">Timothy R. McIntosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Susnjak_T/0/1/0/all/0/1">Teo Susnjak</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Watters_P/0/1/0/all/0/1">Paul Watters</a>, <a href="http://arxiv.org/find/cs/1/au:+Halgamuge_M/0/1/0/all/0/1">Malka N. Halgamuge</a></p>
<p>This comprehensive survey explored the evolving landscape of generative
Artificial Intelligence (AI), with a specific focus on the transformative
impacts of Mixture of Experts (MoE), multimodal learning, and the speculated
advancements towards Artificial General Intelligence (AGI). It critically
examined the current state and future trajectory of generative Artificial
Intelligence (AI), exploring how innovations like Google's Gemini and the
anticipated OpenAI Q* project are reshaping research priorities and
applications across various domains, including an impact analysis on the
generative AI research taxonomy. It assessed the computational challenges,
scalability, and real-world implications of these technologies while
highlighting their potential in driving significant progress in fields like
healthcare, finance, and education. It also addressed the emerging academic
challenges posed by the proliferation of both AI-themed and AI-generated
preprints, examining their impact on the peer-review process and scholarly
communication. The study highlighted the importance of incorporating ethical
and human-centric methods in AI development, ensuring alignment with societal
norms and welfare, and outlined a strategy for future AI research that focuses
on a balanced and conscientious use of MoE, multimodality, and AGI in
generative AI.
</p>
</p>
</div>

    </div>
    </body>
    