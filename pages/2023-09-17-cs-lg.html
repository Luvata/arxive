<!DOCTYPE html>
<html>
<head>
<title>2023-09-17-cs-lg</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2309.07133">Using wearable device-based machine learning models to autonomously identify older adults with poor cognition. (arXiv:2309.07133v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Sakal_C/0/1/0/all/0/1">Collin Sakal</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_T/0/1/0/all/0/1">Tingyou Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1">Juan Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1">Xinyue Li</a></p>
<p>Conducting cognitive tests is time-consuming for patients and clinicians.
Wearable device-based prediction models allow for continuous health monitoring
under normal living conditions and could offer an alternative to identifying
older adults with cognitive impairments for early interventions. In this study,
we first derived novel wearable-based features related to circadian rhythms,
ambient light exposure, physical activity levels, sleep, and signal processing.
Then, we quantified the ability of wearable-based machine-learning models to
predict poor cognition based on outcomes from the Digit Symbol Substitution
Test (DSST), the Consortium to Establish a Registry for Alzheimers Disease
Word-Learning subtest (CERAD-WL), and the Animal Fluency Test (AFT). We found
that the wearable-based models had significantly higher AUCs when predicting
all three cognitive outcomes compared to benchmark models containing age, sex,
education, marital status, household income, diabetic status, depression
symptoms, and functional independence scores. In addition to uncovering
previously unidentified wearable-based features that are predictive of poor
cognition such as the standard deviation of the midpoints of each persons most
active 10-hour periods and least active 5-hour periods, our paper provides
proof-of-concept that wearable-based machine learning models can be used to
autonomously screen older adults for possible cognitive impairments. Such
models offer cost-effective alternatives to conducting initial screenings
manually in clinical settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07134">Entropy-based machine learning model for diagnosis and monitoring of Parkinson&#x27;s Disease in smart IoT environment. (arXiv:2309.07134v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Belyaev_M/0/1/0/all/0/1">Maksim Belyaev</a>, <a href="http://arxiv.org/find/eess/1/au:+Murugappan_M/0/1/0/all/0/1">Murugappan Murugappan</a>, <a href="http://arxiv.org/find/eess/1/au:+Velichko_A/0/1/0/all/0/1">Andrei Velichko</a>, <a href="http://arxiv.org/find/eess/1/au:+Korzun_D/0/1/0/all/0/1">Dmitry Korzun</a></p>
<p>The study presents the concept of a computationally efficient machine
learning (ML) model for diagnosing and monitoring Parkinson's disease (PD) in
an Internet of Things (IoT) environment using rest-state EEG signals (rs-EEG).
We computed different types of entropy from EEG signals and found that Fuzzy
Entropy performed the best in diagnosing and monitoring PD using rs-EEG. We
also investigated different combinations of signal frequency ranges and EEG
channels to accurately diagnose PD. Finally, with a fewer number of features
(11 features), we achieved a maximum classification accuracy (ARKF) of ~99.9%.
The most prominent frequency range of EEG signals has been identified, and we
have found that high classification accuracy depends on low-frequency signal
components (0-4 Hz). Moreover, the most informative signals were mainly
received from the right hemisphere of the head (F8, P8, T8, FC6). Furthermore,
we assessed the accuracy of the diagnosis of PD using three different lengths
of EEG data (150-1000 samples). Because the computational complexity is reduced
by reducing the input data. As a result, we have achieved a maximum mean
accuracy of 99.9% for a sample length (LEEG) of 1000 (~7.8 seconds), 98.2% with
a LEEG of 800 (~6.2 seconds), and 79.3% for LEEG = 150 (~1.2 seconds). By
reducing the number of features and segment lengths, the computational cost of
classification can be reduced. Lower-performance smart ML sensors can be used
in IoT environments for enhances human resilience to PD.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07135">EpiDeNet: An Energy-Efficient Approach to Seizure Detection for Embedded Systems. (arXiv:2309.07135v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ingolfsson_T/0/1/0/all/0/1">Thorir Mar Ingolfsson</a>, <a href="http://arxiv.org/find/eess/1/au:+Chakraborty_U/0/1/0/all/0/1">Upasana Chakraborty</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1">Xiaying Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Beniczky_S/0/1/0/all/0/1">Sandor Beniczky</a>, <a href="http://arxiv.org/find/eess/1/au:+Ducouret_P/0/1/0/all/0/1">Pauline Ducouret</a>, <a href="http://arxiv.org/find/eess/1/au:+Benatti_S/0/1/0/all/0/1">Simone Benatti</a>, <a href="http://arxiv.org/find/eess/1/au:+Ryvlin_P/0/1/0/all/0/1">Philippe Ryvlin</a>, <a href="http://arxiv.org/find/eess/1/au:+Cossettini_A/0/1/0/all/0/1">Andrea Cossettini</a>, <a href="http://arxiv.org/find/eess/1/au:+Benini_L/0/1/0/all/0/1">Luca Benini</a></p>
<p>Epilepsy is a prevalent neurological disorder that affects millions of
individuals globally, and continuous monitoring coupled with automated seizure
detection appears as a necessity for effective patient treatment. To enable
long-term care in daily-life conditions, comfortable and smart wearable devices
with long battery life are required, which in turn set the demand for
resource-constrained and energy-efficient computing solutions. In this context,
the development of machine learning algorithms for seizure detection faces the
challenge of heavily imbalanced datasets. This paper introduces EpiDeNet, a new
lightweight seizure detection network, and Sensitivity-Specificity Weighted
Cross-Entropy (SSWCE), a new loss function that incorporates sensitivity and
specificity, to address the challenge of heavily unbalanced datasets. The
proposed EpiDeNet-SSWCE approach demonstrates the successful detection of
91.16% and 92.00% seizure events on two different datasets (CHB-MIT and
PEDESITE, respectively), with only four EEG channels. A three-window majority
voting-based smoothing scheme combined with the SSWCE loss achieves 3x
reduction of false positives to 1.18 FP/h. EpiDeNet is well suited for
implementation on low-power embedded platforms, and we evaluate its performance
on two ARM Cortex-based platforms (M4F/M7) and two parallel ultra-low power
(PULP) systems (GAP8, GAP9). The most efficient implementation (GAP9) achieves
an energy efficiency of 40 GMAC/s/W, with an energy consumption per inference
of only 0.051 mJ at high performance (726.46 MMAC/s), outperforming the best
ARM Cortex-based solutions by approximately 160x in energy efficiency. The
EpiDeNet-SSWCE method demonstrates effective and accurate seizure detection
performance on heavily imbalanced datasets, while being suited for
implementation on energy-constrained platforms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07136">Masked Transformer for Electrocardiogram Classification. (arXiv:2309.07136v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1">Ya Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Diao_X/0/1/0/all/0/1">Xiaolin Diao</a>, <a href="http://arxiv.org/find/eess/1/au:+Huo_Y/0/1/0/all/0/1">Yanni Huo</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Fan_X/0/1/0/all/0/1">Xiaohan Fan</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhao_W/0/1/0/all/0/1">Wei Zhao</a></p>
<p>Electrocardiogram (ECG) is one of the most important diagnostic tools in
clinical applications. With the advent of advanced algorithms, various deep
learning models have been adopted for ECG tasks. However, the potential of
Transformers for ECG data is not yet realized, despite their widespread success
in computer vision and natural language processing. In this work, we present a
useful masked Transformer method for ECG classification referred to as MTECG,
which expands the application of masked autoencoders to ECG time series. We
construct a dataset comprising 220,251 ECG recordings with a broad range of
diagnoses annoated by medical experts to explore the properties of MTECG. Under
the proposed training strategies, a lightweight model with 5.7M parameters
performs stably well on a broad range of masking ratios (5%-75%). The ablation
studies highlight the importance of fluctuated reconstruction targets, training
schedule length, layer-wise LR decay and DropPath rate. The experiments on both
private and public ECG datasets demonstrate that MTECG-T significantly
outperforms the recent state-of-the-art algorithms in ECG classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07137">Bringing PDEs to JAX with forward and reverse modes automatic differentiation. (arXiv:2309.07137v1 [cs.MS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yashchuk_I/0/1/0/all/0/1">Ivan Yashchuk</a></p>
<p>Partial differential equations (PDEs) are used to describe a variety of
physical phenomena. Often these equations do not have analytical solutions and
numerical approximations are used instead. One of the common methods to solve
PDEs is the finite element method. Computing derivative information of the
solution with respect to the input parameters is important in many tasks in
scientific computing. We extend JAX automatic differentiation library with an
interface to Firedrake finite element library. High-level symbolic
representation of PDEs allows bypassing differentiating through low-level
possibly many iterations of the underlying nonlinear solvers. Differentiating
through Firedrake solvers is done using tangent-linear and adjoint equations.
This enables the efficient composition of finite element solvers with arbitrary
differentiable programs. The code is available at
github.com/IvanYashchuk/jax-firedrake.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07138">Self-Supervised Blind Source Separation via Multi-Encoder Autoencoders. (arXiv:2309.07138v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Webster_M/0/1/0/all/0/1">Matthew B. Webster</a>, <a href="http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1">Joonnyong Lee</a></p>
<p>The task of blind source separation (BSS) involves separating sources from a
mixture without prior knowledge of the sources or the mixing system. This is a
challenging problem that often requires making restrictive assumptions about
both the mixing system and the sources. In this paper, we propose a novel
method for addressing BSS of non-linear mixtures by leveraging the natural
feature subspace specialization ability of multi-encoder autoencoders with
fully self-supervised learning without strong priors. During the training
phase, our method unmixes the input into the separate encoding spaces of the
multi-encoder network and then remixes these representations within the decoder
for a reconstruction of the input. Then to perform source inference, we
introduce a novel encoding masking technique whereby masking out all but one of
the encodings enables the decoder to estimate a source signal. To this end, we
also introduce a so-called pathway separation loss that encourages sparsity
between the unmixed encoding spaces throughout the decoder's layers and a
so-called zero reconstruction loss on the decoder for coherent source
estimations. In order to carefully evaluate our method, we conduct experiments
on a toy dataset and with real-world biosignal recordings from a
polysomnography sleep study for extracting respiration.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07140">Short-term power load forecasting method based on CNN-SAEDN-Res. (arXiv:2309.07140v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Cui_Y/0/1/0/all/0/1">Yang Cui</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhu_H/0/1/0/all/0/1">Han Zhu</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1">Yijian Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1">Lu Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1">Yang Li</a></p>
<p>In deep learning, the load data with non-temporal factors are difficult to
process by sequence models. This problem results in insufficient precision of
the prediction. Therefore, a short-term load forecasting method based on
convolutional neural network (CNN), self-attention encoder-decoder network
(SAEDN) and residual-refinement (Res) is proposed. In this method, feature
extraction module is composed of a two-dimensional convolutional neural
network, which is used to mine the local correlation between data and obtain
high-dimensional data features. The initial load fore-casting module consists
of a self-attention encoder-decoder network and a feedforward neural network
(FFN). The module utilizes self-attention mechanisms to encode high-dimensional
features. This operation can obtain the global correlation between data.
Therefore, the model is able to retain important information based on the
coupling relationship between the data in data mixed with non-time series
factors. Then, self-attention decoding is per-formed and the feedforward neural
network is used to regression initial load. This paper introduces the residual
mechanism to build the load optimization module. The module generates residual
load values to optimize the initial load. The simulation results show that the
proposed load forecasting method has advantages in terms of prediction accuracy
and prediction stability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07141">Design of Recognition and Evaluation System for Table Tennis Players&#x27; Motor Skills Based on Artificial Intelligence. (arXiv:2309.07141v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Shi_Z/0/1/0/all/0/1">Zhuo-yong Shi</a>, <a href="http://arxiv.org/find/eess/1/au:+Jia_Y/0/1/0/all/0/1">Ye-tao Jia</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_K/0/1/0/all/0/1">Ke-xin Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1">Ding-han Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Ji_L/0/1/0/all/0/1">Long-meng Ji</a>, <a href="http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1">Yong Wu</a></p>
<p>With the rapid development of electronic science and technology, the research
on wearable devices is constantly updated, but for now, it is not comprehensive
for wearable devices to recognize and analyze the movement of specific sports.
Based on this, this paper improves wearable devices of table tennis sport, and
realizes the pattern recognition and evaluation of table tennis players' motor
skills through artificial intelligence. Firstly, a device is designed to
collect the movement information of table tennis players and the actual
movement data is processed. Secondly, a sliding window is made to divide the
collected motion data into a characteristic database of six table tennis
benchmark movements. Thirdly, motion features were constructed based on feature
engineering, and motor skills were identified for different models after
dimensionality reduction. Finally, the hierarchical evaluation system of motor
skills is established with the loss functions of different evaluation indexes.
The results show that in the recognition of table tennis players' motor skills,
the feature-based BP neural network proposed in this paper has higher
recognition accuracy and stronger generalization ability than the traditional
convolutional neural network.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07145">ETP: Learning Transferable ECG Representations via ECG-Text Pre-training. (arXiv:2309.07145v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1">Che Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Wan_Z/0/1/0/all/0/1">Zhongwei Wan</a>, <a href="http://arxiv.org/find/eess/1/au:+Cheng_S/0/1/0/all/0/1">Sibo Cheng</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_M/0/1/0/all/0/1">Mi Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Arcucci_R/0/1/0/all/0/1">Rossella Arcucci</a></p>
<p>In the domain of cardiovascular healthcare, the Electrocardiogram (ECG)
serves as a critical, non-invasive diagnostic tool. Although recent strides in
self-supervised learning (SSL) have been promising for ECG representation
learning, these techniques often require annotated samples and struggle with
classes not present in the fine-tuning stages. To address these limitations, we
introduce ECG-Text Pre-training (ETP), an innovative framework designed to
learn cross-modal representations that link ECG signals with textual reports.
For the first time, this framework leverages the zero-shot classification task
in the ECG domain. ETP employs an ECG encoder along with a pre-trained language
model to align ECG signals with their corresponding textual reports. The
proposed framework excels in both linear evaluation and zero-shot
classification tasks, as demonstrated on the PTB-XL and CPSC2018 datasets,
showcasing its ability for robust and generalizable cross-modal ECG feature
learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07147">DGSD: Dynamical Graph Self-Distillation for EEG-Based Auditory Spatial Attention Detection. (arXiv:2309.07147v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Fan_C/0/1/0/all/0/1">Cunhang Fan</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1">Hongyu Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Huang_W/0/1/0/all/0/1">Wei Huang</a>, <a href="http://arxiv.org/find/eess/1/au:+Xue_J/0/1/0/all/0/1">Jun Xue</a>, <a href="http://arxiv.org/find/eess/1/au:+Tao_J/0/1/0/all/0/1">Jianhua Tao</a>, <a href="http://arxiv.org/find/eess/1/au:+Yi_J/0/1/0/all/0/1">Jiangyan Yi</a>, <a href="http://arxiv.org/find/eess/1/au:+Lv_Z/0/1/0/all/0/1">Zhao Lv</a>, <a href="http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1">Xiaopei Wu</a></p>
<p>Auditory Attention Detection (AAD) aims to detect target speaker from brain
signals in a multi-speaker environment. Although EEG-based AAD methods have
shown promising results in recent years, current approaches primarily rely on
traditional convolutional neural network designed for processing Euclidean data
like images. This makes it challenging to handle EEG signals, which possess
non-Euclidean characteristics. In order to address this problem, this paper
proposes a dynamical graph self-distillation (DGSD) approach for AAD, which
does not require speech stimuli as input. Specifically, to effectively
represent the non-Euclidean properties of EEG signals, dynamical graph
convolutional networks are applied to represent the graph structure of EEG
signals, which can also extract crucial features related to auditory spatial
attention in EEG signals. In addition, to further improve AAD detection
performance, self-distillation, consisting of feature distillation and
hierarchical distillation strategies at each layer, is integrated. These
strategies leverage features and classification results from the deepest
network layers to guide the learning of shallow layers. Our experiments are
conducted on two publicly available datasets, KUL and DTU. Under a 1-second
time window, we achieve results of 90.0\% and 79.6\% accuracy on KUL and DTU,
respectively. We compare our DGSD method with competitive baselines, and the
experimental results indicate that the detection performance of our proposed
DGSD method is not only superior to the best reproducible baseline but also
significantly reduces the number of trainable parameters by approximately 100
times.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07149">Decoding visual brain representations from electroencephalography through Knowledge Distillation and latent diffusion models. (arXiv:2309.07149v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ferrante_M/0/1/0/all/0/1">Matteo Ferrante</a>, <a href="http://arxiv.org/find/eess/1/au:+Boccato_T/0/1/0/all/0/1">Tommaso Boccato</a>, <a href="http://arxiv.org/find/eess/1/au:+Bargione_S/0/1/0/all/0/1">Stefano Bargione</a>, <a href="http://arxiv.org/find/eess/1/au:+Toschi_N/0/1/0/all/0/1">Nicola Toschi</a></p>
<p>Decoding visual representations from human brain activity has emerged as a
thriving research domain, particularly in the context of brain-computer
interfaces. Our study presents an innovative method that employs to classify
and reconstruct images from the ImageNet dataset using electroencephalography
(EEG) data from subjects that had viewed the images themselves (i.e. "brain
decoding"). We analyzed EEG recordings from 6 participants, each exposed to 50
images spanning 40 unique semantic categories. These EEG readings were
converted into spectrograms, which were then used to train a convolutional
neural network (CNN), integrated with a knowledge distillation procedure based
on a pre-trained Contrastive Language-Image Pre-Training (CLIP)-based image
classification teacher network. This strategy allowed our model to attain a
top-5 accuracy of 80%, significantly outperforming a standard CNN and various
RNN-based benchmarks. Additionally, we incorporated an image reconstruction
mechanism based on pre-trained latent diffusion models, which allowed us to
generate an estimate of the images which had elicited EEG activity. Therefore,
our architecture not only decodes images from neural activity but also offers a
credible image reconstruction from EEG only, paving the way for e.g. swift,
individualized feedback experiments. Our research represents a significant step
forward in connecting neural signals with visual cognition.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07153">Finding Influencers in Complex Networks: An Effective Deep Reinforcement Learning Approach. (arXiv:2309.07153v1 [cs.SI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Changan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1">Changjun Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhongzhi Zhang</a></p>
<p>Maximizing influences in complex networks is a practically important but
computationally challenging task for social network analysis, due to its NP-
hard nature. Most current approximation or heuristic methods either require
tremendous human design efforts or achieve unsatisfying balances between
effectiveness and efficiency. Recent machine learning attempts only focus on
speed but lack performance enhancement. In this paper, different from previous
attempts, we propose an effective deep reinforcement learning model that
achieves superior performances over traditional best influence maximization
algorithms. Specifically, we design an end-to-end learning framework that
combines graph neural network as the encoder and reinforcement learning as the
decoder, named DREIM. Trough extensive training on small synthetic graphs,
DREIM outperforms the state-of-the-art baseline methods on very large synthetic
and real-world networks on solution quality, and we also empirically show its
linear scalability with regard to the network size, which demonstrates its
superiority in solving this problem.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07156">A Deep Dive into Sleep: Single-Channel EEG-Based Sleep Stage Classification with Model Interpretability. (arXiv:2309.07156v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Sharma_S/0/1/0/all/0/1">Shivam Sharma</a>, <a href="http://arxiv.org/find/eess/1/au:+Maiti_S/0/1/0/all/0/1">Suvadeep Maiti</a>, <a href="http://arxiv.org/find/eess/1/au:+Mythirayee_S/0/1/0/all/0/1">S.Mythirayee</a>, <a href="http://arxiv.org/find/eess/1/au:+Rajendran_S/0/1/0/all/0/1">Srijithesh Rajendran</a>, <a href="http://arxiv.org/find/eess/1/au:+Raju_B/0/1/0/all/0/1">Bapi Raju</a></p>
<p>Sleep, a fundamental physiological process, occupies a significant portion of
our lives. Accurate classification of sleep stages serves as a crucial tool for
evaluating sleep quality and identifying probable sleep disorders. This work
introduces a novel methodology that utilises a SE-Resnet-Bi-LSTM architecture
to classify sleep into five separate stages. The classification process is
based on the analysis of single-channel electroencephalograms (EEGs). The
framework that has been suggested consists of two fundamental elements: a
feature extractor that utilises SE-ResNet, and a temporal context encoder that
use stacks of Bi-LSTM units.The effectiveness of our approach is substantiated
by thorough assessments conducted on three different datasets, namely
SLeepEDF-20, SleepEDF-78, and SHHS. Significantly, our methodology attains
notable levels of accuracy, specifically 87.5\%, 83.9\%, and 87.8\%, along with
macro-F1 scores of 82.5, 78.9, and 81.9 for the corresponding datasets.
Notably, we introduce the utilization of 1D-GradCAM visualization to shed light
on the decision-making process of our model in the realm of sleep stage
classification. This visualization method not only provides valuable insights
into the model's classification rationale but also aligns its outcomes with the
annotations made by sleep experts. One notable feature of our research is the
integration of an expedited training approach, which effectively preserves the
model's resilience in terms of performance. The experimental evaluations
conducted provide a comprehensive evaluation of the effectiveness of our
proposed model in comparison to existing approaches, highlighting its potential
for practical applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07157">Distribution Grid Line Outage Identification with Unknown Pattern and Performance Guarantee. (arXiv:2309.07157v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Chenhan Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1">Yizheng Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1">Yang Weng</a></p>
<p>Line outage identification in distribution grids is essential for sustainable
grid operation. In this work, we propose a practical yet robust detection
approach that utilizes only readily available voltage magnitudes, eliminating
the need for costly phase angles or power flow data. Given the sensor data,
many existing detection methods based on change-point detection require prior
knowledge of outage patterns, which are unknown for real-world outage
scenarios. To remove this impractical requirement, we propose a data-driven
method to learn the parameters of the post-outage distribution through gradient
descent. However, directly using gradient descent presents feasibility issues.
To address this, we modify our approach by adding a Bregman divergence
constraint to control the trajectory of the parameter updates, which eliminates
the feasibility problems. As timely operation is the key nowadays, we prove
that the optimal parameters can be learned with convergence guarantees via
leveraging the statistical and physical properties of voltage data. We evaluate
our approach using many representative distribution grids and real load
profiles with 17 outage configurations. The results show that we can detect and
localize the outage in a timely manner with only voltage magnitudes and without
assuming a prior knowledge of outage patterns.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07158">Compressed Real Numbers for AI: a case-study using a RISC-V CPU. (arXiv:2309.07158v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rossi_F/0/1/0/all/0/1">Federico Rossi</a>, <a href="http://arxiv.org/find/cs/1/au:+Cococcioni_M/0/1/0/all/0/1">Marco Cococcioni</a>, <a href="http://arxiv.org/find/cs/1/au:+Ibanez_R/0/1/0/all/0/1">Roger Ferrer Ib&#xe0;&#xf1;ez</a>, <a href="http://arxiv.org/find/cs/1/au:+Labarta_J/0/1/0/all/0/1">Jes&#xf9;s Labarta</a>, <a href="http://arxiv.org/find/cs/1/au:+Mantovani_F/0/1/0/all/0/1">Filippo Mantovani</a>, <a href="http://arxiv.org/find/cs/1/au:+Casas_M/0/1/0/all/0/1">Marc Casas</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruffaldi_E/0/1/0/all/0/1">Emanuele Ruffaldi</a>, <a href="http://arxiv.org/find/cs/1/au:+Saponara_S/0/1/0/all/0/1">Sergio Saponara</a></p>
<p>As recently demonstrated, Deep Neural Networks (DNN), usually trained using
single precision IEEE 754 floating point numbers (binary32), can also work
using lower precision. Therefore, 16-bit and 8-bit compressed format have
attracted considerable attention. In this paper, we focused on two families of
formats that have already achieved interesting results in compressing binary32
numbers in machine learning applications, without sensible degradation of the
accuracy: bfloat and posit. Even if 16-bit and 8-bit bfloat/posit are routinely
used for reducing the storage of the weights/biases of trained DNNs, the
inference still often happens on the 32-bit FPU of the CPU (especially if GPUs
are not available). In this paper we propose a way to decompress a tensor of
bfloat/posits just before computations, i.e., after the compressed operands
have been loaded within the vector registers of a vector capable CPU, in order
to save bandwidth usage and increase cache efficiency. Finally, we show the
architectural parameters and considerations under which this solution is
advantageous with respect to the uncompressed one.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07159">A Strong and Simple Deep Learning Baseline for BCI MI Decoding. (arXiv:2309.07159v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ouahidi_Y/0/1/0/all/0/1">Yassine El Ouahidi</a>, <a href="http://arxiv.org/find/eess/1/au:+Gripon_V/0/1/0/all/0/1">Vincent Gripon</a>, <a href="http://arxiv.org/find/eess/1/au:+Pasdeloup_B/0/1/0/all/0/1">Bastien Pasdeloup</a>, <a href="http://arxiv.org/find/eess/1/au:+Bouallegue_G/0/1/0/all/0/1">Ghaith Bouallegue</a>, <a href="http://arxiv.org/find/eess/1/au:+Farrugia_N/0/1/0/all/0/1">Nicolas Farrugia</a>, <a href="http://arxiv.org/find/eess/1/au:+Lioi_G/0/1/0/all/0/1">Giulia Lioi</a></p>
<p>We propose EEG-SimpleConv, a straightforward 1D convolutional neural network
for Motor Imagery decoding in BCI. Our main motivation is to propose a very
simple baseline to compare to, using only very standard ingredients from the
literature. We evaluate its performance on four EEG Motor Imagery datasets,
including simulated online setups, and compare it to recent Deep Learning and
Machine Learning approaches. EEG-SimpleConv is at least as good or far more
efficient than other approaches, showing strong knowledge-transfer capabilities
across subjects, at the cost of a low inference time. We advocate that using
off-the-shelf ingredients rather than coming with ad-hoc solutions can
significantly help the adoption of Deep Learning approaches for BCI. We make
the code of the models and the experiments accessible.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07163">Systematic Review of Experimental Paradigms and Deep Neural Networks for Electroencephalography-Based Cognitive Workload Detection. (arXiv:2309.07163v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+KN_V/0/1/0/all/0/1">Vishnu KN</a>, <a href="http://arxiv.org/find/eess/1/au:+Gupta_C/0/1/0/all/0/1">Cota Navin Gupta</a></p>
<p>This article summarizes a systematic review of the electroencephalography
(EEG)-based cognitive workload (CWL) estimation. The focus of the article is
twofold: identify the disparate experimental paradigms used for reliably
eliciting discreet and quantifiable levels of cognitive load and the specific
nature and representational structure of the commonly used input formulations
in deep neural networks (DNNs) used for signal classification. The analysis
revealed a number of studies using EEG signals in its native representation of
a two-dimensional matrix for offline classification of CWL. However, only a few
studies adopted an online or pseudo-online classification strategy for
real-time CWL estimation. Further, only a couple of interpretable DNNs and a
single generative model were employed for cognitive load detection till date
during this review. More often than not, researchers were using DNNs as
black-box type models. In conclusion, DNNs prove to be valuable tools for
classifying EEG signals, primarily due to the substantial modeling power
provided by the depth of their network architecture. It is further suggested
that interpretable and explainable DNN models must be employed for cognitive
workload estimation since existing methods are limited in the face of the
non-stationary nature of the signal.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07168">Goal Space Abstraction in Hierarchical Reinforcement Learning via Reachability Analysis. (arXiv:2309.07168v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zadem_M/0/1/0/all/0/1">Mehdi Zadem</a> (LIX, U2IS), <a href="http://arxiv.org/find/cs/1/au:+Mover_S/0/1/0/all/0/1">Sergio Mover</a> (LIX), <a href="http://arxiv.org/find/cs/1/au:+Nguyen_S/0/1/0/all/0/1">Sao Mai Nguyen</a> (U2IS, Flowers, IMT Atlantique - INFO, Lab-STICC_RAMBO)</p>
<p>Open-ended learning benefits immensely from the use of symbolic methods for
goal representation as they offer ways to structure knowledge for efficient and
transferable learning. However, the existing Hierarchical Reinforcement
Learning (HRL) approaches relying on symbolic reasoning are often limited as
they require a manual goal representation. The challenge in autonomously
discovering a symbolic goal representation is that it must preserve critical
information, such as the environment dynamics. In this work, we propose a
developmental mechanism for subgoal discovery via an emergent representation
that abstracts (i.e., groups together) sets of environment states that have
similar roles in the task. We create a HRL algorithm that gradually learns this
representation along with the policies and evaluate it on navigation tasks to
show the learned representation is interpretable and results in data
efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07169">Frequency Convergence of Complexon Shift Operators. (arXiv:2309.07169v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Zhang_P/0/1/0/all/0/1">Purui Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Jian_X/0/1/0/all/0/1">Xingchao Jian</a>, <a href="http://arxiv.org/find/eess/1/au:+Ji_F/0/1/0/all/0/1">Feng Ji</a>, <a href="http://arxiv.org/find/eess/1/au:+Tay_W/0/1/0/all/0/1">Wee Peng Tay</a>, <a href="http://arxiv.org/find/eess/1/au:+Wen_B/0/1/0/all/0/1">Bihan Wen</a></p>
<p>Topological signal processing (TSP) utilizes simplicial complexes to model
structures with higher order than vertices and edges. In this paper, we study
the transferability of TSP via a generalized higher-order version of graphon,
known as complexon. We recall the notion of a complexon as the limit of a
simplicial complex sequence [1]. Inspired by the integral operator form of
graphon shift operators, we construct a marginal complexon and complexon shift
operator (CSO) according to components of all possible dimensions from the
complexon. We investigate the CSO's eigenvalues and eigenvectors, and relate
them to a new family of weighted adjacency matrices. We prove that when a
simplicial complex sequence converges to a complexon, the eigenvalues of the
corresponding CSOs converge to that of the limit complexon. These results hint
at learning transferability on large simplicial complexes or simplicial complex
sequences, which generalize the graphon signal processing framework.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07170">Overview of Human Activity Recognition Using Sensor Data. (arXiv:2309.07170v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Hamad_R/0/1/0/all/0/1">Rebeen Ali Hamad</a>, <a href="http://arxiv.org/find/eess/1/au:+Woo_W/0/1/0/all/0/1">Wai Lok Woo</a>, <a href="http://arxiv.org/find/eess/1/au:+Wei_B/0/1/0/all/0/1">Bo Wei</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_L/0/1/0/all/0/1">Longzhi Yang</a></p>
<p>Human activity recognition (HAR) is an essential research field that has been
used in different applications including home and workplace automation,
security and surveillance as well as healthcare. Starting from conventional
machine learning methods to the recently developing deep learning techniques
and the Internet of things, significant contributions have been shown in the
HAR area in the last decade. Even though several review and survey studies have
been published, there is a lack of sensor-based HAR overview studies focusing
on summarising the usage of wearable sensors and smart home sensors data as
well as applications of HAR and deep learning techniques. Hence, we overview
sensor-based HAR, discuss several important applications that rely on HAR, and
highlight the most common machine learning methods that have been used for HAR.
Finally, several challenges of HAR are explored that should be addressed to
further improve the robustness of HAR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07172">Exploring Large Language Models for Ontology Alignment. (arXiv:2309.07172v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yuan He</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiaoyan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1">Hang Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Horrocks_I/0/1/0/all/0/1">Ian Horrocks</a></p>
<p>This work investigates the applicability of recent generative Large Language
Models (LLMs), such as the GPT series and Flan-T5, to ontology alignment for
identifying concept equivalence mappings across ontologies. To test the
zero-shot performance of Flan-T5-XXL and GPT-3.5-turbo, we leverage challenging
subsets from two equivalence matching datasets of the OAEI Bio-ML track, taking
into account concept labels and structural contexts. Preliminary findings
suggest that LLMs have the potential to outperform existing ontology alignment
systems like BERTMap, given careful framework and prompt design.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07173">Using Unsupervised and Supervised Learning and Digital Twin for Deep Convective Ice Storm Classification. (arXiv:2309.07173v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Swope_J/0/1/0/all/0/1">Jason Swope</a>, <a href="http://arxiv.org/find/cs/1/au:+Chien_S/0/1/0/all/0/1">Steve Chien</a>, <a href="http://arxiv.org/find/cs/1/au:+Dunkel_E/0/1/0/all/0/1">Emily Dunkel</a>, <a href="http://arxiv.org/find/cs/1/au:+Bosch_Lluis_X/0/1/0/all/0/1">Xavier Bosch-Lluis</a>, <a href="http://arxiv.org/find/cs/1/au:+Yue_Q/0/1/0/all/0/1">Qing Yue</a>, <a href="http://arxiv.org/find/cs/1/au:+Deal_W/0/1/0/all/0/1">William Deal</a></p>
<p>Smart Ice Cloud Sensing (SMICES) is a small-sat concept in which a primary
radar intelligently targets ice storms based on information collected by a
lookahead radiometer. Critical to the intelligent targeting is accurate
identification of storm/cloud types from eight bands of radiance collected by
the radiometer. The cloud types of interest are: clear sky, thin cirrus,
cirrus, rainy anvil, and convection core.
</p>
<p>We describe multi-step use of Machine Learning and Digital Twin of the
Earth's atmosphere to derive such a classifier. First, a digital twin of
Earth's atmosphere called a Weather Research Forecast (WRF) is used generate
simulated lookahead radiometer data as well as deeper "science" hidden
variables. The datasets simulate a tropical region over the Caribbean and a
non-tropical region over the Atlantic coast of the United States. A K-means
clustering over the scientific hidden variables was utilized by human experts
to generate an automatic labelling of the data - mapping each physical data
point to cloud types by scientists informed by mean/centroids of hidden
variables of the clusters. Next, classifiers were trained with the inputs of
the simulated radiometer data and its corresponding label. The classifiers of a
random decision forest (RDF), support vector machine (SVM), Gaussian na\"ive
bayes, feed forward artificial neural network (ANN), and a convolutional neural
network (CNN) were trained. Over the tropical dataset, the best performing
classifier was able to identify non-storm and storm clouds with over 80%
accuracy in each class for a held-out test set. Over the non-tropical dataset,
the best performing classifier was able to classify non-storm clouds with over
90% accuracy and storm clouds with over 40% accuracy. Additionally both sets of
classifiers were shown to be resilient to instrument noise.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07174">HurriCast: An Automatic Framework Using Machine Learning and Statistical Modeling for Hurricane Forecasting. (arXiv:2309.07174v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1">Shouwei Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1">Meiyan Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuepeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1">Wenqian Dong</a></p>
<p>Hurricanes present major challenges in the U.S. due to their devastating
impacts. Mitigating these risks is important, and the insurance industry is
central in this effort, using intricate statistical models for risk assessment.
However, these models often neglect key temporal and spatial hurricane patterns
and are limited by data scarcity. This study introduces a refined approach
combining the ARIMA model and K-MEANS to better capture hurricane trends, and
an Autoencoder for enhanced hurricane simulations. Our experiments show that
this hybrid methodology effectively simulate historical hurricane behaviors
while providing detailed projections of potential future trajectories and
intensities. Moreover, by leveraging a comprehensive yet selective dataset, our
simulations enrich the current understanding of hurricane patterns and offer
actionable insights for risk management strategies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07175">MELAGE: A purely python based Neuroimaging software (Neonatal). (arXiv:2309.07175v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Jafrasteh_B/0/1/0/all/0/1">Bahram Jafrasteh</a>, <a href="http://arxiv.org/find/eess/1/au:+Lopez_S/0/1/0/all/0/1">Sim&#xf3;n Pedro Lubi&#xe1;n L&#xf3;pez</a>, <a href="http://arxiv.org/find/eess/1/au:+Fernandez_I/0/1/0/all/0/1">Isabel Benavente Fern&#xe1;ndez</a></p>
<p>MELAGE, a pioneering Python-based neuroimaging software, emerges as a
versatile tool for the visualization, processing, and analysis of medical
images. Initially conceived to address the unique challenges of processing 3D
ultrasound and MRI brain images during the neonatal period, MELAGE exhibits
remarkable adaptability, extending its utility to the domain of adult human
brain imaging. At its core, MELAGE features a semi-automatic brain extraction
tool empowered by a deep learning module, ensuring precise and efficient brain
structure extraction from MRI and 3D Ultrasound data. Moreover, MELAGE offers a
comprehensive suite of features, encompassing dynamic 3D visualization,
accurate measurements, and interactive image segmentation. This transformative
software holds immense promise for researchers and clinicians, offering
streamlined image analysis, seamless integration with deep learning algorithms,
and broad applicability in the realm of medical imaging.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07176">Optimal and Fair Encouragement Policy Evaluation and Learning. (arXiv:2309.07176v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1">Angela Zhou</a></p>
<p>In consequential domains, it is often impossible to compel individuals to
take treatment, so that optimal policy rules are merely suggestions in the
presence of human non-adherence to treatment recommendations. In these same
domains, there may be heterogeneity both in who responds in taking-up
treatment, and heterogeneity in treatment efficacy. While optimal treatment
rules can maximize causal outcomes across the population, access parity
constraints or other fairness considerations can be relevant in the case of
encouragement. For example, in social services, a persistent puzzle is the gap
in take-up of beneficial services among those who may benefit from them the
most. When in addition the decision-maker has distributional preferences over
both access and average outcomes, the optimal decision rule changes. We study
causal identification, statistical variance-reduced estimation, and robust
estimation of optimal treatment rules, including under potential violations of
positivity. We consider fairness constraints such as demographic parity in
treatment take-up, and other constraints, via constrained optimization. Our
framework can be extended to handle algorithmic recommendations under an
often-reasonable covariate-conditional exclusion restriction, using our
robustness checks for lack of positivity in the recommendation. We develop a
two-stage algorithm for solving over parametrized policy classes under general
constraints to obtain variance-sensitive regret bounds. We illustrate the
methods in two case studies based on data from randomized encouragement to
enroll in insurance and from pretrial supervised release with electronic
monitoring.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07178">CloudBrain-NMR: An Intelligent Cloud Computing Platform for NMR Spectroscopy Processing, Reconstruction and Analysis. (arXiv:2309.07178v1 [q-bio.QM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Guo_D/0/1/0/all/0/1">Di Guo</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Li_S/0/1/0/all/0/1">Sijin Li</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Liu_J/0/1/0/all/0/1">Jun Liu</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Tu_Z/0/1/0/all/0/1">Zhangren Tu</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Qiu_T/0/1/0/all/0/1">Tianyu Qiu</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Xu_J/0/1/0/all/0/1">Jingjing Xu</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Feng_L/0/1/0/all/0/1">Liubin Feng</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Lin_D/0/1/0/all/0/1">Donghai Lin</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Hong_Q/0/1/0/all/0/1">Qing Hong</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Lin_M/0/1/0/all/0/1">Meijin Lin</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Lin_Y/0/1/0/all/0/1">Yanqin Lin</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Qu_X/0/1/0/all/0/1">Xiaobo Qu</a></p>
<p>Nuclear Magnetic Resonance (NMR) spectroscopy has served as a powerful
analytical tool for studying molecular structure and dynamics in chemistry and
biology. However, the processing of raw data acquired from NMR spectrometers
and subsequent quantitative analysis involves various specialized tools, which
necessitates comprehensive knowledge in programming and NMR. Particularly, the
emerging deep learning tools is hard to be widely used in NMR due to the
sophisticated setup of computation. Thus, NMR processing is not an easy task
for chemist and biologists. In this work, we present CloudBrain-NMR, an
intelligent online cloud computing platform designed for NMR data reading,
processing, reconstruction, and quantitative analysis. The platform is
conveniently accessed through a web browser, eliminating the need for any
program installation on the user side. CloudBrain-NMR uses parallel computing
with graphics processing units and central processing units, resulting in
significantly shortened computation time. Furthermore, it incorporates
state-of-the-art deep learning-based algorithms offering comprehensive
functionalities that allow users to complete the entire processing procedure
without relying on additional software. This platform has empowered NMR
applications with advanced artificial intelligence processing. CloudBrain-NMR
is openly accessible for free usage at https://csrc.xmu.edu.cn/CloudBrain.html
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07181">The Grand Illusion: The Myth of Software Portability and Implications for ML Progress. (arXiv:2309.07181v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mince_F/0/1/0/all/0/1">Fraser Mince</a>, <a href="http://arxiv.org/find/cs/1/au:+Dinh_D/0/1/0/all/0/1">Dzung Dinh</a>, <a href="http://arxiv.org/find/cs/1/au:+Kgomo_J/0/1/0/all/0/1">Jonas Kgomo</a>, <a href="http://arxiv.org/find/cs/1/au:+Thompson_N/0/1/0/all/0/1">Neil Thompson</a>, <a href="http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1">Sara Hooker</a></p>
<p>Pushing the boundaries of machine learning often requires exploring different
hardware and software combinations. However, the freedom to experiment across
different tooling stacks can be at odds with the drive for efficiency, which
has produced increasingly specialized AI hardware and incentivized
consolidation around a narrow set of ML frameworks. Exploratory research can be
restricted if software and hardware are co-evolving, making it even harder to
stray away from mainstream ideas that work well with popular tooling stacks.
While this friction increasingly impacts the rate of innovation in machine
learning, to our knowledge the lack of portability in tooling has not been
quantified. In this work, we ask: How portable are popular ML software
frameworks? We conduct a large-scale study of the portability of mainstream ML
frameworks across different hardware types. Our findings paint an uncomfortable
picture -- frameworks can lose more than 40% of their key functions when ported
to other hardware. Worse, even when functions are portable, the slowdown in
their performance can be extreme and render performance untenable.
Collectively, our results reveal how costly straying from a narrow set of
hardware-software combinations can be - and suggest that specialization of
hardware impedes innovation in machine learning research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07182">Sleep Stage Classification Using a Pre-trained Deep Learning Model. (arXiv:2309.07182v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ardeshir_H/0/1/0/all/0/1">Hassan Ardeshir</a>, <a href="http://arxiv.org/find/eess/1/au:+Araghi_M/0/1/0/all/0/1">Mohammad Araghi</a></p>
<p>One of the common human diseases is sleep disorders. The classification of
sleep stages plays a fundamental role in diagnosing sleep disorders, monitoring
treatment effectiveness, and understanding the relationship between sleep
stages and various health conditions. A precise and efficient classification of
these stages can significantly enhance our understanding of sleep-related
phenomena and ultimately lead to improved health outcomes and disease
treatment.
</p>
<p>Models others propose are often time-consuming and lack sufficient accuracy,
especially in stage N1. The main objective of this research is to present a
machine-learning model called "EEGMobile". This model utilizes pre-trained
models and learns from electroencephalogram (EEG) spectrograms of brain
signals. The model achieved an accuracy of 86.97% on a publicly available
dataset named "Sleep-EDF20", outperforming other models proposed by different
researchers. Moreover, it recorded an accuracy of 56.4% in stage N1, which is
better than other models. These findings demonstrate that this model has the
potential to achieve better results for the treatment of this disease.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07183">Audio-Based Classification of Respiratory Diseases using Advanced Signal Processing and Machine Learning for Assistive Diagnosis Support. (arXiv:2309.07183v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Casado_C/0/1/0/all/0/1">Constantino &#xc1;lvarez Casado</a>, <a href="http://arxiv.org/find/eess/1/au:+Canellas_M/0/1/0/all/0/1">Manuel Lage Ca&#xf1;ellas</a>, <a href="http://arxiv.org/find/eess/1/au:+Pedone_M/0/1/0/all/0/1">Matteo Pedone</a>, <a href="http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1">Xiaoting Wu</a>, <a href="http://arxiv.org/find/eess/1/au:+Lopez_M/0/1/0/all/0/1">Miguel Bordallo L&#xf3;pez</a></p>
<p>In global healthcare, respiratory diseases are a leading cause of mortality,
underscoring the need for rapid and accurate diagnostics. To advance rapid
screening techniques via auscultation, our research focuses on employing one of
the largest publicly available medical database of respiratory sounds to train
multiple machine learning models able to classify different health conditions.
Our method combines Empirical Mode Decomposition (EMD) and spectral analysis to
extract physiologically relevant biosignals from acoustic data, closely tied to
cardiovascular and respiratory patterns, making our approach apart in its
departure from conventional audio feature extraction practices. We use Power
Spectral Density analysis and filtering techniques to select Intrinsic Mode
Functions (IMFs) strongly correlated with underlying physiological phenomena.
These biosignals undergo a comprehensive feature extraction process for
predictive modeling. Initially, we deploy a binary classification model that
demonstrates a balanced accuracy of 87% in distinguishing between healthy and
diseased individuals. Subsequently, we employ a six-class classification model
that achieves a balanced accuracy of 72% in diagnosing specific respiratory
conditions like pneumonia and chronic obstructive pulmonary disease (COPD). For
the first time, we also introduce regression models that estimate age and body
mass index (BMI) based solely on acoustic data, as well as a model for gender
classification. Our findings underscore the potential of this approach to
significantly enhance assistive and remote diagnostic capabilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07187">Multi-step prediction of chlorophyll concentration based on Adaptive Graph-Temporal Convolutional Network with Series Decomposition. (arXiv:2309.07187v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Ying Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongbo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1">Wenyang Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Xv_C/0/1/0/all/0/1">Chongxuan Xv</a></p>
<p>Chlorophyll concentration can well reflect the nutritional status and algal
blooms of water bodies, and is an important indicator for evaluating water
quality. The prediction of chlorophyll concentration change trend is of great
significance to environmental protection and aquaculture. However, there is a
complex and indistinguishable nonlinear relationship between many factors
affecting chlorophyll concentration. In order to effectively mine the nonlinear
features contained in the data. This paper proposes a time-series decomposition
adaptive graph-time convolutional network ( AGTCNSD ) prediction model.
Firstly, the original sequence is decomposed into trend component and periodic
component by moving average method. Secondly, based on the graph convolutional
neural network, the water quality parameter data is modeled, and a parameter
embedding matrix is defined. The idea of matrix decomposition is used to assign
weight parameters to each node. The adaptive graph convolution learns the
relationship between different water quality parameters, updates the state
information of each parameter, and improves the learning ability of the update
relationship between nodes. Finally, time dependence is captured by time
convolution to achieve multi-step prediction of chlorophyll concentration. The
validity of the model is verified by the water quality data of the coastal city
Beihai. The results show that the prediction effect of this method is better
than other methods. It can be used as a scientific resource for environmental
management decision-making.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07188">Predicting Survival Time of Ball Bearings in the Presence of Censoring. (arXiv:2309.07188v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Lillelund_C/0/1/0/all/0/1">Christian Marius Lillelund</a>, <a href="http://arxiv.org/find/eess/1/au:+Pannullo_F/0/1/0/all/0/1">Fernando Pannullo</a>, <a href="http://arxiv.org/find/eess/1/au:+Jakobsen_M/0/1/0/all/0/1">Morten Opprud Jakobsen</a>, <a href="http://arxiv.org/find/eess/1/au:+Pedersen_C/0/1/0/all/0/1">Christian Fischer Pedersen</a></p>
<p>Ball bearings find widespread use in various manufacturing and mechanical
domains, and methods based on machine learning have been widely adopted in the
field to monitor wear and spot defects before they lead to failures. Few
studies, however, have addressed the problem of censored data, in which failure
is not observed. In this paper, we propose a novel approach to predict the time
to failure in ball bearings using survival analysis. First, we analyze bearing
data in the frequency domain and annotate when a bearing fails by comparing the
Kullback-Leibler divergence and the standard deviation between its break-in
frequency bins and its break-out frequency bins. Second, we train several
survival models to estimate the time to failure based on the annotated data and
covariates extracted from the time domain, such as skewness, kurtosis and
entropy. The models give a probabilistic prediction of risk over time and allow
us to compare the survival function between groups of bearings. We demonstrate
our approach on the XJTU and PRONOSTIA datasets. On XJTU, the best result is a
0.70 concordance-index and 0.21 integrated Brier score. On PRONOSTIA, the best
is a 0.76 concordance-index and 0.19 integrated Brier score. Our work motivates
further work on incorporating censored data in models for predictive
maintenance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07189">Learning From Drift: Federated Learning on Non-IID Data via Drift Regularization. (arXiv:2309.07189v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Yeachan Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Shin_B/0/1/0/all/0/1">Bonggun Shin</a></p>
<p>Federated learning algorithms perform reasonably well on independent and
identically distributed (IID) data. They, on the other hand, suffer greatly
from heterogeneous environments, i.e., Non-IID data. Despite the fact that many
research projects have been done to address this issue, recent findings
indicate that they are still sub-optimal when compared to training on IID data.
In this work, we carefully analyze the existing methods in heterogeneous
environments. Interestingly, we find that regularizing the classifier's outputs
is quite effective in preventing performance degradation on Non-IID data.
Motivated by this, we propose Learning from Drift (LfD), a novel method for
effectively training the model in heterogeneous settings. Our scheme
encapsulates two key components: drift estimation and drift regularization.
Specifically, LfD first estimates how different the local model is from the
global model (i.e., drift). The local model is then regularized such that it
does not fall in the direction of the estimated drift. In the experiment, we
evaluate each method through the lens of the five aspects of federated
learning, i.e., Generalization, Heterogeneity, Scalability, Forgetting, and
Efficiency. Comprehensive evaluation results clearly support the superiority of
LfD in federated learning with Non-IID data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07192">The effect of data augmentation and 3D-CNN depth on Alzheimer&#x27;s Disease detection. (arXiv:2309.07192v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Turrisi_R/0/1/0/all/0/1">Rosanna Turrisi</a>, <a href="http://arxiv.org/find/eess/1/au:+Verri_A/0/1/0/all/0/1">Alessandro Verri</a>, <a href="http://arxiv.org/find/eess/1/au:+Barla_A/0/1/0/all/0/1">Annalisa Barla</a></p>
<p>Machine Learning (ML) has emerged as a promising approach in healthcare,
outperforming traditional statistical techniques. However, to establish ML as a
reliable tool in clinical practice, adherence to best practices regarding data
handling, experimental design, and model evaluation is crucial. This work
summarizes and strictly observes such practices to ensure reproducible and
reliable ML. Specifically, we focus on Alzheimer's Disease (AD) detection,
which serves as a paradigmatic example of challenging problem in healthcare. We
investigate the impact of different data augmentation techniques and model
complexity on the overall performance. We consider MRI data from ADNI dataset
to address a classification problem employing 3D Convolutional Neural Network
(CNN). The experiments are designed to compensate for data scarcity and initial
random parameters by utilizing cross-validation and multiple training trials.
Within this framework, we train 15 predictive models, considering three
different data augmentation strategies and five distinct 3D CNN architectures,
each varying in the number of convolutional layers. Specifically, the
augmentation strategies are based on affine transformations, such as zoom,
shift, and rotation, applied concurrently or separately. The combined effect of
data augmentation and model complexity leads to a variation in prediction
performance up to 10% of accuracy. When affine transformation are applied
separately, the model is more accurate, independently from the adopted
architecture. For all strategies, the model accuracy followed a concave
behavior at increasing number of convolutional layers, peaking at an
intermediate value of layers. The best model (8 CL, (B)) is the most stable
across cross-validation folds and training trials, reaching excellent
performance both on the testing set and on an external test set.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07193">A Robust SINDy Approach by Combining Neural Networks and an Integral Form. (arXiv:2309.07193v1 [math.DS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Forootani_A/0/1/0/all/0/1">Ali Forootani</a>, <a href="http://arxiv.org/find/math/1/au:+Goyal_P/0/1/0/all/0/1">Pawan Goyal</a>, <a href="http://arxiv.org/find/math/1/au:+Benner_P/0/1/0/all/0/1">Peter Benner</a></p>
<p>The discovery of governing equations from data has been an active field of
research for decades. One widely used methodology for this purpose is sparse
regression for nonlinear dynamics, known as SINDy. Despite several attempts,
noisy and scarce data still pose a severe challenge to the success of the SINDy
approach. In this work, we discuss a robust method to discover nonlinear
governing equations from noisy and scarce data. To do this, we make use of
neural networks to learn an implicit representation based on measurement data
so that not only it produces the output in the vicinity of the measurements but
also the time-evolution of output can be described by a dynamical system.
Additionally, we learn such a dynamic system in the spirit of the SINDy
framework. Leveraging the implicit representation using neural networks, we
obtain the derivative information -- required for SINDy -- using an automatic
differentiation tool. To enhance the robustness of our methodology, we further
incorporate an integral condition on the output of the implicit networks.
Furthermore, we extend our methodology to handle data collected from multiple
initial conditions. We demonstrate the efficiency of the proposed methodology
to discover governing equations under noisy and scarce data regimes by means of
several examples and compare its performance with existing methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07196">Attention-based Dynamic Graph Convolutional Recurrent Neural Network for Traffic Flow Prediction in Highway Transportation. (arXiv:2309.07196v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tianpu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1">Weilong Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_M/0/1/0/all/0/1">Mengda Xing</a></p>
<p>As one of the important tools for spatial feature extraction, graph
convolution has been applied in a wide range of fields such as traffic flow
prediction. However, current popular works of graph convolution cannot
guarantee spatio-temporal consistency in a long period. The ignorance of
correlational dynamics, convolutional locality and temporal comprehensiveness
would limit predictive accuracy. In this paper, a novel Attention-based Dynamic
Graph Convolutional Recurrent Neural Network (ADGCRNN) is proposed to improve
traffic flow prediction in highway transportation. Three temporal resolutions
of data sequence are effectively integrated by self-attention to extract
characteristics; multi-dynamic graphs and their weights are dynamically created
to compliantly combine the varying characteristics; a dedicated gated kernel
emphasizing highly relative nodes is introduced on these complete graphs to
reduce overfitting for graph convolution operations. Experiments on two public
datasets show our work better than state-of-the-art baselines, and case studies
of a real Web system prove practical benefit in highway transportation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07197">Mitigating Adversarial Attacks in Federated Learning with Trusted Execution Environments. (arXiv:2309.07197v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Queyrut_S/0/1/0/all/0/1">Simon Queyrut</a>, <a href="http://arxiv.org/find/cs/1/au:+Schiavoni_V/0/1/0/all/0/1">Valerio Schiavoni</a>, <a href="http://arxiv.org/find/cs/1/au:+Felber_P/0/1/0/all/0/1">Pascal Felber</a></p>
<p>The main premise of federated learning (FL) is that machine learning model
updates are computed locally to preserve user data privacy. This approach
avoids by design user data to ever leave the perimeter of their device. Once
the updates aggregated, the model is broadcast to all nodes in the federation.
However, without proper defenses, compromised nodes can probe the model inside
their local memory in search for adversarial examples, which can lead to
dangerous real-world scenarios. For instance, in image-based applications,
adversarial examples consist of images slightly perturbed to the human eye
getting misclassified by the local model. These adversarial images are then
later presented to a victim node's counterpart model to replay the attack.
Typical examples harness dissemination strategies such as altered traffic signs
(patch attacks) no longer recognized by autonomous vehicles or seemingly
unaltered samples that poison the local dataset of the FL scheme to undermine
its robustness. Pelta is a novel shielding mechanism leveraging Trusted
Execution Environments (TEEs) that reduce the ability of attackers to craft
adversarial samples. Pelta masks inside the TEE the first part of the
back-propagation chain rule, typically exploited by attackers to craft the
malicious samples. We evaluate Pelta on state-of-the-art accurate models using
three well-established datasets: CIFAR-10, CIFAR-100 and ImageNet. We show the
effectiveness of Pelta in mitigating six white-box state-of-the-art adversarial
attacks, such as Projected Gradient Descent, Momentum Iterative Method, Auto
Projected Gradient Descent, the Carlini &amp; Wagner attack. In particular, Pelta
constitutes the first attempt at defending an ensemble model against the
Self-Attention Gradient attack to the best of our knowledge. Our code is
available to the research community at https://github.com/queyrusi/Pelta.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07200">Latent Representation and Simulation of Markov Processes via Time-Lagged Information Bottleneck. (arXiv:2309.07200v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Federici_M/0/1/0/all/0/1">Marco Federici</a>, <a href="http://arxiv.org/find/cs/1/au:+Forre_P/0/1/0/all/0/1">Patrick Forr&#xe9;</a>, <a href="http://arxiv.org/find/cs/1/au:+Tomioka_R/0/1/0/all/0/1">Ryota Tomioka</a>, <a href="http://arxiv.org/find/cs/1/au:+Veeling_B/0/1/0/all/0/1">Bastiaan S. Veeling</a></p>
<p>Markov processes are widely used mathematical models for describing dynamic
systems in various fields. However, accurately simulating large-scale systems
at long time scales is computationally expensive due to the short time steps
required for accurate integration. In this paper, we introduce an inference
process that maps complex systems into a simplified representational space and
models large jumps in time. To achieve this, we propose Time-lagged Information
Bottleneck (T-IB), a principled objective rooted in information theory, which
aims to capture relevant temporal features while discarding high-frequency
information to simplify the simulation task and minimize the inference error.
Our experiments demonstrate that T-IB learns information-optimal
representations for accurately modeling the statistical properties and dynamics
of the original process at a selected time lag, outperforming existing
time-lagged dimensionality reduction methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07207">EarthPT: a foundation model for Earth Observation. (arXiv:2309.07207v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Smith_M/0/1/0/all/0/1">Michael J. Smith</a>, <a href="http://arxiv.org/find/cs/1/au:+Fleming_L/0/1/0/all/0/1">Luke Fleming</a>, <a href="http://arxiv.org/find/cs/1/au:+Geach_J/0/1/0/all/0/1">James E. Geach</a></p>
<p>We introduce EarthPT -- an Earth Observation (EO) pretrained transformer.
EarthPT is a 700 million parameter decoding transformer foundation model
trained in an autoregressive self-supervised manner and developed specifically
with EO use-cases in mind. We demonstrate that EarthPT is an effective
forecaster that can accurately predict future pixel-level surface reflectances
across the 400-2300 nm range well into the future. For example, forecasts of
the evolution of the Normalised Difference Vegetation Index (NDVI) have a
typical error of approximately 0.05 (over a natural range of -1 -&gt; 1) at the
pixel level over a five month test set horizon, out-performing simple
phase-folded models based on historical averaging. We also demonstrate that
embeddings learnt by EarthPT hold semantically meaningful information and could
be exploited for downstream tasks such as highly granular, dynamic land use
classification. Excitingly, we note that the abundance of EO data provides us
with -- in theory -- quadrillions of training tokens. Therefore, if we assume
that EarthPT follows neural scaling laws akin to those derived for Large
Language Models (LLMs), there is currently no data-imposed limit to scaling
EarthPT and other similar `Large Observation Models.'
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07235">Autotuning Apache TVM-based Scientific Applications Using Bayesian Optimization. (arXiv:2309.07235v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xingfu Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Paramasivam_P/0/1/0/all/0/1">Praveen Paramasivam</a>, <a href="http://arxiv.org/find/cs/1/au:+Taylor_V/0/1/0/all/0/1">Valerie Taylor</a></p>
<p>Apache TVM (Tensor Virtual Machine), an open source machine learning compiler
framework designed to optimize computations across various hardware platforms,
provides an opportunity to improve the performance of dense matrix
factorizations such as LU (Lower Upper) decomposition and Cholesky
decomposition on GPUs and AI (Artificial Intelligence) accelerators. In this
paper, we propose a new TVM autotuning framework using Bayesian Optimization
and use the TVM tensor expression language to implement linear algebra kernels
such as LU, Cholesky, and 3mm. We use these scientific computation kernels to
evaluate the effectiveness of our methods on a GPU cluster, called Swing, at
Argonne National Laboratory. We compare the proposed autotuning framework with
the TVM autotuning framework AutoTVM with four tuners and find that our
framework outperforms AutoTVM in most cases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07250">All you need is spin: SU(2) equivariant variational quantum circuits based on spin networks. (arXiv:2309.07250v1 [quant-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+East_R/0/1/0/all/0/1">Richard D. P. East</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Alonso_Linaje_G/0/1/0/all/0/1">Guillermo Alonso-Linaje</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Park_C/0/1/0/all/0/1">Chae-Yeun Park</a></p>
<p>Variational algorithms require architectures that naturally constrain the
optimisation space to run efficiently. In geometric quantum machine learning,
one achieves this by encoding group structure into parameterised quantum
circuits to include the symmetries of a problem as an inductive bias. However,
constructing such circuits is challenging as a concrete guiding principle has
yet to emerge. In this paper, we propose the use of spin networks, a form of
directed tensor network invariant under a group transformation, to devise SU(2)
equivariant quantum circuit ans\"atze -- circuits possessing spin rotation
symmetry. By changing to the basis that block diagonalises SU(2) group action,
these networks provide a natural building block for constructing parameterised
equivariant quantum circuits. We prove that our construction is mathematically
equivalent to other known constructions, such as those based on twirling and
generalised permutations, but more direct to implement on quantum hardware. The
efficacy of our constructed circuits is tested by solving the ground state
problem of SU(2) symmetric Heisenberg models on the one-dimensional triangular
lattice and on the Kagome lattice. Our results highlight that our equivariant
circuits boost the performance of quantum variational algorithms, indicating
broader applicability to other real-world problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07259">Solving Recurrence Relations using Machine Learning, with Application to Cost Analysis. (arXiv:2309.07259v1 [cs.PL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Klemen_M/0/1/0/all/0/1">Maximiliano Klemen</a>, <a href="http://arxiv.org/find/cs/1/au:+Carreira_Perpinan_M/0/1/0/all/0/1">Miguel &#xc1;. Carreira-Perpi&#xf1;&#xe1;n</a>, <a href="http://arxiv.org/find/cs/1/au:+Lopez_Garcia_P/0/1/0/all/0/1">Pedro Lopez-Garcia</a></p>
<p>Automatic static cost analysis infers information about the resources used by
programs without actually running them with concrete data, and presents such
information as functions of input data sizes. Most of the analysis tools for
logic programs (and other languages) are based on setting up recurrence
relations representing (bounds on) the computational cost of predicates, and
solving them to find closed-form functions that are equivalent to (or a bound
on) them. Such recurrence solving is a bottleneck in current tools: many of the
recurrences that arise during the analysis cannot be solved with current
solvers, such as Computer Algebra Systems (CASs), so that specific methods for
different classes of recurrences need to be developed. We address such a
challenge by developing a novel, general approach for solving arbitrary,
constrained recurrence relations, that uses machine-learning sparse regression
techniques to guess a candidate closed-form function, and a combination of an
SMT-solver and a CAS to check whether such function is actually a solution of
the recurrence. We have implemented a prototype and evaluated it with
recurrences generated by a cost analysis system (the one in CiaoPP). The
experimental results are quite promising, showing that our approach can find
closed-form solutions, in a reasonable time, for classes of recurrences that
cannot be solved by such a system, nor by current CASs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07261">Simultaneous inference for generalized linear models with unmeasured confounders. (arXiv:2309.07261v1 [stat.ME])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Du_J/0/1/0/all/0/1">Jin-Hong Du</a>, <a href="http://arxiv.org/find/stat/1/au:+Wasserman_L/0/1/0/all/0/1">Larry Wasserman</a>, <a href="http://arxiv.org/find/stat/1/au:+Roeder_K/0/1/0/all/0/1">Kathryn Roeder</a></p>
<p>Tens of thousands of simultaneous hypothesis tests are routinely performed in
genomic studies to identify differentially expressed genes. However, due to
unmeasured confounders, many standard statistical approaches may be
substantially biased. This paper investigates the large-scale hypothesis
testing problem for multivariate generalized linear models in the presence of
confounding effects. Under arbitrary confounding mechanisms, we propose a
unified statistical estimation and inference framework that harnesses
orthogonal structures and integrates linear projections into three key stages.
It first leverages multivariate responses to separate marginal and uncorrelated
confounding effects, recovering the confounding coefficients' column space.
Subsequently, latent factors and primary effects are jointly estimated,
utilizing $\ell_1$-regularization for sparsity while imposing orthogonality
onto confounding coefficients. Finally, we incorporate projected and weighted
bias-correction steps for hypothesis testing. Theoretically, we establish
various effects' identification conditions and non-asymptotic error bounds. We
show effective Type-I error control of asymptotic $z$-tests as sample and
response sizes approach infinity. Numerical experiments demonstrate that the
proposed method controls the false discovery rate by the Benjamini-Hochberg
procedure and is more powerful than alternative methods. By comparing
single-cell RNA-seq counts from two groups of samples, we demonstrate the
suitability of adjusting confounding effects when significant covariates are
absent from the model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07265">Safe and Accelerated Deep Reinforcement Learning-based O-RAN Slicing: A Hybrid Transfer Learning Approach. (arXiv:2309.07265v1 [cs.NI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nagib_A/0/1/0/all/0/1">Ahmad M. Nagib</a>, <a href="http://arxiv.org/find/cs/1/au:+Abou_Zeid_H/0/1/0/all/0/1">Hatem Abou-Zeid</a>, <a href="http://arxiv.org/find/cs/1/au:+Hassanein_H/0/1/0/all/0/1">Hossam S. Hassanein</a></p>
<p>The open radio access network (O-RAN) architecture supports intelligent
network control algorithms as one of its core capabilities. Data-driven
applications incorporate such algorithms to optimize radio access network (RAN)
functions via RAN intelligent controllers (RICs). Deep reinforcement learning
(DRL) algorithms are among the main approaches adopted in the O-RAN literature
to solve dynamic radio resource management problems. However, despite the
benefits introduced by the O-RAN RICs, the practical adoption of DRL algorithms
in real network deployments falls behind. This is primarily due to the slow
convergence and unstable performance exhibited by DRL agents upon deployment
and when facing previously unseen network conditions. In this paper, we address
these challenges by proposing transfer learning (TL) as a core component of the
training and deployment workflows for the DRL-based closed-loop control of
O-RAN functionalities. To this end, we propose and design a hybrid TL-aided
approach that leverages the advantages of both policy reuse and distillation TL
methods to provide safe and accelerated convergence in DRL-based O-RAN slicing.
We conduct a thorough experiment that accommodates multiple services, including
real VR gaming traffic to reflect practical scenarios of O-RAN slicing. We also
propose and implement policy reuse and distillation-aided DRL and non-TL-aided
DRL as three separate baselines. The proposed hybrid approach shows at least:
7.7% and 20.7% improvements in the average initial reward value and the
percentage of converged scenarios, and a 64.6% decrease in reward variance
while maintaining fast convergence and enhancing the generalizability compared
with the baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07277">Unbiased Face Synthesis With Diffusion Models: Are We There Yet?. (arXiv:2309.07277v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rosenberg_H/0/1/0/all/0/1">Harrison Rosenberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1">Shimaa Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramesh_G/0/1/0/all/0/1">Guruprasad V Ramesh</a>, <a href="http://arxiv.org/find/cs/1/au:+Vinayak_R/0/1/0/all/0/1">Ramya Korlakai Vinayak</a>, <a href="http://arxiv.org/find/cs/1/au:+Fawaz_K/0/1/0/all/0/1">Kassem Fawaz</a></p>
<p>Text-to-image diffusion models have achieved widespread popularity due to
their unprecedented image generation capability. In particular, their ability
to synthesize and modify human faces has spurred research into using generated
face images in both training data augmentation and model performance
assessments. In this paper, we study the efficacy and shortcomings of
generative models in the context of face generation. Utilizing a combination of
qualitative and quantitative measures, including embedding-based metrics and
user studies, we present a framework to audit the characteristics of generated
faces conditioned on a set of social attributes. We applied our framework on
faces generated through state-of-the-art text-to-image diffusion models. We
identify several limitations of face image generation that include faithfulness
to the text prompt, demographic disparities, and distributional shifts.
Furthermore, we present an analytical model that provides insights into how
training data selection contributes to the performance of generative models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07289">User Training with Error Augmentation for Electromyogram-based Gesture Classification. (arXiv:2309.07289v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bicer_Y/0/1/0/all/0/1">Yunus Bicer</a>, <a href="http://arxiv.org/find/cs/1/au:+Smedemark_Margulies_N/0/1/0/all/0/1">Niklas Smedemark-Margulies</a>, <a href="http://arxiv.org/find/cs/1/au:+Celik_B/0/1/0/all/0/1">Basak Celik</a>, <a href="http://arxiv.org/find/cs/1/au:+Sunger_E/0/1/0/all/0/1">Elifnur Sunger</a>, <a href="http://arxiv.org/find/cs/1/au:+Orendorff_R/0/1/0/all/0/1">Ryan Orendorff</a>, <a href="http://arxiv.org/find/cs/1/au:+Naufel_S/0/1/0/all/0/1">Stephanie Naufel</a>, <a href="http://arxiv.org/find/cs/1/au:+Imbiriba_T/0/1/0/all/0/1">Tales Imbiriba</a>, <a href="http://arxiv.org/find/cs/1/au:+Erdo%7Bg%7Dmu%7Bs%7D_D/0/1/0/all/0/1">Deniz Erdo{&#x11f;}mu{&#x15f;}</a>, <a href="http://arxiv.org/find/cs/1/au:+Tunik_E/0/1/0/all/0/1">Eugene Tunik</a>, <a href="http://arxiv.org/find/cs/1/au:+Yarossi_M/0/1/0/all/0/1">Mathew Yarossi</a></p>
<p>We designed and tested a system for real-time control of a user interface by
extracting surface electromyographic (sEMG) activity from eight electrodes in a
wrist-band configuration. sEMG data were streamed into a machine-learning
algorithm that classified hand gestures in real-time. After an initial model
calibration, participants were presented with one of three types of feedback
during a human-learning stage: veridical feedback, in which predicted
probabilities from the gesture classification algorithm were displayed without
alteration, modified feedback, in which we applied a hidden augmentation of
error to these probabilities, and no feedback. User performance was then
evaluated in a series of minigames, in which subjects were required to use
eight gestures to manipulate their game avatar to complete a task. Experimental
results indicated that, relative to baseline, the modified feedback condition
led to significantly improved accuracy and improved gesture class separation.
These findings suggest that real-time feedback in a gamified user interface
with manipulation of feedback may enable intuitive, rapid, and accurate task
acquisition for sEMG-based gesture recognition applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07315">Traveling Words: A Geometric Interpretation of Transformers. (arXiv:2309.07315v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Molina_R/0/1/0/all/0/1">Raul Molina</a></p>
<p>Transformers have significantly advanced the field of natural language
processing, but comprehending their internal mechanisms remains a challenge. In
this paper, we introduce a novel geometric perspective that elucidates the
inner mechanisms of transformer operations. Our primary contribution is
illustrating how layer normalization confines the latent features to a
hyper-sphere, subsequently enabling attention to mold the semantic
representation of words on this surface. This geometric viewpoint seamlessly
connects established properties such as iterative refinement and contextual
embeddings. We validate our insights by probing a pre-trained 124M parameter
GPT-2 model. Our findings reveal clear query-key attention patterns in early
layers and build upon prior observations regarding the subject-specific nature
of attention heads at deeper layers. Harnessing these geometric insights, we
present an intuitive understanding of transformers, depicting them as processes
that model the trajectory of word particles along the hyper-sphere.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07332">Reliability-based cleaning of noisy training labels with inductive conformal prediction in multi-modal biomedical data mining. (arXiv:2309.07332v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1">Xianghao Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1">Qinmei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1">Yuanning Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1">Guangming Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gevaert_O/0/1/0/all/0/1">Olivier Gevaert</a></p>
<p>Accurately labeling biomedical data presents a challenge. Traditional
semi-supervised learning methods often under-utilize available unlabeled data.
To address this, we propose a novel reliability-based training data cleaning
method employing inductive conformal prediction (ICP). This method capitalizes
on a small set of accurately labeled training data and leverages ICP-calculated
reliability metrics to rectify mislabeled data and outliers within vast
quantities of noisy training data. The efficacy of the method is validated
across three classification tasks within distinct modalities: filtering
drug-induced-liver-injury (DILI) literature with title and abstract, predicting
ICU admission of COVID-19 patients through CT radiomics and electronic health
records, and subtyping breast cancer using RNA-sequencing data. Varying levels
of noise to the training labels were introduced through label permutation.
Results show significant enhancements in classification performance: accuracy
enhancement in 86 out of 96 DILI experiments (up to 11.4%), AUROC and AUPRC
enhancements in all 48 COVID-19 experiments (up to 23.8% and 69.8%), and
accuracy and macro-average F1 score improvements in 47 out of 48 RNA-sequencing
experiments (up to 74.6% and 89.0%). Our method offers the potential to
substantially boost classification performance in multi-modal biomedical
machine learning tasks. Importantly, it accomplishes this without necessitating
an excessive volume of meticulously curated training data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07339">Efficient quantum recurrent reinforcement learning via quantum reservoir computing. (arXiv:2309.07339v1 [quant-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Chen_S/0/1/0/all/0/1">Samuel Yen-Chi Chen</a></p>
<p>Quantum reinforcement learning (QRL) has emerged as a framework to solve
sequential decision-making tasks, showcasing empirical quantum advantages. A
notable development is through quantum recurrent neural networks (QRNNs) for
memory-intensive tasks such as partially observable environments. However, QRL
models incorporating QRNN encounter challenges such as inefficient training of
QRL with QRNN, given that the computation of gradients in QRNN is both
computationally expensive and time-consuming. This work presents a novel
approach to address this challenge by constructing QRL agents utilizing
QRNN-based reservoirs, specifically employing quantum long short-term memory
(QLSTM). QLSTM parameters are randomly initialized and fixed without training.
The model is trained using the asynchronous advantage actor-aritic (A3C)
algorithm. Through numerical simulations, we validate the efficacy of our
QLSTM-Reservoir RL framework. Its performance is assessed on standard
benchmarks, demonstrating comparable results to a fully trained QLSTM RL model
with identical architecture and training settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07344">Efficient Learning of PDEs via Taylor Expansion and Sparse Decomposition into Value and Fourier Domains. (arXiv:2309.07344v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nasim_M/0/1/0/all/0/1">Md Nasim</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1">Yexiang Xue</a></p>
<p>Accelerating the learning of Partial Differential Equations (PDEs) from
experimental data will speed up the pace of scientific discovery. Previous
randomized algorithms exploit sparsity in PDE updates for acceleration. However
such methods are applicable to a limited class of decomposable PDEs, which have
sparse features in the value domain. We propose Reel, which accelerates the
learning of PDEs via random projection and has much broader applicability. Reel
exploits the sparsity by decomposing dense updates into sparse ones in both the
value and frequency domains. This decomposition enables efficient learning when
the source of the updates consists of gradually changing terms across large
areas (sparse in the frequency domain) in addition to a few rapid updates
concentrated in a small set of "interfacial" regions (sparse in the value
domain). Random projection is then applied to compress the sparse signals for
learning. To expand the model applicability, Taylor series expansion is used in
Reel to approximate the nonlinear PDE updates with polynomials in the
decomposable form. Theoretically, we derive a constant factor approximation
between the projected loss function and the original one with poly-logarithmic
number of projected dimensions. Experimentally, we provide empirical evidence
that our proposed Reel can lead to faster learning of PDE models (70-98%
reduction in training time when the data is compressed to 1% of its original
size) with comparable quality as the non-compressed models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07352">Tackling the dimensions in imaging genetics with CLUB-PLS. (arXiv:2309.07352v1 [q-bio.GN])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Altmann_A/0/1/0/all/0/1">Andre Altmann</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Aquila_A/0/1/0/all/0/1">Ana C Lawry Aquila</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Jahanshad_N/0/1/0/all/0/1">Neda Jahanshad</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Thompson_P/0/1/0/all/0/1">Paul M Thompson</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Lorenzi_M/0/1/0/all/0/1">Marco Lorenzi</a></p>
<p>A major challenge in imaging genetics and similar fields is to link
high-dimensional data in one domain, e.g., genetic data, to high dimensional
data in a second domain, e.g., brain imaging data. The standard approach in the
area are mass univariate analyses across genetic factors and imaging
phenotypes. That entails executing one genome-wide association study (GWAS) for
each pre-defined imaging measure. Although this approach has been tremendously
successful, one shortcoming is that phenotypes must be pre-defined.
Consequently, effects that are not confined to pre-selected regions of interest
or that reflect larger brain-wide patterns can easily be missed. In this work
we introduce a Partial Least Squares (PLS)-based framework, which we term
Cluster-Bootstrap PLS (CLUB-PLS), that can work with large input dimensions in
both domains as well as with large sample sizes. One key factor of the
framework is to use cluster bootstrap to provide robust statistics for single
input features in both domains. We applied CLUB-PLS to investigating the
genetic basis of surface area and cortical thickness in a sample of 33,000
subjects from the UK Biobank. We found 107 genome-wide significant
locus-phenotype pairs that are linked to 386 different genes. We found that a
vast majority of these loci could be technically validated at a high rate:
using classic GWAS or Genome-Wide Inferred Statistics (GWIS) we found that 85
locus-phenotype pairs exceeded the genome-wide suggestive (P&lt;1e-05) threshold.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07364">Hodge-Aware Contrastive Learning. (arXiv:2309.07364v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mollers_A/0/1/0/all/0/1">Alexander M&#xf6;llers</a>, <a href="http://arxiv.org/find/cs/1/au:+Immer_A/0/1/0/all/0/1">Alexander Immer</a>, <a href="http://arxiv.org/find/cs/1/au:+Fortuin_V/0/1/0/all/0/1">Vincent Fortuin</a>, <a href="http://arxiv.org/find/cs/1/au:+Isufi_E/0/1/0/all/0/1">Elvin Isufi</a></p>
<p>Simplicial complexes prove effective in modeling data with multiway
dependencies, such as data defined along the edges of networks or within other
higher-order structures. Their spectrum can be decomposed into three
interpretable subspaces via the Hodge decomposition, resulting foundational in
numerous applications. We leverage this decomposition to develop a contrastive
self-supervised learning approach for processing simplicial data and generating
embeddings that encapsulate specific spectral information.Specifically, we
encode the pertinent data invariances through simplicial neural networks and
devise augmentations that yield positive contrastive examples with suitable
spectral properties for downstream tasks. Additionally, we reweight the
significance of negative examples in the contrastive loss, considering the
similarity of their Hodge components to the anchor. By encouraging a stronger
separation among less similar instances, we obtain an embedding space that
reflects the spectral properties of the data. The numerical results on two
standard edge flow classification tasks show a superior performance even when
compared to supervised learning techniques. Our findings underscore the
importance of adopting a spectral perspective for contrastive learning with
higher-order data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07367">The kernel-balanced equation for deep neural networks. (arXiv:2309.07367v1 [cond-mat.dis-nn])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Nakazato_K/0/1/0/all/0/1">Kenichi Nakazato</a></p>
<p>Deep neural networks have shown many fruitful applications in this decade. A
network can get the generalized function through training with a finite
dataset. The degree of generalization is a realization of the proximity scale
in the data space. Specifically, the scale is not clear if the dataset is
complicated. Here we consider a network for the distribution estimation of the
dataset. We show the estimation is unstable and the instability depends on the
data density and training duration. We derive the kernel-balanced equation,
which gives a short phenomenological description of the solution. The equation
tells us the reason for the instability and the mechanism of the scale. The
network outputs a local average of the dataset as a prediction and the scale of
averaging is determined along the equation. The scale gradually decreases along
training and finally results in instability in our case.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07374">Beta quantile regression for robust estimation of uncertainty in the presence of outliers. (arXiv:2309.07374v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Akrami_H/0/1/0/all/0/1">Haleh Akrami</a>, <a href="http://arxiv.org/find/cs/1/au:+Zamzam_O/0/1/0/all/0/1">Omar Zamzam</a>, <a href="http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1">Anand Joshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Aydore_S/0/1/0/all/0/1">Sergul Aydore</a>, <a href="http://arxiv.org/find/cs/1/au:+Leahy_R/0/1/0/all/0/1">Richard Leahy</a></p>
<p>Quantile Regression (QR) can be used to estimate aleatoric uncertainty in
deep neural networks and can generate prediction intervals. Quantifying
uncertainty is particularly important in critical applications such as clinical
diagnosis, where a realistic assessment of uncertainty is essential in
determining disease status and planning the appropriate treatment. The most
common application of quantile regression models is in cases where the
parametric likelihood cannot be specified. Although quantile regression is
quite robust to outlier response observations, it can be sensitive to outlier
covariate observations (features). Outlier features can compromise the
performance of deep learning regression problems such as style translation,
image reconstruction, and deep anomaly detection, potentially leading to
misleading conclusions. To address this problem, we propose a robust solution
for quantile regression that incorporates concepts from robust divergence. We
compare the performance of our proposed method with (i) least trimmed quantile
regression and (ii) robust regression based on the regularization of
case-specific parameters in a simple real dataset in the presence of outlier.
These methods have not been applied in a deep learning framework. We also
demonstrate the applicability of the proposed method by applying it to a
medical imaging translation task using diffusion models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07383">Rates of Convergence in Certain Native Spaces of Approximations used in Reinforcement Learning. (arXiv:2309.07383v1 [eess.SY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Bouland_A/0/1/0/all/0/1">Ali Bouland</a>, <a href="http://arxiv.org/find/eess/1/au:+Niu_S/0/1/0/all/0/1">Shengyuan Niu</a>, <a href="http://arxiv.org/find/eess/1/au:+Paruchuri_S/0/1/0/all/0/1">Sai Tej Paruchuri</a>, <a href="http://arxiv.org/find/eess/1/au:+Kurdila_A/0/1/0/all/0/1">Andrew Kurdila</a>, <a href="http://arxiv.org/find/eess/1/au:+Burns_J/0/1/0/all/0/1">John Burns</a>, <a href="http://arxiv.org/find/eess/1/au:+Schuster_E/0/1/0/all/0/1">Eugenio Schuster</a></p>
<p>This paper studies convergence rates for some value function approximations
that arise in a collection of reproducing kernel Hilbert spaces (RKHS)
$H(\Omega)$. By casting an optimal control problem in a specific class of
native spaces, strong rates of convergence are derived for the operator
equation that enables offline approximations that appear in policy iteration.
Explicit upper bounds on error in value function approximations are derived in
terms of power function $\Pwr_{H,N}$ for the space of finite dimensional
approximants $H_N$ in the native space $H(\Omega)$. These bounds are geometric
in nature and refine some well-known, now classical results concerning
convergence of approximations of value functions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07391">EnCodecMAE: Leveraging neural codecs for universal audio representation learning. (arXiv:2309.07391v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pepino_L/0/1/0/all/0/1">Leonardo Pepino</a>, <a href="http://arxiv.org/find/cs/1/au:+Riera_P/0/1/0/all/0/1">Pablo Riera</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferrer_L/0/1/0/all/0/1">Luciana Ferrer</a></p>
<p>The goal of universal audio representation learning is to obtain foundational
models that can be used for a variety of downstream tasks involving speech,
music or environmental sounds. To approach this problem, methods inspired by
self-supervised models from NLP, like BERT, are often used and adapted to
audio. These models rely on the discrete nature of text, hence adopting this
type of approach for audio processing requires either a change in the learning
objective or mapping the audio signal to a set of discrete classes. In this
work, we explore the use of EnCodec, a neural audio codec, to generate discrete
targets for learning an universal audio model based on a masked autoencoder
(MAE). We evaluate this approach, which we call EncodecMAE, on a wide range of
audio tasks spanning speech, music and environmental sounds, achieving
performances comparable or better than leading audio representation models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07398">Semantic Adversarial Attacks via Diffusion Models. (arXiv:2309.07398v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chenan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1">Jinhao Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Chaowei Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1">Edward Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Stamm_M/0/1/0/all/0/1">Matthew Stamm</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Kaidi Xu</a></p>
<p>Traditional adversarial attacks concentrate on manipulating clean examples in
the pixel space by adding adversarial perturbations. By contrast, semantic
adversarial attacks focus on changing semantic attributes of clean examples,
such as color, context, and features, which are more feasible in the real
world. In this paper, we propose a framework to quickly generate a semantic
adversarial attack by leveraging recent diffusion models since semantic
information is included in the latent space of well-trained diffusion models.
Then there are two variants of this framework: 1) the Semantic Transformation
(ST) approach fine-tunes the latent space of the generated image and/or the
diffusion model itself; 2) the Latent Masking (LM) approach masks the latent
space with another target image and local backpropagation-based interpretation
methods. Additionally, the ST approach can be applied in either white-box or
black-box settings. Extensive experiments are conducted on CelebA-HQ and AFHQ
datasets, and our framework demonstrates great fidelity, generalizability, and
transferability compared to other baselines. Our approaches achieve
approximately 100% attack success rate in multiple settings with the best FID
as 36.61. Code is available at
https://github.com/steven202/semantic_adv_via_dm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07402">Semi-supervised Domain Adaptation on Graphs with Contrastive Learning and Minimax Entropy. (arXiv:2309.07402v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1">Jiaren Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1">Quanyu Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1">Xiao Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xiaochen Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1">Jing Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Lam_J/0/1/0/all/0/1">James Lam</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwok_K/0/1/0/all/0/1">Ka-Wai Kwok</a></p>
<p>Label scarcity in a graph is frequently encountered in real-world
applications due to the high cost of data labeling. To this end,
semi-supervised domain adaptation (SSDA) on graphs aims to leverage the
knowledge of a labeled source graph to aid in node classification on a target
graph with limited labels. SSDA tasks need to overcome the domain gap between
the source and target graphs. However, to date, this challenging research
problem has yet to be formally considered by the existing approaches designed
for cross-graph node classification. To tackle the SSDA problem on graphs, a
novel method called SemiGCL is proposed, which benefits from graph contrastive
learning and minimax entropy training. SemiGCL generates informative node
representations by contrasting the representations learned from a graph's local
and global views. Additionally, SemiGCL is adversarially optimized with the
entropy loss of unlabeled target nodes to reduce domain divergence.
Experimental results on benchmark datasets demonstrate that SemiGCL outperforms
the state-of-the-art baselines on the SSDA tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07412">Advancing Regular Language Reasoning in Linear Recurrent Neural Networks. (arXiv:2309.07412v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_T/0/1/0/all/0/1">Ting-Han Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chi_T/0/1/0/all/0/1">Ta-Chung Chi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rudnicky_A/0/1/0/all/0/1">Alexander I. Rudnicky</a></p>
<p>In recent studies, linear recurrent neural networks (LRNNs) have achieved
Transformer-level performance in natural language modeling and long-range
modeling while offering rapid parallel training and constant inference costs.
With the resurged interest in LRNNs, we study whether they can learn the hidden
rules in training sequences, such as the grammatical structures of regular
language. We theoretically analyze some existing LRNNs and discover their
limitations on regular language. Motivated by the analysis, we propose a new
LRNN equipped with a block-diagonal and input-dependent transition matrix.
Experiments suggest that the proposed model is the only LRNN that can perform
length extrapolation on regular language tasks such as Sum, Even Pair, and
Modular Arithmetic.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07418">A Fast Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time. (arXiv:2309.07418v1 [cs.DS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yeqi Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1">Zhao Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Weixin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1">Junze Yin</a></p>
<p>Large language models (LLMs) have played a pivotal role in revolutionizing
various facets of our daily existence. Solving attention regression is a
fundamental task in optimizing LLMs. In this work, we focus on giving a
provable guarantee for the one-layer attention network objective function
$L(X,Y) = \sum_{j_0 = 1}^n \sum_{i_0 = 1}^d ( \langle \langle \exp(
\mathsf{A}_{j_0} x ) , {\bf 1}_n \rangle^{-1} \exp( \mathsf{A}_{j_0} x ), A_{3}
Y_{*,i_0} \rangle - b_{j_0,i_0} )^2$. Here $\mathsf{A} \in \mathbb{R}^{n^2
\times d^2}$ is Kronecker product between $A_1 \in \mathbb{R}^{n \times d}$ and
$A_2 \in \mathbb{R}^{n \times d}$. $A_3$ is a matrix in $\mathbb{R}^{n \times
d}$, $\mathsf{A}_{j_0} \in \mathbb{R}^{n \times d^2}$ is the $j_0$-th block of
$\mathsf{A}$. The $X, Y \in \mathbb{R}^{d \times d}$ are variables we want to
learn. $B \in \mathbb{R}^{n \times d}$ and $b_{j_0,i_0} \in \mathbb{R}$ is one
entry at $j_0$-th row and $i_0$-th column of $B$, $Y_{*,i_0} \in \mathbb{R}^d$
is the $i_0$-column vector of $Y$, and $x \in \mathbb{R}^{d^2}$ is the
vectorization of $X$.
</p>
<p>In a multi-layer LLM network, the matrix $B \in \mathbb{R}^{n \times d}$ can
be viewed as the output of a layer, and $A_1= A_2 = A_3 \in \mathbb{R}^{n
\times d}$ can be viewed as the input of a layer. The matrix version of $x$ can
be viewed as $QK^\top$ and $Y$ can be viewed as $V$. We provide an iterative
greedy algorithm to train loss function $L(X,Y)$ up $\epsilon$ that runs in
$\widetilde{O}( ({\cal T}_{\mathrm{mat}}(n,n,d) + {\cal
T}_{\mathrm{mat}}(n,d,d) + d^{2\omega}) \log(1/\epsilon) )$ time. Here ${\cal
T}_{\mathrm{mat}}(a,b,c)$ denotes the time of multiplying $a \times b$ matrix
another $b \times c$ matrix, and $\omega\approx 2.37$ denotes the exponent of
matrix multiplication.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07450">TensorFlow Chaotic Prediction and Blow Up. (arXiv:2309.07450v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Andrecut_M/0/1/0/all/0/1">M. Andrecut</a></p>
<p>Predicting the dynamics of chaotic systems is one of the most challenging
tasks for neural networks, and machine learning in general. Here we aim to
predict the spatiotemporal chaotic dynamics of a high-dimensional non-linear
system. In our attempt we use the TensorFlow library, representing the state of
the art for deep neural networks training and prediction. While our results are
encouraging, and show that the dynamics of the considered system can be
predicted for short time, we also indirectly discovered an unexpected and
undesirable behavior of the TensorFlow library. More specifically, the longer
term prediction of the system's chaotic behavior quickly deteriorates and blows
up due to the nondeterministic behavior of the TensorFlow library. Here we
provide numerical evidence of the short time prediction ability, and of the
longer term predictability blow up.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07452">Is Solving Graph Neural Tangent Kernel Equivalent to Training Graph Neural Network?. (arXiv:2309.07452v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1">Lianke Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1">Zhao Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1">Baocheng Sun</a></p>
<p>A rising trend in theoretical deep learning is to understand why deep
learning works through Neural Tangent Kernel (NTK) [jgh18], a kernel method
that is equivalent to using gradient descent to train a multi-layer
infinitely-wide neural network. NTK is a major step forward in the theoretical
deep learning because it allows researchers to use traditional mathematical
tools to analyze properties of deep neural networks and to explain various
neural network techniques from a theoretical view. A natural extension of NTK
on graph learning is \textit{Graph Neural Tangent Kernel (GNTK)}, and
researchers have already provide GNTK formulation for graph-level regression
and show empirically that this kernel method can achieve similar accuracy as
GNNs on various bioinformatics datasets [dhs+19]. The remaining question now is
whether solving GNTK regression is equivalent to training an infinite-wide
multi-layer GNN using gradient descent. In this paper, we provide three new
theoretical results. First, we formally prove this equivalence for graph-level
regression. Second, we present the first GNTK formulation for node-level
regression. Finally, we prove the equivalence for node-level regression.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07453">SC-MAD: Mixtures of Higher-order Networks for Data Augmentation. (arXiv:2309.07453v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Navarro_M/0/1/0/all/0/1">Madeline Navarro</a>, <a href="http://arxiv.org/find/stat/1/au:+Segarra_S/0/1/0/all/0/1">Santiago Segarra</a></p>
<p>The myriad complex systems with multiway interactions motivate the extension
of graph-based pairwise connections to higher-order relations. In particular,
the simplicial complex has inspired generalizations of graph neural networks
(GNNs) to simplicial complex-based models. Learning on such systems requires
large amounts of data, which can be expensive or impossible to obtain. We
propose data augmentation of simplicial complexes through both linear and
nonlinear mixup mechanisms that return mixtures of existing labeled samples. In
addition to traditional pairwise mixup, we present a convex clustering mixup
approach for a data-driven relationship among several simplicial complexes. We
theoretically demonstrate that the resultant synthetic simplicial complexes
interpolate among existing data with respect to homomorphism densities. Our
method is demonstrated on both synthetic and real-world datasets for simplicial
complex classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07461">Detecting Unknown Attacks in IoT Environments: An Open Set Classifier for Enhanced Network Intrusion Detection. (arXiv:2309.07461v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Farrukh_Y/0/1/0/all/0/1">Yasir Ali Farrukh</a>, <a href="http://arxiv.org/find/cs/1/au:+Wali_S/0/1/0/all/0/1">Syed Wali</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_I/0/1/0/all/0/1">Irfan Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bastian_N/0/1/0/all/0/1">Nathaniel D. Bastian</a></p>
<p>The widespread integration of Internet of Things (IoT) devices across all
facets of life has ushered in an era of interconnectedness, creating new
avenues for cybersecurity challenges and underscoring the need for robust
intrusion detection systems. However, traditional security systems are designed
with a closed-world perspective and often face challenges in dealing with the
ever-evolving threat landscape, where new and unfamiliar attacks are constantly
emerging. In this paper, we introduce a framework aimed at mitigating the open
set recognition (OSR) problem in the realm of Network Intrusion Detection
Systems (NIDS) tailored for IoT environments. Our framework capitalizes on
image-based representations of packet-level data, extracting spatial and
temporal patterns from network traffic. Additionally, we integrate stacking and
sub-clustering techniques, enabling the identification of unknown attacks by
effectively modeling the complex and diverse nature of benign behavior. The
empirical results prominently underscore the framework's efficacy, boasting an
impressive 88\% detection rate for previously unseen attacks when compared
against existing approaches and recent advancements. Future work will perform
extensive experimentation across various openness levels and attack scenarios,
further strengthening the adaptability and performance of our proposed solution
in safeguarding IoT environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07478">Direct Text to Speech Translation System using Acoustic Units. (arXiv:2309.07478v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mingote_V/0/1/0/all/0/1">Victoria Mingote</a>, <a href="http://arxiv.org/find/cs/1/au:+Gimeno_P/0/1/0/all/0/1">Pablo Gimeno</a>, <a href="http://arxiv.org/find/cs/1/au:+Vicente_L/0/1/0/all/0/1">Luis Vicente</a>, <a href="http://arxiv.org/find/cs/1/au:+Khurana_S/0/1/0/all/0/1">Sameer Khurana</a>, <a href="http://arxiv.org/find/cs/1/au:+Laurent_A/0/1/0/all/0/1">Antoine Laurent</a>, <a href="http://arxiv.org/find/cs/1/au:+Duret_J/0/1/0/all/0/1">Jarod Duret</a></p>
<p>This paper proposes a direct text to speech translation system using discrete
acoustic units. This framework employs text in different source languages as
input to generate speech in the target language without the need for text
transcriptions in this language. Motivated by the success of acoustic units in
previous works for direct speech to speech translation systems, we use the same
pipeline to extract the acoustic units using a speech encoder combined with a
clustering algorithm. Once units are obtained, an encoder-decoder architecture
is trained to predict them. Then a vocoder generates speech from units. Our
approach for direct text to speech translation was tested on the new CVSS
corpus with two different text mBART models employed as initialisation. The
systems presented report competitive performance for most of the language pairs
evaluated. Besides, results show a remarkable improvement when initialising our
proposed architecture with a model pre-trained with more languages.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07481">Improved Auto-Encoding using Deterministic Projected Belief Networks. (arXiv:2309.07481v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Baggenstoss_P/0/1/0/all/0/1">Paul M Baggenstoss</a></p>
<p>In this paper, we exploit the unique properties of a deterministic projected
belief network (D-PBN) to take full advantage of trainable compound activation
functions (TCAs). A D-PBN is a type of auto-encoder that operates by "backing
up" through a feed-forward neural network. TCAs are activation functions with
complex monotonic-increasing shapes that change the distribution of the data so
that the linear transformation that follows is more effective. Because a D-PBN
operates by "backing up", the TCAs are inverted in the reconstruction process,
restoring the original distribution of the data, thus taking advantage of a
given TCA in both analysis and reconstruction. In this paper, we show that a
D-PBN auto-encoder with TCAs can significantly out-perform standard
auto-encoders including variational auto-encoders.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07486">Massively-Parallel Heat Map Sorting and Applications To Explainable Clustering. (arXiv:2309.07486v1 [cs.DS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aghamolaei_S/0/1/0/all/0/1">Sepideh Aghamolaei</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghodsi_M/0/1/0/all/0/1">Mohammad Ghodsi</a></p>
<p>Given a set of points labeled with $k$ labels, we introduce the heat map
sorting problem as reordering and merging the points and dimensions while
preserving the clusters (labels). A cluster is preserved if it remains
connected, i.e., if it is not split into several clusters and no two clusters
are merged.
</p>
<p>We prove the problem is NP-hard and we give a fixed-parameter algorithm with
a constant number of rounds in the massively parallel computation model, where
each machine has a sublinear memory and the total memory of the machines is
linear. We give an approximation algorithm for a NP-hard special case of the
problem. We empirically compare our algorithm with k-means and density-based
clustering (DBSCAN) using a dimensionality reduction via locality-sensitive
hashing on several directed and undirected graphs of email and computer
networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07526">Learning Beyond Similarities: Incorporating Dissimilarities between Positive Pairs in Self-Supervised Time Series Learning. (arXiv:2309.07526v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Atienza_A/0/1/0/all/0/1">Adrian Atienza</a>, <a href="http://arxiv.org/find/cs/1/au:+Bardram_J/0/1/0/all/0/1">Jakob Bardram</a>, <a href="http://arxiv.org/find/cs/1/au:+Puthusserypady_S/0/1/0/all/0/1">Sadasivan Puthusserypady</a></p>
<p>By identifying similarities between successive inputs, Self-Supervised
Learning (SSL) methods for time series analysis have demonstrated their
effectiveness in encoding the inherent static characteristics of temporal data.
However, an exclusive emphasis on similarities might result in representations
that overlook the dynamic attributes critical for modeling cardiovascular
diseases within a confined subject cohort. Introducing Distilled Encoding
Beyond Similarities (DEBS), this paper pioneers an SSL approach that transcends
mere similarities by integrating dissimilarities among positive pairs. The
framework is applied to electrocardiogram (ECG) signals, leading to a notable
enhancement of +10\% in the detection accuracy of Atrial Fibrillation (AFib)
across diverse subjects. DEBS underscores the potential of attaining a more
refined representation by encoding the dynamic characteristics of time series
data, tapping into dissimilarities during the optimization process. Broadly,
the strategy delineated in this study holds the promise of unearthing novel
avenues for advancing SSL methodologies tailored to temporal data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07530">Adaptive approximation of monotone functions. (arXiv:2309.07530v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gaillard_P/0/1/0/all/0/1">Pierre Gaillard</a> (Thoth), <a href="http://arxiv.org/find/cs/1/au:+Gerchinovitz_S/0/1/0/all/0/1">S&#xe9;bastien Gerchinovitz</a> (IMT), <a href="http://arxiv.org/find/cs/1/au:+Montbrun_E/0/1/0/all/0/1">&#xc9;tienne de Montbrun</a> (TSE-R)</p>
<p>We study the classical problem of approximating a non-decreasing function $f:
\mathcal{X} \to \mathcal{Y}$ in $L^p(\mu)$ norm by sequentially querying its
values, for known compact real intervals $\mathcal{X}$, $\mathcal{Y}$ and a
known probability measure $\mu$ on $\cX$. For any function~$f$ we characterize
the minimum number of evaluations of $f$ that algorithms need to guarantee an
approximation $\hat{f}$ with an $L^p(\mu)$ error below $\epsilon$ after
stopping. Unlike worst-case results that hold uniformly over all $f$, our
complexity measure is dependent on each specific function $f$. To address this
problem, we introduce GreedyBox, a generalization of an algorithm originally
proposed by Novak (1992) for numerical integration. We prove that GreedyBox
achieves an optimal sample complexity for any function $f$, up to logarithmic
factors. Additionally, we uncover results regarding piecewise-smooth functions.
Perhaps as expected, the $L^p(\mu)$ error of GreedyBox decreases much faster
for piecewise-$C^2$ functions than predicted by the algorithm (without any
knowledge on the smoothness of $f$). A simple modification even achieves
optimal minimax approximation rates for such functions, which we compute
explicitly. In particular, our findings highlight multiple performance gaps
between adaptive and non-adaptive algorithms, smooth and piecewise-smooth
functions, as well as monotone or non-monotone functions. Finally, we provide
numerical experiments to support our theoretical results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07544">VerilogEval: Evaluating Large Language Models for Verilog Code Generation. (arXiv:2309.07544v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Mingjie Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pinckney_N/0/1/0/all/0/1">Nathaniel Pinckney</a>, <a href="http://arxiv.org/find/cs/1/au:+Khailany_B/0/1/0/all/0/1">Brucek Khailany</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1">Haoxing Ren</a></p>
<p>The increasing popularity of large language models (LLMs) has paved the way
for their application in diverse domains. This paper proposes a benchmarking
framework tailored specifically for evaluating LLM performance in the context
of Verilog code generation for hardware design and verification. We present a
comprehensive evaluation dataset consisting of 156 problems from the Verilog
instructional website HDLBits. The evaluation set consists of a diverse set of
Verilog code generation tasks, ranging from simple combinational circuits to
complex finite state machines. The Verilog code completions can be
automatically tested for functional correctness by comparing the transient
simulation outputs of the generated design with a golden solution. We also
demonstrate that the Verilog code generation capability of pretrained language
models could be improved with supervised fine-tuning by bootstrapping with LLM
generated synthetic problem-code pairs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07548">Proximal Bellman mappings for reinforcement learning and their application to robust adaptive filtering. (arXiv:2309.07548v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Akiyama_Y/0/1/0/all/0/1">Yuki Akiyama</a>, <a href="http://arxiv.org/find/eess/1/au:+Slavakis_K/0/1/0/all/0/1">Konstantinos Slavakis</a></p>
<p>This paper aims at the algorithmic/theoretical core of reinforcement learning
(RL) by introducing the novel class of proximal Bellman mappings. These
mappings are defined in reproducing kernel Hilbert spaces (RKHSs), to benefit
from the rich approximation properties and inner product of RKHSs, they are
shown to belong to the powerful Hilbertian family of (firmly) nonexpansive
mappings, regardless of the values of their discount factors, and possess ample
degrees of design freedom to even reproduce attributes of the classical Bellman
mappings and to pave the way for novel RL designs. An approximate
policy-iteration scheme is built on the proposed class of mappings to solve the
problem of selecting online, at every time instance, the "optimal" exponent $p$
in a $p$-norm loss to combat outliers in linear adaptive filtering, without
training data and any knowledge on the statistical properties of the outliers.
Numerical tests on synthetic data showcase the superior performance of the
proposed framework over several non-RL and kernel-based RL schemes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07550">Naturalistic Robot Arm Trajectory Generation via Representation Learning. (arXiv:2309.07550v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jayjun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Spiers_A/0/1/0/all/0/1">Adam J. Spiers</a></p>
<p>The integration of manipulator robots in household environments suggests a
need for more predictable and human-like robot motion. This holds especially
true for wheelchair-mounted assistive robots that can support the independence
of people with paralysis. One method of generating naturalistic motion
trajectories is via the imitation of human demonstrators. This paper explores a
self-supervised imitation learning method using an autoregressive
spatio-temporal graph neural network for an assistive drinking task. We address
learning from diverse human motion trajectory data that were captured via
wearable IMU sensors on a human arm as the action-free task demonstrations.
Observed arm motion data from several participants is used to generate natural
and functional drinking motion trajectories for a UR5e robot arm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07578">Equivariant Data Augmentation for Generalization in Offline Reinforcement Learning. (arXiv:2309.07578v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pinneri_C/0/1/0/all/0/1">Cristina Pinneri</a>, <a href="http://arxiv.org/find/cs/1/au:+Bechtle_S/0/1/0/all/0/1">Sarah Bechtle</a>, <a href="http://arxiv.org/find/cs/1/au:+Wulfmeier_M/0/1/0/all/0/1">Markus Wulfmeier</a>, <a href="http://arxiv.org/find/cs/1/au:+Byravan_A/0/1/0/all/0/1">Arunkumar Byravan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jingwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Whitney_W/0/1/0/all/0/1">William F. Whitney</a>, <a href="http://arxiv.org/find/cs/1/au:+Riedmiller_M/0/1/0/all/0/1">Martin Riedmiller</a></p>
<p>We present a novel approach to address the challenge of generalization in
offline reinforcement learning (RL), where the agent learns from a fixed
dataset without any additional interaction with the environment. Specifically,
we aim to improve the agent's ability to generalize to out-of-distribution
goals. To achieve this, we propose to learn a dynamics model and check if it is
equivariant with respect to a fixed type of transformation, namely translations
in the state space. We then use an entropy regularizer to increase the
equivariant set and augment the dataset with the resulting transformed samples.
Finally, we learn a new policy offline based on the augmented dataset, with an
off-the-shelf offline RL algorithm. Our experimental results demonstrate that
our approach can greatly improve the test performance of the policy on the
considered environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07579">Structure-Preserving Transformers for Sequences of SPD Matrices. (arXiv:2309.07579v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Seraphim_M/0/1/0/all/0/1">Mathieu Seraphim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lechervy_A/0/1/0/all/0/1">Alexis Lechervy</a>, <a href="http://arxiv.org/find/cs/1/au:+Yger_F/0/1/0/all/0/1">Florian Yger</a>, <a href="http://arxiv.org/find/cs/1/au:+Brun_L/0/1/0/all/0/1">Luc Brun</a>, <a href="http://arxiv.org/find/cs/1/au:+Etard_O/0/1/0/all/0/1">Olivier Etard</a></p>
<p>In recent years, Transformer-based auto-attention mechanisms have been
successfully applied to the analysis of a variety of context-reliant data
types, from texts to images and beyond, including data from non-Euclidean
geometries. In this paper, we present such a mechanism, designed to classify
sequences of Symmetric Positive Definite matrices while preserving their
Riemannian geometry throughout the analysis. We apply our method to automatic
sleep staging on timeseries of EEG-derived covariance matrices from a standard
dataset, obtaining high levels of stage-wise performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07593">Statistically Valid Variable Importance Assessment through Conditional Permutations. (arXiv:2309.07593v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chamma_A/0/1/0/all/0/1">Ahmad Chamma</a> (1 and 2 and 3), <a href="http://arxiv.org/find/cs/1/au:+Engemann_D/0/1/0/all/0/1">Denis A. Engemann</a> (4), <a href="http://arxiv.org/find/cs/1/au:+Thirion_B/0/1/0/all/0/1">Bertrand Thirion</a> (1 and 2 and 3) ((1) Inria, (2) Universite Paris Saclay, (3) CEA, (4) Roche Pharma Research and Early Development, Neuroscience and Rare Diseases, Roche Innovation Center Basel, F. Hoffmann-La Roche Ltd., Basel, Switzerland)</p>
<p>Variable importance assessment has become a crucial step in machine-learning
applications when using complex learners, such as deep neural networks, on
large-scale data. Removal-based importance assessment is currently the
reference approach, particularly when statistical guarantees are sought to
justify variable inclusion. It is often implemented with variable permutation
schemes. On the flip side, these approaches risk misidentifying unimportant
variables as important in the presence of correlations among covariates. Here
we develop a systematic approach for studying Conditional Permutation
Importance (CPI) that is model agnostic and computationally lean, as well as
reusable benchmarks of state-of-the-art variable importance estimators. We show
theoretically and empirically that $\textit{CPI}$ overcomes the limitations of
standard permutation importance by providing accurate type-I error control.
When used with a deep neural network, $\textit{CPI}$ consistently showed top
accuracy across benchmarks. An empirical benchmark on real-world data analysis
in a large-scale medical dataset showed that $\textit{CPI}$ provides a more
parsimonious selection of statistically significant variables. Our results
suggest that $\textit{CPI}$ can be readily used as drop-in replacement for
permutation-based methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07601">Detecting Misinformation with LLM-Predicted Credibility Signals and Weak Supervision. (arXiv:2309.07601v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Leite_J/0/1/0/all/0/1">Jo&#xe3;o A. Leite</a>, <a href="http://arxiv.org/find/cs/1/au:+Razuvayevskaya_O/0/1/0/all/0/1">Olesya Razuvayevskaya</a>, <a href="http://arxiv.org/find/cs/1/au:+Bontcheva_K/0/1/0/all/0/1">Kalina Bontcheva</a>, <a href="http://arxiv.org/find/cs/1/au:+Scarton_C/0/1/0/all/0/1">Carolina Scarton</a></p>
<p>Credibility signals represent a wide range of heuristics that are typically
used by journalists and fact-checkers to assess the veracity of online content.
Automating the task of credibility signal extraction, however, is very
challenging as it requires high-accuracy signal-specific extractors to be
trained, while there are currently no sufficiently large datasets annotated
with all credibility signals. This paper investigates whether large language
models (LLMs) can be prompted effectively with a set of 18 credibility signals
to produce weak labels for each signal. We then aggregate these potentially
noisy labels using weak supervision in order to predict content veracity. We
demonstrate that our approach, which combines zero-shot LLM credibility signal
labeling and weak supervision, outperforms state-of-the-art classifiers on two
misinformation datasets without using any ground-truth labels for training. We
also analyse the contribution of the individual credibility signals towards
predicting content veracity, which provides new valuable insights into their
role in misinformation detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07602">Turning Dross Into Gold Loss: is BERT4Rec really better than SASRec?. (arXiv:2309.07602v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Klenitskiy_A/0/1/0/all/0/1">Anton Klenitskiy</a>, <a href="http://arxiv.org/find/cs/1/au:+Vasilev_A/0/1/0/all/0/1">Alexey Vasilev</a></p>
<p>Recently sequential recommendations and next-item prediction task has become
increasingly popular in the field of recommender systems. Currently, two
state-of-the-art baselines are Transformer-based models SASRec and BERT4Rec.
Over the past few years, there have been quite a few publications comparing
these two algorithms and proposing new state-of-the-art models. In most of the
publications, BERT4Rec achieves better performance than SASRec. But BERT4Rec
uses cross-entropy over softmax for all items, while SASRec uses negative
sampling and calculates binary cross-entropy loss for one positive and one
negative item. In our work, we show that if both models are trained with the
same loss, which is used by BERT4Rec, then SASRec will significantly outperform
BERT4Rec both in terms of quality and training speed. In addition, we show that
SASRec could be effectively trained with negative sampling and still outperform
BERT4Rec, but the number of negative examples should be much larger than one.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07609">Learning Quasi-Static 3D Models of Markerless Deformable Linear Objects for Bimanual Robotic Manipulation. (arXiv:2309.07609v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kicki_P/0/1/0/all/0/1">Piotr Kicki</a>, <a href="http://arxiv.org/find/cs/1/au:+Bidzinski_M/0/1/0/all/0/1">Micha&#x142; Bidzi&#x144;ski</a>, <a href="http://arxiv.org/find/cs/1/au:+Walas_K/0/1/0/all/0/1">Krzysztof Walas</a></p>
<p>The robotic manipulation of Deformable Linear Objects (DLOs) is a vital and
challenging task that is important in many practical applications. Classical
model-based approaches to this problem require an accurate model to capture how
robot motions affect the deformation of the DLO. Nowadays, data-driven models
offer the best tradeoff between quality and computation time. This paper
analyzes several learning-based 3D models of the DLO and proposes a new one
based on the Transformer architecture that achieves superior accuracy, even on
the DLOs of different lengths, thanks to the proposed scaling method. Moreover,
we introduce a data augmentation technique, which improves the prediction
performance of almost all considered DLO data-driven models. Thanks to this
technique, even a simple Multilayer Perceptron (MLP) achieves close to
state-of-the-art performance while being significantly faster to evaluate. In
the experiments, we compare the performance of the learning-based 3D models of
the DLO on several challenging datasets quantitatively and demonstrate their
applicability in the task of shaping a DLO.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07610">Feature Engineering in Learning-to-Rank for Community Question Answering Task. (arXiv:2309.07610v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sajid_N/0/1/0/all/0/1">Nafis Sajid</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1">Md Rashidul Hasan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ibrahim_M/0/1/0/all/0/1">Muhammad Ibrahim</a></p>
<p>Community question answering (CQA) forums are Internet-based platforms where
users ask questions about a topic and other expert users try to provide
solutions. Many CQA forums such as Quora, Stackoverflow, Yahoo!Answer,
StackExchange exist with a lot of user-generated data. These data are leveraged
in automated CQA ranking systems where similar questions (and answers) are
presented in response to the query of the user. In this work, we empirically
investigate a few aspects of this domain. Firstly, in addition to traditional
features like TF-IDF, BM25 etc., we introduce a BERT-based feature that
captures the semantic similarity between the question and answer. Secondly,
most of the existing research works have focused on features extracted only
from the question part; features extracted from answers have not been explored
extensively. We combine both types of features in a linear fashion. Thirdly,
using our proposed concepts, we conduct an empirical investigation with
different rank-learning algorithms, some of which have not been used so far in
CQA domain. On three standard CQA datasets, our proposed framework achieves
state-of-the-art performance. We also analyze importance of the features we use
in our investigation. This work is expected to guide the practitioners to
select a better set of features for the CQA retrieval task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07663">Dataset Size Dependence of Rate-Distortion Curve and Threshold of Posterior Collapse in Linear VAE. (arXiv:2309.07663v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Ichikawa_Y/0/1/0/all/0/1">Yuma Ichikawa</a>, <a href="http://arxiv.org/find/stat/1/au:+Hukushima_K/0/1/0/all/0/1">Koji Hukushima</a></p>
<p>In the Variational Autoencoder (VAE), the variational posterior often aligns
closely with the prior, which is known as posterior collapse and hinders the
quality of representation learning. To mitigate this problem, an adjustable
hyperparameter beta has been introduced in the VAE. This paper presents a
closed-form expression to assess the relationship between the beta in VAE, the
dataset size, the posterior collapse, and the rate-distortion curve by
analyzing a minimal VAE in a high-dimensional limit. These results clarify that
a long plateau in the generalization error emerges with a relatively larger
beta. As the beta increases, the length of the plateau extends and then becomes
infinite beyond a certain beta threshold. This implies that the choice of beta,
unlike the usual regularization parameters, can induce posterior collapse
regardless of the dataset size. Thus, beta is a risky parameter that requires
careful tuning. Furthermore, considering the dataset-size dependence on the
rate-distortion curve, a relatively large dataset is required to obtain a
rate-distortion curve with high rates. Extensive numerical experiments support
our analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07666">Multi-Source Domain Adaptation meets Dataset Distillation through Dataset Dictionary Learning. (arXiv:2309.07666v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Montesuma_E/0/1/0/all/0/1">Eduardo Fernandes Montesuma</a>, <a href="http://arxiv.org/find/cs/1/au:+Mboula_F/0/1/0/all/0/1">Fred Ngol&#xe8; Mboula</a>, <a href="http://arxiv.org/find/cs/1/au:+Souloumiac_A/0/1/0/all/0/1">Antoine Souloumiac</a></p>
<p>In this paper, we consider the intersection of two problems in machine
learning: Multi-Source Domain Adaptation (MSDA) and Dataset Distillation (DD).
On the one hand, the first considers adapting multiple heterogeneous labeled
source domains to an unlabeled target domain. On the other hand, the second
attacks the problem of synthesizing a small summary containing all the
information about the datasets. We thus consider a new problem called MSDA-DD.
To solve it, we adapt previous works in the MSDA literature, such as
Wasserstein Barycenter Transport and Dataset Dictionary Learning, as well as DD
method Distribution Matching. We thoroughly experiment with this novel problem
on four benchmarks (Caltech-Office 10, Tennessee-Eastman Process, Continuous
Stirred Tank Reactor, and Case Western Reserve University), where we show that,
even with as little as 1 sample per class, one achieves state-of-the-art
adaptation performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07670">Federated Dataset Dictionary Learning for Multi-Source Domain Adaptation. (arXiv:2309.07670v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Castellon_F/0/1/0/all/0/1">Fabiola Espinosa Castellon</a>, <a href="http://arxiv.org/find/cs/1/au:+Montesuma_E/0/1/0/all/0/1">Eduardo Fernandes Montesuma</a>, <a href="http://arxiv.org/find/cs/1/au:+Mboula_F/0/1/0/all/0/1">Fred Ngol&#xe8; Mboula</a>, <a href="http://arxiv.org/find/cs/1/au:+Mayoue_A/0/1/0/all/0/1">Aur&#xe9;lien Mayoue</a>, <a href="http://arxiv.org/find/cs/1/au:+Souloumiac_A/0/1/0/all/0/1">Antoine Souloumiac</a>, <a href="http://arxiv.org/find/cs/1/au:+Gouy_Pallier_C/0/1/0/all/0/1">C&#xe9;dric Gouy-Pallier</a></p>
<p>In this article, we propose an approach for federated domain adaptation, a
setting where distributional shift exists among clients and some have unlabeled
data. The proposed framework, FedDaDiL, tackles the resulting challenge through
dictionary learning of empirical distributions. In our setting, clients'
distributions represent particular domains, and FedDaDiL collectively trains a
federated dictionary of empirical distributions. In particular, we build upon
the Dataset Dictionary Learning framework by designing collaborative
communication protocols and aggregation operations. The chosen protocols keep
clients' data private, thus enhancing overall privacy compared to its
centralized counterpart. We empirically demonstrate that our approach
successfully generates labeled data on the target domain with extensive
experiments on (i) Caltech-Office, (ii) TEP, and (iii) CWRU benchmarks.
Furthermore, we compare our method to its centralized counterpart and other
benchmarks in federated domain adaptation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07672">Physics-constrained robust learning of open-form PDEs from limited and noisy data. (arXiv:2309.07672v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1">Mengge Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1">Longfeng Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Lou_S/0/1/0/all/0/1">Siyu Lou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chenc_Y/0/1/0/all/0/1">Yuntian Chenc</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dongxiao Zhang</a></p>
<p>Unveiling the underlying governing equations of nonlinear dynamic systems
remains a significant challenge, especially when encountering noisy
observations and no prior knowledge available. This study proposes R-DISCOVER,
a framework designed to robustly uncover open-form partial differential
equations (PDEs) from limited and noisy data. The framework operates through
two alternating update processes: discovering and embedding. The discovering
phase employs symbolic representation and a reinforcement learning (RL)-guided
hybrid PDE generator to efficiently produce diverse open-form PDEs with tree
structures. A neural network-based predictive model fits the system response
and serves as the reward evaluator for the generated PDEs. PDEs with superior
fits are utilized to iteratively optimize the generator via the RL method and
the best-performing PDE is selected by a parameter-free stability metric. The
embedding phase integrates the initially identified PDE from the discovering
process as a physical constraint into the predictive model for robust training.
The traversal of PDE trees automates the construction of the computational
graph and the embedding process without human intervention. Numerical
experiments demonstrate our framework's capability to uncover governing
equations from nonlinear dynamic systems with limited and highly noisy data and
outperform other physics-informed neural network-based discovery methods. This
work opens new potential for exploring real-world systems with limited
understanding.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07675">Goal Space Abstraction in Hierarchical Reinforcement Learning via Set-Based Reachability Analysis. (arXiv:2309.07675v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zadem_M/0/1/0/all/0/1">Mehdi Zadem</a>, <a href="http://arxiv.org/find/cs/1/au:+Mover_S/0/1/0/all/0/1">Sergio Mover</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_S/0/1/0/all/0/1">Sao Mai Nguyen</a></p>
<p>Open-ended learning benefits immensely from the use of symbolic methods for
goal representation as they offer ways to structure knowledge for efficient and
transferable learning. However, the existing Hierarchical Reinforcement
Learning (HRL) approaches relying on symbolic reasoning are often limited as
they require a manual goal representation. The challenge in autonomously
discovering a symbolic goal representation is that it must preserve critical
information, such as the environment dynamics. In this paper, we propose a
developmental mechanism for goal discovery via an emergent representation that
abstracts (i.e., groups together) sets of environment states that have similar
roles in the task. We introduce a Feudal HRL algorithm that concurrently learns
both the goal representation and a hierarchical policy. The algorithm uses
symbolic reachability analysis for neural networks to approximate the
transition relation among sets of states and to refine the goal representation.
We evaluate our approach on complex navigation tasks, showing the learned
representation is interpretable, transferrable and results in data efficient
learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07679">Benchmarking machine learning models for quantum state classification. (arXiv:2309.07679v1 [quant-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Pedicillo_E/0/1/0/all/0/1">Edoardo Pedicillo</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Pasquale_A/0/1/0/all/0/1">Andrea Pasquale</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Carrazza_S/0/1/0/all/0/1">Stefano Carrazza</a></p>
<p>Quantum computing is a growing field where the information is processed by
two-levels quantum states known as qubits. Current physical realizations of
qubits require a careful calibration, composed by different experiments, due to
noise and decoherence phenomena. Among the different characterization
experiments, a crucial step is to develop a model to classify the measured
state by discriminating the ground state from the excited state. In this
proceedings we benchmark multiple classification techniques applied to real
quantum devices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07684">deepFDEnet: A Novel Neural Network Architecture for Solving Fractional Differential Equations. (arXiv:2309.07684v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Firoozsalari_A/0/1/0/all/0/1">Ali Nosrati Firoozsalari</a>, <a href="http://arxiv.org/find/cs/1/au:+Mazraeh_H/0/1/0/all/0/1">Hassan Dana Mazraeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Aghaei_A/0/1/0/all/0/1">Alireza Afzal Aghaei</a>, <a href="http://arxiv.org/find/cs/1/au:+Parand_K/0/1/0/all/0/1">Kourosh Parand</a></p>
<p>The primary goal of this research is to propose a novel architecture for a
deep neural network that can solve fractional differential equations
accurately. A Gaussian integration rule and a $L_1$ discretization technique
are used in the proposed design. In each equation, a deep neural network is
used to approximate the unknown function. Three forms of fractional
differential equations have been examined to highlight the method's
versatility: a fractional ordinary differential equation, a fractional order
integrodifferential equation, and a fractional order partial differential
equation. The results show that the proposed architecture solves different
forms of fractional differential equations with excellent precision.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07690">A DenseNet-based method for decoding auditory spatial attention with EEG. (arXiv:2309.07690v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1">Xiran Xu</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_B/0/1/0/all/0/1">Bo Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Yan_Y/0/1/0/all/0/1">Yujie Yan</a>, <a href="http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1">Xihong Wu</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1">Jing Chen</a></p>
<p>Auditory spatial attention detection (ASAD) aims to decode the attended
spatial location with EEG in a multiple-speaker setting. ASAD methods are
inspired by the brain lateralization of cortical neural responses during the
processing of auditory spatial attention, and show promising performance for
the task of auditory attention decoding (AAD) with neural recordings. In the
previous ASAD methods, the spatial distribution of EEG electrodes is not fully
exploited, which may limit the performance of these methods. In the present
work, by transforming the original EEG channels into a two-dimensional (2D)
spatial topological map, the EEG data is transformed into a three-dimensional
(3D) arrangement containing spatial-temporal information. And then a 3D deep
convolutional neural network (DenseNet-3D) is used to extract temporal and
spatial features of the neural representation for the attended locations. The
results show that the proposed method achieves higher decoding accuracy than
the state-of-the-art (SOTA) method (94.4% compared to XANet's 90.6%) with
1-second decision window for the widely used KULeuven (KUL) dataset, and the
code to implement our work is available on Github:
</p>
<p>https://github.com/xuxiran/ASAD_DenseNet
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07694">Tree of Uncertain Thoughts Reasoning for Large Language Models. (arXiv:2309.07694v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1">Shentong Mo</a>, <a href="http://arxiv.org/find/cs/1/au:+Xin_M/0/1/0/all/0/1">Miao Xin</a></p>
<p>While the recently introduced Tree of Thoughts (ToT) has heralded
advancements in allowing Large Language Models (LLMs) to reason through
foresight and backtracking for global decision-making, it has overlooked the
inherent local uncertainties in intermediate decision points or "thoughts".
These local uncertainties, intrinsic to LLMs given their potential for diverse
responses, remain a significant concern in the reasoning process. Addressing
this pivotal gap, we introduce the Tree of Uncertain Thoughts (TouT) - a
reasoning framework tailored for LLMs. Our TouT effectively leverages Monte
Carlo Dropout to quantify uncertainty scores associated with LLMs' diverse
local responses at these intermediate steps. By marrying this local uncertainty
quantification with global search algorithms, TouT enhances the model's
precision in response generation. We substantiate our approach with rigorous
experiments on two demanding planning tasks: Game of 24 and Mini Crosswords.
The empirical evidence underscores TouT's superiority over both ToT and
chain-of-thought prompting methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07703">Causal Entropy and Information Gain for Measuring Causal Control. (arXiv:2309.07703v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Simoes_F/0/1/0/all/0/1">Francisco Nunes Ferreira Quialheiro Simoes</a>, <a href="http://arxiv.org/find/cs/1/au:+Dastani_M/0/1/0/all/0/1">Mehdi Dastani</a>, <a href="http://arxiv.org/find/cs/1/au:+Ommen_T/0/1/0/all/0/1">Thijs van Ommen</a></p>
<p>Artificial intelligence models and methods commonly lack causal
interpretability. Despite the advancements in interpretable machine learning
(IML) methods, they frequently assign importance to features which lack causal
influence on the outcome variable. Selecting causally relevant features among
those identified as relevant by these methods, or even before model training,
would offer a solution. Feature selection methods utilizing information
theoretical quantities have been successful in identifying statistically
relevant features. However, the information theoretical quantities they are
based on do not incorporate causality, rendering them unsuitable for such
scenarios. To address this challenge, this article proposes information
theoretical quantities that incorporate the causal structure of the system,
which can be used to evaluate causal importance of features for some given
outcome variable. Specifically, we introduce causal versions of entropy and
mutual information, termed causal entropy and causal information gain, which
are designed to assess how much control a feature provides over the outcome
variable. These newly defined quantities capture changes in the entropy of a
variable resulting from interventions on other variables. Fundamental results
connecting these quantities to the existence of causal effects are derived. The
use of causal information gain in feature selection is demonstrated,
highlighting its superiority over standard mutual information in revealing
which features provide control over a chosen outcome variable. Our
investigation paves the way for the development of methods with improved
interpretability in domains involving causation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07708">Market-GAN: Adding Control to Financial Market Data Generation with Semantic Context. (arXiv:2309.07708v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xia_H/0/1/0/all/0/1">Haochong Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1">Shuo Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinrun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1">Bo An</a></p>
<p>Financial simulators play an important role in enhancing forecasting
accuracy, managing risks, and fostering strategic financial decision-making.
Despite the development of financial market simulation methodologies, existing
frameworks often struggle with adapting to specialized simulation context. We
pinpoint the challenges as i) current financial datasets do not contain context
labels; ii) current techniques are not designed to generate financial data with
context as control, which demands greater precision compared to other
modalities; iii) the inherent difficulties in generating context-aligned,
high-fidelity data given the non-stationary, noisy nature of financial data. To
address these challenges, our contributions are: i) we proposed the Contextual
Market Dataset with market dynamics, stock ticker, and history state as
context, leveraging a market dynamics modeling method that combines linear
regression and Dynamic Time Warping clustering to extract market dynamics; ii)
we present Market-GAN, a novel architecture incorporating a Generative
Adversarial Networks (GAN) for the controllable generation with context, an
autoencoder for learning low-dimension features, and supervisors for knowledge
transfer; iii) we introduce a two-stage training scheme to ensure that
Market-GAN captures the intrinsic market distribution with multiple objectives.
In the pertaining stage, with the use of the autoencoder and supervisors, we
prepare the generator with a better initialization for the adversarial training
stage. We propose a set of holistic evaluation metrics that consider alignment,
fidelity, data usability on downstream tasks, and market facts. We evaluate
Market-GAN with the Dow Jones Industrial Average data from 2000 to 2023 and
showcase superior performance in comparison to 4 state-of-the-art time-series
generative models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07716">Understanding Vector-Valued Neural Networks and Their Relationship with Real and Hypercomplex-Valued Neural Networks. (arXiv:2309.07716v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Valle_M/0/1/0/all/0/1">Marcos Eduardo Valle</a></p>
<p>Despite the many successful applications of deep learning models for
multidimensional signal and image processing, most traditional neural networks
process data represented by (multidimensional) arrays of real numbers. The
intercorrelation between feature channels is usually expected to be learned
from the training data, requiring numerous parameters and careful training. In
contrast, vector-valued neural networks are conceived to process arrays of
vectors and naturally consider the intercorrelation between feature channels.
Consequently, they usually have fewer parameters and often undergo more robust
training than traditional neural networks. This paper aims to present a broad
framework for vector-valued neural networks, referred to as V-nets. In this
context, hypercomplex-valued neural networks are regarded as vector-valued
models with additional algebraic properties. Furthermore, this paper explains
the relationship between vector-valued and traditional neural networks.
Precisely, a vector-valued neural network can be obtained by placing
restrictions on a real-valued model to consider the intercorrelation between
feature channels. Finally, we show how V-nets, including hypercomplex-valued
neural networks, can be implemented in current deep-learning libraries as
real-valued networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07742">Interpretability is in the Mind of the Beholder: A Causal Framework for Human-interpretable Representation Learning. (arXiv:2309.07742v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Marconato_E/0/1/0/all/0/1">Emanuele Marconato</a>, <a href="http://arxiv.org/find/cs/1/au:+Passerini_A/0/1/0/all/0/1">Andrea Passerini</a>, <a href="http://arxiv.org/find/cs/1/au:+Teso_S/0/1/0/all/0/1">Stefano Teso</a></p>
<p>Focus in Explainable AI is shifting from explanations defined in terms of
low-level elements, such as input features, to explanations encoded in terms of
interpretable concepts learned from data. How to reliably acquire such concepts
is, however, still fundamentally unclear. An agreed-upon notion of concept
interpretability is missing, with the result that concepts used by both
post-hoc explainers and concept-based neural networks are acquired through a
variety of mutually incompatible strategies. Critically, most of these neglect
the human side of the problem: a representation is understandable only insofar
as it can be understood by the human at the receiving end. The key challenge in
Human-interpretable Representation Learning (HRL) is how to model and
operationalize this human element. In this work, we propose a mathematical
framework for acquiring interpretable representations suitable for both
post-hoc explainers and concept-based neural networks. Our formalization of HRL
builds on recent advances in causal representation learning and explicitly
models a human stakeholder as an external observer. This allows us to derive a
principled notion of alignment between the machine representation and the
vocabulary of concepts understood by the human. In doing so, we link alignment
and interpretability through a simple and intuitive name transfer game, and
clarify the relationship between alignment and a well-known property of
representations, namely disentanglment. We also show that alignment is linked
to the issue of undesirable correlations among concepts, also known as concept
leakage, and to content-style separation, all through a general
information-theoretic reformulation of these properties. Our conceptualization
aims to bridge the gap between the human and algorithmic sides of
interpretability and establish a stepping stone for new research on
human-interpretable representations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07760">PRE: Vision-Language Prompt Learning with Reparameterization Encoder. (arXiv:2309.07760v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Minh_A/0/1/0/all/0/1">Anh Pham Thi Minh</a></p>
<p>Large pre-trained vision-language models such as CLIP have demonstrated great
potential in zero-shot transferability to downstream tasks. However, to attain
optimal performance, the manual selection of prompts is necessary to improve
alignment between the downstream image distribution and the textual class
descriptions. This manual prompt engineering is the major challenge for
deploying such models in practice since it requires domain expertise and is
extremely time-consuming. To avoid non-trivial prompt engineering, recent work
Context Optimization (CoOp) introduced the concept of prompt learning to the
vision domain using learnable textual tokens. While CoOp can achieve
substantial improvements over manual prompts, its learned context is worse
generalizable to wider unseen classes within the same dataset. In this work, we
present Prompt Learning with Reparameterization Encoder (PRE) - a simple and
efficient method that enhances the generalization ability of the learnable
prompt to unseen classes while maintaining the capacity to learn Base classes.
Instead of directly optimizing the prompts, PRE employs a prompt encoder to
reparameterize the input prompt embeddings, enhancing the exploration of
task-specific knowledge from few-shot samples. Experiments and extensive
ablation studies on 8 benchmarks demonstrate that our approach is an efficient
method for prompt learning. Specifically, PRE achieves a notable enhancement of
5.60% in average accuracy on New classes and 3% in Harmonic mean compared to
CoOp in the 16-shot setting, all achieved within a good training time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07770">Variational Quantum Linear Solver enhanced Quantum Support Vector Machine. (arXiv:2309.07770v1 [quant-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Yi_J/0/1/0/all/0/1">Jianming Yi</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Suresh_K/0/1/0/all/0/1">Kalyani Suresh</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Moghiseh_A/0/1/0/all/0/1">Ali Moghiseh</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Wehn_N/0/1/0/all/0/1">Norbert Wehn</a></p>
<p>Quantum Support Vector Machines (QSVM) play a vital role in using quantum
resources for supervised machine learning tasks, such as classification.
However, current methods are strongly limited in terms of scalability on Noisy
Intermediate Scale Quantum (NISQ) devices. In this work, we propose a novel
approach called the Variational Quantum Linear Solver (VQLS) enhanced QSVM.
This is built upon our idea of utilizing the variational quantum linear solver
to solve system of linear equations of a least squares-SVM on a NISQ device.
The implementation of our approach is evaluated by an extensive series of
numerical experiments with the Iris dataset, which consists of three distinct
iris plant species. Based on this, we explore the practicality and
effectiveness of our algorithm by constructing a classifier capable of
classification in a feature space ranging from one to seven dimensions.
Furthermore, by strategically exploiting both classical and quantum computing
for various subroutines of our algorithm, we effectively mitigate practical
challenges associated with the implementation. These include significant
improvement in the trainability of the variational ansatz and notable
reductions in run-time for cost calculations. Based on the numerical
experiments, our approach exhibits the capability of identifying a separating
hyperplane in an 8-dimensional feature space. Moreover, it consistently
demonstrated strong performance across various instances with the same dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07778">Virchow: A Million-Slide Digital Pathology Foundation Model. (arXiv:2309.07778v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Vorontsov_E/0/1/0/all/0/1">Eugene Vorontsov</a>, <a href="http://arxiv.org/find/eess/1/au:+Bozkurt_A/0/1/0/all/0/1">Alican Bozkurt</a>, <a href="http://arxiv.org/find/eess/1/au:+Casson_A/0/1/0/all/0/1">Adam Casson</a>, <a href="http://arxiv.org/find/eess/1/au:+Shaikovski_G/0/1/0/all/0/1">George Shaikovski</a>, <a href="http://arxiv.org/find/eess/1/au:+Zelechowski_M/0/1/0/all/0/1">Michal Zelechowski</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1">Siqi Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Mathieu_P/0/1/0/all/0/1">Philippe Mathieu</a>, <a href="http://arxiv.org/find/eess/1/au:+Eck_A/0/1/0/all/0/1">Alexander van Eck</a>, <a href="http://arxiv.org/find/eess/1/au:+Lee_D/0/1/0/all/0/1">Donghun Lee</a>, <a href="http://arxiv.org/find/eess/1/au:+Viret_J/0/1/0/all/0/1">Julian Viret</a>, <a href="http://arxiv.org/find/eess/1/au:+Robert_E/0/1/0/all/0/1">Eric Robert</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1">Yi Kan Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Kun_J/0/1/0/all/0/1">Jeremy D. Kun</a>, <a href="http://arxiv.org/find/eess/1/au:+Le_M/0/1/0/all/0/1">Matthew C. H. Le</a>, <a href="http://arxiv.org/find/eess/1/au:+Bernhard_J/0/1/0/all/0/1">Jan Bernhard</a>, <a href="http://arxiv.org/find/eess/1/au:+Godrich_R/0/1/0/all/0/1">Ran A. Godrich</a>, <a href="http://arxiv.org/find/eess/1/au:+Oakley_G/0/1/0/all/0/1">Gerard Oakley</a>, <a href="http://arxiv.org/find/eess/1/au:+Millar_E/0/1/0/all/0/1">Ewan Millar</a>, <a href="http://arxiv.org/find/eess/1/au:+Hanna_M/0/1/0/all/0/1">Matthew Hanna</a>, <a href="http://arxiv.org/find/eess/1/au:+Retamero_J/0/1/0/all/0/1">Juan Retamero</a>, <a href="http://arxiv.org/find/eess/1/au:+Moye_W/0/1/0/all/0/1">William A. Moye</a>, <a href="http://arxiv.org/find/eess/1/au:+Yousfi_R/0/1/0/all/0/1">Razik Yousfi</a>, <a href="http://arxiv.org/find/eess/1/au:+Kanan_C/0/1/0/all/0/1">Christopher Kanan</a>, <a href="http://arxiv.org/find/eess/1/au:+Klimstra_D/0/1/0/all/0/1">David Klimstra</a>, <a href="http://arxiv.org/find/eess/1/au:+Rothrock_B/0/1/0/all/0/1">Brandon Rothrock</a>, <a href="http://arxiv.org/find/eess/1/au:+Fuchs_T/0/1/0/all/0/1">Thomas J. Fuchs</a></p>
<p>Computational pathology uses artificial intelligence to enable precision
medicine and decision support systems through the analysis of whole slide
images. It has the potential to revolutionize the diagnosis and treatment of
cancer. However, a major challenge to this objective is that for many specific
computational pathology tasks the amount of data is inadequate for development.
To address this challenge, we created Virchow, a 632 million parameter deep
neural network foundation model for computational pathology. Using
self-supervised learning, Virchow is trained on 1.5 million hematoxylin and
eosin stained whole slide images from diverse tissue groups, which is orders of
magnitude more data than previous works. When evaluated on downstream tasks
including tile-level pan-cancer detection and subtyping and slide-level
biomarker prediction, Virchow outperforms state-of-the-art systems both on
internal datasets drawn from the same population as the pretraining data as
well as external public datasets. Virchow achieves 93% balanced accuracy for
pancancer tile classification, and AUCs of 0.983 for colon microsatellite
instability status prediction and 0.967 for breast CDH1 status prediction. The
gains in performance highlight the importance of pretraining on massive
pathology image datasets, suggesting pretraining on even larger datasets could
continue improving performance for many high-impact applications where limited
amounts of training data are available, such as drug outcome prediction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07794">Improving Multimodal Classification of Social Media Posts by Leveraging Image-Text Auxiliary tasks. (arXiv:2309.07794v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Villegas_D/0/1/0/all/0/1">Danae S&#xe1;nchez Villegas</a>, <a href="http://arxiv.org/find/cs/1/au:+Preotiuc_Pietro_D/0/1/0/all/0/1">Daniel Preo&#x163;iuc-Pietro</a>, <a href="http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1">Nikolaos Aletras</a></p>
<p>Effectively leveraging multimodal information from social media posts is
essential to various downstream tasks such as sentiment analysis, sarcasm
detection and hate speech classification. However, combining text and image
information is challenging because of the idiosyncratic cross-modal semantics
with hidden or complementary information present in matching image-text pairs.
In this work, we aim to directly model this by proposing the use of two
auxiliary losses jointly with the main task when fine-tuning any pre-trained
multimodal model. Image-Text Contrastive (ITC) brings image-text
representations of a post closer together and separates them from different
posts, capturing underlying dependencies. Image-Text Matching (ITM) facilitates
the understanding of semantic correspondence between images and text by
penalizing unrelated pairs. We combine these objectives with five multimodal
models, demonstrating consistent improvements across four popular social media
datasets. Furthermore, through detailed analysis, we shed light on the specific
scenarios and cases where each auxiliary task proves to be most effective.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07808">What Matters to Enhance Traffic Rule Compliance of Imitation Learning for Automated Driving. (arXiv:2309.07808v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Hongkuan Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Sui_A/0/1/0/all/0/1">Aifen Sui</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_W/0/1/0/all/0/1">Wei Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1">Letian Shi</a></p>
<p>More research attention has recently been given to end-to-end autonomous
driving technologies where the entire driving pipeline is replaced with a
single neural network because of its simpler structure and faster inference
time. Despite this appealing approach largely reducing the components in
driving pipeline, its simplicity also leads to interpretability problems and
safety issues <a href="/abs/2003.06404">arXiv:2003.06404</a>. The trained policy is not always compliant with
the traffic rules and it is also hard to discover the reason for the
misbehavior because of the lack of intermediate outputs. Meanwhile, Sensors are
also critical to autonomous driving's security and feasibility to perceive the
surrounding environment under complex driving scenarios. In this paper, we
proposed P-CSG, a novel penalty-based imitation learning approach with cross
semantics generation sensor fusion technologies to increase the overall
performance of End-to-End Autonomous Driving. We conducted an assessment of our
model's performance using the Town 05 Long benchmark, achieving an impressive
driving score improvement of over 15%. Furthermore, we conducted robustness
evaluations against adversarial attacks like FGSM and Dot attacks, revealing a
substantial increase in robustness compared to baseline models.More detailed
information, such as code-based resources, ablation studies and videos can be
found at https://hk-zh.github.io/p-csg-plus.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07809">Communication Efficient Private Federated Learning Using Dithering. (arXiv:2309.07809v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hasircioglu_B/0/1/0/all/0/1">Burak Hasircioglu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1">Deniz Gunduz</a></p>
<p>The task of preserving privacy while ensuring efficient communication is a
fundamental challenge in federated learning. In this work, we tackle this
challenge in the trusted aggregator model, and propose a solution that achieves
both objectives simultaneously. We show that employing a quantization scheme
based on subtractive dithering at the clients can effectively replicate the
normal noise addition process at the aggregator. This implies that we can
guarantee the same level of differential privacy against other clients while
substantially reducing the amount of communication required, as opposed to
transmitting full precision gradients and using central noise addition. We also
experimentally demonstrate that the accuracy of our proposed approach matches
that of the full precision gradient method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07812">Text Classification of Cancer Clinical Trial Eligibility Criteria. (arXiv:2309.07812v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yumeng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jayaraj_S/0/1/0/all/0/1">Soumya Jayaraj</a>, <a href="http://arxiv.org/find/cs/1/au:+Ludmir_E/0/1/0/all/0/1">Ethan B Ludmir</a>, <a href="http://arxiv.org/find/cs/1/au:+Roberts_K/0/1/0/all/0/1">Kirk Roberts</a></p>
<p>Automatic identification of clinical trials for which a patient is eligible
is complicated by the fact that trial eligibility is stated in natural
language. A potential solution to this problem is to employ text classification
methods for common types of eligibility criteria. In this study, we focus on
seven common exclusion criteria in cancer trials: prior malignancy, human
immunodeficiency virus, hepatitis B, hepatitis C, psychiatric illness,
drug/substance abuse, and autoimmune illness. Our dataset consists of 764 phase
III cancer trials with these exclusions annotated at the trial level. We
experiment with common transformer models as well as a new pre-trained clinical
trial BERT model. Our results demonstrate the feasibility of automatically
classifying common exclusion criteria. Additionally, we demonstrate the value
of a pre-trained language model specifically for clinical trials, which yields
the highest average performance across all criteria.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07813">Directed Scattering for Knowledge Graph-based Cellular Signaling Analysis. (arXiv:2309.07813v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Venkat_A/0/1/0/all/0/1">Aarthi Venkat</a>, <a href="http://arxiv.org/find/cs/1/au:+Chew_J/0/1/0/all/0/1">Joyce Chew</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_F/0/1/0/all/0/1">Ferran Cardoso Rodriguez</a>, <a href="http://arxiv.org/find/cs/1/au:+Tape_C/0/1/0/all/0/1">Christopher J. Tape</a>, <a href="http://arxiv.org/find/cs/1/au:+Perlmutter_M/0/1/0/all/0/1">Michael Perlmutter</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishnaswamy_S/0/1/0/all/0/1">Smita Krishnaswamy</a></p>
<p>Directed graphs are a natural model for many phenomena, in particular
scientific knowledge graphs such as molecular interaction or chemical reaction
networks that define cellular signaling relationships. In these situations,
source nodes typically have distinct biophysical properties from sinks. Due to
their ordered and unidirectional relationships, many such networks also have
hierarchical and multiscale structure. However, the majority of methods
performing node- and edge-level tasks in machine learning do not take these
properties into account, and thus have not been leveraged effectively for
scientific tasks such as cellular signaling network inference. We propose a new
framework called Directed Scattering Autoencoder (DSAE) which uses a directed
version of a geometric scattering transform, combined with the non-linear
dimensionality reduction properties of an autoencoder and the geometric
properties of the hyperbolic space to learn latent hierarchies. We show this
method outperforms numerous others on tasks such as embedding directed graphs
and learning cellular signaling networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07835">Learning to Warm-Start Fixed-Point Optimization Algorithms. (arXiv:2309.07835v1 [math.OC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Sambharya_R/0/1/0/all/0/1">Rajiv Sambharya</a>, <a href="http://arxiv.org/find/math/1/au:+Hall_G/0/1/0/all/0/1">Georgina Hall</a>, <a href="http://arxiv.org/find/math/1/au:+Amos_B/0/1/0/all/0/1">Brandon Amos</a>, <a href="http://arxiv.org/find/math/1/au:+Stellato_B/0/1/0/all/0/1">Bartolomeo Stellato</a></p>
<p>We introduce a machine-learning framework to warm-start fixed-point
optimization algorithms. Our architecture consists of a neural network mapping
problem parameters to warm starts, followed by a predefined number of
fixed-point iterations. We propose two loss functions designed to either
minimize the fixed-point residual or the distance to a ground truth solution.
In this way, the neural network predicts warm starts with the end-to-end goal
of minimizing the downstream loss. An important feature of our architecture is
its flexibility, in that it can predict a warm start for fixed-point algorithms
run for any number of steps, without being limited to the number of steps it
has been trained on. We provide PAC-Bayes generalization bounds on unseen data
for common classes of fixed-point operators: contractive, linearly convergent,
and averaged. Applying this framework to well-known applications in control,
statistics, and signal processing, we observe a significant reduction in the
number of iterations and solution time required to solve these problems,
through learned warm starts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07860">Identifying the Group-Theoretic Structure of Machine-Learned Symmetries. (arXiv:2309.07860v1 [hep-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/hep-ph/1/au:+Forestano_R/0/1/0/all/0/1">Roy T. Forestano</a>, <a href="http://arxiv.org/find/hep-ph/1/au:+Matchev_K/0/1/0/all/0/1">Konstantin T. Matchev</a>, <a href="http://arxiv.org/find/hep-ph/1/au:+Matcheva_K/0/1/0/all/0/1">Katia Matcheva</a>, <a href="http://arxiv.org/find/hep-ph/1/au:+Roman_A/0/1/0/all/0/1">Alexander Roman</a>, <a href="http://arxiv.org/find/hep-ph/1/au:+Unlu_E/0/1/0/all/0/1">Eyup B. Unlu</a>, <a href="http://arxiv.org/find/hep-ph/1/au:+Verner_S/0/1/0/all/0/1">Sarunas Verner</a></p>
<p>Deep learning was recently successfully used in deriving symmetry
transformations that preserve important physics quantities. Being completely
agnostic, these techniques postpone the identification of the discovered
symmetries to a later stage. In this letter we propose methods for examining
and identifying the group-theoretic structure of such machine-learned
symmetries. We design loss functions which probe the subalgebra structure
either during the deep learning stage of symmetry discovery or in a subsequent
post-processing stage. We illustrate the new methods with examples from the
U(n) Lie group family, obtaining the respective subalgebra decompositions. As
an application to particle physics, we demonstrate the identification of the
residual symmetries after the spontaneous breaking of non-Abelian gauge
symmetries like SU(3) and SU(5) which are commonly used in model building.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07867">Beta Diffusion. (arXiv:2309.07867v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1">Mingyuan Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tianqi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhendong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1">Huangjie Zheng</a></p>
<p>We introduce beta diffusion, a novel generative modeling method that
integrates demasking and denoising to generate data within bounded ranges.
Using scaled and shifted beta distributions, beta diffusion utilizes
multiplicative transitions over time to create both forward and reverse
diffusion processes, maintaining beta distributions in both the forward
marginals and the reverse conditionals, given the data at any point in time.
Unlike traditional diffusion-based generative models relying on additive
Gaussian noise and reweighted evidence lower bounds (ELBOs), beta diffusion is
multiplicative and optimized with KL-divergence upper bounds (KLUBs) derived
from the convexity of the KL divergence. We demonstrate that the proposed KLUBs
are more effective for optimizing beta diffusion compared to negative ELBOs,
which can also be derived as the KLUBs of the same KL divergence with its two
arguments swapped. The loss function of beta diffusion, expressed in terms of
Bregman divergence, further supports the efficacy of KLUBs for optimization.
Experimental results on both synthetic data and natural images demonstrate the
unique capabilities of beta diffusion in generative modeling of range-bounded
data and validate the effectiveness of KLUBs in optimizing diffusion models,
thereby making them valuable additions to the family of diffusion-based
generative models and the optimization techniques used to train them.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07887">Some notes concerning a generalized KMM-type optimization method for density ratio estimation. (arXiv:2309.07887v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alecsa_C/0/1/0/all/0/1">Cristian Daniel Alecsa</a></p>
<p>In the present paper we introduce new optimization algorithms for the task of
density ratio estimation. More precisely, we consider extending the well-known
KMM method using the construction of a suitable loss function, in order to
encompass more general situations involving the estimation of density ratio
with respect to subsets of the training data and test data, respectively. The
associated codes can be found at https://github.com/CDAlecsa/Generalized-KMM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07888">A Novel Local-Global Feature Fusion Framework for Body-weight Exercise Recognition with Pressure Mapping Sensors. (arXiv:2309.07888v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singh_D/0/1/0/all/0/1">Davinder Pal Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Ray_L/0/1/0/all/0/1">Lala Shakti Swarup Ray</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1">Bo Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Suh_S/0/1/0/all/0/1">Sungho Suh</a>, <a href="http://arxiv.org/find/cs/1/au:+Lukowicz_P/0/1/0/all/0/1">Paul Lukowicz</a></p>
<p>We present a novel local-global feature fusion framework for body-weight
exercise recognition with floor-based dynamic pressure maps. One step further
from the existing studies using deep neural networks mainly focusing on global
feature extraction, the proposed framework aims to combine local and global
features using image processing techniques and the YOLO object detection to
localize pressure profiles from different body parts and consider physical
constraints. The proposed local feature extraction method generates two sets of
high-level local features consisting of cropped pressure mapping and numerical
features such as angular orientation, location on the mat, and pressure area.
In addition, we adopt a knowledge distillation for regularization to preserve
the knowledge of the global feature extraction and improve the performance of
the exercise recognition. Our experimental results demonstrate a notable 11
percent improvement in F1 score for exercise recognition while preserving
label-specific features.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07893">Choosing a Proxy Metric from Past Experiments. (arXiv:2309.07893v1 [stat.ME])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Tripuraneni_N/0/1/0/all/0/1">Nilesh Tripuraneni</a>, <a href="http://arxiv.org/find/stat/1/au:+Richardson_L/0/1/0/all/0/1">Lee Richardson</a>, <a href="http://arxiv.org/find/stat/1/au:+DAmour_A/0/1/0/all/0/1">Alexander D&#x27;Amour</a>, <a href="http://arxiv.org/find/stat/1/au:+Soriano_J/0/1/0/all/0/1">Jacopo Soriano</a>, <a href="http://arxiv.org/find/stat/1/au:+Yadlowsky_S/0/1/0/all/0/1">Steve Yadlowsky</a></p>
<p>In many randomized experiments, the treatment effect of the long-term metric
(i.e. the primary outcome of interest) is often difficult or infeasible to
measure. Such long-term metrics are often slow to react to changes and
sufficiently noisy they are challenging to faithfully estimate in short-horizon
experiments. A common alternative is to measure several short-term proxy
metrics in the hope they closely track the long-term metric -- so they can be
used to effectively guide decision-making in the near-term. We introduce a new
statistical framework to both define and construct an optimal proxy metric for
use in a homogeneous population of randomized experiments. Our procedure first
reduces the construction of an optimal proxy metric in a given experiment to a
portfolio optimization problem which depends on the true latent treatment
effects and noise level of experiment under consideration. We then denoise the
observed treatment effects of the long-term metric and a set of proxies in a
historical corpus of randomized experiments to extract estimates of the latent
treatment effects for use in the optimization problem. One key insight derived
from our approach is that the optimal proxy metric for a given experiment is
not apriori fixed; rather it should depend on the sample size (or effective
noise level) of the randomized experiment for which it is deployed. To
instantiate and evaluate our framework, we employ our methodology in a large
corpus of randomized experiments from an industrial recommendation system and
construct proxy metrics that perform favorably relative to several baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07899">Improving physics-informed DeepONets with hard constraints. (arXiv:2309.07899v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Brecht_R/0/1/0/all/0/1">R&#xfc;diger Brecht</a>, <a href="http://arxiv.org/find/cs/1/au:+Popovych_D/0/1/0/all/0/1">Dmytro R. Popovych</a>, <a href="http://arxiv.org/find/cs/1/au:+Bihlo_A/0/1/0/all/0/1">Alex Bihlo</a>, <a href="http://arxiv.org/find/cs/1/au:+Popovych_R/0/1/0/all/0/1">Roman O. Popovych</a></p>
<p>Current physics-informed (standard or operator) neural networks still rely on
accurately learning the initial conditions of the system they are solving. In
contrast, standard numerical methods evolve such initial conditions without
needing to learn these. In this study, we propose to improve current
physics-informed deep learning strategies such that initial conditions do not
need to be learned and are represented exactly in the predicted solution.
Moreover, this method guarantees that when a DeepONet is applied multiple times
to time step a solution, the resulting function is continuous.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07907">Physically Plausible Full-Body Hand-Object Interaction Synthesis. (arXiv:2309.07907v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Braun_J/0/1/0/all/0/1">Jona Braun</a>, <a href="http://arxiv.org/find/cs/1/au:+Christen_S/0/1/0/all/0/1">Sammy Christen</a>, <a href="http://arxiv.org/find/cs/1/au:+Kocabas_M/0/1/0/all/0/1">Muhammed Kocabas</a>, <a href="http://arxiv.org/find/cs/1/au:+Aksan_E/0/1/0/all/0/1">Emre Aksan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1">Otmar Hilliges</a></p>
<p>We propose a physics-based method for synthesizing dexterous hand-object
interactions in a full-body setting. While recent advancements have addressed
specific facets of human-object interactions, a comprehensive physics-based
approach remains a challenge. Existing methods often focus on isolated segments
of the interaction process and rely on data-driven techniques that may result
in artifacts. In contrast, our proposed method embraces reinforcement learning
(RL) and physics simulation to mitigate the limitations of data-driven
approaches. Through a hierarchical framework, we first learn skill priors for
both body and hand movements in a decoupled setting. The generic skill priors
learn to decode a latent skill embedding into the motion of the underlying
part. A high-level policy then controls hand-object interactions in these
pretrained latent spaces, guided by task objectives of grasping and 3D target
trajectory following. It is trained using a novel reward function that combines
an adversarial style term with a task reward, encouraging natural motions while
fulfilling the task incentives. Our method successfully accomplishes the
complete interaction task, from approaching an object to grasping and
subsequent manipulation. We compare our approach against kinematics-based
baselines and show that it leads to more physically plausible motions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07909">Boosting Unsupervised Contrastive Learning Using Diffusion-Based Data Augmentation From Scratch. (arXiv:2309.07909v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zang_Z/0/1/0/all/0/1">Zelin Zang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1">Hao Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Panpan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Fan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Stan.Z Li</a>, <a href="http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1">Yang You</a></p>
<p>Unsupervised contrastive learning methods have recently seen significant
improvements, particularly through data augmentation strategies that aim to
produce robust and generalizable representations. However, prevailing data
augmentation methods, whether hand designed or based on foundation models, tend
to rely heavily on prior knowledge or external data. This dependence often
compromises their effectiveness and efficiency. Furthermore, the applicability
of most existing data augmentation strategies is limited when transitioning to
other research domains, especially science-related data. This limitation stems
from the paucity of prior knowledge and labeled data available in these
domains. To address these challenges, we introduce DiffAug-a novel and
efficient Diffusion-based data Augmentation technique. DiffAug aims to ensure
that the augmented and original data share a smoothed latent space, which is
achieved through diffusion steps. Uniquely, unlike traditional methods, DiffAug
first mines sufficient prior semantic knowledge about the neighborhood. This
provides a constraint to guide the diffusion steps, eliminating the need for
labels, external data/models, or prior knowledge. Designed as an
architecture-agnostic framework, DiffAug provides consistent improvements.
Specifically, it improves image classification and clustering accuracy by
1.6%~4.5%. When applied to biological data, DiffAug improves performance by up
to 10.1%, with an average improvement of 5.8%. DiffAug shows good performance
in both vision and biological domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/1906.00331">On Gradient Descent Ascent for Nonconvex-Concave Minimax Problems. (arXiv:1906.00331v9 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1">Tianyi Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1">Chi Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1">Michael I. Jordan</a></p>
<p>We consider nonconvex-concave minimax problems, $\min_{\mathbf{x}}
\max_{\mathbf{y} \in \mathcal{Y}} f(\mathbf{x}, \mathbf{y})$, where $f$ is
nonconvex in $\mathbf{x}$ but concave in $\mathbf{y}$ and $\mathcal{Y}$ is a
convex and bounded set. One of the most popular algorithms for solving this
problem is the celebrated gradient descent ascent (GDA) algorithm, which has
been widely used in machine learning, control theory and economics. Despite the
extensive convergence results for the convex-concave setting, GDA with equal
stepsize can converge to limit cycles or even diverge in a general setting. In
this paper, we present the complexity results on two-time-scale GDA for solving
nonconvex-concave minimax problems, showing that the algorithm can find a
stationary point of the function $\Phi(\cdot) := \max_{\mathbf{y} \in
\mathcal{Y}} f(\cdot, \mathbf{y})$ efficiently. To the best our knowledge, this
is the first nonasymptotic analysis for two-time-scale GDA in this setting,
shedding light on its superior practical performance in training generative
adversarial networks (GANs) and other real applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2008.05558">On the complexity of finding a local minimizer of a quadratic function over a polytope. (arXiv:2008.05558v5 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Ahmadi_A/0/1/0/all/0/1">Amir Ali Ahmadi</a>, <a href="http://arxiv.org/find/math/1/au:+Zhang_J/0/1/0/all/0/1">Jeffrey Zhang</a></p>
<p>We show that unless P=NP, there cannot be a polynomial-time algorithm that
finds a point within Euclidean distance $c^n$ (for any constant $c \ge 0$) of a
local minimizer of an $n$-variate quadratic function over a polytope. This
result (even with $c=0$) answers a question of Pardalos and Vavasis that
appeared in 1992 on a list of seven open problems in complexity theory for
numerical optimization. Our proof technique also implies that the problem of
deciding whether a quadratic function has a local minimizer over an (unbounded)
polyhedron, and that of deciding if a quartic polynomial has a local minimizer
are NP-hard.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2009.01726">Survival Estimation for Missing not at Random Censoring Indicators based on Copula Models. (arXiv:2009.01726v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Escobar_Bach_M/0/1/0/all/0/1">Mikael Escobar-Bach</a>, <a href="http://arxiv.org/find/stat/1/au:+Goudet_O/0/1/0/all/0/1">Olivier Goudet</a></p>
<p>In the presence of right-censored data with covariates, the conditional
Kaplan-Meier estimator (also known as the Beran estimator) consistently
estimates the conditional survival function of the random follow-up for the
event of interest. However, a necessary condition is the unambiguous knowledge
of whether each individual is censored or not, which may be incomplete in
practice. We therefore propose a study of the Beran estimator when the
censoring indicators are generic random variables and discuss necessary
conditions for the efficiency of the Beran estimator. From this, we provide a
new estimator for the conditional survival function with missing not at random
(MNAR) censoring indicators based on a conditional copula model for the
missingness mechanism. In addition to the theoretical results, we illustrate
how the estimators work for small samples through a simulation study and show
their practical applicability by analyzing synthetic and real data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2105.00495">BAARD: Blocking Adversarial Examples by Testing for Applicability, Reliability and Decidability. (arXiv:2105.00495v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1">Xinglong Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dost_K/0/1/0/all/0/1">Katharina Dost</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1">Kaiqi Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Demontis_A/0/1/0/all/0/1">Ambra Demontis</a>, <a href="http://arxiv.org/find/cs/1/au:+Roli_F/0/1/0/all/0/1">Fabio Roli</a>, <a href="http://arxiv.org/find/cs/1/au:+Dobbie_G/0/1/0/all/0/1">Gill Dobbie</a>, <a href="http://arxiv.org/find/cs/1/au:+Wicker_J/0/1/0/all/0/1">J&#xf6;rg Wicker</a></p>
<p>Adversarial defenses protect machine learning models from adversarial
attacks, but are often tailored to one type of model or attack. The lack of
information on unknown potential attacks makes detecting adversarial examples
challenging. Additionally, attackers do not need to follow the rules made by
the defender. To address this problem, we take inspiration from the concept of
Applicability Domain in cheminformatics. Cheminformatics models struggle to
make accurate predictions because only a limited number of compounds are known
and available for training. Applicability Domain defines a domain based on the
known compounds and rejects any unknown compound that falls outside the domain.
Similarly, adversarial examples start as harmless inputs, but can be
manipulated to evade reliable classification by moving outside the domain of
the classifier. We are the first to identify the similarity between
Applicability Domain and adversarial detection. Instead of focusing on unknown
attacks, we focus on what is known, the training data. We propose a simple yet
robust triple-stage data-driven framework that checks the input globally and
locally, and confirms that they are coherent with the model's output. This
framework can be applied to any classification model and is not limited to
specific attacks. We demonstrate these three stages work as one unit,
effectively detecting various attacks, even for a white-box scenario.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2105.06031">Joint Community Detection and Rotational Synchronization via Semidefinite Programming. (arXiv:2105.06031v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Fan_Y/0/1/0/all/0/1">Yifeng Fan</a>, <a href="http://arxiv.org/find/stat/1/au:+Khoo_Y/0/1/0/all/0/1">Yuehaw Khoo</a>, <a href="http://arxiv.org/find/stat/1/au:+Zhao_Z/0/1/0/all/0/1">Zhizhen Zhao</a></p>
<p>In the presence of heterogeneous data, where randomly rotated objects fall
into multiple underlying categories, it is challenging to simultaneously
classify them into clusters and synchronize them based on pairwise relations.
This gives rise to the joint problem of community detection and
synchronization. We propose a series of semidefinite relaxations, and prove
their exact recovery when extending the celebrated stochastic block model to
this new setting where both rotations and cluster identities are to be
determined. Numerical experiments demonstrate the efficacy of our proposed
algorithms and confirm our theoretical result which indicates a sharp phase
transition for exact recovery.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2110.12539">Discrete Acoustic Space for an Efficient Sampling in Neural Text-To-Speech. (arXiv:2110.12539v3 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Strong_M/0/1/0/all/0/1">Marek Strong</a>, <a href="http://arxiv.org/find/cs/1/au:+Rohnke_J/0/1/0/all/0/1">Jonas Rohnke</a>, <a href="http://arxiv.org/find/cs/1/au:+Bonafonte_A/0/1/0/all/0/1">Antonio Bonafonte</a>, <a href="http://arxiv.org/find/cs/1/au:+Lajszczak_M/0/1/0/all/0/1">Mateusz &#x141;ajszczak</a>, <a href="http://arxiv.org/find/cs/1/au:+Wood_T/0/1/0/all/0/1">Trevor Wood</a></p>
<p>We present a Split Vector Quantized Variational Autoencoder (SVQ-VAE)
architecture using a split vector quantizer for NTTS, as an enhancement to the
well-known Variational Autoencoder (VAE) and Vector Quantized Variational
Autoencoder (VQ-VAE) architectures. Compared to these previous architectures,
our proposed model retains the benefits of using an utterance-level bottleneck,
while keeping significant representation power and a discretized latent space
small enough for efficient prediction from text. We train the model on
recordings in the expressive task-oriented dialogues domain and show that
SVQ-VAE achieves a statistically significant improvement in naturalness over
the VAE and VQ-VAE models. Furthermore, we demonstrate that the SVQ-VAE latent
acoustic space is predictable from text, reducing the gap between the standard
constant vector synthesis and vocoded recordings by 32%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2111.01996">Pareto Adversarial Robustness: Balancing Spatial Robustness and Sensitivity-based Robustness. (arXiv:2111.01996v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1">Ke Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Mingjie Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zhouchen Lin</a></p>
<p>Adversarial robustness, which primarily comprises sensitivity-based
robustness and spatial robustness, plays an integral part in achieving robust
generalization. In this paper, we endeavor to design strategies to achieve
universal adversarial robustness. To achieve this, we first investigate the
relatively less-explored realm of spatial robustness. Then, we integrate the
existing spatial robustness methods by incorporating both local and global
spatial vulnerability into a unified spatial attack and adversarial training
approach. Furthermore, we present a comprehensive relationship between natural
accuracy, sensitivity-based robustness, and spatial robustness, supported by
strong evidence from the perspective of robust representation. Crucially, to
reconcile the interplay between the mutual impacts of various robustness
components into one unified framework, we incorporate the \textit{Pareto
criterion} into the adversarial robustness analysis, yielding a novel strategy
called Pareto Adversarial Training for achieving universal robustness. The
resulting Pareto front, which delineates the set of optimal solutions, provides
an optimal balance between natural accuracy and various adversarial robustness.
This sheds light on solutions for achieving universal robustness in the future.
To the best of our knowledge, we are the first to consider universal
adversarial robustness via multi-objective optimization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.07611">Speeding up Learning Quantum States through Group Equivariant Convolutional Quantum Ans\&quot;atze. (arXiv:2112.07611v3 [quant-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Zheng_H/0/1/0/all/0/1">Han Zheng</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Li_Z/0/1/0/all/0/1">Zimu Li</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Liu_J/0/1/0/all/0/1">Junyu Liu</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Strelchuk_S/0/1/0/all/0/1">Sergii Strelchuk</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Kondor_R/0/1/0/all/0/1">Risi Kondor</a></p>
<p>We develop a theoretical framework for $S_n$-equivariant convolutional
quantum circuits with SU$(d)$-symmetry, building on and significantly
generalizing Jordan's Permutational Quantum Computing (PQC) formalism based on
Schur-Weyl duality connecting both SU$(d)$ and $S_n$ actions on qudits. In
particular, we utilize the Okounkov-Vershik approach to prove Harrow's
statement (Ph.D. Thesis 2005 p.160) on the equivalence between
$\operatorname{SU}(d)$ and $S_n$ irrep bases and to establish the
$S_n$-equivariant Convolutional Quantum Alternating Ans\"atze ($S_n$-CQA) using
Young-Jucys-Murphy (YJM) elements. We prove that $S_n$-CQA is able to generate
any unitary in any given $S_n$ irrep sector, which may serve as a universal
model for a wide array of quantum machine learning problems with the presence
of SU($d$) symmetry. Our method provides another way to prove the universality
of Quantum Approximate Optimization Algorithm (QAOA) and verifies that 4-local
SU($d$) symmetric unitaries are sufficient to build generic SU($d$) symmetric
quantum circuits up to relative phase factors. We present numerical simulations
to showcase the effectiveness of the ans\"atze to find the ground state energy
of the $J_1$--$J_2$ antiferromagnetic Heisenberg model on the rectangular and
Kagome lattices. Our work provides the first application of the celebrated
Okounkov-Vershik's $S_n$ representation theory to quantum physics and machine
learning, from which to propose quantum variational ans\"atze that strongly
suggests to be classically intractable tailored towards a specific optimization
problem.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.03609">PolicyCleanse: Backdoor Detection and Mitigation in Reinforcement Learning. (arXiv:2202.03609v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Junfeng Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1">Ang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Cong Liu</a></p>
<p>While real-world applications of reinforcement learning are becoming popular,
the security and robustness of RL systems are worthy of more attention and
exploration. In particular, recent works have revealed that, in a multi-agent
RL environment, backdoor trigger actions can be injected into a victim agent
(a.k.a. Trojan agent), which can result in a catastrophic failure as soon as it
sees the backdoor trigger action. To ensure the security of RL agents against
malicious backdoors, in this work, we propose the problem of Backdoor Detection
in a multi-agent competitive reinforcement learning system, with the objective
of detecting Trojan agents as well as the corresponding potential trigger
actions, and further trying to mitigate their Trojan behavior. In order to
solve this problem, we propose PolicyCleanse that is based on the property that
the activated Trojan agents accumulated rewards degrade noticeably after
several timesteps. Along with PolicyCleanse, we also design a machine
unlearning-based approach that can effectively mitigate the detected backdoor.
Extensive experiments demonstrate that the proposed methods can accurately
detect Trojan agents, and outperform existing backdoor mitigation baseline
approaches by at least 3% in winning rate across various types of agents and
environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.05928">Benign Overfitting without Linearity: Neural Network Classifiers Trained by Gradient Descent for Noisy Linear Data. (arXiv:2202.05928v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Frei_S/0/1/0/all/0/1">Spencer Frei</a>, <a href="http://arxiv.org/find/cs/1/au:+Chatterji_N/0/1/0/all/0/1">Niladri S. Chatterji</a>, <a href="http://arxiv.org/find/cs/1/au:+Bartlett_P/0/1/0/all/0/1">Peter L. Bartlett</a></p>
<p>Benign overfitting, the phenomenon where interpolating models generalize well
in the presence of noisy data, was first observed in neural network models
trained with gradient descent. To better understand this empirical observation,
we consider the generalization error of two-layer neural networks trained to
interpolation by gradient descent on the logistic loss following random
initialization. We assume the data comes from well-separated class-conditional
log-concave distributions and allow for a constant fraction of the training
labels to be corrupted by an adversary. We show that in this setting, neural
networks exhibit benign overfitting: they can be driven to zero training error,
perfectly fitting any noisy training labels, and simultaneously achieve minimax
optimal test error. In contrast to previous work on benign overfitting that
require linear or kernel-based predictors, our analysis holds in a setting
where both the model and learning dynamics are fundamentally nonlinear.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.07626">Random Feature Amplification: Feature Learning and Generalization in Neural Networks. (arXiv:2202.07626v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Frei_S/0/1/0/all/0/1">Spencer Frei</a>, <a href="http://arxiv.org/find/cs/1/au:+Chatterji_N/0/1/0/all/0/1">Niladri S. Chatterji</a>, <a href="http://arxiv.org/find/cs/1/au:+Bartlett_P/0/1/0/all/0/1">Peter L. Bartlett</a></p>
<p>In this work, we provide a characterization of the feature-learning process
in two-layer ReLU networks trained by gradient descent on the logistic loss
following random initialization. We consider data with binary labels that are
generated by an XOR-like function of the input features. We permit a constant
fraction of the training labels to be corrupted by an adversary. We show that,
although linear classifiers are no better than random guessing for the
distribution we consider, two-layer ReLU networks trained by gradient descent
achieve generalization error close to the label noise rate. We develop a novel
proof technique that shows that at initialization, the vast majority of neurons
function as random features that are only weakly correlated with useful
features, and the gradient descent dynamics 'amplify' these weak, random
features to strong, useful features.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.10629">Model Reprogramming: Resource-Efficient Cross-Domain Machine Learning. (arXiv:2202.10629v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1">Pin-Yu Chen</a></p>
<p>In data-rich domains such as vision, language, and speech, deep learning
prevails to deliver high-performance task-specific models and can even learn
general task-agnostic representations for efficient finetuning to downstream
tasks. However, deep learning in resource-limited domains still faces multiple
challenges including (i) limited data, (ii) constrained model development cost,
and (iii) lack of adequate pre-trained models for effective finetuning. This
paper provides an overview of model reprogramming to bridge this gap. Model
reprogramming enables resource-efficient cross-domain machine learning by
repurposing and reusing a well-developed pre-trained model from a source domain
to solve tasks in a target domain without model finetuning, where the source
and target domains can be vastly different. In many applications, model
reprogramming outperforms transfer learning and training from scratch. This
paper elucidates the methodology of model reprogramming, summarizes existing
use cases, provides a theoretical explanation of the success of model
reprogramming, and concludes with a discussion on open-ended research questions
and opportunities. A list of model reprogramming studies is actively maintained
and updated at https://github.com/IBM/model-reprogramming.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.16874">An Optimal Control Method to Compute the Most Likely Transition Path for Stochastic Dynamical Systems with Jumps. (arXiv:2203.16874v2 [math.NA] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Wei_W/0/1/0/all/0/1">Wei Wei</a>, <a href="http://arxiv.org/find/math/1/au:+Gao_T/0/1/0/all/0/1">Ting Gao</a>, <a href="http://arxiv.org/find/math/1/au:+Duan_J/0/1/0/all/0/1">Jinqiao Duan</a>, <a href="http://arxiv.org/find/math/1/au:+Chen_X/0/1/0/all/0/1">Xiaoli Chen</a></p>
<p>Many complex real world phenomena exhibit abrupt, intermittent or jumping
behaviors, which are more suitable to be described by stochastic differential
equations under non-Gaussian L\'evy noise. Among these complex phenomena, the
most likely transition paths between metastable states are important since
these rare events may have a high impact in certain scenarios. Based on the
large deviation principle, the most likely transition path could be treated as
the minimizer of the rate function upon paths that connect two points. One of
the challenges to calculate the most likely transition path for stochastic
dynamical systems under non-Gaussian L\'evy noise is that the associated rate
function can not be explicitly expressed by paths. For this reason, we
formulate an optimal control problem to obtain the optimal state as the most
likely transition path. We then develop a neural network method to solve this
issue. Several experiments are investigated for both Gaussian and non-Gaussian
cases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2204.10372">Model-free Learning of Regions of Attraction via Recurrent Sets. (arXiv:2204.10372v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yue Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Bichuch_M/0/1/0/all/0/1">Maxim Bichuch</a>, <a href="http://arxiv.org/find/cs/1/au:+Mallada_E/0/1/0/all/0/1">Enrique Mallada</a></p>
<p>We consider the problem of learning an inner approximation of the region of
attraction (ROA) of an asymptotically stable equilibrium point without an
explicit model of the dynamics. Rather than leveraging approximate models with
bounded uncertainty to find a (robust) invariant set contained in the ROA, we
propose to learn sets that satisfy a more relaxed notion of containment known
as recurrence. We define a set to be $\tau$-recurrent (resp. $k$-recurrent) if
every trajectory that starts within the set, returns to it after at most $\tau$
seconds (resp. $k$ steps). We show that under mild assumptions a
$\tau$-recurrent set containing a stable equilibrium must be a subset of its
ROA. We then leverage this property to develop algorithms that compute inner
approximations of the ROA using counter-examples of recurrence that are
obtained by sampling finite-length trajectories. Our algorithms process samples
sequentially, which allow them to continue being executed even after an initial
offline training stage. We further provide an upper bound on the number of
counter-examples used by the algorithm, and almost sure convergence guarantees.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.11110">Meta-Learning Regrasping Strategies for Physical-Agnostic Objects. (arXiv:2205.11110v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_N/0/1/0/all/0/1">Ning Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jingyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1">Ruijie Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Vien_N/0/1/0/all/0/1">Ngo Anh Vien</a>, <a href="http://arxiv.org/find/cs/1/au:+Ziesche_H/0/1/0/all/0/1">Hanna Ziesche</a>, <a href="http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1">Gerhard Neumann</a></p>
<p>Grasping inhomogeneous objects in real-world applications remains a
challenging task due to the unknown physical properties such as mass
distribution and coefficient of friction. In this study, we propose a
meta-learning algorithm called ConDex, which incorporates Conditional Neural
Processes (CNP) with DexNet-2.0 to autonomously discern the underlying physical
properties of objects using depth images. ConDex efficiently acquires physical
embeddings from limited trials, enabling precise grasping point estimation.
Furthermore, ConDex is capable of updating the predicted grasping quality
iteratively from new trials in an online fashion. To the best of our knowledge,
we are the first who generate two object datasets focusing on inhomogeneous
physical properties with varying mass distributions and friction coefficients.
Extensive evaluations in simulation demonstrate ConDex's superior performance
over DexNet-2.0 and existing meta-learning-based grasping pipelines.
Furthermore, ConDex shows robust generalization to previously unseen real-world
objects despite training solely in the simulation. The synthetic and real-world
datasets will be published as well.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.03420">An Adaptive Federated Relevance Framework for Spatial Temporal Graph Learning. (arXiv:2206.03420v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tiehua Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuze Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1">Zhishu Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">Rui Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xiaowei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1">Xi Zheng</a></p>
<p>Spatial-temporal data contains rich information and has been widely studied
in recent years due to the rapid development of relevant applications in many
fields. For instance, medical institutions often use electrodes attached to
different parts of a patient to analyse the electorencephal data rich with
spatial and temporal features for health assessment and disease diagnosis.
Existing research has mainly used deep learning techniques such as
convolutional neural network (CNN) or recurrent neural network (RNN) to extract
hidden spatial-temporal features. Yet, it is challenging to incorporate both
inter-dependencies spatial information and dynamic temporal changes
simultaneously. In reality, for a model that leverages these spatial-temporal
features to fulfil complex prediction tasks, it often requires a colossal
amount of training data in order to obtain satisfactory model performance.
Considering the above-mentioned challenges, we propose an adaptive federated
relevance framework, namely FedRel, for spatial-temporal graph learning in this
paper. After transforming the raw spatial-temporal data into high quality
features, the core Dynamic Inter-Intra Graph (DIIG) module in the framework is
able to use these features to generate the spatial-temporal graphs capable of
capturing the hidden topological and long-term temporal correlation information
in these graphs. To improve the model generalization ability and performance
while preserving the local data privacy, we also design a relevance-driven
federated learning module in our framework to leverage diverse data
distributions from different participants with attentive aggregations of their
models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.00085">Machine Learning and Computer Vision Techniques in Continuous Beehive Monitoring Applications: A survey. (arXiv:2208.00085v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bilik_S/0/1/0/all/0/1">Simon Bilik</a>, <a href="http://arxiv.org/find/cs/1/au:+Zemcik_T/0/1/0/all/0/1">Tomas Zemcik</a>, <a href="http://arxiv.org/find/cs/1/au:+Kratochvila_L/0/1/0/all/0/1">Lukas Kratochvila</a>, <a href="http://arxiv.org/find/cs/1/au:+Ricanek_D/0/1/0/all/0/1">Dominik Ricanek</a>, <a href="http://arxiv.org/find/cs/1/au:+Richter_M/0/1/0/all/0/1">Milos Richter</a>, <a href="http://arxiv.org/find/cs/1/au:+Zambanini_S/0/1/0/all/0/1">Sebastian Zambanini</a>, <a href="http://arxiv.org/find/cs/1/au:+Horak_K/0/1/0/all/0/1">Karel Horak</a></p>
<p>Wide use and availability of the machine learning and computer vision
techniques allows development of relatively complex monitoring systems in many
domains. Besides the traditional industrial domain, new application appears
also in biology and agriculture, where we could speak about the detection of
infections, parasites and weeds, but also about automated monitoring and early
warning systems. This is also connected with the introduction of the easily
accessible hardware and development kits such as Arduino, or RaspberryPi
family. In this paper, we survey 50 existing papers focusing on the methods of
automated beehive monitoring methods using the computer vision techniques,
particularly on the pollen and Varroa mite detection together with the bee
traffic monitoring. Such systems could also be used for the monitoring of the
honeybee colonies and for the inspection of their health state, which could
identify potentially dangerous states before the situation is critical, or to
better plan periodic bee colony inspections and therefore save significant
costs. Later, we also include analysis of the research trends in this
application field and we outline the possible direction of the new
explorations. Our paper is aimed also at veterinary and apidology professionals
and experts, who might not be familiar with machine learning to introduce them
to its possibilities, therefore each family of applications is opened by a
brief theoretical introduction and motivation related to its base method. We
hope that this paper will inspire other scientists to use machine learning
techniques for other applications in beehive monitoring.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.06028">Gaussian Process Surrogate Models for Neural Networks. (arXiv:2208.06028v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Michael Y. Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Grant_E/0/1/0/all/0/1">Erin Grant</a>, <a href="http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1">Thomas L. Griffiths</a></p>
<p>Not being able to understand and predict the behavior of deep learning
systems makes it hard to decide what architecture and algorithm to use for a
given problem. In science and engineering, modeling is a methodology used to
understand complex systems whose internal processes are opaque. Modeling
replaces a complex system with a simpler, more interpretable surrogate. Drawing
inspiration from this, we construct a class of surrogate models for neural
networks using Gaussian processes. Rather than deriving kernels for infinite
neural networks, we learn kernels empirically from the naturalistic behavior of
finite neural networks. We demonstrate our approach captures existing phenomena
related to the spectral bias of neural networks, and then show that our
surrogate models can be used to solve practical problems such as identifying
which points most influence the behavior of specific neural networks and
predicting which architectures and algorithms will generalize well for specific
datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.13049">TrojViT: Trojan Insertion in Vision Transformers. (arXiv:2208.13049v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1">Mengxin Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lou_Q/0/1/0/all/0/1">Qian Lou</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1">Lei Jiang</a></p>
<p>Vision Transformers (ViTs) have demonstrated the state-of-the-art performance
in various vision-related tasks. The success of ViTs motivates adversaries to
perform backdoor attacks on ViTs. Although the vulnerability of traditional
CNNs to backdoor attacks is well-known, backdoor attacks on ViTs are
seldom-studied. Compared to CNNs capturing pixel-wise local features by
convolutions, ViTs extract global context information through patches and
attentions. Na\"ively transplanting CNN-specific backdoor attacks to ViTs
yields only a low clean data accuracy and a low attack success rate. In this
paper, we propose a stealth and practical ViT-specific backdoor attack
$TrojViT$. Rather than an area-wise trigger used by CNN-specific backdoor
attacks, TrojViT generates a patch-wise trigger designed to build a Trojan
composed of some vulnerable bits on the parameters of a ViT stored in DRAM
memory through patch salience ranking and attention-target loss. TrojViT
further uses minimum-tuned parameter update to reduce the bit number of the
Trojan. Once the attacker inserts the Trojan into the ViT model by flipping the
vulnerable bits, the ViT model still produces normal inference accuracy with
benign inputs. But when the attacker embeds a trigger into an input, the ViT
model is forced to classify the input to a predefined target class. We show
that flipping only few vulnerable bits identified by TrojViT on a ViT model
using the well-known RowHammer can transform the model into a backdoored one.
We perform extensive experiments of multiple datasets on various ViT models.
TrojViT can classify $99.64\%$ of test images to a target class by flipping
$345$ bits on a ViT for ImageNet.Our codes are available at
https://github.com/mxzheng/TrojViT
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.00305">LambdaKG: A Library for Pre-trained Language Model-Based Knowledge Graph Embeddings. (arXiv:2210.00305v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xin Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhoubo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaohan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1">Zekun Xi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Ningyu Zhang</a></p>
<p>Knowledge Graphs (KGs) often have two characteristics: heterogeneous graph
structure and text-rich entity/relation information. Text-based KG embeddings
can represent entities by encoding descriptions with pre-trained language
models, but no open-sourced library is specifically designed for KGs with PLMs
at present. In this paper, we present LambdaKG, a library for KGE that equips
with many pre-trained language models (e.g., BERT, BART, T5, GPT-3), and
supports various tasks (e.g., knowledge graph completion, question answering,
recommendation, and knowledge probing). LambdaKG is publicly open-sourced at
https://github.com/zjunlp/PromptKG/tree/main/lambdaKG, with a demo video at
<a href="http://deepke.zjukg.cn/lambdakg.mp4">this http URL</a> and long-term maintenance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.04688">BAFFLE: Backdoor Attack in Offline Reinforcement Learning. (arXiv:2210.04688v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1">Chen Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhou Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1">Yunpeng Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Junda He</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1">Jieke Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Kecen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1">Arunesh Sinha</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1">Bowen Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1">Xinwen Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Lo_D/0/1/0/all/0/1">David Lo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tianhao Wang</a></p>
<p>A growing body of research has focused on the Reinforcement Learning (RL)
methods which allow the agent to learn from trial-and-error experiences
gathered during the interaction with the environment. Recently, offline RL
becomes a popular RL paradigm because it saves the interactions with
environments. In offline RL, data providers share large pre-collected datasets,
and others can train high-quality agents without interacting with the
environments. This paradigm has demonstrated effectiveness in critical tasks
like robot control, autonomous driving, etc. However, less attention is paid to
investigating the security threats to the offline RL system. This paper focuses
on backdoor attacks, where some perturbations are added to the data
(observations) such that given normal observations, the agent takes
high-rewards actions, and low-reward actions on observations injected with
triggers. In this paper, we propose Baffle (Backdoor Attack for Offline
Reinforcement Learning), an approach that automatically implants backdoors to
RL agents by poisoning the offline RL dataset, and evaluate how different
offline RL algorithms react to this attack. Our experiments conducted on four
tasks and four offline RL algorithms expose a disquieting fact: none of the
existing offline RL algorithms is immune to such a backdoor attack. Baffle
modifies $10\%$ of the datasets for four tasks. Agents trained on the poisoned
datasets perform well in normal settings. However, when triggers are presented,
the agents' performance decreases drastically by $63.2\%$, $53.9\%$, $64.7\%$,
and $47.4\%$ in the four tasks on average. The backdoor still persists after
fine-tuning poisoned agents on clean datasets. We further show that the
inserted backdoor is also hard to be detected by a popular defensive method.
This paper calls attention to developing more effective protection for the
open-source offline RL dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.05845">ConSpec: honing in on critical steps for rapid learning and generalization in RL. (arXiv:2210.05845v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1">Chen Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1">Wannan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiralerspong_T/0/1/0/all/0/1">Thomas Jiralerspong</a>, <a href="http://arxiv.org/find/cs/1/au:+Malenfant_D/0/1/0/all/0/1">Dane Malenfant</a>, <a href="http://arxiv.org/find/cs/1/au:+Alsbury_Nealy_B/0/1/0/all/0/1">Benjamin Alsbury-Nealy</a>, <a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1">Yoshua Bengio</a>, <a href="http://arxiv.org/find/cs/1/au:+Richards_B/0/1/0/all/0/1">Blake Richards</a></p>
<p>In real life, success is often contingent upon multiple critical steps that
are distant in time from each other and from the final reward. These critical
steps are challenging to identify with traditional reinforcement learning (RL)
methods that rely on the Bellman equation for credit assignment. Here, we
present a new RL algorithm that uses offline contrastive learning to hone in on
critical steps. This algorithm, which we call contrastive introspection
(ConSpec), can be added to any existing RL algorithm. ConSpec learns a set of
prototypes for the critical steps in a task by a novel contrastive loss and
delivers an intrinsic reward when the current state matches one of these
prototypes. The prototypes in ConSpec provide two key benefits for credit
assignment: (1) They enable rapid identification of all the critical steps. (2)
They do so in a readily interpretable manner, enabling out-of-distribution
generalization when sensory features are altered. Distinct from other
contemporary RL approaches to credit assignment, ConSpec takes advantage of the
fact that it is easier to retrospectively identify the small set of steps that
success is contingent upon than it is to prospectively predict reward at every
step taken in the environment. Altogether, ConSpec improves learning in a
diverse set of RL tasks, including both those with explicit, discrete critical
steps and those with complex, continuous critical steps.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.02048">Efficient Spatially Sparse Inference for Conditional GANs and Diffusion Models. (arXiv:2211.02048v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Muyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Ji Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1">Chenlin Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1">Stefano Ermon</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1">Song Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jun-Yan Zhu</a></p>
<p>During image editing, existing deep generative models tend to re-synthesize
the entire output from scratch, including the unedited regions. This leads to a
significant waste of computation, especially for minor editing operations. In
this work, we present Spatially Sparse Inference (SSI), a general-purpose
technique that selectively performs computation for edited regions and
accelerates various generative models, including both conditional GANs and
diffusion models. Our key observation is that users prone to gradually edit the
input image. This motivates us to cache and reuse the feature maps of the
original image. Given an edited image, we sparsely apply the convolutional
filters to the edited regions while reusing the cached features for the
unedited areas. Based on our algorithm, we further propose Sparse Incremental
Generative Engine (SIGE) to convert the computation reduction to latency
reduction on off-the-shelf hardware. With about $1\%$-area edits, SIGE
accelerates DDPM by $3.0\times$ on NVIDIA RTX 3090 and $4.6\times$ on Apple M1
Pro GPU, Stable Diffusion by $7.2\times$ on 3090, and GauGAN by $5.6\times$ on
3090 and $5.2\times$ on M1 Pro GPU. Compared to our conference version, we
extend SIGE to accommodate attention layers and apply it to Stable Diffusion.
Additionally, we offer support for Apple M1 Pro GPU and include more results
with large and sequential edits.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.10851">Reward is not Necessary: How to Create a Compositional Self-Preserving Agent for Life-Long Learning. (arXiv:2211.10851v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ringstrom_T/0/1/0/all/0/1">Thomas J. Ringstrom</a></p>
<p>Reinforcement Learning views the maximization of rewards and avoidance of
punishments as central to explaining goal-directed behavior. However, over a
life, organisms will need to learn about many different aspects of the world's
structure: the states of the world and state-vector transition dynamics. The
number of combinations of states grows exponentially as an agent incorporates
new knowledge, and there is no obvious weighted combination of pre-existing
rewards or costs defined for a given combination of states, as such a weighting
would need to encode information about good and bad combinations prior to an
agent's experience in the world. Therefore, we must develop more naturalistic
accounts of behavior and motivation in large state-spaces. We show that it is
possible to use only the intrinsic motivation metric of empowerment, which
measures the agent's capacity to realize many possible futures under a
transition operator. We propose to scale empowerment to hierarchical
state-spaces by using Operator Bellman Equations. These equations produce
state-time feasibility functions, which are compositional hierarchical
state-time transition operators that map an initial state and time when an
agent begins a policy to the final states and times of completing a goal.
Because these functions are hierarchical operators we can define hierarchical
empowerment measures on them. An agent can then optimize plans to distant
states and times to maximize its hierarchical empowerment-gain, allowing it to
discover goals that bring about a more favorable coupling of its internal
structure (physiological states) to its external environment (world structure &amp;
spatial state). Life-long agents could therefore be primarily animated by
principles of compositionality and empowerment, exhibiting self-concern for the
growth &amp; maintenance of their own structural integrity without recourse to
reward-maximization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.04953">TargetCall: Eliminating the Wasted Computation in Basecalling via Pre-Basecalling Filtering. (arXiv:2212.04953v2 [q-bio.GN] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Cavlak_M/0/1/0/all/0/1">Meryem Banu Cavlak</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Singh_G/0/1/0/all/0/1">Gagandeep Singh</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Alser_M/0/1/0/all/0/1">Mohammed Alser</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Firtina_C/0/1/0/all/0/1">Can Firtina</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Lindegger_J/0/1/0/all/0/1">Jo&#xeb;l Lindegger</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Sadrosadati_M/0/1/0/all/0/1">Mohammad Sadrosadati</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Ghiasi_N/0/1/0/all/0/1">Nika Mansouri Ghiasi</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Alkan_C/0/1/0/all/0/1">Can Alkan</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Mutlu_O/0/1/0/all/0/1">Onur Mutlu</a></p>
<p>Basecalling is an essential step in nanopore sequencing analysis where the
raw signals of nanopore sequencers are converted into nucleotide sequences,
i.e., reads. State-of-the-art basecallers employ complex deep learning models
to achieve high basecalling accuracy. This makes basecalling
computationally-inefficient and memory-hungry; bottlenecking the entire genome
analysis pipeline. However, for many applications, the majority of reads do no
match the reference genome of interest (i.e., target reference) and thus are
discarded in later steps in the genomics pipeline, wasting the basecalling
computation. To overcome this issue, we propose TargetCall, the first
pre-basecalling filter to eliminate the wasted computation in basecalling.
TargetCall's key idea is to discard reads that will not match the target
reference (i.e., off-target reads) prior to basecalling. TargetCall consists of
two main components: (1) LightCall, a lightweight neural network basecaller
that produces noisy reads; and (2) Similarity Check, which labels each of these
noisy reads as on-target or off-target by matching them to the target
reference. TargetCall aims to filter out all off-target reads before
basecalling. The highly-accurate but slow basecalling is performed only on the
raw signals whose noisy reads are labeled as on-target. Our thorough
experimental evaluations using both real and simulated data show that
TargetCall 1) improves the end-to-end basecalling performance while maintaining
high sensitivity in keeping on-target reads, 2) maintains high accuracy in
downstream analysis, 3) precisely filters out up to 94.71% of off-target reads,
and 4) achieves better performance, throughput, sensitivity, precision, and
generality compared to prior works. We open-source TargetCall at
https://github.com/CMU-SAFARI/TargetCall
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.09597">Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v7 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1">Shuofei Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ou_Y/0/1/0/all/0/1">Yixin Ou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Ningyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xiang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1">Yunzhi Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1">Shumin Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1">Chuanqi Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1">Fei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huajun Chen</a></p>
<p>Reasoning, as an essential ability for complex problem-solving, can provide
back-end support for various real-world applications, such as medical
diagnosis, negotiation, etc. This paper provides a comprehensive survey of
cutting-edge research on reasoning with language model prompting. We introduce
research works with comparisons and summaries and provide systematic resources
to help beginners. We also discuss the potential reasons for emerging such
reasoning abilities and highlight future research directions. Resources are
available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated
periodically).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.01952">On a continuous time model of gradient descent dynamics and instability in deep learning. (arXiv:2302.01952v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Rosca_M/0/1/0/all/0/1">Mihaela Rosca</a>, <a href="http://arxiv.org/find/stat/1/au:+Wu_Y/0/1/0/all/0/1">Yan Wu</a>, <a href="http://arxiv.org/find/stat/1/au:+Qin_C/0/1/0/all/0/1">Chongli Qin</a>, <a href="http://arxiv.org/find/stat/1/au:+Dherin_B/0/1/0/all/0/1">Benoit Dherin</a></p>
<p>The recipe behind the success of deep learning has been the combination of
neural networks and gradient-based optimization. Understanding the behavior of
gradient descent however, and particularly its instability, has lagged behind
its empirical success. To add to the theoretical tools available to study
gradient descent we propose the principal flow (PF), a continuous time flow
that approximates gradient descent dynamics. To our knowledge, the PF is the
only continuous flow that captures the divergent and oscillatory behaviors of
gradient descent, including escaping local minima and saddle points. Through
its dependence on the eigendecomposition of the Hessian the PF sheds light on
the recently observed edge of stability phenomena in deep learning. Using our
new understanding of instability we propose a learning rate adaptation method
which enables us to control the trade-off between training stability and test
set evaluation performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.07260">Scalable Bayesian optimization with high-dimensional outputs using randomized prior networks. (arXiv:2302.07260v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bhouri_M/0/1/0/all/0/1">Mohamed Aziz Bhouri</a>, <a href="http://arxiv.org/find/cs/1/au:+Joly_M/0/1/0/all/0/1">Michael Joly</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1">Robert Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1">Soumalya Sarkar</a>, <a href="http://arxiv.org/find/cs/1/au:+Perdikaris_P/0/1/0/all/0/1">Paris Perdikaris</a></p>
<p>Several fundamental problems in science and engineering consist of global
optimization tasks involving unknown high-dimensional (black-box) functions
that map a set of controllable variables to the outcomes of an expensive
experiment. Bayesian Optimization (BO) techniques are known to be effective in
tackling global optimization problems using a relatively small number objective
function evaluations, but their performance suffers when dealing with
high-dimensional outputs. To overcome the major challenge of dimensionality,
here we propose a deep learning framework for BO and sequential decision making
based on bootstrapped ensembles of neural architectures with randomized priors.
Using appropriate architecture choices, we show that the proposed framework can
approximate functional relationships between design variables and quantities of
interest, even in cases where the latter take values in high-dimensional vector
spaces or even infinite-dimensional function spaces. In the context of BO, we
augmented the proposed probabilistic surrogates with re-parameterized Monte
Carlo approximations of multiple-point (parallel) acquisition functions, as
well as methodological extensions for accommodating black-box constraints and
multi-fidelity information sources. We test the proposed framework against
state-of-the-art methods for BO and demonstrate superior performance across
several challenging tasks with high-dimensional outputs, including a
constrained multi-fidelity optimization task involving shape optimization of
rotor blades in turbo-machinery.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.10848">Deep reinforced learning heuristic tested on spin-glass ground states: The larger picture. (arXiv:2302.10848v2 [cond-mat.dis-nn] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Boettcher_S/0/1/0/all/0/1">Stefan Boettcher</a> (Emory U)</p>
<p>In Changjun Fan et al. [Nature Communications
https://doi.org/10.1038/s41467-023-36363-w (2023)], the authors present a deep
reinforced learning approach to augment combinatorial optimization heuristics.
In particular, they present results for several spin glass ground state
problems, for which instances on non-planar networks are generally NP-hard, in
comparison with several Monte Carlo based methods, such as simulated annealing
(SA) or parallel tempering (PT). Indeed, those results demonstrate that the
reinforced learning improves the results over those obtained with SA or PT, or
at least allows for reduced runtimes for the heuristics before results of
comparable quality have been obtained relative to those other methods. To
facilitate the conclusion that their method is ''superior'', the authors pursue
two basic strategies: (1) A commercial GUROBI solver is called on to procure a
sample of exact ground states as a testbed to compare with, and (2) a
head-to-head comparison between the heuristics is given for a sample of larger
instances where exact ground states are hard to ascertain. Here, we put these
studies into a larger context, showing that the claimed superiority is at best
marginal for smaller samples and becomes essentially irrelevant with respect to
any sensible approximation of true ground states in the larger samples. For
example, this method becomes irrelevant as a means to determine stiffness
exponents $\theta$ in $d&gt;2$, as mentioned by the authors, where the problem is
not only NP-hard but requires the subtraction of two almost equal ground-state
energies and systemic errors in each of $\approx 1\%$ found here are
unacceptable. This larger picture on the method arises from a straightforward
finite-size corrections study over the spin glass ensembles the authors employ,
using data that has been available for decades.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.13348">Kernel Conditional Moment Constraints for Confounding Robust Inference. (arXiv:2302.13348v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Ishikawa_K/0/1/0/all/0/1">Kei Ishikawa</a>, <a href="http://arxiv.org/find/stat/1/au:+He_N/0/1/0/all/0/1">Niao He</a></p>
<p>We study policy evaluation of offline contextual bandits subject to
unobserved confounders. Sensitivity analysis methods are commonly used to
estimate the policy value under the worst-case confounding over a given
uncertainty set. However, existing work often resorts to some coarse relaxation
of the uncertainty set for the sake of tractability, leading to overly
conservative estimation of the policy value. In this paper, we propose a
general estimator that provides a sharp lower bound of the policy value. It can
be shown that our estimator contains the recently proposed sharp estimator by
Dorn and Guo (2022) as a special case, and our method enables a novel extension
of the classical marginal sensitivity model using f-divergence. To construct
our estimator, we leverage the kernel method to obtain a tractable
approximation to the conditional moment constraints, which traditional
non-sharp estimators failed to take into account. In the theoretical analysis,
we provide a condition for the choice of the kernel which guarantees no
specification error that biases the lower bound estimation. Furthermore, we
provide consistency guarantees of policy evaluation and learning. In the
experiments with synthetic and real-world data, we demonstrate the
effectiveness of the proposed method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.08447">MAHTM: A Multi-Agent Framework for Hierarchical Transactive Microgrids. (arXiv:2303.08447v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cuadrado_N/0/1/0/all/0/1">Nicolas Cuadrado</a>, <a href="http://arxiv.org/find/cs/1/au:+Gutierrez_R/0/1/0/all/0/1">Roberto Gutierrez</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yongli Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Takac_M/0/1/0/all/0/1">Martin Takac</a></p>
<p>Integrating variable renewable energy into the grid has posed challenges to
system operators in achieving optimal trade-offs among energy availability,
cost affordability, and pollution controllability. This paper proposes a
multi-agent reinforcement learning framework for managing energy transactions
in microgrids. The framework addresses the challenges above: it seeks to
optimize the usage of available resources by minimizing the carbon footprint
while benefiting all stakeholders. The proposed architecture consists of three
layers of agents, each pursuing different objectives. The first layer,
comprised of prosumers and consumers, minimizes the total energy cost. The
other two layers control the energy price to decrease the carbon impact while
balancing the consumption and production of both renewable and conventional
energy. This framework also takes into account fluctuations in energy demand
and supply.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.12814">Nowhere coexpanding functions. (arXiv:2303.12814v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Cook_A/0/1/0/all/0/1">Andrew Cook</a>, <a href="http://arxiv.org/find/stat/1/au:+Hammerlindl_A/0/1/0/all/0/1">Andy Hammerlindl</a>, <a href="http://arxiv.org/find/stat/1/au:+Tucker_W/0/1/0/all/0/1">Warwick Tucker</a></p>
<p>We define a family of $C^1$ functions which we call "nowhere coexpanding
functions" that is closed under composition and includes all $C^3$ functions
with non-positive Schwarzian derivative. We establish results on the number and
nature of the fixed points of these functions, including a generalisation of a
classic result of Singer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.01029">Domain Generalization for Crop Segmentation with Knowledge Distillation. (arXiv:2304.01029v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Angarano_S/0/1/0/all/0/1">Simone Angarano</a>, <a href="http://arxiv.org/find/cs/1/au:+Martini_M/0/1/0/all/0/1">Mauro Martini</a>, <a href="http://arxiv.org/find/cs/1/au:+Navone_A/0/1/0/all/0/1">Alessandro Navone</a>, <a href="http://arxiv.org/find/cs/1/au:+Chiaberge_M/0/1/0/all/0/1">Marcello Chiaberge</a></p>
<p>In recent years, precision agriculture has gradually oriented farming closer
to automation processes to support all the activities related to field
management. Service robotics plays a predominant role in this evolution by
deploying autonomous agents that can navigate fields while performing tasks
without human intervention, such as monitoring, spraying, and harvesting. To
execute these precise actions, mobile robots need a real-time perception system
that understands their surroundings and identifies their targets in the wild.
Generalizing to new crops and environmental conditions is critical for
practical applications, as labeled samples are rarely available. In this paper,
we investigate the problem of crop segmentation and propose a novel approach to
enhance domain generalization using knowledge distillation. In the proposed
framework, we transfer knowledge from an ensemble of models individually
trained on source domains to a student model that can adapt to unseen target
domains. To evaluate the proposed method, we present a synthetic multi-domain
dataset for crop segmentation containing plants of variegate shapes and
covering different terrain styles, weather conditions, and light scenarios for
more than 50,000 samples. We demonstrate significant improvements in
performance over state-of-the-art methods and superior sim-to-real
generalization. Our approach provides a promising solution for domain
generalization in crop segmentation and has the potential to enhance a wide
variety of precision agriculture applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.03778">Conformal Regression in Calorie Prediction for Team Jumbo-Visma. (arXiv:2304.03778v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kuijk_K/0/1/0/all/0/1">Kristian van Kuijk</a>, <a href="http://arxiv.org/find/cs/1/au:+Dirksen_M/0/1/0/all/0/1">Mark Dirksen</a>, <a href="http://arxiv.org/find/cs/1/au:+Seiler_C/0/1/0/all/0/1">Christof Seiler</a></p>
<p>UCI WorldTour races, the premier men's elite road cycling tour, are grueling
events that put physical fitness and endurance of riders to the test. The
coaches of Team Jumbo-Visma have long been responsible for predicting the
energy needs of each rider of the Dutch team for every race on the calendar.
Those must be estimated to ensure riders have the energy and resources
necessary to maintain a high level of performance throughout a race. This task,
however, is both time-consuming and challenging, as it requires precise
estimates of race speed and power output. Traditionally, the approach to
predicting energy needs has relied on judgement and experience of coaches, but
this method has its limitations and often leads to inaccurate predictions. In
this paper, we propose a new, more effective approach to predicting energy
needs for cycling races. By predicting the speed and power with regression
models, we provide the coaches with calorie needs estimates for each individual
rider per stage instantly. In addition, we compare methods to quantify
uncertainty using conformal prediction. The empirical analysis of the
jackknife+, jackknife-minmax, jackknife-minmax-after-bootstrap, CV+, CV-minmax,
conformalized quantile regression, and inductive conformal prediction methods
in conformal prediction reveals that all methods achieve valid prediction
intervals. All but minmax-based methods also produce sufficiently narrow
prediction intervals for decision-making. Furthermore, methods computing
prediction intervals of fixed size produce tighter intervals for low
significance values. Among the methods computing intervals of varying length
across the input space, inductive conformal prediction computes narrower
prediction intervals at larger significance level.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.07097">Interpretable Weighted Siamese Network to Predict the Time to Onset of Alzheimer&#x27;s Disease from MRI Images. (arXiv:2304.07097v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Hagos_M/0/1/0/all/0/1">Misgina Tsighe Hagos</a>, <a href="http://arxiv.org/find/eess/1/au:+Belton_N/0/1/0/all/0/1">Niamh Belton</a>, <a href="http://arxiv.org/find/eess/1/au:+Killeen_R/0/1/0/all/0/1">Ronan P. Killeen</a>, <a href="http://arxiv.org/find/eess/1/au:+Curran_K/0/1/0/all/0/1">Kathleen M. Curran</a>, <a href="http://arxiv.org/find/eess/1/au:+Namee_B/0/1/0/all/0/1">Brian Mac Namee</a></p>
<p>Alzheimer's Disease (AD) is a progressive disease preceded by Mild Cognitive
Impairment (MCI). Early detection of AD is crucial for making treatment
decisions. However, most of the literature on computer-assisted detection of AD
focuses on classifying brain images into one of three major categories:
healthy, MCI, and AD; or categorizing MCI patients into (1) progressive: those
who progress from MCI to AD at a future examination time, and (2) stable: those
who stay as MCI and never progress to AD. This misses the opportunity to
accurately identify the trajectory of progressive MCI patients. In this paper,
we revisit the brain image classification task for AD identification and
re-frame it as an ordinal classification task to predict how close a patient is
to the severe AD stage. To this end, we select progressive MCI patients from
the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset and construct an
ordinal dataset with a prediction target that indicates the time to progression
to AD. We train a Siamese network model to predict the time to onset of AD
based on MRI brain images. We also propose a Weighted variety of Siamese
network and compare its performance to a baseline model. Our evaluations show
that incorporating a weighting factor to Siamese networks brings considerable
performance gain at predicting how close input brain MRI images are to
progressing to AD. Moreover, we complement our results with an interpretation
of the learned embedding space of the Siamese networks using a model
explainability technique.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.09960">A Latent Space Theory for Emergent Abilities in Large Language Models. (arXiv:2304.09960v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1">Hui Jiang</a></p>
<p>Languages are not created randomly but rather to communicate information.
There is a strong association between languages and their underlying meanings,
resulting in a sparse joint distribution that is heavily peaked according to
their correlations. Moreover, these peak values happen to match with the
marginal distribution of languages due to the sparsity. With the advent of LLMs
trained on big data and large models, we can now precisely assess the marginal
distribution of languages, providing a convenient means of exploring the sparse
structures in the joint distribution for effective inferences. In this paper,
we categorize languages as either unambiguous or {\epsilon}-ambiguous and
present quantitative results to demonstrate that the emergent abilities of
LLMs, such as language understanding, in-context learning, chain-of-thought
prompting, and effective instruction fine-tuning, can all be attributed to
Bayesian inference on the sparse joint distribution of languages.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.10520">Contrastive Tuning: A Little Help to Make Masked Autoencoders Forget. (arXiv:2304.10520v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lehner_J/0/1/0/all/0/1">Johannes Lehner</a>, <a href="http://arxiv.org/find/cs/1/au:+Alkin_B/0/1/0/all/0/1">Benedikt Alkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Furst_A/0/1/0/all/0/1">Andreas F&#xfc;rst</a>, <a href="http://arxiv.org/find/cs/1/au:+Rumetshofer_E/0/1/0/all/0/1">Elisabeth Rumetshofer</a>, <a href="http://arxiv.org/find/cs/1/au:+Miklautz_L/0/1/0/all/0/1">Lukas Miklautz</a>, <a href="http://arxiv.org/find/cs/1/au:+Hochreiter_S/0/1/0/all/0/1">Sepp Hochreiter</a></p>
<p>Masked Image Modeling (MIM) methods, like Masked Autoencoders (MAE),
efficiently learn a rich representation of the input. However, for adapting to
downstream tasks, they require a sufficient amount of labeled data since their
rich features code not only objects but also less relevant image background. In
contrast, Instance Discrimination (ID) methods focus on objects. In this work,
we study how to combine the efficiency and scalability of MIM with the ability
of ID to perform downstream classification in the absence of large amounts of
labeled data. To this end, we introduce Masked Autoencoder Contrastive Tuning
(MAE-CT), a sequential approach that utilizes the implicit clustering of the
Nearest Neighbor Contrastive Learning (NNCLR) objective to induce abstraction
in the topmost layers of a pre-trained MAE. MAE-CT tunes the rich features such
that they form semantic clusters of objects without using any labels. Notably,
MAE-CT does not rely on hand-crafted augmentations and frequently achieves its
best performances while using only minimal augmentations (crop &amp; flip).
Further, MAE-CT is compute efficient as it requires at most 10% overhead
compared to MAE re-training. Applied to large and huge Vision Transformer (ViT)
models, MAE-CT excels over previous self-supervised methods trained on ImageNet
in linear probing, k-NN and low-shot classification accuracy as well as in
unsupervised clustering accuracy. With ViT-H/16 MAE-CT achieves a new
state-of-the-art in linear probing of 82.2%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.12876">Evaluation of Parameter-based Attacks against Embedded Neural Networks with Laser Injection. (arXiv:2304.12876v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dumont_M/0/1/0/all/0/1">Mathieu Dumont</a>, <a href="http://arxiv.org/find/cs/1/au:+Hector_K/0/1/0/all/0/1">Kevin Hector</a>, <a href="http://arxiv.org/find/cs/1/au:+Moellic_P/0/1/0/all/0/1">Pierre-Alain Moellic</a>, <a href="http://arxiv.org/find/cs/1/au:+Dutertre_J/0/1/0/all/0/1">Jean-Max Dutertre</a>, <a href="http://arxiv.org/find/cs/1/au:+Pontie_S/0/1/0/all/0/1">Simon Ponti&#xe9;</a></p>
<p>Upcoming certification actions related to the security of machine learning
(ML) based systems raise major evaluation challenges that are amplified by the
large-scale deployment of models in many hardware platforms. Until recently,
most of research works focused on API-based attacks that consider a ML model as
a pure algorithmic abstraction. However, new implementation-based threats have
been revealed, emphasizing the urgency to propose both practical and
simulation-based methods to properly evaluate the robustness of models. A major
concern is parameter-based attacks (such as the Bit-Flip Attack, BFA) that
highlight the lack of robustness of typical deep neural network models when
confronted by accurate and optimal alterations of their internal parameters
stored in memory. Setting in a security testing purpose, this work practically
reports, for the first time, a successful variant of the BFA on a 32-bit
Cortex-M microcontroller using laser fault injection. It is a standard fault
injection means for security evaluation, that enables to inject spatially and
temporally accurate faults. To avoid unrealistic brute-force strategies, we
show how simulations help selecting the most sensitive set of bits from the
parameters taking into account the laser fault model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.14131">TempEE: Temporal-Spatial Parallel Transformer for Radar Echo Extrapolation Beyond Auto-Regression. (arXiv:2304.14131v2 [eess.SP] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Chen_S/0/1/0/all/0/1">Shengchao Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Shu_T/0/1/0/all/0/1">Ting Shu</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhao_H/0/1/0/all/0/1">Huan Zhao</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhong_G/0/1/0/all/0/1">Guo Zhong</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1">Xunlai Chen</a></p>
<p>Meteorological radar reflectivity data (i.e. radar echo) significantly
influences precipitation prediction. It can facilitate accurate and expeditious
forecasting of short-term heavy rainfall bypassing the need for complex
Numerical Weather Prediction (NWP) models. In comparison to conventional
models, Deep Learning (DL)-based radar echo extrapolation algorithms exhibit
higher effectiveness and efficiency. Nevertheless, the development of reliable
and generalized echo extrapolation algorithm is impeded by three primary
challenges: cumulative error spreading, imprecise representation of sparsely
distributed echoes, and inaccurate description of non-stationary motion
processes. To tackle these challenges, this paper proposes a novel radar echo
extrapolation algorithm called Temporal-Spatial Parallel Transformer, referred
to as TempEE. TempEE avoids using auto-regression and instead employs a
one-step forward strategy to prevent cumulative error spreading during the
extrapolation process. Additionally, we propose the incorporation of a
Multi-level Temporal-Spatial Attention mechanism to improve the algorithm's
capability of capturing both global and local information while emphasizing
task-related regions, including sparse echo representations, in an efficient
manner. Furthermore, the algorithm extracts spatio-temporal representations
from continuous echo images using a parallel encoder to model the
non-stationary motion process for echo extrapolation. The superiority of our
TempEE has been demonstrated in the context of the classic radar echo
extrapolation task, utilizing a real-world dataset. Extensive experiments have
further validated the efficacy and indispensability of various components
within TempEE.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.14541">Deep Spatiotemporal Clustering: A Temporal Clustering Approach for Multi-dimensional Climate Data. (arXiv:2304.14541v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Faruque_O/0/1/0/all/0/1">Omar Faruque</a>, <a href="http://arxiv.org/find/cs/1/au:+Nji_F/0/1/0/all/0/1">Francis Ndikum Nji</a>, <a href="http://arxiv.org/find/cs/1/au:+Cham_M/0/1/0/all/0/1">Mostafa Cham</a>, <a href="http://arxiv.org/find/cs/1/au:+Salvi_R/0/1/0/all/0/1">Rohan Mandar Salvi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1">Xue Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jianwu Wang</a></p>
<p>Clustering high-dimensional spatiotemporal data using an unsupervised
approach is a challenging problem for many data-driven applications. Existing
state-of-the-art methods for unsupervised clustering use different similarity
and distance functions but focus on either spatial or temporal features of the
data. Concentrating on joint deep representation learning of spatial and
temporal features, we propose Deep Spatiotemporal Clustering (DSC), a novel
algorithm for the temporal clustering of high-dimensional spatiotemporal data
using an unsupervised deep learning method. Inspired by the U-net architecture,
DSC utilizes an autoencoder integrating CNN-RNN layers to learn latent
representations of the spatiotemporal data. DSC also includes a unique layer
for cluster assignment on latent representations that uses the Student's
t-distribution. By optimizing the clustering loss and data reconstruction loss
simultaneously, the algorithm gradually improves clustering assignments and the
nonlinear mapping between low-dimensional latent feature space and
high-dimensional original data space. A multivariate spatiotemporal climate
dataset is used to evaluate the efficacy of the proposed method. Our extensive
experiments show our approach outperforms both conventional and deep
learning-based unsupervised clustering algorithms. Additionally, we compared
the proposed model with its various variants (CNN encoder, CNN autoencoder,
CNN-RNN encoder, CNN-RNN autoencoder, etc.) to get insight into using both the
CNN and RNN layers in the autoencoder, and our proposed technique outperforms
these variants in terms of clustering results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.11322">SpikeCP: Delay-Adaptive Reliable Spiking Neural Networks via Conformal Prediction. (arXiv:2305.11322v3 [cs.NE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiechen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1">Sangwoo Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Simeone_O/0/1/0/all/0/1">Osvaldo Simeone</a></p>
<p>Spiking neural networks (SNNs) process time-series data via internal
event-driven neural dynamics whose energy consumption depends on the number of
spikes exchanged between neurons over the course of the input presentation. In
typical implementations of an SNN classifier, decisions are produced after the
entire input sequence has been processed, resulting in latency and energy
consumption levels that are fairly uniform across inputs. Recently introduced
delay-adaptive SNNs tailor the inference latency -- and, with it, the energy
consumption -- to the difficulty of each example, by producing an early
decision when the SNN model is sufficiently ``confident''. In this paper, we
start by observing that, as an SNN processes input samples, its classification
decisions tend to be first under-confident and then over-confident with respect
to the decision's ground-truth, unknown, test accuracy. This makes it difficult
to determine a stopping time that ensures a desired level of accuracy. To
address this problem, we introduce a novel delay-adaptive SNN-based inference
methodology that, wrapping around any pre-trained SNN classifier, provides
guaranteed reliability for the decisions produced at input-dependent stopping
times. The approach entails minimal added complexity as compared to the
underlying SNN, requiring only thresholding and counting operations at run
time, and it leverages tools from conformal prediction (CP).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15021">EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought. (arXiv:2305.15021v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1">Yao Mu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qinglong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1">Mengkang Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenhai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1">Mingyu Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1">Jun Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1">Jifeng Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1">Ping Luo</a></p>
<p>Embodied AI is a crucial frontier in robotics, capable of planning and
executing action sequences for robots to accomplish long-horizon tasks in
physical environments. In this work, we introduce EmbodiedGPT, an end-to-end
multi-modal foundation model for embodied AI, empowering embodied agents with
multi-modal understanding and execution capabilities. To achieve this, we have
made the following efforts: (i) We craft a large-scale embodied planning
dataset, termed EgoCOT. The dataset consists of carefully selected videos from
the Ego4D dataset, along with corresponding high-quality language instructions.
Specifically, we generate a sequence of sub-goals with the "Chain of Thoughts"
mode for effective embodied planning. (ii) We introduce an efficient training
approach to EmbodiedGPT for high-quality plan generation, by adapting a 7B
large language model (LLM) to the EgoCOT dataset via prefix tuning. (iii) We
introduce a paradigm for extracting task-related features from LLM-generated
planning queries to form a closed loop between high-level planning and
low-level control. Extensive experiments show the effectiveness of EmbodiedGPT
on embodied tasks, including embodied planning, embodied control, visual
captioning, and visual question answering. Notably, EmbodiedGPT significantly
enhances the success rate of the embodied control task by extracting more
effective features. It has achieved a remarkable 1.6 times increase in success
rate on the Franka Kitchen benchmark and a 1.3 times increase on the Meta-World
benchmark, compared to the BLIP-2 baseline fine-tuned with the Ego4D dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.07937">Gibbs-Duhem-Informed Neural Networks for Binary Activity Coefficient Prediction. (arXiv:2306.07937v2 [physics.chem-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Rittig_J/0/1/0/all/0/1">Jan G. Rittig</a>, <a href="http://arxiv.org/find/physics/1/au:+Felton_K/0/1/0/all/0/1">Kobi C. Felton</a>, <a href="http://arxiv.org/find/physics/1/au:+Lapkin_A/0/1/0/all/0/1">Alexei A. Lapkin</a>, <a href="http://arxiv.org/find/physics/1/au:+Mitsos_A/0/1/0/all/0/1">Alexander Mitsos</a></p>
<p>We propose Gibbs-Duhem-informed neural networks for the prediction of binary
activity coefficients at varying compositions. That is, we include the
Gibbs-Duhem equation explicitly in the loss function for training neural
networks, which is straightforward in standard machine learning (ML) frameworks
enabling automatic differentiation. In contrast to recent hybrid ML approaches,
our approach does not rely on embedding a specific thermodynamic model inside
the neural network and corresponding prediction limitations. Rather,
Gibbs-Duhem consistency serves as regularization, with the flexibility of ML
models being preserved. Our results show increased thermodynamic consistency
and generalization capabilities for activity coefficient predictions by
Gibbs-Duhem-informed graph neural networks and matrix completion methods. We
also find that the model architecture, particularly the activation function,
can have a strong influence on the prediction quality. The approach can be
easily extended to account for other thermodynamic consistency conditions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.15679">Generating Parametric BRDFs from Natural Language Descriptions. (arXiv:2306.15679v2 [cs.GR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Memery_S/0/1/0/all/0/1">Sean Memery</a>, <a href="http://arxiv.org/find/cs/1/au:+Cedron_O/0/1/0/all/0/1">Osmar Cedron</a>, <a href="http://arxiv.org/find/cs/1/au:+Subr_K/0/1/0/all/0/1">Kartic Subr</a></p>
<p>Artistic authoring of 3D environments is a laborious enterprise that also
requires skilled content creators. There have been impressive improvements in
using machine learning to address different aspects of generating 3D content,
such as generating meshes, arranging geometry, synthesizing textures, etc. In
this paper we develop a model to generate Bidirectional Reflectance
Distribution Functions (BRDFs) from descriptive textual prompts. BRDFs are four
dimensional probability distributions that characterize the interaction of
light with surface materials. They are either represented parametrically, or by
tabulating the probability density associated with every pair of incident and
outgoing angles. The former lends itself to artistic editing while the latter
is used when measuring the appearance of real materials. Numerous works have
focused on hypothesizing BRDF models from images of materials. We learn a
mapping from textual descriptions of materials to parametric BRDFs. Our model
is first trained using a semi-supervised approach before being tuned via an
unsupervised scheme. Although our model is general, in this paper we
specifically generate parameters for MDL materials, conditioned on natural
language descriptions, within NVIDIA's Omniverse platform. This enables use
cases such as real-time text prompts to change materials of objects in 3D
environments such as "dull plastic" or "shiny iron". Since the output of our
model is a parametric BRDF, rather than an image of the material, it may be
used to render materials using any shape under arbitrarily specified viewing
and lighting conditions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05735">Effective Latent Differential Equation Models via Attention and Multiple Shooting. (arXiv:2307.05735v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abrevaya_G/0/1/0/all/0/1">Germ&#xe1;n Abrevaya</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramezanian_Panahi_M/0/1/0/all/0/1">Mahta Ramezanian-Panahi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gagnon_Audet_J/0/1/0/all/0/1">Jean-Christophe Gagnon-Audet</a>, <a href="http://arxiv.org/find/cs/1/au:+Polosecki_P/0/1/0/all/0/1">Pablo Polosecki</a>, <a href="http://arxiv.org/find/cs/1/au:+Rish_I/0/1/0/all/0/1">Irina Rish</a>, <a href="http://arxiv.org/find/cs/1/au:+Dawson_S/0/1/0/all/0/1">Silvina Ponce Dawson</a>, <a href="http://arxiv.org/find/cs/1/au:+Cecchi_G/0/1/0/all/0/1">Guillermo Cecchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Dumas_G/0/1/0/all/0/1">Guillaume Dumas</a></p>
<p>Scientific Machine Learning (SciML) is a burgeoning field that
synergistically combines domain-aware and interpretable models with agnostic
machine learning techniques. In this work, we introduce GOKU-UI, an evolution
of the SciML generative model GOKU-nets. GOKU-UI not only broadens the original
model's spectrum to incorporate other classes of differential equations, such
as Stochastic Differential Equations (SDEs), but also integrates attention
mechanisms and a novel multiple shooting training strategy in the latent space.
These modifications have led to a significant increase in its performance in
both reconstruction and forecast tasks, as demonstrated by our evaluation of
simulated and empirical data. Specifically, GOKU-UI outperformed all baseline
models on synthetic datasets even with a training set 16-fold smaller,
underscoring its remarkable data efficiency. Furthermore, when applied to
empirical human brain data, while incorporating stochastic Stuart-Landau
oscillators into its dynamical core, our proposed enhancements markedly
increased the model's effectiveness in capturing complex brain dynamics. This
augmented version not only surpassed all baseline methods in the reconstruction
task, but also demonstrated lower prediction error of future brain activity up
to 15 seconds ahead. By training GOKU-UI on resting state fMRI data, we encoded
whole-brain dynamics into a latent representation, learning a low-dimensional
dynamical system model that could offer insights into brain functionality and
open avenues for practical applications such as the classification of mental
states or psychiatric conditions. Ultimately, our research provides further
impetus for the field of Scientific Machine Learning, showcasing the potential
for advancements when established scientific insights are interwoven with
modern machine learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.16834">Benchmarking Jetson Edge Devices with an End-to-end Video-based Anomaly Detection System. (arXiv:2307.16834v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1">Hoang Viet Pham</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1">Thinh Gia Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_C/0/1/0/all/0/1">Chuong Dinh Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_A/0/1/0/all/0/1">An Dinh Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Vo_H/0/1/0/all/0/1">Hien Bich Vo</a></p>
<p>Innovative enhancement in embedded system platforms, specifically hardware
accelerations, significantly influence the application of deep learning in
real-world scenarios. These innovations translate human labor efforts into
automated intelligent systems employed in various areas such as autonomous
driving, robotics, Internet-of-Things (IoT), and numerous other impactful
applications. NVIDIA's Jetson platform is one of the pioneers in offering
optimal performance regarding energy efficiency and throughput in the execution
of deep learning algorithms. Previously, most benchmarking analysis was based
on 2D images with a single deep learning model for each comparison result. In
this paper, we implement an end-to-end video-based crime-scene anomaly
detection system inputting from surveillance videos and the system is deployed
and completely operates on multiple Jetson edge devices (Nano, AGX Xavier, Orin
Nano). The comparison analysis includes the integration of Torch-TensorRT as a
software developer kit from NVIDIA for the model performance optimisation. The
system is built based on the PySlowfast open-source project from Facebook as
the coding template. The end-to-end system process comprises the videos from
camera, data preprocessing pipeline, feature extractor and the anomaly
detection. We provide the experience of an AI-based system deployment on
various Jetson Edge devices with Docker technology. Regarding anomaly
detectors, a weakly supervised video-based deep learning model called Robust
Temporal Feature Magnitude Learning (RTFM) is applied in the system. The
approach system reaches 47.56 frames per second (FPS) inference speed on a
Jetson edge device with only 3.11 GB RAM usage total. We also discover the
promising Jetson device that the AI system achieves 15% better performance than
the previous version of Jetson devices while consuming 50% less energy power.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01921">Transferable Graph Neural Fingerprint Models for Quick Response to Future Bio-Threats. (arXiv:2308.01921v2 [q-bio.BM] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Chen_W/0/1/0/all/0/1">Wei Chen</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Ren_Y/0/1/0/all/0/1">Yihui Ren</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Kagawa_A/0/1/0/all/0/1">Ai Kagawa</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Carbone_M/0/1/0/all/0/1">Matthew R. Carbone</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Chen_S/0/1/0/all/0/1">Samuel Yen-Chi Chen</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Qu_X/0/1/0/all/0/1">Xiaohui Qu</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Yoo_S/0/1/0/all/0/1">Shinjae Yoo</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Clyde_A/0/1/0/all/0/1">Austin Clyde</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Ramanathan_A/0/1/0/all/0/1">Arvind Ramanathan</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Stevens_R/0/1/0/all/0/1">Rick L. Stevens</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Dam_H/0/1/0/all/0/1">Hubertus J. J. van Dam</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Liu_D/0/1/0/all/0/1">Deyu Liu</a></p>
<p>Fast screening of drug molecules based on the ligand binding affinity is an
important step in the drug discovery pipeline. Graph neural fingerprint is a
promising method for developing molecular docking surrogates with high
throughput and great fidelity. In this study, we built a COVID-19 drug docking
dataset of about 300,000 drug candidates on 23 coronavirus protein targets.
With this dataset, we trained graph neural fingerprint docking models for
high-throughput virtual COVID-19 drug screening. The graph neural fingerprint
models yield high prediction accuracy on docking scores with the mean squared
error lower than $0.21$ kcal/mol for most of the docking targets, showing
significant improvement over conventional circular fingerprint methods. To make
the neural fingerprints transferable for unknown targets, we also propose a
transferable graph neural fingerprint method trained on multiple targets. With
comparable accuracy to target-specific graph neural fingerprint models, the
transferable model exhibits superb training and data efficiency. We highlight
that the impact of this study extends beyond COVID-19 dataset, as our approach
for fast virtual ligand screening can be easily adapted and integrated into a
general machine learning-accelerated pipeline to battle future bio-threats.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05969">Learning nonparametric DAGs with incremental information via high-order HSIC. (arXiv:2308.05969v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yafei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jianguo Liu</a></p>
<p>Score-based methods for learning Bayesain networks(BN) aim to maximizing the
global score functions. However, if local variables have direct and indirect
dependence simultaneously, the global optimization on score functions misses
edges between variables with indirect dependent relationship, of which scores
are smaller than those with direct dependent relationship. In this paper, we
present an identifiability condition based on a determined subset of parents to
identify the underlying DAG. By the identifiability condition, we develop a
two-phase algorithm namely optimal-tuning (OT) algorithm to locally amend the
global optimization. In the optimal phase, an optimization problem based on
first-order Hilbert-Schmidt independence criterion (HSIC) gives an estimated
skeleton as the initial determined parents subset. In the tuning phase, the
skeleton is locally tuned by deletion, addition and DAG-formalization
strategies using the theoretically proved incremental properties of high-order
HSIC. Numerical experiments for different synthetic datasets and real-world
datasets show that the OT algorithm outperforms existing methods. Especially in
Sigmoid Mix model with the size of the graph being ${\rm\bf d=40}$, the
structure intervention distance (SID) of the OT algorithm is 329.7 smaller than
the one obtained by CAM, which indicates that the graph estimated by the OT
algorithm misses fewer edges compared with CAM.Source code of the OT algorithm
is available at https://github.com/YafeiannWang/optimal-tune-algorithm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.07200">Neural Categorical Priors for Physics-Based Character Control. (arXiv:2308.07200v2 [cs.GR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1">Qingxu Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">He Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lan_M/0/1/0/all/0/1">Mengting Lan</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1">Lei Han</a></p>
<p>Recent advances in learning reusable motion priors have demonstrated their
effectiveness in generating naturalistic behaviors. In this paper, we propose a
new learning framework in this paradigm for controlling physics-based
characters with significantly improved motion quality and diversity over
existing state-of-the-art methods. The proposed method uses reinforcement
learning (RL) to initially track and imitate life-like movements from
unstructured motion clips using the discrete information bottleneck, as adopted
in the Vector Quantized Variational AutoEncoder (VQ-VAE). This structure
compresses the most relevant information from the motion clips into a compact
yet informative latent space, i.e., a discrete space over vector quantized
codes. By sampling codes in the space from a trained categorical prior
distribution, high-quality life-like behaviors can be generated, similar to the
usage of VQ-VAE in computer vision. Although this prior distribution can be
trained with the supervision of the encoder's output, it follows the original
motion clip distribution in the dataset and could lead to imbalanced behaviors
in our setting. To address the issue, we further propose a technique named
prior shifting to adjust the prior distribution using curiosity-driven RL. The
outcome distribution is demonstrated to offer sufficient behavioral diversity
and significantly facilitates upper-level policy learning for downstream tasks.
We conduct comprehensive experiments using humanoid characters on two
challenging downstream tasks, sword-shield striking and two-player boxing game.
Our results demonstrate that the proposed framework is capable of controlling
the character to perform considerably high-quality movements in terms of
behavioral strategies, diversity, and realism. Videos, codes, and data are
available at https://tencent-roboticsx.github.io/NCP/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.08841">Machine Learning-Assisted Discovery of Novel Reactor Designs. (arXiv:2308.08841v2 [cs.CE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Savage_T/0/1/0/all/0/1">Tom Savage</a>, <a href="http://arxiv.org/find/cs/1/au:+Basha_N/0/1/0/all/0/1">Nausheen Basha</a>, <a href="http://arxiv.org/find/cs/1/au:+McDonough_J/0/1/0/all/0/1">Jonathan McDonough</a>, <a href="http://arxiv.org/find/cs/1/au:+Matar_O/0/1/0/all/0/1">Omar K Matar</a>, <a href="http://arxiv.org/find/cs/1/au:+Chanona_E/0/1/0/all/0/1">Ehecatl Antonio del Rio Chanona</a></p>
<p>Additive manufacturing has enabled the fabrication of advanced reactor
geometries, permitting larger, more complex design spaces. Identifying
promising configurations within such spaces presents a significant challenge
for current approaches. Furthermore, existing parameterisations of reactor
geometries are low-dimensional with expensive optimisation limiting more
complex solutions. To address this challenge, we establish a machine
learning-assisted approach for the design of the next-generation of chemical
reactors, combining the application of high-dimensional parameterisations,
computational fluid dynamics, and multi-fidelity Bayesian optimisation. We
associate the development of mixing-enhancing vortical flow structures in novel
coiled reactors with performance, and use our approach to identify key
characteristics of optimal designs. By appealing to fluid mechanical
principles, we rationalise the selection of novel design features that lead to
experimental performance improvements of ~60% over conventional designs. Our
results demonstrate that coupling advanced manufacturing techniques with
`augmented-intelligence' approaches can lead to superior design performance
and, consequently, emissions-reduction and sustainability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.11721">When Are Two Lists Better than One?: Benefits and Harms in Joint Decision-making. (arXiv:2308.11721v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Donahue_K/0/1/0/all/0/1">Kate Donahue</a>, <a href="http://arxiv.org/find/cs/1/au:+Gollapudi_S/0/1/0/all/0/1">Sreenivas Gollapudi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kollias_K/0/1/0/all/0/1">Kostas Kollias</a></p>
<p>Historically, much of machine learning research has focused on the
performance of the algorithm alone, but recently more attention has been
focused on optimizing joint human-algorithm performance. Here, we analyze a
specific type of human-algorithm collaboration where the algorithm has access
to a set of $n$ items, and presents a subset of size $k$ to the human, who
selects a final item from among those $k$. This scenario could model content
recommendation, route planning, or any type of labeling task. Because both the
human and algorithm have imperfect, noisy information about the true ordering
of items, the key question is: which value of $k$ maximizes the probability
that the best item will be ultimately selected? For $k=1$, performance is
optimized by the algorithm acting alone, and for $k=n$ it is optimized by the
human acting alone. Surprisingly, we show that for multiple of noise models, it
is optimal to set $k \in [2, n-1]$ - that is, there are strict benefits to
collaborating, even when the human and algorithm have equal accuracy
separately. We demonstrate this theoretically for the Mallows model and
experimentally for the Random Utilities models of noisy permutations. However,
we show this pattern is reversed when the human is anchored on the algorithm's
presented ordering - the joint system always has strictly worse performance. We
extend these results to the case where the human and algorithm differ in their
accuracy levels, showing that there always exist regimes where a more accurate
agent would strictly benefit from collaborating with a less accurate one, but
these regimes are asymmetric between the human and the algorithm's accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.00855">DoRA: Domain-Based Self-Supervised Learning Framework for Low-Resource Real Estate Appraisal. (arXiv:2309.00855v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1">Wei-Wei Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wei-Yao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1">Wen-Chih Peng</a></p>
<p>The marketplace system connecting demands and supplies has been explored to
develop unbiased decision-making in valuing properties. Real estate appraisal
serves as one of the high-cost property valuation tasks for financial
institutions since it requires domain experts to appraise the estimation based
on the corresponding knowledge and the judgment of the market. Existing
automated valuation models reducing the subjectivity of domain experts require
a large number of transactions for effective evaluation, which is predominantly
limited to not only the labeling efforts of transactions but also the
generalizability of new developing and rural areas. To learn representations
from unlabeled real estate sets, existing self-supervised learning (SSL) for
tabular data neglects various important features, and fails to incorporate
domain knowledge. In this paper, we propose DoRA, a Domain-based
self-supervised learning framework for low-resource Real estate Appraisal. DoRA
is pre-trained with an intra-sample geographic prediction as the pretext task
based on the metadata of the real estate for equipping the real estate
representations with prior domain knowledge. Furthermore, inter-sample
contrastive learning is employed to generalize the representations to be robust
for limited transactions of downstream tasks. Our benchmark results on three
property types of real-world transactions show that DoRA significantly
outperforms the SSL baselines for tabular data, the graph-based methods, and
the supervised approaches in the few-shot scenarios by at least 7.6% for MAPE,
11.59% for MAE, and 3.34% for HR10%. We expect DoRA to be useful to other
financial practitioners with similar marketplace applications who need general
models for properties that are newly built and have limited records. The source
code is available at https://github.com/wwweiwei/DoRA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.00923">GBE-MLZSL: A Group Bi-Enhancement Framework for Multi-Label Zero-Shot Learning. (arXiv:2309.00923v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Ziming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jingcai Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1">Xiaocheng Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1">Song Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_P/0/1/0/all/0/1">Peiran Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiewei Zhang</a></p>
<p>This paper investigates a challenging problem of zero-shot learning in the
multi-label scenario (MLZSL), wherein, the model is trained to recognize
multiple unseen classes within a sample (e.g., an image) based on seen classes
and auxiliary knowledge, e.g., semantic information. Existing methods usually
resort to analyzing the relationship of various seen classes residing in a
sample from the dimension of spatial or semantic characteristics, and transfer
the learned model to unseen ones. But they ignore the effective integration of
local and global features. That is, in the process of inferring unseen classes,
global features represent the principal direction of the image in the feature
space, while local features should maintain uniqueness within a certain range.
This integrated neglect will make the model lose its grasp of the main
components of the image. Relying only on the local existence of seen classes
during the inference stage introduces unavoidable bias. In this paper, we
propose a novel and effective group bi-enhancement framework for MLZSL, dubbed
GBE-MLZSL, to fully make use of such properties and enable a more accurate and
robust visual-semantic projection. Specifically, we split the feature maps into
several feature groups, of which each feature group can be trained
independently with the Local Information Distinguishing Module (LID) to ensure
uniqueness. Meanwhile, a Global Enhancement Module (GEM) is designed to
preserve the principal direction. Besides, a static graph structure is designed
to construct the correlation of local features. Experiments on large-scale
MLZSL benchmark datasets NUS-WIDE and Open-Images-v4 demonstrate that the
proposed GBE-MLZSL outperforms other state-of-the-art methods with large
margins.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.00964">eDKM: An Efficient and Accurate Train-time Weight Clustering for Large Language Models. (arXiv:2309.00964v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1">Minsik Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Vahid_K/0/1/0/all/0/1">Keivan A. Vahid</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1">Qichen Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Adya_S/0/1/0/all/0/1">Saurabh Adya</a>, <a href="http://arxiv.org/find/cs/1/au:+Mundo_C/0/1/0/all/0/1">Carlo C Del Mundo</a>, <a href="http://arxiv.org/find/cs/1/au:+Rastegari_M/0/1/0/all/0/1">Mohammad Rastegari</a>, <a href="http://arxiv.org/find/cs/1/au:+Naik_D/0/1/0/all/0/1">Devang Naik</a>, <a href="http://arxiv.org/find/cs/1/au:+Zatloukal_P/0/1/0/all/0/1">Peter Zatloukal</a></p>
<p>Since Large Language Models or LLMs have demonstrated high-quality
performance on many complex language tasks, there is a great interest in
bringing these LLMs to mobile devices for faster responses and better privacy
protection. However, the size of LLMs (i.e., billions of parameters) requires
highly effective compression to fit into storage-limited devices. Among many
compression techniques, weight-clustering, a form of non-linear quantization,
is one of the leading candidates for LLM compression, and supported by modern
smartphones. Yet, its training overhead is prohibitively significant for LLM
fine-tuning. Especially, Differentiable KMeans Clustering, or DKM, has shown
the state-of-the-art trade-off between compression ratio and accuracy
regression, but its large memory complexity makes it nearly impossible to apply
to train-time LLM compression. In this paper, we propose a memory-efficient DKM
implementation, eDKM powered by novel techniques to reduce the memory footprint
of DKM by orders of magnitudes. For a given tensor to be saved on CPU for the
backward pass of DKM, we compressed the tensor by applying uniquification and
sharding after checking if there is no duplicated tensor previously copied to
CPU. Our experimental results demonstrate that \prjname can fine-tune and
compress a pretrained LLaMA 7B model from 12.6 GB to 2.5 GB (3bit/weight) with
the Alpaca dataset by reducing the train-time memory footprint of a decoder
layer by 130$\times$, while delivering good accuracy on broader LLM benchmarks
(i.e., 77.7% for PIQA, 66.1% for Winograde, and so on).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.04100">Preserved Edge Convolutional Neural Network for Sensitivity Enhancement of Deuterium Metabolic Imaging (DMI). (arXiv:2309.04100v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Dong_S/0/1/0/all/0/1">Siyuan Dong</a>, <a href="http://arxiv.org/find/eess/1/au:+Feyter_H/0/1/0/all/0/1">Henk M. De Feyter</a>, <a href="http://arxiv.org/find/eess/1/au:+Thomas_M/0/1/0/all/0/1">Monique A. Thomas</a>, <a href="http://arxiv.org/find/eess/1/au:+Graaf_R/0/1/0/all/0/1">Robin A. de Graaf</a>, <a href="http://arxiv.org/find/eess/1/au:+Duncan_J/0/1/0/all/0/1">James S. Duncan</a></p>
<p>Purpose: Common to most MRSI techniques, the spatial resolution and the
minimal scan duration of Deuterium Metabolic Imaging (DMI) are limited by the
achievable SNR. This work presents a deep learning method for sensitivity
enhancement of DMI.
</p>
<p>Methods: A convolutional neural network (CNN) was designed to estimate the
2H-labeled metabolite concentrations from low SNR and distorted DMI FIDs. The
CNN was trained with synthetic data that represent a range of SNR levels
typically encountered in vivo. The estimation precision was further improved by
fine-tuning the CNN with MRI-based edge-preserving regularization for each DMI
dataset. The proposed processing method, PReserved Edge ConvolutIonal neural
network for Sensitivity Enhanced DMI (PRECISE-DMI), was applied to simulation
studies and in vivo experiments to evaluate the anticipated improvements in SNR
and investigate the potential for inaccuracies.
</p>
<p>Results: PRECISE-DMI visually improved the metabolic maps of low SNR
datasets, and quantitatively provided higher precision than the standard
Fourier reconstruction. Processing of DMI data acquired in rat brain tumor
models resulted in more precise determination of 2H-labeled lactate and
glutamate + glutamine levels, at increased spatial resolution (from &gt;8 to 2
$\mu$L) or shortened scan time (from 32 to 4 min) compared to standard
acquisitions. However, rigorous SD-bias analyses showed that overuse of the
edge-preserving regularization can compromise the accuracy of the results.
</p>
<p>Conclusion: PRECISE-DMI allows a flexible trade-off between enhancing the
sensitivity of DMI and minimizing the inaccuracies. With typical settings, the
DMI sensitivity can be improved by 3-fold while retaining the capability to
detect local signal variations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.04612">Self-optimizing Feature Generation via Categorical Hashing Representation and Hierarchical Reinforcement Crossing. (arXiv:2309.04612v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ying_W/0/1/0/all/0/1">Wangyang Ying</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dongjie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1">Kunpeng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Leilei Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1">Yanjie Fu</a></p>
<p>Feature generation aims to generate new and meaningful features to create a
discriminative representation space.A generated feature is meaningful when the
generated feature is from a feature pair with inherent feature interaction. In
the real world, experienced data scientists can identify potentially useful
feature-feature interactions, and generate meaningful dimensions from an
exponentially large search space, in an optimal crossing form over an optimal
generation path. But, machines have limited human-like abilities.We generalize
such learning tasks as self-optimizing feature generation. Self-optimizing
feature generation imposes several under-addressed challenges on existing
systems: meaningful, robust, and efficient generation. To tackle these
challenges, we propose a principled and generic representation-crossing
framework to solve self-optimizing feature generation.To achieve hashing
representation, we propose a three-step approach: feature discretization,
feature hashing, and descriptive summarization. To achieve reinforcement
crossing, we develop a hierarchical reinforcement feature crossing approach.We
present extensive experimental results to demonstrate the effectiveness and
efficiency of the proposed method. The code is available at
https://github.com/yingwangyang/HRC_feature_cross.git.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.04824">Correcting sampling biases via importance reweighting for spatial modeling. (arXiv:2309.04824v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Prokhorov_B/0/1/0/all/0/1">Boris Prokhorov</a>, <a href="http://arxiv.org/find/cs/1/au:+Koldasbayeva_D/0/1/0/all/0/1">Diana Koldasbayeva</a>, <a href="http://arxiv.org/find/cs/1/au:+Zaytsev_A/0/1/0/all/0/1">Alexey Zaytsev</a></p>
<p>In machine learning models, the estimation of errors is often complex due to
distribution bias, particularly in spatial data such as those found in
environmental studies. We introduce an approach based on the ideas of
importance sampling to obtain an unbiased estimate of the target error. By
taking into account difference between desirable error and available data, our
method reweights errors at each sample point and neutralizes the shift.
Importance sampling technique and kernel density estimation were used for
reweighteing. We validate the effectiveness of our approach using artificial
data that resemble real-world spatial datasets. Our findings demonstrate
advantages of the proposed approach for the estimation of the target error,
offering a solution to a distribution shift problem. Overall error of
predictions dropped from 7% to just 2% and it gets smaller for larger samples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.06604">Hybrid Algorithm Selection and Hyperparameter Tuning on Distributed Machine Learning Resources: A Hierarchical Agent-based Approach. (arXiv:2309.06604v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Esmaeili_A/0/1/0/all/0/1">Ahmad Esmaeili</a>, <a href="http://arxiv.org/find/cs/1/au:+Rayz_J/0/1/0/all/0/1">Julia T. Rayz</a>, <a href="http://arxiv.org/find/cs/1/au:+Matson_E/0/1/0/all/0/1">Eric T. Matson</a></p>
<p>Algorithm selection and hyperparameter tuning are critical steps in both
academic and applied machine learning. On the other hand, these steps are
becoming ever increasingly delicate due to the extensive rise in the number,
diversity, and distributedness of machine learning resources. Multi-agent
systems, when applied to the design of machine learning platforms, bring about
several distinctive characteristics such as scalability, flexibility, and
robustness, just to name a few. This paper proposes a fully automatic and
collaborative agent-based mechanism for selecting distributedly organized
machine learning algorithms and simultaneously tuning their hyperparameters.
Our method builds upon an existing agent-based hierarchical machine-learning
platform and augments its query structure to support the aforementioned
functionalities without being limited to specific learning, selection, and
tuning mechanisms. We have conducted theoretical assessments, formal
verification, and analytical study to demonstrate the correctness, resource
utilization, and computational efficiency of our technique. According to the
results, our solution is totally correct and exhibits linear time and space
complexity in relation to the size of available resources. To provide concrete
examples of how the proposed methodologies can effectively adapt and perform
across a range of algorithmic options and datasets, we have also conducted a
series of experiments using a system comprised of 24 algorithms and 9 datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.06724">Deep Nonparametric Convexified Filtering for Computational Photography, Image Synthesis and Adversarial Defense. (arXiv:2309.06724v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wangni_J/0/1/0/all/0/1">Jianqiao Wangni</a></p>
<p>We aim to provide a general framework of for computational photography that
recovers the real scene from imperfect images, via the Deep Nonparametric
Convexified Filtering (DNCF). It is consists of a nonparametric deep network to
resemble the physical equations behind the image formation, such as denoising,
super-resolution, inpainting, and flash. DNCF has no parameterization dependent
on training data, therefore has a strong generalization and robustness to
adversarial image manipulation. During inference, we also encourage the network
parameters to be nonnegative and create a bi-convex function on the input and
parameters, and this adapts to second-order optimization algorithms with
insufficient running time, having 10X acceleration over Deep Image Prior. With
these tools, we empirically verify its capability to defend image
classification deep networks against adversary attack algorithms in real-time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.06800">Uncertainty-aware Traffic Prediction under Missing Data. (arXiv:2309.06800v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1">Hao Mei</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Junxian Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1">Zhiming Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1">Guanjie Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1">Bin Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1">Hua Wei</a></p>
<p>Traffic prediction is a crucial topic because of its broad scope of
applications in the transportation domain. Recently, various studies have
achieved promising results. However, most studies assume the prediction
locations have complete or at least partial historical records and cannot be
extended to non-historical recorded locations. In real-life scenarios, the
deployment of sensors could be limited due to budget limitations and
installation availability, which makes most current models not applicable.
Though few pieces of literature tried to impute traffic states at the missing
locations, these methods need the data simultaneously observed at the locations
with sensors, making them not applicable to prediction tasks. Another drawback
is the lack of measurement of uncertainty in prediction, making prior works
unsuitable for risk-sensitive tasks or involving decision-making. To fill the
gap, inspired by the previous inductive graph neural network, this work
proposed an uncertainty-aware framework with the ability to 1) extend
prediction to missing locations with no historical records and significantly
extend spatial coverage of prediction locations while reducing deployment of
sensors and 2) generate probabilistic prediction with uncertainty
quantification to help the management of risk and decision making in the
down-stream tasks. Through extensive experiments on real-life datasets, the
result shows our method achieved promising results on prediction tasks, and the
uncertainty quantification gives consistent results which highly correlated
with the locations with and without historical data. We also show that our
model could help support sensor deployment tasks in the transportation field to
achieve higher accuracy with a limited sensor deployment budget.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07030">Optimal transport distances for directed, weighted graphs: a case study with cell-cell communication networks. (arXiv:2309.07030v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nagai_J/0/1/0/all/0/1">James S. Nagai</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Costa_I/0/1/0/all/0/1">Ivan G. Costa</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Schaub_M/0/1/0/all/0/1">Michael T. Schaub</a> (2) ((1) Institute for Computational Genomics, RWTH Aachen Medical Faculty, Germany, (2) Department of Computer Science, RWTH Aachen University, Germany)</p>
<p>Comparing graphs by means of optimal transport has recently gained
significant attention, as the distances induced by optimal transport provide
both a principled metric between graphs as well as an interpretable description
of the associated changes between graphs in terms of a transport plan. As the
lack of symmetry introduces challenges in the typically considered
formulations, optimal transport distances for graphs have mostly been developed
for undirected graphs. Here, we propose two distance measures to compare
directed graphs based on variants of optimal transport: (i) an earth movers
distance (Wasserstein) and (ii) a Gromov-Wasserstein (GW) distance. We evaluate
these two distances and discuss their relative performance for both simulated
graph data and real-world directed cell-cell communication graphs, inferred
from single-cell RNA-seq data.
</p>
</p>
</div>

    </div>
    </body>
    