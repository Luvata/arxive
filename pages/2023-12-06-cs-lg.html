<!DOCTYPE html>
<html>
<head>
<title>2023-12-06-cs-lg</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2312.00794">Informative Priors Improve the Reliability of Multimodal Clinical Data Classification. (arXiv:2312.00794v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lopez_L/0/1/0/all/0/1">L. Julian Lechuga Lopez</a>, <a href="http://arxiv.org/find/cs/1/au:+Rudner_T/0/1/0/all/0/1">Tim G. J. Rudner</a>, <a href="http://arxiv.org/find/cs/1/au:+Shamout_F/0/1/0/all/0/1">Farah E. Shamout</a></p>
<p>Machine learning-aided clinical decision support has the potential to
significantly improve patient care. However, existing efforts in this domain
for principled quantification of uncertainty have largely been limited to
applications of ad-hoc solutions that do not consistently improve reliability.
In this work, we consider stochastic neural networks and design a tailor-made
multimodal data-driven (M2D2) prior distribution over network parameters. We
use simple and scalable Gaussian mean-field variational inference to train a
Bayesian neural network using the M2D2 prior. We train and evaluate the
proposed approach using clinical time-series data in MIMIC-IV and corresponding
chest X-ray images in MIMIC-CXR for the classification of acute care
conditions. Our empirical results show that the proposed method produces a more
reliable predictive model compared to deterministic and Bayesian neural network
baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00795">Talent-Interview: Web-Client Cheating Detection for Online Exams. (arXiv:2312.00795v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ege_M/0/1/0/all/0/1">Mert Ege</a>, <a href="http://arxiv.org/find/cs/1/au:+Ceyhan_M/0/1/0/all/0/1">Mustafa Ceyhan</a></p>
<p>Online exams are more attractive after the Covid-19 pandemic. Furthermore,
during recruitment, online exams are used. However, there are more cheating
possibilities for online exams. Assigning a proctor for each exam increases
cost. At this point, automatic proctor systems detect possible cheating status.
This article proposes an end-to-end system and submodules to get better results
for online proctoring. Object detection, face recognition, human voice
detection, and segmentation are used in our system. Furthermore, our proposed
model works on the PCs of users, meaning a client-based system. So, server cost
is eliminated. As far as we know, it is the first time the client-based online
proctoring system has been used for recruitment. Online exams are more
attractive after the Covid-19 pandemic. Furthermore, during recruitment, online
exams are used. However, there are more cheating possibilities for online
exams. Assigning a proctor for each exam increases cost. At this point,
automatic proctor systems detect possible cheating status. This article
proposes an end-to-end system and submodules to get better results for online
proctoring. Object detection, face recognition, human voice detection, and
segmentation are used in our system. Furthermore, our proposed model works on
the PCs of users, meaning a client-based system. So, server cost is eliminated.
As far as we know, it is the first time the client-based online proctoring
system has been used for recruitment. Furthermore, this cheating system works
at https://www.talent-interview.com/tr/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00799">hvEEGNet: exploiting hierarchical VAEs on EEG data for neuroscience applications. (arXiv:2312.00799v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Cisotto_G/0/1/0/all/0/1">Giulia Cisotto</a>, <a href="http://arxiv.org/find/eess/1/au:+Zancanaro_A/0/1/0/all/0/1">Alberto Zancanaro</a>, <a href="http://arxiv.org/find/eess/1/au:+Zoppis_I/0/1/0/all/0/1">Italo F. Zoppis</a>, <a href="http://arxiv.org/find/eess/1/au:+Manzoni_S/0/1/0/all/0/1">Sara L. Manzoni</a></p>
<p>With the recent success of artificial intelligence in neuroscience, a number
of deep learning (DL) models were proposed for classification, anomaly
detection, and pattern recognition tasks in electroencephalography (EEG). EEG
is a multi-channel time-series that provides information about the individual
brain activity for diagnostics, neuro-rehabilitation, and other applications
(including emotions recognition). Two main issues challenge the existing
DL-based modeling methods for EEG: the high variability between subjects and
the low signal-to-noise ratio making it difficult to ensure a good quality in
the EEG data. In this paper, we propose two variational autoencoder models,
namely vEEGNet-ver3 and hvEEGNet, to target the problem of high-fidelity EEG
reconstruction. We properly designed their architectures using the blocks of
the well-known EEGNet as the encoder, and proposed a loss function based on
dynamic time warping. We tested the models on the public Dataset 2a - BCI
Competition IV, where EEG was collected from 9 subjects and 22 channels.
hvEEGNet was found to reconstruct the EEG data with very high-fidelity,
outperforming most previous solutions (including our vEEGNet-ver3 ).
Furthermore, this was consistent across all subjects. Interestingly, hvEEGNet
made it possible to discover that this popular dataset includes a number of
corrupted EEG recordings that might have influenced previous literature
results. We also investigated the training behaviour of our models and related
it with the quality and the size of the input EEG dataset, aiming at opening a
new research debate on this relationship. In the future, hvEEGNet could be used
as anomaly (e.g., artefact) detector in large EEG datasets to support the
domain experts, but also the latent representations it provides could be used
in other classification problems and EEG data generation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00802">Continuous Authentication Using Mouse Clickstream Data Analysis. (arXiv:2312.00802v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Almalki_S/0/1/0/all/0/1">Sultan Almalki</a>, <a href="http://arxiv.org/find/eess/1/au:+Chatterjee_P/0/1/0/all/0/1">Prosenjit Chatterjee</a>, <a href="http://arxiv.org/find/eess/1/au:+Roy_K/0/1/0/all/0/1">Kaushik Roy</a></p>
<p>Biometrics is used to authenticate an individual based on physiological or
behavioral traits. Mouse dynamics is an example of a behavioral biometric that
can be used to perform continuous authentication as protection against security
breaches. Recent research on mouse dynamics has shown promising results in
identifying users; however, it has not yet reached an acceptable level of
accuracy. In this paper, an empirical evaluation of different classification
techniques is conducted on a mouse dynamics dataset, the Balabit Mouse
Challenge dataset. User identification is carried out using three mouse
actions: mouse move, point and click, and drag and drop. Verification and
authentication methods are conducted using three machine-learning classifiers:
the Decision Tree classifier, the K-Nearest Neighbors classifier, and the
Random Forest classifier. The results show that the three classifiers can
distinguish between a genuine user and an impostor with a relatively high
degree of accuracy. In the verification mode, all the classifiers achieve a
perfect accuracy of 100%. In authentication mode, all three classifiers
achieved the highest accuracy (ACC) and Area Under Curve (AUC) from scenario B
using the point and click action data: (Decision Tree ACC:87.6%, AUC:90.3%),
(K-Nearest Neighbors ACC:99.3%, AUC:99.9%), and (Random Forest ACC:89.9%,
AUC:92.5%).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00803">InceptionCaps: A Performant Glaucoma Classification Model for Data-scarce Environment. (arXiv:2312.00803v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Manohar_G/0/1/0/all/0/1">Gyanendar Manohar</a>, <a href="http://arxiv.org/find/cs/1/au:+OReilly_R/0/1/0/all/0/1">Ruairi O&#x27;Reilly</a></p>
<p>Glaucoma is an irreversible ocular disease and is the second leading cause of
visual disability worldwide. Slow vision loss and the asymptomatic nature of
the disease make its diagnosis challenging. Early detection is crucial for
preventing irreversible blindness. Ophthalmologists primarily use retinal
fundus images as a non-invasive screening method. Convolutional neural networks
(CNN) have demonstrated high accuracy in the classification of medical images.
Nevertheless, CNN's translation-invariant nature and inability to handle the
part-whole relationship between objects make its direct application unsuitable
for glaucomatous fundus image classification, as it requires a large number of
labelled images for training. This work reviews existing state of the art
models and proposes InceptionCaps, a novel capsule network (CapsNet) based deep
learning model having pre-trained InceptionV3 as its convolution base, for
automatic glaucoma classification. InceptionCaps achieved an accuracy of 0.956,
specificity of 0.96, and AUC of 0.9556, which surpasses several
state-of-the-art deep learning model performances on the RIM-ONE v2 dataset.
The obtained result demonstrates the robustness of the proposed deep learning
model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00804">Automatic detection of problem-gambling signs from online texts using large language models. (arXiv:2312.00804v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Smith_E/0/1/0/all/0/1">Elke Smith</a>, <a href="http://arxiv.org/find/cs/1/au:+Reiter_N/0/1/0/all/0/1">Nils Reiter</a>, <a href="http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1">Jan Peters</a></p>
<p>Problem gambling is a major public health concern and is associated with
profound psychological distress and economic problems. There are numerous
gambling communities on the internet where users exchange information about
games, gambling tactics, as well as gambling-related problems. Individuals
exhibiting higher levels of problem gambling engage more in such communities.
Online gambling communities may provide insights into problem-gambling
behaviour. Using data scraped from a major German gambling discussion board, we
fine-tuned a large language model, specifically a Bidirectional Encoder
Representations from Transformers (BERT) model, to predict signs of
problem-gambling from forum posts. Training data were generated by manual
annotation and by taking into account diagnostic criteria and gambling-related
cognitive distortions. Using k-fold cross-validation, our models achieved a
precision of 0.95 and F1 score of 0.71, demonstrating that satisfactory
classification performance can be achieved by generating high-quality training
material through manual annotation based on diagnostic criteria. The current
study confirms that a BERT-based model can be reliably used on small data sets
and to detect signatures of problem gambling in online communication data. Such
computational approaches may have potential for the detection of changes in
problem-gambling prevalence among online users.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00808">Transforming organic chemistry research paradigms: moving from manual efforts to the intersection of automation and artificial intelligence. (arXiv:2312.00808v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chengchun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuntian Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Mo_F/0/1/0/all/0/1">Fanyang Mo</a></p>
<p>Organic chemistry is undergoing a major paradigm shift, moving from a
labor-intensive approach to a new era dominated by automation and artificial
intelligence (AI). This transformative shift is being driven by technological
advances, the ever-increasing demand for greater research efficiency and
accuracy, and the burgeoning growth of interdisciplinary research. AI models,
supported by computational power and algorithms, are drastically reshaping
synthetic planning and introducing groundbreaking ways to tackle complex
molecular synthesis. In addition, autonomous robotic systems are rapidly
accelerating the pace of discovery by performing tedious tasks with
unprecedented speed and precision. This article examines the multiple
opportunities and challenges presented by this paradigm shift and explores its
far-reaching implications. It provides valuable insights into the future
trajectory of organic chemistry research, which is increasingly defined by the
synergistic interaction of automation and AI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00811">Seizure detection from Electroencephalogram signals via Wavelets and Graph Theory metrics. (arXiv:2312.00811v1 [q-bio.NC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Grant_P/0/1/0/all/0/1">Paul Grant</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Islam_M/0/1/0/all/0/1">Md Zahidul Islam</a></p>
<p>Epilepsy is one of the most prevalent neurological conditions, where an
epileptic seizure is a transient occurrence due to abnormal, excessive and
synchronous activity in the brain. Electroencephalogram signals emanating from
the brain may be captured, analysed and then play a significant role in
detection and prediction of epileptic seizures. In this work we enhance upon a
previous approach that relied on the differing properties of the wavelet
transform. Here we apply the Maximum Overlap Discrete Wavelet Transform to both
reduce signal \textit{noise} and use signal variance exhibited at differing
inherent frequency levels to develop various metrics of connection between the
electrodes placed upon the scalp. %The properties of both the noise reduced
signal and the interconnected electrodes differ significantly during the
different brain states.
</p>
<p>Using short duration epochs, to approximate close to real time monitoring,
together with simple statistical parameters derived from the reconstructed
noise reduced signals we initiate seizure detection. To further improve
performance we utilise graph theoretic indicators from derived electrode
connectivity. From there we build the attribute space. We utilise open-source
software and publicly available data to highlight the superior
Recall/Sensitivity performance of our approach, when compared to existing
published methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00812">Empowering Autonomous Driving with Large Language Models: A Safety Perspective. (arXiv:2312.00812v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yixuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiao_R/0/1/0/all/0/1">Ruochen Jiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lang_C/0/1/0/all/0/1">Chengtian Lang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhan_S/0/1/0/all/0/1">Sinong Simon Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1">Chao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhaoran Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhuoran Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1">Qi Zhu</a></p>
<p>Autonomous Driving (AD) faces crucial hurdles for commercial launch, notably
in the form of diminished public trust and safety concerns from long-tail
unforeseen driving scenarios. This predicament is due to the limitation of deep
neural networks in AD software, which struggle with interpretability and
exhibit poor generalization capabilities in out-of-distribution and uncertain
scenarios. To this end, this paper advocates for the integration of Large
Language Models (LLMs) into the AD system, leveraging their robust common-sense
knowledge, reasoning abilities, and human-interaction capabilities. The
proposed approach deploys the LLM as an intelligent decision-maker in planning,
incorporating safety verifiers for contextual safety learning to enhance
overall AD performance and safety. We present results from two case studies
that affirm the efficacy of our approach. We further discuss the potential
integration of LLM for other AD software components including perception,
prediction, and simulation. Despite the observed challenges in the case
studies, the integration of LLMs is promising and beneficial for reinforcing
both safety and performance in AD.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00817">TimelyGPT: Recurrent Convolutional Transformer for Long Time-series Representation. (arXiv:2312.00817v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1">Ziyang Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1">Qincheng Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yue Li</a></p>
<p>Pre-trained models (PTMs) have gained prominence in Natural Language
Processing and Computer Vision domains. When it comes to time-series PTMs,
their development has been limited. Previous research on time-series
transformers has mainly been devoted to small-scale tasks, yet these models
have not consistently outperformed traditional models. Additionally, the
performance of these transformers on large-scale data remains unexplored. These
findings raise doubts about Transformer's capabilities to scale up and capture
temporal dependencies. In this study, we re-examine time-series transformers
and identify the shortcomings of prior studies. Drawing from these insights, we
then introduce a pioneering architecture called Timely Generative Pre-trained
Transformer (\model). This architecture integrates recurrent attention and
temporal convolution modules to effectively capture global-local temporal
dependencies in long sequences. The relative position embedding with time decay
can effectively deal with trend and periodic patterns from time-series. Our
experiments show that \model~excels in modeling continuously monitored
biosignal as well as irregularly-sampled time-series data commonly observed in
longitudinal electronic health records. This breakthrough suggests a priority
shift in time-series deep learning research, moving from small-scale modeling
from scratch to large-scale pre-training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00818">The perpetual motion machine of AI-generated data and the distraction of ChatGPT-as-scientist. (arXiv:2312.00818v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Listgarten_J/0/1/0/all/0/1">Jennifer Listgarten</a></p>
<p>Since ChatGPT works so well, are we on the cusp of solving science with AI?
Is not AlphaFold2 suggestive that the potential of LLMs in biology and the
sciences more broadly is limitless? Can we use AI itself to bridge the lack of
data in the sciences in order to then train an AI? Herein we present a
discussion of these topics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00819">Large Language Models for Travel Behavior Prediction. (arXiv:2312.00819v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mo_B/0/1/0/all/0/1">Baichuan Mo</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hanyong Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_D/0/1/0/all/0/1">Dingyi Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1">Ruoyun Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1">Xiaotong Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jinhua Zhao</a></p>
<p>Travel behavior prediction is a fundamental task in transportation demand
management. The conventional methods for travel behavior prediction rely on
numerical data to construct mathematical models and calibrate model parameters
to represent human preferences. Recent advancement in large language models
(LLMs) has shown great reasoning abilities to solve complex problems. In this
study, we propose to use LLMs to predict travel behavior with prompt
engineering without data-based parameter learning. Specifically, we carefully
design our prompts that include 1) task description, 2) travel characteristics,
3) individual attributes, and 4) guides of thinking with domain knowledge, and
ask the LLMs to predict an individual's travel behavior and explain the
results. We select the travel mode choice task as a case study. Results show
that, though no training samples are provided, LLM-based predictions have
competitive accuracy and F1-score as canonical supervised learning methods such
as multinomial logit, random forest, and neural networks. LLMs can also output
reasons that support their prediction. However, though in most of the cases,
the output explanations are reasonable, we still observe cases that violate
logic or with hallucinations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00820">Non-Cross Diffusion for Semantic Consistency. (arXiv:2312.00820v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Ziyang Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1">Ruiyuan Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1">Qiang Xu</a></p>
<p>In diffusion models, deviations from a straight generative flow are a common
issue, resulting in semantic inconsistencies and suboptimal generations. To
address this challenge, we introduce `Non-Cross Diffusion', an innovative
approach in generative modeling for learning ordinary differential equation
(ODE) models. Our methodology strategically incorporates an ascending dimension
of input to effectively connect points sampled from two distributions with
uncrossed paths. This design is pivotal in ensuring enhanced semantic
consistency throughout the inference process, which is especially critical for
applications reliant on consistent generative flows, including various
distillation methods and deterministic sampling, which are fundamental in image
editing and interpolation tasks. Our empirical results demonstrate the
effectiveness of Non-Cross Diffusion, showing a substantial reduction in
semantic inconsistencies at different inference steps and a notable enhancement
in the overall performance of diffusion models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00823">Adaptive Multi-Modality Prompt Learning. (arXiv:2312.00823v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zongqian Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yujing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhan_M/0/1/0/all/0/1">Mengmeng Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1">Jialie Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1">Ping Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiaofeng Zhu</a></p>
<p>Although current prompt learning methods have successfully been designed to
effectively reuse the large pre-trained models without fine-tuning their large
number of parameters, they still have limitations to be addressed, i.e.,
without considering the adverse impact of meaningless patches in every image
and without simultaneously considering in-sample generalization and
out-of-sample generalization. In this paper, we propose an adaptive
multi-modality prompt learning to address the above issues. To do this, we
employ previous text prompt learning and propose a new image prompt learning.
The image prompt learning achieves in-sample and out-of-sample generalization,
by first masking meaningless patches and then padding them with the learnable
parameters and the information from texts. Moreover, each of the prompts
provides auxiliary information to each other, further strengthening these two
kinds of generalization. Experimental results on real datasets demonstrate that
our method outperforms SOTA methods, in terms of different downstream tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00824">Variational Self-Supervised Contrastive Learning Using Beta Divergence. (arXiv:2312.00824v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yavuz_M/0/1/0/all/0/1">Mehmet Can Yavuz</a>, <a href="http://arxiv.org/find/cs/1/au:+Yanikoglu_B/0/1/0/all/0/1">Berrin Yanikoglu</a></p>
<p>Learning a discriminative semantic space using unlabelled and noisy data
remains unaddressed in a multi-label setting. We present a contrastive
self-supervised learning method which is robust to data noise, grounded in the
domain of variational methods. The method (VCL) utilizes variational
contrastive learning with beta-divergence to learn robustly from unlabelled
datasets, including uncurated and noisy datasets. We demonstrate the
effectiveness of the proposed method through rigorous experiments including
linear evaluation and fine-tuning scenarios with multi-label datasets in the
face understanding domain. In almost all tested scenarios, VCL surpasses the
performance of state-of-the-art self-supervised methods, achieving a noteworthy
increase in accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00839">PipeOptim: Ensuring Effective 1F1B Schedule with Optimizer-Dependent Weight Prediction. (arXiv:2312.00839v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guan_L/0/1/0/all/0/1">Lei Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dongsheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1">Jiye Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenjian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1">Xicheng Lu</a></p>
<p>Asynchronous pipeline model parallelism with a "1F1B" (one forward, one
backward) schedule generates little bubble overhead and always provides quite a
high throughput. However, the "1F1B" schedule inevitably leads to weight
inconsistency and weight staleness issues due to the cross-training of
different mini-batches across GPUs. To simultaneously address these two
problems, in this paper, we propose an optimizer-dependent weight prediction
strategy (a.k.a PipeOptim) for asynchronous pipeline training. The key insight
of our proposal is that we employ a weight prediction strategy in the forward
pass to ensure that each mini-batch uses consistent and staleness-free weights
to compute the forward pass. To be concrete, we first construct the weight
prediction scheme based on the update rule of the used optimizer when training
the deep neural network models. Then throughout the "1F1B" pipelined training,
each mini-batch is mandated to execute weight prediction ahead of the forward
pass, subsequently employing the predicted weights to perform the forward pass.
As a result, PipeOptim 1) inherits the advantage of the "1F1B" schedule and
generates pretty high throughput, and 2) can ensure effective parameter
learning regardless of the type of the used optimizer. To verify the
effectiveness of our proposal, we conducted extensive experimental evaluations
using eight different deep-learning models spanning three machine-learning
tasks including image classification, sentiment analysis, and machine
translation. The experiment results demonstrate that PipeOptim outperforms the
popular pipelined approaches including GPipe, PipeDream, PipeDream-2BW, and
SpecTrain. The code of PipeOptim will be accessible at
https://github.com/guanleics/PipeOptim.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00840">Towards Redundancy-Free Sub-networks in Continual Learning. (arXiv:2312.00840v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Cheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1">Jingkuan Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1">LianLi Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1">Heng Tao Shen</a></p>
<p>Catastrophic Forgetting (CF) is a prominent issue in continual learning.
Parameter isolation addresses this challenge by masking a sub-network for each
task to mitigate interference with old tasks. However, these sub-networks are
constructed relying on weight magnitude, which does not necessarily correspond
to the importance of weights, resulting in maintaining unimportant weights and
constructing redundant sub-networks. To overcome this limitation, inspired by
information bottleneck, which removes redundancy between adjacent network
layers, we propose \textbf{\underline{I}nformation \underline{B}ottleneck
\underline{M}asked sub-network (IBM)} to eliminate redundancy within
sub-networks. Specifically, IBM accumulates valuable information into essential
weights to construct redundancy-free sub-networks, not only effectively
mitigating CF by freezing the sub-networks but also facilitating new tasks
training through the transfer of valuable knowledge. Additionally, IBM
decomposes hidden representations to automate the construction process and make
it flexible. Extensive experiments demonstrate that IBM consistently
outperforms state-of-the-art methods. Notably, IBM surpasses the
state-of-the-art parameter isolation method with a 70\% reduction in the number
of parameters within sub-networks and an 80\% decrease in training time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00842">ESM-NBR: fast and accurate nucleic acid-binding residue prediction via protein language model feature representation and multi-task learning. (arXiv:2312.00842v1 [q-bio.QM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Zeng_W/0/1/0/all/0/1">Wenwu Zeng</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Lv_D/0/1/0/all/0/1">Dafeng Lv</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Liu_W/0/1/0/all/0/1">Wenjuan Liu</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Peng_S/0/1/0/all/0/1">Shaoliang Peng</a></p>
<p>Protein-nucleic acid interactions play a very important role in a variety of
biological activities. Accurate identification of nucleic acid-binding residues
is a critical step in understanding the interaction mechanisms. Although many
computationally based methods have been developed to predict nucleic
acid-binding residues, challenges remain. In this study, a fast and accurate
sequence-based method, called ESM-NBR, is proposed. In ESM-NBR, we first use
the large protein language model ESM2 to extract discriminative biological
properties feature representation from protein primary sequences; then, a
multi-task deep learning model composed of stacked bidirectional long
short-term memory (BiLSTM) and multi-layer perceptron (MLP) networks is
employed to explore common and private information of DNA- and RNA-binding
residues with ESM2 feature as input. Experimental results on benchmark data
sets demonstrate that the prediction performance of ESM2 feature representation
comprehensively outperforms evolutionary information-based hidden Markov model
(HMM) features. Meanwhile, the ESM-NBR obtains the MCC values for DNA-binding
residues prediction of 0.427 and 0.391 on two independent test sets, which are
18.61 and 10.45% higher than those of the second-best methods, respectively.
Moreover, by completely discarding the time-cost multiple sequence alignment
process, the prediction speed of ESM-NBR far exceeds that of existing methods
(5.52s for a protein sequence of length 500, which is about 16 times faster
than the second-fastest method). A user-friendly standalone package and the
data of ESM-NBR are freely available for academic use at:
https://github.com/wwzll123/ESM-NBR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00843">Exploring the Robustness of Decentralized Training for Large Language Models. (arXiv:2312.00843v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1">Lin Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_C/0/1/0/all/0/1">Chenxi Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_W/0/1/0/all/0/1">Wangcheng Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1">Binhang Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yanan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1">Pan Zhou</a></p>
<p>Decentralized training of large language models has emerged as an effective
way to democratize this technology. However, the potential threats associated
with this approach have not been carefully discussed, which would hinder the
development of decentralized training infrastructures. This paper aims to
initiate discussion towards this end by exploring the robustness of
decentralized training from three main perspectives. First, we demonstrate the
vulnerabilities inherent in decentralized training frameworks in terms of
hardware, data, and models. Second, we highlight the fundamental difference
between decentralized foundation model training and vanilla federated learning,
where the security techniques employed in federated learning cannot be applied
directly. Third, we discuss the essential components required for a robust and
efficient decentralized training framework and present a case study by modeling
a concrete threat model. Our objective in this vision paper is to emphasize the
importance of addressing security concerns in the context of decentralized
training for large language models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00845">VMC: Video Motion Customization using Temporal Attention Adaption for Text-to-Video Diffusion Models. (arXiv:2312.00845v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jeong_H/0/1/0/all/0/1">Hyeonho Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_G/0/1/0/all/0/1">Geon Yeong Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1">Jong Chul Ye</a></p>
<p>Text-to-video diffusion models have advanced video generation significantly.
However, customizing these models to generate videos with tailored motions
presents a substantial challenge. In specific, they encounter hurdles in (a)
accurately reproducing motion from a target video, and (b) creating diverse
visual variations. For example, straightforward extensions of static image
customization methods to video often lead to intricate entanglements of
appearance and motion data. To tackle this, here we present the Video Motion
Customization (VMC) framework, a novel one-shot tuning approach crafted to
adapt temporal attention layers within video diffusion models. Our approach
introduces a novel motion distillation objective using residual vectors between
consecutive frames as a motion reference. The diffusion process then preserves
low-frequency motion trajectories while mitigating high-frequency
motion-unrelated noise in image space. We validate our method against
state-of-the-art video generative models across diverse real-world motions and
contexts. Our codes, data and the project demo can be found at
https://video-motion-customization.github.io
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00851">Physics Inspired Criterion for Pruning-Quantization Joint Learning. (arXiv:2312.00851v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1">Weiying Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1">Xiaoyi Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yunsong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1">Jie Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1">Leyuan Fang</a></p>
<p>Pruning-quantization joint learning always facilitates the deployment of deep
neural networks (DNNs) on resource-constrained edge devices. However, most
existing methods do not jointly learn a global criterion for pruning and
quantization in an interpretable way. In this paper, we propose a novel physics
inspired criterion for pruning-quantization joint learning (PIC-PQ), which is
explored from an analogy we first draw between elasticity dynamics (ED) and
model compression (MC). Specifically, derived from Hooke's law in ED, we
establish a linear relationship between the filters' importance distribution
and the filter property (FP) by a learnable deformation scale in the physics
inspired criterion (PIC). Furthermore, we extend PIC with a relative shift
variable for a global view. To ensure feasibility and flexibility, available
maximum bitwidth and penalty factor are introduced in quantization bitwidth
assignment. Experiments on benchmarks of image classification demonstrate that
PIC-PQ yields a good trade-off between accuracy and bit-operations (BOPs)
compression ratio e.g., 54.96X BOPs compression ratio in ResNet56 on CIFAR10
with 0.10% accuracy drop and 53.24X in ResNet18 on ImageNet with 0.61% accuracy
drop). The code will be available at https://github.com/fanxxxxyi/PIC-PQ.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00852">Beyond First-Order Tweedie: Solving Inverse Problems using Latent Diffusion. (arXiv:2312.00852v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rout_L/0/1/0/all/0/1">Litu Rout</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yujia Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1">Abhishek Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Caramanis_C/0/1/0/all/0/1">Constantine Caramanis</a>, <a href="http://arxiv.org/find/cs/1/au:+Shakkottai_S/0/1/0/all/0/1">Sanjay Shakkottai</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1">Wen-Sheng Chu</a></p>
<p>Sampling from the posterior distribution poses a major computational
challenge in solving inverse problems using latent diffusion models. Common
methods rely on Tweedie's first-order moments, which are known to induce a
quality-limiting bias. Existing second-order approximations are impractical due
to prohibitive computational costs, making standard reverse diffusion processes
intractable for posterior sampling. This paper introduces Second-order Tweedie
sampler from Surrogate Loss (STSL), a novel sampler that offers efficiency
comparable to first-order Tweedie with a tractable reverse process using
second-order approximation. Our theoretical results reveal that the
second-order approximation is lower bounded by our surrogate loss that only
requires $O(1)$ compute using the trace of the Hessian, and by the lower bound
we derive a new drift term to make the reverse process tractable. Our method
surpasses SoTA solvers PSLD and P2L, achieving 4X and 8X reduction in neural
function evaluations, respectively, while notably enhancing sampling quality on
FFHQ, ImageNet, and COCO benchmarks. In addition, we show STSL extends to
text-guided image editing and addresses residual distortions present from
corrupted images in leading text-guided image editing methods. To our best
knowledge, this is the first work to offer an efficient second-order
approximation in solving inverse problems using latent diffusion and editing
real-world images with corruptions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00854">A Probabilistic Neural Twin for Treatment Planning in Peripheral Pulmonary Artery Stenosis. (arXiv:2312.00854v1 [physics.med-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Lee_J/0/1/0/all/0/1">John D. Lee</a>, <a href="http://arxiv.org/find/physics/1/au:+Richter_J/0/1/0/all/0/1">Jakob Richter</a>, <a href="http://arxiv.org/find/physics/1/au:+Pfaller_M/0/1/0/all/0/1">Martin R. Pfaller</a>, <a href="http://arxiv.org/find/physics/1/au:+Szafron_J/0/1/0/all/0/1">Jason M. Szafron</a>, <a href="http://arxiv.org/find/physics/1/au:+Menon_K/0/1/0/all/0/1">Karthik Menon</a>, <a href="http://arxiv.org/find/physics/1/au:+Zanoni_A/0/1/0/all/0/1">Andrea Zanoni</a>, <a href="http://arxiv.org/find/physics/1/au:+Ma_M/0/1/0/all/0/1">Michael R. Ma</a>, <a href="http://arxiv.org/find/physics/1/au:+Feinstein_J/0/1/0/all/0/1">Jeffrey A. Feinstein</a>, <a href="http://arxiv.org/find/physics/1/au:+Kreutzer_J/0/1/0/all/0/1">Jacqueline Kreutzer</a>, <a href="http://arxiv.org/find/physics/1/au:+Marsden_A/0/1/0/all/0/1">Alison L. Marsden</a>, <a href="http://arxiv.org/find/physics/1/au:+Schiavazzi_D/0/1/0/all/0/1">Daniele E. Schiavazzi</a></p>
<p>The substantial computational cost of high-fidelity models in numerical
hemodynamics has, so far, relegated their use mainly to offline treatment
planning. New breakthroughs in data-driven architectures and optimization
techniques for fast surrogate modeling provide an exciting opportunity to
overcome these limitations, enabling the use of such technology for
time-critical decisions. We discuss an application to the repair of multiple
stenosis in peripheral pulmonary artery disease through either transcatheter
pulmonary artery rehabilitation or surgery, where it is of interest to achieve
desired pressures and flows at specific locations in the pulmonary artery tree,
while minimizing the risk for the patient. Since different degrees of success
can be achieved in practice during treatment, we formulate the problem in
probability, and solve it through a sample-based approach. We propose a new
offline-online pipeline for probabilsitic real-time treatment planning which
combines offline assimilation of boundary conditions, model reduction, and
training dataset generation with online estimation of marginal probabilities,
possibly conditioned on the degree of augmentation observed in already repaired
lesions. Moreover, we propose a new approach for the parametrization of
arbitrarily shaped vascular repairs through iterative corrections of a
zero-dimensional approximant. We demonstrate this pipeline for a diseased model
of the pulmonary artery tree available through the Vascular Model Repository.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00855">Refine, Discriminate and Align: Stealing Encoders via Sample-Wise Prototypes and Multi-Relational Extraction. (arXiv:2312.00855v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Shuchi Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1">Chuan Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1">Kang Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiaogang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1">Ming Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1">Yuwen Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1">Tao Xiang</a></p>
<p>This paper introduces RDA, a pioneering approach designed to address two
primary deficiencies prevalent in previous endeavors aiming at stealing
pre-trained encoders: (1) suboptimal performances attributed to biased
optimization objectives, and (2) elevated query costs stemming from the
end-to-end paradigm that necessitates querying the target encoder every epoch.
Specifically, we initially Refine the representations of the target encoder for
each training sample, thereby establishing a less biased optimization objective
before the steal-training phase. This is accomplished via a sample-wise
prototype, which consolidates the target encoder's representations for a given
sample's various perspectives. Demanding exponentially fewer queries compared
to the end-to-end approach, prototypes can be instantiated to guide subsequent
query-free training. For more potent efficacy, we develop a multi-relational
extraction loss that trains the surrogate encoder to Discriminate mismatched
embedding-prototype pairs while Aligning those matched ones in terms of both
amplitude and angle. In this way, the trained surrogate encoder achieves
state-of-the-art results across the board in various downstream datasets with
limited queries. Moreover, RDA is shown to be robust to multiple widely-used
defenses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00857">Latent Space Explorer: Visual Analytics for Multimodal Latent Space Exploration. (arXiv:2312.00857v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kwon_B/0/1/0/all/0/1">Bum Chul Kwon</a>, <a href="http://arxiv.org/find/cs/1/au:+Friedman_S/0/1/0/all/0/1">Samuel Friedman</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Kai Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lubitz_S/0/1/0/all/0/1">Steven A Lubitz</a>, <a href="http://arxiv.org/find/cs/1/au:+Philippakis_A/0/1/0/all/0/1">Anthony Philippakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Batra_P/0/1/0/all/0/1">Puneet Batra</a>, <a href="http://arxiv.org/find/cs/1/au:+Ellinor_P/0/1/0/all/0/1">Patrick T Ellinor</a>, <a href="http://arxiv.org/find/cs/1/au:+Ng_K/0/1/0/all/0/1">Kenney Ng</a></p>
<p>Machine learning models built on training data with multiple modalities can
reveal new insights that are not accessible through unimodal datasets. For
example, cardiac magnetic resonance images (MRIs) and electrocardiograms (ECGs)
are both known to capture useful information about subjects' cardiovascular
health status. A multimodal machine learning model trained from large datasets
can potentially predict the onset of heart-related diseases and provide novel
medical insights about the cardiovascular system. Despite the potential
benefits, it is difficult for medical experts to explore multimodal
representation models without visual aids and to test the predictive
performance of the models on various subpopulations. To address the challenges,
we developed a visual analytics system called Latent Space Explorer. Latent
Space Explorer provides interactive visualizations that enable users to explore
the multimodal representation of subjects, define subgroups of interest,
interactively decode data with different modalities with the selected subjects,
and inspect the accuracy of the embedding in downstream prediction tasks. A
user study was conducted with medical experts and their feedback provided
useful insights into how Latent Space Explorer can help their analysis and
possible new direction for further development in the medical domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00870">3DiFACE: Diffusion-based Speech-driven 3D Facial Animation and Editing. (arXiv:2312.00870v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Thambiraja_B/0/1/0/all/0/1">Balamurugan Thambiraja</a>, <a href="http://arxiv.org/find/cs/1/au:+Aliakbarian_S/0/1/0/all/0/1">Sadegh Aliakbarian</a>, <a href="http://arxiv.org/find/cs/1/au:+Cosker_D/0/1/0/all/0/1">Darren Cosker</a>, <a href="http://arxiv.org/find/cs/1/au:+Thies_J/0/1/0/all/0/1">Justus Thies</a></p>
<p>We present 3DiFACE, a novel method for personalized speech-driven 3D facial
animation and editing. While existing methods deterministically predict facial
animations from speech, they overlook the inherent one-to-many relationship
between speech and facial expressions, i.e., there are multiple reasonable
facial expression animations matching an audio input. It is especially
important in content creation to be able to modify generated motion or to
specify keyframes. To enable stochasticity as well as motion editing, we
propose a lightweight audio-conditioned diffusion model for 3D facial motion.
This diffusion model can be trained on a small 3D motion dataset, maintaining
expressive lip motion output. In addition, it can be finetuned for specific
subjects, requiring only a short video of the person. Through quantitative and
qualitative evaluations, we show that our method outperforms existing
state-of-the-art techniques and yields speech-driven animations with greater
fidelity and diversity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00886">Nash Learning from Human Feedback. (arXiv:2312.00886v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Munos_R/0/1/0/all/0/1">R&#xe9;mi Munos</a>, <a href="http://arxiv.org/find/stat/1/au:+Valko_M/0/1/0/all/0/1">Michal Valko</a>, <a href="http://arxiv.org/find/stat/1/au:+Calandriello_D/0/1/0/all/0/1">Daniele Calandriello</a>, <a href="http://arxiv.org/find/stat/1/au:+Azar_M/0/1/0/all/0/1">Mohammad Gheshlaghi Azar</a>, <a href="http://arxiv.org/find/stat/1/au:+Rowland_M/0/1/0/all/0/1">Mark Rowland</a>, <a href="http://arxiv.org/find/stat/1/au:+Guo_D/0/1/0/all/0/1">Daniel Guo</a>, <a href="http://arxiv.org/find/stat/1/au:+Tang_Y/0/1/0/all/0/1">Yunhao Tang</a>, <a href="http://arxiv.org/find/stat/1/au:+Geist_M/0/1/0/all/0/1">Matthieu Geist</a>, <a href="http://arxiv.org/find/stat/1/au:+Mesnard_T/0/1/0/all/0/1">Thomas M&#xe9;snard</a>, <a href="http://arxiv.org/find/stat/1/au:+Michi_A/0/1/0/all/0/1">Andrea Michi</a>, <a href="http://arxiv.org/find/stat/1/au:+Selvi_M/0/1/0/all/0/1">Marco Selvi</a>, <a href="http://arxiv.org/find/stat/1/au:+Girgin_S/0/1/0/all/0/1">Sertan Girgin</a>, <a href="http://arxiv.org/find/stat/1/au:+Momchev_N/0/1/0/all/0/1">Nikola Momchev</a>, <a href="http://arxiv.org/find/stat/1/au:+Bachem_O/0/1/0/all/0/1">Olivier Bachem</a>, <a href="http://arxiv.org/find/stat/1/au:+Mankowitz_D/0/1/0/all/0/1">Daniel J. Mankowitz</a>, <a href="http://arxiv.org/find/stat/1/au:+Precup_D/0/1/0/all/0/1">Doina Precup</a>, <a href="http://arxiv.org/find/stat/1/au:+Piot_B/0/1/0/all/0/1">Bilal Piot</a></p>
<p>Reinforcement learning from human feedback (RLHF) has emerged as the main
paradigm for aligning large language models (LLMs) with human preferences.
Typically, RLHF involves the initial step of learning a reward model from human
feedback, often expressed as preferences between pairs of text generations
produced by a pre-trained LLM. Subsequently, the LLM's policy is fine-tuned by
optimizing it to maximize the reward model through a reinforcement learning
algorithm. However, an inherent limitation of current reward models is their
inability to fully represent the richness of human preferences and their
dependency on the sampling distribution.
</p>
<p>In this study, we introduce an alternative pipeline for the fine-tuning of
LLMs using pairwise human feedback. Our approach entails the initial learning
of a preference model, which is conditioned on two inputs given a prompt,
followed by the pursuit of a policy that consistently generates responses
preferred over those generated by any competing policy, thus defining the Nash
equilibrium of this preference model. We term this approach Nash learning from
human feedback (NLHF).
</p>
<p>In the context of a tabular policy representation, we present a novel
algorithmic solution, Nash-MD, founded on the principles of mirror descent.
This algorithm produces a sequence of policies, with the last iteration
converging to the regularized Nash equilibrium. Additionally, we explore
parametric representations of policies and introduce gradient descent
algorithms for deep-learning architectures. To demonstrate the effectiveness of
our approach, we present experimental results involving the fine-tuning of a
LLM for a text summarization task. We believe NLHF offers a compelling avenue
for preference learning and policy optimization with the potential of advancing
the field of aligning LLMs with human preferences.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00907">Extreme Event Prediction with Multi-agent Reinforcement Learning-based Parametrization of Atmospheric and Oceanic Turbulence. (arXiv:2312.00907v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mojgani_R/0/1/0/all/0/1">Rambod Mojgani</a>, <a href="http://arxiv.org/find/cs/1/au:+Waelchli_D/0/1/0/all/0/1">Daniel Waelchli</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1">Yifei Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Koumoutsakos_P/0/1/0/all/0/1">Petros Koumoutsakos</a>, <a href="http://arxiv.org/find/cs/1/au:+Hassanzadeh_P/0/1/0/all/0/1">Pedram Hassanzadeh</a></p>
<p>Global climate models (GCMs) are the main tools for understanding and
predicting climate change. However, due to limited numerical resolutions, these
models suffer from major structural uncertainties; e.g., they cannot resolve
critical processes such as small-scale eddies in atmospheric and oceanic
turbulence. Thus, such small-scale processes have to be represented as a
function of the resolved scales via closures (parametrization). The accuracy of
these closures is particularly important for capturing climate extremes.
Traditionally, such closures are based on heuristics and simplifying
assumptions about the unresolved physics. Recently, supervised-learned
closures, trained offline on high-fidelity data, have been shown to outperform
the classical physics-based closures. However, this approach requires a
significant amount of high-fidelity training data and can also lead to
instabilities. Reinforcement learning is emerging as a potent alternative for
developing such closures as it requires only low-order statistics and leads to
stable closures. In Scientific Multi-Agent Reinforcement Learning (SMARL)
computational elements serve a dual role of discretization points and learning
agents. We leverage SMARL and fundamentals of turbulence physics to learn
closures for prototypes of atmospheric and oceanic turbulence. The policy is
trained using only the enstrophy spectrum, which is nearly invariant and can be
estimated from a few high-fidelity samples (these few samples are far from
enough for supervised/offline learning). We show that these closures lead to
stable low-resolution simulations that, at a fraction of the cost, can
reproduce the high-fidelity simulations' statistics, including the tails of the
probability density functions. The results demonstrate the high potential of
SMARL for closure modeling for GCMs, especially in the regime of scarce data
and indirect observations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00910">Effectiveness of probabilistic contact tracing in epidemic containment: the role of super-spreaders and transmission paths reconstruction. (arXiv:2312.00910v1 [q-bio.PE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Muntoni_A/0/1/0/all/0/1">A.P. Muntoni</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Mazza_F/0/1/0/all/0/1">F. Mazza</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Braunstein_A/0/1/0/all/0/1">A. Braunstein</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Catania_G/0/1/0/all/0/1">G. Catania</a>, <a href="http://arxiv.org/find/q-bio/1/au:+DallAsta_L/0/1/0/all/0/1">L. Dall&#x27;Asta</a></p>
<p>The recent COVID-19 pandemic underscores the significance of early-stage
non-pharmacological intervention strategies. The widespread use of masks and
the systematic implementation of contact tracing strategies provide a
potentially equally effective and socially less impactful alternative to more
conventional approaches, such as large-scale mobility restrictions. However,
manual contact tracing faces strong limitations in accessing the network of
contacts, and the scalability of currently implemented protocols for
smartphone-based digital contact tracing becomes impractical during the rapid
expansion phases of the outbreaks, due to the surge in exposure notifications
and associated tests. A substantial improvement in digital contact tracing can
be obtained through the integration of probabilistic techniques for risk
assessment that can more effectively guide the allocation of new diagnostic
tests. In this study, we first quantitatively analyze the diagnostic and social
costs associated with these containment measures based on contact tracing,
employing three state-of-the-art models of SARS-CoV-2 spreading. Our results
suggest that probabilistic techniques allow for more effective mitigation at a
lower cost. Secondly, our findings reveal a remarkable efficacy of
probabilistic contact-tracing techniques in capturing backward propagations and
super-spreading events, relevant features of the diffusion of many pathogens,
including SARS-CoV-2.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00912">Quick Back-Translation for Unsupervised Machine Translation. (arXiv:2312.00912v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Brimacombe_B/0/1/0/all/0/1">Benjamin Brimacombe</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jiawei Zhou</a></p>
<p>The field of unsupervised machine translation has seen significant
advancement from the marriage of the Transformer and the back-translation
algorithm. The Transformer is a powerful generative model, and back-translation
leverages Transformer's high-quality translations for iterative
self-improvement. However, the Transformer is encumbered by the run-time of
autoregressive inference during back-translation, and back-translation is
limited by a lack of synthetic data efficiency. We propose a two-for-one
improvement to Transformer back-translation: Quick Back-Translation (QBT). QBT
re-purposes the encoder as a generative model, and uses encoder-generated
sequences to train the decoder in conjunction with the original autoregressive
back-translation step, improving data throughput and utilization. Experiments
on various WMT benchmarks demonstrate that a relatively small number of
refining steps of QBT improve current unsupervised machine translation models,
and that QBT dramatically outperforms standard back-translation only method in
terms of training efficiency for comparable translation qualities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00918">PACE: A Program Analysis Framework for Continuous Performance Prediction. (arXiv:2312.00918v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Biringa_C/0/1/0/all/0/1">Chidera Biringa</a>, <a href="http://arxiv.org/find/cs/1/au:+Kul_G/0/1/0/all/0/1">Gokhan Kul</a></p>
<p>Software development teams establish elaborate continuous integration
pipelines containing automated test cases to accelerate the development process
of software. Automated tests help to verify the correctness of code
modifications decreasing the response time to changing requirements. However,
when the software teams do not track the performance impact of pending
modifications, they may need to spend considerable time refactoring existing
code. This paper presents PACE, a program analysis framework that provides
continuous feedback on the performance impact of pending code updates. We
design performance microbenchmarks by mapping the execution time of functional
test cases given a code update. We map microbenchmarks to code stylometry
features and feed them to predictors for performance predictions. Our
experiments achieved significant performance in predicting code performance,
outperforming current state-of-the-art by 75% on neural-represented code
stylometry features.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00923">Label Delay in Continual Learning. (arXiv:2312.00923v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Csaba_B/0/1/0/all/0/1">Botos Csaba</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenxuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Muller_M/0/1/0/all/0/1">Matthias M&#xfc;ller</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1">Ser-Nam Lim</a>, <a href="http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1">Mohamed Elhoseiny</a>, <a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1">Philip Torr</a>, <a href="http://arxiv.org/find/cs/1/au:+Bibi_A/0/1/0/all/0/1">Adel Bibi</a></p>
<p>Online continual learning, the process of training models on streaming data,
has gained increasing attention in recent years. However, a critical aspect
often overlooked is the label delay, where new data may not be labeled due to
slow and costly annotation processes. We introduce a new continual learning
framework with explicit modeling of the label delay between data and label
streams over time steps. In each step, the framework reveals both unlabeled
data from the current time step $t$ and labels delayed with $d$ steps, from the
time step $t-d$. In our extensive experiments amounting to 1060 GPU days, we
show that merely augmenting the computational resources is insufficient to
tackle this challenge. Our findings underline a notable performance decline
when solely relying on labeled data when the label delay becomes significant.
More surprisingly, when using state-of-the-art SSL and TTA techniques to
utilize the newer, unlabeled data, they fail to surpass the performance of a
na\"ive method that simply trains on the delayed supervised stream. To this
end, we introduce a simple, efficient baseline that rehearses from the labeled
memory samples that are most similar to the new unlabeled samples. This method
bridges the accuracy gap caused by label delay without significantly increasing
computational complexity. We show experimentally that our method is the least
affected by the label delay factor and in some cases successfully recovers the
accuracy of the non-delayed counterpart. We conduct various ablations and
sensitivity experiments, demonstrating the effectiveness of our approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00935">A Theory of Unimodal Bias in Multimodal Learning. (arXiv:2312.00935v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yedi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Latham_P/0/1/0/all/0/1">Peter E. Latham</a>, <a href="http://arxiv.org/find/cs/1/au:+Saxe_A/0/1/0/all/0/1">Andrew Saxe</a></p>
<p>Using multiple input streams simultaneously in training multimodal neural
networks is intuitively advantageous, but practically challenging. A key
challenge is unimodal bias, where a network overly relies on one modality and
ignores others during joint training. While unimodal bias is well-documented
empirically, our theoretical understanding of how architecture and data
statistics influence this bias remains incomplete. Here we develop a theory of
unimodal bias with deep multimodal linear networks. We calculate the duration
of the unimodal phase in learning as a function of the depth at which
modalities are fused within the network, dataset statistics, and
initialization. We find that the deeper the layer at which fusion occurs, the
longer the unimodal phase. A long unimodal phase can lead to a generalization
deficit and permanent unimodal bias in the overparametrized regime. In
addition, our theory reveals the modality learned first is not necessarily the
modality that contributes more to the output. Our results, derived for
multimodal linear networks, extend to ReLU networks in certain settings. Taken
together, this work illuminates pathologies of multimodal learning under joint
training, showing that late and intermediate fusion architectures can give rise
to long unimodal phases and permanent unimodal bias.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00960">The Cost of Compression: Investigating the Impact of Compression on Parametric Knowledge in Language Models. (arXiv:2312.00960v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Namburi_S/0/1/0/all/0/1">Satya Sai Srinath Namburi</a>, <a href="http://arxiv.org/find/cs/1/au:+Sreedhar_M/0/1/0/all/0/1">Makesh Sreedhar</a>, <a href="http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1">Srinath Srinivasan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sala_F/0/1/0/all/0/1">Frederic Sala</a></p>
<p>Compressing large language models (LLMs), often consisting of billions of
parameters, provides faster inference, smaller memory footprints, and enables
local deployment. Two standard compression techniques are pruning and
quantization, with the former eliminating redundant connections in model layers
and the latter representing model parameters with fewer bits. The key tradeoff
is between the degree of compression and the impact on the quality of the
compressed model. Existing research on LLM compression primarily focuses on
performance in terms of general metrics like perplexity or downstream task
accuracy. More fine-grained metrics, such as those measuring parametric
knowledge, remain significantly underexplored. To help bridge this gap, we
present a comprehensive analysis across multiple model families (ENCODER,
ENCODER-DECODER, and DECODER) using the LAMA and LM-HARNESS benchmarks in order
to systematically quantify the effect of commonly employed compression
techniques on model performance. A particular focus is on tradeoffs involving
parametric knowledge, with the goal of providing practitioners with practical
insights to help make informed decisions on compression. We release our
codebase1 to enable further research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00963">Spatiotemporal Transformer for Imputing Sparse Data: A Deep Learning Approach. (arXiv:2312.00963v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_K/0/1/0/all/0/1">Kehui Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jingyi Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jun Zhu</a></p>
<p>Effective management of environmental resources and agricultural
sustainability heavily depends on accurate soil moisture data. However,
datasets like the SMAP/Sentinel-1 soil moisture product often contain missing
values across their spatiotemporal grid, which poses a significant challenge.
This paper introduces a novel Spatiotemporal Transformer model (ST-Transformer)
specifically designed to address the issue of missing values in sparse
spatiotemporal datasets, particularly focusing on soil moisture data. The
ST-Transformer employs multiple spatiotemporal attention layers to capture the
complex spatiotemporal correlations in the data and can integrate additional
spatiotemporal covariates during the imputation process, thereby enhancing its
accuracy. The model is trained using a self-supervised approach, enabling it to
autonomously predict missing values from observed data points. Our model's
efficacy is demonstrated through its application to the SMAP 1km soil moisture
data over a 36 x 36 km grid in Texas. It showcases superior accuracy compared
to well-known imputation methods. Additionally, our simulation studies on other
datasets highlight the model's broader applicability in various spatiotemporal
imputation tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00966">Spectral Temporal Contrastive Learning. (arXiv:2312.00966v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Morin_S/0/1/0/all/0/1">Sacha Morin</a>, <a href="http://arxiv.org/find/cs/1/au:+Nath_S/0/1/0/all/0/1">Somjit Nath</a>, <a href="http://arxiv.org/find/cs/1/au:+Kahou_S/0/1/0/all/0/1">Samira Ebrahimi Kahou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wolf_G/0/1/0/all/0/1">Guy Wolf</a></p>
<p>Learning useful data representations without requiring labels is a
cornerstone of modern deep learning. Self-supervised learning methods,
particularly contrastive learning (CL), have proven successful by leveraging
data augmentations to define positive pairs. This success has prompted a number
of theoretical studies to better understand CL and investigate theoretical
bounds for downstream linear probing tasks. This work is concerned with the
temporal contrastive learning (TCL) setting where the sequential structure of
the data is used instead to define positive pairs, which is more commonly used
in RL and robotics contexts. In this paper, we adapt recent work on Spectral CL
to formulate Spectral Temporal Contrastive Learning (STCL). We discuss a
population loss based on a state graph derived from a time-homogeneous
reversible Markov chain with uniform stationary distribution. The STCL loss
enables to connect the linear probing performance to the spectral properties of
the graph, and can be estimated by considering previously observed data
sequences as an ensemble of MCMC chains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00975">Noisy probing dose facilitated dose prediction for pencil beam scanning proton therapy: physics enhances generalizability. (arXiv:2312.00975v1 [physics.med-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Zhang_L/0/1/0/all/0/1">Lian Zhang</a>, <a href="http://arxiv.org/find/physics/1/au:+Holmes_J/0/1/0/all/0/1">Jason M. Holmes</a>, <a href="http://arxiv.org/find/physics/1/au:+Liu_Z/0/1/0/all/0/1">Zhengliang Liu</a>, <a href="http://arxiv.org/find/physics/1/au:+Feng_H/0/1/0/all/0/1">Hongying Feng</a>, <a href="http://arxiv.org/find/physics/1/au:+Sio_T/0/1/0/all/0/1">Terence T. Sio</a>, <a href="http://arxiv.org/find/physics/1/au:+Vargas_C/0/1/0/all/0/1">Carlos E. Vargas</a>, <a href="http://arxiv.org/find/physics/1/au:+Keole_S/0/1/0/all/0/1">Sameer R. Keole</a>, <a href="http://arxiv.org/find/physics/1/au:+Stutzer_K/0/1/0/all/0/1">Kristin St&#xfc;tzer</a>, <a href="http://arxiv.org/find/physics/1/au:+Li_S/0/1/0/all/0/1">Sheng Li</a>, <a href="http://arxiv.org/find/physics/1/au:+Liu_T/0/1/0/all/0/1">Tianming Liu</a>, <a href="http://arxiv.org/find/physics/1/au:+Shen_J/0/1/0/all/0/1">Jiajian Shen</a>, <a href="http://arxiv.org/find/physics/1/au:+Wong_W/0/1/0/all/0/1">William W. Wong</a>, <a href="http://arxiv.org/find/physics/1/au:+Vora_S/0/1/0/all/0/1">Sujay A. Vora</a>, <a href="http://arxiv.org/find/physics/1/au:+Liu_W/0/1/0/all/0/1">Wei Liu</a></p>
<p>Purpose: Prior AI-based dose prediction studies in photon and proton therapy
often neglect underlying physics, limiting their generalizability to handle
outlier clinical cases, especially for pencil beam scanning proton therapy
(PBSPT). Our aim is to design a physics-aware and generalizable AI-based PBSPT
dose prediction method that has the underlying physics considered to achieve
high generalizability to properly handle the outlier clinical cases. Methods
and Materials: This study analyzed PBSPT plans of 103 prostate and 78 lung
cancer patients from our institution,with each case comprising CT images,
structure sets, and plan doses from our Monte-Carlo dose engine (serving as the
ground truth). Three methods were evaluated in the ablation study: the
ROI-based method, the beam mask and sliding window method, and the noisy
probing dose method. Twelve cases with uncommon beam angles or prescription
doses tested the methods' generalizability to rare treatment planning
scenarios. Performance evaluation used DVH indices, 3D Gamma passing rates
(3%/2mm/10%), and dice coefficients for dose agreement. Results: The noisy
probing dose method showed improved agreement of DVH indices, 3D Gamma passing
rates, and dice coefficients compared to the conventional methods for the
testing cases. The noisy probing dose method showed better generalizability in
the 6 outlier cases than the ROI-based and beam mask-based methods with 3D
Gamma passing rates (for prostate cancer, targets: 89.32%$\pm$1.45% vs.
93.48%$\pm$1.51% vs. 96.79%$\pm$0.83%, OARs: 85.87%$\pm$1.73% vs.
91.15%$\pm$1.13% vs. 94.29%$\pm$1.01%). The dose predictions were completed
within 0.3 seconds. Conclusions: We've devised a novel noisy probing dose
method for PBSPT dose prediction in prostate and lung cancer patients. With
more physics included, it enhances the generalizability of dose prediction in
handling outlier clinical cases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00991">Convergences for Minimax Optimization Problems over Infinite-Dimensional Spaces Towards Stability in Adversarial Training. (arXiv:2312.00991v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Furuya_T/0/1/0/all/0/1">Takashi Furuya</a>, <a href="http://arxiv.org/find/stat/1/au:+Okuda_S/0/1/0/all/0/1">Satoshi Okuda</a>, <a href="http://arxiv.org/find/stat/1/au:+Suetake_K/0/1/0/all/0/1">Kazuma Suetake</a>, <a href="http://arxiv.org/find/stat/1/au:+Sawada_Y/0/1/0/all/0/1">Yoshihide Sawada</a></p>
<p>Training neural networks that require adversarial optimization, such as
generative adversarial networks (GANs) and unsupervised domain adaptations
(UDAs), suffers from instability. This instability problem comes from the
difficulty of the minimax optimization, and there have been various approaches
in GANs and UDAs to overcome this problem. In this study, we tackle this
problem theoretically through a functional analysis. Specifically, we show the
convergence property of the minimax problem by the gradient descent over the
infinite-dimensional spaces of continuous functions and probability measures
under certain conditions. Using this setting, we can discuss GANs and UDAs
comprehensively, which have been studied independently. In addition, we show
that the conditions necessary for the convergence property are interpreted as
stabilization techniques of adversarial training such as the spectral
normalization and the gradient penalty.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00992">Improving Normative Modeling for Multi-modal Neuroimaging Data using mixture-of-product-of-experts variational autoencoders. (arXiv:2312.00992v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1">Sayantan Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Payne_P/0/1/0/all/0/1">Philip Payne</a>, <a href="http://arxiv.org/find/cs/1/au:+Sotiras_A/0/1/0/all/0/1">Aristeidis Sotiras</a></p>
<p>Normative models in neuroimaging learn the brain patterns of healthy
population distribution and estimate how disease subjects like Alzheimer's
Disease (AD) deviate from the norm. Existing variational autoencoder
(VAE)-based normative models using multimodal neuroimaging data aggregate
information from multiple modalities by estimating product or averaging of
unimodal latent posteriors. This can often lead to uninformative joint latent
distributions which affects the estimation of subject-level deviations. In this
work, we addressed the prior limitations by adopting the
Mixture-of-Product-of-Experts (MoPoE) technique which allows better modelling
of the joint latent posterior. Our model labelled subjects as outliers by
calculating deviations from the multimodal latent space. Further, we identified
which latent dimensions and brain regions were associated with abnormal
deviations due to AD pathology.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00995">Second-Order Uncertainty Quantification: A Distance-Based Approach. (arXiv:2312.00995v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sale_Y/0/1/0/all/0/1">Yusuf Sale</a>, <a href="http://arxiv.org/find/cs/1/au:+Bengs_V/0/1/0/all/0/1">Viktor Bengs</a>, <a href="http://arxiv.org/find/cs/1/au:+Caprio_M/0/1/0/all/0/1">Michele Caprio</a>, <a href="http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1">Eyke H&#xfc;llermeier</a></p>
<p>In the past couple of years, various approaches to representing and
quantifying different types of predictive uncertainty in machine learning,
notably in the setting of classification, have been proposed on the basis of
second-order probability distributions, i.e., predictions in the form of
distributions on probability distributions. A completely conclusive solution
has not yet been found, however, as shown by recent criticisms of commonly used
uncertainty measures associated with second-order distributions, identifying
undesirable theoretical properties of these measures. In light of these
criticisms, we propose a set of formal criteria that meaningful uncertainty
measures for predictive uncertainty based on second-order distributions should
obey. Moreover, we provide a general framework for developing uncertainty
measures to account for these criteria, and offer an instantiation based on the
Wasserstein distance, for which we prove that all criteria are satisfied.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01005">Generating Images of the M87* Black Hole Using GANs. (arXiv:2312.01005v1 [astro-ph.GA])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/astro-ph/1/au:+Mohan_A/0/1/0/all/0/1">Arya Mohan</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Protopapas_P/0/1/0/all/0/1">Pavlos Protopapas</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Kunnumkai_K/0/1/0/all/0/1">Keerthi Kunnumkai</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Garraffo_C/0/1/0/all/0/1">Cecilia Garraffo</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Blackburn_L/0/1/0/all/0/1">Lindy Blackburn</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Chatterjee_K/0/1/0/all/0/1">Koushik Chatterjee</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Doeleman_S/0/1/0/all/0/1">Sheperd S. Doeleman</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Emami_R/0/1/0/all/0/1">Razieh Emami</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Fromm_C/0/1/0/all/0/1">Christian M. Fromm</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Mizuno_Y/0/1/0/all/0/1">Yosuke Mizuno</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Ricarte_A/0/1/0/all/0/1">Angelo Ricarte</a></p>
<p>In this paper, we introduce a novel data augmentation methodology based on
Conditional Progressive Generative Adversarial Networks (CPGAN) to generate
diverse black hole (BH) images, accounting for variations in spin and electron
temperature prescriptions. These generated images are valuable resources for
training deep learning algorithms to accurately estimate black hole parameters
from observational data. Our model can generate BH images for any spin value
within the range of [-1, 1], given an electron temperature distribution. To
validate the effectiveness of our approach, we employ a convolutional neural
network to predict the BH spin using both the GRMHD images and the images
generated by our proposed model. Our results demonstrate a significant
performance improvement when training is conducted with the augmented dataset
while testing is performed using GRMHD simulated data, as indicated by the high
R2 score. Consequently, we propose that GANs can be employed as cost effective
models for black hole image generation and reliably augment training datasets
for other parameterization algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01017">Unveiling the Power of Audio-Visual Early Fusion Transformers with Dense Interactions through Masked Modeling. (arXiv:2312.01017v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1">Shentong Mo</a>, <a href="http://arxiv.org/find/cs/1/au:+Morgado_P/0/1/0/all/0/1">Pedro Morgado</a></p>
<p>Humans possess a remarkable ability to integrate auditory and visual
information, enabling a deeper understanding of the surrounding environment.
This early fusion of audio and visual cues, demonstrated through cognitive
psychology and neuroscience research, offers promising potential for developing
multimodal perception models. However, training early fusion architectures
poses significant challenges, as the increased model expressivity requires
robust learning frameworks to harness their enhanced capabilities. In this
paper, we address this challenge by leveraging the masked reconstruction
framework, previously successful in unimodal settings, to train audio-visual
encoders with early fusion. Additionally, we propose an attention-based fusion
module that captures interactions between local audio and visual
representations, enhancing the model's ability to capture fine-grained
interactions. While effective, this procedure can become computationally
intractable, as the number of local representations increases. Thus, to address
the computational complexity, we propose an alternative procedure that
factorizes the local representations before representing audio-visual
interactions. Extensive evaluations on a variety of datasets demonstrate the
superiority of our approach in audio-event classification, visual sound
localization, sound separation, and audio-visual segmentation. These
contributions enable the efficient training of deeply integrated audio-visual
models and significantly advance the usefulness of early fusion architectures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01020">ResNLS: An Improved Model for Stock Price Forecasting. (arXiv:2312.01020v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1">Yuanzhe Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Anaissi_A/0/1/0/all/0/1">Ali Anaissi</a>, <a href="http://arxiv.org/find/cs/1/au:+Suleiman_B/0/1/0/all/0/1">Basem Suleiman</a></p>
<p>Stock prices forecasting has always been a challenging task. Although many
research projects adopt machine learning and deep learning algorithms to
address the problem, few of them pay attention to the varying degrees of
dependencies between stock prices. In this paper we introduce a hybrid model
that improves stock price prediction by emphasizing the dependencies between
adjacent stock prices. The proposed model, ResNLS, is mainly composed of two
neural architectures, ResNet and LSTM. ResNet serves as a feature extractor to
identify dependencies between stock prices across time windows, while LSTM
analyses the initial time-series data with the combination of dependencies
which considered as residuals. In predicting the SSE Composite Index, our
experiment reveals that when the closing price data for the previous 5
consecutive trading days is used as the input, the performance of the model
(ResNLS-5) is optimal compared to those with other inputs. Furthermore,
ResNLS-5 outperforms vanilla CNN, RNN, LSTM, and BiLSTM models in terms of
prediction accuracy. It also demonstrates at least a 20% improvement over the
current state-of-the-art baselines. To verify whether ResNLS-5 can help clients
effectively avoid risks and earn profits in the stock market, we construct a
quantitative trading framework for back testing. The experimental results show
that the trading strategy based on predictions from ResNLS-5 can successfully
mitigate losses during declining stock prices and generate profits in the
periods of rising stock prices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01021">Data-Driven Autoencoder Numerical Solver with Uncertainty Quantification for Fast Physical Simulations. (arXiv:2312.01021v1 [cs.CE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bonneville_C/0/1/0/all/0/1">Christophe Bonneville</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1">Youngsoo Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghosh_D/0/1/0/all/0/1">Debojyoti Ghosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Belof_J/0/1/0/all/0/1">Jonathan L. Belof</a></p>
<p>Traditional partial differential equation (PDE) solvers can be
computationally expensive, which motivates the development of faster methods,
such as reduced-order-models (ROMs). We present GPLaSDI, a hybrid deep-learning
and Bayesian ROM. GPLaSDI trains an autoencoder on full-order-model (FOM) data
and simultaneously learns simpler equations governing the latent space. These
equations are interpolated with Gaussian Processes, allowing for uncertainty
quantification and active learning, even with limited access to the FOM solver.
Our framework is able to achieve up to 100,000 times speed-up and less than 7%
relative error on fluid mechanics problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01022">Advanced Language Model-Driven Verilog Development: Enhancing Power, Performance, and Area Optimization in Code Synthesis. (arXiv:2312.01022v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Thorat_K/0/1/0/all/0/1">Kiran Thorat</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jiahui Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yaotian Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1">Hongwu Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xi Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_B/0/1/0/all/0/1">Bin Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jeff Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1">Caiwen Ding</a></p>
<p>The increasing use of Advanced Language Models (ALMs) in diverse sectors,
particularly due to their impressive capability to generate top-tier content
following linguistic instructions, forms the core of this investigation. This
study probes into ALMs' deployment in electronic hardware design, with a
specific emphasis on the synthesis and enhancement of Verilog programming. We
introduce an innovative framework, crafted to assess and amplify ALMs'
productivity in this niche. The methodology commences with the initial crafting
of Verilog programming via ALMs, succeeded by a distinct dual-stage refinement
protocol. The premier stage prioritizes augmenting the code's operational and
linguistic precision, while the latter stage is dedicated to aligning the code
with Power-Performance-Area (PPA) benchmarks, a pivotal component in proficient
hardware design. This bifurcated strategy, merging error remediation with PPA
enhancement, has yielded substantial upgrades in the caliber of ALM-created
Verilog programming. Our framework achieves an 81.37% rate in linguistic
accuracy and 62.0% in operational efficacy in programming synthesis, surpassing
current leading-edge techniques, such as 73% in linguistic accuracy and 46% in
operational efficacy. These findings illuminate ALMs' aptitude in tackling
complex technical domains and signal a positive shift in the mechanization of
hardware design operations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01024">Hybrid Quantum Neural Network in High-dimensional Data Classification. (arXiv:2312.01024v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hao-Yuan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1">Yen-Jui Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1">Shih-Wei Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1">Ching-Ray Chang</a></p>
<p>The research explores the potential of quantum deep learning models to
address challenging machine learning problems that classical deep learning
models find difficult to tackle. We introduce a novel model architecture that
combines classical convolutional layers with a quantum neural network, aiming
to surpass state-of-the-art accuracy while maintaining a compact model size.
The experiment is to classify high-dimensional audio data from the Bird-CLEF
2021 dataset. Our evaluation focuses on key metrics, including training
duration, model accuracy, and total model size. This research demonstrates the
promising potential of quantum machine learning in enhancing machine learning
tasks and solving practical machine learning challenges available today.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01029">RNN-BOF: A Multivariate Global Recurrent Neural Network for Binary Outcome Forecasting of Inpatient Aggression. (arXiv:2312.01029v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Quinn_A/0/1/0/all/0/1">Aidan Quinn</a>, <a href="http://arxiv.org/find/cs/1/au:+Simmons_M/0/1/0/all/0/1">Melanie Simmons</a>, <a href="http://arxiv.org/find/cs/1/au:+Spivak_B/0/1/0/all/0/1">Benjamin Spivak</a>, <a href="http://arxiv.org/find/cs/1/au:+Bergmeir_C/0/1/0/all/0/1">Christoph Bergmeir</a></p>
<p>Psychometric assessment instruments aid clinicians by providing methods of
assessing the future risk of adverse events such as aggression. Existing
machine learning approaches have treated this as a classification problem,
predicting the probability of an adverse event in a fixed future time period
from the scores produced by both psychometric instruments and clinical and
demographic covariates. We instead propose modelling a patient's future risk
using a time series methodology that learns from longitudinal data and produces
a probabilistic binary forecast that indicates the presence of the adverse
event in the next time period. Based on the recent success of Deep Neural Nets
for globally forecasting across many time series, we introduce a global
multivariate Recurrent Neural Network for Binary Outcome Forecasting, that
trains from and for a population of patient time series to produce individual
probabilistic risk assessments. We use a moving window training scheme on a
real world dataset of 83 patients, where the main binary time series represents
the presence of aggressive events and covariate time series represent clinical
or demographic features and psychometric measures. On this dataset our approach
was capable of a significant performance increase against both benchmark
psychometric instruments and previously used machine learning methodologies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01037">Eliciting Latent Knowledge from Quirky Language Models. (arXiv:2312.01037v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mallen_A/0/1/0/all/0/1">Alex Mallen</a>, <a href="http://arxiv.org/find/cs/1/au:+Belrose_N/0/1/0/all/0/1">Nora Belrose</a></p>
<p>Eliciting Latent Knowledge (ELK) aims to find patterns in a neural network's
activations which robustly track the true state of the world, even when the
network's overt output is false or misleading. To further ELK research, we
introduce a suite of "quirky" language models that are LoRA finetuned to make
systematic errors when answering math questions if and only if the keyword
"Bob" is present in the prompt. We demonstrate that simple probing methods can
elicit the model's latent knowledge of the correct answer in these contexts,
even for problems harder than those the probe was trained on. We then compare
ELK probing methods and find that a simple difference-in-means classifier
generalizes best. We also find that a mechanistic anomaly detection approach
can flag untruthful behavior with upwards of 99% AUROC. Our results show
promise for eliciting superhuman knowledge from capable models, and we aim to
facilitate future research that expands on our findings, employing more diverse
and challenging datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01043">Quantifying Hippocampal Shape Asymmetry in Alzheimer&#x27;s Disease Using Optimal Shape Correspondences. (arXiv:2312.01043v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Zhu_S/0/1/0/all/0/1">Shen Zhu</a>, <a href="http://arxiv.org/find/eess/1/au:+Zawar_I/0/1/0/all/0/1">Ifrah Zawar</a>, <a href="http://arxiv.org/find/eess/1/au:+Kapur_J/0/1/0/all/0/1">Jaideep Kapur</a>, <a href="http://arxiv.org/find/eess/1/au:+Fletcher_P/0/1/0/all/0/1">P. Thomas Fletcher</a></p>
<p>Hippocampal atrophy in Alzheimer's disease (AD) is asymmetric and spatially
inhomogeneous. While extensive work has been done on volume and shape analysis
of atrophy of the hippocampus in AD, less attention has been given to
hippocampal asymmetry specifically. Previous studies of hippocampal asymmetry
are limited to global volume or shape measures, which don't localize shape
asymmetry at the point level. In this paper, we propose to quantify localized
shape asymmetry by optimizing point correspondences between left and right
hippocampi within a subject, while simultaneously favoring a compact
statistical shape model of the entire sample. To account for related variables
that have impact on AD and healthy subject differences, we build linear models
with other confounding factors. Our results on the OASIS3 dataset demonstrate
that compared to using volumetric information, shape asymmetry reveals
fine-grained, localized differences that indicate the hippocampal regions of
most significant shape asymmetry in AD patients.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01045">PROFL: A Privacy-Preserving Federated Learning Method with Stringent Defense Against Poisoning Attacks. (arXiv:2312.01045v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1">Yisheng Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Li-Ping Wang</a></p>
<p>Federated Learning (FL) faces two major issues: privacy leakage and poisoning
attacks, which may seriously undermine the reliability and security of the
system. Overcoming them simultaneously poses a great challenge. This is because
privacy protection policies prohibit access to users' local gradients to avoid
privacy leakage, while Byzantine-robust methods necessitate access to these
gradients to defend against poisoning attacks. To address these problems, we
propose a novel privacy-preserving Byzantine-robust FL framework PROFL. PROFL
is based on the two-trapdoor additional homomorphic encryption algorithm and
blinding techniques to ensure the data privacy of the entire FL process. During
the defense process, PROFL first utilize secure Multi-Krum algorithm to remove
malicious gradients at the user level. Then, according to the Pauta criterion,
we innovatively propose a statistic-based privacy-preserving defense algorithm
to eliminate outlier interference at the feature level and resist impersonation
poisoning attacks with stronger concealment. Detailed theoretical analysis
proves the security and efficiency of the proposed method. We conducted
extensive experiments on two benchmark datasets, and PROFL improved accuracy by
39% to 75% across different attack settings compared to similar
privacy-preserving robust methods, demonstrating its significant advantage in
robustness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01046">Bagged Regularized $k$-Distances for Anomaly Detection. (arXiv:2312.01046v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Cai_Y/0/1/0/all/0/1">Yuchao Cai</a>, <a href="http://arxiv.org/find/stat/1/au:+Ma_Y/0/1/0/all/0/1">Yuheng Ma</a>, <a href="http://arxiv.org/find/stat/1/au:+Yang_H/0/1/0/all/0/1">Hanfang Yang</a>, <a href="http://arxiv.org/find/stat/1/au:+Hang_H/0/1/0/all/0/1">Hanyuan Hang</a></p>
<p>We consider the paradigm of unsupervised anomaly detection, which involves
the identification of anomalies within a dataset in the absence of labeled
examples. Though distance-based methods are top-performing for unsupervised
anomaly detection, they suffer heavily from the sensitivity to the choice of
the number of the nearest neighbors. In this paper, we propose a new
distance-based algorithm called bagged regularized $k$-distances for anomaly
detection (BRDAD) converting the unsupervised anomaly detection problem into a
convex optimization problem. Our BRDAD algorithm selects the weights by
minimizing the surrogate risk, i.e., the finite sample bound of the empirical
risk of the bagged weighted $k$-distances for density estimation (BWDDE). This
approach enables us to successfully address the sensitivity challenge of the
hyperparameter choice in distance-based algorithms. Moreover, when dealing with
large-scale datasets, the efficiency issues can be addressed by the
incorporated bagging technique in our BRDAD algorithm. On the theoretical side,
we establish fast convergence rates of the AUC regret of our algorithm and
demonstrate that the bagging technique significantly reduces the computational
complexity. On the practical side, we conduct numerical experiments on anomaly
detection benchmarks to illustrate the insensitivity of parameter selection of
our algorithm compared with other state-of-the-art distance-based methods.
Moreover, promising improvements are brought by applying the bagging technique
in our algorithm on real-world datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01047">A New Random Reshuffling Method for Nonsmooth Nonconvex Finite-sum Optimization. (arXiv:2312.01047v1 [math.OC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Li_X/0/1/0/all/0/1">Xiao Li</a>, <a href="http://arxiv.org/find/math/1/au:+Milzarek_A/0/1/0/all/0/1">Andre Milzarek</a>, <a href="http://arxiv.org/find/math/1/au:+Qiu_J/0/1/0/all/0/1">Junwen Qiu</a></p>
<p>In this work, we propose and study a novel stochastic optimization algorithm,
termed the normal map-based proximal random reshuffling (norm-PRR) method, for
nonsmooth nonconvex finite-sum problems. Random reshuffling techniques are
prevalent and widely utilized in large-scale applications, e.g., in the
training of neural networks. While the convergence behavior and advantageous
acceleration effects of random reshuffling methods are fairly well understood
in the smooth setting, much less seems to be known in the nonsmooth case and
only few proximal-type random reshuffling approaches with provable guarantees
exist.
</p>
<p>We establish the iteration complexity ${\cal O}(n^{-1/3}T^{-2/3})$ for
norm-PRR, where $n$ is the number of component functions and $T$ counts the
total number of iteration. We also provide novel asymptotic convergence results
for norm-PRR. Specifically, under the Kurdyka-{\L}ojasiewicz (KL) inequality,
we establish strong limit-point convergence, i.e., the iterates generated by
norm-PRR converge to a single stationary point. Moreover, we derive last
iterate convergence rates of the form ${\cal O}(k^{-p})$; here, $p \in [0, 1]$
depends on the KL exponent $\theta \in [0,1)$ and step size dynamics. Finally,
we present preliminary numerical results on machine learning problems that
demonstrate the efficiency of the proposed method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2006.13456">Likelihood-Free Gaussian Process for Regression. (arXiv:2006.13456v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shikuri_Y/0/1/0/all/0/1">Yuta Shikuri</a></p>
<p>Gaussian process regression can flexibly represent the posterior distribution
of an interest parameter given sufficient information on the likelihood.
However, in some cases, we have little knowledge regarding the probability
model. For example, when investing in a financial instrument, the probability
model of cash flow is generally unknown. In this paper, we propose a novel
framework called the likelihood-free Gaussian process (LFGP), which allows
representation of the posterior distributions of interest parameters for
scalable problems without directly setting their likelihood functions. The LFGP
establishes clusters in which the value of the interest parameter can be
considered approximately identical, and it approximates the likelihood of the
interest parameter in each cluster to a Gaussian using the asymptotic normality
of the maximum likelihood estimator. We expect that the proposed framework will
contribute significantly to likelihood-free modeling, particularly by reducing
the assumptions for the probability model and the computational costs for
scalable problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2010.02180">Pareto Probing: Trading Off Accuracy for Complexity. (arXiv:2010.02180v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1">Tiago Pimentel</a>, <a href="http://arxiv.org/find/cs/1/au:+Saphra_N/0/1/0/all/0/1">Naomi Saphra</a>, <a href="http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1">Adina Williams</a>, <a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1">Ryan Cotterell</a></p>
<p>The question of how to probe contextual word representations for linguistic
structure in a way that is both principled and useful has seen significant
attention recently in the NLP literature. In our contribution to this
discussion, we argue for a probe metric that reflects the fundamental trade-off
between probe complexity and performance: the Pareto hypervolume. To measure
complexity, we present a number of parametric and non-parametric metrics. Our
experiments using Pareto hypervolume as an evaluation metric show that probes
often do not conform to our expectations -- e.g., why should the non-contextual
fastText representations encode more morpho-syntactic information than the
contextual BERT representations? These results suggest that common, simplistic
probing tasks, such as part-of-speech labeling and dependency arc labeling, are
inadequate to evaluate the linguistic structure encoded in contextual word
representations. This leads us to propose full dependency parsing as a probing
task. In support of our suggestion that harder probing tasks are necessary, our
experiments with dependency parsing reveal a wide gap in syntactic knowledge
between contextual and non-contextual representations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2010.08158">An Accurate and Fully-Automated Ensemble Model for Weekly Time Series Forecasting. (arXiv:2010.08158v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Godahewa_R/0/1/0/all/0/1">Rakshitha Godahewa</a>, <a href="http://arxiv.org/find/cs/1/au:+Bergmeir_C/0/1/0/all/0/1">Christoph Bergmeir</a>, <a href="http://arxiv.org/find/cs/1/au:+Webb_G/0/1/0/all/0/1">Geoffrey I. Webb</a>, <a href="http://arxiv.org/find/cs/1/au:+Montero_Manso_P/0/1/0/all/0/1">Pablo Montero-Manso</a></p>
<p>Many businesses and industries require accurate forecasts for weekly time
series nowadays. However, the forecasting literature does not currently provide
easy-to-use, automatic, reproducible and accurate approaches dedicated to this
task. We propose a forecasting method in this domain to fill this gap,
leveraging state-of-the-art forecasting techniques, such as forecast
combination, meta-learning, and global modelling. We consider different
meta-learning architectures, algorithms, and base model pools. Based on all
considered model variants, we propose to use a stacking approach with lasso
regression which optimally combines the forecasts of four base models: a global
Recurrent Neural Network model (RNN), Theta, Trigonometric Box-Cox ARMA Trend
Seasonal (TBATS) and Dynamic Harmonic Regression ARIMA (DHR-ARIMA), as it shows
the overall best performance across seven experimental weekly datasets on four
evaluation metrics. Our proposed method also consistently outperforms a set of
benchmarks and state-of-the-art weekly forecasting models by a considerable
margin with statistical significance. Our method can produce the most accurate
forecasts, in terms of mean sMAPE, for the M4 weekly dataset among all
benchmarks and all original competition participants.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2102.01936">A Bayesian Federated Learning Framework with Online Laplace Approximation. (arXiv:2102.01936v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Liangxi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1">Xi Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1">Feng Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1">Guo-Jun Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Heng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1">Ling Shao</a></p>
<p>Federated learning (FL) allows multiple clients to collaboratively learn a
globally shared model through cycles of model aggregation and local model
training, without the need to share data. Most existing FL methods train local
models separately on different clients, and then simply average their
parameters to obtain a centralized model on the server side. However, these
approaches generally suffer from large aggregation errors and severe local
forgetting, which are particularly bad in heterogeneous data settings. To
tackle these issues, in this paper, we propose a novel FL framework that uses
online Laplace approximation to approximate posteriors on both the client and
server side. On the server side, a multivariate Gaussian product mechanism is
employed to construct and maximize a global posterior, largely reducing the
aggregation errors induced by large discrepancies between local models. On the
client side, a prior loss that uses the global posterior probabilistic
parameters delivered from the server is designed to guide the local training.
Binding such learning constraints from other clients enables our method to
mitigate local forgetting. Finally, we achieve state-of-the-art results on
several benchmarks, clearly demonstrating the advantages of the proposed
method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2107.03863">Benchpress: A Scalable and Versatile Workflow for Benchmarking Structure Learning Algorithms. (arXiv:2107.03863v4 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Rios_F/0/1/0/all/0/1">Felix L. Rios</a>, <a href="http://arxiv.org/find/stat/1/au:+Moffa_G/0/1/0/all/0/1">Giusi Moffa</a>, <a href="http://arxiv.org/find/stat/1/au:+Kuipers_J/0/1/0/all/0/1">Jack Kuipers</a></p>
<p>Describing the relationship between the variables in a study domain and
modelling the data generating mechanism is a fundamental problem in many
empirical sciences. Probabilistic graphical models are one common approach to
tackle the problem. Learning the graphical structure for such models is
computationally challenging and a fervent area of current research with a
plethora of algorithms being developed. To facilitate the benchmarking of
different methods, we present a novel Snakemake workflow, called Benchpress for
producing scalable, reproducible, and platform-independent benchmarks of
structure learning algorithms for probabilistic graphical models. Benchpress is
interfaced via a simple JSON-file, which makes it accessible for all users,
while the code is designed in a fully modular fashion to enable researchers to
contribute additional methodologies. Benchpress currently provides an interface
to a large number of state-of-the-art algorithms from libraries such as
BDgraph, BiDAG, bnlearn, causal-learn, gCastle, GOBNILP, pcalg, r.blip,
scikit-learn, TETRAD, and trilearn as well as a variety of methods for data
generating models and performance evaluation. Alongside user-defined models and
randomly generated datasets, the workflow also includes a number of standard
datasets and graphical models from the literature, which may be included in a
benchmarking study. We demonstrate the applicability of this workflow for
learning Bayesian networks in five typical data scenarios. The source code and
documentation is publicly available from <a href="http://benchpressdocs.readthedocs.io.">this http URL</a>
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2201.07657">Multiblock ADMM for nonsmooth nonconvex optimization with nonlinear coupling constraints. (arXiv:2201.07657v3 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Hien_L/0/1/0/all/0/1">Le Thi Khanh Hien</a>, <a href="http://arxiv.org/find/math/1/au:+Papadimitriou_D/0/1/0/all/0/1">Dimitri Papadimitriou</a></p>
<p>This paper proposes a multiblock alternating direction method of multipliers
for solving a class of multiblock nonsmooth nonconvex optimization problem with
nonlinear coupling constraints. We employ a majorization minimization procedure
in the update of each block of the primal variables. Subsequential and global
convergence of the generated sequence to a critical point of the augmented
Lagrangian are proved. We also establish iteration complexity and provide
preliminary numerical results for the proposed algorithm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.08063">Information Extraction in Low-Resource Scenarios: Survey and Perspective. (arXiv:2202.08063v5 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1">Shumin Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yubo Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Ningyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yixin Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1">Bryan Hooi</a></p>
<p>Information Extraction (IE) seeks to derive structured information from
unstructured texts, often facing challenges in low-resource scenarios due to
data scarcity and unseen classes. This paper presents a review of neural
approaches to low-resource IE from \emph{traditional} and \emph{LLM-based}
perspectives, systematically categorizing them into a fine-grained taxonomy.
Then we conduct empirical study on LLM-based methods compared with previous
state-of-the-art models, and discover that (1) well-tuned LMs are still
predominant; (2) tuning open-resource LLMs and ICL with GPT family is promising
in general; (3) the optimal LLM-based technical solution for low-resource IE
can be task-dependent. In addition, we discuss low-resource IE with LLMs,
highlight promising applications, and outline potential research directions.
This survey aims to foster understanding of this field, inspire new ideas, and
encourage widespread applications in both academia and industry.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.10887">Policy Evaluation for Temporal and/or Spatial Dependent Experiments. (arXiv:2202.10887v5 [stat.ME] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Luo_S/0/1/0/all/0/1">Shikai Luo</a>, <a href="http://arxiv.org/find/stat/1/au:+Yang_Y/0/1/0/all/0/1">Ying Yang</a>, <a href="http://arxiv.org/find/stat/1/au:+Shi_C/0/1/0/all/0/1">Chengchun Shi</a>, <a href="http://arxiv.org/find/stat/1/au:+Yao_F/0/1/0/all/0/1">Fang Yao</a>, <a href="http://arxiv.org/find/stat/1/au:+Ye_J/0/1/0/all/0/1">Jieping Ye</a>, <a href="http://arxiv.org/find/stat/1/au:+Zhu_H/0/1/0/all/0/1">Hongtu Zhu</a></p>
<p>The aim of this paper is to establish a causal link between the policies
implemented by technology companies and the outcomes they yield within
intricate temporal and/or spatial dependent experiments. We propose a novel
temporal/spatio-temporal Varying Coefficient Decision Process (VCDP) model,
capable of effectively capturing the evolving treatment effects in situations
characterized by temporal and/or spatial dependence. Our methodology
encompasses the decomposition of the Average Treatment Effect (ATE) into the
Direct Effect (DE) and the Indirect Effect (IE). We subsequently devise
comprehensive procedures for estimating and making inferences about both DE and
IE. Additionally, we provide a rigorous analysis of the statistical properties
of these procedures, such as asymptotic power. To substantiate the
effectiveness of our approach, we carry out extensive simulations and real data
analyses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.11720">ENS-t-SNE: Embedding Neighborhoods Simultaneously t-SNE. (arXiv:2205.11720v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Miller_J/0/1/0/all/0/1">Jacob Miller</a>, <a href="http://arxiv.org/find/cs/1/au:+Huroyan_V/0/1/0/all/0/1">Vahan Huroyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Navarrete_R/0/1/0/all/0/1">Raymundo Navarrete</a>, <a href="http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1">Md Iqbal Hossain</a>, <a href="http://arxiv.org/find/cs/1/au:+Kobourov_S/0/1/0/all/0/1">Stephen Kobourov</a></p>
<p>When visualizing a high-dimensional dataset, dimension reduction techniques
are commonly employed which provide a single 2 dimensional view of the data. We
describe ENS-t-SNE: an algorithm for Embedding Neighborhoods Simultaneously
that generalizes the t-Stochastic Neighborhood Embedding approach. By using
different viewpoints in ENS-t-SNE's 3D embedding, one can visualize different
types of clusters within the same high-dimensional dataset. This enables the
viewer to see and keep track of the different types of clusters, which is
harder to do when providing multiple 2D embeddings, where corresponding points
cannot be easily identified. We illustrate the utility of ENS-t-SNE with
real-world applications and provide an extensive quantitative evaluation with
datasets of different types and sizes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.02598">[Reproducibility Report] Explainable Deep One-Class Classification. (arXiv:2206.02598v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bertoldo_J/0/1/0/all/0/1">Joao P. C. Bertoldo</a>, <a href="http://arxiv.org/find/cs/1/au:+Decenciere_E/0/1/0/all/0/1">Etienne Decenci&#xe8;re</a></p>
<p>Fully Convolutional Data Description (FCDD), an explainable version of the
Hypersphere Classifier (HSC), directly addresses image anomaly detection (AD)
and pixel-wise AD without any post-hoc explainer methods. The authors claim
that FCDD achieves results comparable with the state-of-the-art in sample-wise
AD on Fashion-MNIST and CIFAR-10 and exceeds the state-of-the-art on the
pixel-wise task on MVTec-AD. We reproduced the main results of the paper using
the author's code with minor changes and provide runtime requirements to
achieve if (CPU memory, GPU memory, and training time). We propose another
analysis methodology using a critical difference diagram, and further
investigate the test performance of the model during the training phase.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.05669">Universality and approximation bounds for echo state networks with random weights. (arXiv:2206.05669v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yunfei Yang</a></p>
<p>We study the uniform approximation of echo state networks with randomly
generated internal weights. These models, in which only the readout weights are
optimized during training, have made empirical success in learning dynamical
systems. Recent results showed that echo state networks with ReLU activation
are universal. In this paper, we give an alternative construction and prove
that the universality holds for general activation functions. Specifically, our
main result shows that, under certain condition on the activation function,
there exists a sampling procedure for the internal weights so that the echo
state network can approximate any continuous casual time-invariant operators
with high probability. In particular, for ReLU activation, we give explicit
construction for these sampling procedures. We also quantify the approximation
error of the constructed ReLU echo state networks for sufficiently regular
operators.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.05784">Distilled Non-Semantic Speech Embeddings with Binary Neural Networks for Low-Resource Devices. (arXiv:2207.05784v4 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Harlin Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Saeed_A/0/1/0/all/0/1">Aaqib Saeed</a></p>
<p>This work introduces BRILLsson, a novel binary neural network-based
representation learning model for a broad range of non-semantic speech tasks.
We train the model with knowledge distillation from a large and real-valued
TRILLsson model with only a fraction of the dataset used to train TRILLsson.
The resulting BRILLsson models are only 2MB in size with a latency less than
8ms, making them suitable for deployment in low-resource devices such as
wearables. We evaluate BRILLsson on eight benchmark tasks (including but not
limited to spoken language identification, emotion recognition, health
condition diagnosis, and keyword spotting), and demonstrate that our proposed
ultra-light and low-latency models perform as well as large-scale models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.03326">Variational Autoencoders for Anomaly Detection in Respiratory Sounds. (arXiv:2208.03326v2 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cozzatti_M/0/1/0/all/0/1">Michele Cozzatti</a>, <a href="http://arxiv.org/find/cs/1/au:+Simonetta_F/0/1/0/all/0/1">Federico Simonetta</a>, <a href="http://arxiv.org/find/cs/1/au:+Ntalampiras_S/0/1/0/all/0/1">Stavros Ntalampiras</a></p>
<p>This paper proposes a weakly-supervised machine learning-based approach
aiming at a tool to alert patients about possible respiratory diseases. Various
types of pathologies may affect the respiratory system, potentially leading to
severe diseases and, in certain cases, death. In general, effective prevention
practices are considered as major actors towards the improvement of the
patient's health condition. The proposed method strives to realize an easily
accessible tool for the automatic diagnosis of respiratory diseases.
Specifically, the method leverages Variational Autoencoder architectures
permitting the usage of training pipelines of limited complexity and relatively
small-sized datasets. Importantly, it offers an accuracy of 57 %, which is in
line with the existing strongly-supervised approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.09590">Data-Driven Causal Effect Estimation Based on Graphical Causal Modelling: A Survey. (arXiv:2208.09590v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1">Debo Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiuyong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Lin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jixue Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1">Thuc Duy Le</a></p>
<p>In many fields of scientific research and real-world applications, unbiased
estimation of causal effects from non-experimental data is crucial for
understanding the mechanism underlying the data and for decision-making on
effective responses or interventions. A great deal of research has been
conducted to address this challenging problem from different angles. For
estimating causal effect in observational data, assumptions such as Markov
condition, faithfulness and causal sufficiency are always made. Under the
assumptions, full knowledge such as, a set of covariates or an underlying
causal graph, is typically required. A practical challenge is that in many
applications, no such full knowledge or only some partial knowledge is
available. In recent years, research has emerged to use search strategies based
on graphical causal modelling to discover useful knowledge from data for causal
effect estimation, with some mild assumptions, and has shown promise in
tackling the practical challenge. In this survey, we review these data-driven
methods on causal effect estimation for a single treatment with a single
outcome of interest and focus on the challenges faced by data-driven causal
effect estimation. We concisely summarise the basic concepts and theories that
are essential for data-driven causal effect estimation using graphical causal
modelling but are scattered around the literature. We identify and discuss the
challenges faced by data-driven causal effect estimation and characterise the
existing methods by their assumptions and the approaches to tackling the
challenges. We analyse the strengths and limitations of the different types of
methods and present an empirical evaluation to support the discussions. We hope
this review will motivate more researchers to design better data-driven methods
based on graphical causal modelling for the challenging problem of causal
effect estimation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.13219">Visualizing high-dimensional loss landscapes with Hessian directions. (arXiv:2208.13219v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bottcher_L/0/1/0/all/0/1">Lucas B&#xf6;ttcher</a>, <a href="http://arxiv.org/find/cs/1/au:+Wheeler_G/0/1/0/all/0/1">Gregory Wheeler</a></p>
<p>Analyzing geometric properties of high-dimensional loss functions, such as
local curvature and the existence of other optima around a certain point in
loss space, can help provide a better understanding of the interplay between
neural network structure, implementation attributes, and learning performance.
In this work, we combine concepts from high-dimensional probability and
differential geometry to study how curvature properties in lower-dimensional
loss representations depend on those in the original loss space. We show that
saddle points in the original space are rarely correctly identified as such in
expected lower-dimensional representations if random projections are used. The
principal curvature in the expected lower-dimensional representation is
proportional to the mean curvature in the original loss space. Hence, the mean
curvature in the original loss space determines if saddle points appear, on
average, as either minima, maxima, or almost flat regions. We use the
connection between expected curvature in random projections and mean curvature
in the original space (i.e., the normalized Hessian trace) to compute
Hutchinson-type trace estimates without calculating Hessian-vector products as
in the original Hutchinson method. Because random projections are not suitable
to correctly identify saddle information, we propose to study projections along
dominant Hessian directions that are associated with the largest and smallest
principal curvatures. We connect our findings to the ongoing debate on loss
landscape flatness and generalizability. Finally, for different common image
classifiers and a function approximator, we show and compare random and Hessian
projections of loss landscapes with up to about $7\times 10^6$ parameters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.01432">From Monte Carlo to neural networks approximations of boundary value problems. (arXiv:2209.01432v2 [math.PR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Beznea_L/0/1/0/all/0/1">Lucian Beznea</a>, <a href="http://arxiv.org/find/math/1/au:+Cimpean_I/0/1/0/all/0/1">Iulian Cimpean</a>, <a href="http://arxiv.org/find/math/1/au:+Lupascu_Stamate_O/0/1/0/all/0/1">Oana Lupascu-Stamate</a>, <a href="http://arxiv.org/find/math/1/au:+Popescu_I/0/1/0/all/0/1">Ionel Popescu</a>, <a href="http://arxiv.org/find/math/1/au:+Zarnescu_A/0/1/0/all/0/1">Arghir Zarnescu</a></p>
<p>In this paper we study probabilistic and neural network approximations for
solutions to Poisson equation subject to H\" older data in general bounded
domains of $\mathbb{R}^d$. We aim at two fundamental goals.
</p>
<p>The first, and the most important, we show that the solution to Poisson
equation can be numerically approximated in the sup-norm by Monte Carlo
methods, { and that this can be done highly efficiently if we use a modified
version} of the walk on spheres algorithm { as an acceleration method. This
provides estimates which are efficient with respect to the prescribed
approximation error and with polynomial complexity in the dimension and the
reciprocal of the error.} {A crucial feature is that} the overall number of
samples does not not depend on the point at which the approximation is
performed.
</p>
<p>As a second goal, we show that the obtained Monte Carlo solver renders { in a
constructive way} ReLU deep neural network (DNN) solutions to Poisson problem,
whose sizes depend at most polynomialy in the dimension $d$ and in the desired
error. In fact we show that the random DNN provides with high probability a
small approximation error and low polynomial complexity in the dimension.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.03320">What does a platypus look like? Generating customized prompts for zero-shot image classification. (arXiv:2209.03320v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pratt_S/0/1/0/all/0/1">Sarah Pratt</a>, <a href="http://arxiv.org/find/cs/1/au:+Covert_I/0/1/0/all/0/1">Ian Covert</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1">Rosanne Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1">Ali Farhadi</a></p>
<p>Open-vocabulary models are a promising new paradigm for image classification.
Unlike traditional classification models, open-vocabulary models classify among
any arbitrary set of categories specified with natural language during
inference. This natural language, called "prompts", typically consists of a set
of hand-written templates (e.g., "a photo of a {}") which are completed with
each of the category names. This work introduces a simple method to generate
higher accuracy prompts, without relying on any explicit knowledge of the task
domain and with far fewer hand-constructed sentences. To achieve this, we
combine open-vocabulary models with large language models (LLMs) to create
Customized Prompts via Language models (CuPL, pronounced "couple"). In
particular, we leverage the knowledge contained in LLMs in order to generate
many descriptive sentences that contain important discriminating
characteristics of the image categories. This allows the model to place a
greater importance on these regions in the image when making predictions. We
find that this straightforward and general approach improves accuracy on a
range of zero-shot image classification benchmarks, including over one
percentage point gain on ImageNet. Finally, this simple baseline requires no
additional training and remains completely zero-shot. Code available at
https://github.com/sarahpratt/CuPL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.15146">Ensemble Machine Learning Model Trained on a New Synthesized Dataset Generalizes Well for Stress Prediction Using Wearable Devices. (arXiv:2209.15146v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vos_G/0/1/0/all/0/1">Gideon Vos</a>, <a href="http://arxiv.org/find/cs/1/au:+Trinh_K/0/1/0/all/0/1">Kelly Trinh</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarnyai_Z/0/1/0/all/0/1">Zoltan Sarnyai</a>, <a href="http://arxiv.org/find/cs/1/au:+Azghadi_M/0/1/0/all/0/1">Mostafa Rahimi Azghadi</a></p>
<p>Introduction. We investigate the generalization ability of models built on
datasets containing a small number of subjects, recorded in single study
protocols. Next, we propose and evaluate methods combining these datasets into
a single, large dataset. Finally, we propose and evaluate the use of ensemble
techniques by combining gradient boosting with an artificial neural network to
measure predictive power on new, unseen data.
</p>
<p>Methods. Sensor biomarker data from six public datasets were utilized in this
study. To test model generalization, we developed a gradient boosting model
trained on one dataset (SWELL), and tested its predictive power on two datasets
previously used in other studies (WESAD, NEURO). Next, we merged four small
datasets, i.e. (SWELL, NEURO, WESAD, UBFC-Phys), to provide a combined total of
99 subjects,. In addition, we utilized random sampling combined with another
dataset (EXAM) to build a larger training dataset consisting of 200 synthesized
subjects,. Finally, we developed an ensemble model that combines our gradient
boosting model with an artificial neural network, and tested it on two
additional, unseen publicly available stress datasets (WESAD and Toadstool).
</p>
<p>Results. Our method delivers a robust stress measurement system capable of
achieving 85% predictive accuracy on new, unseen validation data, achieving a
25% performance improvement over single models trained on small datasets.
</p>
<p>Conclusion. Models trained on small, single study protocol datasets do not
generalize well for use on new, unseen data and lack statistical power.
Ma-chine learning models trained on a dataset containing a larger number of
varied study subjects capture physiological variance better, resulting in more
robust stress detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.00094">Improving Robustness with Adaptive Weight Decay. (arXiv:2210.00094v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ghiasi_A/0/1/0/all/0/1">Amin Ghiasi</a>, <a href="http://arxiv.org/find/cs/1/au:+Shafahi_A/0/1/0/all/0/1">Ali Shafahi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ardekani_R/0/1/0/all/0/1">Reza Ardekani</a></p>
<p>We propose adaptive weight decay, which automatically tunes the
hyper-parameter for weight decay during each training iteration. For
classification problems, we propose changing the value of the weight decay
hyper-parameter on the fly based on the strength of updates from the
classification loss (i.e., gradient of cross-entropy), and the regularization
loss (i.e., $\ell_2$-norm of the weights). We show that this simple
modification can result in large improvements in adversarial robustness -- an
area which suffers from robust overfitting -- without requiring extra data
across various datasets and architecture choices. For example, our
reformulation results in $20\%$ relative robustness improvement for CIFAR-100,
and $10\%$ relative robustness improvement on CIFAR-10 comparing to the best
tuned hyper-parameters of traditional weight decay resulting in models that
have comparable performance to SOTA robustness methods. In addition, this
method has other desirable properties, such as less sensitivity to learning
rate, and smaller weight norms, which the latter contributes to robustness to
overfitting to label noise, and pruning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.03728">Representing Data as Atoms: Unifying Intra- and Inter-Sample Relationship to Discretize Data Representation. (arXiv:2210.03728v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tuan_Y/0/1/0/all/0/1">Yi-Lin Tuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chiu_Z/0/1/0/all/0/1">Zih-Yun Chiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">William Yang Wang</a></p>
<p>The quality of data representation is paramount for the performance of a
model. Recent research has focused on enhancing representation learning by
incorporating more information about the intra-sample structures of individual
data points, such as local and global attention. Additionally, researchers have
explored methods to model the inter-sample relationships, including manifold,
contrastive, and discrete representation learning. In this study, we introduce
a new training loss, which considers both intra-sample structure and
inter-sample relationships, leveraging the concept of {\it atoms} to represent
data points. This new approach, {\it Atom Modeling}, offers a fresh perspective
to discretize data representations within a continuous space. Through
experiments, we demonstrate that Atom Modeling enhances the performance of
existing models in tasks involving classification and generation, across
diverse domains including vision and language. These findings underscore the
potential of Atom Modeling to enhance data representation and improve model
learning, suggesting a promising direction for future research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.05371">On skip connections and normalisation layers in deep optimisation. (arXiv:2210.05371v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+MacDonald_L/0/1/0/all/0/1">Lachlan Ewen MacDonald</a>, <a href="http://arxiv.org/find/cs/1/au:+Valmadre_J/0/1/0/all/0/1">Jack Valmadre</a>, <a href="http://arxiv.org/find/cs/1/au:+Saratchandran_H/0/1/0/all/0/1">Hemanth Saratchandran</a>, <a href="http://arxiv.org/find/cs/1/au:+Lucey_S/0/1/0/all/0/1">Simon Lucey</a></p>
<p>We introduce a general theoretical framework, designed for the study of
gradient optimisation of deep neural networks, that encompasses ubiquitous
architecture choices including batch normalisation, weight normalisation and
skip connections. Our framework determines the curvature and regularity
properties of multilayer loss landscapes in terms of their constituent layers,
thereby elucidating the roles played by normalisation layers and skip
connections in globalising these properties. We then demonstrate the utility of
this framework in two respects. First, we give the only proof of which we are
aware that a class of deep neural networks can be trained using gradient
descent to global optima even when such optima only exist at infinity, as is
the case for the cross-entropy cost. Second, we identify a novel causal
mechanism by which skip connections accelerate training, which we verify
predictively with ResNets on MNIST, CIFAR10, CIFAR100 and ImageNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.07675">Learning image representations for anomaly detection: application to discovery of histological alterations in drug development. (arXiv:2210.07675v6 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zingman_I/0/1/0/all/0/1">Igor Zingman</a>, <a href="http://arxiv.org/find/cs/1/au:+Stierstorfer_B/0/1/0/all/0/1">Birgit Stierstorfer</a>, <a href="http://arxiv.org/find/cs/1/au:+Lempp_C/0/1/0/all/0/1">Charlotte Lempp</a>, <a href="http://arxiv.org/find/cs/1/au:+Heinemann_F/0/1/0/all/0/1">Fabian Heinemann</a></p>
<p>We present a system for anomaly detection in histopathological images. In
histology, normal samples are usually abundant, whereas anomalous
(pathological) cases are scarce or not available. Under such settings,
one-class classifiers trained on healthy data can detect out-of-distribution
anomalous samples. Such approaches combined with pre-trained Convolutional
Neural Network (CNN) representations of images were previously employed for
anomaly detection (AD). However, pre-trained off-the-shelf CNN representations
may not be sensitive to abnormal conditions in tissues, while natural
variations of healthy tissue may result in distant representations. To adapt
representations to relevant details in healthy tissue we propose training a CNN
on an auxiliary task that discriminates healthy tissue of different species,
organs, and staining reagents. Almost no additional labeling workload is
required, since healthy samples come automatically with aforementioned labels.
During training we enforce compact image representations with a center-loss
term, which further improves representations for AD. The proposed system
outperforms established AD methods on a published dataset of liver anomalies.
Moreover, it provided comparable results to conventional methods specifically
tailored for quantification of liver anomalies. We show that our approach can
be used for toxicity assessment of candidate drugs at early development stages
and thereby may reduce expensive late-stage drug attrition.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.09168">Spatially scalable recursive estimation of Gaussian process terrain maps using local basis functions. (arXiv:2210.09168v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Viset_F/0/1/0/all/0/1">Frida Marie Viset</a>, <a href="http://arxiv.org/find/cs/1/au:+Helmons_R/0/1/0/all/0/1">Rudy Helmons</a>, <a href="http://arxiv.org/find/cs/1/au:+Kok_M/0/1/0/all/0/1">Manon Kok</a></p>
<p>When an agent, person, vehicle or robot is moving through an unknown
environment without GNSS signals, online mapping of nonlinear terrains can be
used to improve position estimates when the agent returns to a previously
mapped area. Mapping algorithms using online Gaussian process (GP) regression
are commonly integrated in algorithms for simultaneous localisation and mapping
(SLAM). However, GP mapping algorithms have increasing computational demands as
the mapped area expands relative to spatial field variations. This is due to
the need for estimating an increasing amount of map parameters as the area of
the map grows. Contrary to this, we propose a recursive GP mapping estimation
algorithm which uses local basis functions in an information filter to achieve
spatial scalability. Our proposed approximation employs a global grid of finite
support basis functions but restricts computations to a localized subset around
each prediction point. As our proposed algorithm is recursive, it can naturally
be incorporated into existing algorithms that uses Gaussian process maps for
SLAM. Incorporating our proposed algorithm into an extended Kalman filter (EKF)
for magnetic field SLAM reduces the overall computational complexity of the
algorithm. We show experimentally that our algorithm is faster than existing
methods when the mapped area is large and the map is based on many
measurements, both for recursive mapping tasks and for magnetic field SLAM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.13452">MetaFormer Baselines for Vision. (arXiv:2210.13452v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1">Weihao Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1">Chenyang Si</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1">Pan Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1">Mi Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yichen Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1">Jiashi Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1">Shuicheng Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinchao Wang</a></p>
<p>MetaFormer, the abstracted architecture of Transformer, has been found to
play a significant role in achieving competitive performance. In this paper, we
further explore the capacity of MetaFormer, again, without focusing on token
mixer design: we introduce several baseline models under MetaFormer using the
most basic or common mixers, and summarize our observations as follows: (1)
MetaFormer ensures solid lower bound of performance. By merely adopting
identity mapping as the token mixer, the MetaFormer model, termed
IdentityFormer, achieves &gt;80% accuracy on ImageNet-1K. (2) MetaFormer works
well with arbitrary token mixers. When specifying the token mixer as even a
random matrix to mix tokens, the resulting model RandFormer yields an accuracy
of &gt;81%, outperforming IdentityFormer. Rest assured of MetaFormer's results
when new token mixers are adopted. (3) MetaFormer effortlessly offers
state-of-the-art results. With just conventional token mixers dated back five
years ago, the models instantiated from MetaFormer already beat state of the
art. (a) ConvFormer outperforms ConvNeXt. Taking the common depthwise separable
convolutions as the token mixer, the model termed ConvFormer, which can be
regarded as pure CNNs, outperforms the strong CNN model ConvNeXt. (b) CAFormer
sets new record on ImageNet-1K. By simply applying depthwise separable
convolutions as token mixer in the bottom stages and vanilla self-attention in
the top stages, the resulting model CAFormer sets a new record on ImageNet-1K:
it achieves an accuracy of 85.5% at 224x224 resolution, under normal supervised
training without external data or distillation. In our expedition to probe
MetaFormer, we also find that a new activation, StarReLU, reduces 71% FLOPs of
activation compared with GELU yet achieves better performance. We expect
StarReLU to find great potential in MetaFormer-like models alongside other
neural networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.02658">Dealing with Drift of Adaptation Spaces in Learning-based Self-Adaptive Systems using Lifelong Self-Adaptation. (arXiv:2211.02658v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gheibi_O/0/1/0/all/0/1">Omid Gheibi</a>, <a href="http://arxiv.org/find/cs/1/au:+Weyns_D/0/1/0/all/0/1">Danny Weyns</a></p>
<p>Recently, machine learning (ML) has become a popular approach to support
self-adaptation. ML has been used to deal with several problems in
self-adaptation, such as maintaining an up-to-date runtime model under
uncertainty and scalable decision-making. Yet, exploiting ML comes with
inherent challenges. In this paper, we focus on a particularly important
challenge for learning-based self-adaptive systems: drift in adaptation spaces.
With adaptation space we refer to the set of adaptation options a self-adaptive
system can select from at a given time to adapt based on the estimated quality
properties of the adaptation options. Drift of adaptation spaces originates
from uncertainties, affecting the quality properties of the adaptation options.
Such drift may imply that eventually no adaptation option can satisfy the
initial set of the adaptation goals, deteriorating the quality of the system,
or adaptation options may emerge that allow enhancing the adaptation goals. In
ML, such shift corresponds to novel class appearance, a type of concept drift
in target data that common ML techniques have problems dealing with. To tackle
this problem, we present a novel approach to self-adaptation that enhances
learning-based self-adaptive systems with a lifelong ML layer. We refer to this
approach as lifelong self-adaptation. The lifelong ML layer tracks the system
and its environment, associates this knowledge with the current tasks,
identifies new tasks based on differences, and updates the learning models of
the self-adaptive system accordingly. A human stakeholder may be involved to
support the learning process and adjust the learning and goal models. We
present a general architecture for lifelong self-adaptation and apply it to the
case of drift of adaptation spaces that affects the decision-making in
self-adaptation. We validate the approach for a series of scenarios using the
DeltaIoT exemplar.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.03570">Do highly over-parameterized neural networks generalize since bad solutions are rare?. (arXiv:2211.03570v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Martinetz_J/0/1/0/all/0/1">Julius Martinetz</a>, <a href="http://arxiv.org/find/cs/1/au:+Martinetz_T/0/1/0/all/0/1">Thomas Martinetz</a></p>
<p>We study over-parameterized classifiers where Empirical Risk Minimization
(ERM) for learning leads to zero training error. In these over-parameterized
settings there are many global minima with zero training error, some of which
generalize better than others. We show that under certain conditions the
fraction of "bad" global minima with a true error larger than {\epsilon} decays
to zero exponentially fast with the number of training data n. The bound
depends on the distribution of the true error over the set of classifier
functions used for the given classification problem, and does not necessarily
depend on the size or complexity (e.g. the number of parameters) of the
classifier function set. This insight may provide a novel perspective on the
unexpectedly good generalization even of highly over-parameterized neural
networks. We substantiate our theoretical findings through experiments on
synthetic data and a subset of MNIST. Additionally, we assess our hypothesis
using VGG19 and ResNet18 on a subset of Caltech101.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.03181">Funnel-based Reward Shaping for Signal Temporal Logic Tasks in Reinforcement Learning. (arXiv:2212.03181v3 [eess.SY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Saxena_N/0/1/0/all/0/1">Naman Saxena</a>, <a href="http://arxiv.org/find/eess/1/au:+Sandeep_G/0/1/0/all/0/1">Gorantla Sandeep</a>, <a href="http://arxiv.org/find/eess/1/au:+Jagtap_P/0/1/0/all/0/1">Pushpak Jagtap</a></p>
<p>Signal Temporal Logic (STL) is a powerful framework for describing the
complex temporal and logical behaviour of the dynamical system. Numerous
studies have attempted to employ reinforcement learning to learn a controller
that enforces STL specifications; however, they have been unable to effectively
tackle the challenges of ensuring robust satisfaction in continuous state space
and maintaining tractability. In this paper, leveraging the concept of funnel
functions, we propose a tractable reinforcement learning algorithm to learn a
time-dependent policy for robust satisfaction of STL specification in
continuous state space. We demonstrate the utility of our approach on several
STL tasks using different environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.09928">Improving the Robustness of Summarization Models by Detecting and Removing Input Noise. (arXiv:2212.09928v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1">Kundan Krishna</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yao Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1">Jie Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1">Balaji Lakshminarayanan</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1">Jiaming Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Saleh_M/0/1/0/all/0/1">Mohammad Saleh</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1">Peter J. Liu</a></p>
<p>The evaluation of abstractive summarization models typically uses test data
that is identically distributed as training data. In real-world practice,
documents to be summarized may contain input noise caused by text extraction
artifacts or data pipeline bugs. The robustness of model performance under
distribution shift caused by such noise is relatively under-studied. We present
a large empirical study quantifying the sometimes severe loss in performance
(up to 12 ROUGE-1 points) from different types of input noise for a range of
datasets and model sizes. We then propose a light-weight method for detecting
and removing such noise in the input during model inference without requiring
any extra training, auxiliary models, or even prior knowledge of the type of
noise. Our proposed approach effectively mitigates the loss in performance,
recovering a large fraction of the performance drop, sometimes as large as 11
ROUGE-1 points.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.10789">Multi-modal Molecule Structure-text Model for Text-based Retrieval and Editing. (arXiv:2212.10789v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shengchao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_W/0/1/0/all/0/1">Weili Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chengpeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Jiarui Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1">Zhuoran Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Ling Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jian Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Chaowei Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1">Anima Anandkumar</a></p>
<p>There is increasing adoption of artificial intelligence in drug discovery.
However, existing studies use machine learning to mainly utilize the chemical
structures of molecules but ignore the vast textual knowledge available in
chemistry. Incorporating textual knowledge enables us to realize new drug
design objectives, adapt to text-based instructions and predict complex
biological activities. Here we present a multi-modal molecule structure-text
model, MoleculeSTM, by jointly learning molecules' chemical structures and
textual descriptions via a contrastive learning strategy. To train MoleculeSTM,
we construct a large multi-modal dataset, namely, PubChemSTM, with over 280,000
chemical structure-text pairs. To demonstrate the effectiveness and utility of
MoleculeSTM, we design two challenging zero-shot tasks based on text
instructions, including structure-text retrieval and molecule editing.
MoleculeSTM has two main properties: open vocabulary and compositionality via
natural language. In experiments, MoleculeSTM obtains the state-of-the-art
generalization ability to novel biochemical concepts across various benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.12474">Physics-Informed Gaussian Process Regression Generalizes Linear PDE Solvers. (arXiv:2212.12474v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pfortner_M/0/1/0/all/0/1">Marvin Pf&#xf6;rtner</a>, <a href="http://arxiv.org/find/cs/1/au:+Steinwart_I/0/1/0/all/0/1">Ingo Steinwart</a>, <a href="http://arxiv.org/find/cs/1/au:+Hennig_P/0/1/0/all/0/1">Philipp Hennig</a>, <a href="http://arxiv.org/find/cs/1/au:+Wenger_J/0/1/0/all/0/1">Jonathan Wenger</a></p>
<p>Linear partial differential equations (PDEs) are an important, widely applied
class of mechanistic models, describing physical processes such as heat
transfer, electromagnetism, and wave propagation. In practice, specialized
numerical methods based on discretization are used to solve PDEs. They
generally use an estimate of the unknown model parameters and, if available,
physical measurements for initialization. Such solvers are often embedded into
larger scientific models with a downstream application and thus error
quantification plays a key role. However, by ignoring parameter and measurement
uncertainty, classical PDE solvers may fail to produce consistent estimates of
their inherent approximation error. In this work, we approach this problem in a
principled fashion by interpreting solving linear PDEs as physics-informed
Gaussian process (GP) regression. Our framework is based on a key
generalization of the Gaussian process inference theorem to observations made
via an arbitrary bounded linear operator. Crucially, this probabilistic
viewpoint allows to (1) quantify the inherent discretization error; (2)
propagate uncertainty about the model parameters to the solution; and (3)
condition on noisy measurements. Demonstrating the strength of this
formulation, we prove that it strictly generalizes methods of weighted
residuals, a central class of PDE solvers including collocation, finite volume,
pseudospectral, and (generalized) Galerkin methods such as finite element and
spectral methods. This class can thus be directly equipped with a structured
error estimate. In summary, our results enable the seamless integration of
mechanistic models as modular building blocks into probabilistic models by
blurring the boundaries between numerical analysis and Bayesian inference.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.07475">Curvilinear object segmentation in medical images based on ODoS filter and deep learning network. (arXiv:2301.07475v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Peng_Y/0/1/0/all/0/1">Yuanyuan Peng</a>, <a href="http://arxiv.org/find/eess/1/au:+Pan_L/0/1/0/all/0/1">Lin Pan</a>, <a href="http://arxiv.org/find/eess/1/au:+Luan_P/0/1/0/all/0/1">Pengpeng Luan</a>, <a href="http://arxiv.org/find/eess/1/au:+Tu_H/0/1/0/all/0/1">Hongbin Tu</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1">Xiong Li</a></p>
<p>Automatic segmentation of curvilinear objects in medical images plays an
important role in the diagnosis and evaluation of human diseases, yet it is a
challenging uncertainty in the complex segmentation tasks due to different
issues such as various image appearances, low contrast between curvilinear
objects and their surrounding backgrounds, thin and uneven curvilinear
structures, and improper background illumination conditions. To overcome these
challenges, we present a unique curvilinear structure segmentation framework
based on an oriented derivative of stick (ODoS) filter and a deep learning
network for curvilinear object segmentation in medical images. Currently, a
large number of deep learning models emphasize developing deep architectures
and ignore capturing the structural features of curvilinear objects, which may
lead to unsatisfactory results. Consequently, a new approach that incorporates
an ODoS filter as part of a deep learning network is presented to improve the
spatial attention of curvilinear objects. Specifically, the input image is
transfered into four-channel image constructed by the ODoS filter. In which,
the original image is considered the principal part to describe various image
appearance and complex background illumination conditions, a multi-step
strategy is used to enhance the contrast between curvilinear objects and their
surrounding backgrounds, and a vector field is applied to discriminate thin and
uneven curvilinear structures. Subsequently, a deep learning framework is
employed to extract various structural features for curvilinear object
segmentation in medical images. The performance of the computational model is
validated in experiments conducted on the publicly available DRIVE, STARE and
CHASEDB1 datasets. The experimental results indicate that the presented model
yields surprising results compared with those of some state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.07902">A Nonstochastic Control Approach to Optimization. (arXiv:2301.07902v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xinyi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hazan_E/0/1/0/all/0/1">Elad Hazan</a></p>
<p>Selecting the best hyperparameters for a particular optimization instance,
such as the learning rate and momentum, is an important but nonconvex problem.
As a result, iterative optimization methods such as hypergradient descent lack
global optimality guarantees in general.
</p>
<p>We propose an online nonstochastic control methodology for mathematical
optimization. First, we formalize the setting of meta-optimization, an online
learning formulation of learning the best optimization algorithm from a class
of methods. The meta-optimization problem over gradient-based methods can be
framed as a feedback control problem over the choice of hyperparameters,
including the learning rate, momentum, and the preconditioner.
</p>
<p>Although the original optimal control problem is nonconvex, we show how
recent methods from online nonstochastic control using convex relaxations can
be used to overcome the challenge of nonconvexity, and obtain regret guarantees
against the best offline solution. This guarantees that in meta-optimization,
given a sequence of optimization problems, we can learn a method that attains
convergence comparable to that of the best optimization method in hindsight
from a class of methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.01178">Convolutional Neural Operators for robust and accurate learning of PDEs. (arXiv:2302.01178v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Raonic_B/0/1/0/all/0/1">Bogdan Raoni&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Molinaro_R/0/1/0/all/0/1">Roberto Molinaro</a>, <a href="http://arxiv.org/find/cs/1/au:+Ryck_T/0/1/0/all/0/1">Tim De Ryck</a>, <a href="http://arxiv.org/find/cs/1/au:+Rohner_T/0/1/0/all/0/1">Tobias Rohner</a>, <a href="http://arxiv.org/find/cs/1/au:+Bartolucci_F/0/1/0/all/0/1">Francesca Bartolucci</a>, <a href="http://arxiv.org/find/cs/1/au:+Alaifari_R/0/1/0/all/0/1">Rima Alaifari</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1">Siddhartha Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Bezenac_E/0/1/0/all/0/1">Emmanuel de B&#xe9;zenac</a></p>
<p>Although very successfully used in conventional machine learning, convolution
based neural network architectures -- believed to be inconsistent in function
space -- have been largely ignored in the context of learning solution
operators of PDEs. Here, we present novel adaptations for convolutional neural
networks to demonstrate that they are indeed able to process functions as
inputs and outputs. The resulting architecture, termed as convolutional neural
operators (CNOs), is designed specifically to preserve its underlying
continuous nature, even when implemented in a discretized form on a computer.
We prove a universality theorem to show that CNOs can approximate operators
arising in PDEs to desired accuracy. CNOs are tested on a novel suite of
benchmarks, encompassing a diverse set of PDEs with possibly multi-scale
solutions and are observed to significantly outperform baselines, paving the
way for an alternative framework for robust and accurate operator learning. Our
code is publicly available at
https://github.com/bogdanraonic3/ConvolutionalNeuralOperator
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.03750">Linking convolutional kernel size to generalization bias in face analysis CNNs. (arXiv:2302.03750v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1">Hao Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Caro_J/0/1/0/all/0/1">Josue Ortega Caro</a>, <a href="http://arxiv.org/find/cs/1/au:+Maheshri_V/0/1/0/all/0/1">Vikram Maheshri</a>, <a href="http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1">Ankit B. Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Balakrishnan_G/0/1/0/all/0/1">Guha Balakrishnan</a></p>
<p>Training dataset biases are by far the most scrutinized factors when
explaining algorithmic biases of neural networks. In contrast, hyperparameters
related to the neural network architecture have largely been ignored even
though different network parameterizations are known to induce different
implicit biases over learned features. For example, convolutional kernel size
is known to affect the frequency content of features learned in CNNs. In this
work, we present a causal framework for linking an architectural hyperparameter
to out-of-distribution algorithmic bias. Our framework is experimental, in that
we train several versions of a network with an intervention to a specific
hyperparameter, and measure the resulting causal effect of this choice on
performance bias when a particular out-of-distribution image perturbation is
applied. In our experiments, we focused on measuring the causal relationship
between convolutional kernel size and face analysis classification bias across
different subpopulations (race/gender), with respect to high-frequency image
details. We show that modifying kernel size, even in one layer of a CNN,
changes the frequency content of learned features significantly across data
subgroups leading to biased generalization performance even in the presence of
a balanced dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.04440">Feature Likelihood Score: Evaluating the Generalization of Generative Models Using Samples. (arXiv:2302.04440v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiralerspong_M/0/1/0/all/0/1">Marco Jiralerspong</a>, <a href="http://arxiv.org/find/cs/1/au:+Bose_A/0/1/0/all/0/1">Avishek Joey Bose</a>, <a href="http://arxiv.org/find/cs/1/au:+Gemp_I/0/1/0/all/0/1">Ian Gemp</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1">Chongli Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Bachrach_Y/0/1/0/all/0/1">Yoram Bachrach</a>, <a href="http://arxiv.org/find/cs/1/au:+Gidel_G/0/1/0/all/0/1">Gauthier Gidel</a></p>
<p>The past few years have seen impressive progress in the development of deep
generative models capable of producing high-dimensional, complex, and
photo-realistic data. However, current methods for evaluating such models
remain incomplete: standard likelihood-based metrics do not always apply and
rarely correlate with perceptual fidelity, while sample-based metrics, such as
FID, are insensitive to overfitting, i.e., inability to generalize beyond the
training set. To address these limitations, we propose a new metric called the
Feature Likelihood Score (FLS), a parametric sample-based score that uses
density estimation to provide a comprehensive trichotomic evaluation accounting
for novelty (i.e., different from the training samples), fidelity, and
diversity of generated samples. We empirically demonstrate the ability of FLS
to identify specific overfitting problem cases, where previously proposed
metrics fail. We also extensively evaluate FLS on various image datasets and
model classes, demonstrating its ability to match intuitions of previous
metrics like FID while offering a more comprehensive evaluation of generative
models. Code is available at https://github.com/marcojira/fls.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.04611">A Text-guided Protein Design Framework. (arXiv:2302.04611v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shengchao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yanjing Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhuoxinran Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gitter_A/0/1/0/all/0/1">Anthony Gitter</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yutao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Jiarui Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zhao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_W/0/1/0/all/0/1">Weili Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramanathan_A/0/1/0/all/0/1">Arvind Ramanathan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Chaowei Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jian Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1">Hongyu Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1">Anima Anandkumar</a></p>
<p>Current AI-assisted protein design mainly utilizes protein sequential and
structural information. Meanwhile, there exists tremendous knowledge curated by
humans in the text format describing proteins' high-level functionalities. Yet,
whether the incorporation of such text data can help protein design tasks has
not been explored. To bridge this gap, we propose ProteinDT, a multi-modal
framework that leverages textual descriptions for protein design. ProteinDT
consists of three subsequent steps: ProteinCLAP which aligns the representation
of two modalities, a facilitator that generates the protein representation from
the text modality, and a decoder that creates the protein sequences from the
representation. To train ProteinDT, we construct a large dataset,
SwissProtCLAP, with 441K text and protein pairs. We quantitatively verify the
effectiveness of ProteinDT on three challenging tasks: (1) over 90\% accuracy
for text-guided protein generation; (2) best hit ratio on 10 zero-shot
text-guided protein editing tasks; (3) superior performance on four out of six
protein property prediction benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.06884">Conservative State Value Estimation for Offline Reinforcement Learning. (arXiv:2302.06884v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Liting Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1">Jie Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1">Zhengdao Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1">Qingwei Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajmohan_S/0/1/0/all/0/1">Saravan Rajmohan</a>, <a href="http://arxiv.org/find/cs/1/au:+Moscibroda_T/0/1/0/all/0/1">Thomas Moscibroda</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dongmei Zhang</a></p>
<p>Offline reinforcement learning faces a significant challenge of value
over-estimation due to the distributional drift between the dataset and the
current learned policy, leading to learning failure in practice. The common
approach is to incorporate a penalty term to reward or value estimation in the
Bellman iterations. Meanwhile, to avoid extrapolation on out-of-distribution
(OOD) states and actions, existing methods focus on conservative Q-function
estimation. In this paper, we propose Conservative State Value Estimation
(CSVE), a new approach that learns conservative V-function via directly
imposing penalty on OOD states. Compared to prior work, CSVE allows more
effective state value estimation with conservative guarantees and further
better policy optimization. Further, we apply CSVE and develop a practical
actor-critic algorithm in which the critic does the conservative value
estimation by additionally sampling and penalizing the states \emph{around} the
dataset, and the actor applies advantage weighted updates extended with state
exploration to improve the policy. We evaluate in classic continual control
tasks of D4RL, showing that our method performs better than the conservative
Q-function learning methods and is strongly competitive among recent SOTA
methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.08224">DIFUSCO: Graph-based Diffusion Solvers for Combinatorial Optimization. (arXiv:2302.08224v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1">Zhiqing Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yiming Yang</a></p>
<p>Neural network-based Combinatorial Optimization (CO) methods have shown
promising results in solving various NP-complete (NPC) problems without relying
on hand-crafted domain knowledge. This paper broadens the current scope of
neural solvers for NPC problems by introducing a new graph-based diffusion
framework, namely DIFUSCO. Our framework casts NPC problems as discrete {0,
1}-vector optimization problems and leverages graph-based denoising diffusion
models to generate high-quality solutions. We investigate two types of
diffusion models with Gaussian and Bernoulli noise, respectively, and devise an
effective inference schedule to enhance the solution quality. We evaluate our
methods on two well-studied NPC combinatorial optimization problems: Traveling
Salesman Problem (TSP) and Maximal Independent Set (MIS). Experimental results
show that DIFUSCO strongly outperforms the previous state-of-the-art neural
solvers, improving the performance gap between ground-truth and neural solvers
from 1.76% to 0.46% on TSP-500, from 2.46% to 1.17% on TSP-1000, and from 3.19%
to 2.58% on TSP10000. For the MIS problem, DIFUSCO outperforms the previous
state-of-the-art neural solver on the challenging SATLIB benchmark.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.04729">Stealing the Decoding Algorithms of Language Models. (arXiv:2303.04729v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Naseh_A/0/1/0/all/0/1">Ali Naseh</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1">Kalpesh Krishna</a>, <a href="http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1">Mohit Iyyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Houmansadr_A/0/1/0/all/0/1">Amir Houmansadr</a></p>
<p>A key component of generating text from modern language models (LM) is the
selection and tuning of decoding algorithms. These algorithms determine how to
generate text from the internal probability distribution generated by the LM.
The process of choosing a decoding algorithm and tuning its hyperparameters
takes significant time, manual effort, and computation, and it also requires
extensive human evaluation. Therefore, the identity and hyperparameters of such
decoding algorithms are considered to be extremely valuable to their owners. In
this work, we show, for the first time, that an adversary with typical API
access to an LM can steal the type and hyperparameters of its decoding
algorithms at very low monetary costs. Our attack is effective against popular
LMs used in text generation APIs, including GPT-2, GPT-3 and GPT-Neo. We
demonstrate the feasibility of stealing such information with only a few
dollars, e.g., $\$0.8$, $\$1$, $\$4$, and $\$40$ for the four versions of
GPT-3.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.04788">Enabling Non-Linear Quantum Operations through Variational Quantum Splines. (arXiv:2303.04788v3 [quant-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Inajetovic_M/0/1/0/all/0/1">Matteo Antonio Inajetovic</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Orazi_F/0/1/0/all/0/1">Filippo Orazi</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Macaluso_A/0/1/0/all/0/1">Antonio Macaluso</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Lodi_S/0/1/0/all/0/1">Stefano Lodi</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Sartori_C/0/1/0/all/0/1">Claudio Sartori</a></p>
<p>The postulates of quantum mechanics impose only unitary transformations on
quantum states, which is a severe limitation for quantum machine learning
algorithms. Quantum Splines (QSplines) have recently been proposed to
approximate quantum activation functions to introduce non-linearity in quantum
algorithms. However, QSplines make use of the HHL as a subroutine and require a
fault-tolerant quantum computer to be correctly implemented. This work proposes
the Generalised Hybrid Quantum Splines (GHQSplines), a novel method for
approximating non-linear quantum activation functions using hybrid
quantum-classical computation. The GHQSplines overcome the highly demanding
requirements of the original QSplines in terms of quantum hardware and can be
implemented using near-term quantum computers. Furthermore, the proposed method
relies on a flexible problem representation for non-linear approximation and it
is suitable to be embedded in existing quantum neural network architectures. In
addition, we provide a practical implementation of the GHQSplines using
Pennylane and show that our model outperforms the original QSplines in terms of
quality of fitting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.06470">Prefix-Tree Decoding for Predicting Mass Spectra from Molecules. (arXiv:2303.06470v3 [q-bio.QM] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Goldman_S/0/1/0/all/0/1">Samuel Goldman</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Bradshaw_J/0/1/0/all/0/1">John Bradshaw</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Xin_J/0/1/0/all/0/1">Jiayi Xin</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Coley_C/0/1/0/all/0/1">Connor W. Coley</a></p>
<p>Computational predictions of mass spectra from molecules have enabled the
discovery of clinically relevant metabolites. However, such predictive tools
are still limited as they occupy one of two extremes, either operating (a) by
fragmenting molecules combinatorially with overly rigid constraints on
potential rearrangements and poor time complexity or (b) by decoding lossy and
nonphysical discretized spectra vectors. In this work, we use a new
intermediate strategy for predicting mass spectra from molecules by treating
mass spectra as sets of molecular formulae, which are themselves multisets of
atoms. After first encoding an input molecular graph, we decode a set of
molecular subformulae, each of which specify a predicted peak in the mass
spectrum, the intensities of which are predicted by a second model. Our key
insight is to overcome the combinatorial possibilities for molecular
subformulae by decoding the formula set using a prefix tree structure,
atom-type by atom-type, representing a general method for ordered multiset
decoding. We show promising empirical results on mass spectra prediction tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.09051">Robust Evaluation of Diffusion-Based Adversarial Purification. (arXiv:2303.09051v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1">Minjong Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1">Dongwoo Kim</a></p>
<p>We question the current evaluation practice on diffusion-based purification
methods. Diffusion-based purification methods aim to remove adversarial effects
from an input data point at test time. The approach gains increasing attention
as an alternative to adversarial training due to the disentangling between
training and testing. Well-known white-box attacks are often employed to
measure the robustness of the purification. However, it is unknown whether
these attacks are the most effective for the diffusion-based purification since
the attacks are often tailored for adversarial training. We analyze the current
practices and provide a new guideline for measuring the robustness of
purification methods against adversarial attacks. Based on our analysis, we
further propose a new purification strategy improving robustness compared to
the current diffusion-based purification methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.16563">Skill Reinforcement Learning and Planning for Open-World Long-Horizon Tasks. (arXiv:2303.16563v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1">Haoqi Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hongcheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_F/0/1/0/all/0/1">Feiyang Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1">Penglin Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1">Hao Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Zongqing Lu</a></p>
<p>We study building multi-task agents in open-world environments. Without human
demonstrations, learning to accomplish long-horizon tasks in a large open-world
environment with reinforcement learning (RL) is extremely inefficient. To
tackle this challenge, we convert the multi-task learning problem into learning
basic skills and planning over the skills. Using the popular open-world game
Minecraft as the testbed, we propose three types of fine-grained basic skills,
and use RL with intrinsic rewards to acquire skills. A novel Finding-skill that
performs exploration to find diverse items provides better initialization for
other skills, improving the sample efficiency for skill learning. In skill
planning, we leverage the prior knowledge in Large Language Models to find the
relationships between skills and build a skill graph. When the agent is solving
a task, our skill search algorithm walks on the skill graph and generates the
proper skill plans for the agent. In experiments, our method accomplishes 40
diverse Minecraft tasks, where many tasks require sequentially executing for
more than 10 skills. Our method outperforms baselines by a large margin and is
the most sample-efficient demonstration-free RL method to solve Minecraft Tech
Tree tasks. The project's website and code can be found at
https://sites.google.com/view/plan4mc.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.17580">HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face. (arXiv:2303.17580v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yongliang Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1">Kaitao Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1">Xu Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dongsheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1">Weiming Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1">Yueting Zhuang</a></p>
<p>Solving complicated AI tasks with different domains and modalities is a key
step toward artificial general intelligence. While there are numerous AI models
available for various domains and modalities, they cannot handle complicated AI
tasks autonomously. Considering large language models (LLMs) have exhibited
exceptional abilities in language understanding, generation, interaction, and
reasoning, we advocate that LLMs could act as a controller to manage existing
AI models to solve complicated AI tasks, with language serving as a generic
interface to empower this. Based on this philosophy, we present HuggingGPT, an
LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI
models in machine learning communities (e.g., Hugging Face) to solve AI tasks.
Specifically, we use ChatGPT to conduct task planning when receiving a user
request, select models according to their function descriptions available in
Hugging Face, execute each subtask with the selected AI model, and summarize
the response according to the execution results. By leveraging the strong
language capability of ChatGPT and abundant AI models in Hugging Face,
HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different
modalities and domains and achieve impressive results in language, vision,
speech, and other challenging tasks, which paves a new way towards the
realization of artificial general intelligence.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.08424">Long-term Forecasting with TiDE: Time-series Dense Encoder. (arXiv:2304.08424v4 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Das_A/0/1/0/all/0/1">Abhimanyu Das</a>, <a href="http://arxiv.org/find/stat/1/au:+Kong_W/0/1/0/all/0/1">Weihao Kong</a>, <a href="http://arxiv.org/find/stat/1/au:+Leach_A/0/1/0/all/0/1">Andrew Leach</a>, <a href="http://arxiv.org/find/stat/1/au:+Mathur_S/0/1/0/all/0/1">Shaan Mathur</a>, <a href="http://arxiv.org/find/stat/1/au:+Sen_R/0/1/0/all/0/1">Rajat Sen</a>, <a href="http://arxiv.org/find/stat/1/au:+Yu_R/0/1/0/all/0/1">Rose Yu</a></p>
<p>Recent work has shown that simple linear models can outperform several
Transformer based approaches in long term time-series forecasting. Motivated by
this, we propose a Multi-layer Perceptron (MLP) based encoder-decoder model,
Time-series Dense Encoder (TiDE), for long-term time-series forecasting that
enjoys the simplicity and speed of linear models while also being able to
handle covariates and non-linear dependencies. Theoretically, we prove that the
simplest linear analogue of our model can achieve near optimal error rate for
linear dynamical systems (LDS) under some assumptions. Empirically, we show
that our method can match or outperform prior approaches on popular long-term
time-series forecasting benchmarks while being 5-10x faster than the best
Transformer based model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.10159">Deep-Q Learning with Hybrid Quantum Neural Network on Solving Maze Problems. (arXiv:2304.10159v3 [quant-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Chen_H/0/1/0/all/0/1">Hao-Yuan Chen</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Chang_Y/0/1/0/all/0/1">Yen-Jui Chang</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Liao_S/0/1/0/all/0/1">Shih-Wei Liao</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Chang_C/0/1/0/all/0/1">Ching-Ray Chang</a></p>
<p>Quantum computing holds great potential for advancing the limitations of
machine learning algorithms to handle higher dimensions of data and reduce
overall training parameters in deep learning (DL) models. This study uses a
trainable variational quantum circuit (VQC) on a gate-based quantum computing
model to investigate the potential for quantum benefit in a model-free
reinforcement learning problem. Through a comprehensive investigation and
evaluation of the current model and capabilities of quantum computers, we
designed and trained a novel hybrid quantum neural network based on the latest
Qiskit and PyTorch framework. We compared its performance with a full-classical
CNN with and without an incorporated VQC. Our research provides insights into
the potential of deep quantum learning to solve a maze problem and,
potentially, other reinforcement learning problems. We conclude that
reinforcement learning problems can be practical with reasonable training
epochs. Moreover, a comparative study of full-classical and hybrid quantum
neural networks is discussed to understand these two approaches' performance,
advantages, and disadvantages to deep-Q learning problems, especially on
larger-scale maze problems larger than 4x4.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.10613">Debiasing Conditional Stochastic Optimization. (arXiv:2304.10613v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1">Lie He</a>, <a href="http://arxiv.org/find/cs/1/au:+Kasiviswanathan_S/0/1/0/all/0/1">Shiva Prasad Kasiviswanathan</a></p>
<p>In this paper, we study the conditional stochastic optimization (CSO) problem
which covers a variety of applications including portfolio selection,
reinforcement learning, robust learning, causal inference, etc. The
sample-averaged gradient of the CSO objective is biased due to its nested
structure, and therefore requires a high sample complexity for convergence. We
introduce a general stochastic extrapolation technique that effectively reduces
the bias. We show that for nonconvex smooth objectives, combining this
extrapolation with variance reduction techniques can achieve a significantly
better sample complexity than the existing bounds. Additionally, we develop new
algorithms for the finite-sum variant of the CSO problem that also
significantly improve upon existing results. Finally, we believe that our
debiasing technique has the potential to be a useful tool for addressing
similar challenges in other stochastic optimization problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.02217">A Theoretical Perspective of Machine Learning with Computational Resource Concerns. (arXiv:2305.02217v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhi-Hua Zhou</a></p>
<p>Conventional theoretical machine learning studies generally assume explicitly
or implicitly that there are enough or even infinitely supplied computational
resources. In real practice, however, computational resources are usually
limited, and the performance of machine learning depends not only on how many
data have been received, but also on how many data can be handled with the
computational resources available. Note that most current ``intelligent
supercomputing'' facilities work like exclusive operating systems, where a
fixed amount of resources are allocated to a machine learning task without
adaptive scheduling strategies considering important factors such as learning
performance demands and learning process status. In this article, we introduce
the notion of machine learning throughput, define Computational Resource
Efficient Learning (CoRE-Learning) and present a theoretical framework that
takes into account the influence of computational resources in learning theory.
This framework can be naturally applied to stream learning where the incoming
data streams can be potentially endless with overwhelming size and it is
impractical to assume that all received data can be handled in time. It may
also provide a theoretical perspective for the design of intelligent
supercomputing operating systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.03047">Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision. (arXiv:2305.03047v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1">Zhiqing Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yikang Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1">Qinhong Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongxin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhenfang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cox_D/0/1/0/all/0/1">David Cox</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yiming Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1">Chuang Gan</a></p>
<p>Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised
fine-tuning (SFT) with human annotations and reinforcement learning from human
feedback (RLHF) to align the output of large language models (LLMs) with human
intentions, ensuring they are helpful, ethical, and reliable. However, this
dependence can significantly constrain the true potential of AI-assistant
agents due to the high cost of obtaining human supervision and the related
issues on quality, reliability, diversity, self-consistency, and undesirable
biases. To address these challenges, we propose a novel approach called
SELF-ALIGN, which combines principle-driven reasoning and the generative power
of LLMs for the self-alignment of AI agents with minimal human supervision. Our
approach encompasses four stages: first, we use an LLM to generate synthetic
prompts, and a topic-guided method to augment the prompt diversity; second, we
use a small set of human-written principles for AI models to follow, and guide
the LLM through in-context learning from demonstrations (of principles
application) to produce helpful, ethical, and reliable responses to user's
queries; third, we fine-tune the original LLM with the high-quality
self-aligned responses so that the resulting model can generate desirable
responses for each query directly without the principle set and the
demonstrations anymore; and finally, we offer a refinement step to address the
issues of overly-brief or indirect responses. Applying SELF-ALIGN to the
LLaMA-65b base language model, we develop an AI assistant named Dromedary. With
fewer than 300 lines of human annotations (including &lt; 200 seed prompts, 16
generic principles, and 5 exemplars for in-context learning). Dromedary
significantly surpasses the performance of several state-of-the-art AI systems,
including Text-Davinci-003 and Alpaca, on benchmark datasets with various
settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.04782">HistAlign: Improving Context Dependency in Language Generation by Aligning with History. (arXiv:2305.04782v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wan_D/0/1/0/all/0/1">David Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shiyue Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1">Mohit Bansal</a></p>
<p>Language models (LMs) can generate hallucinations and incoherent outputs,
which highlights their weak context dependency. Cache-LMs, which augment LMs
with a memory of recent history, can increase context dependency and have shown
remarkable performance in diverse language generation tasks. However, we find
that even with training, the performance gain stemming from the cache component
of current cache-LMs is suboptimal due to the misalignment between the current
hidden states and those stored in the memory. In this work, we present
HistAlign, a new training approach to ensure good cache alignment such that the
model receives useful signals from the history. We first prove our concept on a
simple and synthetic task where the memory is essential for correct
predictions, and we show that the cache component of HistAlign is better
aligned and improves overall performance. Next, we evaluate HistAlign on
diverse downstream language generation tasks, including prompt continuation,
abstractive summarization, and data-to-text. We demonstrate that HistAlign
improves text coherence and faithfulness in open-ended and conditional
generation settings respectively. HistAlign is also generalizable across
different model families, showcasing its strength in improving context
dependency of LMs in diverse scenarios. Our code is publicly available at
https://github.com/meetdavidwan/histalign
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.10601">Tree of Thoughts: Deliberate Problem Solving with Large Language Models. (arXiv:2305.10601v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1">Shunyu Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1">Dian Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jeffrey Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Shafran_I/0/1/0/all/0/1">Izhak Shafran</a>, <a href="http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1">Thomas L. Griffiths</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yuan Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1">Karthik Narasimhan</a></p>
<p>Language models are increasingly being deployed for general problem solving
across a wide range of tasks, but are still confined to token-level,
left-to-right decision-making processes during inference. This means they can
fall short in tasks that require exploration, strategic lookahead, or where
initial decisions play a pivotal role. To surmount these challenges, we
introduce a new framework for language model inference, Tree of Thoughts (ToT),
which generalizes over the popular Chain of Thought approach to prompting
language models, and enables exploration over coherent units of text (thoughts)
that serve as intermediate steps toward problem solving. ToT allows LMs to
perform deliberate decision making by considering multiple different reasoning
paths and self-evaluating choices to decide the next course of action, as well
as looking ahead or backtracking when necessary to make global choices. Our
experiments show that ToT significantly enhances language models'
problem-solving abilities on three novel tasks requiring non-trivial planning
or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in
Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of
tasks, our method achieved a success rate of 74%. Code repo with all prompts:
https://github.com/princeton-nlp/tree-of-thought-llm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.11032">Optimistic Natural Policy Gradient: a Simple Efficient Policy Optimization Framework for Online RL. (arXiv:2305.11032v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qinghua Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Weisz_G/0/1/0/all/0/1">Gell&#xe9;rt Weisz</a>, <a href="http://arxiv.org/find/cs/1/au:+Gyorgy_A/0/1/0/all/0/1">Andr&#xe1;s Gy&#xf6;rgy</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1">Chi Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Szepesvari_C/0/1/0/all/0/1">Csaba Szepesv&#xe1;ri</a></p>
<p>While policy optimization algorithms have played an important role in recent
empirical success of Reinforcement Learning (RL), the existing theoretical
understanding of policy optimization remains rather limited -- they are either
restricted to tabular MDPs or suffer from highly suboptimal sample complexity,
especial in online RL where exploration is necessary. This paper proposes a
simple efficient policy optimization framework -- Optimistic NPG for online RL.
Optimistic NPG can be viewed as a simple combination of the classic natural
policy gradient (NPG) algorithm [Kakade, 2001] with optimistic policy
evaluation subroutines to encourage exploration. For $d$-dimensional linear
MDPs, Optimistic NPG is computationally efficient, and learns an
$\varepsilon$-optimal policy within $\tilde{O}(d^2/\varepsilon^3)$ samples,
which is the first computationally efficient algorithm whose sample complexity
has the optimal dimension dependence $\tilde{\Theta}(d^2)$. It also improves
over state-of-the-art results of policy optimization algorithms [Zanette et
al., 2021] by a factor of $d$. In the realm of general function approximation,
which subsumes linear MDPs, Optimistic NPG, to our best knowledge, stands as
the first policy optimization algorithm that achieves polynomial sample
complexity for learning near-optimal policies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13289">Achieving the Minimax Optimal Sample Complexity of Offline Reinforcement Learning: A DRO-Based Approach. (arXiv:2305.13289v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yue Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1">Jinjun Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_S/0/1/0/all/0/1">Shaofeng Zou</a></p>
<p>Offline reinforcement learning aims to learn from pre-collected datasets
without active exploration. This problem faces significant challenges,
including limited data availability and distributional shifts. Existing
approaches adopt a pessimistic stance towards uncertainty by penalizing rewards
of under-explored state-action pairs to estimate value functions
conservatively. In this paper, we show that the distributionally robust
optimization (DRO) based approach can also address these challenges and is
minimax optimal. Specifically, we directly model the uncertainty in the
transition kernel and construct an uncertainty set of statistically plausible
transition kernels. We then find the policy that optimizes the worst-case
performance over this uncertainty set. We first design a metric-based
Hoeffding-style uncertainty set such that with high probability the true
transition kernel is in this set. We prove that to achieve a sub-optimality gap
of $\epsilon$, the sample complexity is
$\mathcal{O}(S^2C^{\pi^*}\epsilon^{-2}(1-\gamma)^{-4})$, where $\gamma$ is the
discount factor, $S$ is the number of states, and $C^{\pi^*}$ is the
single-policy clipped concentrability coefficient which quantifies the
distribution shift. To achieve the optimal sample complexity, we further
propose a less conservative Bernstein-style uncertainty set, which, however,
does not necessarily include the true transition kernel. We show that an
improved sample complexity of
$\mathcal{O}(SC^{\pi^*}\epsilon^{-2}(1-\gamma)^{-3})$ can be obtained, which
matches with the minimax lower bound for offline reinforcement learning, and
thus is minimax optimal.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13998">SMT 2.0: A Surrogate Modeling Toolbox with a focus on Hierarchical and Mixed Variables Gaussian Processes. (arXiv:2305.13998v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Saves_P/0/1/0/all/0/1">Paul Saves</a>, <a href="http://arxiv.org/find/cs/1/au:+Lafage_R/0/1/0/all/0/1">Remi Lafage</a>, <a href="http://arxiv.org/find/cs/1/au:+Bartoli_N/0/1/0/all/0/1">Nathalie Bartoli</a>, <a href="http://arxiv.org/find/cs/1/au:+Diouane_Y/0/1/0/all/0/1">Youssef Diouane</a>, <a href="http://arxiv.org/find/cs/1/au:+Bussemaker_J/0/1/0/all/0/1">Jasper Bussemaker</a>, <a href="http://arxiv.org/find/cs/1/au:+Lefebvre_T/0/1/0/all/0/1">Thierry Lefebvre</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1">John T. Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Morlier_J/0/1/0/all/0/1">Joseph Morlier</a>, <a href="http://arxiv.org/find/cs/1/au:+Martins_J/0/1/0/all/0/1">Joaquim R. R. A. Martins</a></p>
<p>The Surrogate Modeling Toolbox (SMT) is an open-source Python package that
offers a collection of surrogate modeling methods, sampling techniques, and a
set of sample problems. This paper presents SMT 2.0, a major new release of SMT
that introduces significant upgrades and new features to the toolbox. This
release adds the capability to handle mixed-variable surrogate models and
hierarchical variables. These types of variables are becoming increasingly
important in several surrogate modeling applications. SMT 2.0 also improves SMT
by extending sampling methods, adding new surrogate models, and computing
variance and kernel derivatives for Kriging. This release also includes new
functions to handle noisy and use multifidelity data. To the best of our
knowledge, SMT 2.0 is the first open-source surrogate library to propose
surrogate models for hierarchical and mixed inputs. This open-source software
is distributed under the New BSD license.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14296">USB: A Unified Summarization Benchmark Across Tasks and Domains. (arXiv:2305.14296v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1">Kundan Krishna</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1">Prakhar Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramprasad_S/0/1/0/all/0/1">Sanjana Ramprasad</a>, <a href="http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1">Byron C. Wallace</a>, <a href="http://arxiv.org/find/cs/1/au:+Bigham_J/0/1/0/all/0/1">Jeffrey P. Bigham</a>, <a href="http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1">Zachary C. Lipton</a></p>
<p>While the NLP community has produced numerous summarization benchmarks, none
provide the rich annotations required to simultaneously address many important
problems related to control and reliability. We introduce a Wikipedia-derived
benchmark, complemented by a rich set of crowd-sourced annotations, that
supports $8$ interrelated tasks: (i) extractive summarization; (ii) abstractive
summarization; (iii) topic-based summarization; (iv) compressing selected
sentences into a one-line summary; (v) surfacing evidence for a summary
sentence; (vi) predicting the factual accuracy of a summary sentence; (vii)
identifying unsubstantiated spans in a summary sentence; (viii) correcting
factual errors in summaries. We compare various methods on this benchmark and
discover that on multiple tasks, moderately-sized fine-tuned models
consistently outperform much larger few-shot prompted language models. For
factuality-related tasks, we also evaluate existing heuristics to create
training data and find that training on them results in worse performance than
training on $20\times$ less human-labeled data. Our articles draw from $6$
domains, facilitating cross-domain analysis. On some tasks, the amount of
training data matters more than the domain where it comes from, while for other
tasks training specifically on data from the target domain, even if limited, is
more beneficial.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14735">Centering the Margins: Outlier-Based Identification of Harmed Populations in Toxicity Detection. (arXiv:2305.14735v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Raman_V/0/1/0/all/0/1">Vyoma Raman</a>, <a href="http://arxiv.org/find/cs/1/au:+Fleisig_E/0/1/0/all/0/1">Eve Fleisig</a>, <a href="http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1">Dan Klein</a></p>
<p>The impact of AI models on marginalized communities has traditionally been
measured by identifying performance differences between specified demographic
subgroups. Though this approach aims to center vulnerable groups, it risks
obscuring patterns of harm faced by intersectional subgroups or shared across
multiple groups. To address this, we draw on theories of marginalization from
disability studies and related disciplines, which state that people farther
from the norm face greater adversity, to consider the "margins" in the domain
of toxicity detection. We operationalize the "margins" of a dataset by
employing outlier detection to identify text about people with demographic
attributes distant from the "norm". We find that model performance is
consistently worse for demographic outliers, with mean squared error (MSE)
between outliers and non-outliers up to 70.4% worse across toxicity types. It
is also worse for text outliers, with a MSE up to 68.4% higher for outliers
than non-outliers. We also find text and demographic outliers to be
particularly susceptible to errors in the classification of severe toxicity and
identity attacks. Compared to analysis of disparities using traditional
demographic breakdowns, we find that our outlier analysis frequently surfaces
greater harms faced by a larger, more intersectional group, which suggests that
outlier analysis is particularly beneficial for identifying harms against those
groups.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14912">SVDinsTN: A Tensor Network Paradigm for Efficient Structure Search from Regularized Modeling Perspective. (arXiv:2305.14912v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1">Yu-Bang Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xi-Le Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1">Junhua Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1">Qibin Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Heng-Chao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1">Ting-Zhu Huang</a></p>
<p>Tensor network (TN) representation is a powerful technique for computer
vision and machine learning. TN structure search (TN-SS) aims to search for a
customized structure to achieve a compact representation, which is a
challenging NP-hard problem. Recent "sampling-evaluation-based" methods require
sampling an extensive collection of structures and evaluating them one by one,
resulting in prohibitively high computational costs. To address this issue, we
propose a novel TN paradigm, named SVD-inspired TN decomposition (SVDinsTN),
which allows us to efficiently solve the TN-SS problem from a regularized
modeling perspective, eliminating the repeated structure evaluations. To be
specific, by inserting a diagonal factor for each edge of the fully-connected
TN, SVDinsTN allows us to calculate TN cores and diagonal factors
simultaneously, with the factor sparsity revealing a compact TN structure. In
theory, we prove a convergence guarantee for the proposed method. Experimental
results demonstrate that the proposed method achieves approximately 100 to 1000
times acceleration compared to the state-of-the-art TN-SS methods while
maintaining a comparable representation ability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19125">Graph Generation with $K^2$-trees. (arXiv:2305.19125v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1">Yunhui Jang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1">Dongwoo Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1">Sungsoo Ahn</a></p>
<p>Generating graphs from a target distribution is a significant challenge
across many domains, including drug discovery and social network analysis. In
this work, we introduce a novel graph generation method leveraging $K^2$-tree
representation, originally designed for lossless graph compression. The
$K^2$-tree representation {encompasses inherent hierarchy while enabling
compact graph generation}. In addition, we make contributions by (1) presenting
a sequential $K^2$-treerepresentation that incorporates pruning, flattening,
and tokenization processes and (2) introducing a Transformer-based architecture
designed to generate the sequence by incorporating a specialized tree
positional encoding scheme. Finally, we extensively evaluate our algorithm on
four general and two molecular graph datasets to confirm its superiority for
graph generation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19518">Label-Retrieval-Augmented Diffusion Models for Learning from Noisy Labels. (arXiv:2305.19518v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jian Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Ruiyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1">Tong Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1">Rohan Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zhiqiang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1">Tong Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Changyou Chen</a></p>
<p>Learning from noisy labels is an important and long-standing problem in
machine learning for real applications. One of the main research lines focuses
on learning a label corrector to purify potential noisy labels. However, these
methods typically rely on strict assumptions and are limited to certain types
of label noise. In this paper, we reformulate the label-noise problem from a
generative-model perspective, $\textit{i.e.}$, labels are generated by
gradually refining an initial random guess. This new perspective immediately
enables existing powerful diffusion models to seamlessly learn the stochastic
generative process. Once the generative uncertainty is modeled, we can perform
classification inference using maximum likelihood estimation of labels. To
mitigate the impact of noisy labels, we propose the
$\textbf{L}$abel-$\textbf{R}$etrieval-$\textbf{A}$ugmented (LRA) diffusion
model, which leverages neighbor consistency to effectively construct
pseudo-clean labels for diffusion training. Our model is flexible and general,
allowing easy incorporation of different types of conditional information,
$\textit{e.g.}$, use of pre-trained models, to further boost model performance.
Extensive experiments are conducted for evaluation. Our model achieves new
state-of-the-art (SOTA) results on all the standard real-world benchmark
datasets. Remarkably, by incorporating conditional information from the
powerful CLIP model, our method can boost the current SOTA accuracy by 10-20
absolute points in many cases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.00392">Coneheads: Hierarchy Aware Attention. (arXiv:2306.00392v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tseng_A/0/1/0/all/0/1">Albert Tseng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1">Tao Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Toni J.B. Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1">Christopher De Sa</a></p>
<p>Attention networks such as transformers have achieved state-of-the-art
performance in many domains. These networks rely heavily on the dot product
attention operator, which computes the similarity between two points by taking
their inner product. However, the inner product does not explicitly model the
complex structural properties of real world datasets, such as hierarchies
between data points. To remedy this, we introduce cone attention, a drop-in
replacement for dot product attention based on hyperbolic entailment cones.
Cone attention associates two points by the depth of their lowest common
ancestor in a hierarchy defined by hyperbolic cones, which intuitively measures
the divergence of two points and gives a hierarchy aware similarity score. We
test cone attention on a wide variety of models and tasks and show that it
improves task-level performance over dot product attention and other baselines,
and is able to match dot-product attention with significantly fewer parameters.
Our results suggest that cone attention is an effective way to capture
hierarchical relationships when calculating attention.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.03072">Explore to Generalize in Zero-Shot RL. (arXiv:2306.03072v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zisselman_E/0/1/0/all/0/1">Ev Zisselman</a>, <a href="http://arxiv.org/find/cs/1/au:+Lavie_I/0/1/0/all/0/1">Itai Lavie</a>, <a href="http://arxiv.org/find/cs/1/au:+Soudry_D/0/1/0/all/0/1">Daniel Soudry</a>, <a href="http://arxiv.org/find/cs/1/au:+Tamar_A/0/1/0/all/0/1">Aviv Tamar</a></p>
<p>We study zero-shot generalization in reinforcement learning-optimizing a
policy on a set of training tasks to perform well on a similar but unseen test
task. To mitigate overfitting, previous work explored different notions of
invariance to the task. However, on problems such as the ProcGen Maze, an
adequate solution that is invariant to the task visualization does not exist,
and therefore invariance-based approaches fail. Our insight is that learning a
policy that effectively $\textit{explores}$ the domain is harder to memorize
than a policy that maximizes reward for a specific task, and therefore we
expect such learned behavior to generalize well; we indeed demonstrate this
empirically on several domains that are difficult for invariance-based
approaches. Our $\textit{Explore to Generalize}$ algorithm (ExpGen) builds on
this insight: we train an additional ensemble of agents that optimize reward.
At test time, either the ensemble agrees on an action, and we generalize well,
or we take exploratory actions, which generalize well and drive us to a novel
part of the state space, where the ensemble may potentially agree again. We
show that our approach is the state-of-the-art on tasks of the ProcGen
challenge that have thus far eluded effective generalization, yielding a
success rate of $83\%$ on the Maze task and $74\%$ on Heist with $200$ training
levels. ExpGen can also be combined with an invariance based approach to gain
the best of both worlds, setting new state-of-the-art results on ProcGen.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.04527">ContriMix: Unsupervised disentanglement of content and attribute for domain generalization in microscopy image analysis. (arXiv:2306.04527v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Nguyen_T/0/1/0/all/0/1">Tan H. Nguyen</a>, <a href="http://arxiv.org/find/eess/1/au:+Juyal_D/0/1/0/all/0/1">Dinkar Juyal</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1">Jin Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Prakash_A/0/1/0/all/0/1">Aaditya Prakash</a>, <a href="http://arxiv.org/find/eess/1/au:+Nofallah_S/0/1/0/all/0/1">Shima Nofallah</a>, <a href="http://arxiv.org/find/eess/1/au:+Shah_C/0/1/0/all/0/1">Chintan Shah</a>, <a href="http://arxiv.org/find/eess/1/au:+Gullapally_S/0/1/0/all/0/1">Sai Chowdary Gullapally</a>, <a href="http://arxiv.org/find/eess/1/au:+Yu_L/0/1/0/all/0/1">Limin Yu</a>, <a href="http://arxiv.org/find/eess/1/au:+Griffin_M/0/1/0/all/0/1">Michael Griffin</a>, <a href="http://arxiv.org/find/eess/1/au:+Sampat_A/0/1/0/all/0/1">Anand Sampat</a>, <a href="http://arxiv.org/find/eess/1/au:+Abel_J/0/1/0/all/0/1">John Abel</a>, <a href="http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1">Justin Lee</a>, <a href="http://arxiv.org/find/eess/1/au:+Taylor_Weiner_A/0/1/0/all/0/1">Amaro Taylor-Weiner</a></p>
<p>Domain generalization is critical for real-world applications of machine
learning to microscopy images, including histopathology and fluorescence
imaging. Artifacts in these modalities arise through a complex combination of
factors relating to tissue collection and laboratory processing, as well as
factors intrinsic to patient samples. In fluorescence imaging, these artifacts
stem from variations across experimental batches. The complexity and subtlety
of these artifacts make the enumeration of data domains intractable. Therefore,
augmentation-based methods of domain generalization that require domain
identifiers and manual fine-tuning are inadequate in this setting. To overcome
this challenge, we introduce ContriMix, a domain generalization technique that
learns to generate synthetic images by disentangling and permuting the
biological content ("content") and technical variations ("attributes") in
microscopy images. ContriMix does not rely on domain identifiers or handcrafted
augmentations and makes no assumptions about the input characteristics of
images. We assess the performance of ContriMix on two pathology datasets
dealing with patch classification and Whole Slide Image label prediction tasks
respectively (Camelyon17-WILDS and RCC subtyping), and one fluorescence
microscopy dataset (RxRx1-WILDS). Without any access to domain identifiers at
train or test time, ContriMix performs similar or better than current
state-of-the-art methods in all these datasets, motivating its usage for
microscopy image analysis in real-world settings where domain information is
hard to come by. The code for ContriMix can be found at
https://gitlab.com/huutan86/contrimix
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.04633">Contrastive Lift: 3D Object Instance Segmentation by Slow-Fast Contrastive Fusion. (arXiv:2306.04633v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bhalgat_Y/0/1/0/all/0/1">Yash Bhalgat</a>, <a href="http://arxiv.org/find/cs/1/au:+Laina_I/0/1/0/all/0/1">Iro Laina</a>, <a href="http://arxiv.org/find/cs/1/au:+Henriques_J/0/1/0/all/0/1">Jo&#xe3;o F. Henriques</a>, <a href="http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1">Andrew Zisserman</a>, <a href="http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1">Andrea Vedaldi</a></p>
<p>Instance segmentation in 3D is a challenging task due to the lack of
large-scale annotated datasets. In this paper, we show that this task can be
addressed effectively by leveraging instead 2D pre-trained models for instance
segmentation. We propose a novel approach to lift 2D segments to 3D and fuse
them by means of a neural field representation, which encourages multi-view
consistency across frames. The core of our approach is a slow-fast clustering
objective function, which is scalable and well-suited for scenes with a large
number of objects. Unlike previous approaches, our method does not require an
upper bound on the number of objects or object tracking across frames. To
demonstrate the scalability of the slow-fast clustering, we create a new
semi-realistic dataset called the Messy Rooms dataset, which features scenes
with up to 500 objects per scene. Our approach outperforms the state-of-the-art
on challenging scenes from the ScanNet, Hypersim, and Replica datasets, as well
as on our newly created Messy Rooms dataset, demonstrating the effectiveness
and scalability of our slow-fast clustering method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.04848">Interpreting and Improving Diffusion Models Using the Euclidean Distance Function. (arXiv:2306.04848v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Permenter_F/0/1/0/all/0/1">Frank Permenter</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1">Chenyang Yuan</a></p>
<p>Denoising is intuitively related to projection. Indeed, under the manifold
hypothesis, adding random noise is approximately equivalent to orthogonal
perturbation. Hence, learning to denoise is approximately learning to project.
In this paper, we use this observation to reinterpret denoising diffusion
models as approximate gradient descent applied to the Euclidean distance
function. We then provide straight-forward convergence analysis of the DDIM
sampler under simple assumptions on the projection-error of the denoiser.
Finally, we propose a new sampler based on two simple modifications to DDIM
using insights from our theoretical results. In as few as 5-10 function
evaluations, our sampler achieves state-of-the-art FID scores on pretrained
CIFAR-10 and CelebA models and can generate high quality samples on latent
diffusion models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.06082">Augmentation-aware Self-supervised Learning with Conditioned Projector. (arXiv:2306.06082v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Przewiezlikowski_M/0/1/0/all/0/1">Marcin Przewi&#x119;&#x17a;likowski</a>, <a href="http://arxiv.org/find/cs/1/au:+Pyla_M/0/1/0/all/0/1">Mateusz Pyla</a>, <a href="http://arxiv.org/find/cs/1/au:+Zielinski_B/0/1/0/all/0/1">Bartosz Zieli&#x144;ski</a>, <a href="http://arxiv.org/find/cs/1/au:+Twardowski_B/0/1/0/all/0/1">Bart&#x142;omiej Twardowski</a>, <a href="http://arxiv.org/find/cs/1/au:+Tabor_J/0/1/0/all/0/1">Jacek Tabor</a>, <a href="http://arxiv.org/find/cs/1/au:+Smieja_M/0/1/0/all/0/1">Marek &#x15a;mieja</a></p>
<p>Self-supervised learning (SSL) is a powerful technique for learning robust
representations from unlabeled data. By learning to remain invariant to applied
data augmentations, methods such as SimCLR and MoCo are able to reach quality
on par with supervised approaches. However, this invariance may be harmful to
solving some downstream tasks which depend on traits affected by augmentations
used during pretraining, such as color. In this paper, we propose to foster
sensitivity to such characteristics in the representation space by modifying
the projector network, a common component of self-supervised architectures.
Specifically, we supplement the projector with information about augmentations
applied to images. In order for the projector to take advantage of this
auxiliary conditioning when solving the SSL task, the feature extractor learns
to preserve the augmentation information in its representations. Our approach,
coined Conditional Augmentation-aware Self-supervised Learning (CASSLE), is
directly applicable to typical joint-embedding SSL methods regardless of their
objective functions. Moreover, it does not require major changes in the network
architecture or prior knowledge of downstream tasks. In addition to an analysis
of sensitivity towards different data augmentations, we conduct a series of
experiments, which show that CASSLE improves over various SSL methods, reaching
state-of-the-art performance in multiple downstream tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.09267">Are ChatGPT and Other Similar Systems the Modern Lernaean Hydras of AI?. (arXiv:2306.09267v2 [cs.CY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ioannidis_D/0/1/0/all/0/1">Dimitrios Ioannidis</a>, <a href="http://arxiv.org/find/cs/1/au:+Kepner_J/0/1/0/all/0/1">Jeremy Kepner</a>, <a href="http://arxiv.org/find/cs/1/au:+Bowne_A/0/1/0/all/0/1">Andrew Bowne</a>, <a href="http://arxiv.org/find/cs/1/au:+Bryant_H/0/1/0/all/0/1">Harriet S. Bryant</a></p>
<p>The rise of Generative Artificial Intelligence systems (''AI systems'') has
created unprecedented social engagement. AI code generation systems provide
responses (output) to questions or requests by accessing the vast library of
open-source code created by developers over the past few decades. However, they
do so by allegedly stealing the open-source code stored in virtual libraries,
known as repositories. This Article focuses on how this happens and whether
there is a solution that protects innovation and avoids years of litigation. We
also touch upon the array of issues raised by the relationship between AI and
copyright. Looking ahead, we propose the following: (a) immediate changes to
the licenses for open-source code created by developers that will limit access
and/or use of any open-source code to humans only; (b) we suggest revisions to
the Massachusetts Institute of Technology (''MIT'') license so that AI systems
are required to procure appropriate licenses from open-source code developers,
which we believe will harmonize standards and build social consensus for the
benefit of all of humanity, rather than promote profit-driven centers of
innovation; (c) we call for urgent legislative action to protect the future of
AI systems while also promoting innovation; and (d) we propose a shift in the
burden of proof to AI systems in obfuscation cases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.09396">Private Federated Frequency Estimation: Adapting to the Hardness of the Instance. (arXiv:2306.09396v2 [cs.DS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jingfeng Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1">Wennan Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kairouz_P/0/1/0/all/0/1">Peter Kairouz</a>, <a href="http://arxiv.org/find/cs/1/au:+Braverman_V/0/1/0/all/0/1">Vladimir Braverman</a></p>
<p>In federated frequency estimation (FFE), multiple clients work together to
estimate the frequencies of their collective data by communicating with a
server that respects the privacy constraints of Secure Summation (SecSum), a
cryptographic multi-party computation protocol that ensures that the server can
only access the sum of client-held vectors. For single-round FFE, it is known
that count sketching is nearly information-theoretically optimal for achieving
the fundamental accuracy-communication trade-offs [Chen et al., 2022]. However,
we show that under the more practical multi-round FEE setting, simple
adaptations of count sketching are strictly sub-optimal, and we propose a novel
hybrid sketching algorithm that is provably more accurate. We also address the
following fundamental question: how should a practitioner set the sketch size
in a way that adapts to the hardness of the underlying problem? We propose a
two-phase approach that allows for the use of a smaller sketch size for simpler
problems (e.g., near-sparse or light-tailed distributions). We conclude our
work by showing how differential privacy can be added to our algorithm and
verifying its superior performance through extensive experiments conducted on
large-scale datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.12059">EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations. (arXiv:2306.12059v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1">Yi-Lun Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wood_B/0/1/0/all/0/1">Brandon Wood</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1">Abhishek Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Smidt_T/0/1/0/all/0/1">Tess Smidt</a></p>
<p>Equivariant Transformers such as Equiformer have demonstrated the efficacy of
applying Transformers to the domain of 3D atomistic systems. However, they are
limited to small degrees of equivariant representations due to their
computational complexity. In this paper, we investigate whether these
architectures can scale well to higher degrees. Starting from Equiformer, we
first replace $SO(3)$ convolutions with eSCN convolutions to efficiently
incorporate higher-degree tensors. Then, to better leverage the power of higher
degrees, we propose three architectural improvements -- attention
re-normalization, separable $S^2$ activation and separable layer normalization.
Putting this all together, we propose EquiformerV2, which outperforms previous
state-of-the-art methods on large-scale OC20 dataset by up to $9\%$ on forces,
$4\%$ on energies, offers better speed-accuracy trade-offs, and $2\times$
reduction in DFT calculations needed for computing adsorption energies.
Additionally, EquiformerV2 trained on only OC22 dataset outperforms GemNet-OC
trained on both OC20 and OC22 datasets, achieving much better data efficiency.
Finally, we compare EquiformerV2 with Equiformer on QM9 and OC20 S2EF-2M
datasets to better understand the performance gain brought by higher degrees.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.12509">Joint Prompt Optimization of Stacked LLMs using Variational Inference. (arXiv:2306.12509v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sordoni_A/0/1/0/all/0/1">Alessandro Sordoni</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1">Xingdi Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Cote_M/0/1/0/all/0/1">Marc-Alexandre C&#xf4;t&#xe9;</a>, <a href="http://arxiv.org/find/cs/1/au:+Pereira_M/0/1/0/all/0/1">Matheus Pereira</a>, <a href="http://arxiv.org/find/cs/1/au:+Trischler_A/0/1/0/all/0/1">Adam Trischler</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1">Ziang Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hosseini_A/0/1/0/all/0/1">Arian Hosseini</a>, <a href="http://arxiv.org/find/cs/1/au:+Niedtner_F/0/1/0/all/0/1">Friederike Niedtner</a>, <a href="http://arxiv.org/find/cs/1/au:+Roux_N/0/1/0/all/0/1">Nicolas Le Roux</a></p>
<p>Large language models (LLMs) can be seen as atomic units of computation
mapping sequences to a distribution over sequences. Thus, they can be seen as
stochastic language layers in a language network, where the learnable
parameters are the natural language prompts at each layer. By stacking two such
layers and feeding the output of one layer to the next, we obtain a Deep
Language Network (DLN). We first show how to effectively perform prompt
optimization for a 1-Layer language network (DLN-1). Then, we present an
extension that applies to 2-layer DLNs (DLN-2), where two prompts must be
learned. The key idea is to consider the output of the first layer as a latent
variable, which requires inference, and prompts to be learned as the parameters
of the generative distribution. We first test the effectiveness of DLN-1 in
multiple reasoning and natural language understanding tasks. Then, we show that
DLN-2 can reach higher performance than a single layer, showing promise that we
might reach comparable performance to GPT-4, even when each LLM in the network
is smaller and less powerful.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.13440">Trading-off price for data quality to achieve fair online allocation. (arXiv:2306.13440v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Molina_M/0/1/0/all/0/1">Mathieu Molina</a>, <a href="http://arxiv.org/find/cs/1/au:+Gast_N/0/1/0/all/0/1">Nicolas Gast</a>, <a href="http://arxiv.org/find/cs/1/au:+Loiseau_P/0/1/0/all/0/1">Patrick Loiseau</a>, <a href="http://arxiv.org/find/cs/1/au:+Perchet_V/0/1/0/all/0/1">Vianney Perchet</a></p>
<p>We consider the problem of online allocation subject to a long-term fairness
penalty. Contrary to existing works, however, we do not assume that the
decision-maker observes the protected attributes -- which is often unrealistic
in practice. Instead they can purchase data that help estimate them from
sources of different quality; and hence reduce the fairness penalty at some
cost. We model this problem as a multi-armed bandit problem where each arm
corresponds to the choice of a data source, coupled with the online allocation
problem. We propose an algorithm that jointly solves both problems and show
that it has a regret bounded by $\mathcal{O}(\sqrt{T})$. A key difficulty is
that the rewards received by selecting a source are correlated by the fairness
penalty, which leads to a need for randomization (despite a stochastic
setting). Our algorithm takes into account contextual information available
before the source selection, and can adapt to many different fairness notions.
We also show that in some instances, the estimates used can be learned on the
fly.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.15427">Adversarial Training for Graph Neural Networks: Pitfalls, Solutions, and New Directions. (arXiv:2306.15427v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gosch_L/0/1/0/all/0/1">Lukas Gosch</a>, <a href="http://arxiv.org/find/cs/1/au:+Geisler_S/0/1/0/all/0/1">Simon Geisler</a>, <a href="http://arxiv.org/find/cs/1/au:+Sturm_D/0/1/0/all/0/1">Daniel Sturm</a>, <a href="http://arxiv.org/find/cs/1/au:+Charpentier_B/0/1/0/all/0/1">Bertrand Charpentier</a>, <a href="http://arxiv.org/find/cs/1/au:+Zugner_D/0/1/0/all/0/1">Daniel Z&#xfc;gner</a>, <a href="http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1">Stephan G&#xfc;nnemann</a></p>
<p>Despite its success in the image domain, adversarial training did not (yet)
stand out as an effective defense for Graph Neural Networks (GNNs) against
graph structure perturbations. In the pursuit of fixing adversarial training
(1) we show and overcome fundamental theoretical as well as practical
limitations of the adopted graph learning setting in prior work; (2) we reveal
that more flexible GNNs based on learnable graph diffusion are able to adjust
to adversarial perturbations, while the learned message passing scheme is
naturally interpretable; (3) we introduce the first attack for structure
perturbations that, while targeting multiple nodes at once, is capable of
handling global (graph-level) as well as local (node-level) constraints.
Including these contributions, we demonstrate that adversarial training is a
state-of-the-art defense against adversarial structure perturbations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.17100">RL4CO: a Unified Reinforcement Learning for Combinatorial Optimization Library. (arXiv:2306.17100v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Berto_F/0/1/0/all/0/1">Federico Berto</a>, <a href="http://arxiv.org/find/cs/1/au:+Hua_C/0/1/0/all/0/1">Chuanbo Hua</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Junyoung Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1">Minsu Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">Hyeonah Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Son_J/0/1/0/all/0/1">Jiwoo Son</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">Haeyeon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Joungho Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Jinkyoo Park</a></p>
<p>Deep reinforcement learning offers notable benefits in addressing
combinatorial problems over traditional solvers, reducing the reliance on
domain-specific knowledge and expert solutions, and improving computational
efficiency. Despite the recent surge in interest in neural combinatorial
optimization, practitioners often do not have access to a standardized code
base. Moreover, different algorithms are frequently based on fragmentized
implementations that hinder reproducibility and fair comparison. To address
these challenges, we introduce RL4CO, a unified Reinforcement Learning (RL) for
Combinatorial Optimization (CO) library. We employ state-of-the-art software
and best practices in implementation, such as modularity and configuration
management, to be flexible, easily modifiable, and extensible by researchers.
Thanks to our unified codebase, we benchmark baseline RL solvers with different
evaluation schemes on zero-shot performance, generalization, and adaptability
on diverse tasks. Notably, we find that some recent methods may fall behind
their predecessors depending on the evaluation settings. We hope RL4CO will
encourage the exploration of novel solutions to complex real-world tasks,
allowing the community to compare with existing methods through a unified
framework that decouples the science from software engineering. We open-source
our library at https://github.com/ai4co/rl4co.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01708">Distributional Model Equivalence for Risk-Sensitive Reinforcement Learning. (arXiv:2307.01708v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kastner_T/0/1/0/all/0/1">Tyler Kastner</a>, <a href="http://arxiv.org/find/cs/1/au:+Erdogdu_M/0/1/0/all/0/1">Murat A. Erdogdu</a>, <a href="http://arxiv.org/find/cs/1/au:+Farahmand_A/0/1/0/all/0/1">Amir-massoud Farahmand</a></p>
<p>We consider the problem of learning models for risk-sensitive reinforcement
learning. We theoretically demonstrate that proper value equivalence, a method
of learning models which can be used to plan optimally in the risk-neutral
setting, is not sufficient to plan optimally in the risk-sensitive setting. We
leverage distributional reinforcement learning to introduce two new notions of
model equivalence, one which is general and can be used to plan for any risk
measure, but is intractable; and a practical variation which allows one to
choose which risk measures they may plan optimally for. We demonstrate how our
framework can be used to augment any model-free risk-sensitive algorithm, and
provide both tabular and large-scale experiments to demonstrate its ability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.02129">How Deep Neural Networks Learn Compositional Data: The Random Hierarchy Model. (arXiv:2307.02129v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cagnetta_F/0/1/0/all/0/1">Francesco Cagnetta</a>, <a href="http://arxiv.org/find/cs/1/au:+Petrini_L/0/1/0/all/0/1">Leonardo Petrini</a>, <a href="http://arxiv.org/find/cs/1/au:+Tomasini_U/0/1/0/all/0/1">Umberto M. Tomasini</a>, <a href="http://arxiv.org/find/cs/1/au:+Favero_A/0/1/0/all/0/1">Alessandro Favero</a>, <a href="http://arxiv.org/find/cs/1/au:+Wyart_M/0/1/0/all/0/1">Matthieu Wyart</a></p>
<p>Deep learning algorithms demonstrate a surprising ability to learn
high-dimensional tasks from limited examples. This is commonly attributed to
the depth of neural networks, enabling them to build a hierarchy of abstract,
low-dimensional data representations. However, how many training examples are
required to learn such representations remains unknown. To quantitatively study
this question, we introduce the Random Hierarchy Model: a family of synthetic
tasks inspired by the hierarchical structure of language and images. The model
is a classification task where each class corresponds to a group of high-level
features, chosen among several equivalent groups associated with the same
class. In turn, each feature corresponds to a group of sub-features chosen
among several equivalent ones and so on, following a hierarchy of composition
rules. We find that deep networks learn the task by developing internal
representations invariant to exchanging equivalent groups. Moreover, the number
of data required corresponds to the point where correlations between low-level
features and classes become detectable. Overall, our results indicate how deep
networks overcome the curse of dimensionality by building invariant
representations, and provide an estimate of the number of data required to
learn a hierarchical task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.02779">Large Language Models Empowered Autonomous Edge AI for Connected Intelligence. (arXiv:2307.02779v2 [cs.IT] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yifei Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1">Jiawei Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xinjie Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zehong Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1">Hao Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dongsheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Letaief_K/0/1/0/all/0/1">Khaled B. Letaief</a></p>
<p>The evolution of wireless networks gravitates towards connected intelligence,
a concept that envisions seamless interconnectivity among humans, objects, and
intelligence in a hyper-connected cyber-physical world. Edge artificial
intelligence (Edge AI) is a promising solution to achieve connected
intelligence by delivering high-quality, low-latency, and privacy-preserving AI
services at the network edge. This article presents a vision of autonomous edge
AI systems that automatically organize, adapt, and optimize themselves to meet
users' diverse requirements, leveraging the power of large language models
(LLMs), i.e., Generative Pretrained Transformer (GPT). By exploiting the
powerful abilities of GPT in language understanding, planning, and code
generation, as well as incorporating classic wisdom such as task-oriented
communication and edge federated learning, we present a versatile framework
that efficiently coordinates edge AI models to cater to users' personal demands
while automatically generating code to train new models in a privacy-preserving
manner. Experimental results demonstrate the system's remarkable ability to
accurately comprehend user demands, efficiently execute AI models with minimal
cost, and effectively create high-performance AI models at edge servers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.02842">Provably Efficient Iterated CVaR Reinforcement Learning with Function Approximation and Human Feedback. (arXiv:2307.02842v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1">Yihan Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1">Pihe Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Siwei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1">Desheng Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1">Longbo Huang</a></p>
<p>Risk-sensitive reinforcement learning (RL) aims to optimize policies that
balance the expected reward and risk. In this paper, we present a novel
risk-sensitive RL framework that employs an Iterated Conditional Value-at-Risk
(CVaR) objective under both linear and general function approximations,
enriched by human feedback. These new formulations provide a principled way to
guarantee safety in each decision making step throughout the control process.
Moreover, integrating human feedback into risk-sensitive RL framework bridges
the gap between algorithmic decision-making and human participation, allowing
us to also guarantee safety for human-in-the-loop systems. We propose provably
sample-efficient algorithms for this Iterated CVaR RL and provide rigorous
theoretical analysis. Furthermore, we establish a matching lower bound to
corroborate the optimality of our algorithms in a linear context.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03288">Optimal Scalarizations for Sublinear Hypervolume Regret. (arXiv:2307.03288v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qiuyi Zhang</a> (Richard)</p>
<p>Scalarization is a general technique that can be deployed in any
multiobjective setting to reduce multiple objectives into one, such as recently
in RLHF for training reward models that align human preferences. Yet some have
dismissed this classical approach because linear scalarizations are known to
miss concave regions of the Pareto frontier. To that end, we aim to find simple
non-linear scalarizations that can explore a diverse set of $k$ objectives on
the Pareto frontier, as measured by the dominated hypervolume. We show that
hypervolume scalarizations with uniformly random weights are surprisingly
optimal for provably minimizing the hypervolume regret, achieving an optimal
sublinear regret bound of $O(T^{-1/k})$, with matching lower bounds that
preclude any algorithm from doing better asymptotically. As a theoretical case
study, we consider the multiobjective stochastic linear bandits problem and
demonstrate that by exploiting the sublinear regret bounds of the hypervolume
scalarizations, we can derive a novel non-Euclidean analysis that produces
improved hypervolume regret bounds of $\tilde{O}( d T^{-1/2} + T^{-1/k})$. We
support our theory with strong empirical performance of using simple
hypervolume scalarizations that consistently outperforms both the linear and
Chebyshev scalarizations, as well as standard multiobjective algorithms in
bayesian optimization, such as EHVI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07566">Reconstruction of 3-Axis Seismocardiogram from Right-to-left and Head-to-foot Components Using A Long Short-Term Memory Network. (arXiv:2307.07566v2 [physics.med-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Rahman_M/0/1/0/all/0/1">Mohammad Muntasir Rahman</a>, <a href="http://arxiv.org/find/physics/1/au:+Taebi_A/0/1/0/all/0/1">Amirtah&#xe0; Taebi</a></p>
<p>This pilot study aims to develop a deep learning model for predicting
seismocardiogram (SCG) signals in the dorsoventral direction from the SCG
signals in the right-to-left and head-to-foot directions ($\textrm{SCG}_x$ and
$\textrm{SCG}_y$). The dataset used for the training and validation of the
model was obtained from 15 healthy adult subjects. The SCG signals were
recorded using tri-axial accelerometers placed on the chest of each subject.
The signals were then segmented using electrocardiogram R waves, and the
segments were downsampled, normalized, and centered around zero. The resulting
dataset was used to train and validate a long short-term memory (LSTM) network
with two layers and a dropout layer to prevent overfitting. The network took as
input 100-time steps of $\textrm{SCG}_x$ and $\textrm{SCG}_y$, representing one
cardiac cycle, and outputted a vector that mapped to the target variable being
predicted. The results showed that the LSTM model had a mean square error of
0.09 between the predicted and actual SCG segments in the dorsoventral
direction. The study demonstrates the potential of deep learning models for
reconstructing 3-axis SCG signals using the data obtained from dual-axis
accelerometers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07816">Minimal Random Code Learning with Mean-KL Parameterization. (arXiv:2307.07816v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Jihao Andreas Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Flamich_G/0/1/0/all/0/1">Gergely Flamich</a>, <a href="http://arxiv.org/find/cs/1/au:+Hernandez_Lobato_J/0/1/0/all/0/1">Jos&#xe9; Miguel Hern&#xe1;ndez-Lobato</a></p>
<p>This paper studies the qualitative behavior and robustness of two variants of
Minimal Random Code Learning (MIRACLE) used to compress variational Bayesian
neural networks. MIRACLE implements a powerful, conditionally Gaussian
variational approximation for the weight posterior $Q_{\mathbf{w}}$ and uses
relative entropy coding to compress a weight sample from the posterior using a
Gaussian coding distribution $P_{\mathbf{w}}$. To achieve the desired
compression rate, $D_{\mathrm{KL}}[Q_{\mathbf{w}} \Vert P_{\mathbf{w}}]$ must
be constrained, which requires a computationally expensive annealing procedure
under the conventional mean-variance (Mean-Var) parameterization for
$Q_{\mathbf{w}}$. Instead, we parameterize $Q_{\mathbf{w}}$ by its mean and KL
divergence from $P_{\mathbf{w}}$ to constrain the compression cost to the
desired value by construction. We demonstrate that variational training with
Mean-KL parameterization converges twice as fast and maintains predictive
performance after compression. Furthermore, we show that Mean-KL leads to more
meaningful variational distributions with heavier tails and compressed weight
samples which are more robust to pruning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08596">Omnipotent Adversarial Training in the Wild. (arXiv:2307.08596v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1">Guanlin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Kangjie Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yuan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1">Han Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tianwei Zhang</a></p>
<p>Adversarial training is an important topic in robust deep learning, but the
community lacks attention to its practical usage. In this paper, we aim to
resolve a real-world challenge, i.e., training a model on an imbalanced and
noisy dataset to achieve high clean accuracy and adversarial robustness, with
our proposed Omnipotent Adversarial Training (OAT) strategy. OAT consists of
two innovative methodologies to address the imperfection in the training set.
We first introduce an oracle into the adversarial training process to help the
model learn a correct data-label conditional distribution. This
carefully-designed oracle can provide correct label annotations for adversarial
training. We further propose logits adjustment adversarial training to overcome
the data imbalance issue, which can help the model learn a Bayes-optimal
distribution. Our comprehensive evaluation results show that OAT outperforms
other baselines by more than 20% clean accuracy improvement and 10% robust
accuracy improvement under complex combinations of data imbalance and label
noise scenarios. The code can be found in https://github.com/GuanlinLee/OAT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10205">Alleviating the Effect of Data Imbalance on Adversarial Training. (arXiv:2307.10205v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1">Guanlin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1">Guowen Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tianwei Zhang</a></p>
<p>In this paper, we study adversarial training on datasets that obey the
long-tailed distribution, which is practical but rarely explored in previous
works. Compared with conventional adversarial training on balanced datasets,
this process falls into the dilemma of generating uneven adversarial examples
(AEs) and an unbalanced feature embedding space, causing the resulting model to
exhibit low robustness and accuracy on tail data. To combat that, we
theoretically analyze the lower bound of the robust risk to train a model on a
long-tailed dataset to obtain the key challenges in addressing the
aforementioned dilemmas. Based on it, we propose a new adversarial training
framework -- Re-balancing Adversarial Training (REAT). This framework consists
of two components: (1) a new training strategy inspired by the effective number
to guide the model to generate more balanced and informative AEs; (2) a
carefully constructed penalty function to force a satisfactory feature space.
Evaluation results on different datasets and model structures prove that REAT
can effectively enhance the model's robustness and preserve the model's clean
accuracy. The code can be found in https://github.com/GuanlinLee/REAT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10936">PASTA: Pretrained Action-State Transformer Agents. (arXiv:2307.10936v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Boige_R/0/1/0/all/0/1">Raphael Boige</a>, <a href="http://arxiv.org/find/cs/1/au:+Flet_Berliac_Y/0/1/0/all/0/1">Yannis Flet-Berliac</a>, <a href="http://arxiv.org/find/cs/1/au:+Flajolet_A/0/1/0/all/0/1">Arthur Flajolet</a>, <a href="http://arxiv.org/find/cs/1/au:+Richard_G/0/1/0/all/0/1">Guillaume Richard</a>, <a href="http://arxiv.org/find/cs/1/au:+Pierrot_T/0/1/0/all/0/1">Thomas Pierrot</a></p>
<p>Self-supervised learning has brought about a revolutionary paradigm shift in
various computing domains, including NLP, vision, and biology. Recent
approaches involve pre-training transformer models on vast amounts of unlabeled
data, serving as a starting point for efficiently solving downstream tasks. In
reinforcement learning, researchers have recently adapted these approaches,
developing models pre-trained on expert trajectories. This advancement enables
the models to tackle a broad spectrum of tasks, ranging from robotics to
recommendation systems. However, existing methods mostly rely on intricate
pre-training objectives tailored to specific downstream applications. This
paper conducts a comprehensive investigation of models, referred to as
pre-trained action-state transformer agents (PASTA). Our study covers a unified
methodology and covers an extensive set of general downstream tasks including
behavioral cloning, offline RL, sensor failure robustness, and dynamics change
adaptation. Our objective is to systematically compare various design choices
and offer valuable insights that will aid practitioners in developing robust
models. Key highlights of our study include tokenization at the component level
for actions and states, the use of fundamental pre-training objectives such as
next token prediction or masked language modeling, simultaneous training of
models across multiple domains, and the application of various fine-tuning
strategies. In this study, the developed models contain fewer than 7 million
parameters allowing a broad community to use these models and reproduce our
experiments. We hope that this study will encourage further research into the
use of transformers with first principle design choices to represent RL
trajectories and contribute to robust policy learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.13771">Accuracy Improvement in Differentially Private Logistic Regression: A Pre-training Approach. (arXiv:2307.13771v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hoseinpour_M/0/1/0/all/0/1">Mohammad Hoseinpour</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoseinpour_M/0/1/0/all/0/1">Milad Hoseinpour</a>, <a href="http://arxiv.org/find/cs/1/au:+Aghagolzadeh_A/0/1/0/all/0/1">Ali Aghagolzadeh</a></p>
<p>Machine learning (ML) models can memorize training datasets. As a result,
training ML models over private datasets can lead to the violation of
individuals' privacy. Differential privacy (DP) is a rigorous privacy notion to
preserve the privacy of underlying training datasets. Yet, training ML models
in a DP framework usually degrades the accuracy of ML models. This paper aims
to boost the accuracy of a DP logistic regression (LR) via a pre-training
module. In more detail, we initially pre-train our LR model on a public
training dataset that there is no privacy concern about it. Then, we fine-tune
our DP-LR model with the private dataset. In the numerical results, we show
that adding a pre-training module significantly improves the accuracy of the
DP-LR model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01729">Telematics Combined Actuarial Neural Networks for Cross-Sectional and Longitudinal Claim Count Data. (arXiv:2308.01729v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Duval_F/0/1/0/all/0/1">Francis Duval</a>, <a href="http://arxiv.org/find/stat/1/au:+Boucher_J/0/1/0/all/0/1">Jean-Philippe Boucher</a>, <a href="http://arxiv.org/find/stat/1/au:+Pigeon_M/0/1/0/all/0/1">Mathieu Pigeon</a></p>
<p>We present novel cross-sectional and longitudinal claim count models for
vehicle insurance built upon the Combined Actuarial Neural Network (CANN)
framework proposed by Mario W\"uthrich and Michael Merz. The CANN approach
combines a classical actuarial model, such as a generalized linear model, with
a neural network. This blending of models results in a two-component model
comprising a classical regression model and a neural network part. The CANN
model leverages the strengths of both components, providing a solid foundation
and interpretability from the classical model while harnessing the flexibility
and capacity to capture intricate relationships and interactions offered by the
neural network. In our proposed models, we use well-known log-linear claim
count regression models for the classical regression part and a multilayer
perceptron (MLP) for the neural network part. The MLP part is used to process
telematics car driving data given as a vector characterizing the driving
behavior of each insured driver. In addition to the Poisson and negative
binomial distributions for cross-sectional data, we propose a procedure for
training our CANN model with a multivariate negative binomial (MVNB)
specification. By doing so, we introduce a longitudinal model that accounts for
the dependence between contracts from the same insured. Our results reveal that
the CANN models exhibit superior performance compared to log-linear models that
rely on manually engineered telematics features.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.02715">Fluid Viscosity Prediction Leveraging Computer Vision and Robot Interaction. (arXiv:2308.02715v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Jong Hoon Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Dalwankar_G/0/1/0/all/0/1">Gauri Pramod Dalwankar</a>, <a href="http://arxiv.org/find/cs/1/au:+Bartsch_A/0/1/0/all/0/1">Alison Bartsch</a>, <a href="http://arxiv.org/find/cs/1/au:+George_A/0/1/0/all/0/1">Abraham George</a>, <a href="http://arxiv.org/find/cs/1/au:+Farimani_A/0/1/0/all/0/1">Amir Barati Farimani</a></p>
<p>Accurately determining fluid viscosity is crucial for various industrial and
scientific applications. Traditional methods of viscosity measurement, though
reliable, often require manual intervention and cannot easily adapt to
real-time monitoring. With advancements in machine learning and computer
vision, this work explores the feasibility of predicting fluid viscosity by
analyzing fluid oscillations captured in video data. The pipeline employs a 3D
convolutional autoencoder pretrained in a self-supervised manner to extract and
learn features from semantic segmentation masks of oscillating fluids. Then,
the latent representations of the input data, produced from the pretrained
autoencoder, is processed with a distinct inference head to infer either the
fluid category (classification) or the fluid viscosity (regression) in a
time-resolved manner. When the latent representations generated by the
pretrained autoencoder are used for classification, the system achieves a 97.1%
accuracy across a total of 4,140 test datapoints. Similarly, for regression
tasks, employing an additional fully-connected network as a regression head
allows the pipeline to achieve a mean absolute error of 0.258 over 4,416 test
datapoints. This study represents an innovative contribution to both fluid
characterization and the evolving landscape of Artificial Intelligence,
demonstrating the potential of deep learning in achieving near real-time
viscosity estimation and addressing practical challenges in fluid dynamics
through the analysis of video data capturing oscillating fluid dynamics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.08693">AI planning in the imagination: High-level planning on learned abstract search spaces. (arXiv:2308.08693v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Martin_C/0/1/0/all/0/1">Carlos Martin</a>, <a href="http://arxiv.org/find/cs/1/au:+Sandholm_T/0/1/0/all/0/1">Tuomas Sandholm</a></p>
<p>Search and planning algorithms have been a cornerstone of artificial
intelligence since the field's inception. Giving reinforcement learning agents
the ability to plan during execution time has resulted in significant
performance improvements in various domains. However, in real-world
environments, the model with respect to which the agent plans has been
constrained to be grounded in the real environment itself, as opposed to a more
abstract model which allows for planning over compound actions and behaviors.
We propose a new method, called PiZero, that gives an agent the ability to plan
in an abstract search space that the agent learns during training, which is
completely decoupled from the real environment. Unlike prior approaches, this
enables the agent to perform high-level planning at arbitrary timescales and
reason in terms of compound or temporally-extended actions, which can be useful
in environments where large numbers of base-level micro-actions are needed to
perform relevant macro-actions. In addition, our method is more general than
comparable prior methods because it seamlessly handles settings with continuous
action spaces, combinatorial action spaces, and partial observability. We
evaluate our method on multiple domains, including the traveling salesman
problem, Sokoban, 2048, the facility location problem, and Pacman.
Experimentally, it outperforms comparable prior methods without assuming access
to an environment simulator at execution time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.09296">CARLA: Self-supervised Contrastive Representation Learning for Time Series Anomaly Detection. (arXiv:2308.09296v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Darban_Z/0/1/0/all/0/1">Zahra Zamanzadeh Darban</a>, <a href="http://arxiv.org/find/cs/1/au:+Webb_G/0/1/0/all/0/1">Geoffrey I. Webb</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1">Shirui Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Aggarwal_C/0/1/0/all/0/1">Charu C. Aggarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Salehi_M/0/1/0/all/0/1">Mahsa Salehi</a></p>
<p>One main challenge in time series anomaly detection (TAD) is the lack of
labelled data in many real-life scenarios. Most of the existing anomaly
detection methods focus on learning the normal behaviour of unlabelled time
series in an unsupervised manner. The normal boundary is often defined tightly,
resulting in slight deviations being classified as anomalies, consequently
leading to a high false positive rate and a limited ability to generalise
normal patterns. To address this, we introduce a novel end-to-end
self-supervised ContrAstive Representation Learning approach for time series
Anomaly detection (CARLA). While existing contrastive learning methods assume
that augmented time series windows are positive samples and temporally distant
windows are negative samples, we argue that these assumptions are limited as
augmentation of time series can transform them to negative samples, and a
temporally distant window can represent a positive sample. Our contrastive
approach leverages existing generic knowledge about time series anomalies and
injects various types of anomalies as negative samples. Therefore, CARLA not
only learns normal behaviour but also learns deviations indicating anomalies.
It creates similar representations for temporally closed windows and distinct
ones for anomalies. Additionally, it leverages the information about
representations' neighbours through a self-supervised approach to classify
windows based on their nearest/furthest neighbours to further enhance the
performance of anomaly detection. In extensive tests on seven major real-world
time series anomaly detection datasets, CARLA shows superior performance over
state-of-the-art self-supervised and unsupervised TAD methods. Our research
shows the potential of contrastive representation learning to advance time
series anomaly detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.14831">Continual Learning with Dynamic Sparse Training: Exploring Algorithms for Effective Model Updates. (arXiv:2308.14831v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yildirim_M/0/1/0/all/0/1">Murat Onur Yildirim</a>, <a href="http://arxiv.org/find/cs/1/au:+Yildirim_E/0/1/0/all/0/1">Elif Ceren Gok Yildirim</a>, <a href="http://arxiv.org/find/cs/1/au:+Sokar_G/0/1/0/all/0/1">Ghada Sokar</a>, <a href="http://arxiv.org/find/cs/1/au:+Mocanu_D/0/1/0/all/0/1">Decebal Constantin Mocanu</a>, <a href="http://arxiv.org/find/cs/1/au:+Vanschoren_J/0/1/0/all/0/1">Joaquin Vanschoren</a></p>
<p>Continual learning (CL) refers to the ability of an intelligent system to
sequentially acquire and retain knowledge from a stream of data with as little
computational overhead as possible. To this end; regularization, replay,
architecture, and parameter isolation approaches were introduced to the
literature. Parameter isolation using a sparse network which enables to
allocate distinct parts of the neural network to different tasks and also
allows to share of parameters between tasks if they are similar. Dynamic Sparse
Training (DST) is a prominent way to find these sparse networks and isolate
them for each task. This paper is the first empirical study investigating the
effect of different DST components under the CL paradigm to fill a critical
research gap and shed light on the optimal configuration of DST for CL if it
exists. Therefore, we perform a comprehensive study in which we investigate
various DST components to find the best topology per task on well-known
CIFAR100 and miniImageNet benchmarks in a task-incremental CL setup since our
primary focus is to evaluate the performance of various DST criteria, rather
than the process of mask selection. We found that, at a low sparsity level,
Erdos-R\'enyi Kernel (ERK) initialization utilizes the backbone more
efficiently and allows to effectively learn increments of tasks. At a high
sparsity level, unless it is extreme, uniform initialization demonstrates a
more reliable and robust performance. In terms of growth strategy; performance
is dependent on the defined initialization strategy and the extent of sparsity.
Finally, adaptivity within DST components is a promising way for better
continual learners.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.15452">When Do Program-of-Thoughts Work for Reasoning?. (arXiv:2308.15452v5 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1">Zhen Bi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Ningyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yinuo Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1">Shumin Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1">Guozhou Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huajun Chen</a></p>
<p>In the realm of embodied artificial intelligence, the reasoning capabilities
of Large Language Models (LLMs) play a pivotal role. Although there are
effective methods like program-of-thought prompting for LLMs which uses
programming language to tackle complex reasoning tasks, the specific impact of
code data on the improvement of reasoning capabilities remains under-explored.
To address this gap, we propose complexity-impacted reasoning score (CIRS),
which combines structural and logical attributes, to measure the correlation
between code and reasoning abilities. Specifically, we use the abstract syntax
tree to encode the structural information and calculate logical complexity by
considering the difficulty and the cyclomatic complexity. Through an empirical
analysis, we find not all code data of complexity can be learned or understood
by LLMs. Optimal level of complexity is critical to the improvement of
reasoning abilities by program-aided prompting. Then we design an
auto-synthesizing and stratifying algorithm, and apply it to instruction
generation for mathematical reasoning and code data filtering for code
generation tasks. Extensive results demonstrates the effectiveness of our
proposed approach. Code will be integrated into the EasyInstruct framework at
https://github.com/zjunlp/EasyInstruct.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.15984">Learning Structure-from-Motion with Graph Attention Networks. (arXiv:2308.15984v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Brynte_L/0/1/0/all/0/1">Lucas Brynte</a>, <a href="http://arxiv.org/find/cs/1/au:+Iglesias_J/0/1/0/all/0/1">Jos&#xe9; Pedro Iglesias</a>, <a href="http://arxiv.org/find/cs/1/au:+Olsson_C/0/1/0/all/0/1">Carl Olsson</a>, <a href="http://arxiv.org/find/cs/1/au:+Kahl_F/0/1/0/all/0/1">Fredrik Kahl</a></p>
<p>In this paper we tackle the problem of learning Structure-from-Motion (SfM)
through the use of graph attention networks. SfM is a classic computer vision
problem that is solved though iterative minimization of reprojection errors,
referred to as Bundle Adjustment (BA), starting from a good initialization. In
order to obtain a good enough initialization to BA, conventional methods rely
on a sequence of sub-problems (such as pairwise pose estimation, pose averaging
or triangulation) which provides an initial solution that can then be refined
using BA. In this work we replace these sub-problems by learning a model that
takes as input the 2D keypoints detected across multiple views, and outputs the
corresponding camera poses and 3D keypoint coordinates. Our model takes
advantage of graph neural networks to learn SfM-specific primitives, and we
show that it can be used for fast inference of the reconstruction for new and
unseen sequences. The experimental results show that the proposed model
outperforms competing learning-based methods, and challenges COLMAP while
having lower runtime.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16458">BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge. (arXiv:2308.16458v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1">Xiangru Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_B/0/1/0/all/0/1">Bill Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1">Rick Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiakang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xinyun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gerstein_M/0/1/0/all/0/1">Mark Gerstein</a></p>
<p>Pre-trained large language models have significantly improved code
generation. As these models scale up, there is an increasing need for the
output to handle more intricate tasks and to be appropriately specialized to
particular domains. Here, we target bioinformatics due to the amount of
specialized domain knowledge, algorithms, and data operations this discipline
requires. We present BioCoder, a benchmark developed to evaluate large language
models (LLMs) in generating bioinformatics-specific code. BioCoder spans a
broad spectrum of the field and covers cross-file dependencies, class
declarations, and global variables. It incorporates 1026 Python functions and
1243 Java methods extracted from GitHub, along with 253 examples from the
Rosalind Project, all pertaining to bioinformatics. Using topic modeling we
show that overall coverage of the included code is representative of the full
spectrum of bioinformatics calculations. BioCoder incorporates a fuzz-testing
framework for evaluation. We have applied it to evaluate many models including
InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT5+,
GPT-3.5, and GPT-4. Furthermore, we finetuned StarCoder, demonstrating how our
dataset can effectively enhance the performance of LLMs on our benchmark (by
&gt;15% in terms of Pass@K in certain prompt configurations and always &gt;3%). The
results highlight two key aspects of successful models: (1) Successful models
accommodate a long prompt (&gt; ~2600 tokens) with full context, for functional
dependencies. (2) They contain specific domain knowledge of bioinformatics,
beyond just general coding knowledge. This is evident from the performance gain
of GPT-3.5/4 compared to the smaller models on the benchmark (50% vs up to
~25%). Our dataset, benchmark, Docker images, and scripts required for testing
are all available at https://github.com/gersteinlab/biocoder.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.02539">A Generalized Bandsplit Neural Network for Cinematic Audio Source Separation. (arXiv:2309.02539v3 [eess.AS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Watcharasupat_K/0/1/0/all/0/1">Karn N. Watcharasupat</a>, <a href="http://arxiv.org/find/eess/1/au:+Wu_C/0/1/0/all/0/1">Chih-Wei Wu</a>, <a href="http://arxiv.org/find/eess/1/au:+Ding_Y/0/1/0/all/0/1">Yiwei Ding</a>, <a href="http://arxiv.org/find/eess/1/au:+Orife_I/0/1/0/all/0/1">Iroro Orife</a>, <a href="http://arxiv.org/find/eess/1/au:+Hipple_A/0/1/0/all/0/1">Aaron J. Hipple</a>, <a href="http://arxiv.org/find/eess/1/au:+Williams_P/0/1/0/all/0/1">Phillip A. Williams</a>, <a href="http://arxiv.org/find/eess/1/au:+Kramer_S/0/1/0/all/0/1">Scott Kramer</a>, <a href="http://arxiv.org/find/eess/1/au:+Lerch_A/0/1/0/all/0/1">Alexander Lerch</a>, <a href="http://arxiv.org/find/eess/1/au:+Wolcott_W/0/1/0/all/0/1">William Wolcott</a></p>
<p>Cinematic audio source separation is a relatively new subtask of audio source
separation, with the aim of extracting the dialogue, music, and effects stems
from their mixture. In this work, we developed a model generalizing the
Bandsplit RNN for any complete or overcomplete partitions of the frequency
axis. Psychoacoustically motivated frequency scales were used to inform the
band definitions which are now defined with redundancy for more reliable
feature extraction. A loss function motivated by the signal-to-noise ratio and
the sparsity-promoting property of the 1-norm was proposed. We additionally
exploit the information-sharing property of a common-encoder setup to reduce
computational complexity during both training and inference, improve separation
performance for hard-to-generalize classes of sounds, and allow flexibility
during inference time with detachable decoders. Our best model sets the state
of the art on the Divide and Remaster dataset with performance above the ideal
ratio mask for the dialogue stem.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.03886">FIND: A Function Description Benchmark for Evaluating Interpretability Methods. (arXiv:2309.03886v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schwettmann_S/0/1/0/all/0/1">Sarah Schwettmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Shaham_T/0/1/0/all/0/1">Tamar Rott Shaham</a>, <a href="http://arxiv.org/find/cs/1/au:+Materzynska_J/0/1/0/all/0/1">Joanna Materzynska</a>, <a href="http://arxiv.org/find/cs/1/au:+Chowdhury_N/0/1/0/all/0/1">Neil Chowdhury</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shuang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1">Jacob Andreas</a>, <a href="http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1">David Bau</a>, <a href="http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1">Antonio Torralba</a></p>
<p>Labeling neural network submodules with human-legible descriptions is useful
for many downstream tasks: such descriptions can surface failures, guide
interventions, and perhaps even explain important model behaviors. To date,
most mechanistic descriptions of trained networks have involved small models,
narrowly delimited phenomena, and large amounts of human labor. Labeling all
human-interpretable sub-computations in models of increasing size and
complexity will almost certainly require tools that can generate and validate
descriptions automatically. Recently, techniques that use learned models
in-the-loop for labeling have begun to gain traction, but methods for
evaluating their efficacy are limited and ad-hoc. How should we validate and
compare open-ended labeling tools? This paper introduces FIND (Function
INterpretation and Description), a benchmark suite for evaluating the building
blocks of automated interpretability methods. FIND contains functions that
resemble components of trained neural networks, and accompanying descriptions
of the kind we seek to generate. The functions span textual and numeric
domains, and involve a range of real-world complexities. We evaluate methods
that use pretrained language models (LMs) to produce descriptions of function
behavior in natural language and code. Additionally, we introduce a new
interactive method in which an Automated Interpretability Agent (AIA) generates
function descriptions. We find that an AIA, built from an LM with black-box
access to functions, can infer function structure, acting as a scientist by
forming hypotheses, proposing experiments, and updating descriptions in light
of new data. However, AIA descriptions tend to capture global function behavior
and miss local details. These results suggest that FIND will be useful for
evaluating more sophisticated interpretability methods before they are applied
to real-world models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.05751">The Effect of Intrinsic Dimension on Metric Learning under Compression. (arXiv:2309.05751v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Palias_E/0/1/0/all/0/1">Efstratios Palias</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaban_A/0/1/0/all/0/1">Ata Kab&#xe1;n</a></p>
<p>Metric learning aims at finding a suitable distance metric over the input
space, to improve the performance of distance-based learning algorithms. In
high-dimensional settings, metric learning can also play the role of
dimensionality reduction, by imposing a low-rank restriction to the learnt
metric. In this paper, instead of training a low-rank metric on
high-dimensional data, we consider a randomly compressed version of the data,
and train a full-rank metric there. We give theoretical guarantees on the error
of distance-based metric learning, with respect to the random compression,
which do not depend on the ambient dimension. Our bounds do not make any
explicit assumptions, aside from i.i.d. data from a bounded support, and
automatically tighten when benign geometrical structures are present.
Experimental results on both synthetic and real data sets support our
theoretical findings in high-dimensional settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.05853">ChemSpaceAL: An Efficient Active Learning Methodology Applied to Protein-Specific Molecular Generation. (arXiv:2309.05853v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kyro_G/0/1/0/all/0/1">Gregory W. Kyro</a>, <a href="http://arxiv.org/find/cs/1/au:+Morgunov_A/0/1/0/all/0/1">Anton Morgunov</a>, <a href="http://arxiv.org/find/cs/1/au:+Brent_R/0/1/0/all/0/1">Rafael I. Brent</a>, <a href="http://arxiv.org/find/cs/1/au:+Batista_V/0/1/0/all/0/1">Victor S. Batista</a></p>
<p>The incredible capabilities of generative artificial intelligence models have
inevitably led to their application in the domain of drug discovery. Within
this domain, the vastness of chemical space motivates the development of more
efficient methods for identifying regions with molecules that exhibit desired
characteristics. In this work, we present a computationally efficient active
learning methodology that requires evaluation of only a subset of the generated
data in the constructed sample space to successfully align a generative model
with respect to a specified objective. We demonstrate the applicability of this
methodology to targeted molecular generation by fine-tuning a GPT-based
molecular generator toward a protein with FDA-approved small-molecule
inhibitors, c-Abl kinase. Remarkably, the model learns to generate molecules
similar to the inhibitors without prior knowledge of their existence, and even
reproduces two of them exactly. We also show that the methodology is effective
for a protein without any commercially available small-molecule inhibitors, the
HNH domain of the CRISPR-associated protein 9 (Cas9) enzyme. We believe that
the inherent generality of this method ensures that it will remain applicable
as the exciting field of in silico molecular generation evolves. To facilitate
implementation and reproducibility, we have made all of our software available
through the open-source ChemSpaceAL Python package.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.06838">Supervised Machine Learning and Physics based Machine Learning approach for prediction of peak temperature distribution in Additive Friction Stir Deposition of Aluminium Alloy. (arXiv:2309.06838v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1">Akshansh Mishra</a></p>
<p>Additive friction stir deposition (AFSD) is a novel solid-state additive
manufacturing technique that circumvents issues of porosity, cracking, and
properties anisotropy that plague traditional powder bed fusion and directed
energy deposition approaches. However, correlations between process parameters,
thermal profiles, and resulting microstructure in AFSD remain poorly
understood. This hinders process optimization for properties. This work employs
a framework combining supervised machine learning (SML) and physics-informed
neural networks (PINNs) to predict peak temperature distribution in AFSD from
process parameters. Eight regression algorithms were implemented for SML
modeling, while four PINNs leveraged governing equations for transport, wave
propagation, heat transfer, and quantum mechanics. Across multiple statistical
measures, ensemble techniques like gradient boosting proved superior for SML,
with lowest MSE of 165.78. The integrated ML approach was also applied to
classify deposition quality from process factors, with logistic regression
delivering robust accuracy. By fusing data-driven learning and fundamental
physics, this dual methodology provides comprehensive insights into tailoring
microstructure through thermal management in AFSD. The work demonstrates the
power of bridging statistical and physics-based modeling for elucidating AM
process-property relationships.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.08420">FedDCSR: Federated Cross-domain Sequential Recommendation via Disentangled Representation Learning. (arXiv:2309.08420v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1">Dongyi Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1">Jiyuan Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1">Qing Liao</a></p>
<p>Cross-domain Sequential Recommendation (CSR) which leverages user sequence
data from multiple domains has received extensive attention in recent years.
However, the existing CSR methods require sharing origin user data across
domains, which violates the General Data Protection Regulation (GDPR). Thus, it
is necessary to combine federated learning (FL) and CSR to fully utilize
knowledge from different domains while preserving data privacy. Nonetheless,
the sequence feature heterogeneity across different domains significantly
impacts the overall performance of FL. In this paper, we propose FedDCSR, a
novel federated cross-domain sequential recommendation framework via
disentangled representation learning. Specifically, to address the sequence
feature heterogeneity across domains, we introduce an approach called
inter-intra domain sequence representation disentanglement (SRD) to disentangle
the user sequence features into domain-shared and domain-exclusive features. In
addition, we design an intra domain contrastive infomax (CIM) strategy to learn
richer domain-exclusive features of users by performing data augmentation on
user sequences. Extensive experiments on three real-world scenarios demonstrate
that FedDCSR achieves significant improvements over existing baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12742">Make the U in UDA Matter: Invariant Consistency Learning for Unsupervised Domain Adaptation. (arXiv:2309.12742v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yue_Z/0/1/0/all/0/1">Zhongqi Yue</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hanwang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1">Qianru Sun</a></p>
<p>Domain Adaptation (DA) is always challenged by the spurious correlation
between domain-invariant features (e.g., class identity) and domain-specific
features (e.g., environment) that does not generalize to the target domain.
Unfortunately, even enriched with additional unsupervised target domains,
existing Unsupervised DA (UDA) methods still suffer from it. This is because
the source domain supervision only considers the target domain samples as
auxiliary data (e.g., by pseudo-labeling), yet the inherent distribution in the
target domain -- where the valuable de-correlation clues hide -- is
disregarded. We propose to make the U in UDA matter by giving equal status to
the two domains. Specifically, we learn an invariant classifier whose
prediction is simultaneously consistent with the labels in the source domain
and clusters in the target domain, hence the spurious correlation inconsistent
in the target domain is removed. We dub our approach "Invariant CONsistency
learning" (ICON). Extensive experiments show that ICON achieves the
state-of-the-art performance on the classic UDA benchmarks: Office-Home and
VisDA-2017, and outperforms all the conventional methods on the challenging
WILDS 2.0 benchmark. Codes are in https://github.com/yue-zhongqi/ICON.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.13409">Time-Series Forecasting: Unleashing Long-Term Dependencies with Fractionally Differenced Data. (arXiv:2309.13409v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maitra_S/0/1/0/all/0/1">Sarit Maitra</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishra_V/0/1/0/all/0/1">Vivek Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Dwivedi_S/0/1/0/all/0/1">Srashti Dwivedi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kundu_S/0/1/0/all/0/1">Sukanya Kundu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kundu_G/0/1/0/all/0/1">Goutam Kumar Kundu</a></p>
<p>This study introduces a novel forecasting strategy that leverages the power
of fractional differencing (FD) to capture both short- and long-term
dependencies in time series data. Unlike traditional integer differencing
methods, FD preserves memory in series while stabilizing it for modeling
purposes. By applying FD to financial data from the SPY index and incorporating
sentiment analysis from news reports, this empirical analysis explores the
effectiveness of FD in conjunction with binary classification of target
variables. Supervised classification algorithms were employed to validate the
performance of FD series. The results demonstrate the superiority of FD over
integer differencing, as confirmed by Receiver Operating Characteristic/Area
Under the Curve (ROCAUC) and Mathews Correlation Coefficient (MCC) evaluations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.15325">Neural Operators for Accelerating Scientific Simulations and Design. (arXiv:2309.15325v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Azizzadenesheli_K/0/1/0/all/0/1">Kamyar Azizzadenesheli</a>, <a href="http://arxiv.org/find/cs/1/au:+Kovachki_N/0/1/0/all/0/1">Nikola Kovachki</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zongyi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Schiaffini_M/0/1/0/all/0/1">Miguel Liu-Schiaffini</a>, <a href="http://arxiv.org/find/cs/1/au:+Kossaifi_J/0/1/0/all/0/1">Jean Kossaifi</a>, <a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1">Anima Anandkumar</a></p>
<p>Scientific discovery and engineering design are currently limited by the time
and cost of physical experiments, selected mostly through trial-and-error and
intuition that require deep domain expertise. Numerical simulations present an
alternative to physical experiments but are usually infeasible for complex
real-world domains due to the computational requirements of existing numerical
methods. Artificial intelligence (AI) presents a potential paradigm shift by
developing fast data-driven surrogate models. In particular, an AI framework,
known as neural operators, presents a principled framework for learning
mappings between functions defined on continuous domains, e.g., spatiotemporal
processes and partial differential equations (PDE). They can extrapolate and
predict solutions at new locations unseen during training, i.e., perform
zero-shot super-resolution. Neural operators can augment or even replace
existing simulators in many applications, such as computational fluid dynamics,
weather forecasting, and material modeling, while being 4-5 orders of magnitude
faster. Further, neural operators can be integrated with physics and other
domain constraints enforced at finer resolutions to obtain high-fidelity
solutions and good generalization. Since neural operators are differentiable,
they can directly optimize parameters for inverse design and other inverse
problems. We believe that neural operators present a transformative approach to
simulation and design, enabling rapid research and development.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.15400">Interpretable AI-Driven Discovery of Terrain-Precipitation Relationships for Enhanced Climate Insights. (arXiv:2309.15400v2 [physics.ao-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Xu_H/0/1/0/all/0/1">Hao Xu</a>, <a href="http://arxiv.org/find/physics/1/au:+Chen_Y/0/1/0/all/0/1">Yuntian Chen</a>, <a href="http://arxiv.org/find/physics/1/au:+Zeng_Z/0/1/0/all/0/1">Zhenzhong Zeng</a>, <a href="http://arxiv.org/find/physics/1/au:+Li_N/0/1/0/all/0/1">Nina Li</a>, <a href="http://arxiv.org/find/physics/1/au:+Li_J/0/1/0/all/0/1">Jian Li</a>, <a href="http://arxiv.org/find/physics/1/au:+Zhang_D/0/1/0/all/0/1">Dongxiao Zhang</a></p>
<p>Despite the remarkable strides made by AI-driven models in modern
precipitation forecasting, these black-box models cannot inherently deepen the
comprehension of underlying mechanisms. To address this limitation, we propose
an AI-driven knowledge discovery framework known as genetic
algorithm-geographic weighted regression (GA-GWR). Our approach seeks to unveil
the explicit equations that govern the intricate relationship between
precipitation patterns and terrain characteristics in regions marked by complex
terrain. Through this AI-driven knowledge discovery, we uncover previously
undisclosed explicit equations that shed light on the connection between
terrain features and precipitation patterns. These equations demonstrate
remarkable accuracy when applied to precipitation data, outperforming
conventional empirical models. Notably, our research reveals that the
parameters within these equations are dynamic, adapting to evolving climate
patterns. Ultimately, the unveiled equations have practical applications,
particularly in fine-scale downscaling for precipitation predictions using
low-resolution future climate data. This capability offers invaluable insights
into the anticipated changes in precipitation patterns across diverse terrains
under future climate scenarios, which enhances our ability to address the
challenges posed by contemporary climate science.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16849">Space-Time Attention with Shifted Non-Local Search. (arXiv:2309.16849v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gauen_K/0/1/0/all/0/1">Kent Gauen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1">Stanley Chan</a></p>
<p>Efficiently computing attention maps for videos is challenging due to the
motion of objects between frames. While a standard non-local search is
high-quality for a window surrounding each query point, the window's small size
cannot accommodate motion. Methods for long-range motion use an auxiliary
network to predict the most similar key coordinates as offsets from each query
location. However, accurately predicting this flow field of offsets remains
challenging, even for large-scale networks. Small spatial inaccuracies
significantly impact the attention module's quality. This paper proposes a
search strategy that combines the quality of a non-local search with the range
of predicted offsets. The method, named Shifted Non-Local Search, executes a
small grid search surrounding the predicted offsets to correct small spatial
errors. Our method's in-place computation consumes 10 times less memory and is
over 3 times faster than previous work. Experimentally, correcting the small
spatial errors improves the video frame alignment quality by over 3 dB PSNR.
Our search upgrades existing space-time attention modules, which improves video
denoising results by 0.30 dB PSNR for a 7.5% increase in overall runtime. We
integrate our space-time attention module into a UNet-like architecture to
achieve state-of-the-art results on video denoising.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00749">SEED: Domain-Specific Data Curation With Large Language Models. (arXiv:2310.00749v2 [cs.DB] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zui Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1">Lei Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Madden_S/0/1/0/all/0/1">Sam Madden</a>, <a href="http://arxiv.org/find/cs/1/au:+Kraska_T/0/1/0/all/0/1">Tim Kraska</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_Z/0/1/0/all/0/1">Zeyuan Shang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1">Ju Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_N/0/1/0/all/0/1">Nan Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_Z/0/1/0/all/0/1">Zihui Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chunwei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cafarella_M/0/1/0/all/0/1">Michael Cafarella</a></p>
<p>Data curation tasks that prepare data for analytics are critical for turning
data into actionable insights. However, due to the diverse requirements of
applications in different domains, generic off-the-shelf tools are typically
insufficient. As a result, data scientists often have to develop
domain-specific solutions tailored to both the dataset and the task, e.g.
writing domain-specific code or training machine learning models on a
sufficient number of annotated examples. This process is notoriously difficult
and time-consuming. We present SEED, an LLM-as-compiler approach that
automatically generates domain-specific data curation solutions via Large
Language Models (LLMs). Once the user describes a task, input data, and
expected output, the SEED compiler produces an executable pipeline composed of
LLM-generated code, small model, and data access modules. SEED uses these
generated modules to process most of the data records and dynamically decides
when the LLM should step in to directly process some individual records,
possibly using the data-access modules to retrieve relevant information from
the data sources to assist the LLM in solving the task. To validate this new,
revolutionary approach, we conducted experiments on 9 datasets spanning over 5
data curation tasks. The results show that SEED generates domain-specific
solutions that significantly outperform their generic counterparts, often
approaching the performance of the manually curated solutions that use
thousands of labeled training examples. Moreover, in comparison to solutions
that use the LLM on every data record, SEED achieves state-of-the-art or
comparable few-shot performance, while significantly reducing the number of LLM
calls.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02066">De Novo Drug Design with Joint Transformers. (arXiv:2310.02066v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Izdebski_A/0/1/0/all/0/1">Adam Izdebski</a>, <a href="http://arxiv.org/find/cs/1/au:+Weglarz_Tomczak_E/0/1/0/all/0/1">Ewelina Weglarz-Tomczak</a>, <a href="http://arxiv.org/find/cs/1/au:+Szczurek_E/0/1/0/all/0/1">Ewa Szczurek</a>, <a href="http://arxiv.org/find/cs/1/au:+Tomczak_J/0/1/0/all/0/1">Jakub M. Tomczak</a></p>
<p>De novo drug design requires simultaneously generating novel molecules
outside of training data and predicting their target properties, making it a
hard task for generative models. To address this, we propose Joint Transformer
that combines a Transformer decoder, Transformer encoder, and a predictor in a
joint generative model with shared weights. We formulate a probabilistic
black-box optimization algorithm that employs Joint Transformer to generate
novel molecules with improved target properties and outperforms other
SMILES-based optimization methods in de novo drug design.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02980">Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors. (arXiv:2310.02980v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Amos_I/0/1/0/all/0/1">Ido Amos</a>, <a href="http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1">Jonathan Berant</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1">Ankit Gupta</a></p>
<p>Modeling long-range dependencies across sequences is a longstanding goal in
machine learning and has led to architectures, such as state space models, that
dramatically outperform Transformers on long sequences. However, these
impressive empirical gains have been by and large demonstrated on benchmarks
(e.g. Long Range Arena), where models are randomly initialized and trained to
predict a target label from an input sequence. In this work, we show that
random initialization leads to gross overestimation of the differences between
architectures and that pretraining with standard denoising objectives, using
$\textit{only the downstream task data}$, leads to dramatic gains across
multiple architectures and to very small gaps between Transformers and state
space models (SSMs). In stark contrast to prior works, we find vanilla
Transformers to match the performance of S4 on Long Range Arena when properly
pretrained, and we improve the best reported results of SSMs on the PathX-256
task by 20 absolute points. Subsequently, we analyze the utility of
previously-proposed structured parameterizations for SSMs and show they become
mostly redundant in the presence of data-driven initialization obtained through
pretraining. Our work shows that, when evaluating different architectures on
supervised tasks, incorporation of data-driven priors via pretraining is
essential for reliable performance estimation, and can be done efficiently.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03234">Non-Smooth Weakly-Convex Finite-sum Coupled Compositional Optimization. (arXiv:2310.03234v2 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Hu_Q/0/1/0/all/0/1">Quanqi Hu</a>, <a href="http://arxiv.org/find/math/1/au:+Zhu_D/0/1/0/all/0/1">Dixian Zhu</a>, <a href="http://arxiv.org/find/math/1/au:+Yang_T/0/1/0/all/0/1">Tianbao Yang</a></p>
<p>This paper investigates new families of compositional optimization problems,
called $\underline{\bf n}$on-$\underline{\bf s}$mooth $\underline{\bf
w}$eakly-$\underline{\bf c}$onvex $\underline{\bf f}$inite-sum $\underline{\bf
c}$oupled $\underline{\bf c}$ompositional $\underline{\bf o}$ptimization (NSWC
FCCO). There has been a growing interest in FCCO due to its wide-ranging
applications in machine learning and AI, as well as its ability to address the
shortcomings of stochastic algorithms based on empirical risk minimization.
However, current research on FCCO presumes that both the inner and outer
functions are smooth, limiting their potential to tackle a more diverse set of
problems. Our research expands on this area by examining non-smooth
weakly-convex FCCO, where the outer function is weakly convex and
non-decreasing, and the inner function is weakly-convex. We analyze a
single-loop algorithm and establish its complexity for finding an
$\epsilon$-stationary point of the Moreau envelop of the objective function.
Additionally, we also extend the algorithm to solving novel non-smooth
weakly-convex tri-level finite-sum coupled compositional optimization problems,
which feature a nested arrangement of three functions. Lastly, we explore the
applications of our algorithms in deep learning for two-way partial AUC
maximization and multi-instance two-way partial AUC maximization, using
empirical studies to showcase the effectiveness of the proposed algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05953">Classification of Spam URLs Using Machine Learning Approaches. (arXiv:2310.05953v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Odeh_O/0/1/0/all/0/1">Omar Husni Odeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Arram_A/0/1/0/all/0/1">Anas Arram</a>, <a href="http://arxiv.org/find/cs/1/au:+Njoum_M/0/1/0/all/0/1">Murad Njoum</a></p>
<p>The Internet is used by billions of users every day because it offers fast
and free communication tools and platforms. Nevertheless, with this significant
increase in usage, huge amounts of spam are generated every second, which
wastes internet resources and, more importantly, users' time. This study
investigates the use of machine learning models to classify URLs as spam or
nonspam. We first extract the features from the URL as it has only one feature,
and then we compare the performance of several models, including k nearest
neighbors, bagging, random forest, logistic regression, and others.
Experimental results demonstrate that bagging outperformed other models and
achieved the highest accuracy of 98.64%. In addition, bagging outperformed the
current state-of-the-art approaches which emphasize its effectiveness in
addressing spam-related challenges on the Internet. This suggests that bagging
is a promising approach for URL spam classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.09718">Efficient and Effective Deep Multi-view Subspace Clustering. (arXiv:2310.09718v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yuxiu Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Ren Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1">Qiang Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Caiming Zhang</a></p>
<p>Recent multi-view subspace clustering achieves impressive results utilizing
deep networks, where the self-expressive correlation is typically modeled by a
fully connected (FC) layer. However, they still suffer from two limitations. i)
The parameter scale of the FC layer is quadratic to sample numbers, resulting
in high time and memory costs that significantly degrade their feasibility in
large-scale datasets. ii) It is under-explored to extract a unified
representation that simultaneously satisfies minimal sufficiency and
discriminability. To this end, we propose a novel deep framework, termed
Efficient and Effective deep Multi-View Subspace Clustering (E$^2$MVSC).
Instead of a parameterized FC layer, we design a Relation-Metric Net that
decouples network parameter scale from sample numbers for greater computational
efficiency. Most importantly, the proposed method devises a multi-type
auto-encoder to explicitly decouple consistent, complementary, and superfluous
information from every view, which is supervised by a soft clustering
assignment similarity constraint. Following information bottleneck theory and
the maximal coding rate reduction principle, a sufficient yet minimal unified
representation can be obtained, as well as pursuing intra-cluster aggregation
and inter-cluster separability within it. Extensive experiments show that
E$^2$MVSC yields comparable results to existing methods and achieves
state-of-the-art performance in various types of multi-view datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.09971">AMAGO: Scalable In-Context Reinforcement Learning for Adaptive Agents. (arXiv:2310.09971v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Grigsby_J/0/1/0/all/0/1">Jake Grigsby</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1">Linxi Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yuke Zhu</a></p>
<p>We introduce AMAGO, an in-context Reinforcement Learning (RL) agent that uses
sequence models to tackle the challenges of generalization, long-term memory,
and meta-learning. Recent works have shown that off-policy learning can make
in-context RL with recurrent policies viable. Nonetheless, these approaches
require extensive tuning and limit scalability by creating key bottlenecks in
agents' memory capacity, planning horizon, and model size. AMAGO revisits and
redesigns the off-policy in-context approach to successfully train
long-sequence Transformers over entire rollouts in parallel with end-to-end RL.
Our agent is uniquely scalable and applicable to a wide range of problems. We
demonstrate its strong performance empirically in meta-RL and long-term memory
domains. AMAGO's focus on sparse rewards and off-policy data also allows
in-context learning to extend to goal-conditioned problems with challenging
exploration. When combined with a novel hindsight relabeling scheme, AMAGO can
solve a previously difficult category of open-world domains, where agents
complete many possible instructions in procedurally generated environments. We
evaluate our agent on three goal-conditioned domains and study how its
individual improvements connect to create a generalist policy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.11211">Understanding Fairness Surrogate Functions in Algorithmic Fairness. (arXiv:2310.11211v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1">Wei Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhanke Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhicong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1">Bo Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yong Liu</a></p>
<p>It has been observed that machine learning algorithms exhibit biased
predictions against certain population groups. To mitigate such bias while
achieving comparable accuracy, a promising approach is to introduce surrogate
functions of the concerned fairness definition and solve a constrained
optimization problem. However, it is intriguing in previous work that such
fairness surrogate functions may yield unfair results and high instability. In
this work, in order to deeply understand them, taking a widely used fairness
definition--demographic parity as an example, we show that there is a
surrogate-fairness gap between the fairness definition and the fairness
surrogate function. Also, the theoretical analysis and experimental results
about the gap motivate us that the fairness and stability will be affected by
the points far from the decision boundary, which is the large margin points
issue investigated in this paper. To address it, we propose the general sigmoid
surrogate to simultaneously reduce both the surrogate-fairness gap and the
variance, and offer a rigorous fairness and stability upper bound.
Interestingly, the theory also provides insights into two important issues that
deal with the large margin points as well as obtaining a more balanced dataset
are beneficial to fairness and stability. Furthermore, we elaborate a novel and
general algorithm called Balanced Surrogate, which iteratively reduces the gap
to mitigate unfairness. Finally, we provide empirical evidence showing that our
methods consistently improve fairness and stability while maintaining accuracy
comparable to the baselines in three real-world datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.11730">Federated Heterogeneous Graph Neural Network for Privacy-preserving Recommendation. (arXiv:2310.11730v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1">Bo Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yang Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haoyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1">Wenchuan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1">Junping Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1">Chuan Shi</a></p>
<p>Heterogeneous information network (HIN), which contains rich semantics
depicted by meta-paths, has become a powerful tool to alleviate data sparsity
in recommender systems. Existing HIN-based recommendations hold the data
centralized storage assumption and conduct centralized model training. However,
the real-world data is often stored in a distributed manner for privacy
concerns, resulting in the failure of centralized HIN-based recommendations. In
this paper, we suggest the HIN is partitioned into private HINs stored in the
client side and shared HINs in the server. Following this setting, we propose a
federated heterogeneous graph neural network (FedHGNN) based framework, which
can collaboratively train a recommendation model on distributed HINs without
leaking user privacy. Specifically, we first formalize the privacy definition
in the light of differential privacy for HIN-based federated recommendation,
which aims to protect user-item interactions of private HIN as well as user's
high-order patterns from shared HINs. To recover the broken meta-path based
semantics caused by distributed data storage and satisfy the proposed privacy,
we elaborately design a semantic-preserving user interactions publishing
method, which locally perturbs user's high-order patterns as well as related
user-item interactions for publishing. After that, we propose a HGNN model for
recommendation, which conducts node- and semantic-level aggregations to capture
recovered semantics. Extensive experiments on three datasets demonstrate our
model outperforms existing methods by a large margin (up to 34% in HR@10 and
42% in NDCG@10) under an acceptable privacy budget.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.11829">Towards Graph Foundation Models: A Survey and Beyond. (arXiv:2310.11829v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiawei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Cheng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Zhiyuan Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Junze Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yibo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Mengmei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_T/0/1/0/all/0/1">Ting Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1">Yuan Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Lichao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1">Philip S. Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1">Chuan Shi</a></p>
<p>Foundation models have emerged as critical components in a variety of
artificial intelligence applications, and showcase significant success in
natural language processing and several other domains. Meanwhile, the field of
graph machine learning is witnessing a paradigm transition from shallow methods
to more sophisticated deep learning approaches. The capabilities of foundation
models to generalize and adapt motivate graph machine learning researchers to
discuss the potential of developing a new graph learning paradigm. This
paradigm envisions models that are pre-trained on extensive graph data and can
be adapted for various graph tasks. Despite this burgeoning interest, there is
a noticeable lack of clear definitions and systematic analyses pertaining to
this new domain. To this end, this article introduces the concept of Graph
Foundation Models (GFMs), and offers an exhaustive explanation of their key
characteristics and underlying technologies. We proceed to classify the
existing work related to GFMs into three distinct categories, based on their
dependence on graph neural networks and large language models. In addition to
providing a thorough review of the current state of GFMs, this article also
outlooks potential avenues for future research in this rapidly evolving domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.14714">BatteryML:An Open-source platform for Machine Learning on Battery Degradation. (arXiv:2310.14714v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Han Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gui_X/0/1/0/all/0/1">Xiaofan Gui</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1">Shun Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Ziheng Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuqi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1">Jiang Bian</a></p>
<p>Battery degradation remains a pivotal concern in the energy storage domain,
with machine learning emerging as a potent tool to drive forward insights and
solutions. However, this intersection of electrochemical science and machine
learning poses complex challenges. Machine learning experts often grapple with
the intricacies of battery science, while battery researchers face hurdles in
adapting intricate models tailored to specific datasets. Beyond this, a
cohesive standard for battery degradation modeling, inclusive of data formats
and evaluative benchmarks, is conspicuously absent. Recognizing these
impediments, we present BatteryML - a one-step, all-encompass, and open-source
platform designed to unify data preprocessing, feature extraction, and the
implementation of both traditional and state-of-the-art models. This
streamlined approach promises to enhance the practicality and efficiency of
research applications. BatteryML seeks to fill this void, fostering an
environment where experts from diverse specializations can collaboratively
contribute, thus elevating the collective understanding and advancement of
battery research.The code for our project is publicly available on GitHub at
https://github.com/microsoft/BatteryML.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.15015">Leveraging Deep Learning for Abstractive Code Summarization of Unofficial Documentation. (arXiv:2310.15015v4 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Naghshzan_A/0/1/0/all/0/1">AmirHossein Naghshzan</a>, <a href="http://arxiv.org/find/cs/1/au:+Guerrouj_L/0/1/0/all/0/1">Latifa Guerrouj</a>, <a href="http://arxiv.org/find/cs/1/au:+Baysal_O/0/1/0/all/0/1">Olga Baysal</a></p>
<p>Usually, programming languages have official documentation to guide
developers with APIs, methods, and classes. However, researchers identified
insufficient or inadequate documentation examples and flaws with the API's
complex structure as barriers to learning an API. As a result, developers may
consult other sources (StackOverflow, GitHub, etc.) to learn more about an API.
Recent research studies have shown that unofficial documentation is a valuable
source of information for generating code summaries. We, therefore, have been
motivated to leverage such a type of documentation along with deep learning
techniques towards generating high-quality summaries for APIs discussed in
informal documentation. This paper proposes an automatic approach using the
BART algorithm, a state-of-the-art transformer model, to generate summaries for
APIs discussed in StackOverflow. We built an oracle of human-generated
summaries to evaluate our approach against it using ROUGE and BLEU metrics
which are the most widely used evaluation metrics in text summarization.
Furthermore, we evaluated our summaries empirically against a previous work in
terms of quality. Our findings demonstrate that using deep learning algorithms
can improve summaries' quality and outperform the previous work by an average
of %57 for Precision, %66 for Recall, and %61 for F-measure, and it runs 4.4
times faster.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.15020">Invariance is Key to Generalization: Examining the Role of Representation in Sim-to-Real Transfer for Visual Navigation. (arXiv:2310.15020v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ai_B/0/1/0/all/0/1">Bo Ai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhanxin Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hsu_D/0/1/0/all/0/1">David Hsu</a></p>
<p>The data-driven approach to robot control has been gathering pace rapidly,
yet generalization to unseen task domains remains a critical challenge. We
argue that the key to generalization is representations that are (i) rich
enough to capture all task-relevant information and (ii) invariant to
superfluous variability between the training and the test domains. We
experimentally study such a representation -- containing both depth and
semantic information -- for visual navigation and show that it enables a
control policy trained entirely in simulated indoor scenes to generalize to
diverse real-world environments, both indoors and outdoors. Further, we show
that our representation reduces the A-distance between the training and test
domains, improving the generalization error bound as a result. Our proposed
approach is scalable: the learned policy improves continuously, as the
foundation models that it exploits absorb more diverse data during
pre-training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17378">Optimization dependent generalization bound for ReLU networks based on sensitivity in the tangent bundle. (arXiv:2310.17378v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Racz_D/0/1/0/all/0/1">D&#xe1;niel R&#xe1;cz</a>, <a href="http://arxiv.org/find/cs/1/au:+Petreczky_M/0/1/0/all/0/1">Mih&#xe1;ly Petreczky</a>, <a href="http://arxiv.org/find/cs/1/au:+Csertan_A/0/1/0/all/0/1">Andr&#xe1;s Csert&#xe1;n</a>, <a href="http://arxiv.org/find/cs/1/au:+Daroczy_B/0/1/0/all/0/1">B&#xe1;lint Dar&#xf3;czy</a></p>
<p>Recent advances in deep learning have given us some very promising results on
the generalization ability of deep neural networks, however literature still
lacks a comprehensive theory explaining why heavily over-parametrized models
are able to generalize well while fitting the training data. In this paper we
propose a PAC type bound on the generalization error of feedforward ReLU
networks via estimating the Rademacher complexity of the set of networks
available from an initial parameter vector via gradient descent. The key idea
is to bound the sensitivity of the network's gradient to perturbation of the
input data along the optimization trajectory. The obtained bound does not
explicitly depend on the depth of the network. Our results are experimentally
verified on the MNIST and CIFAR-10 datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.18144">Improving Intrinsic Exploration by Creating Stationary Objectives. (arXiv:2310.18144v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Castanyer_R/0/1/0/all/0/1">Roger Creus Castanyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Romoff_J/0/1/0/all/0/1">Joshua Romoff</a>, <a href="http://arxiv.org/find/cs/1/au:+Berseth_G/0/1/0/all/0/1">Glen Berseth</a></p>
<p>Exploration bonuses in reinforcement learning guide long-horizon exploration
by defining custom intrinsic objectives. Several exploration objectives like
count-based bonuses, pseudo-counts, and state-entropy maximization are
non-stationary and hence are difficult to optimize for the agent. While this
issue is generally known, it is usually omitted and solutions remain
under-explored. The key contribution of our work lies in transforming the
original non-stationary rewards into stationary rewards through an augmented
state representation. For this purpose, we introduce the Stationary Objectives
For Exploration (SOFE) framework. SOFE requires identifying sufficient
statistics for different exploration bonuses and finding an efficient encoding
of these statistics to use as input to a deep network. SOFE is based on
proposing state augmentations that expand the state space but hold the promise
of simplifying the optimization of the agent's objective. We show that SOFE
improves the performance of several exploration objectives, including
count-based bonuses, pseudo-counts, and state-entropy maximization. Moreover,
SOFE outperforms prior methods that attempt to stabilize the optimization of
intrinsic objectives. We demonstrate the efficacy of SOFE in hard-exploration
problems, including sparse-reward tasks, pixel-based observations, 3D
navigation, and procedurally generated environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.18534">Multi Time Scale World Models. (arXiv:2310.18534v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shaj_V/0/1/0/all/0/1">Vaisakh Shaj</a>, <a href="http://arxiv.org/find/cs/1/au:+Zadeh_S/0/1/0/all/0/1">Saleh Gholam Zadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Demir_O/0/1/0/all/0/1">Ozan Demir</a>, <a href="http://arxiv.org/find/cs/1/au:+Douat_L/0/1/0/all/0/1">Luiz Ricardo Douat</a>, <a href="http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1">Gerhard Neumann</a></p>
<p>Intelligent agents use internal world models to reason and make predictions
about different courses of their actions at many scales. Devising learning
paradigms and architectures that allow machines to learn world models that
operate at multiple levels of temporal abstractions while dealing with complex
uncertainty predictions is a major technical hurdle. In this work, we propose a
probabilistic formalism to learn multi-time scale world models which we call
the Multi Time Scale State Space (MTS3) model. Our model uses a computationally
efficient inference scheme on multiple time scales for highly accurate
long-horizon predictions and uncertainty estimates over several seconds into
the future. Our experiments, which focus on action conditional long horizon
future predictions, show that MTS3 outperforms recent methods on several system
identification benchmarks including complex simulated and real-world dynamical
systems. Code is available at this repository: https://github.com/ALRhub/MTS3.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.18940">Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game. (arXiv:2310.18940v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zelai Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1">Chao Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_F/0/1/0/all/0/1">Fei Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yi Wu</a></p>
<p>Agents built with large language models (LLMs) have recently achieved great
advancements. However, most of the efforts focus on single-agent or cooperative
settings, leaving more general multi-agent environments underexplored. We
propose a new framework powered by reinforcement learning (RL) to develop
strategic language agents, i.e., LLM-based agents with strategic thinking
ability, for a popular language game, Werewolf. Werewolf is a social deduction
game with hidden roles that involves both cooperation and competition and
emphasizes deceptive communication and diverse gameplay. Our agent tackles this
game by first using LLMs to reason about potential deceptions and generate a
set of strategically diverse actions. Then an RL policy, which selects an
action from the candidates, is learned by population-based training to enhance
the agents' decision-making ability. By combining LLMs with the RL policy, our
agent produces a variety of emergent strategies, achieves the highest win rate
against other LLM-based agents, and stays robust against adversarial human
players in the Werewolf game.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19112">Efficient IoT Inference via Context-Awareness. (arXiv:2310.19112v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rastikerdar_M/0/1/0/all/0/1">Mohammad Mehdi Rastikerdar</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1">Shiwei Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_H/0/1/0/all/0/1">Hui Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganesan_D/0/1/0/all/0/1">Deepak Ganesan</a></p>
<p>While existing strategies to execute deep learning-based classification on
low-power platforms assume the models are trained on all classes of interest,
this paper posits that adopting context-awareness i.e. narrowing down a
classification task to the current deployment context consisting of only recent
inference queries can substantially enhance performance in resource-constrained
environments. We propose a new paradigm, CACTUS, for scalable and efficient
context-aware classification where a micro-classifier recognizes a small set of
classes relevant to the current context and, when context change happens (e.g.,
a new class comes into the scene), rapidly switches to another suitable
micro-classifier. CACTUS features several innovations, including optimizing the
training cost of context-aware classifiers, enabling on-the-fly context-aware
switching between classifiers, and balancing context switching costs and
performance gains via simple yet effective switching policies. We show that
CACTUS achieves significant benefits in accuracy, latency, and compute budget
across a range of datasets and IoT platforms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19308">Free from Bellman Completeness: Trajectory Stitching via Model-based Return-conditioned Supervised Learning. (arXiv:2310.19308v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhaoyi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1">Chuning Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1">Runlong Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_Q/0/1/0/all/0/1">Qiwen Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1">Abhishek Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1">Simon Shaolei Du</a></p>
<p>Off-policy dynamic programming (DP) techniques such as $Q$-learning have
proven to be important in sequential decision-making problems. In the presence
of function approximation, however, these techniques often diverge due to the
absence of Bellman completeness in the function classes considered, a crucial
condition for the success of DP-based methods. In this paper, we show how
off-policy learning techniques based on return-conditioned supervised learning
(RCSL) are able to circumvent these challenges of Bellman completeness,
converging under significantly more relaxed assumptions inherited from
supervised learning. We prove there exists a natural environment in which if
one uses two-layer multilayer perceptron as the function approximator, the
layer width needs to grow linearly with the state space size to satisfy Bellman
completeness while a constant layer width is enough for RCSL. These findings
take a step towards explaining the superior empirical performance of RCSL
methods compared to DP-based methods in environments with near-optimal
datasets. Furthermore, in order to learn from sub-optimal datasets, we propose
a simple framework called MBRCSL, granting RCSL methods the ability of dynamic
programming to stitch together segments from distinct trajectories. MBRCSL
leverages learned dynamics models and forward sampling to accomplish trajectory
stitching while avoiding the need for Bellman completeness that plagues all
dynamic programming algorithms. We propose both theoretical analysis and
experimental evaluation to back these claims, outperforming state-of-the-art
model-free and model-based offline RL algorithms across several simulated
robotics problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19788">Worst-Case Optimal Multi-Armed Gaussian Best Arm Identification with a Fixed Budget. (arXiv:2310.19788v2 [math.ST] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Kato_M/0/1/0/all/0/1">Masahiro Kato</a></p>
<p>Experimental design is crucial in evidence-based decision-making with
multiple treatment arms, such as online advertisements and medical treatments.
This study investigates the problem of identifying the treatment arm with the
highest expected outcome, referred to as the best treatment arm, while
minimizing the probability of misidentification. This problem has been studied
across numerous research fields, including best arm identification (BAI) and
ordinal optimization. In our experiments, the number of treatment-allocation
rounds is fixed. During each round, a decision-maker allocates a treatment arm
to an experimental unit and observes a corresponding outcome, which follows a
Gaussian distribution with variances that can differ among the treatment arms.
At the end of the experiment, we recommend one of the treatment arms as an
estimate of the best treatment arm based on the observations. To design an
experiment, we first discuss the worst-case lower bound for the probability of
misidentification through an information-theoretic approach. Then, under the
assumption that the variances are known, we propose the
Generalized-Neyman-Allocation (GNA)-empirical-best-arm (EBA) strategy, an
extension of the Neyman allocation proposed by Neyman (1934). We show that the
GNA-EBA strategy is asymptotically optimal in the sense that its probability of
misidentification aligns with the lower bounds as the sample size increases
indefinitely and the differences between the expected outcomes of the best and
other suboptimal arms converge to a uniform value. We refer to such strategies
as asymptotically worst-case optimal.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00226">Transformers are Efficient In-Context Estimators for Wireless Communication. (arXiv:2311.00226v2 [eess.SP] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Rajagopalan_V/0/1/0/all/0/1">Vicram Rajagopalan</a>, <a href="http://arxiv.org/find/eess/1/au:+Kunde_V/0/1/0/all/0/1">Vishnu Teja Kunde</a>, <a href="http://arxiv.org/find/eess/1/au:+Valmeekam_C/0/1/0/all/0/1">Chandra Shekhara Kaushik Valmeekam</a>, <a href="http://arxiv.org/find/eess/1/au:+Narayanan_K/0/1/0/all/0/1">Krishna Narayanan</a>, <a href="http://arxiv.org/find/eess/1/au:+Shakkottai_S/0/1/0/all/0/1">Srinivas Shakkottai</a>, <a href="http://arxiv.org/find/eess/1/au:+Kalathil_D/0/1/0/all/0/1">Dileep Kalathil</a>, <a href="http://arxiv.org/find/eess/1/au:+Chamberland_J/0/1/0/all/0/1">Jean-Francois Chamberland</a></p>
<p>Pre-trained transformers can perform in-context learning, where they adapt to
a new task using only a small number of prompts without any explicit model
optimization. Inspired by this attribute, we propose a novel approach, called
in-context estimation, for the canonical communication problem of estimating
transmitted symbols from received symbols. A communication channel is
essentially a noisy function that maps transmitted symbols to received symbols,
and this function can be represented by an unknown parameter whose statistics
depend on an (also unknown) latent context. Conventional approaches typically
do not fully exploit hierarchical model with the latent context. Instead, they
often use mismatched priors to form a linear minimum mean-squared error
estimate of the channel parameter, which is then used to estimate successive,
unknown transmitted symbols. We make the basic connection that transformers
show excellent contextual sequence completion with a few prompts, and so they
should be able to implicitly determine the latent context from pilot symbols to
perform end-to-end in-context estimation of transmitted symbols. Furthermore,
the transformer should use information efficiently, i.e., it should utilize any
pilots received to attain the best possible symbol estimates. Through extensive
simulations, we show that in-context estimation not only significantly
outperforms standard approaches, but also achieves the same performance as an
estimator with perfect knowledge of the latent context within a few context
examples. Thus, we make a strong case that transformers are efficient
in-context estimators in the communication setting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00690">What User Behaviors Make the Differences During the Process of Visual Analytics?. (arXiv:2311.00690v3 [cs.HC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zekun Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Doroudian_S/0/1/0/all/0/1">Shahin Doroudian</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_A/0/1/0/all/0/1">Aidong Lu</a></p>
<p>The understanding of visual analytics process can benefit visualization
researchers from multiple aspects, including improving visual designs and
developing advanced interaction functions. However, the log files of user
behaviors are still hard to analyze due to the complexity of sensemaking and
our lack of knowledge on the related user behaviors. This work presents a study
on a comprehensive data collection of user behaviors, and our analysis approach
with time-series classification methods. We have chosen a classical
visualization application, Covid-19 data analysis, with common analysis tasks
covering geo-spatial, time-series and multi-attributes. Our user study collects
user behaviors on a diverse set of visualization tasks with two comparable
systems, desktop and immersive visualizations. We summarize the classification
results with three time-series machine learning algorithms at two scales, and
explore the influences of behavior features. Our results reveal that user
behaviors can be distinguished during the process of visual analytics and there
is a potentially strong association between the physical behaviors of users and
the visualization tasks they perform. We also demonstrate the usage of our
models by interpreting open sessions of visual analytics, which provides an
automatic way to study sensemaking without tedious manual annotations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04472">Autonomous Advanced Aerial Mobility -- An End-to-end Autonomy Framework for UAVs and Beyond. (arXiv:2311.04472v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1">Sakshi Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Palanisamy_P/0/1/0/all/0/1">Praveen Palanisamy</a></p>
<p>Developing aerial robots that can both safely navigate and execute assigned
mission without any human intervention - i.e., fully autonomous aerial mobility
of passengers and goods - is the larger vision that guides the research,
design, and development efforts in the aerial autonomy space. However, it is
highly challenging to concurrently operationalize all types of aerial vehicles
that are operating fully autonomously sharing the airspace. Full autonomy of
the aerial transportation sector includes several aspects, such as design of
the technology that powers the vehicles, operations of multi-agent fleets, and
process of certification that meets stringent safety requirements of aviation
sector. Thereby, Autonomous Advanced Aerial Mobility is still a vague term and
its consequences for researchers and professionals are ambiguous. To address
this gap, we present a comprehensive perspective on the emerging field of
autonomous advanced aerial mobility, which involves the use of unmanned aerial
vehicles (UAVs) and electric vertical takeoff and landing (eVTOL) aircraft for
various applications, such as urban air mobility, package delivery, and
surveillance. The article proposes a scalable and extensible autonomy framework
consisting of four main blocks: sensing, perception, planning, and controls.
Furthermore, the article discusses the challenges and opportunities in
multi-agent fleet operations and management, as well as the testing,
validation, and certification aspects of autonomous aerial systems. Finally,
the article explores the potential of monolithic models for aerial autonomy and
analyzes their advantages and limitations. The perspective aims to provide a
holistic picture of the autonomous advanced aerial mobility field and its
future directions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05435">Parkinson&#x27;s Disease Detection through Vocal Biomarkers and Advanced Machine Learning Algorithms. (arXiv:2311.05435v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sayed_M/0/1/0/all/0/1">Md Abu Sayed</a>, <a href="http://arxiv.org/find/cs/1/au:+Tayaba_M/0/1/0/all/0/1">Maliha Tayaba</a>, <a href="http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1">MD Tanvir Islam</a>, <a href="http://arxiv.org/find/cs/1/au:+Pavel_M/0/1/0/all/0/1">Md Eyasin Ul Islam Pavel</a>, <a href="http://arxiv.org/find/cs/1/au:+Mia_M/0/1/0/all/0/1">Md Tuhin Mia</a>, <a href="http://arxiv.org/find/cs/1/au:+Ayon_E/0/1/0/all/0/1">Eftekhar Hossain Ayon</a>, <a href="http://arxiv.org/find/cs/1/au:+Nob_N/0/1/0/all/0/1">Nur Nob</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghosh_B/0/1/0/all/0/1">Bishnu Padh Ghosh</a></p>
<p>Parkinson's disease (PD) is a prevalent neurodegenerative disorder known for
its impact on motor neurons, causing symptoms like tremors, stiffness, and gait
difficulties. This study explores the potential of vocal feature alterations in
PD patients as a means of early disease prediction. This research aims to
predict the onset of Parkinson's disease. Utilizing a variety of advanced
machine-learning algorithms, including XGBoost, LightGBM, Bagging, AdaBoost,
and Support Vector Machine, among others, the study evaluates the predictive
performance of these models using metrics such as accuracy, area under the
curve (AUC), sensitivity, and specificity. The findings of this comprehensive
analysis highlight LightGBM as the most effective model, achieving an
impressive accuracy rate of 96% alongside a matching AUC of 96%. LightGBM
exhibited a remarkable sensitivity of 100% and specificity of 94.43%,
surpassing other machine learning algorithms in accuracy and AUC scores. Given
the complexities of Parkinson's disease and its challenges in early diagnosis,
this study underscores the significance of leveraging vocal biomarkers coupled
with advanced machine-learning techniques for precise and timely PD detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06650">Heuristic Optimal Transport in Branching Networks. (arXiv:2311.06650v2 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Andrecut_M/0/1/0/all/0/1">M. Andrecut</a></p>
<p>Optimal transport aims to learn a mapping of sources to targets by minimizing
the cost, which is typically defined as a function of distance. The solution to
this problem consists of straight line segments optimally connecting sources to
targets, and it does not exhibit branching. These optimal solutions are in
stark contrast with both natural, and man-made transportation networks, where
branching structures are prevalent. Here we discuss a fast heuristic branching
method for optimal transport in networks, and we provide several applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08105">DiLoCo: Distributed Low-Communication Training of Language Models. (arXiv:2311.08105v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Douillard_A/0/1/0/all/0/1">Arthur Douillard</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1">Qixuan Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Rusu_A/0/1/0/all/0/1">Andrei A. Rusu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chhaparia_R/0/1/0/all/0/1">Rachita Chhaparia</a>, <a href="http://arxiv.org/find/cs/1/au:+Donchev_Y/0/1/0/all/0/1">Yani Donchev</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuncoro_A/0/1/0/all/0/1">Adhiguna Kuncoro</a>, <a href="http://arxiv.org/find/cs/1/au:+Ranzato_M/0/1/0/all/0/1">Marc&#x27;Aurelio Ranzato</a>, <a href="http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1">Arthur Szlam</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1">Jiajun Shen</a></p>
<p>Large language models (LLM) have become a critical component in many
applications of machine learning. However, standard approaches to training LLM
require a large number of tightly interconnected accelerators, with devices
exchanging gradients and other intermediate states at each optimization step.
While it is difficult to build and maintain a single computing cluster hosting
many accelerators, it might be easier to find several computing clusters each
hosting a smaller number of devices. In this work, we propose a distributed
optimization algorithm, Distributed Low-Communication (DiLoCo), that enables
training of language models on islands of devices that are poorly connected.
The approach is a variant of federated averaging, where the number of inner
steps is large, the inner optimizer is AdamW, and the outer optimizer is
Nesterov momentum. On the widely used C4 dataset, we show that DiLoCo on 8
workers performs as well as fully synchronous optimization while communicating
500 times less. DiLoCo exhibits great robustness to the data distribution of
each worker. It is also robust to resources becoming unavailable over time, and
vice versa, it can seamlessly leverage resources that become available during
training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09018">On the Foundation of Distributionally Robust Reinforcement Learning. (arXiv:2311.09018v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shengbo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Si_N/0/1/0/all/0/1">Nian Si</a>, <a href="http://arxiv.org/find/cs/1/au:+Blanchet_J/0/1/0/all/0/1">Jose Blanchet</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhengyuan Zhou</a></p>
<p>Motivated by the need for a robust policy in the face of environment shifts
between training and the deployment, we contribute to the theoretical
foundation of distributionally robust reinforcement learning (DRRL). This is
accomplished through a comprehensive modeling framework centered around
distributionally robust Markov decision processes (DRMDPs). This framework
obliges the decision maker to choose an optimal policy under the worst-case
distributional shift orchestrated by an adversary. By unifying and extending
existing formulations, we rigorously construct DRMDPs that embraces various
modeling attributes for both the decision maker and the adversary. These
attributes include adaptability granularity, exploring history-dependent,
Markov, and Markov time-homogeneous decision maker and adversary dynamics.
Additionally, we delve into the flexibility of shifts induced by the adversary,
examining SA and S-rectangularity. Within this DRMDP framework, we investigate
conditions for the existence or absence of the dynamic programming principle
(DPP). From an algorithmic standpoint, the existence of DPP holds significant
implications, as the vast majority of existing data and computationally
efficiency RL algorithms are reliant on the DPP. To study its existence, we
comprehensively examine combinations of controller and adversary attributes,
providing streamlined proofs grounded in a unified methodology. We also offer
counterexamples for settings in which a DPP with full generality is absent.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10777">A Systematic Review of Aspect-based Sentiment Analysis (ABSA): Domains, Methods, and Trends. (arXiv:2311.10777v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1">Yan Cathy Hua</a>, <a href="http://arxiv.org/find/cs/1/au:+Denny_P/0/1/0/all/0/1">Paul Denny</a>, <a href="http://arxiv.org/find/cs/1/au:+Taskova_K/0/1/0/all/0/1">Katerina Taskova</a>, <a href="http://arxiv.org/find/cs/1/au:+Wicker_J/0/1/0/all/0/1">J&#xf6;rg Wicker</a></p>
<p>Aspect-based Sentiment Analysis (ABSA) is a type of fine-grained sentiment
analysis (SA) that identifies aspects and the associated opinions from a given
text. In the digital era, ABSA gained increasing popularity and applications in
mining opinionated text data to obtain insights and support decisions. ABSA
research employs linguistic, statistical, and machine-learning approaches and
utilises resources such as labelled datasets, aspect and sentiment lexicons and
ontology. By its nature, ABSA is domain-dependent and can be sensitive to the
impact of misalignment between the resource and application domains. However,
to our knowledge, this topic has not been explored by the existing ABSA
literature reviews. In this paper, we present a Systematic Literature Review
(SLR) of ABSA studies with a focus on the research application domain, dataset
domain, and the research methods to examine their relationships and identify
trends over time. Our results suggest a number of potential systemic issues in
the ABSA research literature, including the predominance of the
``product/service review'' dataset domain among the majority of studies that
did not have a specific research application domain, coupled with the
prevalence of dataset-reliant methods such as supervised machine learning. This
review makes a number of unique contributions to the ABSA research field: 1) To
our knowledge, it is the first SLR that links the research domain, dataset
domain, and research method through a systematic perspective; 2) it is one of
the largest scoped SLR on ABSA, with 519 eligible studies filtered from 4191
search results without time constraint; and 3) our review methodology adopted
an innovative automatic filtering process based on PDF-mining, which enhanced
screening quality and reliability. Suggestions and our review limitations are
also discussed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.11629">Generating Realistic Counterfactuals for Retinal Fundus and OCT Images using Diffusion Models. (arXiv:2311.11629v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ilanchezian_I/0/1/0/all/0/1">Indu Ilanchezian</a>, <a href="http://arxiv.org/find/cs/1/au:+Boreiko_V/0/1/0/all/0/1">Valentyn Boreiko</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuhlewein_L/0/1/0/all/0/1">Laura K&#xfc;hlewein</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Ziwei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ayhan_M/0/1/0/all/0/1">Murat Se&#xe7;kin Ayhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hein_M/0/1/0/all/0/1">Matthias Hein</a>, <a href="http://arxiv.org/find/cs/1/au:+Koch_L/0/1/0/all/0/1">Lisa Koch</a>, <a href="http://arxiv.org/find/cs/1/au:+Berens_P/0/1/0/all/0/1">Philipp Berens</a></p>
<p>Counterfactual reasoning is often used in clinical settings to explain
decisions or weigh alternatives. Therefore, for imaging based specialties such
as ophthalmology, it would be beneficial to be able to create counterfactual
images, illustrating answers to questions like "If the subject had had diabetic
retinopathy, how would the fundus image have looked?". Here, we demonstrate
that using a diffusion model in combination with an adversarially robust
classifier trained on retinal disease classification tasks enables the
generation of highly realistic counterfactuals of retinal fundus images and
optical coherence tomography (OCT) B-scans. The key to the realism of
counterfactuals is that these classifiers encode salient features indicative
for each disease class and can steer the diffusion model to depict disease
signs or remove disease-related lesions in a realistic way. In a user study,
domain experts also found the counterfactuals generated using our method
significantly more realistic than counterfactuals generated from a previous
method, and even indistinguishable from real images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12589">Improving Source-Free Target Adaptation with Vision Transformers Leveraging Domain Representation Images. (arXiv:2311.12589v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sawhney_G/0/1/0/all/0/1">Gauransh Sawhney</a>, <a href="http://arxiv.org/find/cs/1/au:+Dave_D/0/1/0/all/0/1">Daksh Dave</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1">Adeel Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jiechao Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Saleem_K/0/1/0/all/0/1">Khalid Saleem</a></p>
<p>Unsupervised Domain Adaptation (UDA) methods facilitate knowledge transfer
from a labeled source domain to an unlabeled target domain, navigating the
obstacle of domain shift. While Convolutional Neural Networks (CNNs) are a
staple in UDA, the rise of Vision Transformers (ViTs) provides new avenues for
domain generalization. This paper presents an innovative method to bolster ViT
performance in source-free target adaptation, beginning with an evaluation of
how key, query, and value elements affect ViT outcomes. Experiments indicate
that altering the key component has negligible effects on Transformer
performance. Leveraging this discovery, we introduce Domain Representation
Images (DRIs), feeding embeddings through the key element. DRIs act as
domain-specific markers, effortlessly merging with the training regimen. To
assess our method, we perform target adaptation tests on the Cross Instance DRI
source-only (SO) control. We measure the efficacy of target adaptation with and
without DRIs, against existing benchmarks like SHOT-B* and adaptations via
CDTrans. Findings demonstrate that excluding DRIs offers limited gains over
SHOT-B*, while their inclusion in the key segment boosts average precision
promoting superior domain generalization. This research underscores the vital
role of DRIs in enhancing ViT efficiency in UDA scenarios, setting a precedent
for further domain adaptation explorations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13743">FinMem: A Performance-Enhanced LLM Trading Agent with Layered Memory and Character Design. (arXiv:2311.13743v2 [q-fin.CP] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+Yu_Y/0/1/0/all/0/1">Yangyang Yu</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Li_H/0/1/0/all/0/1">Haohang Li</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Chen_Z/0/1/0/all/0/1">Zhi Chen</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Jiang_Y/0/1/0/all/0/1">Yuechen Jiang</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Li_Y/0/1/0/all/0/1">Yang Li</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Zhang_D/0/1/0/all/0/1">Denghui Zhang</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Liu_R/0/1/0/all/0/1">Rong Liu</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Suchow_J/0/1/0/all/0/1">Jordan W. Suchow</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Khashanah_K/0/1/0/all/0/1">Khaldoun Khashanah</a></p>
<p>Recent advancements in Large Language Models (LLMs) have exhibited notable
efficacy in question-answering (QA) tasks across diverse domains. Their prowess
in integrating extensive web knowledge has fueled interest in developing
LLM-based autonomous agents. While LLMs are efficient in decoding human
instructions and deriving solutions by holistically processing historical
inputs, transitioning to purpose-driven agents requires a supplementary
rational architecture to process multi-source information, establish reasoning
chains, and prioritize critical tasks. Addressing this, we introduce
\textsc{FinMem}, a novel LLM-based agent framework devised for financial
decision-making. It encompasses three core modules: Profiling, to customize the
agent's characteristics; Memory, with layered message processing, to aid the
agent in assimilating hierarchical financial data; and Decision-making, to
convert insights gained from memories into investment decisions. Notably,
\textsc{FinMem}'s memory module aligns closely with the cognitive structure of
human traders, offering robust interpretability and real-time tuning. Its
adjustable cognitive span allows for the retention of critical information
beyond human perceptual limits, thereby enhancing trading outcomes. This
framework enables the agent to self-evolve its professional knowledge, react
agilely to new investment cues, and continuously refine trading decisions in
the volatile financial environment. We first compare \textsc{FinMem} with
various algorithmic agents on a scalable real-world financial dataset,
underscoring its leading trading performance in stocks. We then fine-tuned the
agent's perceptual span and character setting to achieve a significantly
enhanced trading performance. Collectively, \textsc{FinMem} presents a
cutting-edge LLM agent framework for automated trading, boosting cumulative
investment returns.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14168">Fast Policy Learning for Linear Quadratic Control with Entropy Regularization. (arXiv:2311.14168v2 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Guo_X/0/1/0/all/0/1">Xin Guo</a>, <a href="http://arxiv.org/find/math/1/au:+Li_X/0/1/0/all/0/1">Xinyu Li</a>, <a href="http://arxiv.org/find/math/1/au:+Xu_R/0/1/0/all/0/1">Renyuan Xu</a></p>
<p>This paper proposes and analyzes two new policy learning methods: regularized
policy gradient (RPG) and iterative policy optimization (IPO), for a class of
discounted linear-quadratic control (LQC) problems over an infinite time
horizon with entropy regularization. Assuming access to the exact policy
evaluation, both proposed approaches are proven to converge linearly in finding
optimal policies of the regularized LQC. Moreover, the IPO method can achieve a
super-linear convergence rate once it enters a local region around the optimal
policy. Finally, when the optimal policy for an RL problem with a known
environment is appropriately transferred as the initial policy to an RL problem
with an unknown environment, the IPO method is shown to enable a super-linear
convergence rate if the two environments are sufficiently close. Performances
of these proposed algorithms are supported by numerical examples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14743">A Baseline Analysis of Reward Models&#x27; Ability To Accurately Analyze Foundation Models Under Distribution Shift. (arXiv:2311.14743v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pikus_B/0/1/0/all/0/1">Ben Pikus</a>, <a href="http://arxiv.org/find/cs/1/au:+LeVine_W/0/1/0/all/0/1">Will LeVine</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tony Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hendryx_S/0/1/0/all/0/1">Sean Hendryx</a></p>
<p>Foundation models, specifically Large Language Models (LLM's), have lately
gained wide-spread attention and adoption. Reinforcement Learning with Human
Feedback (RLHF) involves training a reward model to capture desired behaviors,
which is then used to align LLM's. These reward models are additionally used at
inference-time to estimate LLM responses' adherence to those desired behaviors.
However, there is little work measuring how robust these reward models are to
distribution shifts. In this work, we evaluate how reward model performance -
measured via accuracy and calibration (i.e. alignment between accuracy and
confidence) - is affected by distribution shift. We show novel calibration
patterns and accuracy drops due to OOD prompts and responses, and that the
reward model is more sensitive to shifts in responses than prompts.
Additionally, we adapt an OOD detection technique commonly used in
classification to the reward model setting to detect these distribution shifts
in prompts and responses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15112">Everybody Needs a Little HELP: Explaining Graphs via Hierarchical Concepts. (arXiv:2311.15112v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jurss_J/0/1/0/all/0/1">Jonas J&#xfc;r&#xdf;</a>, <a href="http://arxiv.org/find/cs/1/au:+Magister_L/0/1/0/all/0/1">Lucie Charlotte Magister</a>, <a href="http://arxiv.org/find/cs/1/au:+Barbiero_P/0/1/0/all/0/1">Pietro Barbiero</a>, <a href="http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1">Pietro Li&#xf2;</a>, <a href="http://arxiv.org/find/cs/1/au:+Simidjievski_N/0/1/0/all/0/1">Nikola Simidjievski</a></p>
<p>Graph neural networks (GNNs) have led to major breakthroughs in a variety of
domains such as drug discovery, social network analysis, and travel time
estimation. However, they lack interpretability which hinders human trust and
thereby deployment to settings with high-stakes decisions. A line of
interpretable methods approach this by discovering a small set of relevant
concepts as subgraphs in the last GNN layer that together explain the
prediction. This can yield oversimplified explanations, failing to explain the
interaction between GNN layers. To address this oversight, we provide HELP
(Hierarchical Explainable Latent Pooling), a novel, inherently interpretable
graph pooling approach that reveals how concepts from different GNN layers
compose to new ones in later steps. HELP is more than 1-WL expressive and is
the first non-spectral, end-to-end-learnable, hierarchical graph pooling method
that can learn to pool a variable number of arbitrary connected components. We
empirically demonstrate that it performs on-par with standard GCNs and popular
pooling methods in terms of accuracy while yielding explanations that are
aligned with expert knowledge in the domains of chemistry and social networks.
In addition to a qualitative analysis, we employ concept completeness scores as
well as concept conformity, a novel metric to measure the noise in discovered
concepts, quantitatively verifying that the discovered concepts are
significantly easier to fully understand than those from previous work. Our
work represents a first step towards an understanding of graph neural networks
that goes beyond a set of concepts from the final layer and instead explains
the complex interplay of concepts on different levels.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15500">Function-constrained Program Synthesis. (arXiv:2311.15500v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hajali_P/0/1/0/all/0/1">Patrick Hajali</a>, <a href="http://arxiv.org/find/cs/1/au:+Budvytis_I/0/1/0/all/0/1">Ignas Budvytis</a></p>
<p>This work introduces (1) a technique that allows large language models (LLMs)
to leverage user-provided code when solving programming tasks and (2) a method
to iteratively generate modular sub-functions that can aid future code
generation attempts when the initial code generated by the LLM is inadequate.
Generating computer programs in general-purpose programming languages like
Python poses a challenge for LLMs when instructed to use code provided in the
prompt. Code-specific LLMs (e.g., GitHub Copilot, CodeLlama2) can generate code
completions in real-time by drawing on all code available in a development
environment. However, restricting code-specific LLMs to use only in-context
code is not straightforward, as the model is not explicitly instructed to use
the user-provided code and users cannot highlight precisely which snippets of
code the model should incorporate into its context. Moreover, current systems
lack effective recovery methods, forcing users to iteratively re-prompt the
model with modified prompts until a sufficient solution is reached. Our method
differs from traditional LLM-powered code-generation by constraining
code-generation to an explicit function set and enabling recovery from failed
attempts through automatically generated sub-functions. When the LLM cannot
produce working code, we generate modular sub-functions to aid subsequent
attempts at generating functional code. A by-product of our method is a library
of reusable sub-functions that can solve related tasks, imitating a software
team where efficiency scales with experience. We also introduce a new
"half-shot" evaluation paradigm that provides tighter estimates of LLMs' coding
abilities compared to traditional zero-shot evaluation. Our proposed evaluation
method encourages models to output solutions in a structured format, decreasing
syntax errors that can be mistaken for poor coding ability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15565">Evaluating the Efficacy of Hybrid Deep Learning Models in Distinguishing AI-Generated Text. (arXiv:2311.15565v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oketunji_F/0/1/0/all/0/1">Finbarrs Oketunji</a></p>
<p>My research investigates the use of cutting-edge hybrid deep learning models
to accurately differentiate between AI-generated text and human writing. I
applied a robust methodology, utilising a carefully selected dataset comprising
AI and human texts from various sources, each tagged with instructions.
Advanced natural language processing techniques facilitated the analysis of
textual features. Combining sophisticated neural networks, the custom model
enabled it to detect nuanced differences between AI and human content.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16614">A Multivariate Unimodality Test Harnessing the Dip Statistic of Mahalanobis Distances Over Random Projections. (arXiv:2311.16614v3 [stat.ME] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Kolyvakis_P/0/1/0/all/0/1">Prodromos Kolyvakis</a>, <a href="http://arxiv.org/find/stat/1/au:+Likas_A/0/1/0/all/0/1">Aristidis Likas</a></p>
<p>Unimodality, pivotal in statistical analysis, offers insights into dataset
structures and drives sophisticated analytical procedures. While unimodality's
confirmation is straightforward for one-dimensional data using methods like
Silverman's approach and Hartigans' dip statistic, its generalization to higher
dimensions remains challenging. By extrapolating one-dimensional unimodality
principles to multi-dimensional spaces through linear random projections and
leveraging point-to-point distancing, our method, rooted in
$\alpha$-unimodality assumptions, presents a novel multivariate unimodality
test named mud-pod. Both theoretical and empirical studies confirm the efficacy
of our method in unimodality assessment of multidimensional datasets as well as
in estimating the number of clusters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16860">Data-efficient operator learning for solving high Mach number fluid flow problems. (arXiv:2311.16860v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ford_N/0/1/0/all/0/1">Noah Ford</a>, <a href="http://arxiv.org/find/cs/1/au:+Leon_V/0/1/0/all/0/1">Victor J. Leon</a>, <a href="http://arxiv.org/find/cs/1/au:+Mrema_H/0/1/0/all/0/1">Honest Mrema</a>, <a href="http://arxiv.org/find/cs/1/au:+Gilbert_J/0/1/0/all/0/1">Jeffrey Gilbert</a>, <a href="http://arxiv.org/find/cs/1/au:+New_A/0/1/0/all/0/1">Alexander New</a></p>
<p>We consider the problem of using SciML to predict solutions of high Mach
fluid flows over irregular geometries. In this setting, data is limited, and so
it is desirable for models to perform well in the low-data setting. We show
that Neural Basis Functions (NBF), which learns a basis of behavior modes from
the data and then uses this basis to make predictions, is more effective than a
basis-unaware baseline model. In addition, we identify continuing challenges in
the space of predicting solutions for this type of problem.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17065">Efficient Deep Speech Understanding at the Edge. (arXiv:2311.17065v2 [eess.AS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1">Rongxiang Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Lin_F/0/1/0/all/0/1">Felix Xiaozhu Lin</a></p>
<p>In contemporary speech understanding (SU), a sophisticated pipeline is
employed, encompassing the ingestion of streaming voice input. The pipeline
executes beam search iteratively, invoking a deep neural network to generate
tentative outputs (referred to as hypotheses) in an autoregressive manner.
Periodically, the pipeline assesses attention and Connectionist Temporal
Classification (CTC) scores.
</p>
<p>This paper aims to enhance SU performance on edge devices with limited
resources. Adopting a hybrid strategy, our approach focuses on accelerating
on-device execution and offloading inputs surpassing the device's capacity.
While this approach is established, we tackle SU's distinctive challenges
through innovative techniques: (1) Late Contextualization: This involves the
parallel execution of a model's attentive encoder during input ingestion. (2)
Pilot Inference: Addressing temporal load imbalances in the SU pipeline, this
technique aims to mitigate them effectively. (3) Autoregression Offramps:
Decisions regarding offloading are made solely based on hypotheses, presenting
a novel approach.
</p>
<p>These techniques are designed to seamlessly integrate with existing speech
models, pipelines, and frameworks, offering flexibility for independent or
combined application. Collectively, they form a hybrid solution for edge SU.
Our prototype, named XYZ, has undergone testing on Arm platforms featuring 6 to
8 cores, demonstrating state-of-the-art accuracy. Notably, it achieves a 2x
reduction in end-to-end latency and a corresponding 2x decrease in offloading
requirements.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17259">SoUnD Framework: Analyzing (So)cial Representation in (Un)structured (D)ata. (arXiv:2311.17259v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Diaz_M/0/1/0/all/0/1">Mark D&#xed;az</a>, <a href="http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1">Sunipa Dev</a>, <a href="http://arxiv.org/find/cs/1/au:+Reif_E/0/1/0/all/0/1">Emily Reif</a>, <a href="http://arxiv.org/find/cs/1/au:+Denton_E/0/1/0/all/0/1">Emily Denton</a>, <a href="http://arxiv.org/find/cs/1/au:+Prabhakaran_V/0/1/0/all/0/1">Vinodkumar Prabhakaran</a></p>
<p>The unstructured nature of data used in foundation model development is a
challenge to systematic analyses for making data use and documentation
decisions. From a Responsible AI perspective, these decisions often rely upon
understanding how people are represented in data. We propose a framework
designed to guide analysis of human representation in unstructured data and
identify downstream risks. We apply the framework in two toy examples using the
Common Crawl web text corpus (C4) and LAION-400M. We also propose a set of
hypothetical action steps in service of dataset use, development, and
documentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17431">Grounding Foundation Models through Federated Transfer Learning: A General Framework. (arXiv:2311.17431v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1">Yan Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_T/0/1/0/all/0/1">Tao Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_H/0/1/0/all/0/1">Hanlin Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1">Lixin Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1">Qiang Yang</a></p>
<p>Foundation Models (FMs) such as GPT-4 encoded with vast knowledge and
powerful emergent abilities have achieved remarkable success in various natural
language processing and computer vision tasks. Grounding FMs by adapting them
to domain-specific tasks or augmenting them with domain-specific knowledge
enables us to exploit the full potential of FMs. However, grounding FMs faces
several challenges, stemming primarily from constrained computing resources,
data privacy, model heterogeneity, and model ownership. Federated Transfer
Learning (FTL), the combination of federated learning and transfer learning,
provides promising solutions to address these challenges. In recent years, the
need for grounding FMs leveraging FTL, coined FTL-FM, has arisen strongly in
both academia and industry. Motivated by the strong growth in FTL-FM research
and the potential impact of FTL-FM on industrial applications, we propose an
FTL-FM framework that formulates problems of grounding FMs in the federated
learning setting, construct a detailed taxonomy based on the FTL-FM framework
to categorize state-of-the-art FTL-FM works, and comprehensively overview
FTL-FM works based on the proposed taxonomy. We also establish correspondences
between FTL-FM and conventional phases of adapting FM so that FM practitioners
can align their research works with FTL-FM. In addition, we overview advanced
efficiency-improving and privacy-preserving techniques because efficiency and
privacy are critical concerns in FTL-FM. Last, we discuss opportunities and
future research directions of FTL-FM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18061">TransNAS-TSAD: Harnessing Transformers for Multi-Objective Neural Architecture Search in Time Series Anomaly Detection. (arXiv:2311.18061v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Haq_I/0/1/0/all/0/1">Ijaz Ul Haq</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1">Byung Suk Lee</a></p>
<p>The surge in real-time data collection across various industries has
underscored the need for advanced anomaly detection in both univariate and
multivariate time series data. Traditional methods, while comprehensive, often
struggle to capture the complex interdependencies in such data. This paper
introduces TransNAS-TSAD, a novel framework that synergizes transformer
architecture with neural architecture search (NAS), enhanced through NSGA-II
algorithm optimization. This innovative approach effectively tackles the
complexities of both univariate and multivariate time series, balancing
computational efficiency with detection accuracy. Our evaluation reveals that
TransNAS-TSAD surpasses conventional anomaly detection models, demonstrating
marked improvements in diverse data scenarios. We also propose the
Efficiency-Accuracy-Complexity Score (EACS) as a new metric for assessing model
performance, emphasizing the crucial balance between accuracy and computational
resources. TransNAS-TSAD sets a new benchmark in time series anomaly detection,
offering a versatile, efficient solution for complex real-world applications.
This research paves the way for future developments in the field, highlighting
its potential in a wide range of industry applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18206">SCOPE-RL: A Python Library for Offline Reinforcement Learning and Off-Policy Evaluation. (arXiv:2311.18206v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kiyohara_H/0/1/0/all/0/1">Haruka Kiyohara</a>, <a href="http://arxiv.org/find/cs/1/au:+Kishimoto_R/0/1/0/all/0/1">Ren Kishimoto</a>, <a href="http://arxiv.org/find/cs/1/au:+Kawakami_K/0/1/0/all/0/1">Kosuke Kawakami</a>, <a href="http://arxiv.org/find/cs/1/au:+Kobayashi_K/0/1/0/all/0/1">Ken Kobayashi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nakata_K/0/1/0/all/0/1">Kazuhide Nakata</a>, <a href="http://arxiv.org/find/cs/1/au:+Saito_Y/0/1/0/all/0/1">Yuta Saito</a></p>
<p>This paper introduces SCOPE-RL, a comprehensive open-source Python software
designed for offline reinforcement learning (offline RL), off-policy evaluation
(OPE), and selection (OPS). Unlike most existing libraries that focus solely on
either policy learning or evaluation, SCOPE-RL seamlessly integrates these two
key aspects, facilitating flexible and complete implementations of both offline
RL and OPE processes. SCOPE-RL put particular emphasis on its OPE modules,
offering a range of OPE estimators and robust evaluation-of-OPE protocols. This
approach enables more in-depth and reliable OPE compared to other packages. For
instance, SCOPE-RL enhances OPE by estimating the entire reward distribution
under a policy rather than its mere point-wise expected value. Additionally,
SCOPE-RL provides a more thorough evaluation-of-OPE by presenting the
risk-return tradeoff in OPE results, extending beyond mere accuracy evaluations
in existing OPE literature. SCOPE-RL is designed with user accessibility in
mind. Its user-friendly APIs, comprehensive documentation, and a variety of
easy-to-follow examples assist researchers and practitioners in efficiently
implementing and experimenting with various offline RL methods and OPE
estimators, tailored to their specific problem contexts. The documentation of
SCOPE-RL is available at https://scope-rl.readthedocs.io/en/latest/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18207">Towards Assessing and Benchmarking Risk-Return Tradeoff of Off-Policy Evaluation. (arXiv:2311.18207v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kiyohara_H/0/1/0/all/0/1">Haruka Kiyohara</a>, <a href="http://arxiv.org/find/cs/1/au:+Kishimoto_R/0/1/0/all/0/1">Ren Kishimoto</a>, <a href="http://arxiv.org/find/cs/1/au:+Kawakami_K/0/1/0/all/0/1">Kosuke Kawakami</a>, <a href="http://arxiv.org/find/cs/1/au:+Kobayashi_K/0/1/0/all/0/1">Ken Kobayashi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nakata_K/0/1/0/all/0/1">Kazuhide Nakata</a>, <a href="http://arxiv.org/find/cs/1/au:+Saito_Y/0/1/0/all/0/1">Yuta Saito</a></p>
<p>Off-Policy Evaluation (OPE) aims to assess the effectiveness of
counterfactual policies using only offline logged data and is often used to
identify the top-k promising policies for deployment in online A/B tests.
Existing evaluation metrics for OPE estimators primarily focus on the
"accuracy" of OPE or that of downstream policy selection, neglecting
risk-return tradeoff in the subsequent online policy deployment. To address
this issue, we draw inspiration from portfolio evaluation in finance and
develop a new metric, called SharpeRatio@k, which measures the risk-return
tradeoff of policy portfolios formed by an OPE estimator under varying online
evaluation budgets (k). We validate our metric in two example scenarios,
demonstrating its ability to effectively distinguish between low-risk and
high-risk estimators and to accurately identify the most efficient estimator.
This efficient estimator is characterized by its capability to form the most
advantageous policy portfolios, maximizing returns while minimizing risks
during online deployment, a nuance that existing metrics typically overlook. To
facilitate a quick, accurate, and consistent evaluation of OPE via
SharpeRatio@k, we have also integrated this metric into an open-source
software, SCOPE-RL. Employing SharpeRatio@k and SCOPE-RL, we conduct
comprehensive benchmarking experiments on various estimators and RL tasks,
focusing on their risk-return tradeoff. These experiments offer several
interesting directions and suggestions for future OPE research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18426">Convergence Analysis of Fractional Gradient Descent. (arXiv:2311.18426v2 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Aggarwal_A/0/1/0/all/0/1">Ashwani Aggarwal</a></p>
<p>Fractional derivatives are a well-studied generalization of integer order
derivatives. Naturally, for optimization, it is of interest to understand the
convergence properties of gradient descent using fractional derivatives.
Convergence analysis of fractional gradient descent is currently limited both
in the methods analyzed and the settings analyzed. This paper aims to fill in
these gaps by analyzing variations of fractional gradient descent in smooth and
convex, smooth and strongly convex, and smooth and non-convex settings. First,
novel bounds will be established bridging fractional and integer derivatives.
Then, these bounds will be applied to the aforementioned settings to prove
$O(1/T)$ convergence for smooth and convex functions and linear convergence for
smooth and strongly convex functions. Additionally, we prove $O(1/T)$
convergence for smooth and non-convex functions using an extended notion of
smoothness that is more natural for fractional derivatives. Finally, empirical
results will be presented on the potential speed up of fractional gradient
descent over standard gradient descent as well as the challenges of predicting
which will be faster in general.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18743">AlignBench: Benchmarking Chinese Alignment of Large Language Models. (arXiv:2311.18743v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_X/0/1/0/all/0/1">Xuanyu Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shengyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yue Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1">Zhuoer Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1">Bosi Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1">Jiale Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ke_P/0/1/0/all/0/1">Pei Ke</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yifan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tam_W/0/1/0/all/0/1">Weng Lam Tam</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaohan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Lichao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hongning Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1">Minlie Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1">Yuxiao Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jie Tang</a></p>
<p>Alignment has become a critical step for instruction-tuned Large Language
Models (LLMs) to become helpful assistants. However, effective evaluation of
alignment for emerging Chinese LLMs is still significantly lacking, calling for
real-scenario grounded, open-ended, challenging and automatic evaluations
tailored for alignment. To fill in this gap, we introduce AlignBench, a
comprehensive multi-dimensional benchmark for evaluating LLMs' alignment in
Chinese. Equipped with a human-in-the-loop data curation pipeline, our
benchmark employs a rule-calibrated multi-dimensional LLM-as-Judge with
Chain-of-Thought to generate explanations and final ratings as evaluations,
ensuring high reliability and interpretability. Furthermore, we report
AlignBench evaluated by CritiqueLLM, a dedicated Chinese evaluator LLM that
recovers 95% of GPT-4's evaluation ability. We will provide public APIs for
evaluating AlignBench with CritiqueLLM to facilitate the evaluation of LLMs'
Chinese alignment. All evaluation codes, data, and LLM generations are
available at \url{https://github.com/THUDM/AlignBench}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18803">BioCLIP: A Vision Foundation Model for the Tree of Life. (arXiv:2311.18803v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stevens_S/0/1/0/all/0/1">Samuel Stevens</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jiaman Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Thompson_M/0/1/0/all/0/1">Matthew J Thompson</a>, <a href="http://arxiv.org/find/cs/1/au:+Campolongo_E/0/1/0/all/0/1">Elizabeth G Campolongo</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1">Chan Hee Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Carlyn_D/0/1/0/all/0/1">David Edward Carlyn</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1">Li Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Dahdul_W/0/1/0/all/0/1">Wasila M Dahdul</a>, <a href="http://arxiv.org/find/cs/1/au:+Stewart_C/0/1/0/all/0/1">Charles Stewart</a>, <a href="http://arxiv.org/find/cs/1/au:+Berger_Wolf_T/0/1/0/all/0/1">Tanya Berger-Wolf</a>, <a href="http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1">Wei-Lun Chao</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1">Yu Su</a></p>
<p>Images of the natural world, collected by a variety of cameras, from drones
to individual phones, are increasingly abundant sources of biological
information. There is an explosion of computational methods and tools,
particularly computer vision, for extracting biologically relevant information
from images for science and conservation. Yet most of these are bespoke
approaches designed for a specific task and are not easily adaptable or
extendable to new questions, contexts, and datasets. A vision model for general
organismal biology questions on images is of timely need. To approach this, we
curate and release TreeOfLife-10M, the largest and most diverse ML-ready
dataset of biology images. We then develop BioCLIP, a foundation model for the
tree of life, leveraging the unique properties of biology captured by
TreeOfLife-10M, namely the abundance and variety of images of plants, animals,
and fungi, together with the availability of rich structured biological
knowledge. We rigorously benchmark our approach on diverse fine-grained biology
classification tasks, and find that BioCLIP consistently and substantially
outperforms existing baselines (by 17% to 20% absolute). Intrinsic evaluation
reveals that BioCLIP has learned a hierarchical representation conforming to
the tree of life, shedding light on its strong generalizability. Our code,
models and data will be made available at
https://github.com/Imageomics/bioclip.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00102">FedEmb: A Vertical and Hybrid Federated Learning Algorithm using Network And Feature Embedding Aggregation. (arXiv:2312.00102v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1">Fanfei Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lele Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuxin Wang</a></p>
<p>Federated learning (FL) is an emerging paradigm for decentralized training of
machine learning models on distributed clients, without revealing the data to
the central server. The learning scheme may be horizontal, vertical or hybrid
(both vertical and horizontal). Most existing research work with deep neural
network (DNN) modelling is focused on horizontal data distributions, while
vertical and hybrid schemes are much less studied. In this paper, we propose a
generalized algorithm FedEmb, for modelling vertical and hybrid DNN-based
learning. The idea of our algorithm is characterised by higher inference
accuracy, stronger privacy-preserving properties, and lower client-server
communication bandwidth demands as compared with existing work. The
experimental results show that FedEmb is an effective method to tackle both
split feature &amp; subject space decentralized problems, shows 0.3% to 4.2%
inference accuracy improvement with limited privacy revealing for datasets
stored in local clients, and reduces 88.9 % time complexity over vertical
baseline method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00656">Simple Transferability Estimation for Regression Tasks. (arXiv:2312.00656v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1">Cuong N. Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_P/0/1/0/all/0/1">Phong Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Ho_L/0/1/0/all/0/1">Lam Si Tung Ho</a>, <a href="http://arxiv.org/find/cs/1/au:+Dinh_V/0/1/0/all/0/1">Vu Dinh</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_A/0/1/0/all/0/1">Anh T. Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Hassner_T/0/1/0/all/0/1">Tal Hassner</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1">Cuong V. Nguyen</a></p>
<p>We consider transferability estimation, the problem of estimating how well
deep learning models transfer from a source to a target task. We focus on
regression tasks, which received little previous attention, and propose two
simple and computationally efficient approaches that estimate transferability
based on the negative regularized mean squared error of a linear regression
model. We prove novel theoretical results connecting our approaches to the
actual transferability of the optimal target models obtained from the transfer
learning process. Despite their simplicity, our approaches significantly
outperform existing state-of-the-art regression transferability estimators in
both accuracy and efficiency. On two large-scale keypoint regression
benchmarks, our approaches yield 12% to 36% better results on average while
being at least 27% faster than previous state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00761">Deep Unlearning: Fast and Efficient Training-free Approach to Controlled Forgetting. (arXiv:2312.00761v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kodge_S/0/1/0/all/0/1">Sangamesh Kodge</a>, <a href="http://arxiv.org/find/cs/1/au:+Saha_G/0/1/0/all/0/1">Gobinda Saha</a>, <a href="http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1">Kaushik Roy</a></p>
<p>Machine unlearning has emerged as a prominent and challenging area of
interest, driven in large part by the rising regulatory demands for industries
to delete user data upon request and the heightened awareness of privacy.
Existing approaches either retrain models from scratch or use several
finetuning steps for every deletion request, often constrained by computational
resource limitations and restricted access to the original training data. In
this work, we introduce a novel class unlearning algorithm designed to
strategically eliminate an entire class or a group of classes from the learned
model. To that end, our algorithm first estimates the Retain Space and the
Forget Space, representing the feature or activation spaces for samples from
classes to be retained and unlearned, respectively. To obtain these spaces, we
propose a novel singular value decomposition-based technique that requires
layer wise collection of network activations from a few forward passes through
the network. We then compute the shared information between these spaces and
remove it from the forget space to isolate class-discriminatory feature space
for unlearning. Finally, we project the model weights in the orthogonal
direction of the class-discriminatory space to obtain the unlearned model. We
demonstrate our algorithm's efficacy on ImageNet using a Vision Transformer
with only $\sim$1.5% drop in retain accuracy compared to the original model
while maintaining under 1% accuracy on the unlearned class samples. Further,
our algorithm consistently performs well when subject to Membership Inference
Attacks showing 7.8% improvement on average across a variety of image
classification datasets and network architectures, as compared to other
baselines while being $\sim$6x more computationally efficient.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/1812.05227">Deep Learning Framework for Wireless Systems: Applications to Optical Wireless Communications. (arXiv:1812.05227v1 [cs.IT] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hoon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Sang Hyun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Quek_T/0/1/0/all/0/1">Tony Q. S. Quek</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1">Inkyu Lee</a></p>
<p>Optical wireless communication (OWC) is a promising technology for future
wireless communications owing to its potentials for cost-effective network
deployment and high data rate. There are several implementation issues in the
OWC which have not been encountered in radio frequency wireless communications.
First, practical OWC transmitters need an illumination control on color,
intensity, and luminance, etc., which poses complicated modulation design
challenges. Furthermore, signal-dependent properties of optical channels raise
non-trivial challenges both in modulation and demodulation of the optical
signals. To tackle such difficulties, deep learning (DL) technologies can be
applied for optical wireless transceiver design. This article addresses recent
efforts on DL-based OWC system designs. A DL framework for emerging image
sensor communication is proposed and its feasibility is verified by simulation.
Finally, technical challenges and implementation issues for the DL-based
optical wireless technology are discussed.
</p>
</p>
</div>

    </div>
    </body>
    