<!DOCTYPE html>
<html>
<head>
<title>2023-12-13-cs-cv</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2312.05258">Automated Small Kidney Cancer Detection in Non-Contrast Computed Tomography. (arXiv:2312.05258v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+McGough_W/0/1/0/all/0/1">William McGough</a>, <a href="http://arxiv.org/find/eess/1/au:+Buddenkotte_T/0/1/0/all/0/1">Thomas Buddenkotte</a>, <a href="http://arxiv.org/find/eess/1/au:+Ursprung_S/0/1/0/all/0/1">Stephan Ursprung</a>, <a href="http://arxiv.org/find/eess/1/au:+Gao_Z/0/1/0/all/0/1">Zeyu Gao</a>, <a href="http://arxiv.org/find/eess/1/au:+Stewart_G/0/1/0/all/0/1">Grant Stewart</a>, <a href="http://arxiv.org/find/eess/1/au:+Crispin_Ortuzar_M/0/1/0/all/0/1">Mireia Crispin-Ortuzar</a></p>
<p>This study introduces an automated pipeline for renal cancer (RC) detection
in non-contrast computed tomography (NCCT). In the development of our pipeline,
we test three detections models: a shape model, a 2D-, and a 3D axial-sample
model. Training (n=1348) and testing (n=64) data were gathered from open
sources (KiTS23, Abdomen1k, CT-ORG) and Cambridge University Hospital (CUH).
Results from cross-validation and testing revealed that the 2D axial sample
model had the highest small ($\leq$40mm diameter) RC detection area under the
curve (AUC) of 0.804. Our pipeline achieves 61.9\% sensitivity and 92.7\%
specificity for small kidney cancers on unseen test data. Our results are much
more accurate than previous attempts to automatically detect small renal
cancers in NCCT, the most likely imaging modality for RC screening. This
pipeline offers a promising advance that may enable screening for kidney
cancers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05265">Multimodal Group Emotion Recognition In-the-wild Using Privacy-Compliant Features. (arXiv:2312.05265v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Augusma_A/0/1/0/all/0/1">Anderson Augusma</a> (M-PSI, SVH), <a href="http://arxiv.org/find/cs/1/au:+Vaufreydaz_D/0/1/0/all/0/1">Dominique Vaufreydaz</a> (M-PSI), <a href="http://arxiv.org/find/cs/1/au:+Letue_F/0/1/0/all/0/1">Fr&#xe9;d&#xe9;rique Letu&#xe9;</a> (SVH)</p>
<p>This paper explores privacy-compliant group-level emotion recognition
''in-the-wild'' within the EmotiW Challenge 2023. Group-level emotion
recognition can be useful in many fields including social robotics,
conversational agents, e-coaching and learning analytics. This research imposes
itself using only global features avoiding individual ones, i.e. all features
that can be used to identify or track people in videos (facial landmarks, body
poses, audio diarization, etc.). The proposed multimodal model is composed of a
video and an audio branches with a cross-attention between modalities. The
video branch is based on a fine-tuned ViT architecture. The audio branch
extracts Mel-spectrograms and feed them through CNN blocks into a transformer
encoder. Our training paradigm includes a generated synthetic dataset to
increase the sensitivity of our model on facial expression within the image in
a data-driven way. The extensive experiments show the significance of our
methodology. Our privacy-compliant proposal performs fairly on the EmotiW
challenge, with 79.24% and 75.13% of accuracy respectively on validation and
test set for the best models. Noticeably, our findings highlight that it is
possible to reach this accuracy level with privacy-compliant features using
only 5 frames uniformly distributed on the video.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05269">LifelongMemory: Leveraging LLMs for Answering Queries in Egocentric Videos. (arXiv:2312.05269v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Ying Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yanlai Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1">Mengye Ren</a></p>
<p>The egocentric video natural language query (NLQ) task involves localizing a
temporal window in an egocentric video that provides an answer to a posed
query, which has wide applications in building personalized AI assistants.
Prior methods for this task have focused on improvements of network
architecture and leveraging pre-training for enhanced image and video features,
but have struggled with capturing long-range temporal dependencies in lengthy
videos, and cumbersome end-to-end training. Motivated by recent advancements in
Large Language Models (LLMs) and vision language models, we introduce
LifelongMemory, a novel framework that utilizes multiple pre-trained models to
answer queries from extensive egocentric video content. We address the unique
challenge by employing a pre-trained captioning model to create detailed
narratives of the videos. These narratives are then used to prompt a frozen LLM
to generate coarse-grained temporal window predictions, which are subsequently
refined using a pre-trained NLQ model. Empirical results demonstrate that our
method achieves competitive performance against existing supervised end-to-end
learning methods, underlining the potential of integrating multiple pre-trained
multimodal large language models in complex vision-language tasks. We provide a
comprehensive analysis of key design decisions and hyperparameters in our
pipeline, offering insights and practical guidelines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05270">Image and AIS Data Fusion Technique for Maritime Computer Vision Applications. (arXiv:2312.05270v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gulsoylu_E/0/1/0/all/0/1">Emre G&#xfc;lsoylu</a>, <a href="http://arxiv.org/find/cs/1/au:+Koch_P/0/1/0/all/0/1">Paul Koch</a>, <a href="http://arxiv.org/find/cs/1/au:+Yildiz_M/0/1/0/all/0/1">Mert Y&#x131;ld&#x131;z</a>, <a href="http://arxiv.org/find/cs/1/au:+Constapel_M/0/1/0/all/0/1">Manfred Constapel</a>, <a href="http://arxiv.org/find/cs/1/au:+Kelm_A/0/1/0/all/0/1">Andr&#xe9; Peter Kelm</a></p>
<p>Deep learning object detection methods, like YOLOv5, are effective in
identifying maritime vessels but often lack detailed information important for
practical applications. In this paper, we addressed this problem by developing
a technique that fuses Automatic Identification System (AIS) data with vessels
detected in images to create datasets. This fusion enriches ship images with
vessel-related data, such as type, size, speed, and direction. Our approach
associates detected ships to their corresponding AIS messages by estimating
distance and azimuth using a homography-based method suitable for both fixed
and periodically panning cameras. This technique is useful for creating
datasets for waterway traffic management, encounter detection, and
surveillance. We introduce a novel dataset comprising of images taken in
various weather conditions and their corresponding AIS messages. This dataset
offers a stable baseline for refining vessel detection algorithms and
trajectory prediction models. To assess our method's performance, we manually
annotated a portion of this dataset. The results are showing an overall
association accuracy of 74.76 %, with the association accuracy for fixed
cameras reaching 85.06 %. This demonstrates the potential of our approach in
creating datasets for vessel detection, pose estimation and auto-labelling
pipelines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05272">StableQ: Enhancing Data-Scarce Quantization with Text-to-Image Data. (arXiv:2312.05272v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuhang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Youngeun Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1">Donghyun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Panda_P/0/1/0/all/0/1">Priyadarshini Panda</a></p>
<p>Though low-bit quantization enables efficient storage and inference of deep
neural networks, it often requires the use of training data to maintain
resilience against quantization errors. However, training data are frequently
subject to privacy or copyright concerns. In this work, we address the
challenge of Data-Scarce Quantization, where access to training data is
severely limited or non-existent for quantization purposes. Conventional
approaches typically rely on inverting dummy images or jointly training
generative models to produce synthetic input samples. However, these methods
struggle to accurately recreate complex objects in large-scale datasets like
ImageNet. To overcome these limitations, we introduce StableQ, a novel method
that utilizes an advanced text-to-image diffusion model to generate
high-resolution, photo-realistic synthetic data. To verify the quality of the
generated data, we implement two robust filtering mechanisms. These mechanisms
are designed to select images that closely resemble the intrinsic
characteristics of the actual training data. Furthermore, in scenarios where
limited training data are available, we use these data to guide the synthetic
data generation process by inverting a learnable token embedding in the text
encoder. Our extensive experimental results demonstrate that StbaleQ sets a new
benchmark in both zero-shot and few-shot quantization, outperforming existing
methods in terms of accuracy and efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05274">Target to Source: Guidance-Based Diffusion Model for Test-Time Adaptation. (arXiv:2312.05274v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1">Kaiyu Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Lai_H/0/1/0/all/0/1">Hanjiang Lai</a></p>
<p>Most recent works of test-time adaptation (TTA) aim to alleviate domain shift
problems by re-training source classifiers in each domain. On the other hand,
the emergence of the diffusion model provides another solution to TTA, which
directly maps the test data from the target domain to the source domain based
on a diffusion model pre-trained in the source domain. The source classifier
does not need to be fine-tuned. However, 1) the semantic information loss from
test data to the source domain and 2) the model shift between the source
classifier and diffusion model would prevent the diffusion model from mapping
the test data back to the source domain correctly. In this paper, we propose a
novel guidance-based diffusion-driven adaptation (GDDA) to overcome the data
shift and let the diffusion model find a better way to go back to the source.
Concretely, we first propose detail and global guidance to better keep the
common semantics of the test and source data. The two guidance include a
contrastive loss and mean squared error to alleviate the information loss by
fully exploring the diffusion model and the test data. Meanwhile, we propose a
classifier-aware guidance to reduce the bias caused by the model shift, which
can incorporate the source classifier's information into the generation process
of the diffusion model. Extensive experiments on three image datasets with
three classifier backbones demonstrate that GDDA significantly performs better
than the state-of-the-art baselines. On CIFAR-10C, CIFAR-100C, and ImageNetC,
GDDA achieves 11.54\%, 19.05\%, and 11.63\% average accuracy improvements,
respectively. GDDA even achieves equal performance compared with methods of
re-training classifiers. The code is available in the supplementary material.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05277">3D Copy-Paste: Physically Plausible Object Insertion for Monocular 3D Detection. (arXiv:2312.05277v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1">Yunhao Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Hong-Xing Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1">Cheng Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yuliang Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xinyu Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1">Liu Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Itti_L/0/1/0/all/0/1">Laurent Itti</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jiajun Wu</a></p>
<p>A major challenge in monocular 3D object detection is the limited diversity
and quantity of objects in real datasets. While augmenting real scenes with
virtual objects holds promise to improve both the diversity and quantity of the
objects, it remains elusive due to the lack of an effective 3D object insertion
method in complex real captured scenes. In this work, we study augmenting
complex real indoor scenes with virtual objects for monocular 3D object
detection. The main challenge is to automatically identify plausible physical
properties for virtual assets (e.g., locations, appearances, sizes, etc.) in
cluttered real scenes. To address this challenge, we propose a physically
plausible indoor 3D object insertion approach to automatically copy virtual
objects and paste them into real scenes. The resulting objects in scenes have
3D bounding boxes with plausible physical locations and appearances. In
particular, our method first identifies physically feasible locations and poses
for the inserted objects to prevent collisions with the existing room layout.
Subsequently, it estimates spatially-varying illumination for the insertion
location, enabling the immersive blending of the virtual objects into the
original scene with plausible appearances and cast shadows. We show that our
augmentation method significantly improves existing monocular 3D object models
and achieves state-of-the-art performance. For the first time, we demonstrate
that a physically plausible 3D object insertion, serving as a generative data
augmentation technique, can lead to significant improvements for discriminative
downstream tasks such as monocular 3D object detection. Project website:
https://gyhandy.github.io/3D-Copy-Paste/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05279">Quantitative perfusion maps using a novelty spatiotemporal convolutional neural network. (arXiv:2312.05279v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Cao_A/0/1/0/all/0/1">Anbo Cao</a>, <a href="http://arxiv.org/find/eess/1/au:+Le_P/0/1/0/all/0/1">Pin-Yu Le</a>, <a href="http://arxiv.org/find/eess/1/au:+Qie_Z/0/1/0/all/0/1">Zhonghui Qie</a>, <a href="http://arxiv.org/find/eess/1/au:+Hassan_H/0/1/0/all/0/1">Haseeb Hassan</a>, <a href="http://arxiv.org/find/eess/1/au:+Guo_Y/0/1/0/all/0/1">Yingwei Guo</a>, <a href="http://arxiv.org/find/eess/1/au:+Zaman_A/0/1/0/all/0/1">Asim Zaman</a>, <a href="http://arxiv.org/find/eess/1/au:+Lu_J/0/1/0/all/0/1">Jiaxi Lu</a>, <a href="http://arxiv.org/find/eess/1/au:+Zeng_X/0/1/0/all/0/1">Xueqiang Zeng</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1">Huihui Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Miao_X/0/1/0/all/0/1">Xiaoqiang Miao</a>, <a href="http://arxiv.org/find/eess/1/au:+Han_T/0/1/0/all/0/1">Taiyu Han</a>, <a href="http://arxiv.org/find/eess/1/au:+Huang_G/0/1/0/all/0/1">Guangtao Huang</a>, <a href="http://arxiv.org/find/eess/1/au:+Kang_Y/0/1/0/all/0/1">Yan Kang</a>, <a href="http://arxiv.org/find/eess/1/au:+Luo_Y/0/1/0/all/0/1">Yu Luo</a>, <a href="http://arxiv.org/find/eess/1/au:+Guo_J/0/1/0/all/0/1">Jia Guo</a></p>
<p>Dynamic susceptibility contrast magnetic resonance imaging (DSC-MRI) is
widely used to evaluate acute ischemic stroke to distinguish salvageable tissue
and infarct core. For this purpose, traditional methods employ deconvolution
techniques, like singular value decomposition, which are known to be vulnerable
to noise, potentially distorting the derived perfusion parameters. However,
deep learning technology could leverage it, which can accurately estimate
clinical perfusion parameters compared to traditional clinical approaches.
Therefore, this study presents a perfusion parameters estimation network that
considers spatial and temporal information, the Spatiotemporal Network
(ST-Net), for the first time. The proposed network comprises a designed
physical loss function to enhance model performance further. The results
indicate that the network can accurately estimate perfusion parameters,
including cerebral blood volume (CBV), cerebral blood flow (CBF), and time to
maximum of the residual function (Tmax). The structural similarity index (SSIM)
mean values for CBV, CBF, and Tmax parameters were 0.952, 0.943, and 0.863,
respectively. The DICE score for the hypo-perfused region reached 0.859,
demonstrating high consistency. The proposed model also maintains time
efficiency, closely approaching the performance of commercial gold-standard
software.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05281">X2-Softmax: Margin Adaptive Loss Function for Face Recognition. (arXiv:2312.05281v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jiamu Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiaoxiang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xinyuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Si_Y/0/1/0/all/0/1">Yain-Whar Si</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaofan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1">Zheng Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Ke Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1">Xueyuan Gong</a></p>
<p>Learning the discriminative features of different faces is an important task
in face recognition. By extracting face features in neural networks, it becomes
easy to measure the similarity of different face images, which makes face
recognition possible. To enhance the neural network's face feature
separability, incorporating an angular margin during training is common
practice. State-of-the-art loss functions CosFace and ArcFace apply fixed
margins between weights of classes to enhance the inter-class separation of
face features. Since the distribution of samples in the training set is
imbalanced, similarities between different identities are unequal. Therefore,
using an inappropriately fixed angular margin may lead to the problem that the
model is difficult to converge or the face features are not discriminative
enough. It is more in line with our intuition that the margins are angular
adaptive, which could increase with the angles between classes growing. In this
paper, we propose a new angular margin loss named X2-Softmax. X2-Softmax loss
has adaptive angular margins, which provide the margin that increases with the
angle between different classes growing. The angular adaptive margin ensures
model flexibility and effectively improves the effect of face recognition. We
have trained the neural network with X2-Softmax loss on the MS1Mv3 dataset and
tested it on several evaluation benchmarks to demonstrate the effectiveness and
superiority of our loss function. The experimental code and trained model are
published in https://github.com/xujiamu123/X2-Softmax/tree/main.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05282">Towards On-device Learning on the Edge: Ways to Select Neurons to Update under a Budget Constraint. (arXiv:2312.05282v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Quelennec_A/0/1/0/all/0/1">A&#xeb;l Qu&#xe9;lennec</a>, <a href="http://arxiv.org/find/cs/1/au:+Tartaglione_E/0/1/0/all/0/1">Enzo Tartaglione</a>, <a href="http://arxiv.org/find/cs/1/au:+Mozharovskyi_P/0/1/0/all/0/1">Pavlo Mozharovskyi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1">Van-Tam Nguyen</a></p>
<p>In the realm of efficient on-device learning under extreme memory and
computation constraints, a significant gap in successful approaches persists.
Although considerable effort has been devoted to efficient inference, the main
obstacle to efficient learning is the prohibitive cost of backpropagation. The
resources required to compute gradients and update network parameters often
exceed the limits of tightly constrained memory budgets. This paper challenges
conventional wisdom and proposes a series of experiments that reveal the
existence of superior sub-networks. Furthermore, we hint at the potential for
substantial gains through a dynamic neuron selection strategy when fine-tuning
a target task. Our efforts extend to the adaptation of a recent dynamic neuron
selection strategy pioneered by Bragagnolo et al. (NEq), revealing its
effectiveness in the most stringent scenarios. Our experiments demonstrate, in
the average case, the superiority of a NEq-inspired approach over a random
selection. This observation prompts a compelling avenue for further exploration
in the area, highlighting the opportunity to design a new class of algorithms
designed to facilitate parameter update selection. Our findings usher in a new
era of possibilities in the field of on-device learning under extreme
constraints and encourage the pursuit of innovative strategies for efficient,
resource-friendly model fine-tuning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05283">Nuvo: Neural UV Mapping for Unruly 3D Representations. (arXiv:2312.05283v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Srinivasan_P/0/1/0/all/0/1">Pratul P. Srinivasan</a>, <a href="http://arxiv.org/find/cs/1/au:+Garbin_S/0/1/0/all/0/1">Stephan J. Garbin</a>, <a href="http://arxiv.org/find/cs/1/au:+Verbin_D/0/1/0/all/0/1">Dor Verbin</a>, <a href="http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1">Jonathan T. Barron</a>, <a href="http://arxiv.org/find/cs/1/au:+Mildenhall_B/0/1/0/all/0/1">Ben Mildenhall</a></p>
<p>Existing UV mapping algorithms are designed to operate on well-behaved
meshes, instead of the geometry representations produced by state-of-the-art 3D
reconstruction and generation techniques. As such, applying these methods to
the volume densities recovered by neural radiance fields and related techniques
(or meshes triangulated from such fields) results in texture atlases that are
too fragmented to be useful for tasks such as view synthesis or appearance
editing. We present a UV mapping method designed to operate on geometry
produced by 3D reconstruction and generation techniques. Instead of computing a
mapping defined on a mesh's vertices, our method Nuvo uses a neural field to
represent a continuous UV mapping, and optimizes it to be a valid and
well-behaved mapping for just the set of visible points, i.e. only points that
affect the scene's appearance. We show that our model is robust to the
challenges posed by ill-behaved geometry, and that it produces editable UV
mappings that can represent detailed appearance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05284">0.1% Data Makes Segment Anything Slim. (arXiv:2312.05284v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zigeng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_G/0/1/0/all/0/1">Gongfan Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xinyin Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinchao Wang</a></p>
<p>The formidable model size and demanding computational requirements of Segment
Anything Model (SAM) have rendered it cumbersome for deployment on
resource-constrained devices. Existing approaches for SAM compression typically
involve training a new network from scratch, posing a challenging trade-off
between compression costs and model performance. To address this issue, this
paper introduces SlimSAM, a novel SAM compression method that achieves superior
performance with remarkably low training costs. This is achieved by the
efficient reuse of pre-trained SAMs through a unified pruning-distillation
framework. To enhance knowledge inheritance from the original SAM, we employ an
innovative alternate slimming strategy that partitions the compression process
into a progressive procedure. Diverging from prior pruning techniques, we
meticulously prune and distill decoupled model structures in an alternating
fashion. Furthermore, a novel label-free pruning criterion is also proposed to
align the pruning objective with the optimization target, thereby boosting the
post-distillation after pruning. SlimSAM yields significant performance
improvements while demanding over 10 times less training costs than any other
existing methods. Even when compared to the original SAM-H, SlimSAM achieves
approaching performance while reducing parameter counts to merely 0.9% (5.7M),
MACs to 0.8% (21G), and requiring only 0.1% (10k) of the SAM training data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05286">Bridging Synthetic and Real Worlds for Pre-training Scene Text Detectors. (arXiv:2312.05286v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guan_T/0/1/0/all/0/1">Tongkun Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1">Wei Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xue Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xuehui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xiaokang Yang</a></p>
<p>Existing scene text detection methods typically rely on extensive real data
for training. Due to the lack of annotated real images, recent works have
attempted to exploit large-scale labeled synthetic data (LSD) for pre-training
text detectors. However, a synth-to-real domain gap emerges, further limiting
the performance of text detectors. Differently, in this work, we propose
\textbf{FreeReal}, a real-domain-aligned pre-training paradigm that enables the
complementary strengths of both LSD and unlabeled real data (URD).
Specifically, to bridge real and synthetic worlds for pre-training, a novel
glyph-based mixing mechanism (GlyphMix) is tailored for text images. GlyphMix
delineates the character structures of synthetic images and embeds them as
graffiti-like units onto real images. Without introducing real domain drift,
GlyphMix freely yields real-world images with annotations derived from
synthetic labels. Furthermore, when given free fine-grained synthetic labels,
GlyphMix can effectively bridge the linguistic domain gap stemming from
English-dominated LSD to URD in various languages. Without bells and whistles,
FreeReal achieves average gains of 4.56\%, 3.85\%, 3.90\%, and 1.97\% in
improving the performance of DBNet, PANet, PSENet, and FCENet methods,
respectively, consistently outperforming previous pre-training methods by a
substantial margin across four public datasets. Code will be released soon.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05287">Human in-the-Loop Estimation of Cluster Count in Datasets via Similarity-Driven Nested Importance Sampling. (arXiv:2312.05287v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Perez_G/0/1/0/all/0/1">Gustavo Perez</a>, <a href="http://arxiv.org/find/cs/1/au:+Sheldon_D/0/1/0/all/0/1">Daniel Sheldon</a>, <a href="http://arxiv.org/find/cs/1/au:+Horn_G/0/1/0/all/0/1">Grant Van Horn</a>, <a href="http://arxiv.org/find/cs/1/au:+Maji_S/0/1/0/all/0/1">Subhransu Maji</a></p>
<p>Identifying the number of clusters serves as a preliminary goal for many data
analysis tasks. A common approach to this problem is to vary the number of
clusters in a clustering algorithm (e.g., 'k' in $k$-means) and pick the value
that best explains the data. However, the count estimates can be unreliable
especially when the image similarity is poor. Human feedback on the pairwise
similarity can be used to improve the clustering, but existing approaches do
not guarantee accurate count estimates. We propose an approach to produce
estimates of the cluster counts in a large dataset given an approximate
pairwise similarity. Our framework samples edges guided by the pairwise
similarity, and we collect human feedback to construct a statistical estimate
of the cluster count. On the technical front we have developed a nested
importance sampling approach that yields (asymptotically) unbiased estimates of
the cluster count with confidence intervals which can guide human effort.
Compared to naive sampling, our similarity-driven sampling produces more
accurate estimates of counts and tighter confidence intervals. We evaluate our
method on a benchmark of six fine-grained image classification datasets
achieving low error rates on the estimated number of clusters with
significantly less human labeling effort compared to baselines and alternative
active clustering approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05288">MotionCrafter: One-Shot Motion Customization of Diffusion Models. (arXiv:2312.05288v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuxin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_F/0/1/0/all/0/1">Fan Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_N/0/1/0/all/0/1">Nisha Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Haibin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1">Chongyang Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1">Weiming Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Changsheng Xu</a></p>
<p>The essence of a video lies in its dynamic motions, including character
actions, object movements, and camera movements. While text-to-video generative
diffusion models have recently advanced in creating diverse contents,
controlling specific motions through text prompts remains a significant
challenge. A primary issue is the coupling of appearance and motion, often
leading to overfitting on appearance. To tackle this challenge, we introduce
MotionCrafter, a novel one-shot instance-guided motion customization method.
MotionCrafter employs a parallel spatial-temporal architecture that injects the
reference motion into the temporal component of the base model, while the
spatial module is independently adjusted for character or style control. To
enhance the disentanglement of motion and appearance, we propose an innovative
dual-branch motion disentanglement approach, comprising a motion
disentanglement loss and an appearance prior enhancement strategy. During
training, a frozen base model provides appearance normalization, effectively
separating appearance from motion and thereby preserving diversity.
Comprehensive quantitative and qualitative experiments, along with user
preference tests, demonstrate that MotionCrafter can successfully integrate
dynamic motions while preserving the coherence and quality of the base model
with a wide range of appearance generation capabilities. Codes are available at
https://github.com/zyxElsa/MotionCrafter.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05290">Noise Adaptor in Spiking Neural Networks. (arXiv:2312.05290v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajendran_B/0/1/0/all/0/1">Bipin Rajendran</a></p>
<p>Recent strides in low-latency spiking neural network (SNN) algorithms have
drawn significant interest, particularly due to their event-driven computing
nature and fast inference capability. One of the most efficient ways to
construct a low-latency SNN is by converting a pre-trained, low-bit artificial
neural network (ANN) into an SNN. However, this conversion process faces two
main challenges: First, converting SNNs from low-bit ANNs can lead to
``occasional noise" -- the phenomenon where occasional spikes are generated in
spiking neurons where they should not be -- during inference, which
significantly lowers SNN accuracy. Second, although low-latency SNNs initially
show fast improvements in accuracy with time steps, these accuracy growths soon
plateau, resulting in their peak accuracy lagging behind both full-precision
ANNs and traditional ``long-latency SNNs'' that prioritize precision over
speed.
</p>
<p>In response to these two challenges, this paper introduces a novel technique
named ``noise adaptor.'' Noise adaptor can model occasional noise during
training and implicitly optimize SNN accuracy, particularly at high simulation
times $T$. Our research utilizes the ResNet model for a comprehensive analysis
of the impact of the noise adaptor on low-latency SNNs. The results demonstrate
that our method outperforms the previously reported quant-ANN-to-SNN conversion
technique. We achieved an accuracy of 95.95\% within 4 time steps on CIFAR-10
using ResNet-18, and an accuracy of 74.37\% within 64 time steps on ImageNet
using ResNet-50. Remarkably, these results were obtained without resorting to
any noise correction methods during SNN inference, such as negative spikes or
two-stage SNN simulations. Our approach significantly boosts the peak accuracy
of low-latency SNNs, bringing them on par with the accuracy of full-precision
ANNs. Code will be open source.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05291">GlitchBench: Can large multimodal models detect video game glitches?. (arXiv:2312.05291v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Taesiri_M/0/1/0/all/0/1">Mohammad Reza Taesiri</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_T/0/1/0/all/0/1">Tianjun Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Bezemer_C/0/1/0/all/0/1">Cor-Paul Bezemer</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1">Anh Nguyen</a></p>
<p>Large multimodal models (LMMs) have evolved from large language models (LLMs)
to integrate multiple input modalities, such as visual inputs. This integration
augments the capacity of LLMs for tasks requiring visual comprehension and
reasoning. However, the extent and limitations of their enhanced abilities are
not fully understood, especially when it comes to real-world tasks. To address
this gap, we introduce GlitchBench, a novel benchmark derived from video game
quality assurance tasks, to test and evaluate the reasoning capabilities of
LMMs. Our benchmark is curated from a variety of unusual and glitched scenarios
from video games and aims to challenge both the visual and linguistic reasoning
powers of LMMs in detecting and interpreting out-of-the-ordinary events. We
evaluate multiple state-of-the-art LMMs, and we show that GlitchBench presents
a new challenge for these models. Code and data are available at:
https://glitchbench.github.io/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05295">Disentangled Clothed Avatar Generation from Text Descriptions. (arXiv:2312.05295v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jionghao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1">Zhiyang Dou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Zhengming Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1">Yongqing Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenping Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1">Rong Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1">Li Song</a></p>
<p>In this paper, we introduced a novel text-to-avatar generation method that
separately generates the human body and the clothes and allows high-quality
animation on the generated avatar. While recent advancements in text-to-avatar
generation have yielded diverse human avatars from text prompts, these methods
typically combine all elements-clothes, hair, and body-into a single 3D
representation. Such an entangled approach poses challenges for downstream
tasks like editing or animation. To overcome these limitations, we propose a
novel disentangled 3D avatar representation named Sequentially Offset-SMPL
(SO-SMPL), building upon the SMPL model. SO-SMPL represents the human body and
clothes with two separate meshes, but associates them with offsets to ensure
the physical alignment between the body and the clothes. Then, we design an
Score Distillation Sampling(SDS)-based distillation framework to generate the
proposed SO-SMPL representation from text prompts. In comparison with existing
text-to-avatar methods, our approach not only achieves higher exture and
geometry quality and better semantic alignment with text prompts, but also
significantly improves the visual quality of character animation, virtual
try-on, and avatar editing. Our project page is at
https://shanemankiw.github.io/SO-SMPL/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05311">360{\deg} Volumetric Portrait Avatar. (arXiv:2312.05311v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nehvi_J/0/1/0/all/0/1">Jalees Nehvi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kabadayi_B/0/1/0/all/0/1">Berna Kabadayi</a>, <a href="http://arxiv.org/find/cs/1/au:+Valentin_J/0/1/0/all/0/1">Julien Valentin</a>, <a href="http://arxiv.org/find/cs/1/au:+Thies_J/0/1/0/all/0/1">Justus Thies</a></p>
<p>We propose 360{\deg} Volumetric Portrait (3VP) Avatar, a novel method for
reconstructing 360{\deg} photo-realistic portrait avatars of human subjects
solely based on monocular video inputs. State-of-the-art monocular avatar
reconstruction methods rely on stable facial performance capturing. However,
the common usage of 3DMM-based facial tracking has its limits; side-views can
hardly be captured and it fails, especially, for back-views, as required inputs
like facial landmarks or human parsing masks are missing. This results in
incomplete avatar reconstructions that only cover the frontal hemisphere. In
contrast to this, we propose a template-based tracking of the torso, head and
facial expressions which allows us to cover the appearance of a human subject
from all sides. Thus, given a sequence of a subject that is rotating in front
of a single camera, we train a neural volumetric representation based on neural
radiance fields. A key challenge to construct this representation is the
modeling of appearance changes, especially, in the mouth region (i.e., lips and
teeth). We, therefore, propose a deformation-field-based blend basis which
allows us to interpolate between different appearance states. We evaluate our
approach on captured real-world data and compare against state-of-the-art
monocular reconstruction methods. In contrast to those, our method is the first
monocular technique that reconstructs an entire 360{\deg} avatar.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05327">Data-Centric Machine Learning for Geospatial Remote Sensing Data. (arXiv:2312.05327v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Roscher_R/0/1/0/all/0/1">Ribana Roscher</a>, <a href="http://arxiv.org/find/cs/1/au:+Russwurm_M/0/1/0/all/0/1">Marc Ru&#xdf;wurm</a>, <a href="http://arxiv.org/find/cs/1/au:+Gevaert_C/0/1/0/all/0/1">Caroline Gevaert</a>, <a href="http://arxiv.org/find/cs/1/au:+Kampffmeyer_M/0/1/0/all/0/1">Michael Kampffmeyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1">Jefersson A. dos Santos</a>, <a href="http://arxiv.org/find/cs/1/au:+Vakalopoulou_M/0/1/0/all/0/1">Maria Vakalopoulou</a>, <a href="http://arxiv.org/find/cs/1/au:+Hansch_R/0/1/0/all/0/1">Ronny H&#xe4;nsch</a>, <a href="http://arxiv.org/find/cs/1/au:+Hansen_S/0/1/0/all/0/1">Stine Hansen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nogueira_K/0/1/0/all/0/1">Keiller Nogueira</a>, <a href="http://arxiv.org/find/cs/1/au:+Prexl_J/0/1/0/all/0/1">Jonathan Prexl</a>, <a href="http://arxiv.org/find/cs/1/au:+Tuia_D/0/1/0/all/0/1">Devis Tuia</a></p>
<p>Recent developments and research in modern machine learning have led to
substantial improvements in the geospatial field. Although numerous deep
learning models have been proposed, the majority of them have been developed on
benchmark datasets that lack strong real-world relevance. Furthermore, the
performance of many methods has already saturated on these datasets. We argue
that shifting the focus towards a complementary data-centric perspective is
necessary to achieve further improvements in accuracy, generalization ability,
and real impact in end-user applications. This work presents a definition and
precise categorization of automated data-centric learning approaches for
geospatial data. It highlights the complementary role of data-centric learning
with respect to model-centric in the larger machine learning deployment cycle.
We review papers across the entire geospatial field and categorize them into
different groups. A set of representative experiments shows concrete
implementation examples. These examples provide concrete steps to act on
geospatial data with data-centric machine learning approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05330">Multi-view Inversion for 3D-aware Generative Adversarial Networks. (arXiv:2312.05330v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Barthel_F/0/1/0/all/0/1">Florian Barthel</a>, <a href="http://arxiv.org/find/cs/1/au:+Hilsmann_A/0/1/0/all/0/1">Anna Hilsmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Eisert_P/0/1/0/all/0/1">Peter Eisert</a></p>
<p>Current 3D GAN inversion methods for human heads typically use only one
single frontal image to reconstruct the whole 3D head model. This leaves out
meaningful information when multi-view data or dynamic videos are available.
Our method builds on existing state-of-the-art 3D GAN inversion techniques to
allow for consistent and simultaneous inversion of multiple views of the same
subject. We employ a multi-latent extension to handle inconsistencies present
in dynamic face videos to re-synthesize consistent 3D representations from the
sequence. As our method uses additional information about the target subject,
we observe significant enhancements in both geometric accuracy and image
quality, particularly when rendering from wide viewing angles. Moreover, we
demonstrate the editability of our inverted 3D renderings, which distinguishes
them from NeRF-based scene reconstructions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05334">ProsDectNet: Bridging the Gap in Prostate Cancer Detection via Transrectal B-mode Ultrasound Imaging. (arXiv:2312.05334v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Vesal_S/0/1/0/all/0/1">Sulaiman Vesal</a>, <a href="http://arxiv.org/find/eess/1/au:+Bhattacharya_I/0/1/0/all/0/1">Indrani Bhattacharya</a>, <a href="http://arxiv.org/find/eess/1/au:+Jahanandish_H/0/1/0/all/0/1">Hassan Jahanandish</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1">Xinran Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Kornberg_Z/0/1/0/all/0/1">Zachary Kornberg</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1">Steve Ran Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Sommer_E/0/1/0/all/0/1">Elijah Richard Sommer</a>, <a href="http://arxiv.org/find/eess/1/au:+Choi_M/0/1/0/all/0/1">Moon Hyung Choi</a>, <a href="http://arxiv.org/find/eess/1/au:+Fan_R/0/1/0/all/0/1">Richard E. Fan</a>, <a href="http://arxiv.org/find/eess/1/au:+Sonn_G/0/1/0/all/0/1">Geoffrey A. Sonn</a>, <a href="http://arxiv.org/find/eess/1/au:+Rusu_M/0/1/0/all/0/1">Mirabela Rusu</a></p>
<p>Interpreting traditional B-mode ultrasound images can be challenging due to
image artifacts (e.g., shadowing, speckle), leading to low sensitivity and
limited diagnostic accuracy. While Magnetic Resonance Imaging (MRI) has been
proposed as a solution, it is expensive and not widely available. Furthermore,
most biopsies are guided by Transrectal Ultrasound (TRUS) alone and can miss up
to 52% cancers, highlighting the need for improved targeting. To address this
issue, we propose ProsDectNet, a multi-task deep learning approach that
localizes prostate cancer on B-mode ultrasound. Our model is pre-trained using
radiologist-labeled data and fine-tuned using biopsy-confirmed labels.
ProsDectNet includes a lesion detection and patch classification head, with
uncertainty minimization using entropy to improve model performance and reduce
false positive predictions. We trained and validated ProsDectNet using a cohort
of 289 patients who underwent MRI-TRUS fusion targeted biopsy. We then tested
our approach on a group of 41 patients and found that ProsDectNet outperformed
the average expert clinician in detecting prostate cancer on B-mode ultrasound
images, achieving a patient-level ROC-AUC of 82%, a sensitivity of 74%, and a
specificity of 67%. Our results demonstrate that ProsDectNet has the potential
to be used as a computer-aided diagnosis system to improve targeted biopsy and
treatment planning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05348">High-Quality Live Video Streaming via Transcoding Time Prediction and Preset Selection. (arXiv:2312.05348v1 [cs.MM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shahre_Babak_Z/0/1/0/all/0/1">Zahra Nabizadeh Shahre-Babak</a>, <a href="http://arxiv.org/find/cs/1/au:+Karimi_N/0/1/0/all/0/1">Nader Karimi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rapaka_K/0/1/0/all/0/1">Krishna Rapaka</a>, <a href="http://arxiv.org/find/cs/1/au:+Amara_T/0/1/0/all/0/1">Tarek Amara</a>, <a href="http://arxiv.org/find/cs/1/au:+Samavi_S/0/1/0/all/0/1">Shadrokh Samavi</a>, <a href="http://arxiv.org/find/cs/1/au:+Shirani_S/0/1/0/all/0/1">Shahram Shirani</a></p>
<p>Video streaming often requires transcoding content into different resolutions
and bitrates to match the recipient's internet speed and screen capabilities.
Video encoders like x264 offer various presets, each with different tradeoffs
between transcoding time and rate-distortion performance. Choosing the best
preset for video transcoding is difficult, especially for live streaming, as
trying all the presets and choosing the best one is not feasible. One solution
is to predict each preset's transcoding time and select the preset that ensures
the highest quality while adhering to live streaming time constraints.
Prediction of video transcoding time is also critical in minimizing streaming
delays, deploying resource management algorithms, and load balancing. We
propose a learning-based framework for predicting the transcoding time of
videos across various presets. Our predictor's features for video transcoding
time prediction are derived directly from the ingested stream, primarily from
the header or metadata. As a result, only minimal additional delay is incurred
for feature extraction, rendering our approach ideal for live-streaming
applications. We evaluated our learning-based transcoding time prediction using
a dataset of videos. The results demonstrate that our framework can accurately
predict the transcoding time for different presets, with a mean absolute
percentage error (MAPE) of nearly 5.0%. Leveraging these predictions, we then
select the most suitable transcoding preset for live video streaming. Utilizing
our transcoding time prediction-based preset selection improved Peak
Signal-to-Noise Ratio (PSNR) of up to 5 dB.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05349">PixLore: A Dataset-driven Approach to Rich Image Captioning. (arXiv:2312.05349v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bonilla_D/0/1/0/all/0/1">Diego Bonilla</a></p>
<p>In the domain of vision-language integration, generating detailed image
captions poses a significant challenge due to the lack of a curated and rich
dataset. This study introduces PixLore, a novel method that leverages Querying
Transformers through the fine-tuning of the BLIP-2 model using the LoRa method
on a standard commercial GPU. Our approach, which involves training on a
carefully assembled dataset from state-of-the-art Computer Vision models
combined and augmented by ChatGPT, addresses the question of whether intricate
image understanding can be achieved with an ensemble of smaller-scale models.
Comparative evaluations against major models such as GPT-4 and Google Bard
demonstrate that PixLore-2.7B, despite having considerably fewer parameters, is
rated higher than the existing State-of-the-Art models in over half of the
assessments. This research not only presents a groundbreaking approach but also
highlights the importance of well-curated datasets in enhancing the performance
of smaller models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05352">A Review of Machine Learning Methods Applied to Video Analysis Systems. (arXiv:2312.05352v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pattichis_M/0/1/0/all/0/1">Marios S. Pattichis</a>, <a href="http://arxiv.org/find/cs/1/au:+Jatla_V/0/1/0/all/0/1">Venkatesh Jatla</a>, <a href="http://arxiv.org/find/cs/1/au:+Cerna_A/0/1/0/all/0/1">Alvaro E. Ullao Cerna</a></p>
<p>The paper provides a survey of the development of machine-learning techniques
for video analysis. The survey provides a summary of the most popular deep
learning methods used for human activity recognition. We discuss how popular
architectures perform on standard datasets and highlight the differences from
real-life datasets dominated by multiple activities performed by multiple
participants over long periods. For real-life datasets, we describe the use of
low-parameter models (with 200X or 1,000X fewer parameters) that are trained to
detect a single activity after the relevant objects have been successfully
detected. Our survey then turns to a summary of machine learning methods that
are specifically developed for working with a small number of labeled video
samples. Our goal here is to describe modern techniques that are specifically
designed so as to minimize the amount of ground truth that is needed for
training and testing video analysis systems. We provide summaries of the
development of self-supervised learning, semi-supervised learning, active
learning, and zero-shot learning for applications in video analysis. For each
method, we provide representative examples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05355">Neither hype nor gloom do DNNs justice. (arXiv:2312.05355v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wichmann_F/0/1/0/all/0/1">Felix A. Wichmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Kornblith_S/0/1/0/all/0/1">Simon Kornblith</a>, <a href="http://arxiv.org/find/cs/1/au:+Geirhos_R/0/1/0/all/0/1">Robert Geirhos</a></p>
<p>Neither the hype exemplified in some exaggerated claims about deep neural
networks (DNNs), nor the gloom expressed by Bowers et al. do DNNs as models in
vision science justice: DNNs rapidly evolve, and today's limitations are often
tomorrow's successes. In addition, providing explanations as well as prediction
and image-computability are model desiderata; one should not be favoured at the
expense of the other.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05357">Filtering Pixel Latent Variables for Unmixing Volumetric Images. (arXiv:2312.05357v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Bouchard_C/0/1/0/all/0/1">Catherine Bouchard</a>, <a href="http://arxiv.org/find/eess/1/au:+Boulanger_V/0/1/0/all/0/1">Vincent Boulanger</a>, <a href="http://arxiv.org/find/eess/1/au:+Lavoie_Cardinal_F/0/1/0/all/0/1">Flavie Lavoie-Cardinal</a>, <a href="http://arxiv.org/find/eess/1/au:+Gagne_C/0/1/0/all/0/1">Christian Gagn&#xe9;</a></p>
<p>Measurements of different overlapping components require robust unmixing
algorithms to convert the raw multi-dimensional measurements to useful unmixed
images. Such algorithms perform reliable separation of the components when the
raw signal is fully resolved and contains enough information to fit curves on
the raw distributions. In experimental physics, measurements are often noisy,
undersampled, or unresolved spatially or spectrally. We propose a novel method
where bandpass filters are applied to the latent space of a multi-dimensional
convolutional neural network to separate the overlapping signal components and
extract each of their relative contributions. Simultaneously processing all
dimensions with multi-dimensional convolution kernels empowers the network to
combine the information from adjacent pixels and time- or spectral-bins,
facilitating component separation in instances where individual pixels lack
well-resolved information. We demonstrate the applicability of the method to
real experimental physics problems using fluorescence lifetime microscopy and
mode decomposition in optical fibers as test cases. The successful application
of our approach to these two distinct experimental cases, characterized by
different measured distributions, highlights the versatility of our approach in
addressing a wide array of imaging tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05390">NoiseCLR: A Contrastive Learning Approach for Unsupervised Discovery of Interpretable Directions in Diffusion Models. (arXiv:2312.05390v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dalva_Y/0/1/0/all/0/1">Yusuf Dalva</a>, <a href="http://arxiv.org/find/cs/1/au:+Yanardag_P/0/1/0/all/0/1">Pinar Yanardag</a></p>
<p>Generative models have been very popular in the recent years for their image
generation capabilities. GAN-based models are highly regarded for their
disentangled latent space, which is a key feature contributing to their success
in controlled image editing. On the other hand, diffusion models have emerged
as powerful tools for generating high-quality images. However, the latent space
of diffusion models is not as thoroughly explored or understood. Existing
methods that aim to explore the latent space of diffusion models usually relies
on text prompts to pinpoint specific semantics. However, this approach may be
restrictive in areas such as art, fashion, or specialized fields like medicine,
where suitable text prompts might not be available or easy to conceive thus
limiting the scope of existing work. In this paper, we propose an unsupervised
method to discover latent semantics in text-to-image diffusion models without
relying on text prompts. Our method takes a small set of unlabeled images from
specific domains, such as faces or cats, and a pre-trained diffusion model, and
discovers diverse semantics in unsupervised fashion using a contrastive
learning objective. Moreover, the learned directions can be applied
simultaneously, either within the same domain (such as various types of facial
edits) or across different domains (such as applying cat and face edits within
the same image) without interfering with each other. Our extensive experiments
show that our method achieves highly disentangled edits, outperforming existing
approaches in both diffusion-based and GAN-based latent space editing methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05391">Loss Functions in the Era of Semantic Segmentation: A Survey and Outlook. (arXiv:2312.05391v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Azad_R/0/1/0/all/0/1">Reza Azad</a>, <a href="http://arxiv.org/find/cs/1/au:+Heidary_M/0/1/0/all/0/1">Moein Heidary</a>, <a href="http://arxiv.org/find/cs/1/au:+Yilmaz_K/0/1/0/all/0/1">Kadir Yilmaz</a>, <a href="http://arxiv.org/find/cs/1/au:+Huttemann_M/0/1/0/all/0/1">Michael H&#xfc;ttemann</a>, <a href="http://arxiv.org/find/cs/1/au:+Karimijafarbigloo_S/0/1/0/all/0/1">Sanaz Karimijafarbigloo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yuli Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmeink_A/0/1/0/all/0/1">Anke Schmeink</a>, <a href="http://arxiv.org/find/cs/1/au:+Merhof_D/0/1/0/all/0/1">Dorit Merhof</a></p>
<p>Semantic image segmentation, the process of classifying each pixel in an
image into a particular class, plays an important role in many visual
understanding systems. As the predominant criterion for evaluating the
performance of statistical models, loss functions are crucial for shaping the
development of deep learning-based segmentation algorithms and improving their
overall performance. To aid researchers in identifying the optimal loss
function for their particular application, this survey provides a comprehensive
and unified review of $25$ loss functions utilized in image segmentation. We
provide a novel taxonomy and thorough review of how these loss functions are
customized and leveraged in image segmentation, with a systematic
categorization emphasizing their significant features and applications.
Furthermore, to evaluate the efficacy of these methods in real-world scenarios,
we propose unbiased evaluations of some distinct and renowned loss functions on
established medical and natural image datasets. We conclude this review by
identifying current challenges and unveiling future research opportunities.
Finally, we have compiled the reviewed studies that have open-source
implementations on our GitHub page.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05407">Active Learning Guided Federated Online Adaptation: Applications in Medical Image Segmentation. (arXiv:2312.05407v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1">Md Shazid Islam</a>, <a href="http://arxiv.org/find/cs/1/au:+Nag_S/0/1/0/all/0/1">Sayak Nag</a>, <a href="http://arxiv.org/find/cs/1/au:+Dutta_A/0/1/0/all/0/1">Arindam Dutta</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_M/0/1/0/all/0/1">Miraj Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Niloy_F/0/1/0/all/0/1">Fahim Faisal Niloy</a>, <a href="http://arxiv.org/find/cs/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1">Amit K.Roy-Chowdhury</a></p>
<p>Data privacy, storage, and distribution shifts are major bottlenecks in
medical image analysis. Data cannot be shared across patients, physicians, and
facilities due to privacy concerns, usually requiring each patient's data to be
analyzed in a discreet setting at a near real-time pace. However, one would
like to take advantage of the accumulated knowledge across healthcare
facilities as the computational systems analyze data of more and more patients
while incorporating feedback provided by physicians to improve accuracy.
Motivated by these, we propose a method for medical image segmentation that
adapts to each incoming data batch (online adaptation), incorporates physician
feedback through active learning, and assimilates knowledge across facilities
in a federated setup. Combining an online adaptation scheme at test time with
an efficient sampling strategy with budgeted annotation helps bridge the gap
between the source and the incoming stream of target domain data. A federated
setup allows collaborative aggregation of knowledge across distinct distributed
models without needing to share the data across different models. This
facilitates the improvement of performance over time by accumulating knowledge
across users. Towards achieving these goals, we propose a computationally
amicable, privacy-preserving image segmentation technique \textbf{DrFRODA} that
uses federated learning to adapt the model in an online manner with feedback
from doctors in the loop. Our experiments on publicly available datasets show
that the proposed distributed active learning-based online adaptation method
outperforms unsupervised online adaptation methods and shows competitive
results with offline active learning-based adaptation methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05412">CMMD: Contrastive Multi-Modal Diffusion for Video-Audio Conditional Modeling. (arXiv:2312.05412v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1">Ruihan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gamper_H/0/1/0/all/0/1">Hannes Gamper</a>, <a href="http://arxiv.org/find/cs/1/au:+Braun_S/0/1/0/all/0/1">Sebastian Braun</a></p>
<p>We introduce a multi-modal diffusion model tailored for the bi-directional
conditional generation of video and audio. Recognizing the importance of
accurate alignment between video and audio events in multi-modal generation
tasks, we propose a joint contrastive training loss to enhance the
synchronization between visual and auditory occurrences. Our research
methodology involves conducting comprehensive experiments on multiple datasets
to thoroughly evaluate the efficacy of our proposed model. The assessment of
generation quality and alignment performance is carried out from various
angles, encompassing both objective and subjective metrics. Our findings
demonstrate that the proposed model outperforms the baseline, substantiating
its effectiveness and efficiency. Notably, the incorporation of the contrastive
loss results in improvements in audio-visual alignment, particularly in the
high-correlation video-to-audio generation task. These results indicate the
potential of our proposed model as a robust solution for improving the quality
and alignment of multi-modal generation, thereby contributing to the
advancement of video and audio conditional generation systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05418">Bauer&#x27;s Spectral Factorization Method for Low Order Multiwavelet Filter Design. (arXiv:2312.05418v1 [math.NA])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Kolev_V/0/1/0/all/0/1">Vasil Kolev</a>, <a href="http://arxiv.org/find/math/1/au:+Cooklev_T/0/1/0/all/0/1">Todor Cooklev</a>, <a href="http://arxiv.org/find/math/1/au:+Keinert_F/0/1/0/all/0/1">Fritz Keinert</a></p>
<p>Para-Hermitian polynomial matrices obtained by matrix spectral factorization
lead to functions useful in control theory systems, basis functions in
numerical methods or multiscaling functions used in signal processing. We
introduce a fast algorithm for matrix spectral factorization based on Bauer$'$s
method. We convert Bauer$'$ method into a nonlinear matrix equation (NME). The
NME is solved by two different numerical algorithms (Fixed Point Iteration and
Newton$'$s Method) which produce approximate scalar or matrix factors, as well
as a symbolic algorithm which produces exact factors in closed form for some
low-order scalar or matrix polynomial matrices, respectively. Convergence rates
of the two numerical algorithms are investigated for a number of singular and
nonsingular scalar and matrix polynomials taken from different areas. In
particular, one of the singular examples leads to new orthogonal multiscaling
and multiwavelet filters. Since the NME can also be solved as a Generalized
Discrete Time Algebraic Riccati Equation (GDARE), numerical results using
built-in routines in Maple 17.0 and 6 Matlab versions are presented.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05430">FT2TF: First-Person Statement Text-To-Talking Face Generation. (arXiv:2312.05430v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Diao_X/0/1/0/all/0/1">Xingjian Diao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1">Ming Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Barrios_W/0/1/0/all/0/1">Wayner Barrios</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1">SouYoung Jin</a></p>
<p>Talking face generation has gained immense popularity in the computer vision
community, with various applications including AR/VR, teleconferencing, digital
assistants, and avatars. Traditional methods are mainly audio-driven ones which
have to deal with the inevitable resource-intensive nature of audio storage and
processing. To address such a challenge, we propose FT2TF - First-Person
Statement Text-To-Talking Face Generation, a novel one-stage end-to-end
pipeline for talking face generation driven by first-person statement text.
Moreover, FT2TF implements accurate manipulation of the facial expressions by
altering the corresponding input text. Different from previous work, our model
only leverages visual and textual information without any other sources (e.g.
audio/landmark/pose) during inference. Extensive experiments are conducted on
LRS2 and LRS3 datasets, and results on multi-dimensional evaluation metrics are
reported. Both quantitative and qualitative results showcase that FT2TF
outperforms existing relevant methods and reaches the state-of-the-art. This
achievement highlights our model capability to bridge first-person statements
and dynamic face generation, providing insightful guidance for future work.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05431">Efficient Quantization Strategies for Latent Diffusion Models. (arXiv:2312.05431v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yuewei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1">Xiaoliang Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jialiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Peizhao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongbo Zhang</a></p>
<p>Latent Diffusion Models (LDMs) capture the dynamic evolution of latent
variables over time, blending patterns and multimodality in a generative
system. Despite the proficiency of LDM in various applications, such as
text-to-image generation, facilitated by robust text encoders and a variational
autoencoder, the critical need to deploy large generative models on edge
devices compels a search for more compact yet effective alternatives. Post
Training Quantization (PTQ), a method to compress the operational size of deep
learning models, encounters challenges when applied to LDM due to temporal and
structural complexities. This study proposes a quantization strategy that
efficiently quantize LDMs, leveraging Signal-to-Quantization-Noise Ratio (SQNR)
as a pivotal metric for evaluation. By treating the quantization discrepancy as
relative noise and identifying sensitive part(s) of a model, we propose an
efficient quantization approach encompassing both global and local strategies.
The global quantization process mitigates relative quantization noise by
initiating higher-precision quantization on sensitive blocks, while local
treatments address specific challenges in quantization-sensitive and
time-sensitive modules. The outcomes of our experiments reveal that the
implementation of both global and local treatments yields a highly efficient
and effective Post Training Quantization (PTQ) of LDMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05447">From Static to Dynamic: Adapting Landmark-Aware Image Models for Facial Expression Recognition in Videos. (arXiv:2312.05447v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jia Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1">Shiguang Shan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Meng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_R/0/1/0/all/0/1">Richang Hong</a></p>
<p>Dynamic facial expression recognition (DFER) in the wild is still hindered by
data limitations, e.g., insufficient quantity and diversity of pose, occlusion
and illumination, as well as the inherent ambiguity of facial expressions. In
contrast, static facial expression recognition (SFER) currently shows much
higher performance and can benefit from more abundant high-quality training
data. Moreover, the appearance features and dynamic dependencies of DFER remain
largely unexplored. To tackle these challenges, we introduce a novel
Static-to-Dynamic model (S2D) that leverages existing SFER knowledge and
dynamic information implicitly encoded in extracted facial landmark-aware
features, thereby significantly improving DFER performance. Firstly, we build
and train an image model for SFER, which incorporates a standard Vision
Transformer (ViT) and Multi-View Complementary Prompters (MCPs) only. Then, we
obtain our video model (i.e., S2D), for DFER, by inserting Temporal-Modeling
Adapters (TMAs) into the image model. MCPs enhance facial expression features
with landmark-aware features inferred by an off-the-shelf facial landmark
detector. And the TMAs capture and model the relationships of dynamic changes
in facial expressions, effectively extending the pre-trained image model for
videos. Notably, MCPs and TMAs only increase a fraction of trainable parameters
(less than +10\%) to the original image model. Moreover, we present a novel
Emotion-Anchors (i.e., reference samples for each emotion category) based
Self-Distillation Loss to reduce the detrimental influence of ambiguous emotion
labels, further enhancing our S2D. Experiments conducted on popular SFER and
DFER datasets show that we achieve the state of the art.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05449">TALDS-Net: Task-Aware Adaptive Local Descriptors Selection for Few-shot Image Classification. (arXiv:2312.05449v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qiao_Q/0/1/0/all/0/1">Qian Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1">Yu Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1">Ziyin Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1">Fanzhang Li</a></p>
<p>Few-shot image classification aims to classify images from unseen novel
classes with few samples. Recent works demonstrate that deep local descriptors
exhibit enhanced representational capabilities compared to image-level
features. However, most existing methods solely rely on either employing all
local descriptors or directly utilizing partial descriptors, potentially
resulting in the loss of crucial information. Moreover, these methods primarily
emphasize the selection of query descriptors while overlooking support
descriptors. In this paper, we propose a novel Task-Aware Adaptive Local
Descriptors Selection Network (TALDS-Net), which exhibits the capacity for
adaptive selection of task-aware support descriptors and query descriptors.
Specifically, we compare the similarity of each local support descriptor with
other local support descriptors to obtain the optimal support descriptor subset
and then compare the query descriptors with the optimal support subset to
obtain discriminative query descriptors. Extensive experiments demonstrate that
our TALDS-Net outperforms state-of-the-art methods on both general and
fine-grained datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05454">Model Evaluation for Domain Identification of Unknown Classes in Open-World Recognition: A Proposal. (arXiv:2312.05454v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alfarisy_G/0/1/0/all/0/1">Gusti Ahmad Fanshuri Alfarisy</a>, <a href="http://arxiv.org/find/cs/1/au:+Malik_O/0/1/0/all/0/1">Owais Ahmed Malik</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_O/0/1/0/all/0/1">Ong Wee Hong</a></p>
<p>Open-World Recognition (OWR) is an emerging field that makes a machine
learning model competent in rejecting the unknowns, managing them, and
incrementally adding novel samples to the base knowledge. However, this broad
objective is not practical for an agent that works on a specific task. Not all
rejected samples will be used for learning continually in the future. Some
novel images in the open environment may not belong to the domain of interest.
Hence, identifying the unknown in the domain of interest is essential for a
machine learning model to learn merely the important samples. In this study, we
propose an evaluation protocol for estimating a model's capability in
separating unknown in-domain (ID) and unknown out-of-domain (OOD). We evaluated
using three approaches with an unknown domain and demonstrated the possibility
of identifying the domain of interest using the pre-trained parameters through
traditional transfer learning, Automated Machine Learning (AutoML), and Nearest
Class Mean (NCM) classifier with First Integer Neighbor Clustering Hierarchy
(FINCH). We experimented with five different domains: garbage, food, dogs,
plants, and birds. The results show that all approaches can be used as an
initial baseline yielding a good accuracy. In addition, a Balanced Accuracy
(BACCU) score from a pre-trained model indicates a tendency to excel in one or
more domains of interest. We observed that MobileNetV3 yielded the highest
BACCU score for the garbage domain and surpassed complex models such as the
transformer network. Meanwhile, our results also suggest that a strong
representation in the pre-trained model is important for identifying unknown
classes in the same domain. This study could open the bridge toward open-world
recognition in domain-specific tasks where the relevancy of the unknown classes
is vital.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05462">HumanReg: Self-supervised Non-rigid Registration of Human Point Cloud. (arXiv:2312.05462v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yifan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1">Zhiyu Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1">Zhicheng Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1">Wenxuan Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1">Jianjiang Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jie Zhou</a></p>
<p>In this paper, we present a novel registration framework, HumanReg, that
learns a non-rigid transformation between two human point clouds end-to-end. We
introduce body prior into the registration process to efficiently handle this
type of point cloud. Unlike most exsisting supervised registration techniques
that require expensive point-wise flow annotations, HumanReg can be trained in
a self-supervised manner benefiting from a set of novel loss functions. To make
our model better converge on real-world data, we also propose a pretraining
strategy, and a synthetic dataset (HumanSyn4D) consists of dynamic, sparse
human point clouds and their auto-generated ground truth annotations. Our
experiments shows that HumanReg achieves state-of-the-art performance on
CAPE-512 dataset and gains a qualitative result on another more challenging
real-world dataset. Furthermore, our ablation studies demonstrate the
effectiveness of our synthetic dataset and novel loss functions. Our code and
synthetic dataset is available at https://github.com/chenyifanthu/HumanReg.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05464">Identifying and Mitigating Model Failures through Few-shot CLIP-aided Diffusion Generation. (arXiv:2312.05464v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chegini_A/0/1/0/all/0/1">Atoosa Chegini</a>, <a href="http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1">Soheil Feizi</a></p>
<p>Deep learning models can encounter unexpected failures, especially when
dealing with challenging sub-populations. One common reason for these failures
is the occurrence of objects in backgrounds that are rarely seen during
training. To gain a better understanding of these failure modes,
human-interpretable descriptions are crucial for further analysis and
improvement which is expensive. In this study, we propose an end-to-end
framework that utilizes the capabilities of large language models (ChatGPT) and
vision-language deep models (CLIP) to generate text descriptions of failure
modes associated with spurious correlations (e.g. rarely seen backgrounds)
without human-in-the-loop intervention. These descriptions can be used to
generate synthetic data using generative models, such as diffusion models. The
model can now use this generated data to learn from its weaknesses and enhance
its performance on backgrounds that are uncommon for each class of data. Our
approach serves as a broad solution, promising progress in comprehending model
failure modes and strengthening deep learning models across a wide range of
failure scenarios (e.g. bacckgrounds, colors) automatically in a few-shot
manner. Our experiments have shown remarkable \textbf{improvements in accuracy
($\sim \textbf{21%}$)} on hard sub-populations (particularly for wrong
background association) across $40$ different models, such as ResNets,
EfficientNets, DenseNets, Vision Transformer (ViT), SwAVs, MoCos, DINOs, and
CLIPs on various datasets such as ImageNet-1000, CIFAR-10, and CIFAR-100.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05468">Image and Data Mining in Reticular Chemistry Using GPT-4V. (arXiv:2312.05468v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Zhiling Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1">Zhiguo He</a>, <a href="http://arxiv.org/find/cs/1/au:+Khattab_O/0/1/0/all/0/1">Omar Khattab</a>, <a href="http://arxiv.org/find/cs/1/au:+Rampal_N/0/1/0/all/0/1">Nakul Rampal</a>, <a href="http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1">Matei A. Zaharia</a>, <a href="http://arxiv.org/find/cs/1/au:+Borgs_C/0/1/0/all/0/1">Christian Borgs</a>, <a href="http://arxiv.org/find/cs/1/au:+Chayes_J/0/1/0/all/0/1">Jennifer T. Chayes</a>, <a href="http://arxiv.org/find/cs/1/au:+Yaghi_O/0/1/0/all/0/1">Omar M. Yaghi</a></p>
<p>The integration of artificial intelligence into scientific research has
reached a new pinnacle with GPT-4V, a large language model featuring enhanced
vision capabilities, accessible through ChatGPT or an API. This study
demonstrates the remarkable ability of GPT-4V to navigate and obtain complex
data for metal-organic frameworks, especially from graphical sources. Our
approach involved an automated process of converting 346 scholarly articles
into 6240 images, which represents a benchmark dataset in this task, followed
by deploying GPT-4V to categorize and analyze these images using natural
language prompts. This methodology enabled GPT-4V to accurately identify and
interpret key plots integral to MOF characterization, such as nitrogen
isotherms, PXRD patterns, and TGA curves, among others, with accuracy and
recall above 93%. The model's proficiency in extracting critical information
from these plots not only underscores its capability in data mining but also
highlights its potential in aiding the creation of comprehensive digital
databases for reticular chemistry. In addition, the extracted nitrogen isotherm
data from the selected literature allowed for a comparison between theoretical
and experimental porosity values for over 200 compounds, highlighting certain
discrepancies and underscoring the importance of integrating computational and
experimental data. This work highlights the potential of AI in accelerating
scientific discovery and innovation, bridging the gap between computational
tools and experimental research, and paving the way for more efficient,
inclusive, and comprehensive scientific inquiry.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05476">Exploring the Naturalness of AI-Generated Images. (arXiv:2312.05476v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zijian Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1">Wei Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Haoning Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zicheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1">Jun Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1">Xiongkuo Min</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1">Guangtao Zhai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenjun Zhang</a></p>
<p>The proliferation of Artificial Intelligence-Generated Images (AGIs) has
greatly expanded the Image Naturalness Assessment (INA) problem. Different from
early definitions that mainly focus on tone-mapped images with limited
distortions (e.g., exposure, contrast, and color reproduction), INA on
AI-generated images is especially challenging as it has more diverse contents
and could be affected by factors from multiple perspectives, including
low-level technical distortions and high-level rationality distortions. In this
paper, we take the first step to benchmark and assess the visual naturalness of
AI-generated images. First, we construct the AI-Generated Image Naturalness
(AGIN) database by conducting a large-scale subjective study to collect human
opinions on the overall naturalness as well as perceptions from technical and
rationality perspectives. AGIN verifies that naturalness is universally and
disparately affected by both technical and rationality distortions. Second, we
propose the Joint Objective Image Naturalness evaluaTor (JOINT), to
automatically learn the naturalness of AGIs that aligns human ratings.
Specifically, JOINT imitates human reasoning in naturalness evaluation by
jointly learning both technical and rationality perspectives. Experimental
results show our proposed JOINT significantly surpasses baselines for providing
more subjectively consistent results on naturalness assessment. Our database
and code will be released in https://github.com/zijianchen98/AGIN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05482">BARET : Balanced Attention based Real image Editing driven by Target-text Inversion. (arXiv:2312.05482v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yuming Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Fanyi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1">Jingwen Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yanhao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yunjie Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Siyu Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1">Guo-Jun Qi</a></p>
<p>Image editing approaches with diffusion models have been rapidly developed,
yet their applicability are subject to requirements such as specific editing
types (e.g., foreground or background object editing, style transfer), multiple
conditions (e.g., mask, sketch, caption), and time consuming fine-tuning of
diffusion models. For alleviating these limitations and realizing efficient
real image editing, we propose a novel editing technique that only requires an
input image and target text for various editing types including non-rigid edits
without fine-tuning diffusion model. Our method contains three novelties:(I)
Target-text Inversion Schedule (TTIS) is designed to fine-tune the input target
text embedding to achieve fast image reconstruction without image caption and
acceleration of convergence.(II) Progressive Transition Scheme applies
progressive linear interpolation between target text embedding and its
fine-tuned version to generate transition embedding for maintaining non-rigid
editing capability.(III) Balanced Attention Module (BAM) balances the tradeoff
between textual description and image semantics.By the means of combining
self-attention map from reconstruction process and cross-attention map from
transition process, the guidance of target text embeddings in diffusion process
is optimized.In order to demonstrate editing capability, effectiveness and
efficiency of the proposed BARET, we have conducted extensive qualitative and
quantitative experiments. Moreover, results derived from user study and
ablation study further prove the superiority over other methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05490">Shapley Values-enabled Progressive Pseudo Bag Augmentation for Whole Slide Image Classification. (arXiv:2312.05490v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1">Renao Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1">Qiehe Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1">Cheng Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yiqing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yonghong He</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_T/0/1/0/all/0/1">Tian Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hao Chen</a></p>
<p>In computational pathology, whole slide image (WSI) classification presents a
formidable challenge due to its gigapixel resolution and limited fine-grained
annotations. Multiple instance learning (MIL) offers a weakly supervised
solution, yet refining instance-level information from bag-level labels remains
complex. While most of the conventional MIL methods use attention scores to
estimate instance importance scores (IIS) which contribute to the prediction of
the slide labels, these often lead to skewed attention distributions and
inaccuracies in identifying crucial instances. To address these issues, we
propose a new approach inspired by cooperative game theory: employing Shapley
values to assess each instance's contribution, thereby improving IIS
estimation. The computation of the Shapley value is then accelerated using
attention, meanwhile retaining the enhanced instance identification and
prioritization. We further introduce a framework for the progressive assignment
of pseudo bags based on estimated IIS, encouraging more balanced attention
distributions in MIL models. Our extensive experiments on CAMELYON-16, BRACS,
and TCGA-LUNG datasets show our method's superiority over existing
state-of-the-art approaches, offering enhanced interpretability and class-wise
insights. We will release the code upon acceptance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05508">Improving Adversarial Robust Fairness via Anti-Bias Soft Label Distillation. (arXiv:2312.05508v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1">Shiji Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xizhe Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1">Xingxing Wei</a></p>
<p>Adversarial Training (AT) has been widely proved to be an effective method to
improve the adversarial robustness against adversarial examples for Deep Neural
Networks (DNNs). As a variant of AT, Adversarial Robustness Distillation (ARD)
has demonstrated its superior performance in improving the robustness of small
student models with the guidance of large teacher models. However, both AT and
ARD encounter the robust fairness problem: these models exhibit strong
robustness when facing part of classes (easy class), but weak robustness when
facing others (hard class). In this paper, we give an in-depth analysis of the
potential factors and argue that the smoothness degree of samples' soft labels
for different classes (i.e., hard class or easy class) will affect the robust
fairness of DNN models from both empirical observation and theoretical
analysis. Based on the above finding, we propose an Anti-Bias Soft Label
Distillation (ABSLD) method to mitigate the adversarial robust fairness problem
within the framework of Knowledge Distillation (KD). Specifically, ABSLD
adaptively reduces the student's error risk gap between different classes to
achieve fairness by adjusting the class-wise smoothness degree of samples' soft
labels during the training process, and the smoothness degree of soft labels is
controlled by assigning different temperatures in KD to different classes.
Extensive experiments demonstrate that ABSLD outperforms state-of-the-art AT,
ARD, and robust fairness methods in terms of overall performance of robustness
and fairness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05525">You Only Learn One Query: Learning Unified Human Query for Single-Stage Multi-Person Multi-Task Human-Centric Perception. (arXiv:2312.05525v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1">Sheng Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shuhuai Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1">Tong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wentao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1">Chen Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1">Ping Luo</a></p>
<p>Human-centric perception (e.g. pedetrian detection, segmentation, pose
estimation, and attribute analysis) is a long-standing problem for computer
vision. This paper introduces a unified and versatile framework (HQNet) for
single-stage multi-person multi-task human-centric perception (HCP). Our
approach centers on learning a unified human query representation, denoted as
Human Query, which captures intricate instance-level features for individual
persons and disentangles complex multi-person scenarios. Although different HCP
tasks have been well-studied individually, single-stage multi-task learning of
HCP tasks has not been fully exploited in the literature due to the absence of
a comprehensive benchmark dataset. To address this gap, we propose
COCO-UniHuman benchmark dataset to enable model development and comprehensive
evaluation. Experimental results demonstrate the proposed method's
state-of-the-art performance among multi-task HCP models and its competitive
performance compared to task-specific HCP models. Moreover, our experiments
underscore Human Query's adaptability to new HCP tasks, thus demonstrating its
robust generalization capability. Codes and data will be publicly accessible.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05528">Exploring 3D U-Net Training Configurations and Post-Processing Strategies for the MICCAI 2023 Kidney and Tumor Segmentation Challenge. (arXiv:2312.05528v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Uhm_K/0/1/0/all/0/1">Kwang-Hyun Uhm</a>, <a href="http://arxiv.org/find/eess/1/au:+Cho_H/0/1/0/all/0/1">Hyunjun Cho</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1">Zhixin Xu</a>, <a href="http://arxiv.org/find/eess/1/au:+Lim_S/0/1/0/all/0/1">Seohoon Lim</a>, <a href="http://arxiv.org/find/eess/1/au:+Jung_S/0/1/0/all/0/1">Seung-Won Jung</a>, <a href="http://arxiv.org/find/eess/1/au:+Hong_S/0/1/0/all/0/1">Sung-Hoo Hong</a>, <a href="http://arxiv.org/find/eess/1/au:+Ko_S/0/1/0/all/0/1">Sung-Jea Ko</a></p>
<p>In 2023, it is estimated that 81,800 kidney cancer cases will be newly
diagnosed, and 14,890 people will die from this cancer in the United States.
Preoperative dynamic contrast-enhanced abdominal computed tomography (CT) is
often used for detecting lesions. However, there exists inter-observer
variability due to subtle differences in the imaging features of kidney and
kidney tumors. In this paper, we explore various 3D U-Net training
configurations and effective post-processing strategies for accurate
segmentation of kidneys, cysts, and kidney tumors in CT images. We validated
our model on the dataset of the 2023 Kidney and Kidney Tumor Segmentation
(KiTS23) challenge. Our method took second place in the final ranking of the
KiTS23 challenge on unseen test data with an average Dice score of 0.820 and an
average Surface Dice of 0.712.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05538">CSL: Class-Agnostic Structure-Constrained Learning for Segmentation Including the Unseen. (arXiv:2312.05538v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1">Fang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1">Lu Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Ming-Hsuan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahuja_N/0/1/0/all/0/1">Narendra Ahuja</a></p>
<p>Addressing Out-Of-Distribution (OOD) Segmentation and Zero-Shot Semantic
Segmentation (ZS3) is challenging, necessitating segmenting unseen classes.
Existing strategies adapt the class-agnostic Mask2Former (CA-M2F) tailored to
specific tasks. However, these methods cater to singular tasks, demand training
from scratch, and we demonstrate certain deficiencies in CA-M2F, which affect
performance. We propose the Class-Agnostic Structure-Constrained Learning
(CSL), a plug-in framework that can integrate with existing methods, thereby
embedding structural constraints and achieving performance gain, including the
unseen, specifically OOD, ZS3, and domain adaptation (DA) tasks. There are two
schemes for CSL to integrate with existing methods (1) by distilling knowledge
from a base teacher network, enforcing constraints across training and
inference phrases, or (2) by leveraging established models to obtain per-pixel
distributions without retraining, appending constraints during the inference
phase. We propose soft assignment and mask split methodologies that enhance OOD
object segmentation. Empirical evaluations demonstrate CSL's prowess in
boosting the performance of existing algorithms spanning OOD segmentation, ZS3,
and DA segmentation, consistently transcending the state-of-art across all
three tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05541">DPoser: Diffusion Model as Robust 3D Human Pose Prior. (arXiv:2312.05541v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Junzhe Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Jing Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Dou_H/0/1/0/all/0/1">Hongkun Dou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yulun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1">Yue Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haoqian Wang</a></p>
<p>Modeling human pose is a cornerstone in applications from human-robot
interaction to augmented reality, yet crafting a robust human pose prior
remains a challenge due to biomechanical constraints and diverse human
movements. Traditional priors like VAEs and NDFs often fall short in realism
and generalization, especially in extreme conditions such as unseen noisy
poses. To address these issues, we introduce DPoser, a robust and versatile
human pose prior built upon diffusion models. Designed with optimization
frameworks, DPoser seamlessly integrates into various pose-centric
applications, including human mesh recovery, pose completion, and motion
denoising. Specifically, by formulating these tasks as inverse problems, we
employ variational diffusion sampling for efficient solving. Furthermore,
acknowledging the disparity between the articulated poses we focus on and
structured images in previous research, we propose a truncated timestep
scheduling to boost performance on downstream tasks. Our exhaustive experiments
demonstrate DPoser's superiority over existing state-of-the-art pose priors
across multiple tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05548">A Unified Multi-Phase CT Synthesis and Classification Framework for Kidney Cancer Diagnosis with Incomplete Data. (arXiv:2312.05548v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Uhm_K/0/1/0/all/0/1">Kwang-Hyun Uhm</a>, <a href="http://arxiv.org/find/eess/1/au:+Jung_S/0/1/0/all/0/1">Seung-Won Jung</a>, <a href="http://arxiv.org/find/eess/1/au:+Choi_M/0/1/0/all/0/1">Moon Hyung Choi</a>, <a href="http://arxiv.org/find/eess/1/au:+Hong_S/0/1/0/all/0/1">Sung-Hoo Hong</a>, <a href="http://arxiv.org/find/eess/1/au:+Ko_S/0/1/0/all/0/1">Sung-Jea Ko</a></p>
<p>Multi-phase CT is widely adopted for the diagnosis of kidney cancer due to
the complementary information among phases. However, the complete set of
multi-phase CT is often not available in practical clinical applications. In
recent years, there have been some studies to generate the missing modality
image from the available data. Nevertheless, the generated images are not
guaranteed to be effective for the diagnosis task. In this paper, we propose a
unified framework for kidney cancer diagnosis with incomplete multi-phase CT,
which simultaneously recovers missing CT images and classifies cancer subtypes
using the completed set of images. The advantage of our framework is that it
encourages a synthesis model to explicitly learn to generate missing CT phases
that are helpful for classifying cancer subtypes. We further incorporate lesion
segmentation network into our framework to exploit lesion-level features for
effective cancer classification in the whole CT volumes. The proposed framework
is based on fully 3D convolutional neural networks to jointly optimize both
synthesis and classification of 3D CT volumes. Extensive experiments on both
in-house and external datasets demonstrate the effectiveness of our framework
for the diagnosis with incomplete data compared with state-of-the-art
baselines. In particular, cancer subtype classification using the completed CT
data by our method achieves higher performance than the classification using
the given incomplete data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05572">R2-Talker: Realistic Real-Time Talking Head Synthesis with Hash Grid Landmarks Encoding and Progressive Multilayer Conditioning. (arXiv:2312.05572v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1">Zhiling Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">LiangGuo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1">Dingheng Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1">Quan Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1">Ning Jiang</a></p>
<p>Dynamic NeRFs have recently garnered growing attention for 3D talking
portrait synthesis. Despite advances in rendering speed and visual quality,
challenges persist in enhancing efficiency and effectiveness. We present
R2-Talker, an efficient and effective framework enabling realistic real-time
talking head synthesis. Specifically, using multi-resolution hash grids, we
introduce a novel approach for encoding facial landmarks as conditional
features. This approach losslessly encodes landmark structures as conditional
features, decoupling input diversity, and conditional spaces by mapping
arbitrary landmarks to a unified feature space. We further propose a scheme of
progressive multilayer conditioning in the NeRF rendering pipeline for
effective conditional feature fusion. Our new approach has the following
advantages as demonstrated by extensive experiments compared with the
state-of-the-art works: 1) The lossless input encoding enables acquiring more
precise features, yielding superior visual quality. The decoupling of inputs
and conditional spaces improves generalizability. 2) The fusing of conditional
features and MLP outputs at each MLP layer enhances conditional impact,
resulting in more accurate lip synthesis and better visual quality. 3) It
compactly structures the fusion of conditional features, significantly
enhancing computational efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05588">Language-assisted Vision Model Debugger: A Sample-Free Approach to Finding Bugs. (arXiv:2312.05588v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1">Chaoquan Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jinqiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1">Rui Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sang_J/0/1/0/all/0/1">Jitao Sang</a></p>
<p>Vision models with high overall accuracy often exhibit systematic errors in
specific scenarios, posing potential serious safety concerns. Diagnosing bugs
of vision models is gaining increased attention, however traditional diagnostic
approaches require annotation efforts (\eg rich metadata accompanying each
samples of CelebA). To address this issue,We propose a language-assisted
diagnostic method that uses texts instead of images to diagnose bugs in vision
models based on multi-modal models (\eg CLIP). Our approach connects the
embedding space of CLIP with the buggy vision model to be diagnosed; meanwhile,
utilizing a shared classifier and the cross-modal transferability of embedding
space from CLIP, the text-branch of CLIP become a proxy model to find bugs in
the buggy model. The proxy model can classify texts paired with images. During
the diagnosis, a Large Language Model (LLM) is employed to obtain task-relevant
corpora, and this corpora is used to extract keywords. Descriptions constructed
with templates containing these keywords serve as input text to probe errors in
the proxy model. Finally, we validate the ability to diagnose existing visual
models using language on the Waterbirds and CelebA datasets, we can identify
bugs comprehensible to human experts, uncovering not only known bugs but also
previously unknown ones.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/1605.06409">R-FCN: Object Detection via Region-based Fully Convolutional Networks. (arXiv:1605.06409v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1">Jifeng Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1">Kaiming He</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jian Sun</a></p>
<p>We present region-based, fully convolutional networks for accurate and
efficient object detection. In contrast to previous region-based detectors such
as Fast/Faster R-CNN that apply a costly per-region subnetwork hundreds of
times, our region-based detector is fully convolutional with almost all
computation shared on the entire image. To achieve this goal, we propose
position-sensitive score maps to address a dilemma between
translation-invariance in image classification and translation-variance in
object detection. Our method can thus naturally adopt fully convolutional image
classifier backbones, such as the latest Residual Networks (ResNets), for
object detection. We show competitive results on the PASCAL VOC datasets (e.g.,
83.6% mAP on the 2007 set) with the 101-layer ResNet. Meanwhile, our result is
achieved at a test-time speed of 170ms per image, 2.5-20x faster than the
Faster R-CNN counterpart. Code is made publicly available at:
https://github.com/daijifeng001/r-fcn
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2012.03170">Food Classification with Convolutional Neural Networks and Multi-Class Linear Discernment Analysis. (arXiv:2012.03170v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ball_J/0/1/0/all/0/1">Joshua Ball</a></p>
<p>Convolutional neural networks (CNNs) have been successful in representing the
fully-connected inferencing ability perceived to be seen in the human brain:
they take full advantage of the hierarchy-style patterns commonly seen in
complex data and develop more patterns using simple features. Countless
implementations of CNNs have shown how strong their ability is to learn these
complex patterns, particularly in the realm of image classification. However,
the cost of getting a high performance CNN to a so-called "state of the art"
level is computationally costly. Even when using transfer learning, which
utilize the very deep layers from models such as MobileNetV2, CNNs still take a
great amount of time and resources. Linear discriminant analysis (LDA), a
generalization of Fisher's linear discriminant, can be implemented in a
multi-class classification method to increase separability of class features
while not needing a high performance system to do so for image classification.
Similarly, we also believe LDA has great promise in performing well. In this
paper, we discuss our process of developing a robust CNN for food
classification as well as our effective implementation of multi-class LDA and
prove that (1) CNN is superior to LDA for image classification and (2) why LDA
should not be left out of the races for image classification, particularly for
binary cases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.01217">Hybrid Tracker with Pixel and Instance for Video Panoptic Segmentation. (arXiv:2203.01217v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1">Weicai Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Lan_X/0/1/0/all/0/1">Xinyue Lan</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_G/0/1/0/all/0/1">Ge Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1">Hujun Bao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1">Zhaopeng Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Guofeng Zhang</a></p>
<p>Video Panoptic Segmentation (VPS) aims to generate coherent panoptic
segmentation and track the identities of all pixels across video frames.
Existing methods predominantly utilize the trained instance embedding to keep
the consistency of panoptic segmentation. However, they inevitably struggle to
cope with the challenges of small objects, similar appearance but inconsistent
identities, occlusion, and strong instance contour deformations. To address
these problems, we present HybridTracker, a lightweight and joint tracking
model attempting to eliminate the limitations of the single tracker.
HybridTracker performs pixel tracker and instance tracker in parallel to obtain
the association matrices, which are fused into a matching matrix. In the
instance tracker, we design a differentiable matching layer, ensuring the
stability of inter-frame matching. In the pixel tracker, we compute the dice
coefficient of the same instance of different frames given the estimated
optical flow, forming the Intersection Over Union (IoU) matrix. We additionally
propose mutual check and temporal consistency constraints during inference to
settle the occlusion and contour deformation challenges. Comprehensive
experiments show that HybridTracker achieves superior performance than
state-of-the-art methods on Cityscapes-VPS and VIPER datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2204.08563">Cylin-Painting: Seamless {360\textdegree} Panoramic Image Outpainting and Beyond. (arXiv:2204.08563v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liao_K/0/1/0/all/0/1">Kang Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiangyu Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1">Chunyu Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1">Wenqi Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1">Yunchao Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yao Zhao</a></p>
<p>Image outpainting gains increasing attention since it can generate the
complete scene from a partial view, providing a valuable solution to construct
{360\textdegree} panoramic images. As image outpainting suffers from the
intrinsic issue of unidirectional completion flow, previous methods convert the
original problem into inpainting, which allows a bidirectional flow. However,
we find that inpainting has its own limitations and is inferior to outpainting
in certain situations. The question of how they may be combined for the best of
both has as yet remained under-explored. In this paper, we provide a deep
analysis of the differences between inpainting and outpainting, which
essentially depends on how the source pixels contribute to the unknown regions
under different spatial arrangements. Motivated by this analysis, we present a
Cylin-Painting framework that involves meaningful collaborations between
inpainting and outpainting and efficiently fuses the different arrangements,
with a view to leveraging their complementary benefits on a seamless cylinder.
Nevertheless, straightforwardly applying the cylinder-style convolution often
generates visually unpleasing results as it discards important positional
information. To address this issue, we further present a learnable positional
embedding strategy to incorporate the missing component of positional encoding
into the cylinder convolution, which significantly improves the panoramic
results. It is noted that while developed for image outpainting, the proposed
algorithm can be effectively extended to other panoramic vision tasks, such as
object detection, depth estimation, and image super-resolution. Code will be
made available at \url{https://github.com/KangLiao929/Cylin-Painting}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.10177">TCJA-SNN: Temporal-Channel Joint Attention for Spiking Neural Networks. (arXiv:2206.10177v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1">Rui-Jie Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1">Qihang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tianjing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1">Haoyu Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1">Yule Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Malu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1">Liang-Jian Deng</a></p>
<p>Spiking Neural Networks (SNNs) are attracting widespread interest due to
their biological plausibility, energy efficiency, and powerful spatio-temporal
information representation ability. Given the critical role of attention
mechanisms in enhancing neural network performance, the integration of SNNs and
attention mechanisms exhibits potential to deliver energy-efficient and
high-performance computing paradigms. We present a novel Temporal-Channel Joint
Attention mechanism for SNNs, referred to as TCJA-SNN. The proposed TCJA-SNN
framework can effectively assess the significance of spike sequence from both
spatial and temporal dimensions. More specifically, our essential technical
contribution lies on: 1) We employ the squeeze operation to compress the spike
stream into an average matrix. Then, we leverage two local attention mechanisms
based on efficient 1D convolutions to facilitate comprehensive feature
extraction at the temporal and channel levels independently. 2) We introduce
the Cross Convolutional Fusion (CCF) layer as a novel approach to model the
inter-dependencies between the temporal and channel scopes. This layer breaks
the independence of these two dimensions and enables the interaction between
features. Experimental results demonstrate that the proposed TCJA-SNN
outperforms SOTA by up to 15.7% accuracy on standard static and neuromorphic
datasets, including Fashion-MNIST, CIFAR10-DVS, N-Caltech 101, and DVS128
Gesture. Furthermore, we apply the TCJA-SNN framework to image generation tasks
by leveraging a variation autoencoder. To the best of our knowledge, this study
is the first instance where the SNN-attention mechanism has been employed for
image classification and generation tasks. Notably, our approach has achieved
SOTA performance in both domains, establishing a significant advancement in the
field. Codes are available at https://github.com/ridgerchu/TCJA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.00259">COVID-19 Detection Using Transfer Learning Approach from Computed Tomography Images. (arXiv:2207.00259v5 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Morani_K/0/1/0/all/0/1">Kenan Morani</a>, <a href="http://arxiv.org/find/eess/1/au:+Ayana_E/0/1/0/all/0/1">Esra Kaya Ayana</a>, <a href="http://arxiv.org/find/eess/1/au:+Unay_D/0/1/0/all/0/1">Devrim Unay</a></p>
<p>The significance of efficient and accurate diagnosis amidst the unique
challenges posed by the COVID-19 pandemic underscores the urgency for
innovative approaches. In response to these challenges, we propose a transfer
learning-based approach using a recently annotated Computed Tomography (CT)
image database. While many approaches propose an intensive data preproseccing
and/or complex model architecture, our method focusses on offering an efficient
solution with minimal manual engineering. Specifically, we investigate the
suitability of a modified Xception model for COVID-19 detection. The method
involves adapting a pre-trained Xception model, incorporating both the
architecture and pre-trained weights from ImageNet. The output of the model was
designed to take the final diagnosis decisions. The training utilized 128 batch
sizes and 224x224 input image dimensions, downsized from standard 512x512. No
further da processing was performed on the input data. Evaluation is conducted
on the 'COV19-CT-DB' CT image dataset, containing labeled COVID-19 and
non-COVID-19 cases. Results reveal the method's superiority in accuracy,
precision, recall, and macro F1 score on the validation subset, outperforming
VGG-16 transfer model and thus offering enhanced precision with fewer
parameters. Furthermore, when compared to alternative methods for the
COV19-CT-DB dataset, our approach exceeds the baseline approach and other
alternatives on the same dataset. Finally, the adaptability of the modified
Xception trasnfer learning-based model to the unique features of the
COV19-CT-DB dataset showcases its potential as a robust tool for enhanced
COVID-19 diagnosis from CT images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.06465">Imaging through the Atmosphere using Turbulence Mitigation Transformer. (arXiv:2207.06465v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1">Xingguang Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Mao_Z/0/1/0/all/0/1">Zhiyuan Mao</a>, <a href="http://arxiv.org/find/eess/1/au:+Chimitt_N/0/1/0/all/0/1">Nicholas Chimitt</a>, <a href="http://arxiv.org/find/eess/1/au:+Chan_S/0/1/0/all/0/1">Stanley H. Chan</a></p>
<p>Restoring images distorted by atmospheric turbulence is a ubiquitous problem
in long-range imaging applications. While existing deep-learning-based methods
have demonstrated promising results in specific testing conditions, they suffer
from three limitations: (1) lack of generalization capability from synthetic
training data to real turbulence data; (2) failure to scale, hence causing
memory and speed challenges when extending the idea to a large number of
frames; (3) lack of a fast and accurate simulator to generate data for training
neural networks. In this paper, we introduce the turbulence mitigation
transformer (TMT) that explicitly addresses these issues. TMT brings three
contributions: Firstly, TMT explicitly uses turbulence physics by decoupling
the turbulence degradation and introducing a multi-scale loss for removing
distortion, thus improving effectiveness. Secondly, TMT presents a new
attention module along the temporal axis to extract extra features efficiently,
thus improving memory and speed. Thirdly, TMT introduces a new simulator based
on the Fourier sampler, temporal correlation, and flexible kernel size, thus
improving our capability to synthesize better training data. TMT outperforms
state-of-the-art video restoration models, especially in generalizing from
synthetic to real turbulence data. Code, videos, and datasets are available at
\href{https://xg416.github.io/TMT}{https://xg416.github.io/TMT}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.11435">Bidirectional Contrastive Split Learning for Visual Question Answering. (arXiv:2208.11435v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yuwei Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Ochiai_H/0/1/0/all/0/1">Hideya Ochiai</a></p>
<p>Visual Question Answering (VQA) based on multi-modal data facilitates
real-life applications such as home robots and medical diagnoses. One
significant challenge is to devise a robust decentralized learning framework
for various client models where centralized data collection is refrained due to
confidentiality concerns. This work aims to tackle privacy-preserving VQA by
decoupling a multi-modal model into representation modules and a contrastive
module and leveraging inter-module gradients sharing and inter-client weight
sharing. To this end, we propose Bidirectional Contrastive Split Learning
(BiCSL) to train a global multi-modal model on the entire data distribution of
decentralized clients. We employ the contrastive loss that enables a more
efficient self-supervised learning of decentralized modules. Comprehensive
experiments are conducted on the VQA-v2 dataset based on five SOTA VQA models,
demonstrating the effectiveness of the proposed method. Furthermore, we inspect
BiCSL's robustness against a dual-key backdoor attack on VQA. Consequently,
BiCSL shows much better robustness to the multi-modal adversarial attack
compared to the centralized learning method, which provides a promising
approach to decentralized multi-modal learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.03300">Spach Transformer: Spatial and Channel-wise Transformer Based on Local and Global Self-attentions for PET Image Denoising. (arXiv:2209.03300v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Jang_S/0/1/0/all/0/1">Se-In Jang</a>, <a href="http://arxiv.org/find/eess/1/au:+Pan_T/0/1/0/all/0/1">Tinsu Pan</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1">Ye Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Heidari_P/0/1/0/all/0/1">Pedram Heidari</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1">Junyu Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1">Quanzheng Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Gong_K/0/1/0/all/0/1">Kuang Gong</a></p>
<p>Position emission tomography (PET) is widely used in clinics and research due
to its quantitative merits and high sensitivity, but suffers from low
signal-to-noise ratio (SNR). Recently convolutional neural networks (CNNs) have
been widely used to improve PET image quality. Though successful and efficient
in local feature extraction, CNN cannot capture long-range dependencies well
due to its limited receptive field. Global multi-head self-attention (MSA) is a
popular approach to capture long-range information. However, the calculation of
global MSA for 3D images has high computational costs. In this work, we
proposed an efficient spatial and channel-wise encoder-decoder transformer,
Spach Transformer, that can leverage spatial and channel information based on
local and global MSAs. Experiments based on datasets of different PET tracers,
i.e., $^{18}$F-FDG, $^{18}$F-ACBC, $^{18}$F-DCFPyL, and $^{68}$Ga-DOTATATE,
were conducted to evaluate the proposed framework. Quantitative results show
that the proposed Spach Transformer framework outperforms state-of-the-art deep
learning architectures. Our codes are available at
https://github.com/sijang/SpachTransformer
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.10916">ECM-OPCC: Efficient Context Model for Octree-based Point Cloud Compression. (arXiv:2211.10916v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1">Yiqi Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Ziyu Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1">Tongda Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yuhuan Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yan Wang</a></p>
<p>Recently, deep learning methods have shown promising results in point cloud
compression. For octree-based point cloud compression, previous works show that
the information of ancestor nodes and sibling nodes are equally important for
predicting current node. However, those works either adopt insufficient context
or bring intolerable decoding complexity (e.g. &gt;600s). To address this problem,
we propose a sufficient yet efficient context model and design an efficient
deep learning codec for point clouds. Specifically, we first propose a
window-constrained multi-group coding strategy to exploit the autoregressive
context while maintaining decoding efficiency. Then, we propose a dual
transformer architecture to utilize the dependency of current node on its
ancestors and siblings. We also propose a random-masking pre-train method to
enhance our model. Experimental results show that our approach achieves
state-of-the-art performance for both lossy and lossless point cloud
compression. Moreover, our multi-group coding strategy saves 98% decoding time
compared with previous octree-based compression method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.13993">Combating noisy labels in object detection datasets. (arXiv:2211.13993v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chachula_K/0/1/0/all/0/1">Krystian Chachu&#x142;a</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyskawa_J/0/1/0/all/0/1">Jakub &#x141;yskawa</a>, <a href="http://arxiv.org/find/cs/1/au:+Olber_B/0/1/0/all/0/1">Bart&#x142;omiej Olber</a>, <a href="http://arxiv.org/find/cs/1/au:+Fratczak_P/0/1/0/all/0/1">Piotr Fr&#x105;tczak</a>, <a href="http://arxiv.org/find/cs/1/au:+Popowicz_A/0/1/0/all/0/1">Adam Popowicz</a>, <a href="http://arxiv.org/find/cs/1/au:+Radlak_K/0/1/0/all/0/1">Krystian Radlak</a></p>
<p>The quality of training datasets for deep neural networks is a key factor
contributing to the accuracy of resulting models. This effect is amplified in
difficult tasks such as object detection. Dealing with errors in datasets is
often limited to accepting that some fraction of examples are incorrect,
estimating their confidence, and either assigning appropriate weights or
ignoring uncertain ones during training. In this work, we propose a different
approach. We introduce the Confident Learning for Object Detection (CLOD)
algorithm for assessing the quality of each label in object detection datasets,
identifying missing, spurious, mislabeled, and mislocated bounding boxes and
suggesting corrections. By focusing on finding incorrect examples in the
training datasets, we can eliminate them at the root. Suspicious bounding boxes
can be reviewed to improve the quality of the dataset, leading to better models
without further complicating their already complex architectures. The proposed
method is able to point out nearly 80% of artificially disturbed bounding boxes
with a false positive rate below 0.1. Cleaning the datasets by applying the
most confident automatic suggestions improved mAP scores by 16% to 46%,
depending on the dataset, without any modifications to the network
architectures. This approach shows promising potential in rectifying
state-of-the-art object detection datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.01593">Make RepVGG Greater Again: A Quantization-aware Approach. (arXiv:2212.01593v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1">Xiangxiang Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Liang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Bo Zhang</a></p>
<p>The tradeoff between performance and inference speed is critical for
practical applications. Architecture reparameterization obtains better
tradeoffs and it is becoming an increasingly popular ingredient in modern
convolutional neural networks. Nonetheless, its quantization performance is
usually too poor to deploy (more than 20% top-1 accuracy drop on ImageNet) when
INT8 inference is desired. In this paper, we dive into the underlying mechanism
of this failure, where the original design inevitably enlarges quantization
error. We propose a simple, robust, and effective remedy to have a
quantization-friendly structure that also enjoys reparameterization benefits.
Our method greatly bridges the gap between INT8 and FP32 accuracy for RepVGG.
Without bells and whistles, the top-1 accuracy drop on ImageNet is reduced
within 2% by standard post-training quantization. Moreover, our method also
achieves similar FP32 performance as RepVGG. Extensive experiments on detection
and semantic segmentation tasks verify its generalization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.04098">EPCL: Frozen CLIP Transformer is An Efficient Point Cloud Encoder. (arXiv:2212.04098v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xiaoshui Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zhou Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Sheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Qu_W/0/1/0/all/0/1">Wentao Qu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1">Tong He</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1">Yuenan Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zuo_Y/0/1/0/all/0/1">Yifan Zuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1">Wanli Ouyang</a></p>
<p>The pretrain-finetune paradigm has achieved great success in NLP and 2D image
fields because of the high-quality representation ability and transferability
of their pretrained models. However, pretraining such a strong model is
difficult in the 3D point cloud field due to the limited amount of point cloud
sequences. This paper introduces \textbf{E}fficient \textbf{P}oint
\textbf{C}loud \textbf{L}earning (EPCL), an effective and efficient point cloud
learner for directly training high-quality point cloud models with a frozen
CLIP transformer. Our EPCL connects the 2D and 3D modalities by semantically
aligning the image features and point cloud features without paired 2D-3D data.
Specifically, the input point cloud is divided into a series of local patches,
which are converted to token embeddings by the designed point cloud tokenizer.
These token embeddings are concatenated with a task token and fed into the
frozen CLIP transformer to learn point cloud representation. The intuition is
that the proposed point cloud tokenizer projects the input point cloud into a
unified token space that is similar to the 2D images. Comprehensive experiments
on 3D detection, semantic segmentation, classification and few-shot learning
demonstrate that the CLIP transformer can serve as an efficient point cloud
encoder and our method achieves promising performance on both indoor and
outdoor benchmarks. In particular, performance gains brought by our EPCL are
$\textbf{19.7}$ AP$_{50}$ on ScanNet V2 detection, $\textbf{4.4}$ mIoU on S3DIS
segmentation and $\textbf{1.2}$ mIoU on SemanticKITTI segmentation compared to
contemporary pretrained models. Code is available at
\url{https://github.com/XiaoshuiHuang/EPCL}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.06512">DifFace: Blind Face Restoration with Diffused Error Contraction. (arXiv:2212.06512v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yue_Z/0/1/0/all/0/1">Zongsheng Yue</a>, <a href="http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1">Chen Change Loy</a></p>
<p>While deep learning-based methods for blind face restoration have achieved
unprecedented success, they still suffer from two major limitations. First,
most of them deteriorate when facing complex degradations out of their training
data. Second, these methods require multiple constraints, e.g., fidelity,
perceptual, and adversarial losses, which require laborious hyper-parameter
tuning to stabilize and balance their influences. In this work, we propose a
novel method named DifFace that is capable of coping with unseen and complex
degradations more gracefully without complicated loss designs. The key of our
method is to establish a posterior distribution from the observed low-quality
(LQ) image to its high-quality (HQ) counterpart. In particular, we design a
transition distribution from the LQ image to the intermediate state of a
pre-trained diffusion model and then gradually transmit from this intermediate
state to the HQ target by recursively applying a pre-trained diffusion model.
The transition distribution only relies on a restoration backbone that is
trained with $L_2$ loss on some synthetic data, which favorably avoids the
cumbersome training process in existing methods. Moreover, the transition
distribution can contract the error of the restoration backbone and thus makes
our method more robust to unknown degradations. Comprehensive experiments show
that DifFace is superior to current state-of-the-art methods, especially in
cases with severe degradations. Code and model are available at
https://github.com/zsyOAOA/DifFace.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.00524">Learning Confident Classifiers in the Presence of Label Noise. (arXiv:2301.00524v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hashmi_A/0/1/0/all/0/1">Asma Ahmed Hashmi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhumabayeva_A/0/1/0/all/0/1">Aigerim Zhumabayeva</a>, <a href="http://arxiv.org/find/cs/1/au:+Kotelevskii_N/0/1/0/all/0/1">Nikita Kotelevskii</a>, <a href="http://arxiv.org/find/cs/1/au:+Agafonov_A/0/1/0/all/0/1">Artem Agafonov</a>, <a href="http://arxiv.org/find/cs/1/au:+Yaqub_M/0/1/0/all/0/1">Mohammad Yaqub</a>, <a href="http://arxiv.org/find/cs/1/au:+Panov_M/0/1/0/all/0/1">Maxim Panov</a>, <a href="http://arxiv.org/find/cs/1/au:+Takac_M/0/1/0/all/0/1">Martin Tak&#xe1;&#x10d;</a></p>
<p>The success of Deep Neural Network (DNN) models significantly depends on the
quality of provided annotations. In medical image segmentation, for example,
having multiple expert annotations for each data point is common to minimize
subjective annotation bias. Then, the goal of estimation is to filter out the
label noise and recover the ground-truth masks, which are not explicitly given.
This paper proposes a probabilistic model for noisy observations that allows us
to build a confident classification and segmentation models. To accomplish it,
we explicitly model label noise and introduce a new information-based
regularization that pushes the network to recover the ground-truth labels. In
addition, for segmentation task we adjust the loss function by prioritizing
learning in high-confidence regions where all the annotators agree on labeling.
We evaluate the proposed method on a series of classification tasks such as
noisy versions of MNIST, CIFAR-10, Fashion-MNIST datasets as well as CIFAR-10N,
which is real-world dataset with noisy human annotations. Additionally, for
segmentation task, we consider several medical imaging datasets, such as, LIDC
and RIGA that reflect real-world inter-variability among multiple annotators.
Our experiments show that our algorithm outperforms state-of-the-art solutions
for the considered classification and segmentation problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.03629">Ethical Considerations for Responsible Data Curation. (arXiv:2302.03629v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Andrews_J/0/1/0/all/0/1">Jerone T. A. Andrews</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1">Dora Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Thong_W/0/1/0/all/0/1">William Thong</a>, <a href="http://arxiv.org/find/cs/1/au:+Modas_A/0/1/0/all/0/1">Apostolos Modas</a>, <a href="http://arxiv.org/find/cs/1/au:+Papakyriakopoulos_O/0/1/0/all/0/1">Orestis Papakyriakopoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_A/0/1/0/all/0/1">Alice Xiang</a></p>
<p>Human-centric computer vision (HCCV) data curation practices often neglect
privacy and bias concerns, leading to dataset retractions and unfair models.
HCCV datasets constructed through nonconsensual web scraping lack crucial
metadata for comprehensive fairness and robustness evaluations. Current
remedies are post hoc, lack persuasive justification for adoption, or fail to
provide proper contextualization for appropriate application. Our research
focuses on proactive, domain-specific recommendations, covering purpose,
privacy and consent, and diversity, for curating HCCV evaluation datasets,
addressing privacy and bias concerns. We adopt an ante hoc reflective
perspective, drawing from current practices, guidelines, dataset withdrawals,
and audits, to inform our considerations and recommendations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.12247">Quantifying &amp; Modeling Multimodal Interactions: An Information Decomposition Framework. (arXiv:2302.12247v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1">Paul Pu Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1">Yun Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1">Xiang Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ling_C/0/1/0/all/0/1">Chun Kai Ling</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_S/0/1/0/all/0/1">Suzanne Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1">Richard Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1">Zihao Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Allen_N/0/1/0/all/0/1">Nicholas Allen</a>, <a href="http://arxiv.org/find/cs/1/au:+Auerbach_R/0/1/0/all/0/1">Randy Auerbach</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahmood_F/0/1/0/all/0/1">Faisal Mahmood</a>, <a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1">Ruslan Salakhutdinov</a>, <a href="http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1">Louis-Philippe Morency</a></p>
<p>The recent explosion of interest in multimodal applications has resulted in a
wide selection of datasets and methods for representing and integrating
information from different modalities. Despite these empirical advances, there
remain fundamental research questions: How can we quantify the interactions
that are necessary to solve a multimodal task? Subsequently, what are the most
suitable multimodal models to capture these interactions? To answer these
questions, we propose an information-theoretic approach to quantify the degree
of redundancy, uniqueness, and synergy relating input modalities with an output
task. We term these three measures as the PID statistics of a multimodal
distribution (or PID for short), and introduce two new estimators for these PID
statistics that scale to high-dimensional distributions. To validate PID
estimation, we conduct extensive experiments on both synthetic datasets where
the PID is known and on large-scale multimodal benchmarks where PID estimations
are compared with human annotations. Finally, we demonstrate their usefulness
in (1) quantifying interactions within multimodal datasets, (2) quantifying
interactions captured by multimodal models, (3) principled approaches for model
selection, and (4) three real-world case studies engaging with domain experts
in pathology, mood prediction, and robotic perception where our framework helps
to recommend strong multimodal models for each application.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.09375">DINAR: Diffusion Inpainting of Neural Textures for One-Shot Human Avatars. (arXiv:2303.09375v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Svitov_D/0/1/0/all/0/1">David Svitov</a>, <a href="http://arxiv.org/find/cs/1/au:+Gudkov_D/0/1/0/all/0/1">Dmitrii Gudkov</a>, <a href="http://arxiv.org/find/cs/1/au:+Bashirov_R/0/1/0/all/0/1">Renat Bashirov</a>, <a href="http://arxiv.org/find/cs/1/au:+Lempitsky_V/0/1/0/all/0/1">Victor Lempitsky</a></p>
<p>We present DINAR, an approach for creating realistic rigged fullbody avatars
from single RGB images. Similarly to previous works, our method uses neural
textures combined with the SMPL-X body model to achieve photo-realistic quality
of avatars while keeping them easy to animate and fast to infer. To restore the
texture, we use a latent diffusion model and show how such model can be trained
in the neural texture space. The use of the diffusion model allows us to
realistically reconstruct large unseen regions such as the back of a person
given the frontal view. The models in our pipeline are trained using 2D images
and videos only. In the experiments, our approach achieves state-of-the-art
rendering quality and good generalization to new poses and viewpoints. In
particular, the approach improves state-of-the-art on the SnapshotPeople public
benchmark.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.09508">LDMVFI: Video Frame Interpolation with Latent Diffusion Models. (arXiv:2303.09508v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Danier_D/0/1/0/all/0/1">Duolikun Danier</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1">Fan Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Bull_D/0/1/0/all/0/1">David Bull</a></p>
<p>Existing works on video frame interpolation (VFI) mostly employ deep neural
networks that are trained by minimizing the L1, L2, or deep feature space
distance (e.g. VGG loss) between their outputs and ground-truth frames.
However, recent works have shown that these metrics are poor indicators of
perceptual VFI quality. Towards developing perceptually-oriented VFI methods,
in this work we propose latent diffusion model-based VFI, LDMVFI. This
approaches the VFI problem from a generative perspective by formulating it as a
conditional generation problem. As the first effort to address VFI using latent
diffusion models, we rigorously benchmark our method on common test sets used
in the existing VFI literature. Our quantitative experiments and user study
indicate that LDMVFI is able to interpolate video content with favorable
perceptual quality compared to the state of the art, even in the
high-resolution regime. Our code is available at
https://github.com/danier97/LDMVFI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.09728">The Cascaded Forward Algorithm for Neural Network Training. (arXiv:2303.09728v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1">Gongpei Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yidong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1">Yi Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lang_C/0/1/0/all/0/1">Congyan Lang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1">Haibin Ling</a></p>
<p>Backpropagation algorithm has been widely used as a mainstream learning
procedure for neural networks in the past decade, and has played a significant
role in the development of deep learning. However, there exist some limitations
associated with this algorithm, such as getting stuck in local minima and
experiencing vanishing/exploding gradients, which have led to questions about
its biological plausibility. To address these limitations, alternative
algorithms to backpropagation have been preliminarily explored, with the
Forward-Forward (FF) algorithm being one of the most well-known. In this paper
we propose a new learning framework for neural networks, namely Cascaded
Forward (CaFo) algorithm, which does not rely on BP optimization as that in FF.
Unlike FF, our framework directly outputs label distributions at each cascaded
block, which does not require generation of additional negative samples and
thus leads to a more efficient process at both training and testing. Moreover,
in our framework each block can be trained independently, so it can be easily
deployed into parallel acceleration systems. The proposed method is evaluated
on four public image classification benchmarks, and the experimental results
illustrate significant improvement in prediction accuracy in comparison with
the baseline.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.10275">MoRF: Mobile Realistic Fullbody Avatars from a Monocular Video. (arXiv:2303.10275v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bashirov_R/0/1/0/all/0/1">Renat Bashirov</a>, <a href="http://arxiv.org/find/cs/1/au:+Larionov_A/0/1/0/all/0/1">Alexey Larionov</a>, <a href="http://arxiv.org/find/cs/1/au:+Ustinova_E/0/1/0/all/0/1">Evgeniya Ustinova</a>, <a href="http://arxiv.org/find/cs/1/au:+Sidorenko_M/0/1/0/all/0/1">Mikhail Sidorenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Svitov_D/0/1/0/all/0/1">David Svitov</a>, <a href="http://arxiv.org/find/cs/1/au:+Zakharkin_I/0/1/0/all/0/1">Ilya Zakharkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lempitsky_V/0/1/0/all/0/1">Victor Lempitsky</a></p>
<p>We present a system to create Mobile Realistic Fullbody (MoRF) avatars. MoRF
avatars are rendered in real-time on mobile devices, learned from monocular
videos, and have high realism. We use SMPL-X as a proxy geometry and render it
with DNR (neural texture and image-2-image network). We improve on prior work,
by overfitting per-frame warping fields in the neural texture space, allowing
to better align the training signal between different frames. We also refine
SMPL-X mesh fitting procedure to improve the overall avatar quality. In the
comparisons to other monocular video-based avatar systems, MoRF avatars achieve
higher image sharpness and temporal consistency. Participants of our user study
also preferred avatars generated by MoRF.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.16078">Relative pose of three calibrated and partially calibrated cameras from four points using virtual correspondences. (arXiv:2303.16078v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tzamos_C/0/1/0/all/0/1">Charalambos Tzamos</a>, <a href="http://arxiv.org/find/cs/1/au:+Barath_D/0/1/0/all/0/1">Daniel Barath</a>, <a href="http://arxiv.org/find/cs/1/au:+Sattler_T/0/1/0/all/0/1">Torsten Sattler</a>, <a href="http://arxiv.org/find/cs/1/au:+Kukelova_Z/0/1/0/all/0/1">Zuzana Kukelova</a></p>
<p>We study challenging problems of estimating the relative pose of three
cameras and propose novel efficient solutions to (1) the notoriously difficult
configuration of four points in three calibrated views, known as the 4p3v
problem, and (2) to the previously unsolved configuration of four points in
three cameras with unknown shared focal length, i.e., the 4p3vf problem. Our
solutions are based on the simple idea of generating one or two additional
virtual point correspondences in two views by using the information from the
locations of the four input correspondences in the three views. We generate
such correspondences using either a very simple and efficient strategy where
the new points are the mean points of three corresponding input points or using
a simple neural network. The new solvers are efficient and easy to implement
since they are based on existing efficient minimal solvers, i.e., the
well-known 5-point and 6-point relative pose solvers and the P3P solver. Our
solvers achieve state-of-the-art results on real data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.00784">Disentangled Pre-training for Image Matting. (arXiv:2304.00784v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yanda Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zilong Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1">Gang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Ling Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1">Yunchao Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1">Jianbo Jiao</a></p>
<p>Image matting requires high-quality pixel-level human annotations to support
the training of a deep model in recent literature. Whereas such annotation is
costly and hard to scale, significantly holding back the development of the
research. In this work, we make the first attempt towards addressing this
problem, by proposing a self-supervised pre-training approach that can leverage
infinite numbers of data to boost the matting performance. The pre-training
task is designed in a similar manner as image matting, where random trimap and
alpha matte are generated to achieve an image disentanglement objective. The
pre-trained model is then used as an initialisation of the downstream matting
task for fine-tuning. Extensive experimental evaluations show that the proposed
approach outperforms both the state-of-the-art matting methods and other
alternative self-supervised initialisation approaches by a large margin. We
also show the robustness of the proposed approach over different backbone
architectures. Our project page is available at
https://crystraldo.github.io/dpt_mat/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.13445">Neural-PBIR Reconstruction of Shape, Material, and Illumination. (arXiv:2304.13445v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1">Cheng Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_G/0/1/0/all/0/1">Guangyan Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhengqin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1">Kai Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Cheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Marshall_C/0/1/0/all/0/1">Carl Marshall</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jia-Bin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1">Shuang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1">Zhao Dong</a></p>
<p>Reconstructing the shape and spatially varying surface appearances of a
physical-world object as well as its surrounding illumination based on 2D
images (e.g., photographs) of the object has been a long-standing problem in
computer vision and graphics. In this paper, we introduce an accurate and
highly efficient object reconstruction pipeline combining neural based object
reconstruction and physics-based inverse rendering (PBIR). Our pipeline firstly
leverages a neural SDF based shape reconstruction to produce high-quality but
potentially imperfect object shape. Then, we introduce a neural material and
lighting distillation stage to achieve high-quality predictions for material
and illumination. In the last stage, initialized by the neural predictions, we
perform PBIR to refine the initial results and obtain the final high-quality
reconstruction of object shape, material, and illumination. Experimental
results demonstrate our pipeline significantly outperforms existing methods
quality-wise and performance-wise.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.02177">Transforming Visual Scene Graphs to Image Captions. (arXiv:2305.02177v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1">Jiawei Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zihua Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Haiyang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1">Qinghao Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chenliang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Songfang Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1">Fei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhangzikang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yu Zhang</a></p>
<p>We propose to Transform Scene Graphs (TSG) into more descriptive captions. In
TSG, we apply multi-head attention (MHA) to design the Graph Neural Network
(GNN) for embedding scene graphs. After embedding, different graph embeddings
contain diverse specific knowledge for generating the words with different
part-of-speech, e.g., object/attribute embedding is good for generating
nouns/adjectives. Motivated by this, we design a Mixture-of-Expert (MOE)-based
decoder, where each expert is built on MHA, for discriminating the graph
embeddings to generate different kinds of words. Since both the encoder and
decoder are built based on the MHA, as a result, we construct a homogeneous
encoder-decoder unlike the previous heterogeneous ones which usually apply
Fully-Connected-based GNN and LSTM-based decoder. The homogeneous architecture
enables us to unify the training configuration of the whole model instead of
specifying different training strategies for diverse sub-networks as in the
heterogeneous pipeline, which releases the training difficulty. Extensive
experiments on the MS-COCO captioning benchmark validate the effectiveness of
our TSG. The code is in: https://github.com/GaryJiajia/TSG.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.06225">DaGAN++: Depth-Aware Generative Adversarial Network for Talking Head Video Generation. (arXiv:2305.06225v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hong_F/0/1/0/all/0/1">Fa-Ting Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1">Li Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1">Dan Xu</a></p>
<p>Predominant techniques on talking head generation largely depend on 2D
information, including facial appearances and motions from input face images.
Nevertheless, dense 3D facial geometry, such as pixel-wise depth, plays a
critical role in constructing accurate 3D facial structures and suppressing
complex background noises for generation. However, dense 3D annotations for
facial videos is prohibitively costly to obtain. In this work, firstly, we
present a novel self-supervised method for learning dense 3D facial geometry
(ie, depth) from face videos, without requiring camera parameters and 3D
geometry annotations in training. We further propose a strategy to learn
pixel-level uncertainties to perceive more reliable rigid-motion pixels for
geometry learning. Secondly, we design an effective geometry-guided facial
keypoint estimation module, providing accurate keypoints for generating motion
fields. Lastly, we develop a 3D-aware cross-modal (ie, appearance and depth)
attention mechanism, which can be applied to each generation layer, to capture
facial geometries in a coarse-to-fine manner. Extensive experiments are
conducted on three challenging benchmarks (ie, VoxCeleb1, VoxCeleb2, and HDTF).
The results demonstrate that our proposed framework can generate highly
realistic-looking reenacted talking videos, with new state-of-the-art
performances established on these benchmarks. The codes and trained models are
publicly available on the GitHub project page at
https://github.com/harlanhong/CVPR2022-DaGAN
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.06324">Alternating Gradient Descent and Mixture-of-Experts for Integrated Multimodal Perception. (arXiv:2305.06324v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Akbari_H/0/1/0/all/0/1">Hassan Akbari</a>, <a href="http://arxiv.org/find/cs/1/au:+Kondratyuk_D/0/1/0/all/0/1">Dan Kondratyuk</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1">Yin Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Hornung_R/0/1/0/all/0/1">Rachel Hornung</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Huisheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Adam_H/0/1/0/all/0/1">Hartwig Adam</a></p>
<p>We present Integrated Multimodal Perception (IMP), a simple and scalable
multimodal multi-task training and modeling approach. IMP integrates multimodal
inputs including image, video, text, and audio into a single Transformer
encoder with minimal modality-specific components. IMP makes use of a novel
design that combines Alternating Gradient Descent (AGD) and Mixture-of-Experts
(MoE) for efficient model and task scaling. We conduct extensive empirical
studies and reveal the following key insights: 1) Performing gradient descent
updates by alternating on diverse modalities, loss functions, and tasks, with
varying input resolutions, efficiently improves the model. 2) Sparsification
with MoE on a single modality-agnostic encoder substantially improves the
performance, outperforming dense models that use modality-specific encoders or
additional fusion layers and greatly mitigates the conflicts between
modalities. IMP achieves competitive performance on a wide range of downstream
tasks including video classification, image classification, image-text, and
video-text retrieval. Most notably, we train a sparse IMP-MoE-L variant
focusing on video tasks that achieves new state-of-the-art in zero-shot video
classification: 77.0% on Kinetics-400, 76.8% on Kinetics-600, and 68.3% on
Kinetics-700, improving the previous state-of-the-art by +5%, +6.7%, and +5.8%,
respectively, while using only 15% of their total training computational cost.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.12032">The Waymo Open Sim Agents Challenge. (arXiv:2305.12032v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Montali_N/0/1/0/all/0/1">Nico Montali</a>, <a href="http://arxiv.org/find/cs/1/au:+Lambert_J/0/1/0/all/0/1">John Lambert</a>, <a href="http://arxiv.org/find/cs/1/au:+Mougin_P/0/1/0/all/0/1">Paul Mougin</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuefler_A/0/1/0/all/0/1">Alex Kuefler</a>, <a href="http://arxiv.org/find/cs/1/au:+Rhinehart_N/0/1/0/all/0/1">Nick Rhinehart</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Michelle Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gulino_C/0/1/0/all/0/1">Cole Gulino</a>, <a href="http://arxiv.org/find/cs/1/au:+Emrich_T/0/1/0/all/0/1">Tristan Emrich</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zoey Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1">Shimon Whiteson</a>, <a href="http://arxiv.org/find/cs/1/au:+White_B/0/1/0/all/0/1">Brandyn White</a>, <a href="http://arxiv.org/find/cs/1/au:+Anguelov_D/0/1/0/all/0/1">Dragomir Anguelov</a></p>
<p>Simulation with realistic, interactive agents represents a key task for
autonomous vehicle software development. In this work, we introduce the Waymo
Open Sim Agents Challenge (WOSAC). WOSAC is the first public challenge to
tackle this task and propose corresponding metrics. The goal of the challenge
is to stimulate the design of realistic simulators that can be used to evaluate
and train a behavior model for autonomous driving. We outline our evaluation
methodology, present results for a number of different baseline simulation
agent methods, and analyze several submissions to the 2023 competition which
ran from March 16, 2023 to May 23, 2023. The WOSAC evaluation server remains
open for submissions and we discuss open problems for the task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.00393">Teacher Agent: A Knowledge Distillation-Free Framework for Rehearsal-based Video Incremental Learning. (arXiv:2306.00393v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1">Shengqin Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1">Yaoyu Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haokui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qingshan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1">Yuankai Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Peng Wang</a></p>
<p>Rehearsal-based video incremental learning often employs knowledge
distillation to mitigate catastrophic forgetting of previously learned data.
However, this method faces two major challenges for video task: substantial
computing resources from loading teacher model and limited replay capability
from performance-limited teacher model. To address these problems, we first
propose a knowledge distillation-free framework for rehearsal-based video
incremental learning called \textit{Teacher Agent}. Instead of loading
parameter-heavy teacher networks, we introduce an agent generator that is
either parameter-free or uses only a few parameters to obtain accurate and
reliable soft labels. This method not only greatly reduces the computing
requirement but also circumvents the problem of knowledge misleading caused by
inaccurate predictions of the teacher model. Moreover, we put forward a
self-correction loss which provides an effective regularization signal for the
review of old knowledge, which in turn alleviates the problem of catastrophic
forgetting. Further, to ensure that the samples in the memory buffer are
memory-efficient and representative, we introduce a unified sampler for
rehearsal-based video incremental learning to mine fixed-length key video
frames. Interestingly, based on the proposed strategies, the network exhibits a
high level of robustness against spatial resolution reduction when compared to
the baseline. Extensive experiments demonstrate the advantages of our method,
yielding significant performance improvements while utilizing only half the
spatial resolution of video clips as network inputs in the incremental phases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.02316">Temporal Dynamic Quantization for Diffusion Models. (arXiv:2306.02316v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+So_J/0/1/0/all/0/1">Junhyuk So</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jungwon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahn_D/0/1/0/all/0/1">Daehyun Ahn</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">Hyungjun Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_E/0/1/0/all/0/1">Eunhyeok Park</a></p>
<p>The diffusion model has gained popularity in vision applications due to its
remarkable generative performance and versatility. However, high storage and
computation demands, resulting from the model size and iterative generation,
hinder its use on mobile devices. Existing quantization techniques struggle to
maintain performance even in 8-bit precision due to the diffusion model's
unique property of temporal variation in activation. We introduce a novel
quantization method that dynamically adjusts the quantization interval based on
time step information, significantly improving output quality. Unlike
conventional dynamic quantization techniques, our approach has no computational
overhead during inference and is compatible with both post-training
quantization (PTQ) and quantization-aware training (QAT). Our extensive
experiments demonstrate substantial improvements in output quality with the
quantized diffusion model across various datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.02956">Explicit Neural Surfaces: Learning Continuous Geometry With Deformation Fields. (arXiv:2306.02956v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Walker_T/0/1/0/all/0/1">Thomas Walker</a>, <a href="http://arxiv.org/find/cs/1/au:+Mariotti_O/0/1/0/all/0/1">Octave Mariotti</a>, <a href="http://arxiv.org/find/cs/1/au:+Vaxman_A/0/1/0/all/0/1">Amir Vaxman</a>, <a href="http://arxiv.org/find/cs/1/au:+Bilen_H/0/1/0/all/0/1">Hakan Bilen</a></p>
<p>We introduce Explicit Neural Surfaces (ENS), an efficient smooth surface
representation that directly encodes topology with a deformation field from a
known base domain. We apply this representation to reconstruct explicit
surfaces from multiple views, where we use a series of neural deformation
fields to progressively transform the base domain into a target shape. By using
meshes as discrete surface proxies, we train the deformation fields through
efficient differentiable rasterization. Using a fixed base domain allows us to
have Laplace-Beltrami eigenfunctions as an intrinsic positional encoding
alongside standard extrinsic Fourier features, with which our approach can
capture fine surface details. Compared to implicit surfaces, ENS trains faster
and has several orders of magnitude faster inference times. The explicit nature
of our approach also allows higher-quality mesh extraction whilst maintaining
competitive surface reconstruction performance and real-time capabilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.04300">CorrMatch: Label Propagation via Correlation Matching for Semi-Supervised Semantic Segmentation. (arXiv:2306.04300v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1">Boyuan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yuqi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Le Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1">Ming-Ming Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1">Qibin Hou</a></p>
<p>This paper presents a simple but performant semi-supervised semantic
segmentation approach, called CorrMatch. Previous approaches mostly employ
complicated training strategies to leverage unlabeled data but overlook the
role of correlation maps in modeling the relationships between pairs of
locations. We observe that the correlation maps not only enable clustering
pixels of the same category easily but also contain good shape information,
which previous works have omitted. Motivated by these, we aim to improve the
use efficiency of unlabeled data by designing two novel label propagation
strategies. First, we propose to conduct pixel propagation by modeling the
pairwise similarities of pixels to spread the high-confidence pixels and dig
out more. Then, we perform region propagation to enhance the pseudo labels with
accurate class-agnostic masks extracted from the correlation maps. CorrMatch
achieves great performance on popular segmentation benchmarks. Taking the
DeepLabV3+ with ResNet-101 backbone as our segmentation model, we receive a
76%+ mIoU score on the Pascal VOC 2012 dataset with only 92 annotated images.
Code is available at https://github.com/BBBBchan/CorrMatch.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.05911">Sketch2Stress: Sketching with Structural Stress Awareness. (arXiv:2306.05911v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1">Deng Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Chufeng Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lau_M/0/1/0/all/0/1">Manfred Lau</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1">Hongbo Fu</a></p>
<p>In the process of product design and digital fabrication, the structural
analysis of a designed prototype is a fundamental and essential step. However,
such a step is usually invisible or inaccessible to designers at the early
sketching phase. This limits the user's ability to consider a shape's physical
properties and structural soundness. To bridge this gap, we introduce a novel
approach Sketch2Stress that allows users to perform structural analysis of
desired objects at the sketching stage. This method takes as input a 2D
freehand sketch and one or multiple locations of user-assigned external forces.
With the specially-designed two-branch generative-adversarial framework, it
automatically predicts a normal map and a corresponding structural stress map
distributed over the user-sketched underlying object. In this way, our method
empowers designers to easily examine the stress sustained everywhere and
identify potential problematic regions of their sketched object. Furthermore,
combined with the predicted normal map, users are able to conduct a region-wise
structural analysis efficiently by aggregating the stress effects of multiple
forces in the same direction. Finally, we demonstrate the effectiveness and
practicality of our system with extensive experiments and user studies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.07894">iSLAM: Imperative SLAM. (arXiv:2306.07894v4 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fu_T/0/1/0/all/0/1">Taimeng Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1">Shaoshu Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yiren Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chen Wang</a></p>
<p>Simultaneous Localization and Mapping (SLAM) stands as one of the critical
challenges in robot navigation. A SLAM system often consists of a front-end
component for motion estimation and a back-end system for eliminating
estimation drift. Recent advancements suggest that data-driven methods are
highly effective for front-end tasks, while geometry-based methods continue to
be essential in the back-end processes. However, such a decoupled paradigm
between the data-driven front-end and geometry-based back-end can lead to
sub-optimal performance, consequently reducing system capabilities and
generalization potential. To solve this problem, we proposed a novel
self-supervised imperative learning framework, named imperative SLAM (iSLAM),
which fosters reciprocal correction between the front-end and back-end, thus
enhancing performance without necessitating any external supervision.
Specifically, we formulate the SLAM problem as a bilevel optimization so that
the front-end and back-end are bidirectionally connected. As a result, the
front-end model can learn global geometric knowledge obtained through pose
graph optimization by back-propagating the residuals from the back-end
component. We showcase the effectiveness of this new framework through an
application of stereo-inertial SLAM. The experiments show that the iSLAM
training strategy achieves an accuracy improvement of 22% on average over a
baseline model. To the best of our knowledge, iSLAM is the first SLAM system
showing that the front-end and back-end can mutually correct each other in a
self-supervised manner.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.09344">DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data. (arXiv:2306.09344v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1">Stephanie Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tamir_N/0/1/0/all/0/1">Netanel Tamir</a>, <a href="http://arxiv.org/find/cs/1/au:+Sundaram_S/0/1/0/all/0/1">Shobhita Sundaram</a>, <a href="http://arxiv.org/find/cs/1/au:+Chai_L/0/1/0/all/0/1">Lucy Chai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Richard Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dekel_T/0/1/0/all/0/1">Tali Dekel</a>, <a href="http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1">Phillip Isola</a></p>
<p>Current perceptual similarity metrics operate at the level of pixels and
patches. These metrics compare images in terms of their low-level colors and
textures, but fail to capture mid-level similarities and differences in image
layout, object pose, and semantic content. In this paper, we develop a
perceptual metric that assesses images holistically. Our first step is to
collect a new dataset of human similarity judgments over image pairs that are
alike in diverse ways. Critical to this dataset is that judgments are nearly
automatic and shared by all observers. To achieve this we use recent
text-to-image models to create synthetic pairs that are perturbed along various
dimensions. We observe that popular perceptual metrics fall short of explaining
our new data, and we introduce a new metric, DreamSim, tuned to better align
with human perception. We analyze how our metric is affected by different
visual attributes, and find that it focuses heavily on foreground objects and
semantic content while also being sensitive to color and layout. Notably,
despite being trained on synthetic data, our metric generalizes to real images,
giving strong results on retrieval and reconstruction tasks. Furthermore, our
metric outperforms both prior learned metrics and recent large vision models on
these tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.11920">NILUT: Conditional Neural Implicit 3D Lookup Tables for Image Enhancement. (arXiv:2306.11920v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Conde_M/0/1/0/all/0/1">Marcos V. Conde</a>, <a href="http://arxiv.org/find/cs/1/au:+Vazquez_Corral_J/0/1/0/all/0/1">Javier Vazquez-Corral</a>, <a href="http://arxiv.org/find/cs/1/au:+Brown_M/0/1/0/all/0/1">Michael S. Brown</a>, <a href="http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1">Radu Timofte</a></p>
<p>3D lookup tables (3D LUTs) are a key component for image enhancement. Modern
image signal processors (ISPs) have dedicated support for these as part of the
camera rendering pipeline. Cameras typically provide multiple options for
picture styles, where each style is usually obtained by applying a unique
handcrafted 3D LUT. Current approaches for learning and applying 3D LUTs are
notably fast, yet not so memory-efficient, as storing multiple 3D LUTs is
required. For this reason and other implementation limitations, their use on
mobile devices is less popular. In this work, we propose a Neural Implicit LUT
(NILUT), an implicitly defined continuous 3D color transformation parameterized
by a neural network. We show that NILUTs are capable of accurately emulating
real 3D LUTs. Moreover, a NILUT can be extended to incorporate multiple styles
into a single network with the ability to blend styles implicitly. Our novel
approach is memory-efficient, controllable and can complement previous methods,
including learned ISPs. Code, models and dataset available at:
https://github.com/mv-lab/nilut
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.12461">On-orbit model training for satellite imagery with label proportions. (arXiv:2306.12461v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ramos_Pollan_R/0/1/0/all/0/1">Ra&#xfa;l Ramos-Poll&#xe1;n</a>, <a href="http://arxiv.org/find/cs/1/au:+Gonzalez_F/0/1/0/all/0/1">Fabio A. Gonz&#xe1;lez</a></p>
<p>This work addresses the challenge of training supervised machine or deep
learning models on orbiting platforms where we are generally constrained by
limited on-board hardware capabilities and restricted uplink bandwidths to
upload. We aim at enabling orbiting spacecrafts to (1) continuously train a
lightweight model as it acquires imagery; and (2) receive new labels while on
orbit to refine or even change the predictive task being trained. For this, we
consider chip level regression tasks (i.e. predicting the vegetation percentage
of a 20 km$^2$ patch) when we only have coarser label proportions, such as
municipality level vegetation statistics (a municipality containing several
patches). Such labels proportions have the additional advantage that usually
come in tabular data and are widely available in many regions of the world and
application areas. This can be framed as a Learning from Label Proportions
(LLP) problem setup. LLP applied to Earth Observation (EO) data is still an
emerging field and performing comparative studies in applied scenarios remains
a challenge due to the lack of standardized datasets. In this work, first, we
show how very simple deep learning and probabilistic methods (with
{\raise.17ex\hbox{$\scriptstyle\sim$}}5K parameters) generally perform better
than standard more complex ones, providing a surprising level of finer grained
spatial detail when trained with much coarser label proportions. Second, we
publish a set of benchmarking datasets enabling comparative LLP applied to EO,
providing both fine grained labels and aggregated data according to existing
administrative divisions. Finally, we show how this approach fits an on-orbit
training scenario by reducing vastly both the amount of computing and the size
of the labels sets. Source code is available at
https://github.com/rramosp/llpeo
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.12562">Neural Spectro-polarimetric Fields. (arXiv:2306.12562v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Youngchan Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1">Wonjoon Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1">Sunghyun Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Baek_S/0/1/0/all/0/1">Seung-Hwan Baek</a></p>
<p>Modeling the spatial radiance distribution of light rays in a scene has been
extensively explored for applications, including view synthesis. Spectrum and
polarization, the wave properties of light, are often neglected due to their
integration into three RGB spectral bands and their non-perceptibility to human
vision. However, these properties are known to encompass substantial material
and geometric information about a scene. Here, we propose to model
spectro-polarimetric fields, the spatial Stokes-vector distribution of any
light ray at an arbitrary wavelength. We present Neural Spectro-polarimetric
Fields (NeSpoF), a neural representation that models the physically-valid
Stokes vector at given continuous variables of position, direction, and
wavelength. NeSpoF manages inherently noisy raw measurements, showcases memory
efficiency, and preserves physically vital signals - factors that are crucial
for representing the high-dimensional signal of a spectro-polarimetric field.
To validate NeSpoF, we introduce the first multi-view
hyperspectral-polarimetric image dataset, comprised of both synthetic and
real-world scenes. These were captured using our compact
hyperspectral-polarimetric imaging system, which has been calibrated for
robustness against system imperfections. We demonstrate the capabilities of
NeSpoF on diverse scenes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.15319">Nano1D: An accurate Computer Vision software for analysis and segmentation of low-dimensional nanostructures. (arXiv:2306.15319v3 [cond-mat.mtrl-sci] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Moradpur_Tari_E/0/1/0/all/0/1">Ehsan Moradpur-Tari</a> (1), <a href="http://arxiv.org/find/cond-mat/1/au:+Vlassov_S/0/1/0/all/0/1">Sergei Vlassov</a> (1,2), <a href="http://arxiv.org/find/cond-mat/1/au:+Oras_S/0/1/0/all/0/1">Sven Oras</a> (1,2), <a href="http://arxiv.org/find/cond-mat/1/au:+Ernits_M/0/1/0/all/0/1">Mart Ernits</a> (1), <a href="http://arxiv.org/find/cond-mat/1/au:+Damerchi_E/0/1/0/all/0/1">Elyad Damerchi</a> (1), <a href="http://arxiv.org/find/cond-mat/1/au:+Polyakovc_B/0/1/0/all/0/1">Boris Polyakovc</a> (3), <a href="http://arxiv.org/find/cond-mat/1/au:+Kyritsakis_A/0/1/0/all/0/1">Andreas Kyritsakis</a> (1), <a href="http://arxiv.org/find/cond-mat/1/au:+Zadin_V/0/1/0/all/0/1">Veronika Zadin</a> (1) ((1) Institute of Technology, University of Tartu, Nooruse 1, 50411 Tartu, Estonia (2) Institute of Physics, University of Tartu, W. Ostwaldi 1, 50411 Tartu, Estonia (3) Institute of Solid State Physics, University of Latvia, Kengaraga street 8, LV-1063 Riga, Latvia)</p>
<p>Nanoparticles in microscopy images are usually analyzed qualitatively or
manually and there is a need for autonomous quantitative analysis of these
objects. In this paper, we present a physics-based computational model for
accurate segmentation and geometrical analysis of one-dimensional deformable
overlapping objects from microscopy images. This model, named Nano1D, has four
steps of preprocessing, segmentation, separating overlapped objects and
geometrical measurements. The model is tested on SEM images of Ag and Au
nanowire taken from different microscopes, and thermally fragmented Ag
nanowires transformed into nanoparticles with different lengths, diameters, and
population densities. It successfully segments and analyzes their geometrical
characteristics including lengths and average diameter. The function of the
algorithm is not undermined by the size, number, density, orientation and
overlapping of objects in images. The main strength of the model is shown to be
its ability to segment and analyze overlapping objects successfully with more
than 99% accuracy, while current machine learning and computational models
suffer from inaccuracy and inability to segment overlapping objects. Benefiting
from a graphical user interface, Nano1D can analyze 1D nanoparticles including
nanowires, nanotubes, nanorods in addition to other 1D features of
microstructures like microcracks, dislocations etc.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.00371">Learning Content-enhanced Mask Transformer for Domain Generalized Urban-Scene Segmentation. (arXiv:2307.00371v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bi_Q/0/1/0/all/0/1">Qi Bi</a>, <a href="http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1">Shaodi You</a>, <a href="http://arxiv.org/find/cs/1/au:+Gevers_T/0/1/0/all/0/1">Theo Gevers</a></p>
<p>Domain-generalized urban-scene semantic segmentation (USSS) aims to learn
generalized semantic predictions across diverse urban-scene styles. Unlike
domain gap challenges, USSS is unique in that the semantic categories are often
similar in different urban scenes, while the styles can vary significantly due
to changes in urban landscapes, weather conditions, lighting, and other
factors. Existing approaches typically rely on convolutional neural networks
(CNNs) to learn the content of urban scenes.
</p>
<p>In this paper, we propose a Content-enhanced Mask TransFormer (CMFormer) for
domain-generalized USSS. The main idea is to enhance the focus of the
fundamental component, the mask attention mechanism, in Transformer
segmentation models on content information. To achieve this, we introduce a
novel content-enhanced mask attention mechanism. It learns mask queries from
both the image feature and its down-sampled counterpart, as lower-resolution
image features usually contain more robust content information and are less
sensitive to style variations. These features are fused into a Transformer
decoder and integrated into a multi-resolution content-enhanced mask attention
learning scheme.
</p>
<p>Extensive experiments conducted on various domain-generalized urban-scene
segmentation datasets demonstrate that the proposed CMFormer significantly
outperforms existing CNN-based methods for domain-generalized semantic
segmentation, achieving improvements of up to 14.00\% in terms of mIoU (mean
intersection over union). The source code is publicly available at
\url{https://github.com/BiQiWHU/CMFormer}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.00553">Partial-label Learning with Mixed Closed-set and Open-set Out-of-candidate Examples. (arXiv:2307.00553v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1">Shuo He</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1">Lei Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1">Guowu Yang</a></p>
<p>Partial-label learning (PLL) relies on a key assumption that the true label
of each training example must be in the candidate label set. This restrictive
assumption may be violated in complex real-world scenarios, and thus the true
label of some collected examples could be unexpectedly outside the assigned
candidate label set. In this paper, we term the examples whose true label is
outside the candidate label set OOC (out-of-candidate) examples, and pioneer a
new PLL study to learn with OOC examples. We consider two types of OOC examples
in reality, i.e., the closed-set/open-set OOC examples whose true label is
inside/outside the known label space. To solve this new PLL problem, we first
calculate the wooden cross-entropy loss from candidate and non-candidate labels
respectively, and dynamically differentiate the two types of OOC examples based
on specially designed criteria. Then, for closed-set OOC examples, we conduct
reversed label disambiguation in the non-candidate label set; for open-set OOC
examples, we leverage them for training by utilizing an effective
regularization strategy that dynamically assigns random candidate labels from
the candidate label set. In this way, the two types of OOC examples can be
differentiated and further leveraged for model training. Extensive experiments
demonstrate that our proposed method outperforms state-of-the-art PLL methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06751">Watch Where You Head: A View-biased Domain Gap in Gait Recognition and Unsupervised Adaptation. (arXiv:2307.06751v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Habib_G/0/1/0/all/0/1">Gavriel Habib</a>, <a href="http://arxiv.org/find/cs/1/au:+Barzilay_N/0/1/0/all/0/1">Noa Barzilay</a>, <a href="http://arxiv.org/find/cs/1/au:+Shimshi_O/0/1/0/all/0/1">Or Shimshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ben_Ari_R/0/1/0/all/0/1">Rami Ben-Ari</a>, <a href="http://arxiv.org/find/cs/1/au:+Darshan_N/0/1/0/all/0/1">Nir Darshan</a></p>
<p>Gait Recognition is a computer vision task aiming to identify people by their
walking patterns. Although existing methods often show high performance on
specific datasets, they lack the ability to generalize to unseen scenarios.
Unsupervised Domain Adaptation (UDA) tries to adapt a model, pre-trained in a
supervised manner on a source domain, to an unlabelled target domain. There are
only a few works on UDA for gait recognition proposing solutions to limited
scenarios. In this paper, we reveal a fundamental phenomenon in adaptation of
gait recognition models, caused by the bias in the target domain to viewing
angle or walking direction. We then suggest a remedy to reduce this bias with a
novel triplet selection strategy combined with curriculum learning. To this
end, we present Gait Orientation-based method for Unsupervised Domain
Adaptation (GOUDA). We provide extensive experiments on four widely-used gait
datasets, CASIA-B, OU-MVLP, GREW, and Gait3D, and on three backbones, GaitSet,
GaitPart, and GaitGL, justifying the view bias and showing the superiority of
our proposed method over prior UDA works.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07976">HRHD-HK: A benchmark dataset of high-rise and high-density urban scenes for 3D semantic segmentation of photogrammetric point clouds. (arXiv:2307.07976v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Maosu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yijie Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yeh_A/0/1/0/all/0/1">Anthony G.O. Yeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1">Fan Xue</a></p>
<p>Many existing 3D semantic segmentation methods, deep learning in computer
vision notably, claimed to achieve desired results on urban point clouds. Thus,
it is significant to assess these methods quantitatively in diversified
real-world urban scenes, encompassing high-rise, low-rise, high-density, and
low-density urban areas. However, existing public benchmark datasets primarily
represent low-rise scenes from European cities and cannot assess the methods
comprehensively. This paper presents a benchmark dataset of high-rise urban
point clouds, namely High-Rise, High-Density urban scenes of Hong Kong
(HRHD-HK). HRHD-HK arranged in 150 tiles contains 273 million colorful
photogrammetric 3D points from diverse urban settings. The semantic labels of
HRHD-HK include building, vegetation, road, waterbody, facility, terrain, and
vehicle. To our best knowledge, HRHD-HK is the first photogrammetric dataset
that focuses on HRHD urban areas. This paper also comprehensively evaluates
eight popular semantic segmentation methods on the HRHD-HK dataset.
Experimental results confirmed plenty of room for enhancing the current 3D
semantic segmentation of point clouds, especially for city objects with small
volumes. Our dataset is publicly available at
https://doi.org/10.25442/hku.23701866.v2.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08939">Runtime Stealthy Perception Attacks against DNN-based Adaptive Cruise Control Systems. (arXiv:2307.08939v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xugui Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1">Anqi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Kouzel_M/0/1/0/all/0/1">Maxfield Kouzel</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1">Haotian Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+McCarty_M/0/1/0/all/0/1">Morgan McCarty</a>, <a href="http://arxiv.org/find/cs/1/au:+Nita_Rotaru_C/0/1/0/all/0/1">Cristina Nita-Rotaru</a>, <a href="http://arxiv.org/find/cs/1/au:+Alemzadeh_H/0/1/0/all/0/1">Homa Alemzadeh</a></p>
<p>Adaptive Cruise Control (ACC) is a widely used driver assistance technology
for maintaining the desired speed and safe distance to the leading vehicle.
This paper evaluates the security of the deep neural network (DNN) based ACC
systems under runtime stealthy perception attacks that strategically inject
perturbations into camera data to cause forward collisions. We present a
context-aware strategy for the selection of the most critical times for
triggering the attacks and a novel optimization-based method for the adaptive
generation of image perturbations at runtime. We evaluate the effectiveness of
the proposed attack using a publicly available driving dataset, an actual
vehicle, and a realistic simulation platform with the control software from a
production ACC system, a physical-world driving simulator, and interventions by
the human driver and safety features such as Advanced Emergency Braking System
(AEBS). Experimental results show that the proposed attack achieves 142.9 times
higher success rate in causing hazards and 89.6% higher evasion rate than
baselines while being stealthy and robust to real-world factors and dynamic
changes in the environment. This study highlights the role of human drivers and
basic safety mechanisms in preventing attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09368">Audio-driven Talking Face Generation by Overcoming Unintended Information Flow. (arXiv:2307.09368v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yaman_D/0/1/0/all/0/1">Dogucan Yaman</a>, <a href="http://arxiv.org/find/cs/1/au:+Eyiokur_F/0/1/0/all/0/1">Fevziye Irem Eyiokur</a>, <a href="http://arxiv.org/find/cs/1/au:+Barmann_L/0/1/0/all/0/1">Leonard B&#xe4;rmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Ekenel_H/0/1/0/all/0/1">Hazim Kemal Ekenel</a>, <a href="http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1">Alexander Waibel</a></p>
<p>Audio-driven talking face generation is the task of creating a
lip-synchronized, realistic face video from given audio and reference frames.
This involves two major challenges: overall visual quality of generated images
on the one hand, and audio-visual synchronization of the mouth part on the
other hand. In this paper, we start by identifying several problematic aspects
of synchronization methods in recent audio-driven talking face generation
approaches. Specifically, this involves unintended flow of lip, pose and other
information from the reference to the generated image, as well as instabilities
during model training. Subsequently, we propose various techniques for
obviating these issues: First, a silent-lip reference image generator prevents
leaking of lips from the reference to the generated image. Second, an adaptive
triplet loss handles the pose leaking problem. Finally, we propose a stabilized
formulation of synchronization loss, circumventing aforementioned training
instabilities while additionally further alleviating the lip leaking issue.
Combining the individual improvements, we present state-of-the-art visual
quality and synchronization performance on LRS2 in five out of seven and LRW in
six out of seven metrics, and competitive results on the remaining ones. We
further validate our design in various ablation experiments, confirming the
individual contributions as well as their complementary effects.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.00428">Multiscale Feature Learning Using Co-Tuplet Loss for Offline Handwritten Signature Verification. (arXiv:2308.00428v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1">Fu-Hsien Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1">Hsin-Min Lu</a></p>
<p>Handwritten signature verification, crucial for legal and financial
institutions, faces challenges including inter-writer similarity, intra-writer
variations, and limited signature samples. To address these, we introduce a
MultiScale Signature feature learning Network (MS-SigNet) with a novel metric
learning loss called the co-tuplet loss, designed for offline handwritten
signature verification. MS-SigNet learns both global and regional signature
features from multiple spatial scales, enhancing feature discrimination. This
approach effectively distinguishes genuine signatures from skilled forgeries by
capturing overall strokes and detailed local differences. The co-tuplet loss,
focusing on multiple positive and negative examples, overcomes the limitations
of typical metric learning losses by addressing inter-writer similarity and
intra-writer variations and emphasizing informative examples. We also present
HanSig, a large-scale Chinese signature dataset (available at
https://github.com/ashleyfhh/HanSig) to support robust system development.
Experimental results on four benchmark datasets in different languages
demonstrate the promising performance of our method in comparison to
state-of-the-art approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.08479">DeDoDe: Detect, Don&#x27;t Describe -- Describe, Don&#x27;t Detect for Local Feature Matching. (arXiv:2308.08479v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Edstedt_J/0/1/0/all/0/1">Johan Edstedt</a>, <a href="http://arxiv.org/find/cs/1/au:+Bokman_G/0/1/0/all/0/1">Georg B&#xf6;kman</a>, <a href="http://arxiv.org/find/cs/1/au:+Wadenback_M/0/1/0/all/0/1">M&#xe5;rten Wadenb&#xe4;ck</a>, <a href="http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1">Michael Felsberg</a></p>
<p>Keypoint detection is a pivotal step in 3D reconstruction, whereby sets of
(up to) K points are detected in each view of a scene. Crucially, the detected
points need to be consistent between views, i.e., correspond to the same 3D
point in the scene. One of the main challenges with keypoint detection is the
formulation of the learning objective. Previous learning-based methods
typically jointly learn descriptors with keypoints, and treat the keypoint
detection as a binary classification task on mutual nearest neighbours.
However, basing keypoint detection on descriptor nearest neighbours is a proxy
task, which is not guaranteed to produce 3D-consistent keypoints. Furthermore,
this ties the keypoints to a specific descriptor, complicating downstream
usage. In this work, we instead learn keypoints directly from 3D consistency.
To this end, we train the detector to detect tracks from large-scale SfM. As
these points are often overly sparse, we derive a semi-supervised two-view
detection objective to expand this set to a desired number of detections. To
train a descriptor, we maximize the mutual nearest neighbour objective over the
keypoints with a separate network. Results show that our approach, DeDoDe,
achieves significant gains on multiple geometry benchmarks. Code is provided at
https://github.com/Parskatt/DeDoDe
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.09300">V2A-Mapper: A Lightweight Solution for Vision-to-Audio Generation by Connecting Foundation Models. (arXiv:2308.09300v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Heng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1">Jianbo Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Pascual_S/0/1/0/all/0/1">Santiago Pascual</a>, <a href="http://arxiv.org/find/cs/1/au:+Cartwright_R/0/1/0/all/0/1">Richard Cartwright</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1">Weidong Cai</a></p>
<p>Building artificial intelligence (AI) systems on top of a set of foundation
models (FMs) is becoming a new paradigm in AI research. Their representative
and generative abilities learnt from vast amounts of data can be easily adapted
and transferred to a wide range of downstream tasks without extra training from
scratch. However, leveraging FMs in cross-modal generation remains
under-researched when audio modality is involved. On the other hand,
automatically generating semantically-relevant sound from visual input is an
important problem in cross-modal generation studies. To solve this
vision-to-audio (V2A) generation problem, existing methods tend to design and
build complex systems from scratch using modestly sized datasets. In this
paper, we propose a lightweight solution to this problem by leveraging
foundation models, specifically CLIP, CLAP, and AudioLDM. We first investigate
the domain gap between the latent space of the visual CLIP and the auditory
CLAP models. Then we propose a simple yet effective mapper mechanism
(V2A-Mapper) to bridge the domain gap by translating the visual input between
CLIP and CLAP spaces. Conditioned on the translated CLAP embedding, pretrained
audio generative FM AudioLDM is adopted to produce high-fidelity and
visually-aligned sound. Compared to previous approaches, our method only
requires a quick training of the V2A-Mapper. We further analyze and conduct
extensive experiments on the choice of the V2A-Mapper and show that a
generative mapper is better at fidelity and variability (FD) while a regression
mapper is slightly better at relevance (CS). Both objective and subjective
evaluation on two V2A datasets demonstrate the superiority of our proposed
method compared to current state-of-the-art approaches - trained with 86% fewer
parameters but achieving 53% and 19% improvement in FD and CS, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.11070">Temporal-Distributed Backdoor Attack Against Video Based Action Recognition. (arXiv:2308.11070v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Songhe Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1">Ruiquan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gowda_M/0/1/0/all/0/1">Mahanth Gowda</a>, <a href="http://arxiv.org/find/cs/1/au:+Kesidis_G/0/1/0/all/0/1">George Kesidis</a></p>
<p>Deep neural networks (DNNs) have achieved tremendous success in various
applications including video action recognition, yet remain vulnerable to
backdoor attacks (Trojans). The backdoor-compromised model will mis-classify to
the target class chosen by the attacker when a test instance (from a non-target
class) is embedded with a specific trigger, while maintaining high accuracy on
attack-free instances. Although there are extensive studies on backdoor attacks
against image data, the susceptibility of video-based systems under backdoor
attacks remains largely unexplored. Current studies are direct extensions of
approaches proposed for image data, e.g., the triggers are independently
embedded within the frames, which tend to be detectable by existing defenses.
In this paper, we introduce a simple yet effective backdoor attack against
video data. Our proposed attack, adding perturbations in a transformed domain,
plants an imperceptible, temporally distributed trigger across the video
frames, and is shown to be resilient to existing defensive strategies. The
effectiveness of the proposed attack is demonstrated by extensive experiments
with various well-known models on two video recognition benchmarks, UCF101 and
HMDB51, and a sign language recognition benchmark, Greek Sign Language (GSL)
dataset. We delve into the impact of several influential factors on our
proposed attack and identify an intriguing effect termed "collateral damage"
through extensive studies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12609">Cross-Video Contextual Knowledge Exploration and Exploitation for Ambiguity Reduction in Weakly Supervised Temporal Action Localization. (arXiv:2308.12609v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Songchun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1">Chunhui Zhao</a></p>
<p>Weakly supervised temporal action localization (WSTAL) aims to localize
actions in untrimmed videos using video-level labels. Despite recent advances,
existing approaches mainly follow a localization-by-classification pipeline,
generally processing each segment individually, thereby exploiting only limited
contextual information. As a result, the model will lack a comprehensive
understanding (e.g. appearance and temporal structure) of various action
patterns, leading to ambiguity in classification learning and temporal
localization. Our work addresses this from a novel perspective, by exploring
and exploiting the cross-video contextual knowledge within the dataset to
recover the dataset-level semantic structure of action instances via weak
labels only, thereby indirectly improving the holistic understanding of
fine-grained action patterns and alleviating the aforementioned ambiguities.
Specifically, an end-to-end framework is proposed, including a Robust
Memory-Guided Contrastive Learning (RMGCL) module and a Global Knowledge
Summarization and Aggregation (GKSA) module. First, the RMGCL module explores
the contrast and consistency of cross-video action features, assisting in
learning more structured and compact embedding space, thus reducing ambiguity
in classification learning. Further, the GKSA module is used to efficiently
summarize and propagate the cross-video representative action knowledge in a
learnable manner to promote holistic action patterns understanding, which in
turn allows the generation of high-confidence pseudo-labels for self-learning,
thus alleviating ambiguity in temporal localization. Extensive experiments on
THUMOS14, ActivityNet1.3, and FineAction demonstrate that our method
outperforms the state-of-the-art methods, and can be easily plugged into other
WSTAL methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.13794">SOGDet: Semantic-Occupancy Guided Multi-view 3D Object Detection. (arXiv:2308.13794v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1">Qiu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1">Jinming Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Leng_H/0/1/0/all/0/1">Hanchao Leng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1">Yifang Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Kun_Y/0/1/0/all/0/1">Yu Kun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1">Roger Zimmermann</a></p>
<p>In the field of autonomous driving, accurate and comprehensive perception of
the 3D environment is crucial. Bird's Eye View (BEV) based methods have emerged
as a promising solution for 3D object detection using multi-view images as
input. However, existing 3D object detection methods often ignore the physical
context in the environment, such as sidewalk and vegetation, resulting in
sub-optimal performance. In this paper, we propose a novel approach called
SOGDet (Semantic-Occupancy Guided Multi-view 3D Object Detection), that
leverages a 3D semantic-occupancy branch to improve the accuracy of 3D object
detection. In particular, the physical context modeled by semantic occupancy
helps the detector to perceive the scenes in a more holistic view. Our SOGDet
is flexible to use and can be seamlessly integrated with most existing
BEV-based methods. To evaluate its effectiveness, we apply this approach to
several state-of-the-art baselines and conduct extensive experiments on the
exclusive nuScenes dataset. Our results show that SOGDet consistently enhance
the performance of three baseline methods in terms of nuScenes Detection Score
(NDS) and mean Average Precision (mAP). This indicates that the combination of
3D object detection and 3D semantic occupancy leads to a more comprehensive
perception of the 3D environment, thereby aiding build more robust autonomous
driving systems. The codes are available at: https://github.com/zhouqiu/SOGDet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16139">MedShapeNet -- A Large-Scale Dataset of 3D Medical Shapes for Computer Vision. (arXiv:2308.16139v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jianning Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zongwei Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jiancheng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pepe_A/0/1/0/all/0/1">Antonio Pepe</a>, <a href="http://arxiv.org/find/cs/1/au:+Gsaxner_C/0/1/0/all/0/1">Christina Gsaxner</a>, <a href="http://arxiv.org/find/cs/1/au:+Luijten_G/0/1/0/all/0/1">Gijs Luijten</a>, <a href="http://arxiv.org/find/cs/1/au:+Qu_C/0/1/0/all/0/1">Chongyu Qu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tiezheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xiaoxi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wenxuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wodzinski_M/0/1/0/all/0/1">Marek Wodzinski</a>, <a href="http://arxiv.org/find/cs/1/au:+Friedrich_P/0/1/0/all/0/1">Paul Friedrich</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_K/0/1/0/all/0/1">Kangxian Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1">Yuan Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ambigapathy_N/0/1/0/all/0/1">Narmada Ambigapathy</a>, <a href="http://arxiv.org/find/cs/1/au:+Nasca_E/0/1/0/all/0/1">Enrico Nasca</a>, <a href="http://arxiv.org/find/cs/1/au:+Solak_N/0/1/0/all/0/1">Naida Solak</a>, <a href="http://arxiv.org/find/cs/1/au:+Melito_G/0/1/0/all/0/1">Gian Marco Melito</a>, <a href="http://arxiv.org/find/cs/1/au:+Vu_V/0/1/0/all/0/1">Viet Duc Vu</a>, <a href="http://arxiv.org/find/cs/1/au:+Memon_A/0/1/0/all/0/1">Afaque R. Memon</a>, <a href="http://arxiv.org/find/cs/1/au:+Schlachta_C/0/1/0/all/0/1">Christopher Schlachta</a>, <a href="http://arxiv.org/find/cs/1/au:+Ribaupierre_S/0/1/0/all/0/1">Sandrine De Ribaupierre</a>, <a href="http://arxiv.org/find/cs/1/au:+Patel_R/0/1/0/all/0/1">Rajnikant Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Eagleson_R/0/1/0/all/0/1">Roy Eagleson</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xiaojun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Machler_H/0/1/0/all/0/1">Heinrich M&#xe4;chler</a>, <a href="http://arxiv.org/find/cs/1/au:+Kirschke_J/0/1/0/all/0/1">Jan Stefan Kirschke</a>, <a href="http://arxiv.org/find/cs/1/au:+Rosa_E/0/1/0/all/0/1">Ezequiel de la Rosa</a>, <a href="http://arxiv.org/find/cs/1/au:+Christ_P/0/1/0/all/0/1">Patrick Ferdinand Christ</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hongwei Bran Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ellis_D/0/1/0/all/0/1">David G. Ellis</a>, <a href="http://arxiv.org/find/cs/1/au:+Aizenberg_M/0/1/0/all/0/1">Michele R. Aizenberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Gatidis_S/0/1/0/all/0/1">Sergios Gatidis</a>, <a href="http://arxiv.org/find/cs/1/au:+Kustner_T/0/1/0/all/0/1">Thomas K&#xfc;stner</a>, <a href="http://arxiv.org/find/cs/1/au:+Shusharina_N/0/1/0/all/0/1">Nadya Shusharina</a>, <a href="http://arxiv.org/find/cs/1/au:+Heller_N/0/1/0/all/0/1">Nicholas Heller</a>, <a href="http://arxiv.org/find/cs/1/au:+Andrearczyk_V/0/1/0/all/0/1">Vincent Andrearczyk</a>, <a href="http://arxiv.org/find/cs/1/au:+Depeursinge_A/0/1/0/all/0/1">Adrien Depeursinge</a>, <a href="http://arxiv.org/find/cs/1/au:+Hatt_M/0/1/0/all/0/1">Mathieu Hatt</a>, <a href="http://arxiv.org/find/cs/1/au:+Sekuboyina_A/0/1/0/all/0/1">Anjany Sekuboyina</a>, <a href="http://arxiv.org/find/cs/1/au:+Loffler_M/0/1/0/all/0/1">Maximilian L&#xf6;ffler</a>, <a href="http://arxiv.org/find/cs/1/au:+Liebl_H/0/1/0/all/0/1">Hans Liebl</a>, <a href="http://arxiv.org/find/cs/1/au:+Dorent_R/0/1/0/all/0/1">Reuben Dorent</a>, <a href="http://arxiv.org/find/cs/1/au:+Vercauteren_T/0/1/0/all/0/1">Tom Vercauteren</a>, <a href="http://arxiv.org/find/cs/1/au:+Shapey_J/0/1/0/all/0/1">Jonathan Shapey</a>, <a href="http://arxiv.org/find/cs/1/au:+Kujawa_A/0/1/0/all/0/1">Aaron Kujawa</a>, <a href="http://arxiv.org/find/cs/1/au:+Cornelissen_S/0/1/0/all/0/1">Stefan Cornelissen</a>, et al. (110 additional authors not shown)</p>
<p>Prior to the deep learning era, \textit{shape} was commonly used to describe
the objects. Nowadays, state-of-the-art (SOTA) algorithms in medical imaging
are predominantly diverging from computer vision, where voxel grids, meshes,
point clouds, and implicit surface models are used. This is seen from numerous
shape-related publications in premier vision conferences as well as the growing
popularity of \textit{ShapeNet} (about 51,300 models) and \textit{Princeton
ModelNet} (127,915 models). For the medical domain, we present a large
collection of anatomical shapes (e.g., bones, organs, vessels) and 3D models of
surgical instrument, called \textit{MedShapeNet}, created to facilitate the
translation of data-driven vision algorithms to medical applications and to
adapt SOTA vision algorithms to medical problems. As a unique feature, we
directly model the majority of shapes on the imaging data of real patients. As
of today, \textit{MedShapeNet} includes 23 dataset with more than 100,000
shapes that are paired with annotations (ground truth). Our data is freely
accessible via a web interface and a Python application programming interface
(API) and can be used for discriminative, reconstructive, and variational
benchmarks as well as various applications in virtual, augmented, or mixed
reality, and 3D printing. Exemplary, we present use cases in the fields of
classification of brain tumors, facial and skull reconstructions, multi-class
anatomy completion, education, and 3D printing. In future, we will extend the
data and improve the interfaces. The project pages are:
\url{https://medshapenet.ikim.nrw/} and
\url{https://github.com/Jianningli/medshapenet-feedback}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.01627">Cross-Consistent Deep Unfolding Network for Adaptive All-In-One Video Restoration. (arXiv:2309.01627v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1">Yuanshuo Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_M/0/1/0/all/0/1">Mingwen Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1">Yecong Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yuanjian Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1">Wangmeng Zuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1">Deyu Meng</a></p>
<p>Existing Video Restoration (VR) methods always necessitate the individual
deployment of models for each adverse weather to remove diverse adverse weather
degradations, lacking the capability for adaptive processing of degradations.
Such limitation amplifies the complexity and deployment costs in practical
applications. To overcome this deficiency, in this paper, we propose a
Cross-consistent Deep Unfolding Network (CDUN) for All-In-One VR, which enables
the employment of a single model to remove diverse degradations for the first
time. Specifically, the proposed CDUN accomplishes a novel iterative
optimization framework, capable of restoring frames corrupted by corresponding
degradations according to the degradation features given in advance. To empower
the framework for eliminating diverse degradations, we devise a Sequence-wise
Adaptive Degradation Estimator (SADE) to estimate degradation features for the
input corrupted video. By orchestrating these two cascading procedures, CDUN
achieves adaptive processing for diverse degradation. In addition, we introduce
a window-based inter-frame fusion strategy to utilize information from more
adjacent frames. This strategy involves the progressive stacking of temporal
windows in multiple iterations, effectively enlarging the temporal receptive
field and enabling each frame's restoration to leverage information from
distant frames. Extensive experiments demonstrate that the proposed method
achieves state-of-the-art performance in All-In-One VR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.02340">Generating Infinite-Size Textures using GANs with Patch-by-Patch Paradigm. (arXiv:2309.02340v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abdellatif_A/0/1/0/all/0/1">Alhasan Abdellatif</a>, <a href="http://arxiv.org/find/cs/1/au:+Elsheikh_A/0/1/0/all/0/1">Ahmed H. Elsheikh</a></p>
<p>In this paper, we introduce a novel approach for generating texture images of
infinite sizes using Generative Adversarial Networks (GANs) based on a
patch-by-patch paradigm. Existing texture synthesis techniques rely on
generating large-scale textures using a single forward pass to the generative
model; this approach limits the scalability and flexibility of the images
produced. In contrast, the proposed approach trains a GAN model on a single
texture image to generate relatively small-size patches that are locally
correlated and can be seamlessly concatenated to form a larger image. The
method relies on local padding in the generator to ensure consistency between
the generated patches. It also utilizes spatial stochastic modulation to allow
for local variations and improve patterns alignment in the large-scale image.
The trained models learn the local texture structure and are able to generate
images of arbitrary sizes, while also maintaining the coherence and diversity.
Experimental results demonstrate constant GPU scalability with respect to the
generated image size compared to existing approaches that exhibit a
proportional growth in GPU memory.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.02356">STEP -- Towards Structured Scene-Text Spotting. (arXiv:2309.02356v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Garcia_Bordils_S/0/1/0/all/0/1">Sergi Garcia-Bordils</a>, <a href="http://arxiv.org/find/cs/1/au:+Karatzas_D/0/1/0/all/0/1">Dimosthenis Karatzas</a>, <a href="http://arxiv.org/find/cs/1/au:+Rusinol_M/0/1/0/all/0/1">Mar&#xe7;al Rusi&#xf1;ol</a></p>
<p>We introduce the structured scene-text spotting task, which requires a
scene-text OCR system to spot text in the wild according to a query regular
expression. Contrary to generic scene text OCR, structured scene-text spotting
seeks to dynamically condition both scene text detection and recognition on
user-provided regular expressions. To tackle this task, we propose the
Structured TExt sPotter (STEP), a model that exploits the provided text
structure to guide the OCR process. STEP is able to deal with regular
expressions that contain spaces and it is not bound to detection at the
word-level granularity. Our approach enables accurate zero-shot structured text
spotting in a wide variety of real-world reading scenarios and is solely
trained on publicly available data. To demonstrate the effectiveness of our
approach, we introduce a new challenging test dataset that contains several
types of out-of-vocabulary structured text, reflecting important reading
applications of fields such as prices, dates, serial numbers, license plates
etc. We demonstrate that STEP can provide specialised OCR performance on demand
in all tested scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.09431">FactoFormer: Factorized Hyperspectral Transformers with Self-Supervised Pre-Training. (arXiv:2309.09431v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mohamed_S/0/1/0/all/0/1">Shaheer Mohamed</a>, <a href="http://arxiv.org/find/cs/1/au:+Haghighat_M/0/1/0/all/0/1">Maryam Haghighat</a>, <a href="http://arxiv.org/find/cs/1/au:+Fernando_T/0/1/0/all/0/1">Tharindu Fernando</a>, <a href="http://arxiv.org/find/cs/1/au:+Sridharan_S/0/1/0/all/0/1">Sridha Sridharan</a>, <a href="http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1">Clinton Fookes</a>, <a href="http://arxiv.org/find/cs/1/au:+Moghadam_P/0/1/0/all/0/1">Peyman Moghadam</a></p>
<p>Hyperspectral images (HSIs) contain rich spectral and spatial information.
Motivated by the success of transformers in the field of natural language
processing and computer vision where they have shown the ability to learn long
range dependencies within input data, recent research has focused on using
transformers for HSIs. However, current state-of-the-art hyperspectral
transformers only tokenize the input HSI sample along the spectral dimension,
resulting in the under-utilization of spatial information. Moreover,
transformers are known to be data-hungry and their performance relies heavily
on large-scale pre-training, which is challenging due to limited annotated
hyperspectral data. Therefore, the full potential of HSI transformers has not
been fully realized. To overcome these limitations, we propose a novel
factorized spectral-spatial transformer that incorporates factorized
self-supervised pre-training procedures, leading to significant improvements in
performance. The factorization of the inputs allows the spectral and spatial
transformers to better capture the interactions within the hyperspectral data
cubes. Inspired by masked image modeling pre-training, we also devise efficient
masking strategies for pre-training each of the spectral and spatial
transformers. We conduct experiments on six publicly available datasets for HSI
classification task and demonstrate that our model achieves state-of-the-art
performance in all the datasets. The code for our model will be made available
at https://github.com/csiro-robotics/factoformer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.13237">Spatial-Temporal Knowledge-Embedded Transformer for Video Scene Graph Generation. (arXiv:2309.13237v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pu_T/0/1/0/all/0/1">Tao Pu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tianshui Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Hefeng Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yongyi Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1">Liang Lin</a></p>
<p>Video scene graph generation (VidSGG) aims to identify objects in visual
scenes and infer their relationships for a given video. It requires not only a
comprehensive understanding of each object scattered on the whole scene but
also a deep dive into their temporal motions and interactions. Inherently,
object pairs and their relationships enjoy spatial co-occurrence correlations
within each image and temporal consistency/transition correlations across
different images, which can serve as prior knowledge to facilitate VidSGG model
learning and inference. In this work, we propose a spatial-temporal
knowledge-embedded transformer (STKET) that incorporates the prior
spatial-temporal knowledge into the multi-head cross-attention mechanism to
learn more representative relationship representations. Specifically, we first
learn spatial co-occurrence and temporal transition correlations in a
statistical manner. Then, we design spatial and temporal knowledge-embedded
layers that introduce the multi-head cross-attention mechanism to fully explore
the interaction between visual representation and the knowledge to generate
spatial- and temporal-embedded representations, respectively. Finally, we
aggregate these representations for each subject-object pair to predict the
final semantic labels and their relationships. Extensive experiments show that
STKET outperforms current competing algorithms by a large margin, e.g.,
improving the mR@50 by 8.1%, 4.7%, and 2.1% on different settings over current
algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.13550">I-AI: A Controllable &amp; Interpretable AI System for Decoding Radiologists&#x27; Intense Focus for Accurate CXR Diagnoses. (arXiv:2309.13550v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1">Trong Thang Pham</a>, <a href="http://arxiv.org/find/cs/1/au:+Brecheisen_J/0/1/0/all/0/1">Jacob Brecheisen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1">Anh Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1">Hien Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1">Ngan Le</a></p>
<p>In the field of chest X-ray (CXR) diagnosis, existing works often focus
solely on determining where a radiologist looks, typically through tasks such
as detection, segmentation, or classification. However, these approaches are
often designed as black-box models, lacking interpretability. In this paper, we
introduce Interpretable Artificial Intelligence (I-AI) a novel and unified
controllable interpretable pipeline for decoding the intense focus of
radiologists in CXR diagnosis. Our I-AI addresses three key questions: where a
radiologist looks, how long they focus on specific areas, and what findings
they diagnose. By capturing the intensity of the radiologist's gaze, we provide
a unified solution that offers insights into the cognitive process underlying
radiological interpretation. Unlike current methods that rely on black-box
machine learning models, which can be prone to extracting erroneous information
from the entire input image during the diagnosis process, we tackle this issue
by effectively masking out irrelevant information. Our proposed I-AI leverages
a vision-language model, allowing for precise control over the interpretation
process while ensuring the exclusion of irrelevant features. To train our I-AI
model, we utilize an eye gaze dataset to extract anatomical gaze information
and generate ground truth heatmaps. Through extensive experimentation, we
demonstrate the efficacy of our method. We showcase that the attention
heatmaps, designed to mimic radiologists' focus, encode sufficient and relevant
information, enabling accurate classification tasks using only a portion of
CXR. The code, checkpoints, and data are at https://github.com/UARK-AICV/IAI
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.14293">NAS-NeRF: Generative Neural Architecture Search for Neural Radiance Fields. (arXiv:2309.14293v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nair_S/0/1/0/all/0/1">Saeejith Nair</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuhao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shafiee_M/0/1/0/all/0/1">Mohammad Javad Shafiee</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1">Alexander Wong</a></p>
<p>Neural radiance fields (NeRFs) enable high-quality novel view synthesis, but
their high computational complexity limits deployability. While existing
neural-based solutions strive for efficiency, they use one-size-fits-all
architectures regardless of scene complexity. The same architecture may be
unnecessarily large for simple scenes but insufficient for complex ones. Thus,
there is a need to dynamically optimize the neural network component of NeRFs
to achieve a balance between computational complexity and specific targets for
synthesis quality. We introduce NAS-NeRF, a generative neural architecture
search strategy that generates compact, scene-specialized NeRF architectures by
balancing architecture complexity and target synthesis quality metrics. Our
method incorporates constraints on target metrics and budgets to guide the
search towards architectures tailored for each scene. Experiments on the
Blender synthetic dataset show the proposed NAS-NeRF can generate architectures
up to 5.74$\times$ smaller, with 4.19$\times$ fewer FLOPs, and 1.93$\times$
faster on a GPU than baseline NeRFs, without suffering a drop in SSIM.
Furthermore, we illustrate that NAS-NeRF can also achieve architectures up to
23$\times$ smaller, with 22$\times$ fewer FLOPs, and 4.7$\times$ faster than
baseline NeRFs with only a 5.3% average SSIM drop. Our source code is also made
publicly available at https://saeejithnair.github.io/NAS-NeRF.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.15726">Factorized Diffusion Architectures for Unsupervised Image Generation and Segmentation. (arXiv:2309.15726v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1">Xin Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Maire_M/0/1/0/all/0/1">Michael Maire</a></p>
<p>We develop a neural network architecture which, trained in an unsupervised
manner as a denoising diffusion model, simultaneously learns to both generate
and segment images. Learning is driven entirely by the denoising diffusion
objective, without any annotation or prior knowledge about regions during
training. A computational bottleneck, built into the neural architecture,
encourages the denoising network to partition an input into regions, denoise
them in parallel, and combine the results. Our trained model generates both
synthetic images and, by simple examination of its internal predicted
partitions, a semantic segmentation of those images. Without any finetuning, we
directly apply our unsupervised model to the downstream task of segmenting real
images via noising and subsequently denoising them. Experiments demonstrate
that our model achieves accurate unsupervised image segmentation and
high-quality synthetic image generation across multiple datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05056">Open-Vocabulary Animal Keypoint Detection with Semantic-feature Matching. (arXiv:2310.05056v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1">Lumin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lai_S/0/1/0/all/0/1">Shenqi Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1">Wenqi Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1">Nanning Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1">Ping Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kaipeng Zhang</a></p>
<p>Current image-based keypoint detection methods for animal (including human)
bodies and faces are generally divided into full-supervised and few-shot
class-agnostic approaches. The former typically relies on laborious and
time-consuming manual annotations, posing considerable challenges in expanding
keypoint detection to a broader range of keypoint categories and animal
species. The latter, though less dependent on extensive manual input, still
requires necessary support images with annotation for reference during testing.
To realize zero-shot keypoint detection without any prior annotation, we
introduce the Open-Vocabulary Keypoint Detection (OVKD) task, which is
innovatively designed to use text prompts for identifying arbitrary keypoints
across any species. In pursuit of this goal, we have developed a novel
framework named Open-Vocabulary Keypoint Detection with Semantic-feature
Matching (KDSM). This framework synergistically combines vision and language
models, creating an interplay between language features and local keypoint
visual features. KDSM enhances its capabilities by integrating Domain
Distribution Matrix Matching (DDMM) and other special modules, such as the
Vision-Keypoint Relational Awareness (VKRA) module, improving the framework's
generalizability and overall performance.Our comprehensive experiments
demonstrate that KDSM significantly outperforms the baseline in terms of
performance and achieves remarkable success in the OVKD task.Impressively, our
method, operating in a zero-shot fashion, still yields results comparable to
state-of-the-art few-shot species class-agnostic keypoint detection methods.We
will make the source code publicly accessible.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07206">DeepSimHO: Stable Pose Estimation for Hand-Object Interaction via Physics Simulation. (arXiv:2310.07206v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Rong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1">Wei Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hongdong Li</a></p>
<p>This paper addresses the task of 3D pose estimation for a hand interacting
with an object from a single image observation. When modeling hand-object
interaction, previous works mainly exploit proximity cues, while overlooking
the dynamical nature that the hand must stably grasp the object to counteract
gravity and thus preventing the object from slipping or falling. These works
fail to leverage dynamical constraints in the estimation and consequently often
produce unstable results. Meanwhile, refining unstable configurations with
physics-based reasoning remains challenging, both by the complexity of contact
dynamics and by the lack of effective and efficient physics inference in the
data-driven learning framework. To address both issues, we present DeepSimHO: a
novel deep-learning pipeline that combines forward physics simulation and
backward gradient approximation with a neural network. Specifically, for an
initial hand-object pose estimated by a base network, we forward it to a
physics simulator to evaluate its stability. However, due to non-smooth contact
geometry and penetration, existing differentiable simulators can not provide
reliable state gradient. To remedy this, we further introduce a deep network to
learn the stability evaluation process from the simulator, while smoothly
approximating its gradient and thus enabling effective back-propagation.
Extensive experiments show that our method noticeably improves the stability of
the estimation and achieves superior efficiency over test-time optimization.
The code is available at https://github.com/rongakowang/DeepSimHO.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07916">Dynamic Appearance Particle Neural Radiance Field. (arXiv:2310.07916v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_A/0/1/0/all/0/1">Ancheng Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jun Li</a></p>
<p>Neural Radiance Fields (NeRFs) have shown great potential in modelling 3D
scenes. Dynamic NeRFs extend this model by capturing time-varying elements,
typically using deformation fields. The existing dynamic NeRFs employ a similar
Eulerian representation for both light radiance and deformation fields. This
leads to a close coupling of appearance and motion and lacks a physical
interpretation. In this work, we propose Dynamic Appearance Particle Neural
Radiance Field (DAP-NeRF), which introduces particle-based representation to
model the motions of visual elements in a dynamic 3D scene. DAP-NeRF consists
of superposition of a static field and a dynamic field. The dynamic field is
quantised as a collection of {\em appearance particles}, which carries the
visual information of a small dynamic element in the scene and is equipped with
a motion model. All components, including the static field, the visual features
and motion models of the particles, are learned from monocular videos without
any prior geometric knowledge of the scene. We develop an efficient
computational framework for the particle-based model. We also construct a new
dataset to evaluate motion modelling. Experimental results show that DAP-NeRF
is an effective technique to capture not only the appearance but also the
physically meaningful motions in a 3D dynamic scene.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.14729">MAS: Multi-view Ancestral Sampling for 3D motion generation using 2D diffusion. (arXiv:2310.14729v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kapon_R/0/1/0/all/0/1">Roy Kapon</a>, <a href="http://arxiv.org/find/cs/1/au:+Tevet_G/0/1/0/all/0/1">Guy Tevet</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1">Daniel Cohen-Or</a>, <a href="http://arxiv.org/find/cs/1/au:+Bermano_A/0/1/0/all/0/1">Amit H. Bermano</a></p>
<p>We introduce Multi-view Ancestral Sampling (MAS), a method for 3D motion
generation, using 2D diffusion models that were trained on motions obtained
from in-the-wild videos. As such, MAS opens opportunities to exciting and
diverse fields of motion previously under-explored as 3D data is scarce and
hard to collect. MAS works by simultaneously denoising multiple 2D motion
sequences representing different views of the same 3D motion. It ensures
consistency across all views at each diffusion step by combining the individual
generations into a unified 3D sequence, and projecting it back to the original
views. We demonstrate MAS on 2D pose data acquired from videos depicting
professional basketball maneuvers, rhythmic gymnastic performances featuring a
ball apparatus, and horse races. In each of these domains, 3D motion capture is
arduous, and yet, MAS generates diverse and realistic 3D sequences. Unlike the
Score Distillation approach, which optimizes each sample by repeatedly applying
small fixes, our method uses a sampling process that was constructed for the
diffusion framework. As we demonstrate, MAS avoids common issues such as
out-of-domain sampling and mode-collapse. https://guytevet.github.io/mas-page/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01766">Support or Refute: Analyzing the Stance of Evidence to Detect Out-of-Context Mis- and Disinformation. (arXiv:2311.01766v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1">Xin Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jie Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1">Weidong Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zheng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shujun Li</a></p>
<p>Mis- and disinformation online have become a major societal problem as major
sources of online harms of different kinds. One common form of mis- and
disinformation is out-of-context (OOC) information, where different pieces of
information are falsely associated, e.g., a real image combined with a false
textual caption or a misleading textual description. Although some past studies
have attempted to defend against OOC mis- and disinformation through external
evidence, they tend to disregard the role of different pieces of evidence with
different stances. Motivated by the intuition that the stance of evidence
represents a bias towards different detection results, we propose a stance
extraction network (SEN) that can extract the stances of different pieces of
multi-modal evidence in a unified framework. Moreover, we introduce a
support-refutation score calculated based on the co-occurrence relations of
named entities into the textual SEN. Extensive experiments on a public
large-scale dataset demonstrated that our proposed method outperformed the
state-of-the-art baselines, with the best model achieving a performance gain of
3.2% in accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04811">Image-Based Virtual Try-On: A Survey. (arXiv:2311.04811v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1">Dan Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xuanpu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Juan Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_W/0/1/0/all/0/1">Weizhi Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Tong_R/0/1/0/all/0/1">Ruofeng Tong</a>, <a href="http://arxiv.org/find/cs/1/au:+Kankanhalli_M/0/1/0/all/0/1">Mohan Kankanhalli</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1">An-An Liu</a></p>
<p>Image-based virtual try-on aims to synthesize a naturally dressed person
image with a clothing image, which revolutionizes online shopping and inspires
related topics within image generation, showing both research significance and
commercial potential. However, there is a big gap between current research
progress and commercial applications and an absence of comprehensive overview
of this field to accelerate the development. In this survey, we provide a
comprehensive analysis of the state-of-the-art techniques and methodologies in
aspects of pipeline architecture, person representation and key modules such as
try-on indication, clothing warping and try-on stage. We propose a new semantic
criteria with CLIP, and evaluate representative methods with uniformly
implemented evaluation metrics on the same dataset. In addition to quantitative
and qualitative evaluation of current open-source methods, we also utilize
ControlNet to fine-tune a recent large image generation model (PBE) to show
future potential of large-scale models on image-based virtual try-on task.
Finally, unresolved issues are highlighted and future research directions are
prospected to identify key trends and inspire further exploration. The
uniformly implemented evaluation metrics, dataset and collected methods will be
made public available at
https://github.com/little-misfit/Survey-Of-Virtual-Try-On.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08811">Correlation-aware active learning for surgery video segmentation. (arXiv:2311.08811v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1">Fei Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Marquez_Neila_P/0/1/0/all/0/1">Pablo Marquez-Neila</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1">Mingyi Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Rafii_Tari_H/0/1/0/all/0/1">Hedyeh Rafii-Tari</a>, <a href="http://arxiv.org/find/cs/1/au:+Sznitman_R/0/1/0/all/0/1">Raphael Sznitman</a></p>
<p>Semantic segmentation is a complex task that relies heavily on large amounts
of annotated image data. However, annotating such data can be time-consuming
and resource-intensive, especially in the medical domain. Active Learning (AL)
is a popular approach that can help to reduce this burden by iteratively
selecting images for annotation to improve the model performance. In the case
of video data, it is important to consider the model uncertainty and the
temporal nature of the sequences when selecting images for annotation. This
work proposes a novel AL strategy for surgery video segmentation, COWAL,
COrrelation-aWare Active Learning. Our approach involves projecting images into
a latent space that has been fine-tuned using contrastive learning and then
selecting a fixed number of representative images from local clusters of video
frames. We demonstrate the effectiveness of this approach on two video datasets
of surgical instruments and three real-world video datasets. The datasets and
code will be made publicly available upon receiving necessary approvals.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09178">RBPGAN: Recurrent Back-Projection GAN for Video Super Resolution. (arXiv:2311.09178v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sulaiman_M/0/1/0/all/0/1">Marwah Sulaiman</a>, <a href="http://arxiv.org/find/cs/1/au:+Shehabeldin_Z/0/1/0/all/0/1">Zahraa Shehabeldin</a>, <a href="http://arxiv.org/find/cs/1/au:+Fahmy_I/0/1/0/all/0/1">Israa Fahmy</a>, <a href="http://arxiv.org/find/cs/1/au:+Barakat_M/0/1/0/all/0/1">Mohammed Barakat</a>, <a href="http://arxiv.org/find/cs/1/au:+El_Naggar_M/0/1/0/all/0/1">Mohammed El-Naggar</a>, <a href="http://arxiv.org/find/cs/1/au:+Hussein_D/0/1/0/all/0/1">Dareen Hussein</a>, <a href="http://arxiv.org/find/cs/1/au:+Youssef_M/0/1/0/all/0/1">Moustafa Youssef</a>, <a href="http://arxiv.org/find/cs/1/au:+Eraqi_H/0/1/0/all/0/1">Hesham M. Eraqi</a></p>
<p>Recently, video super resolution (VSR) has become a very impactful task in
the area of Computer Vision due to its various applications. In this paper, we
propose Recurrent Back-Projection Generative Adversarial Network (RBPGAN) for
VSR in an attempt to generate temporally coherent solutions while preserving
spatial details. RBPGAN integrates two state-of-the-art models to get the best
in both worlds without compromising the accuracy of produced video. The
generator of the model is inspired by RBPN system, while the discriminator is
inspired by TecoGAN. We also utilize Ping-Pong loss to increase temporal
consistency over time. Our contribution together results in a model that
outperforms earlier work in terms of temporally consistent details, as we will
demonstrate qualitatively and quantitatively using different datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10931">FLORIDA: Fake-looking Real Images Dataset. (arXiv:2311.10931v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Borji_A/0/1/0/all/0/1">Ali Borji</a></p>
<p>Although extensive research has been carried out to evaluate the
effectiveness of AI tools and models in detecting deep fakes, the question
remains unanswered regarding whether these models can accurately identify
genuine images that appear artificial. In this study, as an initial step
towards addressing this issue, we have curated a dataset of 510 genuine images
that exhibit a fake appearance and conducted an assessment using two AI models.
We show that two models exhibited subpar performance when applied to our
dataset. Additionally, our dataset can serve as a valuable tool for assessing
the ability of deep learning models to comprehend complex visual stimuli. We
anticipate that this research will stimulate further discussions and
investigations in this area. Our dataset is accessible at
https://github.com/aliborji/FLORIDA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12397">Rich and Poor Texture Contrast: A Simple yet Effective Approach for AI-generated Image Detection. (arXiv:2311.12397v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhong_N/0/1/0/all/0/1">Nan Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yiran Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1">Zhenxing Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xinpeng Zhang</a></p>
<p>Recent generative models show impressive performance in generating
photographic images. Humans can hardly distinguish such incredibly
realistic-looking AI-generated images from real ones. AI-generated images may
lead to ubiquitous disinformation dissemination. Therefore, it is of utmost
urgency to develop a detector to identify AI-generated images. Most existing
detectors suffer from sharp performance drops over unseen generative models. In
this paper, we propose a novel AI-generated image detector capable of
identifying fake images created by a wide range of generative models. Our
approach leverages the inter-pixel correlation contrast between rich and poor
texture regions within an image. Pixels in rich texture regions exhibit more
significant fluctuations than those in poor texture regions. This discrepancy
reflects that the entropy of rich texture regions is larger than that of poor
ones. Consequently, synthesizing realistic rich texture regions proves to be
more challenging for existing generative models. Based on this principle, we
divide an image into multiple patches and reconstruct them into two images,
comprising rich-texture and poor-texture patches respectively. Subsequently, we
extract the inter-pixel correlation discrepancy feature between rich and poor
texture regions. This feature serves as a universal fingerprint used for
AI-generated image forensics across different generative models. In addition,
we build a comprehensive AI-generated image detection benchmark, which includes
16 kinds of prevalent generative models, to evaluate the effectiveness of
existing baselines and our approach. Our benchmark provides a leaderboard for
follow-up studies. Extensive experimental results show that our approach
outperforms state-of-the-art baselines by a significant margin. Our project:
https://fdmas.github.io/AIGCDetect/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14262">ZeroPS: High-quality Cross-modal Knowledge Transfer for Zero-Shot 3D Part Segmentation. (arXiv:2311.14262v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1">Yuheng Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1">Nenglun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1">Wenyun Sun</a></p>
<p>Recently, many 2D pretrained foundational models have demonstrated impressive
zero-shot prediction capabilities. In this work, we design a novel pipeline for
zero-shot 3D part segmentation, called ZeroPS. It high-quality transfers
knowledge from 2D pretrained foundational models to 3D point clouds. The main
idea of our approach is to explore the natural relationship between multi-view
correspondences and the prompt mechanism of foundational models and build
bridges on it. Our pipeline consists of two components: 1) a self-extension
component that extends 2D groups from a single viewpoint to spatial
global-level 3D groups; 2) a multi-modal labeling component that introduces a
two-dimensional checking mechanism to vote each 2D predicted bounding box to
the best matching 3D part, and a Class Non-highest Vote Penalty function to
refine the Vote Matrix. Additionally, a merging algorithm is included to merge
part-level 3D groups. Extensive evaluation of three zero-shot segmentation
tasks on PartnetE datasets, achieving state-of-the-art results with significant
improvements (+19.6%, +5.2% and +4.9%, respectively) over existing methods. Our
proposed approach does not need any training, fine-tuning or learnable
parameters. It is hardly affected by domain shift. The code will be released.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16945">UC-NeRF: Neural Radiance Field for Under-Calibrated Multi-view Cameras in Autonomous Driving. (arXiv:2311.16945v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1">Kai Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Long_X/0/1/0/all/0/1">Xiaoxiao Long</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1">Wei Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhiqiang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yuexin Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kaixuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xiaozhi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xuejin Chen</a></p>
<p>Multi-camera setups find widespread use across various applications, such as
autonomous driving, as they greatly expand sensing capabilities. Despite the
fast development of Neural radiance field (NeRF) techniques and their wide
applications in both indoor and outdoor scenes, applying NeRF to multi-camera
systems remains very challenging. This is primarily due to the inherent
under-calibration issues in multi-camera setup, including inconsistent imaging
effects stemming from separately calibrated image signal processing units in
diverse cameras, and system errors arising from mechanical vibrations during
driving that affect relative camera poses. In this paper, we present UC-NeRF, a
novel method tailored for novel view synthesis in under-calibrated multi-view
camera systems. Firstly, we propose a layer-based color correction to rectify
the color inconsistency in different image regions. Second, we propose virtual
warping to generate more viewpoint-diverse but color-consistent virtual views
for color correction and 3D recovery. Finally, a spatiotemporally constrained
pose refinement is designed for more robust and accurate pose calibration in
multi-camera systems. Our method not only achieves state-of-the-art performance
of novel view synthesis in multi-camera setups, but also effectively
facilitates depth estimation in large-scale outdoor scenes with the synthesized
novel views.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00377">SynFundus: A synthetic fundus images dataset with millions of samples and multi-disease annotations. (arXiv:2312.00377v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shang_F/0/1/0/all/0/1">Fangxin Shang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1">Jie Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yehui Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Haifeng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Junwei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1">Lei Ma</a></p>
<p>In the field of medical imaging, there are seldom large-scale public datasets
with high-quality annotations due to data privacy and annotation cost. To
address this issue, we release SynFundus-1M, a high-quality synthetic dataset
containing over \textbf{1 million} fundus images w.r.t. 11 disease types.
Moreover, we intentionally diversify the readability of the images and
accordingly provide 4 types of the quality score for each image. To the best of
our knowledge, SynFundus-1M is currently the largest fundus dataset with the
most sophisticated annotations. All the images are generated by a Denoising
Diffusion Probabilistic Model, named SynFundus-Generator. Trained with over 1.3
million private fundus images, our SynFundus-Generator achieves significant
superior performance in generating fundus images compared to some recent
related works. Furthermore, we blend some synthetic images from SynFundus-1M
with real fundus images, and ophthalmologists can hardly distinguish the
synthetic images from real ones. Through extensive experiments, we demonstrate
that both convolutional neural networs (CNN) and Vision Transformer (ViT) can
benefit from SynFundus-1M by pretraining or training directly. Compared to
datasets like ImageNet or EyePACS, models trained on SynFundus-1M not only
achieve better performance but also faster convergence on various downstream
tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00534">LiDAR-based curb detection for ground truth annotation in automated driving validation. (arXiv:2312.00534v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Apellaniz_J/0/1/0/all/0/1">Jose Luis Apell&#xe1;niz</a>, <a href="http://arxiv.org/find/cs/1/au:+Garcia_M/0/1/0/all/0/1">Mikel Garc&#xed;a</a>, <a href="http://arxiv.org/find/cs/1/au:+Aranjuelo_N/0/1/0/all/0/1">Nerea Aranjuelo</a>, <a href="http://arxiv.org/find/cs/1/au:+Barandiaran_J/0/1/0/all/0/1">Javier Barandiar&#xe1;n</a>, <a href="http://arxiv.org/find/cs/1/au:+Nieto_M/0/1/0/all/0/1">Marcos Nieto</a></p>
<p>Curb detection is essential for environmental awareness in Automated Driving
(AD), as it typically limits drivable and non-drivable areas. Annotated data
are necessary for developing and validating an AD function. However, the number
of public datasets with annotated point cloud curbs is scarce. This paper
presents a method for detecting 3D curbs in a sequence of point clouds captured
from a LiDAR sensor, which consists of two main steps. First, our approach
detects the curbs at each scan using a segmentation deep neural network. Then,
a sequence-level processing step estimates the 3D curbs in the reconstructed
point cloud using the odometry of the vehicle. From these 3D points of the
curb, we obtain polylines structured following ASAM OpenLABEL standard. These
detections can be used as pre-annotations in labelling pipelines to efficiently
generate curb-related ground truth data. We validate our approach through an
experiment in which different human annotators were required to annotate curbs
in a group of LiDAR-based sequences with and without our automatically
generated pre-annotations. The results show that the manual annotation time is
reduced by 50.99% thanks to our detections, keeping the data quality level.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01027">Taming Latent Diffusion Models to See in the Dark. (arXiv:2312.01027v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wen_Q/0/1/0/all/0/1">Qiang Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_Y/0/1/0/all/0/1">Yazhou Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Qifeng Chen</a></p>
<p>Enhancing a low-light noisy RAW image into a well-exposed and clean sRGB
image is a significant challenge in computational photography. Due to the
limitation of large-scale paired data, prior approaches have difficulty in
recovering fine details and true colors in extremely low-light regions.
Meanwhile, recent advancements in generative diffusion models have shown
promising generating capabilities, which inspires this work to explore
generative priors from a diffusion model trained on a large-scale open-domain
dataset to benefit the low-light image enhancement (LLIE) task. Based on this
intention, we propose a novel diffusion-model-based LLIE method, dubbed
LDM-SID. LDM-SID aims at inserting a set of proposed taming modules into a
frozen pre-trained diffusion model to steer its generating process.
Specifically, the taming module fed with low-light information serves to output
a pair of affine transformation parameters to modulate the intermediate feature
in the diffusion model. Additionally, based on the observation of dedicated
generative priors across different portions of the diffusion model, we propose
to apply 2D discrete wavelet transforms on the input RAW image, resulting in
dividing the LLIE task into two essential parts: low-frequency content
generation and high-frequency detail maintenance. This enables us to skillfully
tame the diffusion model for optimized structural generation and detail
enhancement. Extensive experiments demonstrate the proposed method not only
achieves state-of-the-art performance in quantitative evaluations but also
shows significant superiority in visual comparisons. These findings highlight
the effectiveness of leveraging a pre-trained diffusion model as a generative
prior to the LLIE task. The project page is available at
https://csqiangwen.github.io/projects/ldm-sid/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01598">Good Questions Help Zero-Shot Image Reasoning. (arXiv:2312.01598v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1">Kaiwen Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1">Tao Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1">Xinmei Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1">Xiubo Geng</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1">Chongyang Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1">Dacheng Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1">Tianyi Zhou</a></p>
<p>Aligning the recent large language models (LLMs) with computer vision models
leads to large vision-language models (LVLMs), which have paved the way for
zero-shot image reasoning tasks. However, LVLMs are usually trained on short
high-level captions only referring to sparse focus regions in images. Such a
``tunnel vision'' limits LVLMs to exploring other relevant contexts in complex
scenes. To address this challenge, we introduce Question-Driven Visual
Exploration (QVix), a novel prompting strategy that enhances the exploratory
capabilities of LVLMs in zero-shot reasoning tasks. QVix leverages LLMs' strong
language prior to generate input-exploratory questions with more details than
the original query, guiding LVLMs to explore visual content more
comprehensively and uncover subtle or peripheral details. QVix enables a wider
exploration of visual scenes, improving the LVLMs' reasoning accuracy and depth
in tasks such as visual question answering and visual entailment. Our
evaluations on various challenging zero-shot vision-language benchmarks,
including ScienceQA and fine-grained visual classification, demonstrate that
QVix significantly outperforms existing methods, highlighting its effectiveness
in bridging the gap between complex visual data and LVLMs' exploratory
abilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01711">Regressor-Segmenter Mutual Prompt Learning for Crowd Counting. (arXiv:2312.01711v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1">Mingyue Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1">Li Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1">Zhaoyi Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Binghui Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yaowei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1">Qixiang Ye</a></p>
<p>Crowd counting has achieved significant progress by training regressors to
predict instance positions. In heavily crowded scenarios, however, regressors
are challenged by uncontrollable annotation variance, which causes density map
bias and context information inaccuracy. In this study, we propose mutual
prompt learning (mPrompt), which leverages a regressor and a segmenter as
guidance for each other, solving bias and inaccuracy caused by annotation
variance while distinguishing foreground from background. In specific, mPrompt
leverages point annotations to tune the segmenter and predict pseudo head masks
in a way of point prompt learning. It then uses the predicted segmentation
masks, which serve as spatial constraint, to rectify biased point annotations
as context prompt learning. mPrompt defines a way of mutual information
maximization from prompt learning, mitigating the impact of annotation variance
while improving model accuracy. Experiments show that mPrompt significantly
reduces the Mean Average Error (MAE), demonstrating the potential to be general
framework for down-stream vision tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01853">Robot Synesthesia: In-Hand Manipulation with Visuotactile Sensing. (arXiv:2312.01853v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1">Ying Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Che_H/0/1/0/all/0/1">Haichuan Che</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1">Yuzhe Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1">Binghao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1">Zhao-Heng Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kang-Won Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yi Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1">Soo-Chul Lim</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaolong Wang</a></p>
<p>Executing contact-rich manipulation tasks necessitates the fusion of tactile
and visual feedback. However, the distinct nature of these modalities poses
significant challenges. In this paper, we introduce a system that leverages
visual and tactile sensory inputs to enable dexterous in-hand manipulation.
Specifically, we propose Robot Synesthesia, a novel point cloud-based tactile
representation inspired by human tactile-visual synesthesia. This approach
allows for the simultaneous and seamless integration of both sensory inputs,
offering richer spatial information and facilitating better reasoning about
robot actions. The method, trained in a simulated environment and then deployed
to a real robot, is applicable to various in-hand object rotation tasks.
Comprehensive ablations are performed on how the integration of vision and
touch can improve reinforcement learning and Sim2Real performance. Our project
page is available at https://yingyuan0414.github.io/visuotactile/ .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02021">VLTSeg: Simple Transfer of CLIP-Based Vision-Language Representations for Domain Generalized Semantic Segmentation. (arXiv:2312.02021v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hummer_C/0/1/0/all/0/1">Christoph H&#xfc;mmer</a>, <a href="http://arxiv.org/find/cs/1/au:+Schwonberg_M/0/1/0/all/0/1">Manuel Schwonberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1">Liangwei Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1">Hu Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1">Alois Knoll</a>, <a href="http://arxiv.org/find/cs/1/au:+Gottschalk_H/0/1/0/all/0/1">Hanno Gottschalk</a></p>
<p>Domain generalization (DG) remains a significant challenge for perception
based on deep neural networks (DNN), where domain shifts occur due to lighting,
weather, or geolocation changes. In this work, we propose VLTSeg to enhance
domain generalization in semantic segmentation, where the network is solely
trained on the source domain and evaluated on unseen target domains. Our method
leverages the inherent semantic robustness of vision-language models. First, by
substituting traditional vision-only backbones with pre-trained encoders from
CLIP and EVA-CLIP as transfer learning setting we find that in the field of DG,
vision-language pre-training significantly outperforms supervised and
self-supervised vision pre-training. We thus propose a new vision-language
approach for domain generalized segmentation, which improves the domain
generalization SOTA by 7.6% mIoU when training on the synthetic GTA5 dataset.
We further show the superior generalization capabilities of vision-language
segmentation models by reaching 76.48% mIoU on the popular Cityscapes-to-ACDC
benchmark, outperforming the previous SOTA approach by 6.9% mIoU on the test
set at the time of writing. Additionally, our approach shows strong in-domain
generalization capabilities indicated by 86.1% mIoU on the Cityscapes test set,
resulting in a shared first place with the previous SOTA on the current
leaderboard at the time of submission.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02338">A Contrastive Compositional Benchmark for Text-to-Image Synthesis: A Study with Unified Text-to-Image Fidelity Metrics. (arXiv:2312.02338v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiangru Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1">Penglei Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chengyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jingping Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhixu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1">Yanghua Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jun Huang</a></p>
<p>Text-to-image (T2I) synthesis has recently achieved significant advancements.
However, challenges remain in the model's compositionality, which is the
ability to create new combinations from known components. We introduce
Winoground-T2I, a benchmark designed to evaluate the compositionality of T2I
models. This benchmark includes 11K complex, high-quality contrastive sentence
pairs spanning 20 categories. These contrastive sentence pairs with subtle
differences enable fine-grained evaluations of T2I synthesis models.
Additionally, to address the inconsistency across different metrics, we propose
a strategy that evaluates the reliability of various metrics by using
comparative sentence pairs. We use Winoground-T2I with a dual objective: to
evaluate the performance of T2I models and the metrics used for their
evaluation. Finally, we provide insights into the strengths and weaknesses of
these metrics and the capabilities of current T2I models in tackling challenges
across a range of complex compositional categories. Our benchmark is publicly
available at https://github.com/zhuxiangru/Winoground-T2I .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02396">Unsupervised Change Detection for Space Habitats Using 3D Point Clouds. (arXiv:2312.02396v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1">Jamie Santos</a>, <a href="http://arxiv.org/find/cs/1/au:+Dinkel_H/0/1/0/all/0/1">Holly Dinkel</a>, <a href="http://arxiv.org/find/cs/1/au:+Di_J/0/1/0/all/0/1">Julia Di</a>, <a href="http://arxiv.org/find/cs/1/au:+Borges_P/0/1/0/all/0/1">Paulo V.K. Borges</a>, <a href="http://arxiv.org/find/cs/1/au:+Moreira_M/0/1/0/all/0/1">Marina Moreira</a>, <a href="http://arxiv.org/find/cs/1/au:+Alexandrov_O/0/1/0/all/0/1">Oleg Alexandrov</a>, <a href="http://arxiv.org/find/cs/1/au:+Coltin_B/0/1/0/all/0/1">Brian Coltin</a>, <a href="http://arxiv.org/find/cs/1/au:+Smith_T/0/1/0/all/0/1">Trey Smith</a></p>
<p>This work presents an algorithm for scene change detection from point clouds
to enable autonomous robotic caretaking in future space habitats. Autonomous
robotic systems will help maintain future deep-space habitats, such as the
Gateway space station, which will be uncrewed for extended periods. Existing
scene analysis software used on the International Space Station (ISS) relies on
manually-labeled images for detecting changes. In contrast, the algorithm
presented in this work uses raw, unlabeled point clouds as inputs. The
algorithm first applies modified Expectation-Maximization Gaussian Mixture
Model (GMM) clustering to two input point clouds. It then performs change
detection by comparing the GMMs using the Earth Mover's Distance. The algorithm
is validated quantitatively and qualitatively using a test dataset collected by
an Astrobee robot in the NASA Ames Granite Lab comprising single frame depth
images taken directly by Astrobee and full-scene reconstructed maps built with
RGB-D and pose data from Astrobee. The runtimes of the approach are also
analyzed in depth. The source code is publicly released to promote further
development.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02725">R3D-SWIN:Use Shifted Window Attention for Single-View 3D Reconstruction. (arXiv:2312.02725v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chenhuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1">Meihua Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+li_z/0/1/0/all/0/1">zehuan li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1">Fangping Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1">Shanshan Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dingli Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1">Mengxi Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Siyi Zhang</a></p>
<p>Recently, vision transformers have performed well in various computer vision
tasks, including voxel 3D reconstruction. However, the windows of the vision
transformer are not multi-scale, and there is no connection between the
windows, which limits the accuracy of voxel 3D reconstruction. Therefore, we
propose a voxel 3D reconstruction network based on shifted window attention. To
the best of our knowledge, this is the first work to apply shifted window
attention to voxel 3D reconstruction. Experimental results on ShapeNet verify
our method achieves SOTA accuracy in single-view reconstruction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02762">Learning Cortical Anomaly through Masked Encoding for Unsupervised Heterogeneity Mapping. (arXiv:2312.02762v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1">Hao-Chun Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Andreassen_O/0/1/0/all/0/1">Ole Andreassen</a>, <a href="http://arxiv.org/find/eess/1/au:+Westlye_L/0/1/0/all/0/1">Lars Tjelta Westlye</a>, <a href="http://arxiv.org/find/eess/1/au:+Marquand_A/0/1/0/all/0/1">Andre F. Marquand</a>, <a href="http://arxiv.org/find/eess/1/au:+Beckmann_C/0/1/0/all/0/1">Christian F. Beckmann</a>, <a href="http://arxiv.org/find/eess/1/au:+Wolfers_T/0/1/0/all/0/1">Thomas Wolfers</a></p>
<p>The detection of heterogeneous mental disorders based on brain readouts
remains challenging due to the complexity of symptoms and the absence of
reliable biomarkers. This paper introduces CAM (Cortical Anomaly Detection
through Masked Image Modeling), a novel self-supervised framework designed for
the unsupervised detection of complex brain disorders using cortical surface
features. We employ this framework for the detection of individuals on the
psychotic spectrum and demonstrate its capabilities compared to state-ofthe-art
methods, achieving an AUC of 0.696 for Schizoaffective and 0.769 for
Schizophreniform, without the need for any labels. Furthermore, the analysis of
atypical cortical regions includes Pars Triangularis and several frontal areas,
often implicated in schizophrenia, provide further confidence in our approach.
Altogether, we demonstrate a scalable approach for anomaly detection of complex
brain disorders based on cortical abnormalities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02957">Classification for everyone : Building geography agnostic models for fairer recognition. (arXiv:2312.02957v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jindal_A/0/1/0/all/0/1">Akshat Jindal</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Shreya Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Gadgil_S/0/1/0/all/0/1">Soham Gadgil</a></p>
<p>In this paper, we analyze different methods to mitigate inherent geographical
biases present in state of the art image classification models. We first
quantitatively present this bias in two datasets - The Dollar Street Dataset
and ImageNet, using images with location information. We then present different
methods which can be employed to reduce this bias. Finally, we analyze the
effectiveness of the different techniques on making these models more robust to
geographical locations of the images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03018">DreamVideo: High-Fidelity Image-to-Video Generation with Image Retention and Text Guidance. (arXiv:2312.03018v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Cong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jiaxi Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1">Panwen Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Songcen Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1">Xiaodan Liang</a></p>
<p>Image-to-video generation, which aims to generate a video starting from a
given reference image, has drawn great attention. Existing methods try to
extend pre-trained text-guided image diffusion models to image-guided video
generation models. Nevertheless, these methods often result in either low
fidelity or flickering over time due to their limitation to shallow image
guidance and poor temporal consistency. To tackle these problems, we propose a
high-fidelity image-to-video generation method by devising a frame retention
branch on the basis of a pre-trained video diffusion model, named DreamVideo.
Instead of integrating the reference image into the diffusion process in a
semantic level, our DreamVideo perceives the reference image via convolution
layers and concatenate the features with the noisy latents as model input. By
this means, the details of the reference image can be preserved to the greatest
extent. In addition, by incorporating double-condition classifier-free
guidance, a single image can be directed to videos of different actions by
providing varying prompt texts. This has significant implications for
controllable video generation and holds broad application prospects. We conduct
comprehensive experiments on the public dataset, both quantitative and
qualitative results indicate that our method outperforms the state-of-the-art
method. Especially for fidelity, our model has powerful image retention ability
and result in high FVD in UCF101 compared to other image-to-video models. Also,
precise control can be achieved by giving different text prompts. Further
details and comprehensive results of our model will be presented in
https://anonymous0769.github.io/DreamVideo/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03325">GCFA:Geodesic Curve Feature Augmentation in the Pre-Shape Space. (arXiv:2312.03325v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1">Yuexing Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_G/0/1/0/all/0/1">Guanxin Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bing Wang</a></p>
<p>Deep learning has yielded remarkable outcomes in various domains. However,
the challenge of requiring large-scale labeled samples still persists in deep
learning. Thus, data augmentation has been introduced as a critical strategy to
train deep learning models. However, data augmentation suffers from information
loss and poor performance in small sample environments. To overcome these
drawbacks, we propose a feature augmentation method based on shape space
theory, i.e., Geodesic curve feature augmentation, called GCFA in
brevity.First, we extract features from the image with the neural network
model. Then, the multiple image features are projected into a pre-shape space
as features. In the pre-shape space, a Geodesic curve is built to fit the
features. Finally, the many generated features on the Geodesic curve are used
to train the various machine learning models. The GCFA module can be seamlessly
integrated with most machine learning methods. And the proposed method is
simple, effective and insensitive for the small sample datasets.Several
examples demonstrate that the GCFA method can greatly improve the performance
of the data preprocessing model in a small sample environment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03430">ShareCMP: Polarization-Aware RGB-P Semantic Segmentation. (arXiv:2312.03430v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhuoyan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lizhi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_C/0/1/0/all/0/1">Chenyu Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Ye Li</a></p>
<p>Multimodal semantic segmentation is developing rapidly, but the modality of
RGB-Polarization remains underexplored. To delve into this problem, we
construct a UPLight RGB-P segmentation benchmark with 12 typical underwater
semantic classes. In this work, we design the ShareCMP, an RGB-P semantic
segmentation framework with a shared dual-branch architecture, which reduces
the number of parameters by about 26-33% compared to previous dual-branch
models. It encompasses a Polarization Generate Attention (PGA) module designed
to generate polarization modal images with richer polarization properties for
the encoder. In addition, we introduce the Class Polarization-Aware Loss
(CPALoss) to improve the learning and understanding of the encoder for
polarization modal information and to optimize the PGA module. With extensive
experiments on a total of three RGB-P benchmarks, our ShareCMP achieves
state-of-the-art performance in mIoU with fewer parameters on the UPLight
(92.45(+0.32)%), ZJU (92.7(+0.1)%), and MCubeS (50.99(+1.51)%) datasets
compared to the previous best methods. The code is available at
https://github.com/LEFTeyex/ShareCMP.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03441">UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity. (arXiv:2312.03441v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zuo_J/0/1/0/all/0/1">Jialong Zuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Hanyu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1">Ying Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1">Feng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1">Tianyu Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Sang_N/0/1/0/all/0/1">Nong Sang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yunhe Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1">Changxin Gao</a></p>
<p>Existing text-based person retrieval datasets often have relatively
coarse-grained text annotations. This hinders the model to comprehend the
fine-grained semantics of query texts in real scenarios. To address this
problem, we contribute a new benchmark named \textbf{UFineBench} for text-based
person retrieval with ultra-fine granularity.
</p>
<p>Firstly, we construct a new \textbf{dataset} named UFine6926. We collect a
large number of person images and manually annotate each image with two
detailed textual descriptions, averaging 80.8 words each. The average word
count is three to four times that of the previous datasets. In addition of
standard in-domain evaluation, we also propose a special \textbf{evaluation
paradigm} more representative of real scenarios. It contains a new evaluation
set with cross domains, cross textual granularity and cross textual styles,
named UFine3C, and a new evaluation metric for accurately measuring retrieval
ability, named mean Similarity Distribution (mSD). Moreover, we propose CFAM, a
more efficient \textbf{algorithm} especially designed for text-based person
retrieval with ultra fine-grained texts. It achieves fine granularity mining by
adopting a shared cross-modal granularity decoder and hard negative match
mechanism.
</p>
<p>With standard in-domain evaluation, CFAM establishes competitive performance
across various datasets, especially on our ultra fine-grained UFine6926.
Furthermore, by evaluating on UFine3C, we demonstrate that training on our
UFine6926 significantly improves generalization to real scenarios compared with
other coarse-grained datasets. The dataset and code will be made publicly
available at \url{https://github.com/Zplusdragon/UFineBench}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03511">Kandinsky 3.0 Technical Report. (arXiv:2312.03511v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Arkhipkin_V/0/1/0/all/0/1">Vladimir Arkhipkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Filatov_A/0/1/0/all/0/1">Andrei Filatov</a>, <a href="http://arxiv.org/find/cs/1/au:+Vasilev_V/0/1/0/all/0/1">Viacheslav Vasilev</a>, <a href="http://arxiv.org/find/cs/1/au:+Maltseva_A/0/1/0/all/0/1">Anastasia Maltseva</a>, <a href="http://arxiv.org/find/cs/1/au:+Azizov_S/0/1/0/all/0/1">Said Azizov</a>, <a href="http://arxiv.org/find/cs/1/au:+Pavlov_I/0/1/0/all/0/1">Igor Pavlov</a>, <a href="http://arxiv.org/find/cs/1/au:+Agafonova_J/0/1/0/all/0/1">Julia Agafonova</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuznetsov_A/0/1/0/all/0/1">Andrey Kuznetsov</a>, <a href="http://arxiv.org/find/cs/1/au:+Dimitrov_D/0/1/0/all/0/1">Denis Dimitrov</a></p>
<p>We present Kandinsky 3.0, a large-scale text-to-image generation model based
on latent diffusion, continuing the series of text-to-image Kandinsky models
and reflecting our progress to achieve higher quality and realism of image
generation. Compared to previous versions of Kandinsky 2.x, Kandinsky 3.0
leverages a two times larger U-Net backbone, a ten times larger text encoder
and removes diffusion mapping. We describe the architecture of the model, the
data collection procedure, the training technique, and the production system of
user interaction. We focus on the key components that, as we have identified as
a result of a large number of experiments, had the most significant impact on
improving the quality of our model compared to the others. By our side-by-side
comparisons, Kandinsky becomes better in text understanding and works better on
specific domains. Project page: https://ai-forever.github.io/Kandinsky-3
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03585">Foundation Model Assisted Weakly Supervised Semantic Segmentation. (arXiv:2312.03585v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xiaobo Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1">Xiaojin Gong</a></p>
<p>This work aims to leverage pre-trained foundation models, such as contrastive
language-image pre-training (CLIP) and segment anything model (SAM), to address
weakly supervised semantic segmentation (WSSS) using image-level labels. To
this end, we propose a coarse-to-fine framework based on CLIP and SAM for
generating high-quality segmentation seeds. Specifically, we construct an image
classification task and a seed segmentation task, which are jointly performed
by CLIP with frozen weights and two sets of learnable task-specific prompts. A
SAM-based seeding (SAMS) module is designed and applied to each task to produce
either coarse or fine seed maps. Moreover, we design a multi-label contrastive
loss supervised by image-level labels and a CAM activation loss supervised by
the generated coarse seed map. These losses are used to learn the prompts,
which are the only parts need to be learned in our framework. Once the prompts
are learned, we input each image along with the learned segmentation-specific
prompts into CLIP and the SAMS module to produce high-quality segmentation
seeds. These seeds serve as pseudo labels to train an off-the-shelf
segmentation network like other two-stage WSSS methods. Experiments show that
our method achieves the state-of-the-art performance on PASCAL VOC 2012 and
competitive results on MS COCO 2014. Code is available at
https://github.com/HAL-42/FMA-WSSS.git.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03763">Gaussian3Diff: 3D Gaussian Diffusion for 3D Full Head Synthesis and Editing. (arXiv:2312.03763v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1">Yushi Lan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_F/0/1/0/all/0/1">Feitong Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_D/0/1/0/all/0/1">Di Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1">Qiangeng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Genova_K/0/1/0/all/0/1">Kyle Genova</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zeng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fanello_S/0/1/0/all/0/1">Sean Fanello</a>, <a href="http://arxiv.org/find/cs/1/au:+Pandey_R/0/1/0/all/0/1">Rohit Pandey</a>, <a href="http://arxiv.org/find/cs/1/au:+Funkhouser_T/0/1/0/all/0/1">Thomas Funkhouser</a>, <a href="http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1">Chen Change Loy</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yinda Zhang</a></p>
<p>We present a novel framework for generating photorealistic 3D human head and
subsequently manipulating and reposing them with remarkable flexibility. The
proposed approach leverages an implicit function representation of 3D human
heads, employing 3D Gaussians anchored on a parametric face model. To enhance
representational capabilities and encode spatial information, we embed a
lightweight tri-plane payload within each Gaussian rather than directly storing
color and opacity. Additionally, we parameterize the Gaussians in a 2D UV space
via a 3DMM, enabling effective utilization of the diffusion model for 3D head
avatar generation. Our method facilitates the creation of diverse and realistic
3D human heads with fine-grained editing over facial features and expressions.
Extensive experiments demonstrate the effectiveness of our method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03996">Stable diffusion for Data Augmentation in COCO and Weed Datasets. (arXiv:2312.03996v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1">Boyang Deng</a></p>
<p>Generative models have increasingly impacted relative tasks, from computer
vision to interior design and other fields. Stable diffusion is an outstanding
diffusion model that paves the way for producing high-resolution images with
thorough details from text prompts or reference images. It will be an
interesting topic about gaining improvements for small datasets with
image-sparse categories. This study utilized seven common categories and three
widespread weed species to evaluate the efficiency of a stable diffusion model.
In detail, Stable diffusion was used to generate synthetic images belonging to
these classes; three techniques (i.e., Image-to-image translation, Dreambooth,
and ControlNet) based on stable diffusion were leveraged for image generation
with different focuses. Then, classification and detection tasks were conducted
based on these synthetic images, whose performance was compared to the models
trained on original images. Promising results have been achieved in some
classes. This seminal study may expedite the adaption of stable diffusion
models to different fields.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04008">Natural-language-driven Simulation Benchmark and Copilot for Efficient Production of Object Interactions in Virtual Road Scenes. (arXiv:2312.04008v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1">Kairui Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Zihao Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1">Gengjie Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1">Haotian Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zuo_D/0/1/0/all/0/1">Die Zuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1">Jibin Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zhao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zhecheng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1">Fupeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_Z/0/1/0/all/0/1">Ziyun Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1">Di Lin</a></p>
<p>We advocate the idea of the natural-language-driven(NLD) simulation to
efficiently produce the object interactions between multiple objects in the
virtual road scenes, for teaching and testing the autonomous driving systems
that should take quick action to avoid collision with obstacles with
unpredictable motions. The NLD simulation allows the brief natural-language
description to control the object interactions, significantly reducing the
human efforts for creating a large amount of interaction data. To facilitate
the research of NLD simulation, we collect the Language-to-Interaction(L2I)
benchmark dataset with 120,000 natural-language descriptions of object
interactions in 6 common types of road topologies. Each description is
associated with the programming code, which the graphic render can use to
visually reconstruct the object interactions in the virtual scenes. As a
methodology contribution, we design SimCopilot to translate the interaction
descriptions to the renderable code. We use the L2I dataset to evaluate
SimCopilot's abilities to control the object motions, generate complex
interactions, and generalize interactions across road topologies. The L2I
dataset and the evaluation results motivate the relevant research of the NLD
simulation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04066">Combining inherent knowledge of vision-language models with unsupervised domain adaptation through self-knowledge distillation. (arXiv:2312.04066v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Westfechtel_T/0/1/0/all/0/1">Thomas Westfechtel</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dexuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1">Tatsuya Harada</a></p>
<p>Unsupervised domain adaptation (UDA) tries to overcome the tedious work of
labeling data by leveraging a labeled source dataset and transferring its
knowledge to a similar but different target dataset. On the other hand, current
vision-language models exhibit astonishing zero-shot prediction capabilities.
In this work, we combine knowledge gained through UDA with the inherent
knowledge of vision-language models. In a first step, we generate the zero-shot
predictions of the source and target dataset using the vision-language model.
Since zero-shot predictions usually exhibit a large entropy, meaning that the
class probabilities are rather evenly distributed, we first adjust the
distribution to accentuate the winning probabilities. This is done using both
source and target data to keep the relative confidence between source and
target data. We then employ a conventional DA method, to gain the knowledge
from the source dataset, in combination with self-knowledge distillation, to
maintain the inherent knowledge of the vision-language model. We further
combine our method with a gradual source domain expansion strategy (GSDE) and
show that this strategy can also benefit by including zero-shot predictions. We
conduct experiments and ablation studies on three benchmarks (OfficeHome,
VisDA, and DomainNet) and outperform state-of-the-art methods. We further show
in ablation studies the contributions of different parts of our algorithm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04519">Bootstrapping Autonomous Radars with Self-Supervised Learning. (arXiv:2312.04519v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1">Yiduo Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Madani_S/0/1/0/all/0/1">Sohrab Madani</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1">Junfeng Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Alloulah_M/0/1/0/all/0/1">Mohammed Alloulah</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1">Saurabh Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Hassanieh_H/0/1/0/all/0/1">Haitham Hassanieh</a></p>
<p>The perception of autonomous vehicles using radars has attracted increased
research interest due its ability to operate in fog and bad weather. However,
training radar models is hindered by the cost and difficulty of annotating
large-scale radar data. To overcome this bottleneck, we propose a
self-supervised learning framework to leverage the large amount of unlabeled
radar data to pre-train radar-only embeddings for self-driving perception
tasks. The proposed method combines radar-to-radar and radar-to-vision
contrastive losses to learn a general representation from unlabeled radar
heatmaps paired with their corresponding camera images. When used for
downstream object detection, we demonstrate that the proposed self-supervision
framework can improve the accuracy of state-of-the-art supervised baselines by
5.8% in mAP.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04584">Towards Sample-specific Backdoor Attack with Clean Labels via Attribute Trigger. (arXiv:2312.04584v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yiming Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1">Mingyan Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Junfeng Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1">Tao Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1">Shu-Tao Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1">Zhan Qin</a></p>
<p>Currently, sample-specific backdoor attacks (SSBAs) are the most advanced and
malicious methods since they can easily circumvent most of the current backdoor
defenses. In this paper, we reveal that SSBAs are not sufficiently stealthy due
to their poisoned-label nature, where users can discover anomalies if they
check the image-label relationship. In particular, we demonstrate that it is
ineffective to directly generalize existing SSBAs to their clean-label variants
by poisoning samples solely from the target class. We reveal that it is
primarily due to two reasons, including \textbf{(1)} the `antagonistic effects'
of ground-truth features and \textbf{(2)} the learning difficulty of
sample-specific features. Accordingly, trigger-related features of existing
SSBAs cannot be effectively learned under the clean-label setting due to their
mild trigger intensity required for ensuring stealthiness. We argue that the
intensity constraint of existing SSBAs is mostly because their trigger patterns
are `content-irrelevant' and therefore act as `noises' for both humans and
DNNs. Motivated by this understanding, we propose to exploit content-relevant
features, $a.k.a.$ (human-relied) attributes, as the trigger patterns to design
clean-label SSBAs. This new attack paradigm is dubbed backdoor attack with
attribute trigger (BAAT). Extensive experiments are conducted on benchmark
datasets, which verify the effectiveness of our BAAT and its resistance to
existing defenses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05107">DreaMoving: A Human Video Generation Framework based on Diffusion Models. (arXiv:2312.05107v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_M/0/1/0/all/0/1">Mengyang Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jinlin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1">Kai Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1">Yuan Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hui_Z/0/1/0/all/0/1">Zheng Hui</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1">Xiefan Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1">Xianhui Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1">Haolan Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1">Chen Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaowen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1">Aojie Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1">Xiaoyang Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_B/0/1/0/all/0/1">Biwen Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_M/0/1/0/all/0/1">Miaomiao Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1">Peiran Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xuansong Xie</a></p>
<p>In this paper, we present DreaMoving, a diffusion-based controllable video
generation framework to produce high-quality customized human videos.
Specifically, given target identity and posture sequences, DreaMoving can
generate a video of the target identity moving or dancing anywhere driven by
the posture sequences. To this end, we propose a Video ControlNet for
motion-controlling and a Content Guider for identity preserving. The proposed
model is easy to use and can be adapted to most stylized diffusion models to
generate diverse results. The project page is available at
https://dreamoving.github.io/dreamoving
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.09936">BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions. (arXiv:2308.09936v2 [cs.CV] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1">Wenbo Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yifan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Weiyue Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zeyuan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1">Zhuowen Tu</a></p>
<p>Vision Language Models (VLMs), which extend Large Language Models (LLM) by
incorporating visual understanding capability, have demonstrated significant
advancements in addressing open-ended visual question-answering (VQA) tasks.
However, these models cannot accurately interpret images infused with text, a
common occurrence in real-world scenarios. Standard procedures for extracting
information from images often involve learning a fixed set of query embeddings.
These embeddings are designed to encapsulate image contexts and are later used
as soft prompt inputs in LLMs. Yet, this process is limited to the token count,
potentially curtailing the recognition of scenes with text-rich context. To
improve upon them, the present study introduces BLIVA: an augmented version of
InstructBLIP with Visual Assistant. BLIVA incorporates the query embeddings
from InstructBLIP and also directly projects encoded patch embeddings into the
LLM, a technique inspired by LLaVA. This approach assists the model to capture
intricate details potentially missed during the query decoding process.
Empirical evidence demonstrates that our model, BLIVA, significantly enhances
performance in processing text-rich VQA benchmarks (up to 17.76% in OCR-VQA
benchmark) and in undertaking general (not particularly text-rich) VQA
benchmarks (up to 7.9% in Visual Spatial Reasoning benchmark), comparing to our
baseline InstructBLIP. BLIVA demonstrates significant capability in decoding
real-world images, irrespective of text presence. To demonstrate the broad
industry applications enabled by BLIVA, we evaluate the model using a new
dataset comprising YouTube thumbnails paired with question-answer sets across
11 diverse categories. For researchers interested in further exploration, our
code and models are freely accessible at https://github.com/mlpc-ucsd/BLIVA.
</p>
</p>
</div>

    </div>
    </body>
    