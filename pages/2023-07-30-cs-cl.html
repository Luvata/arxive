<!DOCTYPE html>
<html>
<head>
<title>2023-07-30-cs-cl</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2307.14361">A Hybrid Machine Learning Model for Classifying Gene Mutations in Cancer using LSTM, BiLSTM, CNN, GRU, and GloVe. (arXiv:2307.14361v1 [q-bio.QM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Aburass_S/0/1/0/all/0/1">Sanad Aburass</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Dorgham_O/0/1/0/all/0/1">Osama Dorgham</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Shaqsi_J/0/1/0/all/0/1">Jamil Al Shaqsi</a></p>
<p>This study presents an ensemble model combining LSTM, BiLSTM, CNN, GRU, and
GloVe to classify gene mutations using Kaggle's Personalized Medicine:
Redefining Cancer Treatment dataset. The results were compared against
well-known transformers like as BERT, Electra, Roberta, XLNet, Distilbert, and
their LSTM ensembles. Our model outperformed all other models in terms of
accuracy, precision, recall, F1 score, and Mean Squared Error. Surprisingly, it
also needed less training time, resulting in a perfect combination of
performance and efficiency. This study demonstrates the utility of ensemble
models for difficult tasks such as gene mutation classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14367">Prot2Text: Multimodal Protein&#x27;s Function Generation with GNNs and Transformers. (arXiv:2307.14367v1 [q-bio.QM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Abdine_H/0/1/0/all/0/1">Hadi Abdine</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Chatzianastasis_M/0/1/0/all/0/1">Michail Chatzianastasis</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Bouyioukos_C/0/1/0/all/0/1">Costas Bouyioukos</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Vazirgiannis_M/0/1/0/all/0/1">Michalis Vazirgiannis</a></p>
<p>The complex nature of big biological systems pushed some scientists to
classify its understanding under the inconceivable missions. Different leveled
challenges complicated this task, one of is the prediction of a protein's
function. In recent years, significant progress has been made in this field
through the development of various machine learning approaches. However, most
existing methods formulate the task as a multi-classification problem, i.e
assigning predefined labels to proteins. In this work, we propose a novel
approach, \textbf{Prot2Text}, which predicts a protein function's in a free
text style, moving beyond the conventional binary or categorical
classifications. By combining Graph Neural Networks(GNNs) and Large Language
Models(LLMs), in an encoder-decoder framework, our model effectively integrates
diverse data types including proteins' sequences, structures, and textual
annotations. This multimodal approach allows for a holistic representation of
proteins' functions, enabling the generation of detailed and accurate
descriptions. To evaluate our model, we extracted a multimodal protein dataset
from SwissProt, and demonstrate empirically the effectiveness of Prot2Text.
These results highlight the transformative impact of multimodal models,
specifically the fusion of GNNs and LLMs, empowering researchers with powerful
tools for more accurate prediction of proteins' functions. The code, the models
and a demo will be publicly released.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14377">How Can Large Language Models Help Humans in Design and Manufacturing?. (arXiv:2307.14377v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Makatura_L/0/1/0/all/0/1">Liane Makatura</a>, <a href="http://arxiv.org/find/cs/1/au:+Foshey_M/0/1/0/all/0/1">Michael Foshey</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bohan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+HahnLein_F/0/1/0/all/0/1">Felix H&#xe4;hnLein</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_P/0/1/0/all/0/1">Pingchuan Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1">Bolei Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Tjandrasuwita_M/0/1/0/all/0/1">Megan Tjandrasuwita</a>, <a href="http://arxiv.org/find/cs/1/au:+Spielberg_A/0/1/0/all/0/1">Andrew Spielberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Owens_C/0/1/0/all/0/1">Crystal Elaine Owens</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1">Peter Yichen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_A/0/1/0/all/0/1">Allan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_A/0/1/0/all/0/1">Amy Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Norton_W/0/1/0/all/0/1">Wil J Norton</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_E/0/1/0/all/0/1">Edward Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jacob_J/0/1/0/all/0/1">Joshua Jacob</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yifei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Schulz_A/0/1/0/all/0/1">Adriana Schulz</a>, <a href="http://arxiv.org/find/cs/1/au:+Matusik_W/0/1/0/all/0/1">Wojciech Matusik</a></p>
<p>The advancement of Large Language Models (LLMs), including GPT-4, provides
exciting new opportunities for generative design. We investigate the
application of this tool across the entire design and manufacturing workflow.
Specifically, we scrutinize the utility of LLMs in tasks such as: converting a
text-based prompt into a design specification, transforming a design into
manufacturing instructions, producing a design space and design variations,
computing the performance of a design, and searching for designs predicated on
performance. Through a series of examples, we highlight both the benefits and
the limitations of the current LLMs. By exposing these limitations, we aspire
to catalyze the continued improvement and progression of these models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14385">Leveraging Large Language Models for Mental Health Prediction via Online Text Data. (arXiv:2307.14385v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xuhai Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1">Bingshen Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1">Yuanzhe Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Hong Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hendler_J/0/1/0/all/0/1">James Hendler</a>, <a href="http://arxiv.org/find/cs/1/au:+Dey_A/0/1/0/all/0/1">Anind K. Dey</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dakuo Wang</a></p>
<p>The recent technology boost of large language models (LLMs) has empowered a
variety of applications. However, there is very little research on
understanding and improving LLMs' capability for the mental health domain. In
this work, we present the first comprehensive evaluation of multiple LLMs,
including Alpaca, Alpaca-LoRA, and GPT-3.5, on various mental health prediction
tasks via online text data. We conduct a wide range of experiments, covering
zero-shot prompting, few-shot prompting, and instruction finetuning. The
results indicate the promising yet limited performance of LLMs with zero-shot
and few-shot prompt designs for mental health tasks. More importantly, our
experiments show that instruction finetuning can significantly boost the
performance of LLMs for all tasks simultaneously. Our best-finetuned model,
Mental-Alpaca, outperforms GPT-3.5 (25 times bigger) by 16.7\% on balanced
accuracy and performs on par with the state-of-the-art task-specific model. We
summarize our findings into a set of action guidelines for future researchers,
engineers, and practitioners on how to empower LLMs with better mental health
domain knowledge and become an expert in mental health prediction tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14389">Diff-E: Diffusion-based Learning for Decoding Imagined Speech EEG. (arXiv:2307.14389v1 [eess.AS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1">Soowon Kim</a>, <a href="http://arxiv.org/find/eess/1/au:+Lee_Y/0/1/0/all/0/1">Young-Eun Lee</a>, <a href="http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1">Seo-Hyun Lee</a>, <a href="http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1">Seong-Whan Lee</a></p>
<p>Decoding EEG signals for imagined speech is a challenging task due to the
high-dimensional nature of the data and low signal-to-noise ratio. In recent
years, denoising diffusion probabilistic models (DDPMs) have emerged as
promising approaches for representation learning in various domains. Our study
proposes a novel method for decoding EEG signals for imagined speech using
DDPMs and a conditional autoencoder named Diff-E. Results indicate that Diff-E
significantly improves the accuracy of decoding EEG signals for imagined speech
compared to traditional machine learning techniques and baseline models. Our
findings suggest that DDPMs can be an effective tool for EEG signal decoding,
with potential implications for the development of brain-computer interfaces
that enable communication through imagined speech.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14430">Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models. (arXiv:2307.14430v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Mayee F. Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Roberts_N/0/1/0/all/0/1">Nicholas Roberts</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhatia_K/0/1/0/all/0/1">Kush Bhatia</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jue Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Ce Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sala_F/0/1/0/all/0/1">Frederic Sala</a>, <a href="http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1">Christopher R&#xe9;</a></p>
<p>The quality of training data impacts the performance of pre-trained large
language models (LMs). Given a fixed budget of tokens, we study how to best
select data that leads to good downstream model performance across tasks. We
develop a new framework based on a simple hypothesis: just as humans acquire
interdependent skills in a deliberate order, language models also follow a
natural order when learning a set of skills from their training data. If such
an order exists, it can be utilized for improved understanding of LMs and for
data-efficient training. Using this intuition, our framework formalizes the
notion of a skill and of an ordered set of skills in terms of the associated
data. First, using both synthetic and real data, we demonstrate that these
ordered skill sets exist, and that their existence enables more advanced skills
to be learned with less data when we train on their prerequisite skills.
Second, using our proposed framework, we introduce an online data sampling
algorithm, Skill-It, over mixtures of skills for both continual pre-training
and fine-tuning regimes, where the objective is to efficiently learn multiple
skills in the former and an individual skill in the latter. On the LEGO
synthetic in the continual pre-training setting, Skill-It obtains 36.5 points
higher accuracy than random sampling. On the Natural Instructions dataset in
the fine-tuning setting, Skill-It reduces the validation loss on the target
skill by 13.6% versus training on data associated with the target skill itself.
We apply our skills framework on the recent RedPajama dataset to continually
pre-train a 3B-parameter LM, achieving higher accuracy on the LM Evaluation
Harness with 1B tokens than the baseline approach of sampling uniformly over
data sources with 3B tokens.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14440">Controllable Generation of Dialogue Acts for Dialogue Systems via Few-Shot Response Generation and Ranking. (arXiv:2307.14440v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ramirez_A/0/1/0/all/0/1">Angela Ramirez</a>, <a href="http://arxiv.org/find/cs/1/au:+Agarwal_K/0/1/0/all/0/1">Karik Agarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Juraska_J/0/1/0/all/0/1">Juraj Juraska</a>, <a href="http://arxiv.org/find/cs/1/au:+Garg_U/0/1/0/all/0/1">Utkarsh Garg</a>, <a href="http://arxiv.org/find/cs/1/au:+Walker_M/0/1/0/all/0/1">Marilyn A. Walker</a></p>
<p>Dialogue systems need to produce responses that realize multiple types of
dialogue acts (DAs) with high semantic fidelity. In the past, natural language
generators (NLGs) for dialogue were trained on large parallel corpora that map
from a domain-specific DA and its semantic attributes to an output utterance.
Recent work shows that pretrained language models (LLMs) offer new
possibilities for controllable NLG using prompt-based learning. Here we develop
a novel few-shot overgenerate-and-rank approach that achieves the controlled
generation of DAs. We compare eight few-shot prompt styles that include a novel
method of generating from textual pseudo-references using a textual style
transfer approach. We develop six automatic ranking functions that identify
outputs with both the correct DA and high semantic accuracy at generation time.
We test our approach on three domains and four LLMs. To our knowledge, this is
the first work on NLG for dialogue that automatically ranks outputs using both
DA and attribute accuracy. For completeness, we compare our results to
fine-tuned few-shot models trained with 5 to 100 instances per DA. Our results
show that several prompt settings achieve perfect DA accuracy, and near perfect
semantic accuracy (99.81%) and perform better than few-shot fine-tuning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14500">A Predictive Model of Digital Information Engagement: Forecasting User Engagement With English Words by Incorporating Cognitive Biases, Computational Linguistics and Natural Language Processing. (arXiv:2307.14500v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dvir_N/0/1/0/all/0/1">Nimrod Dvir</a>, <a href="http://arxiv.org/find/cs/1/au:+Friedman_E/0/1/0/all/0/1">Elaine Friedman</a>, <a href="http://arxiv.org/find/cs/1/au:+Commuri_S/0/1/0/all/0/1">Suraj Commuri</a>, <a href="http://arxiv.org/find/cs/1/au:+yang_F/0/1/0/all/0/1">Fan yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Romano_J/0/1/0/all/0/1">Jennifer Romano</a></p>
<p>This study introduces and empirically tests a novel predictive model for
digital information engagement (IE) - the READ model, an acronym for the four
pivotal attributes of engaging information: Representativeness, Ease-of-use,
Affect, and Distribution. Conceptualized within the theoretical framework of
Cumulative Prospect Theory, the model integrates key cognitive biases with
computational linguistics and natural language processing to develop a
multidimensional perspective on information engagement. A rigorous testing
protocol was implemented, involving 50 randomly selected pairs of synonymous
words (100 words in total) from the WordNet database. These words' engagement
levels were evaluated through a large-scale online survey (n = 80,500) to
derive empirical IE metrics. The READ attributes for each word were then
computed and their predictive efficacy examined. The findings affirm the READ
model's robustness, accurately predicting a word's IE level and distinguishing
the more engaging word from a pair of synonyms with an 84% accuracy rate. The
READ model's potential extends across various domains, including business,
education, government, and healthcare, where it could enhance content
engagement and inform AI language model development and generative text work.
Future research should address the model's scalability and adaptability across
different domains and languages, thereby broadening its applicability and
efficacy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14511">Words That Stick: Predicting Decision Making and Synonym Engagement Using Cognitive Biases and Computational Linguistics. (arXiv:2307.14511v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dvir_N/0/1/0/all/0/1">Nimrod Dvir</a>, <a href="http://arxiv.org/find/cs/1/au:+Friedman_E/0/1/0/all/0/1">Elaine Friedman</a>, <a href="http://arxiv.org/find/cs/1/au:+Commuri_S/0/1/0/all/0/1">Suraj Commuri</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1">Fan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Romano_J/0/1/0/all/0/1">Jennifer Romano</a></p>
<p>This research draws upon cognitive psychology and information systems studies
to anticipate user engagement and decision-making on digital platforms. By
employing natural language processing (NLP) techniques and insights from
cognitive bias research, we delve into user interactions with synonyms within
digital content. Our methodology synthesizes four cognitive
biasesRepresentativeness, Ease-of-use, Affect, and Distributioninto the READ
model. Through a comprehensive user survey, we assess the model's ability to
predict user engagement, discovering that synonyms that accurately represent
core ideas, are easy to understand, elicit emotional responses, and are
commonly encountered, promote greater user engagement. Crucially, our work
offers a fresh lens on human-computer interaction, digital behaviors, and
decision-making processes. Our results highlight the promise of cognitive
biases as potent indicators of user engagement, underscoring their significance
in designing effective digital content across fields like education and
marketing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14522">CliniDigest: A Case Study in Large Language Model Based Large-Scale Summarization of Clinical Trial Descriptions. (arXiv:2307.14522v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+White_R/0/1/0/all/0/1">Renee D. White</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Peng_T/0/1/0/all/0/1">Tristan Peng</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Sripitak_P/0/1/0/all/0/1">Pann Sripitak</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Johansen_A/0/1/0/all/0/1">Alexander Rosenberg Johansen</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Snyder_M/0/1/0/all/0/1">Michael Snyder</a> (1) (1) <a href="http://arxiv.org/find/cs/1/au:+University_S/0/1/0/all/0/1">Stanford University</a></p>
<p>A clinical trial is a study that evaluates new biomedical interventions. To
design new trials, researchers draw inspiration from those current and
completed. In 2022, there were on average more than 100 clinical trials
submitted to ClinicalTrials.gov every day, with each trial having a mean of
approximately 1500 words [1]. This makes it nearly impossible to keep up to
date. To mitigate this issue, we have created a batch clinical trial summarizer
called CliniDigest using GPT-3.5. CliniDigest is, to our knowledge, the first
tool able to provide real-time, truthful, and comprehensive summaries of
clinical trials. CliniDigest can reduce up to 85 clinical trial descriptions
(approximately 10,500 words) into a concise 200-word summary with references
and limited hallucinations. We have tested CliniDigest on its ability to
summarize 457 trials divided across 27 medical subdomains. For each field,
CliniDigest generates summaries of $\mu=153,\ \sigma=69 $ words, each of which
utilizes $\mu=54\%,\ \sigma=30\% $ of the sources. A more comprehensive
evaluation is planned and outlined in this paper.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14539">Plug and Pray: Exploiting off-the-shelf components of Multi-Modal Models. (arXiv:2307.14539v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shayegani_E/0/1/0/all/0/1">Erfan Shayegani</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1">Yue Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Abu_Ghazaleh_N/0/1/0/all/0/1">Nael Abu-Ghazaleh</a></p>
<p>The rapid growth and increasing popularity of incorporating additional
modalities (e.g., vision) into large language models (LLMs) has raised
significant security concerns. This expansion of modality, akin to adding more
doors to a house, unintentionally creates multiple access points for
adversarial attacks. In this paper, by introducing adversarial embedding space
attacks, we emphasize the vulnerabilities present in multi-modal systems that
originate from incorporating off-the-shelf components like public pre-trained
encoders in a plug-and-play manner into these systems. In contrast to existing
work, our approach does not require access to the multi-modal system's weights
or parameters but instead relies on the huge under-explored embedding space of
such pre-trained encoders. Our proposed embedding space attacks involve seeking
input images that reside within the dangerous or targeted regions of the
extensive embedding space of these pre-trained components. These crafted
adversarial images pose two major threats: 'Context Contamination' and 'Hidden
Prompt Injection'-both of which can compromise multi-modal models like LLaVA
and fully change the behavior of the associated language model. Our findings
emphasize the need for a comprehensive examination of the underlying
components, particularly pre-trained encoders, before incorporating them into
systems in a plug-and-play manner to ensure robust security.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14544">Speed Reading Tool Powered by Artificial Intelligence for Students with ADHD, Dyslexia, or Short Attention Span. (arXiv:2307.14544v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kamarozaman_M/0/1/0/all/0/1">Megat Irfan Zackry Bin Ismail Ahmad Nazran bin Yusri Muhammad Hafizzul Bin Abdul Manap Muhammad Muizzuddin Bin Kamarozaman</a></p>
<p>This paper presents a novel approach to assist students with dyslexia, ADHD,
and short attention span in digesting any text-based information more
efficiently. The proposed solution utilizes the Multilayer Perceptron (MLP)
algorithm for complex text processing and summarization tasks. The tool
leverages the T5 (Text-to-Text Transfer Transformer) model from Hugging Face,
which treats every NLP task as a text generation task. The model is fine-tuned
on specific tasks using a smaller dataset. The NLTK's Punkt Sentence Tokenizer
is used to divide a text into a list of sentences. The application is served
using Flask, a lightweight web server and framework. The tool also applies
principles from Bionic Reading to enhance readability, which includes a bolding
function and adjustments to line, word, and character spacing. The paper
discusses the methodology, implementation, and results of the AI-based speed
reading tool.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14632">Metric-Based In-context Learning: A Case Study in Text Simplification. (arXiv:2307.14632v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vadlamannati_S/0/1/0/all/0/1">Subha Vadlamannati</a>, <a href="http://arxiv.org/find/cs/1/au:+Sahin_G/0/1/0/all/0/1">G&#xf6;zde G&#xfc;l &#x15e;ahin</a></p>
<p>In-context learning (ICL) for large language models has proven to be a
powerful approach for many natural language processing tasks. However,
determining the best method to select examples for ICL is nontrivial as the
results can vary greatly depending on the quality, quantity, and order of
examples used. In this paper, we conduct a case study on text simplification
(TS) to investigate how to select the best and most robust examples for ICL. We
propose Metric-Based in-context Learning (MBL) method that utilizes commonly
used TS metrics such as SARI, compression ratio, and BERT-Precision for
selection. Through an extensive set of experiments with various-sized GPT
models on standard TS benchmarks such as TurkCorpus and ASSET, we show that
examples selected by the top SARI scores perform the best on larger models such
as GPT-175B, while the compression ratio generally performs better on smaller
models such as GPT-13B and GPT-6.7B. Furthermore, we demonstrate that MBL is
generally robust to example orderings and out-of-domain test sets, and
outperforms strong baselines and state-of-the-art finetuned language models.
Finally, we show that the behaviour of large GPT models can be implicitly
controlled by the chosen metric. Our research provides a new framework for
selecting examples in ICL, and demonstrates its effectiveness in text
simplification tasks, breaking new ground for more accurate and efficient NLG
systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14666">Improving Natural Language Inference in Arabic using Transformer Models and Linguistically Informed Pre-Training. (arXiv:2307.14666v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Deen_M/0/1/0/all/0/1">Mohammad Majd Saad Al Deen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pielka_M/0/1/0/all/0/1">Maren Pielka</a>, <a href="http://arxiv.org/find/cs/1/au:+Hees_J/0/1/0/all/0/1">J&#xf6;rn Hees</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdou_B/0/1/0/all/0/1">Bouthaina Soulef Abdou</a>, <a href="http://arxiv.org/find/cs/1/au:+Sifa_R/0/1/0/all/0/1">Rafet Sifa</a></p>
<p>This paper addresses the classification of Arabic text data in the field of
Natural Language Processing (NLP), with a particular focus on Natural Language
Inference (NLI) and Contradiction Detection (CD). Arabic is considered a
resource-poor language, meaning that there are few data sets available, which
leads to limited availability of NLP methods. To overcome this limitation, we
create a dedicated data set from publicly available resources. Subsequently,
transformer-based machine learning models are being trained and evaluated. We
find that a language-specific model (AraBERT) performs competitively with
state-of-the-art multilingual approaches, when we apply linguistically informed
pre-training methods such as Named Entity Recognition (NER). To our knowledge,
this is the first large-scale evaluation for this task in Arabic, as well as
the first application of multi-task pre-training in this context.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14712">Evaluating Generative Models for Graph-to-Text Generation. (arXiv:2307.14712v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1">Shuzhou Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Farber_M/0/1/0/all/0/1">Michael F&#xe4;rber</a></p>
<p>Large language models (LLMs) have been widely employed for graph-to-text
generation tasks. However, the process of finetuning LLMs requires significant
training resources and annotation work. In this paper, we explore the
capability of generative models to generate descriptive text from graph data in
a zero-shot setting. Specifically, we evaluate GPT-3 and ChatGPT on two
graph-to-text datasets and compare their performance with that of finetuned LLM
models such as T5 and BART. Our results demonstrate that generative models are
capable of generating fluent and coherent text, achieving BLEU scores of 10.57
and 11.08 for the AGENDA and WebNLG datasets, respectively. However, our error
analysis reveals that generative models still struggle with understanding the
semantic relations between entities, and they also tend to generate text with
hallucinations or irrelevant information. As a part of error analysis, we
utilize BERT to detect machine-generated text and achieve high macro-F1 scores.
We have made the text generated by generative models publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14743">Turning Whisper into Real-Time Transcription System. (arXiv:2307.14743v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Machacek_D/0/1/0/all/0/1">Dominik Mach&#xe1;&#x10d;ek</a>, <a href="http://arxiv.org/find/cs/1/au:+Dabre_R/0/1/0/all/0/1">Raj Dabre</a>, <a href="http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1">Ond&#x159;ej Bojar</a></p>
<p>Whisper is one of the recent state-of-the-art multilingual speech recognition
and translation models, however, it is not designed for real time
transcription. In this paper, we build on top of Whisper and create
Whisper-Streaming, an implementation of real-time speech transcription and
translation of Whisper-like models. Whisper-Streaming uses local agreement
policy with self-adaptive latency to enable streaming transcription. We show
that Whisper-Streaming achieves high quality and 3.3 seconds latency on
unsegmented long-form speech transcription test set, and we demonstrate its
robustness and practical usability as a component in live transcription service
at a multilingual conference.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14783">Emotion4MIDI: a Lyrics-based Emotion-Labeled Symbolic Music Dataset. (arXiv:2307.14783v1 [eess.AS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Sulun_S/0/1/0/all/0/1">Serkan Sulun</a>, <a href="http://arxiv.org/find/eess/1/au:+Oliveira_P/0/1/0/all/0/1">Pedro Oliveira</a>, <a href="http://arxiv.org/find/eess/1/au:+Viana_P/0/1/0/all/0/1">Paula Viana</a></p>
<p>We present a new large-scale emotion-labeled symbolic music dataset
consisting of 12k MIDI songs. To create this dataset, we first trained emotion
classification models on the GoEmotions dataset, achieving state-of-the-art
results with a model half the size of the baseline. We then applied these
models to lyrics from two large-scale MIDI datasets. Our dataset covers a wide
range of fine-grained emotions, providing a valuable resource to explore the
connection between music and emotions and, especially, to develop models that
can generate music based on specific emotions. Our code for inference, trained
models, and datasets are available online.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14785">Improving Aspect-Based Sentiment with End-to-End Semantic Role Labeling Model. (arXiv:2307.14785v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Priban_P/0/1/0/all/0/1">Pavel P&#x159;ib&#xe1;&#x148;</a>, <a href="http://arxiv.org/find/cs/1/au:+Prazak_O/0/1/0/all/0/1">Ond&#x159;ej Pra&#x17e;&#xe1;k</a></p>
<p>This paper presents a series of approaches aimed at enhancing the performance
of Aspect-Based Sentiment Analysis (ABSA) by utilizing extracted semantic
information from a Semantic Role Labeling (SRL) model. We propose a novel
end-to-end Semantic Role Labeling model that effectively captures most of the
structured semantic information within the Transformer hidden state. We believe
that this end-to-end model is well-suited for our newly proposed models that
incorporate semantic information. We evaluate the proposed models in two
languages, English and Czech, employing ELECTRA-small models. Our combined
models improve ABSA performance in both languages. Moreover, we achieved new
state-of-the-art results on the Czech ABSA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14817">Models of reference production: How do they withstand the test of time?. (arXiv:2307.14817v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Same_F/0/1/0/all/0/1">Fahime Same</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1">Guanyi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Deemter_K/0/1/0/all/0/1">Kees van Deemter</a></p>
<p>In recent years, many NLP studies have focused solely on performance
improvement. In this work, we focus on the linguistic and scientific aspects of
NLP. We use the task of generating referring expressions in context
(REG-in-context) as a case study and start our analysis from GREC, a
comprehensive set of shared tasks in English that addressed this topic over a
decade ago. We ask what the performance of models would be if we assessed them
(1) on more realistic datasets, and (2) using more advanced methods. We test
the models using different evaluation metrics and feature selection
experiments. We conclude that GREC can no longer be regarded as offering a
reliable assessment of models' ability to mimic human reference production,
because the results are highly impacted by the choice of corpus and evaluation
metrics. Our results also suggest that pre-trained language models are less
dependent on the choice of corpus than classic Machine Learning models, and
therefore make more robust class predictions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14818">What Makes a Good Paraphrase: Do Automated Evaluations Work?. (arXiv:2307.14818v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Moskvina_A/0/1/0/all/0/1">Anna Moskvina</a>, <a href="http://arxiv.org/find/cs/1/au:+Kotnis_B/0/1/0/all/0/1">Bhushan Kotnis</a>, <a href="http://arxiv.org/find/cs/1/au:+Catacata_C/0/1/0/all/0/1">Chris Catacata</a>, <a href="http://arxiv.org/find/cs/1/au:+Janz_M/0/1/0/all/0/1">Michael Janz</a>, <a href="http://arxiv.org/find/cs/1/au:+Saef_N/0/1/0/all/0/1">Nasrin Saef</a></p>
<p>Paraphrasing is the task of expressing an essential idea or meaning in
different words. But how different should the words be in order to be
considered an acceptable paraphrase? And can we exclusively use automated
metrics to evaluate the quality of a paraphrase? We attempt to answer these
questions by conducting experiments on a German data set and performing
automatic and expert linguistic evaluation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14850">Turkish Native Language Identification. (arXiv:2307.14850v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Uluslu_A/0/1/0/all/0/1">Ahmet Yavuz Uluslu</a>, <a href="http://arxiv.org/find/cs/1/au:+Schneider_G/0/1/0/all/0/1">Gerold Schneider</a></p>
<p>In this paper, we present the first application of Native Language
Identification (NLI) for the Turkish language. NLI involves predicting the
writer's first language by analysing their writing in different languages.
While most NLI research has focused on English, our study extends its scope to
Turkish. We used the recently constructed Turkish Learner Corpus and employed a
combination of three syntactic features (CFG production rules, part-of-speech
n-grams and function words) with L2 texts to demonstrate their effectiveness in
this task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14852">ArcGPT: A Large Language Model Tailored for Real-world Archival Applications. (arXiv:2307.14852v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shitou Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1">Jingrui Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1">Siyuan Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zuchao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1">Qibiao Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Ping Wang</a></p>
<p>Archives play a crucial role in preserving information and knowledge, and the
exponential growth of such data necessitates efficient and automated tools for
managing and utilizing archive information resources. Archival applications
involve managing massive data that are challenging to process and analyze.
Although LLMs have made remarkable progress in diverse domains, there are no
publicly available archives tailored LLM. Addressing this gap, we introduce
ArcGPT, to our knowledge, the first general-purpose LLM tailored to the
archival field. To enhance model performance on real-world archival tasks,
ArcGPT has been pre-trained on massive and extensive archival domain data.
Alongside ArcGPT, we release AMBLE, a benchmark comprising four real-world
archival tasks. Evaluation on AMBLE shows that ArcGPT outperforms existing
state-of-the-art models, marking a substantial step forward in effective
archival data management. Ultimately, ArcGPT aims to better serve the archival
community, aiding archivists in their crucial role of preserving and harnessing
our collective information and knowledge.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14856">Exploiting the Potential of Seq2Seq Models as Robust Few-Shot Learners. (arXiv:2307.14856v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jihyeon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1">Dain Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_D/0/1/0/all/0/1">Doohae Jung</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1">Boseop Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+On_K/0/1/0/all/0/1">Kyoung-Woon On</a></p>
<p>In-context learning, which offers substantial advantages over fine-tuning, is
predominantly observed in decoder-only models, while encoder-decoder (i.e.,
seq2seq) models excel in methods that rely on weight updates. Recently, a few
studies have demonstrated the feasibility of few-shot learning with seq2seq
models; however, this has been limited to tasks that align well with the
seq2seq architecture, such as summarization and translation. Inspired by these
initial studies, we provide a first-ever extensive experiment comparing the
in-context few-shot learning capabilities of decoder-only and encoder-decoder
models on a broad range of tasks. Furthermore, we propose two methods to more
effectively elicit in-context learning ability in seq2seq models:
objective-aligned prompting and a fusion-based approach. Remarkably, our
approach outperforms a decoder-only model that is six times larger and exhibits
significant performance improvements compared to conventional seq2seq models
across a variety of settings. We posit that, with the right configuration and
prompt design, seq2seq models can be highly effective few-shot learners for a
wide spectrum of applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14878">MESED: A Multi-modal Entity Set Expansion Dataset with Fine-grained Semantic Classes and Hard Negative Entities. (arXiv:2307.14878v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yangning Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1">Tingwei Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yinghui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1">Tianyu Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Shulin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1">Hai-Tao Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Rui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1">Jun Yuan</a></p>
<p>The Entity Set Expansion (ESE) task aims to expand a handful of seed entities
with new entities belonging to the same semantic class. Conventional ESE
methods are based on mono-modality (i.e., literal modality), which struggle to
deal with complex entities in the real world such as: (1) Negative entities
with fine-grained semantic differences. (2) Synonymous entities. (3) Polysemous
entities. (4) Long-tailed entities. These challenges prompt us to propose
Multi-modal Entity Set Expansion (MESE), where models integrate information
from multiple modalities to represent entities. Intuitively, the benefits of
multi-modal information for ESE are threefold: (1) Different modalities can
provide complementary information. (2) Multi-modal information provides a
unified signal via common visual properties for the same semantic class or
entity. (3) Multi-modal information offers robust alignment signal for
synonymous entities. To assess the performance of model in MESE and facilitate
further research, we constructed the MESED dataset which is the first
multi-modal dataset for ESE with large-scale and elaborate manual calibration.
A powerful multi-modal model MultiExpan is proposed which is pre-trained on
four multimodal pre-training tasks. The extensive experiments and analyses on
MESED demonstrate the high quality of the dataset and the effectiveness of our
MultiExpan, as well as pointing the direction for future research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14899">Retrieval-based Text Selection for Addressing Class-Imbalanced Data in Classification. (arXiv:2307.14899v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ahmadi_S/0/1/0/all/0/1">Sareh Ahmadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1">Aditya Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Fox_E/0/1/0/all/0/1">Edward Fox</a></p>
<p>This paper addresses the problem of selecting of a set of texts for
annotation in text classification using retrieval methods when there are limits
on the number of annotations due to constraints on human resources. An
additional challenge addressed is dealing with binary categories that have a
small number of positive instances, reflecting severe class imbalance. In our
situation, where annotation occurs over a long time period, the selection of
texts to be annotated can be made in batches, with previous annotations guiding
the choice of the next set. To address these challenges, the paper proposes
leveraging SHAP to construct a quality set of queries for Elasticsearch and
semantic search, to try to identify optimal sets of texts for annotation that
will help with class imbalance. The approach is tested on sets of cue texts
describing possible future events, constructed by participants involved in
studies aimed to help with the management of obesity and diabetes. We introduce
an effective method for selecting a small set of texts for annotation and
building high-quality classifiers. We integrate vector search, semantic search,
and machine learning classifiers to yield a good solution. Our experiments
demonstrate improved F1 scores for the minority classes in binary
classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14912">ARC-NLP at PAN 2023: Hierarchical Long Text Classification for Trigger Detection. (arXiv:2307.14912v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sahin_U/0/1/0/all/0/1">Umitcan Sahin</a>, <a href="http://arxiv.org/find/cs/1/au:+Kucukkaya_I/0/1/0/all/0/1">Izzet Emre Kucukkaya</a>, <a href="http://arxiv.org/find/cs/1/au:+Toraman_C/0/1/0/all/0/1">Cagri Toraman</a></p>
<p>Fanfiction, a popular form of creative writing set within established
fictional universes, has gained a substantial online following. However,
ensuring the well-being and safety of participants has become a critical
concern in this community. The detection of triggering content, material that
may cause emotional distress or trauma to readers, poses a significant
challenge. In this paper, we describe our approach for the Trigger Detection
shared task at PAN CLEF 2023, where we want to detect multiple triggering
content in a given Fanfiction document. For this, we build a hierarchical model
that uses recurrence over Transformer-based language models. In our approach,
we first split long documents into smaller sized segments and use them to
fine-tune a Transformer model. Then, we extract feature embeddings from the
fine-tuned Transformer model, which are used as input in the training of
multiple LSTM models for trigger detection in a multi-label setting. Our model
achieves an F1-macro score of 0.372 and F1-micro score of 0.736 on the
validation set, which are higher than the baseline results shared at PAN CLEF
2023.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14913">ARC-NLP at PAN 2023: Transition-Focused Natural Language Inference for Writing Style Detection. (arXiv:2307.14913v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kucukkaya_I/0/1/0/all/0/1">Izzet Emre Kucukkaya</a>, <a href="http://arxiv.org/find/cs/1/au:+Sahin_U/0/1/0/all/0/1">Umitcan Sahin</a>, <a href="http://arxiv.org/find/cs/1/au:+Toraman_C/0/1/0/all/0/1">Cagri Toraman</a></p>
<p>The task of multi-author writing style detection aims at finding any
positions of writing style change in a given text document. We formulate the
task as a natural language inference problem where two consecutive paragraphs
are paired. Our approach focuses on transitions between paragraphs while
truncating input tokens for the task. As backbone models, we employ different
Transformer-based encoders with warmup phase during training. We submit the
model version that outperforms baselines and other proposed model versions in
our experiments. For the easy and medium setups, we submit transition-focused
natural language inference based on DeBERTa with warmup training, and the same
model without transition for the hard setup.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14936">PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback. (arXiv:2307.14936v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1">Bo Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiaxin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Taihong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zan_D/0/1/0/all/0/1">Daoguang Zan</a>, <a href="http://arxiv.org/find/cs/1/au:+Geng_B/0/1/0/all/0/1">Bing Geng</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_A/0/1/0/all/0/1">An Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1">Muhan Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1">Ailun Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1">Jichuan Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jingyang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yuenan Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qianxiang Wang</a></p>
<p>Large Language Models for Code (Code LLM) are flourishing. New and powerful
models are released on a weekly basis, demonstrating remarkable performance on
the code generation task. Various approaches have been proposed to boost the
code generation performance of pre-trained Code LLMs, such as supervised
fine-tuning, instruction tuning, reinforcement learning, etc. In this paper, we
propose a novel RRTF (Rank Responses to align Test&amp;Teacher Feedback) framework,
which can effectively and efficiently boost pre-trained large language models
for code generation. Under this framework, we present PanGu-Coder2, which
achieves 62.20% pass@1 on the OpenAI HumanEval benchmark. Furthermore, through
an extensive evaluation on CoderEval and LeetCode benchmarks, we show that
PanGu-Coder2 consistently outperforms all previous Code LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14988">Incrementally-Computable Neural Networks: Efficient Inference for Dynamic Inputs. (arXiv:2307.14988v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sharir_O/0/1/0/all/0/1">Or Sharir</a>, <a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1">Anima Anandkumar</a></p>
<p>Deep learning often faces the challenge of efficiently processing dynamic
inputs, such as sensor data or user inputs. For example, an AI writing
assistant is required to update its suggestions in real time as a document is
edited. Re-running the model each time is expensive, even with compression
techniques like knowledge distillation, pruning, or quantization. Instead, we
take an incremental computing approach, looking to reuse calculations as the
inputs change. However, the dense connectivity of conventional architectures
poses a major obstacle to incremental computation, as even minor input changes
cascade through the network and restrict information reuse. To address this, we
use vector quantization to discretize intermediate values in the network, which
filters out noisy and unnecessary modifications to hidden neurons, facilitating
the reuse of their values. We apply this approach to the transformers
architecture, creating an efficient incremental inference algorithm with
complexity proportional to the fraction of the modified inputs. Our experiments
with adapting the OPT-125M pre-trained language model demonstrate comparable
accuracy on document classification while requiring 12.1X (median) fewer
operations for processing sequences of atomic edits.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14995">Scaling TransNormer to 175 Billion Parameters. (arXiv:2307.14995v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1">Zhen Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1">Weigao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1">Weixuan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1">Xuyang Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1">Xiaodong Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1">Yunshen Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_B/0/1/0/all/0/1">Baohong Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_F/0/1/0/all/0/1">Fei Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1">Xiao Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1">Yiran Zhong</a></p>
<p>We present TransNormerLLM, the first linear attention-based Large Language
Model (LLM) that outperforms conventional softmax attention-based models in
terms of both accuracy and efficiency. TransNormerLLM evolves from the previous
linear attention architecture TransNormer by making advanced modifications that
include positional embedding, linear attention acceleration, gating mechanism,
tensor normalization, inference acceleration and stabilization. Specifically,
we use LRPE together with an exponential decay to avoid attention dilution
issues while allowing the model to retain global interactions between tokens.
Additionally, we propose Lightning Attention, a cutting-edge technique that
accelerates linear attention by more than twice in runtime and reduces memory
usage by a remarkable four times. To further enhance the performance of
TransNormer, we leverage a gating mechanism to smooth training and a new tensor
normalization scheme to accelerate the model, resulting in an impressive
acceleration of over 20%. Furthermore, we have developed a robust inference
algorithm that ensures numerical stability and consistent inference speed,
regardless of the sequence length, showcasing superior efficiency during both
training and inference stages. Scalability is at the heart of our model's
design, enabling seamless deployment on large-scale clusters and facilitating
expansion to even more extensive models, all while maintaining outstanding
performance metrics. Rigorous validation of our model design is achieved
through a series of comprehensive experiments on our self-collected corpus,
boasting a size exceeding 6TB and containing over 2 trillion tokens. To ensure
data quality and relevance, we implement a new self-cleaning strategy to filter
our collected data. Our pre-trained models will be released to foster community
advancements in efficient LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.15002">Gzip versus bag-of-words for text classification with KNN. (arXiv:2307.15002v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Opitz_J/0/1/0/all/0/1">Juri Opitz</a></p>
<p>The effectiveness of compression distance in KNN-based text classification
('gzip') has recently garnered lots of attention. In this note, we show that
similar or better effectiveness can be achieved with simpler means, and text
compression may not be necessary. Indeed, we find that a simple 'bag-of-words'
matching can achieve similar or better accuracy, and is more efficient.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.15020">SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark. (arXiv:2307.15020v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1">Liang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1">Anqi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1">Lei Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1">Hang Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1">Changtai Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1">Kangkang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1">Haonan He</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xuanwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_Q/0/1/0/all/0/1">Qiyue Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1">Zhenzhong Lan</a></p>
<p>Large language models (LLMs) have shown the potential to be integrated into
human daily lives. Therefore, user preference is the most critical criterion
for assessing LLMs' performance in real-world scenarios. However, existing
benchmarks mainly focus on measuring models' accuracy using multi-choice
questions, which limits the understanding of their capabilities in real
applications. We fill this gap by proposing a comprehensive Chinese benchmark
SuperCLUE, named after another popular Chinese LLM benchmark CLUE. SuperCLUE
encompasses three sub-tasks: actual users' queries and ratings derived from an
LLM battle platform (CArena), open-ended questions with single and
multiple-turn dialogues (OPEN), and closed-ended questions with the same stems
as open-ended single-turn ones (CLOSE). Our study shows that accuracy on
closed-ended questions is insufficient to reflect human preferences achieved on
open-ended ones. At the same time, they can complement each other to predict
actual user preferences. We also demonstrate that GPT-4 is a reliable judge to
automatically evaluate human preferences on open-ended questions in a Chinese
context. Our benchmark will be released at https://www.CLUEbenchmarks.com
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.15043">Universal and Transferable Adversarial Attacks on Aligned Language Models. (arXiv:2307.15043v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zou_A/0/1/0/all/0/1">Andy Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zifan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1">J. Zico Kolter</a>, <a href="http://arxiv.org/find/cs/1/au:+Fredrikson_M/0/1/0/all/0/1">Matt Fredrikson</a></p>
<p>Because "out-of-the-box" large language models are capable of generating a
great deal of objectionable content, recent work has focused on aligning these
models in an attempt to prevent undesirable generation. While there has been
some success at circumventing these measures -- so-called "jailbreaks" against
LLMs -- these attacks have required significant human ingenuity and are brittle
in practice. In this paper, we propose a simple and effective attack method
that causes aligned language models to generate objectionable behaviors.
Specifically, our approach finds a suffix that, when attached to a wide range
of queries for an LLM to produce objectionable content, aims to maximize the
probability that the model produces an affirmative response (rather than
refusing to answer). However, instead of relying on manual engineering, our
approach automatically produces these adversarial suffixes by a combination of
greedy and gradient-based search techniques, and also improves over past
automatic prompt generation methods.
</p>
<p>Surprisingly, we find that the adversarial prompts generated by our approach
are quite transferable, including to black-box, publicly released LLMs.
Specifically, we train an adversarial attack suffix on multiple prompts (i.e.,
queries asking for many different types of objectionable content), as well as
multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting
attack suffix is able to induce objectionable content in the public interfaces
to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat,
Pythia, Falcon, and others. In total, this work significantly advances the
state-of-the-art in adversarial attacks against aligned language models,
raising important questions about how such systems can be prevented from
producing objectionable information. Code is available at
github.com/llm-attacks/llm-attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.15051">Matching Patients to Clinical Trials with Large Language Models. (arXiv:2307.15051v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1">Qiao Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zifeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Floudas_C/0/1/0/all/0/1">Charalampos S. Floudas</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jimeng Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Zhiyong Lu</a></p>
<p>Clinical trials are vital in advancing drug development and evidence-based
medicine, but their success is often hindered by challenges in patient
recruitment. In this work, we investigate the potential of large language
models (LLMs) to assist individual patients and referral physicians in
identifying suitable clinical trials from an extensive selection. Specifically,
we introduce TrialGPT, a novel architecture employing LLMs to predict
criterion-level eligibility with detailed explanations, which are then
aggregated for ranking and excluding candidate clinical trials based on
free-text patient notes. We evaluate TrialGPT on three publicly available
cohorts of 184 patients and 18,238 annotated clinical trials. The experimental
results demonstrate several key findings: First, TrialGPT achieves high
criterion-level prediction accuracy with faithful explanations. Second, the
aggregated trial-level TrialGPT scores are highly correlated with expert
eligibility annotations. Third, these scores prove effective in ranking
clinical trials and exclude ineligible candidates. Our error analysis suggests
that current LLMs still make some mistakes due to limited medical knowledge and
domain-specific context understanding. Nonetheless, we believe the explanatory
capabilities of LLMs are highly valuable. Future research is warranted on how
such AI assistants can be integrated into the routine trial matching workflow
in real-world settings to improve its efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.15054">A Geometric Notion of Causal Probing. (arXiv:2307.15054v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guerner_C/0/1/0/all/0/1">Cl&#xe9;ment Guerner</a>, <a href="http://arxiv.org/find/cs/1/au:+Svete_A/0/1/0/all/0/1">Anej Svete</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tianyu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Warstadt_A/0/1/0/all/0/1">Alexander Warstadt</a>, <a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1">Ryan Cotterell</a></p>
<p>Large language models rely on real-valued representations of text to make
their predictions. These representations contain information learned from the
data that the model has trained on, including knowledge of linguistic
properties and forms of demographic bias, e.g., based on gender. A growing body
of work has considered information about concepts such as these using
orthogonal projections onto subspaces of the representation space. We
contribute to this body of work by proposing a formal definition of intrinsic
information in a subspace of a language model's representation space. We
propose a counterfactual approach that avoids the failure mode of spurious
correlations (Kumar et al., 2022) by treating components in the subspace and
its orthogonal complement independently. We show that our counterfactual notion
of information in a subspace is optimizing by an causal concept subspace.
Furthermore, this intervention allows us to attempt concept controlled
generation by manipulating the value of the conceptual component of a
representation. Empirically, we find that R-LACE (Ravfogel et al., 2022)
returns a one-dimensional subspace containing roughly half of total concept
information under our framework. Our causal controlled intervention shows that,
for at least one model, the subspace returned by R-LACE can be used to
manipulate the concept value of the generated word with precision.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2108.04657">Differentiable Subset Pruning of Transformer Heads. (arXiv:2108.04657v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiaoda Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1">Ryan Cotterell</a>, <a href="http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1">Mrinmaya Sachan</a></p>
<p>Multi-head attention, a collection of several attention mechanisms that
independently attend to different parts of the input, is the key ingredient in
the Transformer. Recent work has shown, however, that a large proportion of the
heads in a Transformer's multi-head attention mechanism can be safely pruned
away without significantly harming the performance of the model; such pruning
leads to models that are noticeably smaller and faster in practice. Our work
introduces a new head pruning technique that we term differentiable subset
pruning. Intuitively, our method learns per-head importance variables and then
enforces a user-specified hard constraint on the number of unpruned heads. The
importance variables are learned via stochastic gradient descent. We conduct
experiments on natural language inference and machine translation; we show that
differentiable subset pruning performs comparably or better than previous works
while offering precise control of the sparsity level.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.14704">Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning. (arXiv:2205.14704v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xiang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Ningyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1">Xiaozhuan Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1">Shumin Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1">Chuanqi Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1">Fei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1">Luo Si</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huajun Chen</a></p>
<p>Prompt learning approaches have made waves in natural language processing by
inducing better few-shot performance while they still follow a parametric-based
learning paradigm; the oblivion and rote memorization problems in learning may
encounter unstable generalization issues. Specifically, vanilla prompt learning
may struggle to utilize atypical instances by rote during fully-supervised
training or overfit shallow patterns with low-shot data. To alleviate such
limitations, we develop RetroPrompt with the motivation of decoupling knowledge
from memorization to help the model strike a balance between generalization and
memorization. In contrast with vanilla prompt learning, RetroPrompt constructs
an open-book knowledge-store from training instances and implements a retrieval
mechanism during the process of input, training and inference, thus equipping
the model with the ability to retrieve related contexts from the training
corpus as cues for enhancement. Extensive experiments demonstrate that
RetroPrompt can obtain better performance in both few-shot and zero-shot
settings. Besides, we further illustrate that our proposed RetroPrompt can
yield better generalization abilities with new datasets. Detailed analysis of
memorization indeed reveals RetroPrompt can reduce the reliance of language
models on memorization; thus, improving generalization for downstream tasks.
Code is available in
https://github.com/zjunlp/PromptKG/tree/main/research/RetroPrompt.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.08411">Large Language Models Struggle to Learn Long-Tail Knowledge. (arXiv:2211.08411v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kandpal_N/0/1/0/all/0/1">Nikhil Kandpal</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1">Haikang Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1">Adam Roberts</a>, <a href="http://arxiv.org/find/cs/1/au:+Wallace_E/0/1/0/all/0/1">Eric Wallace</a>, <a href="http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1">Colin Raffel</a></p>
<p>The Internet contains a wealth of knowledge -- from the birthdays of
historical figures to tutorials on how to code -- all of which may be learned
by language models. However, while certain pieces of information are ubiquitous
on the web, others appear extremely rarely. In this paper, we study the
relationship between the knowledge memorized by large language models and the
information in pre-training datasets scraped from the web. In particular, we
show that a language model's ability to answer a fact-based question relates to
how many documents associated with that question were seen during pre-training.
We identify these relevant documents by entity linking pre-training datasets
and counting documents that contain the same entities as a given
question-answer pair. Our results demonstrate strong correlational and causal
relationships between accuracy and relevant document count for numerous
question answering datasets (e.g., TriviaQA), pre-training corpora (e.g.,
ROOTS), and model sizes (e.g., 176B parameters). Moreover, while larger models
are better at learning long-tail knowledge, we estimate that today's models
must be scaled by many orders of magnitude to reach competitive QA performance
on questions with little support in the pre-training data. Finally, we show
that retrieval-augmentation can reduce the dependence on relevant pre-training
information, presenting a promising approach for capturing the long-tail.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.11596">ThoughtSource: A central hub for large language model reasoning data. (arXiv:2301.11596v5 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ott_S/0/1/0/all/0/1">Simon Ott</a>, <a href="http://arxiv.org/find/cs/1/au:+Hebenstreit_K/0/1/0/all/0/1">Konstantin Hebenstreit</a>, <a href="http://arxiv.org/find/cs/1/au:+Lievin_V/0/1/0/all/0/1">Valentin Li&#xe9;vin</a>, <a href="http://arxiv.org/find/cs/1/au:+Hother_C/0/1/0/all/0/1">Christoffer Egeberg Hother</a>, <a href="http://arxiv.org/find/cs/1/au:+Moradi_M/0/1/0/all/0/1">Milad Moradi</a>, <a href="http://arxiv.org/find/cs/1/au:+Mayrhauser_M/0/1/0/all/0/1">Maximilian Mayrhauser</a>, <a href="http://arxiv.org/find/cs/1/au:+Praas_R/0/1/0/all/0/1">Robert Praas</a>, <a href="http://arxiv.org/find/cs/1/au:+Winther_O/0/1/0/all/0/1">Ole Winther</a>, <a href="http://arxiv.org/find/cs/1/au:+Samwald_M/0/1/0/all/0/1">Matthias Samwald</a></p>
<p>Large language models (LLMs) such as GPT-4 have recently demonstrated
impressive results across a wide range of tasks. LLMs are still limited,
however, in that they frequently fail at complex reasoning, their reasoning
processes are opaque, they are prone to 'hallucinate' facts, and there are
concerns about their underlying biases. Letting models verbalize reasoning
steps as natural language, a technique known as chain-of-thought prompting, has
recently been proposed as a way to address some of these issues. Here we
present ThoughtSource, a meta-dataset and software library for chain-of-thought
(CoT) reasoning. The goal of ThoughtSource is to improve future artificial
intelligence systems by facilitating qualitative understanding of CoTs,
enabling empirical evaluations, and providing training data. This first release
of ThoughtSource integrates seven scientific/medical, three general-domain and
five math word question answering datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.00083">In-Context Retrieval-Augmented Language Models. (arXiv:2302.00083v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ram_O/0/1/0/all/0/1">Ori Ram</a>, <a href="http://arxiv.org/find/cs/1/au:+Levine_Y/0/1/0/all/0/1">Yoav Levine</a>, <a href="http://arxiv.org/find/cs/1/au:+Dalmedigos_I/0/1/0/all/0/1">Itay Dalmedigos</a>, <a href="http://arxiv.org/find/cs/1/au:+Muhlgay_D/0/1/0/all/0/1">Dor Muhlgay</a>, <a href="http://arxiv.org/find/cs/1/au:+Shashua_A/0/1/0/all/0/1">Amnon Shashua</a>, <a href="http://arxiv.org/find/cs/1/au:+Leyton_Brown_K/0/1/0/all/0/1">Kevin Leyton-Brown</a>, <a href="http://arxiv.org/find/cs/1/au:+Shoham_Y/0/1/0/all/0/1">Yoav Shoham</a></p>
<p>Retrieval-Augmented Language Modeling (RALM) methods, which condition a
language model (LM) on relevant documents from a grounding corpus during
generation, were shown to significantly improve language modeling performance.
In addition, they can mitigate the problem of factually inaccurate text
generation and provide natural source attribution mechanism. Existing RALM
approaches focus on modifying the LM architecture in order to facilitate the
incorporation of external information, significantly complicating deployment.
This paper considers a simple alternative, which we dub In-Context RALM:
leaving the LM architecture unchanged and prepending grounding documents to the
input, without any further training of the LM. We show that In-Context RALM
that builds on off-the-shelf general purpose retrievers provides surprisingly
large LM gains across model sizes and diverse corpora. We also demonstrate that
the document retrieval and ranking mechanism can be specialized to the RALM
setting to further boost performance. We conclude that In-Context RALM has
considerable potential to increase the prevalence of LM grounding, particularly
in settings where a pretrained LM must be used without modification or even via
API access.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.12485">A Confidence-based Partial Label Learning Model for Crowd-Annotated Named Entity Recognition. (arXiv:2305.12485v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1">Limao Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jie Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1">Qunxi Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yuanbin Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1">Tao Gui</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xuanjing Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1">Jin Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1">Ying Shan</a></p>
<p>Existing models for named entity recognition (NER) are mainly based on
large-scale labeled datasets, which always obtain using crowdsourcing. However,
it is hard to obtain a unified and correct label via majority voting from
multiple annotators for NER due to the large labeling space and complexity of
this task. To address this problem, we aim to utilize the original
multi-annotator labels directly. Particularly, we propose a Confidence-based
Partial Label Learning (CPLL) method to integrate the prior confidence (given
by annotators) and posterior confidences (learned by models) for
crowd-annotated NER. This model learns a token- and content-dependent
confidence via an Expectation-Maximization (EM) algorithm by minimizing
empirical risk. The true posterior estimator and confidence estimator perform
iteratively to update the true posterior and confidence respectively. We
conduct extensive experimental results on both real-world and synthetic
datasets, which show that our model can improve performance effectively
compared with strong baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15299">Science in the Era of ChatGPT, Large Language Models and Generative AI: Challenges for Research Ethics and How to Respond. (arXiv:2305.15299v3 [cs.CY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pournaras_E/0/1/0/all/0/1">Evangelos Pournaras</a></p>
<p>Large language models of artificial intelligence (AI), such as ChatGPT, find
remarkable but controversial applicability in science and research. This paper
reviews epistemological challenges, ethical and integrity risks in science
conduct in the advent of generative AI. This is with the aim to lay new timely
foundations for a high-quality research ethics review. The role of AI language
models as a research instrument and subject is scrutinized along with ethical
implications for scientists, participants and reviewers. New emerging practices
for research ethics review are discussed, concluding with ten recommendations
that shape a response for a more responsible research conduct in the era of AI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16406">Context-aware attention layers coupled with optimal transport domain adaptation and multimodal fusion methods for recognizing dementia from spontaneous speech. (arXiv:2305.16406v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ilias_L/0/1/0/all/0/1">Loukas Ilias</a>, <a href="http://arxiv.org/find/cs/1/au:+Askounis_D/0/1/0/all/0/1">Dimitris Askounis</a></p>
<p>Alzheimer's disease (AD) constitutes a complex neurocognitive disease and is
the main cause of dementia. Although many studies have been proposed targeting
at diagnosing dementia through spontaneous speech, there are still limitations.
Existing state-of-the-art approaches, which propose multimodal methods, train
separately language and acoustic models, employ majority-vote approaches, and
concatenate the representations of the different modalities either at the input
level, i.e., early fusion, or during training. Also, some of them employ
self-attention layers, which calculate the dependencies between representations
without considering the contextual information. In addition, no prior work has
taken into consideration the model calibration. To address these limitations,
we propose some new methods for detecting AD patients, which capture the intra-
and cross-modal interactions. First, we convert the audio files into log-Mel
spectrograms, their delta, and delta-delta and create in this way an image per
audio file consisting of three channels. Next, we pass each transcript and
image through BERT and DeiT models respectively. After that, context-based
self-attention layers, self-attention layers with a gate model, and optimal
transport domain adaptation methods are employed for capturing the intra- and
inter-modal interactions. Finally, we exploit two methods for fusing the self
and cross-attention features. For taking into account the model calibration, we
apply label smoothing. We use both performance and calibration metrics.
Experiments conducted on the ADReSS and ADReSSo Challenge datasets indicate the
efficacy of our introduced approaches over existing research initiatives with
our best performing model reaching Accuracy and F1-score up to 91.25% and
91.06% respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16731">Automatic Emotion Experiencer Recognition. (arXiv:2305.16731v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wegge_M/0/1/0/all/0/1">Maximilian Wegge</a>, <a href="http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1">Roman Klinger</a></p>
<p>The most prominent subtask in emotion analysis is emotion classification; to
assign a category to a textual unit, for instance a social media post. Many
research questions from the social sciences do, however, not only require the
detection of the emotion of an author of a post but to understand who is
ascribed an emotion in text. This task is tackled by emotion role labeling
which aims at extracting who is described in text to experience an emotion,
why, and towards whom. This could, however, be considered overly sophisticated
if the main question to answer is who feels which emotion. A targeted approach
for such setup is to classify emotion experiencer mentions (aka "emoters")
regarding the emotion they presumably perceive. This task is similar to named
entity recognition of person names with the difference that not every mentioned
entity name is an emoter. While, very recently, data with emoter annotations
has been made available, no experiments have yet been performed to detect such
mentions. With this paper, we provide baseline experiments to understand how
challenging the task is. We further evaluate the impact on experiencer-specific
emotion categorization and appraisal detection in a pipeline, when gold
mentions are not available. We show that experiencer detection in text is a
challenging task, with a precision of .82 and a recall of .56 (F1 =.66). These
results motivate future work of jointly modeling emoter spans and
emotion/appraisal predictions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19472">PlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning. (arXiv:2305.19472v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1">Faeze Brahman</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1">Chandra Bhagavatula</a>, <a href="http://arxiv.org/find/cs/1/au:+Pyatkin_V/0/1/0/all/0/1">Valentina Pyatkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1">Jena D. Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Lorraine Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Arai_H/0/1/0/all/0/1">Hirona J. Arai</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanyal_S/0/1/0/all/0/1">Soumya Sanyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Sakaguchi_K/0/1/0/all/0/1">Keisuke Sakaguchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1">Xiang Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1">Yejin Choi</a></p>
<p>Procedural planning, which entails decomposing a high-level goal into a
sequence of temporally ordered steps, is an important yet intricate task for
machines. It involves integrating common-sense knowledge to reason about
complex contextualized situations that are often counterfactual, e.g.
"scheduling a doctor's appointment without a phone". While current approaches
show encouraging results using large language models (LLMs), they are hindered
by drawbacks such as costly API calls and reproducibility issues. In this
paper, we advocate planning using smaller language models. We present PlaSma, a
novel two-pronged approach to endow small language models with procedural
knowledge and (counterfactual) planning capabilities. More concretely, we
develop symbolic procedural knowledge distillation to enhance the implicit
knowledge in small language models and an inference-time algorithm to
facilitate more structured and accurate reasoning. In addition, we introduce a
novel task, Counterfactual Planning, that requires a revision of a plan to cope
with a counterfactual situation. In both the original and counterfactual
setting, we show that orders-of-magnitude smaller models (770M-11B parameters)
can compete and often surpass their larger teacher models' capabilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.00017">Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse Engineering of Language at Scale. (arXiv:2306.00017v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Saba_W/0/1/0/all/0/1">Walid S. Saba</a></p>
<p>Large language models (LLMs) have achieved a milestone that undenia-bly
changed many held beliefs in artificial intelligence (AI). However, there
remains many limitations of these LLMs when it comes to true language
understanding, limitations that are a byproduct of the under-lying architecture
of deep neural networks. Moreover, and due to their subsymbolic nature,
whatever knowledge these models acquire about how language works will always be
buried in billions of microfeatures (weights), none of which is meaningful on
its own, making such models hopelessly unexplainable. To address these
limitations, we suggest com-bining the strength of symbolic representations
with what we believe to be the key to the success of LLMs, namely a successful
bottom-up re-verse engineering of language at scale. As such we argue for a
bottom-up reverse engineering of language in a symbolic setting. Hints on what
this project amounts to have been suggested by several authors, and we discuss
in some detail here how this project could be accomplished.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.00610">Fraunhofer SIT at CheckThat! 2023: Mixing Single-Modal Classifiers to Estimate the Check-Worthiness of Multi-Modal Tweets. (arXiv:2307.00610v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Frick_R/0/1/0/all/0/1">Raphael Frick</a>, <a href="http://arxiv.org/find/cs/1/au:+Vogel_I/0/1/0/all/0/1">Inna Vogel</a></p>
<p>The option of sharing images, videos and audio files on social media opens up
new possibilities for distinguishing between false information and fake news on
the Internet. Due to the vast amount of data shared every second on social
media, not all data can be verified by a computer or a human expert. Here, a
check-worthiness analysis can be used as a first step in the fact-checking
pipeline and as a filtering mechanism to improve efficiency. This paper
proposes a novel way of detecting the check-worthiness in multi-modal tweets.
It takes advantage of two classifiers, each trained on a single modality. For
image data, extracting the embedded text with an OCR analysis has shown to
perform best. By combining the two classifiers, the proposed solution was able
to place first in the CheckThat! 2023 Task 1A with an F1 score of 0.7297
achieved on the private test set.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.00866">Mining Clues from Incomplete Utterance: A Query-enhanced Network for Incomplete Utterance Rewriting. (arXiv:2307.00866v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1">Shuzheng Si</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1">Shuang Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1">Baobao Chang</a></p>
<p>Incomplete utterance rewriting has recently raised wide attention. However,
previous works do not consider the semantic structural information between
incomplete utterance and rewritten utterance or model the semantic structure
implicitly and insufficiently. To address this problem, we propose a
QUEry-Enhanced Network (QUEEN). Firstly, our proposed query template explicitly
brings guided semantic structural knowledge between the incomplete utterance
and the rewritten utterance making model perceive where to refer back to or
recover omitted tokens. Then, we adopt a fast and effective edit operation
scoring network to model the relation between two tokens. Benefiting from
proposed query template and the well-designed edit operation scoring network,
QUEEN achieves state-of-the-art performance on several public datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.02377">Fraunhofer SIT at CheckThat! 2023: Tackling Classification Uncertainty Using Model Souping on the Example of Check-Worthiness Classification. (arXiv:2307.02377v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Frick_R/0/1/0/all/0/1">Raphael Frick</a>, <a href="http://arxiv.org/find/cs/1/au:+Vogel_I/0/1/0/all/0/1">Inna Vogel</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1">Jeong-Eun Choi</a></p>
<p>This paper describes the second-placed approach developed by the Fraunhofer
SIT team in the CLEF-2023 CheckThat! lab Task 1B for English. Given a text
snippet from a political debate, the aim of this task is to determine whether
it should be assessed for check-worthiness. Detecting check-worthy statements
aims to facilitate manual fact-checking efforts by prioritizing the claims that
fact-checkers should consider first. It can also be considered as primary step
of a fact-checking system. Our best-performing method took advantage of an
ensemble classification scheme centered on Model Souping. When applied to the
English data set, our submitted model achieved an overall F1 score of 0.878 and
was ranked as the second-best model in the competition.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03952">Is ChatGPT a Good Personality Recognizer? A Preliminary Study. (arXiv:2307.03952v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1">Yu Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1">Wen Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1">Hong Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yi Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1">Liang He</a></p>
<p>In recent years, personality has been regarded as a valuable personal factor
being incorporated into numerous tasks such as sentiment analysis and product
recommendation. This has led to widespread attention to text-based personality
recognition task, which aims to identify an individual's personality based on
given text. Considering that ChatGPT has recently exhibited remarkable
abilities on various natural language processing tasks, we provide a
preliminary evaluation of ChatGPT on text-based personality recognition task
for generating effective personality data. Concretely, we employ a variety of
prompting strategies to explore ChatGPT's ability in recognizing personality
from given text, especially the level-oriented prompting strategy we designed
for guiding ChatGPT in analyzing given text at a specified level. The
experimental results on two representative real-world datasets reveal that
ChatGPT with zero-shot chain-of-thought prompting exhibits impressive
personality recognition ability and is capable to provide natural language
explanations through text-based logical reasoning. Furthermore, by employing
the level-oriented prompting strategy to optimize zero-shot chain-of-thought
prompting, the performance gap between ChatGPT and corresponding
state-of-the-art model has been narrowed even more. However, we observe that
ChatGPT shows unfairness towards certain sensitive demographic attributes such
as gender and age. Additionally, we discover that eliciting the personality
recognition ability of ChatGPT helps improve its performance on
personality-related downstream tasks such as sentiment classification and
stress prediction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.12798">RRAML: Reinforced Retrieval Augmented Machine Learning. (arXiv:2307.12798v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bacciu_A/0/1/0/all/0/1">Andrea Bacciu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cuconasu_F/0/1/0/all/0/1">Florin Cuconasu</a>, <a href="http://arxiv.org/find/cs/1/au:+Siciliano_F/0/1/0/all/0/1">Federico Siciliano</a>, <a href="http://arxiv.org/find/cs/1/au:+Silvestri_F/0/1/0/all/0/1">Fabrizio Silvestri</a>, <a href="http://arxiv.org/find/cs/1/au:+Tonellotto_N/0/1/0/all/0/1">Nicola Tonellotto</a>, <a href="http://arxiv.org/find/cs/1/au:+Trappolini_G/0/1/0/all/0/1">Giovanni Trappolini</a></p>
<p>The emergence of large language models (LLMs) has revolutionized machine
learning and related fields, showcasing remarkable abilities in comprehending,
generating, and manipulating human language. However, their conventional usage
through API-based text prompt submissions imposes certain limitations in terms
of context constraints and external source availability. To address these
challenges, we propose a novel framework called Reinforced Retrieval Augmented
Machine Learning (RRAML). RRAML integrates the reasoning capabilities of LLMs
with supporting information retrieved by a purpose-built retriever from a vast
user-provided database. By leveraging recent advancements in reinforcement
learning, our method effectively addresses several critical challenges.
Firstly, it circumvents the need for accessing LLM gradients. Secondly, our
method alleviates the burden of retraining LLMs for specific tasks, as it is
often impractical or impossible due to restricted access to the model and the
computational intensity involved. Additionally we seamlessly link the
retriever's task with the reasoner, mitigating hallucinations and reducing
irrelevant, and potentially damaging retrieved documents. We believe that the
research agenda outlined in this paper has the potential to profoundly impact
the field of AI, democratizing access to and utilization of LLMs for a wide
range of entities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.13365">Empower Your Model with Longer and Better Context Comprehension. (arXiv:2307.13365v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yifei Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1">Jun Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1">Longhua Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1">Jun Cheng</a></p>
<p>Recently, with the emergence of numerous Large Language Models (LLMs), the
implementation of AI has entered a new era. Irrespective of these models' own
capacity and structure, there is a growing demand for LLMs to possess enhanced
comprehension of longer and more complex contexts with relatively smaller
sizes. Models often encounter an upper limit when processing sequences of
sentences that extend beyond their comprehension capacity and result in
off-topic or even chaotic responses. While several recent works attempt to
address this issue in various ways, they rarely focus on "why models are unable
to compensate or strengthen their capabilities on their own". In this paper, we
thoroughly investigate the nature of information transfer within LLMs and
propose a novel technique called Attention Transition. This technique empowers
models to achieve longer and better context comprehension with minimal
additional training or impact on generation fluency. Our experiments are
conducted on the challenging XSum dataset using LLaMa-7b model with context
token length ranging from 800 to 1900. Results demonstrate that we achieve
substantial improvements compared with the original generation results
evaluated by GPT4.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.13693">Evaluating Large Language Models for Radiology Natural Language Processing. (arXiv:2307.13693v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhengliang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_T/0/1/0/all/0/1">Tianyang Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yiwei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yutong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1">Yi Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zihao Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_P/0/1/0/all/0/1">Peixin Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1">Chao Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuxiao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shu_P/0/1/0/all/0/1">Peng Shu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1">Yaonai Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zihao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1">Chong Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiaqi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Sheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1">Mengyue Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1">Zuowei Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chunlin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Holmes_J/0/1/0/all/0/1">Jason Holmes</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Shaochen Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1">Haixing Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1">Lin Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuanhao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Peilong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_P/0/1/0/all/0/1">Pingkun Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_B/0/1/0/all/0/1">Bao Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Lichao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1">Dajiang Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1">Xiaoyan Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xintao Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1">Xi Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tuo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1">Shijie Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Quanzheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1">Hongtu Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1">Dinggang Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tianming Liu</a></p>
<p>The rise of large language models (LLMs) has marked a pivotal shift in the
field of natural language processing (NLP). LLMs have revolutionized a
multitude of domains, and they have made a significant impact in the medical
field. Large language models are now more abundant than ever, and many of these
models exhibit bilingual capabilities, proficient in both English and Chinese.
However, a comprehensive evaluation of these models remains to be conducted.
This lack of assessment is especially apparent within the context of radiology
NLP. This study seeks to bridge this gap by critically evaluating thirty two
LLMs in interpreting radiology reports, a crucial component of radiology NLP.
Specifically, the ability to derive impressions from radiologic findings is
assessed. The outcomes of this evaluation provide key insights into the
performance, strengths, and weaknesses of these LLMs, informing their practical
applications within the medical domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14132">Say Goodbye to RNN-T Loss: A Novel CIF-based Transducer Architecture for Automatic Speech Recognition. (arXiv:2307.14132v2 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tian-Hao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1">Dinghao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_G/0/1/0/all/0/1">Guiping Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Baoxiang Li</a></p>
<p>RNN-T models are widely used in ASR, which rely on the RNN-T loss to achieve
length alignment between input audio and target sequence. However, the
implementation complexity and the alignment-based optimization target of RNN-T
loss lead to computational redundancy and a reduced role for predictor network,
respectively. In this paper, we propose a novel model named CIF-Transducer
(CIF-T) which incorporates the Continuous Integrate-and-Fire (CIF) mechanism
with the RNN-T model to achieve efficient alignment. In this way, the RNN-T
loss is abandoned, thus bringing a computational reduction and allowing the
predictor network a more significant role. We also introduce Funnel-CIF,
Context Blocks, Unified Gating and Bilinear Pooling joint network, and
auxiliary training strategy to further improve performance. Experiments on the
178-hour AISHELL-1 and 10000-hour WenetSpeech datasets show that CIF-T achieves
state-of-the-art results with lower computational overhead compared to RNN-T
models.
</p>
</p>
</div>

    </div>
    </body>
    