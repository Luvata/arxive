<!DOCTYPE html>
<html>
<head>
<title>2024-01-13-cs-cl</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2401.05399">Automated Assessment of Students&#x27; Code Comprehension using LLMs. (arXiv:2401.05399v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oli_P/0/1/0/all/0/1">Priti Oli</a>, <a href="http://arxiv.org/find/cs/1/au:+Banjade_R/0/1/0/all/0/1">Rabin Banjade</a>, <a href="http://arxiv.org/find/cs/1/au:+Chapagain_J/0/1/0/all/0/1">Jeevan Chapagain</a>, <a href="http://arxiv.org/find/cs/1/au:+Rus_V/0/1/0/all/0/1">Vasile Rus</a></p>
<p>Assessing student's answers and in particular natural language answers is a
crucial challenge in the field of education. Advances in machine learning,
including transformer-based models such as Large Language Models(LLMs), have
led to significant progress in various natural language tasks. Nevertheless,
amidst the growing trend of evaluating LLMs across diverse tasks, evaluating
LLMs in the realm of automated answer assesment has not received much
attention. To address this gap, we explore the potential of using LLMs for
automated assessment of student's short and open-ended answer. Particularly, we
use LLMs to compare students' explanations with expert explanations in the
context of line-by-line explanations of computer programs.
</p>
<p>For comparison purposes, we assess both Large Language Models (LLMs) and
encoder-based Semantic Textual Similarity (STS) models in the context of
assessing the correctness of students' explanation of computer code. Our
findings indicate that LLMs, when prompted in few-shot and chain-of-thought
setting perform comparable to fine-tuned encoder-based models in evaluating
students' short answers in programming domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05433">Enhancing Essay Scoring with Adversarial Weights Perturbation and Metric-specific AttentionPooling. (arXiv:2401.05433v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jiaxin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xinyu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Che_C/0/1/0/all/0/1">Chang Che</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1">Qunwei Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bo Liu</a></p>
<p>The objective of this study is to improve automated feedback tools designed
for English Language Learners (ELLs) through the utilization of data science
techniques encompassing machine learning, natural language processing, and
educational data analytics. Automated essay scoring (AES) research has made
strides in evaluating written essays, but it often overlooks the specific needs
of English Language Learners (ELLs) in language development. This study
explores the application of BERT-related techniques to enhance the assessment
of ELLs' writing proficiency within AES.
</p>
<p>To address the specific needs of ELLs, we propose the use of DeBERTa, a
state-of-the-art neural language model, for improving automated feedback tools.
DeBERTa, pretrained on large text corpora using self-supervised learning,
learns universal language representations adaptable to various natural language
understanding tasks. The model incorporates several innovative techniques,
including adversarial training through Adversarial Weights Perturbation (AWP)
and Metric-specific AttentionPooling (6 kinds of AP) for each label in the
competition.
</p>
<p>The primary focus of this research is to investigate the impact of
hyperparameters, particularly the adversarial learning rate, on the performance
of the model. By fine-tuning the hyperparameter tuning process, including the
influence of 6AP and AWP, the resulting models can provide more accurate
evaluations of language proficiency and support tailored learning tasks for
ELLs. This work has the potential to significantly benefit ELLs by improving
their English language proficiency and facilitating their educational journey.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05443">LLM4PLC: Harnessing Large Language Models for Verifiable Programming of PLCs in Industrial Control Systems. (arXiv:2401.05443v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fakih_M/0/1/0/all/0/1">Mohamad Fakih</a>, <a href="http://arxiv.org/find/cs/1/au:+Dharmaji_R/0/1/0/all/0/1">Rahul Dharmaji</a>, <a href="http://arxiv.org/find/cs/1/au:+Moghaddas_Y/0/1/0/all/0/1">Yasamin Moghaddas</a>, <a href="http://arxiv.org/find/cs/1/au:+Araya_G/0/1/0/all/0/1">Gustavo Quiros Araya</a>, <a href="http://arxiv.org/find/cs/1/au:+Ogundare_O/0/1/0/all/0/1">Oluwatosin Ogundare</a>, <a href="http://arxiv.org/find/cs/1/au:+Faruque_M/0/1/0/all/0/1">Mohammad Abdullah Al Faruque</a></p>
<p>Although Large Language Models (LLMs) have established pre-dominance in
automated code generation, they are not devoid of shortcomings. The pertinent
issues primarily relate to the absence of execution guarantees for generated
code, a lack of explainability, and suboptimal support for essential but niche
programming languages. State-of-the-art LLMs such as GPT-4 and LLaMa2 fail to
produce valid programs for Industrial Control Systems (ICS) operated by
Programmable Logic Controllers (PLCs). We propose LLM4PLC, a user-guided
iterative pipeline leveraging user feedback and external verification tools
including grammar checkers, compilers and SMV verifiers to guide the LLM's
generation. We further enhance the generation potential of LLM by employing
Prompt Engineering and model fine-tuning through the creation and usage of
LoRAs. We validate this system using a FischerTechnik Manufacturing TestBed
(MFTB), illustrating how LLMs can evolve from generating structurally flawed
code to producing verifiably correct programs for industrial applications. We
run a complete test suite on GPT-3.5, GPT-4, Code Llama-7B, a fine-tuned Code
Llama-7B model, Code Llama-34B, and a fine-tuned Code Llama-34B model. The
proposed pipeline improved the generation success rate from 47% to 72%, and the
Survey-of-Experts code quality from 2.25/10 to 7.75/10. To promote open
research, we share the complete experimental setup, the LLM Fine-Tuning
Weights, and the video demonstrations of the different programs on our
dedicated webpage.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05507">InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks. (arXiv:2401.05507v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xueyu Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Ziyu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1">Shuang Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Chai_Z/0/1/0/all/0/1">Ziwei Chai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guoyin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xuwu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1">Jing Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jingjing Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1">Ming Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1">Yao Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1">Jianbo Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1">Kun Kuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Hongxia Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1">Fei Wu</a></p>
<p>In this paper, we introduce "InfiAgent-DABench", the first benchmark
specifically designed to evaluate LLM-based agents in data analysis tasks. This
benchmark contains DAEval, a dataset consisting of 311 data analysis questions
derived from 55 CSV files, and an agent framework to evaluate LLMs as data
analysis agents. We adopt a format-prompting technique, ensuring questions to
be closed-form that can be automatically evaluated. Our extensive benchmarking
of 23 state-of-the-art LLMs uncovers the current challenges encountered in data
analysis tasks. In addition, we have developed DAAgent, a specialized agent
trained on instruction-tuning datasets. Evaluation datasets and toolkits for
InfiAgent-DABench are released at https://github.com/InfiAgent/InfiAgent.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05520">From Pampas to Pixels: Fine-Tuning Diffusion Models for Ga\&#x27;ucho Heritage. (arXiv:2401.05520v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Amadeus_M/0/1/0/all/0/1">Marcellus Amadeus</a>, <a href="http://arxiv.org/find/cs/1/au:+Castaneda_W/0/1/0/all/0/1">William Alberto Cruz Casta&#xf1;eda</a>, <a href="http://arxiv.org/find/cs/1/au:+Zanella_A/0/1/0/all/0/1">Andr&#xe9; Felipe Zanella</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahlow_F/0/1/0/all/0/1">Felipe Rodrigues Perche Mahlow</a></p>
<p>Generative AI has become pervasive in society, witnessing significant
advancements in various domains. Particularly in the realm of Text-to-Image
(TTI) models, Latent Diffusion Models (LDMs), showcase remarkable capabilities
in generating visual content based on textual prompts. This paper addresses the
potential of LDMs in representing local cultural concepts, historical figures,
and endangered species. In this study, we use the cultural heritage of Rio
Grande do Sul (RS), Brazil, as an illustrative case. Our objective is to
contribute to the broader understanding of how generative models can help to
capture and preserve the cultural and historical identity of regions. The paper
outlines the methodology, including subject selection, dataset creation, and
the fine-tuning process. The results showcase the images generated, alongside
the challenges and feasibility of each concept. In conclusion, this work shows
the power of these models to represent and preserve unique aspects of diverse
regions and communities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05544">CodePrompt: Improving Source Code-Related Classification with Knowledge Features through Prompt Learning. (arXiv:2401.05544v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yong Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1">Senlin Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_Y/0/1/0/all/0/1">Yu-Ming Shang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yifei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhengjun Li</a></p>
<p>Researchers have explored the potential of utilizing pre-trained language
models, such as CodeBERT, to improve source code-related tasks. Previous
studies have mainly relied on CodeBERT's text embedding capability and the
`[CLS]' sentence embedding information as semantic representations for
fine-tuning downstream source code-related tasks. However, these methods
require additional neural network layers to extract effective features,
resulting in higher computational costs. Furthermore, existing approaches have
not leveraged the rich knowledge contained in both source code and related
text, which can lead to lower accuracy. This paper presents a novel approach,
CodePrompt, which utilizes rich knowledge recalled from a pre-trained model by
prompt learning and an attention mechanism to improve source code-related
classification tasks. Our approach initially motivates the language model with
prompt information to retrieve abundant knowledge associated with the input as
representative features, thus avoiding the need for additional neural network
layers and reducing computational costs. Subsequently, we employ an attention
mechanism to aggregate multiple layers of related knowledge for each task as
final features to boost their accuracy. We conducted extensive experiments on
four downstream source code-related tasks to evaluate our approach and our
results demonstrate that CodePrompt achieves new state-of-the-art performance
on the accuracy metric while also exhibiting computation cost-saving
capabilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05551">Useful Blunders: Can Automated Speech Recognition Errors Improve Downstream Dementia Classification?. (arXiv:2401.05551v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Changye Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Weizhe Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_T/0/1/0/all/0/1">Trevor Cohen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pakhomov_S/0/1/0/all/0/1">Serguei Pakhomov</a></p>
<p>\textbf{Objectives}: We aimed to investigate how errors from automatic speech
recognition (ASR) systems affect dementia classification accuracy, specifically
in the ``Cookie Theft'' picture description task. We aimed to assess whether
imperfect ASR-generated transcripts could provide valuable information for
distinguishing between language samples from cognitively healthy individuals
and those with Alzheimer's disease (AD).
</p>
<p>\textbf{Methods}: We conducted experiments using various ASR models, refining
their transcripts with post-editing techniques. Both these imperfect ASR
transcripts and manually transcribed ones were used as inputs for the
downstream dementia classification. We conducted comprehensive error analysis
to compare model performance and assess ASR-generated transcript effectiveness
in dementia classification.
</p>
<p>\textbf{Results}: Imperfect ASR-generated transcripts surprisingly
outperformed manual transcription for distinguishing between individuals with
AD and those without in the ``Cookie Theft'' task. These ASR-based models
surpassed the previous state-of-the-art approach, indicating that ASR errors
may contain valuable cues related to dementia. The synergy between ASR and
classification models improved overall accuracy in dementia classification.
</p>
<p>\textbf{Conclusion}: Imperfect ASR transcripts effectively capture linguistic
anomalies linked to dementia, improving accuracy in classification tasks. This
synergy between ASR and classification models underscores ASR's potential as a
valuable tool in assessing cognitive impairment and related clinical
applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05561">TrustLLM: Trustworthiness in Large Language Models. (arXiv:2401.05561v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Lichao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yue Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haoran Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Siyuan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qihui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1">Chujie Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yixin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_W/0/1/0/all/0/1">Wenhan Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yixuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiner Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhengliang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yixin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yijue Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhikun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1">Bhavya Kailkhura</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1">Caiming Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Chaowei Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chunyuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1">Eric Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1">Furong Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1">Heng Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hongyi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Huan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1">Huaxiu Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Kellis_M/0/1/0/all/0/1">Manolis Kellis</a>, <a href="http://arxiv.org/find/cs/1/au:+Zitnik_M/0/1/0/all/0/1">Marinka Zitnik</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1">Meng Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1">Mohit Bansal</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1">James Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1">Jian Pei</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jian Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jianfeng Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Jiawei Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jieyu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jiliang Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jindong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitchell_J/0/1/0/all/0/1">John Mitchell</a>, <a href="http://arxiv.org/find/cs/1/au:+Shu_K/0/1/0/all/0/1">Kai Shu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Kaidi Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1">Kai-Wei Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1">Lifang He</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1">Lifu Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Backes_M/0/1/0/all/0/1">Michael Backes</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1">Neil Zhenqiang Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1">Philip S. Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1">Pin-Yu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1">Quanquan Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">Ran Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ying_R/0/1/0/all/0/1">Rex Ying</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1">Shuiwang Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Jana_S/0/1/0/all/0/1">Suman Jana</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tianlong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tianming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1">Tianyi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Willian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiangliang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xing Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xuyu Wang</a>, et al. (4 additional authors not shown)</p>
<p>Large language models (LLMs), exemplified by ChatGPT, have gained
considerable attention for their excellent natural language processing
capabilities. Nonetheless, these LLMs present many challenges, particularly in
the realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs
emerges as an important topic. This paper introduces TrustLLM, a comprehensive
study of trustworthiness in LLMs, including principles for different dimensions
of trustworthiness, established benchmark, evaluation, and analysis of
trustworthiness for mainstream LLMs, and discussion of open challenges and
future directions. Specifically, we first propose a set of principles for
trustworthy LLMs that span eight different dimensions. Based on these
principles, we further establish a benchmark across six dimensions including
truthfulness, safety, fairness, robustness, privacy, and machine ethics. We
then present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of
over 30 datasets. Our findings firstly show that in general trustworthiness and
utility (i.e., functional effectiveness) are positively related. Secondly, our
observations reveal that proprietary LLMs generally outperform most open-source
counterparts in terms of trustworthiness, raising concerns about the potential
risks of widely accessible open-source LLMs. However, a few open-source LLMs
come very close to proprietary ones. Thirdly, it is important to note that some
LLMs may be overly calibrated towards exhibiting trustworthiness, to the extent
that they compromise their utility by mistakenly treating benign prompts as
harmful and consequently not responding. Finally, we emphasize the importance
of ensuring transparency not only in the models themselves but also in the
technologies that underpin trustworthiness. Knowing the specific trustworthy
technologies that have been employed is crucial for analyzing their
effectiveness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05566">Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training. (arXiv:2401.05566v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hubinger_E/0/1/0/all/0/1">Evan Hubinger</a>, <a href="http://arxiv.org/find/cs/1/au:+Denison_C/0/1/0/all/0/1">Carson Denison</a>, <a href="http://arxiv.org/find/cs/1/au:+Mu_J/0/1/0/all/0/1">Jesse Mu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lambert_M/0/1/0/all/0/1">Mike Lambert</a>, <a href="http://arxiv.org/find/cs/1/au:+Tong_M/0/1/0/all/0/1">Meg Tong</a>, <a href="http://arxiv.org/find/cs/1/au:+MacDiarmid_M/0/1/0/all/0/1">Monte MacDiarmid</a>, <a href="http://arxiv.org/find/cs/1/au:+Lanham_T/0/1/0/all/0/1">Tamera Lanham</a>, <a href="http://arxiv.org/find/cs/1/au:+Ziegler_D/0/1/0/all/0/1">Daniel M. Ziegler</a>, <a href="http://arxiv.org/find/cs/1/au:+Maxwell_T/0/1/0/all/0/1">Tim Maxwell</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1">Newton Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Jermyn_A/0/1/0/all/0/1">Adam Jermyn</a>, <a href="http://arxiv.org/find/cs/1/au:+Askell_A/0/1/0/all/0/1">Amanda Askell</a>, <a href="http://arxiv.org/find/cs/1/au:+Radhakrishnan_A/0/1/0/all/0/1">Ansh Radhakrishnan</a>, <a href="http://arxiv.org/find/cs/1/au:+Anil_C/0/1/0/all/0/1">Cem Anil</a>, <a href="http://arxiv.org/find/cs/1/au:+Duvenaud_D/0/1/0/all/0/1">David Duvenaud</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganguli_D/0/1/0/all/0/1">Deep Ganguli</a>, <a href="http://arxiv.org/find/cs/1/au:+Barez_F/0/1/0/all/0/1">Fazl Barez</a>, <a href="http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1">Jack Clark</a>, <a href="http://arxiv.org/find/cs/1/au:+Ndousse_K/0/1/0/all/0/1">Kamal Ndousse</a>, <a href="http://arxiv.org/find/cs/1/au:+Sachan_K/0/1/0/all/0/1">Kshitij Sachan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sellitto_M/0/1/0/all/0/1">Michael Sellitto</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_M/0/1/0/all/0/1">Mrinank Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+DasSarma_N/0/1/0/all/0/1">Nova DasSarma</a>, <a href="http://arxiv.org/find/cs/1/au:+Grosse_R/0/1/0/all/0/1">Roger Grosse</a>, <a href="http://arxiv.org/find/cs/1/au:+Kravec_S/0/1/0/all/0/1">Shauna Kravec</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1">Yuntao Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Witten_Z/0/1/0/all/0/1">Zachary Witten</a>, <a href="http://arxiv.org/find/cs/1/au:+Favaro_M/0/1/0/all/0/1">Marina Favaro</a>, <a href="http://arxiv.org/find/cs/1/au:+Brauner_J/0/1/0/all/0/1">Jan Brauner</a>, <a href="http://arxiv.org/find/cs/1/au:+Karnofsky_H/0/1/0/all/0/1">Holden Karnofsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Christiano_P/0/1/0/all/0/1">Paul Christiano</a>, <a href="http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1">Samuel R. Bowman</a>, <a href="http://arxiv.org/find/cs/1/au:+Graham_L/0/1/0/all/0/1">Logan Graham</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaplan_J/0/1/0/all/0/1">Jared Kaplan</a>, <a href="http://arxiv.org/find/cs/1/au:+Mindermann_S/0/1/0/all/0/1">S&#xf6;ren Mindermann</a>, <a href="http://arxiv.org/find/cs/1/au:+Greenblatt_R/0/1/0/all/0/1">Ryan Greenblatt</a>, <a href="http://arxiv.org/find/cs/1/au:+Shlegeris_B/0/1/0/all/0/1">Buck Shlegeris</a>, <a href="http://arxiv.org/find/cs/1/au:+Schiefer_N/0/1/0/all/0/1">Nicholas Schiefer</a>, <a href="http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1">Ethan Perez</a></p>
<p>Humans are capable of strategically deceptive behavior: behaving helpfully in
most situations, but then behaving very differently in order to pursue
alternative objectives when given the opportunity. If an AI system learned such
a deceptive strategy, could we detect it and remove it using current
state-of-the-art safety training techniques? To study this question, we
construct proof-of-concept examples of deceptive behavior in large language
models (LLMs). For example, we train models that write secure code when the
prompt states that the year is 2023, but insert exploitable code when the
stated year is 2024. We find that such backdoored behavior can be made
persistent, so that it is not removed by standard safety training techniques,
including supervised fine-tuning, reinforcement learning, and adversarial
training (eliciting unsafe behavior and then training to remove it). The
backdoored behavior is most persistent in the largest models and in models
trained to produce chain-of-thought reasoning about deceiving the training
process, with the persistence remaining even when the chain-of-thought is
distilled away. Furthermore, rather than removing backdoors, we find that
adversarial training can teach models to better recognize their backdoor
triggers, effectively hiding the unsafe behavior. Our results suggest that,
once a model exhibits deceptive behavior, standard techniques could fail to
remove such deception and create a false impression of safety.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05596">POMP: Probability-driven Meta-graph Prompter for LLMs in Low-resource Unsupervised Neural Machine Translation. (arXiv:2401.05596v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1">Shilong Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1">Zhiliang Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1">Liang Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zhen Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1">Zhihua Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dongsheng Li</a></p>
<p>Low-resource languages (LRLs) face challenges in supervised neural machine
translation due to limited parallel data, prompting research into unsupervised
methods. Unsupervised neural machine translation (UNMT) methods, including
back-translation, transfer learning, and pivot-based translation, offer
practical solutions for LRL translation, but they are hindered by issues like
synthetic data noise, language bias, and error propagation, which can
potentially be mitigated by Large Language Models (LLMs). LLMs have advanced
NMT with in-context learning (ICL) and supervised fine-tuning methods, but
insufficient training data results in poor performance in LRLs. We argue that
LLMs can mitigate the linguistic noise with auxiliary languages to improve
translations in LRLs. In this paper, we propose Probability-driven Meta-graph
Prompter (POMP), a novel approach employing a dynamic, sampling-based graph of
multiple auxiliary languages to enhance LLMs' translation capabilities for
LRLs. POMP involves constructing a directed acyclic meta-graph for each source
language, from which we dynamically sample multiple paths to prompt LLMs to
mitigate the linguistic noise and improve translations during training. We use
the BLEURT metric to evaluate the translations and back-propagate rewards,
estimated by scores, to update the probabilities of auxiliary languages in the
paths. Our experiments show significant improvements in the translation quality
of three LRLs, demonstrating the effectiveness of our approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05604">REBUS: A Robust Evaluation Benchmark of Understanding Symbols. (arXiv:2401.05604v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gritsevskiy_A/0/1/0/all/0/1">Andrew Gritsevskiy</a>, <a href="http://arxiv.org/find/cs/1/au:+Panickssery_A/0/1/0/all/0/1">Arjun Panickssery</a>, <a href="http://arxiv.org/find/cs/1/au:+Kirtland_A/0/1/0/all/0/1">Aaron Kirtland</a>, <a href="http://arxiv.org/find/cs/1/au:+Kauffman_D/0/1/0/all/0/1">Derik Kauffman</a>, <a href="http://arxiv.org/find/cs/1/au:+Gundlach_H/0/1/0/all/0/1">Hans Gundlach</a>, <a href="http://arxiv.org/find/cs/1/au:+Gritsevskaya_I/0/1/0/all/0/1">Irina Gritsevskaya</a>, <a href="http://arxiv.org/find/cs/1/au:+Cavanagh_J/0/1/0/all/0/1">Joe Cavanagh</a>, <a href="http://arxiv.org/find/cs/1/au:+Chiang_J/0/1/0/all/0/1">Jonathan Chiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Roux_L/0/1/0/all/0/1">Lydia La Roux</a>, <a href="http://arxiv.org/find/cs/1/au:+Hung_M/0/1/0/all/0/1">Michelle Hung</a></p>
<p>We propose a new benchmark evaluating the performance of multimodal large
language models on rebus puzzles. The dataset covers 333 original examples of
image-based wordplay, cluing 13 categories such as movies, composers, major
cities, and food. To achieve good performance on the benchmark of identifying
the clued word or phrase, models must combine image recognition and string
manipulation with hypothesis testing, multi-step reasoning, and an
understanding of human cognition, making for a complex, multimodal evaluation
of capabilities. We find that proprietary models such as GPT-4V and Gemini Pro
significantly outperform all other tested models. However, even the best model
has a final accuracy of just 24%, highlighting the need for substantial
improvements in reasoning. Further, models rarely understand all parts of a
puzzle, and are almost always incapable of retroactively explaining the correct
answer. Our benchmark can therefore be used to identify major shortcomings in
the knowledge and reasoning of multimodal large language models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05605">Scaling Laws for Forgetting When Fine-Tuning Large Language Models. (arXiv:2401.05605v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kalajdzievski_D/0/1/0/all/0/1">Damjan Kalajdzievski</a></p>
<p>We study and quantify the problem of forgetting when fine-tuning pre-trained
large language models (LLMs) on a downstream task. We find that
parameter-efficient fine-tuning (PEFT) strategies, such as Low-Rank Adapters
(LoRA), still suffer from catastrophic forgetting. In particular, we identify a
strong inverse linear relationship between the fine-tuning performance and the
amount of forgetting when fine-tuning LLMs with LoRA. We further obtain precise
scaling laws that show forgetting increases as a shifted power law in the
number of parameters fine-tuned and the number of update steps. We also examine
the impact of forgetting on knowledge, reasoning, and the safety guardrails
trained into Llama 2 7B chat. Our study suggests that forgetting cannot be
avoided through early stopping or by varying the number of parameters
fine-tuned. We believe this opens up an important safety-critical direction for
future research to evaluate and develop fine-tuning schemes which mitigate
forgetting
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05618">The Benefits of a Concise Chain of Thought on Problem-Solving in Large Language Models. (arXiv:2401.05618v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Renze_M/0/1/0/all/0/1">Matthew Renze</a>, <a href="http://arxiv.org/find/cs/1/au:+Guven_E/0/1/0/all/0/1">Erhan Guven</a></p>
<p>In this paper, we introduce Concise Chain-of-Thought (CCoT) prompting. We
compared standard CoT and CCoT prompts to see how conciseness impacts response
length and correct-answer accuracy. We evaluated this using GPT-3.5 and GPT-4
with a multiple-choice question-and-answer (MCQA) benchmark. CCoT reduced
average response length by 48.70% for both GPT-3.5 and GPT-4 while having a
negligible impact on problem-solving performance. However, on math problems,
GPT-3.5 with CCoT incurs a performance penalty of 27.69%. Overall, CCoT leads
to an average per-token cost reduction of 22.67%. These results have practical
implications for AI systems engineers using LLMs to solve real-world problems
with CoT prompt-engineering techniques. In addition, these results provide more
general insight for AI researchers studying the emergent behavior of
step-by-step reasoning in LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05631">DrawTalking: Building Interactive Worlds by Sketching and Speaking. (arXiv:2401.05631v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rosenberg_K/0/1/0/all/0/1">Karl Toby Rosenberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Kazi_R/0/1/0/all/0/1">Rubaiat Habib Kazi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1">Li-Yi Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_H/0/1/0/all/0/1">Haijun Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Perlin_K/0/1/0/all/0/1">Ken Perlin</a></p>
<p>We introduce an interactive approach, DrawTalking, in which the user builds
interactive worlds by sketching and speaking. It emphasizes user control and
flexibility, and gives programming-like capability without code. We implemented
it on the iPad. An open-ended study shows the mechanics resonate and are
applicable to many creative-exploratory use cases. We hope to inspire and
inform research in future natural user-centered interfaces.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05632">Natural Language Processing for Dialects of a Language: A Survey. (arXiv:2401.05632v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1">Aditya Joshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Dabre_R/0/1/0/all/0/1">Raj Dabre</a>, <a href="http://arxiv.org/find/cs/1/au:+Kanojia_D/0/1/0/all/0/1">Diptesh Kanojia</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhuang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1">Haolan Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1">Gholamreza Haffari</a>, <a href="http://arxiv.org/find/cs/1/au:+Dippold_D/0/1/0/all/0/1">Doris Dippold</a></p>
<p>State-of-the-art natural language processing (NLP) models are trained on
massive training corpora, and report a superlative performance on evaluation
datasets. This survey delves into an important attribute of these datasets: the
dialect of a language. Motivated by the performance degradation of NLP models
for dialectic datasets and its implications for the equity of language
technologies, we survey past research in NLP for dialects in terms of datasets,
and approaches. We describe a wide range of NLP tasks in terms of two
categories: natural language understanding (NLU) (for tasks such as dialect
classification, sentiment analysis, parsing, and NLU benchmarks) and natural
language generation (NLG) (for summarisation, machine translation, and dialogue
systems). The survey is also broad in its coverage of languages which include
English, Arabic, German among others. We observe that past work in NLP
concerning dialects goes deeper than mere dialect classification, and . This
includes early approaches that used sentence transduction that lead to the
recent approaches that integrate hypernetworks into LoRA. We expect that this
survey will be useful to NLP researchers interested in building equitable
language technologies by rethinking LLM benchmarks and model architectures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05650">On Detecting Cherry-picking in News Coverage Using Large Language Models. (arXiv:2401.05650v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jaradat_I/0/1/0/all/0/1">Israa Jaradat</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haiqi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chengkai Li</a></p>
<p>Cherry-picking refers to the deliberate selection of evidence or facts that
favor a particular viewpoint while ignoring or distorting evidence that
supports an opposing perspective. Manually identifying instances of
cherry-picked statements in news stories can be challenging, particularly when
the opposing viewpoint's story is absent. This study introduces Cherry, an
innovative approach for automatically detecting cherry-picked statements in
news articles by finding missing important statements in the target news story.
Cherry utilizes the analysis of news coverage from multiple sources to identify
instances of cherry-picking. Our approach relies on language models that
consider contextual information from other news sources to classify statements
based on their importance to the event covered in the target news story.
Furthermore, this research introduces a novel dataset specifically designed for
cherry-picking detection, which was used to train and evaluate the performance
of the models. Our best performing model achieves an F-1 score of about %89 in
detecting important statements when tested on unseen set of news stories.
Moreover, results show the importance incorporating external knowledge from
alternative unbiased narratives when assessing a statement's importance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05654">Towards Conversational Diagnostic AI. (arXiv:2401.05654v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tu_T/0/1/0/all/0/1">Tao Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Palepu_A/0/1/0/all/0/1">Anil Palepu</a>, <a href="http://arxiv.org/find/cs/1/au:+Schaekermann_M/0/1/0/all/0/1">Mike Schaekermann</a>, <a href="http://arxiv.org/find/cs/1/au:+Saab_K/0/1/0/all/0/1">Khaled Saab</a>, <a href="http://arxiv.org/find/cs/1/au:+Freyberg_J/0/1/0/all/0/1">Jan Freyberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Tanno_R/0/1/0/all/0/1">Ryutaro Tanno</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1">Amy Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Brenna Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Amin_M/0/1/0/all/0/1">Mohamed Amin</a>, <a href="http://arxiv.org/find/cs/1/au:+Tomasev_N/0/1/0/all/0/1">Nenad Tomasev</a>, <a href="http://arxiv.org/find/cs/1/au:+Azizi_S/0/1/0/all/0/1">Shekoofeh Azizi</a>, <a href="http://arxiv.org/find/cs/1/au:+Singhal_K/0/1/0/all/0/1">Karan Singhal</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1">Yong Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1">Le Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Webson_A/0/1/0/all/0/1">Albert Webson</a>, <a href="http://arxiv.org/find/cs/1/au:+Kulkarni_K/0/1/0/all/0/1">Kavita Kulkarni</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahdavi_S/0/1/0/all/0/1">S Sara Mahdavi</a>, <a href="http://arxiv.org/find/cs/1/au:+Semturs_C/0/1/0/all/0/1">Christopher Semturs</a>, <a href="http://arxiv.org/find/cs/1/au:+Gottweis_J/0/1/0/all/0/1">Juraj Gottweis</a>, <a href="http://arxiv.org/find/cs/1/au:+Barral_J/0/1/0/all/0/1">Joelle Barral</a>, <a href="http://arxiv.org/find/cs/1/au:+Chou_K/0/1/0/all/0/1">Katherine Chou</a>, <a href="http://arxiv.org/find/cs/1/au:+Corrado_G/0/1/0/all/0/1">Greg S Corrado</a>, <a href="http://arxiv.org/find/cs/1/au:+Matias_Y/0/1/0/all/0/1">Yossi Matias</a>, <a href="http://arxiv.org/find/cs/1/au:+Karthikesalingam_A/0/1/0/all/0/1">Alan Karthikesalingam</a>, <a href="http://arxiv.org/find/cs/1/au:+Natarajan_V/0/1/0/all/0/1">Vivek Natarajan</a></p>
<p>At the heart of medicine lies the physician-patient dialogue, where skillful
history-taking paves the way for accurate diagnosis, effective management, and
enduring trust. Artificial Intelligence (AI) systems capable of diagnostic
dialogue could increase accessibility, consistency, and quality of care.
However, approximating clinicians' expertise is an outstanding grand challenge.
Here, we introduce AMIE (Articulate Medical Intelligence Explorer), a Large
Language Model (LLM) based AI system optimized for diagnostic dialogue.
</p>
<p>AMIE uses a novel self-play based simulated environment with automated
feedback mechanisms for scaling learning across diverse disease conditions,
specialties, and contexts. We designed a framework for evaluating
clinically-meaningful axes of performance including history-taking, diagnostic
accuracy, management reasoning, communication skills, and empathy. We compared
AMIE's performance to that of primary care physicians (PCPs) in a randomized,
double-blind crossover study of text-based consultations with validated patient
actors in the style of an Objective Structured Clinical Examination (OSCE). The
study included 149 case scenarios from clinical providers in Canada, the UK,
and India, 20 PCPs for comparison with AMIE, and evaluations by specialist
physicians and patient actors. AMIE demonstrated greater diagnostic accuracy
and superior performance on 28 of 32 axes according to specialist physicians
and 24 of 26 axes according to patient actors. Our research has several
limitations and should be interpreted with appropriate caution. Clinicians were
limited to unfamiliar synchronous text-chat which permits large-scale
LLM-patient interactions but is not representative of usual clinical practice.
While further research is required before AMIE could be translated to
real-world settings, the results represent a milestone towards conversational
diagnostic AI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05655">Unveiling the Tapestry of Automated Essay Scoring: A Comprehensive Investigation of Accuracy, Fairness, and Generalizability. (arXiv:2401.05655v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1">Kaixun Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Rakovic_M/0/1/0/all/0/1">Mladen Rakovi&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_Q/0/1/0/all/0/1">Quanlong Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Gasevic_D/0/1/0/all/0/1">Dragan Ga&#x161;evi&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1">Guanliang Chen</a></p>
<p>Automatic Essay Scoring (AES) is a well-established educational pursuit that
employs machine learning to evaluate student-authored essays. While much effort
has been made in this area, current research primarily focuses on either (i)
boosting the predictive accuracy of an AES model for a specific prompt (i.e.,
developing prompt-specific models), which often heavily relies on the use of
the labeled data from the same target prompt; or (ii) assessing the
applicability of AES models developed on non-target prompts to the intended
target prompt (i.e., developing the AES models in a cross-prompt setting).
Given the inherent bias in machine learning and its potential impact on
marginalized groups, it is imperative to investigate whether such bias exists
in current AES methods and, if identified, how it intervenes with an AES
model's accuracy and generalizability. Thus, our study aimed to uncover the
intricate relationship between an AES model's accuracy, fairness, and
generalizability, contributing practical insights for developing effective AES
models in real-world education. To this end, we meticulously selected nine
prominent AES methods and evaluated their performance using seven metrics on an
open-sourced dataset, which contains over 25,000 essays and various demographic
information about students such as gender, English language learner status, and
economic status. Through extensive evaluations, we demonstrated that: (1)
prompt-specific models tend to outperform their cross-prompt counterparts in
terms of predictive accuracy; (2) prompt-specific models frequently exhibit a
greater bias towards students of different economic statuses compared to
cross-prompt models; (3) in the pursuit of generalizability, traditional
machine learning models coupled with carefully engineered features hold greater
potential for achieving both high accuracy and fairness than complex neural
network models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05669">ConcEPT: Concept-Enhanced Pre-Training for Language Models. (arXiv:2401.05669v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xintao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_Z/0/1/0/all/0/1">Zhouhong Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1">Jiaqing Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1">Dakuan Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1">Yanghua Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wei Wang</a></p>
<p>Pre-trained language models (PLMs) have been prevailing in state-of-the-art
methods for natural language processing, and knowledge-enhanced PLMs are
further proposed to promote model performance in knowledge-intensive tasks.
However, conceptual knowledge, one essential kind of knowledge for human
cognition, still remains understudied in this line of research. This limits
PLMs' performance in scenarios requiring human-like cognition, such as
understanding long-tail entities with concepts. In this paper, we propose
ConcEPT, which stands for Concept-Enhanced Pre-Training for language models, to
infuse conceptual knowledge into PLMs. ConcEPT exploits external taxonomies
with entity concept prediction, a novel pre-training objective to predict the
concepts of entities mentioned in the pre-training contexts. Unlike previous
concept-enhanced methods, ConcEPT can be readily adapted to various downstream
applications without entity linking or concept mapping. Results of extensive
experiments show the effectiveness of ConcEPT in four tasks such as entity
typing, which validates that our model gains improved conceptual knowledge with
concept-enhanced pre-training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05689">UCorrect: An Unsupervised Framework for Automatic Speech Recognition Error Correction. (arXiv:2401.05689v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jiaxin Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Minghan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_X/0/1/0/all/0/1">Xiaosong Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1">Daimeng Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_H/0/1/0/all/0/1">Hengchao Shang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zongyao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Zhengzhe Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yinglu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1">Chang Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Min Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_S/0/1/0/all/0/1">Shimin Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Hao Yang</a></p>
<p>Error correction techniques have been used to refine the output sentences
from automatic speech recognition (ASR) models and achieve a lower word error
rate (WER). Previous works usually adopt end-to-end models and has strong
dependency on Pseudo Paired Data and Original Paired Data. But when only
pre-training on Pseudo Paired Data, previous models have negative effect on
correction. While fine-tuning on Original Paired Data, the source side data
must be transcribed by a well-trained ASR model, which takes a lot of time and
not universal. In this paper, we propose UCorrect, an unsupervised
Detector-Generator-Selector framework for ASR Error Correction. UCorrect has no
dependency on the training data mentioned before. The whole procedure is first
to detect whether the character is erroneous, then to generate some candidate
characters and finally to select the most confident one to replace the error
character. Experiments on the public AISHELL-1 dataset and WenetSpeech dataset
show the effectiveness of UCorrect for ASR error correction: 1) it achieves
significant WER reduction, achieves 6.83\% even without fine-tuning and 14.29\%
after fine-tuning; 2) it outperforms the popular NAR correction models by a
large margin with a competitive low latency; and 3) it is an universal method,
as it reduces all WERs of the ASR model with different decoding strategies and
reduces all WERs of ASR models trained on different scale datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05695">Integrating Physician Diagnostic Logic into Large Language Models: Preference Learning from Process Feedback. (arXiv:2401.05695v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dou_C/0/1/0/all/0/1">Chengfeng Dou</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1">Zhi Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiao_W/0/1/0/all/0/1">Wenpin Jiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Haiyan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yongqiang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_Z/0/1/0/all/0/1">Zhenwei Tao</a></p>
<p>The use of large language models in medical dialogue generation has garnered
significant attention, with a focus on improving response quality and fluency.
While previous studies have made progress in optimizing model performance for
single-round medical Q&amp;A tasks, there is a need to enhance the model's
capability for multi-round conversations to avoid logical inconsistencies. To
address this, we propose an approach called preference learning from process
feedback~(PLPF), which integrates the doctor's diagnostic logic into LLMs. PLPF
involves rule modeling, preference data generation, and preference alignment to
train the model to adhere to the diagnostic process. Experimental results using
Standardized Patient Testing show that PLPF enhances the diagnostic accuracy of
the baseline model in medical conversations by 17.6%, outperforming traditional
reinforcement learning from human feedback. Additionally, PLPF demonstrates
effectiveness in both multi-round and single-round dialogue tasks, showcasing
its potential for improving medical dialogue generation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05700">R-BI: Regularized Batched Inputs enhance Incremental Decoding Framework for Low-Latency Simultaneous Speech Translation. (arXiv:2401.05700v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jiaxin Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhanglin Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zongyao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_H/0/1/0/all/0/1">Hengchao Shang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1">Daimeng Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xiaoyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Rao_Z/0/1/0/all/0/1">Zhiqiang Rao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shaojun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Hao Yang</a></p>
<p>Incremental Decoding is an effective framework that enables the use of an
offline model in a simultaneous setting without modifying the original model,
making it suitable for Low-Latency Simultaneous Speech Translation. However,
this framework may introduce errors when the system outputs from incomplete
input. To reduce these output errors, several strategies such as Hold-$n$,
LA-$n$, and SP-$n$ can be employed, but the hyper-parameter $n$ needs to be
carefully selected for optimal performance. Moreover, these strategies are more
suitable for end-to-end systems than cascade systems. In our paper, we propose
a new adaptable and efficient policy named "Regularized Batched Inputs". Our
method stands out by enhancing input diversity to mitigate output errors. We
suggest particular regularization techniques for both end-to-end and cascade
systems. We conducted experiments on IWSLT Simultaneous Speech Translation
(SimulST) tasks, which demonstrate that our approach achieves low latency while
maintaining no more than 2 BLEU points loss compared to offline systems.
Furthermore, our SimulST systems attained several new state-of-the-art results
in various language directions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05707">CAT-LLM: Prompting Large Language Models with Text Style Definition for Chinese Article-style Transfer. (arXiv:2401.05707v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tao_Z/0/1/0/all/0/1">Zhen Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xi_D/0/1/0/all/0/1">Dinghao Xi</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhiyu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1">Liumin Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Wei Xu</a></p>
<p>Text style transfer is increasingly prominent in online entertainment and
social media. However, existing research mainly concentrates on style transfer
within individual English sentences, while ignoring the complexity of long
Chinese texts, which limits the wider applicability of style transfer in
digital media realm. To bridge this gap, we propose a Chinese Article-style
Transfer framework (CAT-LLM), leveraging the capabilities of Large Language
Models (LLMs). CAT-LLM incorporates a bespoke, pluggable Text Style Definition
(TSD) module aimed at comprehensively analyzing text features in articles,
prompting LLMs to efficiently transfer Chinese article-style. The TSD module
integrates a series of machine learning algorithms to analyze article-style
from both words and sentences levels, thereby aiding LLMs thoroughly grasp the
target style without compromising the integrity of the original text. In
addition, this module supports dynamic expansion of internal style trees,
showcasing robust compatibility and allowing flexible optimization in
subsequent research. Moreover, we select five Chinese articles with distinct
styles and create five parallel datasets using ChatGPT, enhancing the models'
performance evaluation accuracy and establishing a novel paradigm for
evaluating subsequent research on article-style transfer. Extensive
experimental results affirm that CAT-LLM outperforms current research in terms
of transfer accuracy and content preservation, and has remarkable applicability
to various types of LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05727">Zero Resource Cross-Lingual Part Of Speech Tagging. (arXiv:2401.05727v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chopra_S/0/1/0/all/0/1">Sahil Chopra</a></p>
<p>Part of speech tagging in zero-resource settings can be an effective approach
for low-resource languages when no labeled training data is available. Existing
systems use two main techniques for POS tagging i.e. pretrained multilingual
large language models(LLM) or project the source language labels into the zero
resource target language and train a sequence labeling model on it. We explore
the latter approach using the off-the-shelf alignment module and train a hidden
Markov model(HMM) to predict the POS tags. We evaluate transfer learning setup
with English as a source language and French, German, and Spanish as target
languages for part-of-speech tagging. Our conclusion is that projected
alignment data in zero-resource language can be beneficial to predict POS tags.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05736">Cross-modal Retrieval for Knowledge-based Visual Question Answering. (arXiv:2401.05736v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lerner_P/0/1/0/all/0/1">Paul Lerner</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferret_O/0/1/0/all/0/1">Olivier Ferret</a> (LIST (CEA), DIASI), <a href="http://arxiv.org/find/cs/1/au:+Guinaudeau_C/0/1/0/all/0/1">Camille Guinaudeau</a></p>
<p>Knowledge-based Visual Question Answering about Named Entities is a
challenging task that requires retrieving information from a multimodal
Knowledge Base. Named entities have diverse visual representations and are
therefore difficult to recognize. We argue that cross-modal retrieval may help
bridge the semantic gap between an entity and its depictions, and is foremost
complementary with mono-modal retrieval. We provide empirical evidence through
experiments with a multimodal dual encoder, namely CLIP, on the recent ViQuAE,
InfoSeek, and Encyclopedic-VQA datasets. Additionally, we study three different
strategies to fine-tune such a model: mono-modal, cross-modal, or joint
training. Our method, which combines mono-and cross-modal retrieval, is
competitive with billion-parameter models on the three datasets, while being
conceptually simpler and computationally cheaper.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05749">A Shocking Amount of the Web is Machine Translated: Insights from Multi-Way Parallelism. (arXiv:2401.05749v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Thompson_B/0/1/0/all/0/1">Brian Thompson</a>, <a href="http://arxiv.org/find/cs/1/au:+Dhaliwal_M/0/1/0/all/0/1">Mehak Preet Dhaliwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Frisch_P/0/1/0/all/0/1">Peter Frisch</a>, <a href="http://arxiv.org/find/cs/1/au:+Domhan_T/0/1/0/all/0/1">Tobias Domhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Federico_M/0/1/0/all/0/1">Marcello Federico</a></p>
<p>We show that content on the web is often translated into many languages, and
the low quality of these multi-way translations indicates they were likely
created using Machine Translation (MT). Multi-way parallel, machine generated
content not only dominates the translations in lower resource languages; it
also constitutes a large fraction of the total web content in those languages.
We also find evidence of a selection bias in the type of content which is
translated into many languages, consistent with low quality English content
being translated en masse into many lower resource languages, via MT. Our work
raises serious concerns about training models such as multilingual large
language models on both monolingual and bilingual data scraped from the web.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05777">Probing Structured Semantics Understanding and Generation of Language Models via Question Answering. (arXiv:2401.05777v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jinxin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1">Shulin Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1">Jiaxin Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tingjian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1">Lei Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Juanzi Li</a></p>
<p>Recent advancement in the capabilities of large language models (LLMs) has
triggered a new surge in LLMs' evaluation. Most recent evaluation works tends
to evaluate the comprehensive ability of LLMs over series of tasks. However,
the deep structure understanding of natural language is rarely explored. In
this work, we examine the ability of LLMs to deal with structured semantics on
the tasks of question answering with the help of the human-constructed formal
language. Specifically, we implement the inter-conversion of natural and formal
language through in-context learning of LLMs to verify their ability to
understand and generate the structured logical forms. Extensive experiments
with models of different sizes and in different formal languages show that
today's state-of-the-art LLMs' understanding of the logical forms can approach
human level overall, but there still are plenty of room in generating correct
logical forms, which suggest that it is more effective to use LLMs to generate
more natural language training data to reinforce a small model than directly
answering questions with LLMs. Moreover, our results also indicate that models
exhibit considerable sensitivity to different formal languages. In general, the
formal language with the lower the formalization level, i.e. the more similar
it is to natural language, is more LLMs-friendly.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05778">Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems. (arXiv:2401.05778v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cui_T/0/1/0/all/0/1">Tianyu Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yanling Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1">Chuanpu Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1">Yong Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Sijia Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1">Xinhao Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yunpeng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qinglin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1">Ziyi Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Peiyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1">Zhixing Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1">Junwu Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1">Xinyu Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1">Zujie Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Ke Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qi Li</a></p>
<p>Large language models (LLMs) have strong capabilities in solving diverse
natural language processing tasks. However, the safety and security issues of
LLM systems have become the major obstacle to their widespread application.
Many studies have extensively investigated risks in LLM systems and developed
the corresponding mitigation strategies. Leading-edge enterprises such as
OpenAI, Google, Meta, and Anthropic have also made lots of efforts on
responsible LLMs. Therefore, there is a growing need to organize the existing
studies and establish comprehensive taxonomies for the community. In this
paper, we delve into four essential modules of an LLM system, including an
input module for receiving prompts, a language model trained on extensive
corpora, a toolchain module for development and deployment, and an output
module for exporting LLM-generated content. Based on this, we propose a
comprehensive taxonomy, which systematically analyzes potential risks
associated with each module of an LLM system and discusses the corresponding
mitigation strategies. Furthermore, we review prevalent benchmarks, aiming to
facilitate the risk assessment of LLM systems. We hope that this paper can help
LLM participants embrace a systematic perspective to build their responsible
LLM systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05787">Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning. (arXiv:2401.05787v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Parvez_M/0/1/0/all/0/1">Md Rizwan Parvez</a></p>
<p>While chain-of-thought (CoT) prompting has revolutionized how LLMs perform
reasoning tasks, its current methods and variations (e.g, Self-consistency,
ReACT, Reflexion, Tree-of-Thoughts (ToT), Cumulative Reasoning (CR)) suffer
from limitations like slowness, limited context grounding, hallucination and
inconsistent outputs. To overcome these challenges, we introduce Evidence to
Generate (E2G), a novel single-agent, two-step prompting framework. Instead of
unverified reasoning claims, this innovative approach leverages the power of
"evidence for decision making" by first focusing exclusively on the thought
sequences (the series of intermediate steps) explicitly mentioned in the
context which then serve as extracted evidence, guiding the LLM's output
generation process with greater precision and efficiency. This simple yet
powerful approach unlocks the true potential of chain-of-thought like
prompting, paving the way for faster, more reliable, and more contextually
aware reasoning in LLMs. \tool achieves remarkable results robustly across a
wide range of knowledge-intensive reasoning and generation tasks, surpassing
baseline approaches with state-of-the-art LLMs. For example, (i) on LogiQA
benchmark using GPT-4 as backbone model, \tool achieves a new state-of-the
Accuracy of 53.8% exceeding CoT by 18%, ToT by 11%, CR by 9% (ii) a variant of
E2G with PaLM2 outperforms the variable-shot performance of Gemini Ultra by 0.9
F1 points, reaching an F1 score of 83.3 on a subset of DROP.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05792">Discovering Low-rank Subspaces for Language-agnostic Multilingual Representations. (arXiv:2401.05792v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1">Zhihui Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Handong Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1">Tong Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shuai Li</a></p>
<p>Large pretrained multilingual language models (ML-LMs) have shown remarkable
capabilities of zero-shot cross-lingual transfer, without direct cross-lingual
supervision. While these results are promising, follow-up works found that,
within the multilingual embedding spaces, there exists strong language identity
information which hinders the expression of linguistic factors shared across
languages. For semantic tasks like cross-lingual sentence retrieval, it is
desired to remove such language identity signals to fully leverage semantic
information. In this work, we provide a novel view of projecting away
language-specific factors from a multilingual embedding space. Specifically, we
discover that there exists a low-rank subspace that primarily encodes
information irrelevant to semantics (e.g., syntactic information). To identify
this subspace, we present a simple but effective unsupervised method based on
singular value decomposition with multiple monolingual corpora as input. Once
the subspace is found, we can directly project the original embeddings into the
null space to boost language agnosticism without finetuning. We systematically
evaluate our method on various tasks including the challenging
language-agnostic QA retrieval task. Empirical results show that applying our
method consistently leads to improvements over commonly used ML-LMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05799">Designing Heterogeneous LLM Agents for Financial Sentiment Analysis. (arXiv:2401.05799v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1">Frank Xing</a></p>
<p>Large language models (LLMs) have drastically changed the possible ways to
design intelligent systems, shifting the focuses from massive data acquisition
and new modeling training to human alignment and strategical elicitation of the
full potential of existing pre-trained models. This paradigm shift, however, is
not fully realized in financial sentiment analysis (FSA), due to the
discriminative nature of this task and a lack of prescriptive knowledge of how
to leverage generative models in such a context. This study investigates the
effectiveness of the new paradigm, i.e., using LLMs without fine-tuning for
FSA. Rooted in Minsky's theory of mind and emotions, a design framework with
heterogeneous LLM agents is proposed. The framework instantiates specialized
agents using prior domain knowledge of the types of FSA errors and reasons on
the aggregated agent discussions. Comprehensive evaluation on FSA datasets show
that the framework yields better accuracies, especially when the discussions
are substantial. This study contributes to the design foundations and paves new
avenues for LLMs-based FSA. Implications on business and management are also
discussed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05811">Tuning LLMs with Contrastive Alignment Instructions for Machine Translation in Unseen, Low-resource Languages. (arXiv:2401.05811v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1">Zhuoyuan Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yen Yu</a></p>
<p>This article introduces contrastive alignment instructions (AlignInstruct) to
address two challenges in machine translation (MT) on large language models
(LLMs). One is the expansion of supported languages to previously unseen ones.
The second relates to the lack of data in low-resource languages. Model
fine-tuning through MT instructions (MTInstruct) is a straightforward approach
to the first challenge. However, MTInstruct is limited by weak cross-lingual
signals inherent in the second challenge. AlignInstruct emphasizes
cross-lingual supervision via a cross-lingual discriminator built using
statistical word alignments. Our results based on fine-tuning the BLOOMZ models
(1b1, 3b, and 7b1) in up to 24 unseen languages showed that: (1) LLMs can
effectively translate unseen languages using MTInstruct; (2) AlignInstruct led
to consistent improvements in translation quality across 48 translation
directions involving English; (3) Discriminator-based instructions outperformed
their generative counterparts as cross-lingual instructions; (4) AlignInstruct
improved performance in 30 zero-shot directions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05822">Towards Goal-Oriented Agents for Evolving Problems Observed via Conversation. (arXiv:2401.05822v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Free_M/0/1/0/all/0/1">Michael Free</a>, <a href="http://arxiv.org/find/cs/1/au:+Langworthy_A/0/1/0/all/0/1">Andrew Langworthy</a>, <a href="http://arxiv.org/find/cs/1/au:+Dimitropoulaki_M/0/1/0/all/0/1">Mary Dimitropoulaki</a>, <a href="http://arxiv.org/find/cs/1/au:+Thompson_S/0/1/0/all/0/1">Simon Thompson</a></p>
<p>The objective of this work is to train a chatbot capable of solving evolving
problems through conversing with a user about a problem the chatbot cannot
directly observe. The system consists of a virtual problem (in this case a
simple game), a simulated user capable of answering natural language questions
that can observe and perform actions on the problem, and a Deep Q-Network
(DQN)-based chatbot architecture. The chatbot is trained with the goal of
solving the problem through dialogue with the simulated user using
reinforcement learning. The contributions of this paper are as follows: a
proposed architecture to apply a conversational DQN-based agent to evolving
problems, an exploration of training methods such as curriculum learning on
model performance and the effect of modified reward functions in the case of
increasing environment complexity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05827">Hallucination Benchmark in Medical Visual Question Answering. (arXiv:2401.05827v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jinge Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Yunsoo Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Honghan Wu</a></p>
<p>The recent success of large language and vision models on vision question
answering (VQA), particularly their applications in medicine (Med-VQA), has
shown a great potential of realizing effective visual assistants for
healthcare. However, these models are not extensively tested on the
hallucination phenomenon in clinical settings. Here, we created a hallucination
benchmark of medical images paired with question-answer sets and conducted a
comprehensive evaluation of the state-of-the-art models. The study provides an
in-depth analysis of current models limitations and reveals the effectiveness
of various prompting strategies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05849">Inferring Intentions to Speak Using Accelerometer Data In-the-Wild. (arXiv:2401.05849v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Litian Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Molhoek_J/0/1/0/all/0/1">Jord Molhoek</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jing Zhou</a></p>
<p>Humans have good natural intuition to recognize when another person has
something to say. It would be interesting if an AI can also recognize
intentions to speak. Especially in scenarios when an AI is guiding a group
discussion, this can be a useful skill. This work studies the inference of
successful and unsuccessful intentions to speak from accelerometer data. This
is chosen because it is privacy-preserving and feasible for in-the-wild
settings since it can be placed in a smart badge. Data from a real-life social
networking event is used to train a machine-learning model that aims to infer
intentions to speak. A subset of unsuccessful intention-to-speak cases in the
data is annotated. The model is trained on the successful intentions to speak
and evaluated on both the successful and unsuccessful cases. In conclusion,
there is useful information in accelerometer data, but not enough to reliably
capture intentions to speak. For example, posture shifts are correlated with
intentions to speak, but people also often shift posture without having an
intention to speak, or have an intention to speak without shifting their
posture. More modalities are likely needed to reliably infer intentions to
speak.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05861">Towards Boosting Many-to-Many Multilingual Machine Translation with Large Language Models. (arXiv:2401.05861v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1">Pengzhi Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1">Zhongjun He</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Hua Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haifeng Wang</a></p>
<p>The training paradigm for machine translation has gradually shifted, from
learning neural machine translation (NMT) models with extensive parallel
corpora to instruction finetuning on pretrained multilingual large language
models (LLMs) with high-quality translation pairs. In this paper, we focus on
boosting the many-to-many multilingual translation performance of LLMs with an
emphasis on zero-shot translation directions. We demonstrate that prompt
strategies adopted during instruction finetuning are crucial to zero-shot
translation performance and introduce a cross-lingual consistency
regularization, XConST, to bridge the representation gap among different
languages and improve zero-shot translation performance. XConST is not a new
method, but a version of CrossConST (Gao et al., 2023a) adapted for
multilingual finetuning on LLMs with translation instructions. Experimental
results on ALMA (Xu et al., 2023) and LLaMA-2 (Touvron et al., 2023) show that
our approach consistently improves translation performance. Our implementations
are available at https://github.com/gpengzhi/CrossConST-LLM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05871">Enhancing Personality Recognition in Dialogue by Data Augmentation and Heterogeneous Conversational Graph Networks. (arXiv:2401.05871v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1">Yahui Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1">Haiyue Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1">Tianyu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Kawahara_T/0/1/0/all/0/1">Tatsuya Kawahara</a></p>
<p>Personality recognition is useful for enhancing robots' ability to tailor
user-adaptive responses, thus fostering rich human-robot interactions. One of
the challenges in this task is a limited number of speakers in existing
dialogue corpora, which hampers the development of robust, speaker-independent
personality recognition models. Additionally, accurately modeling both the
interdependencies among interlocutors and the intra-dependencies within the
speaker in dialogues remains a significant issue. To address the first
challenge, we introduce personality trait interpolation for speaker data
augmentation. For the second, we propose heterogeneous conversational graph
networks to independently capture both contextual influences and inherent
personality traits. Evaluations on the RealPersonaChat corpus demonstrate our
method's significant improvements over existing baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05883">Generative Deduplication For Socia Media Data Selection. (arXiv:2401.05883v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xianming Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jing Li</a></p>
<p>Social media data is plagued by the redundancy problem caused by its noisy
nature, leading to increased training time and model bias. To address this
issue, we propose a novel approach called generative duplication. It aims to
remove duplicate text from noisy social media data and mitigate model bias. By
doing so, it can improve social media language understanding performance and
save training time. Extensive experiments demonstrate that the proposed
generative deduplication can effectively reduce training samples while
improving performance. This evidence suggests the effectiveness of generative
deduplication and its importance in social media language understanding.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05908">EpilepsyLLM: Domain-Specific Large Language Model Fine-tuned with Epilepsy Medical Knowledge. (arXiv:2401.05908v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xuyang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1">Qibin Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tanaka_T/0/1/0/all/0/1">Toshihisa Tanaka</a></p>
<p>With large training datasets and massive amounts of computing sources, large
language models (LLMs) achieve remarkable performance in comprehensive and
generative ability. Based on those powerful LLMs, the model fine-tuned with
domain-specific datasets posseses more specialized knowledge and thus is more
practical like medical LLMs. However, the existing fine-tuned medical LLMs are
limited to general medical knowledge with English language. For
disease-specific problems, the model's response is inaccurate and sometimes
even completely irrelevant, especially when using a language other than
English. In this work, we focus on the particular disease of Epilepsy with
Japanese language and introduce a customized LLM termed as EpilepsyLLM. Our
model is trained from the pre-trained LLM by fine-tuning technique using
datasets from the epilepsy domain. The datasets contain knowledge of basic
information about disease, common treatment methods and drugs, and important
notes in life and work. The experimental results demonstrate that EpilepsyLLM
can provide more reliable and specialized medical knowledge responses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05912">Prompt-based mental health screening from social media text. (arXiv:2401.05912v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Santos_W/0/1/0/all/0/1">Wesley Ramos dos Santos</a>, <a href="http://arxiv.org/find/cs/1/au:+Paraboni_I/0/1/0/all/0/1">Ivandre Paraboni</a></p>
<p>This article presents a method for prompt-based mental health screening from
a large and noisy dataset of social media text. Our method uses GPT 3.5.
prompting to distinguish publications that may be more relevant to the task,
and then uses a straightforward bag-of-words text classifier to predict actual
user labels. Results are found to be on pair with a BERT mixture of experts
classifier, and incurring only a fraction of its computational costs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05914">How Teachers Can Use Large Language Models and Bloom&#x27;s Taxonomy to Create Educational Quizzes. (arXiv:2401.05914v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Elkins_S/0/1/0/all/0/1">Sabina Elkins</a>, <a href="http://arxiv.org/find/cs/1/au:+Kochmar_E/0/1/0/all/0/1">Ekaterina Kochmar</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheung_J/0/1/0/all/0/1">Jackie C.K. Cheung</a>, <a href="http://arxiv.org/find/cs/1/au:+Serban_I/0/1/0/all/0/1">Iulian Serban</a></p>
<p>Question generation (QG) is a natural language processing task with an
abundance of potential benefits and use cases in the educational domain. In
order for this potential to be realized, QG systems must be designed and
validated with pedagogical needs in mind. However, little research has assessed
or designed QG approaches with the input from real teachers or students. This
paper applies a large language model-based QG approach where questions are
generated with learning goals derived from Bloom's taxonomy. The automatically
generated questions are used in multiple experiments designed to assess how
teachers use them in practice. The results demonstrate that teachers prefer to
write quizzes with automatically generated questions, and that such quizzes
have no loss in quality compared to handwritten versions. Further, several
metrics indicate that automatically generated questions can even improve the
quality of the quizzes created, showing the promise for large scale use of QG
in the classroom setting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05928">Mitigating Unhelpfulness in Emotional Support Conversations with Multifaceted AI Feedback. (arXiv:2401.05928v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiashuo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Chunpu Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Leong_C/0/1/0/all/0/1">Chak Tou Leong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wenjie Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jing Li</a></p>
<p>An emotional support conversation system aims to alleviate users' emotional
distress and assist them in addressing their challenges. To generate supportive
responses, it is critical to consider multiple factors such as empathy, support
strategies, and response coherence, as established in prior methods.
Nonetheless, previous models occasionally generate unhelpful responses, which
intend to provide support but display counterproductive effects. According to
psychology and communication theories, poor performance in just one
contributing factor might cause a response to be unhelpful. From the model
training perspective, since these models have not been exposed to unhelpful
responses during their training phase, they are unable to distinguish if the
tokens they generate might result in unhelpful responses during inference. To
address this issue, we introduce a novel model-agnostic framework named
mitigating unhelpfulness with multifaceted AI feedback for emotional support
(Muffin). Specifically, Muffin employs a multifaceted AI feedback module to
assess the helpfulness of responses generated by a specific model with
consideration of multiple factors. Using contrastive learning, it then reduces
the likelihood of the model generating unhelpful responses compared to the
helpful ones. Experimental results demonstrate that Muffin effectively
mitigates the generation of unhelpful responses while slightly increasing
response fluency and relevance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05930">SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully. (arXiv:2401.05930v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kai_J/0/1/0/all/0/1">Jushi Kai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tianhang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1">Hai Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zhouhan Lin</a></p>
<p>Large language models (LLMs) demonstrate great performance in text
generation. However, LLMs are still suffering from hallucinations. In this
work, we propose an inference-time method, Self-Highlighted Hesitation (SH2),
to help LLMs decode more truthfully. SH2 is based on a simple fact rooted in
information theory that for an LLM, the tokens predicted with lower
probabilities are prone to be more informative than others. Our analysis shows
that the tokens assigned with lower probabilities by an LLM are more likely to
be closely related to factual information, such as nouns, proper nouns, and
adjectives. Therefore, we propose to ''highlight'' the factual information by
selecting the tokens with the lowest probabilities and concatenating them to
the original context, thus forcing the model to repeatedly read and hesitate on
these tokens before generation. During decoding, we also adopt contrastive
decoding to emphasize the difference in the output probabilities brought by the
hesitation. Experimental results demonstrate that our SH2, requiring no
additional data or models, can effectively help LLMs elicit factual knowledge
and distinguish hallucinated contexts. Significant and consistent improvements
are achieved by SH2 for LLaMA-7b and LLaMA2-7b on multiple hallucination tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05949">Universal Vulnerabilities in Large Language Models: In-context Learning Backdoor Attacks. (arXiv:2401.05949v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1">Shuai Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_M/0/1/0/all/0/1">Meihuizi Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Tuan_L/0/1/0/all/0/1">Luu Anh Tuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1">Jinming Wen</a></p>
<p>In-context learning, a paradigm bridging the gap between pre-training and
fine-tuning, has demonstrated high efficacy in several NLP tasks, especially in
few-shot settings. Unlike traditional fine-tuning methods, in-context learning
adapts pre-trained models to unseen tasks without updating any parameters.
Despite being widely applied, in-context learning is vulnerable to malicious
attacks. In this work, we raise security concerns regarding this paradigm. Our
studies demonstrate that an attacker can manipulate the behavior of large
language models by poisoning the demonstration context, without the need for
fine-tuning the model. Specifically, we have designed a new backdoor attack
method, named ICLAttack, to target large language models based on in-context
learning. Our method encompasses two types of attacks: poisoning demonstration
examples and poisoning prompts, which can make models behave in accordance with
predefined intentions. ICLAttack does not require additional fine-tuning to
implant a backdoor, thus preserving the model's generality. Furthermore, the
poisoned examples are correctly labeled, enhancing the natural stealth of our
attack method. Extensive experimental results across several language models,
ranging in size from 1.3B to 40B parameters, demonstrate the effectiveness of
our attack method, exemplified by a high average attack success rate of 95.0%
across the three datasets on OPT models. Our findings highlight the
vulnerabilities of language models, and we hope this work will raise awareness
of the possible security threats associated with in-context learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05952">LLM-as-a-Coauthor: The Challenges of Detecting LLM-Human Mixcase. (arXiv:2401.05952v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1">Chujie Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Dongping Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qihui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yue Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1">Yao Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Lichao Sun</a></p>
<p>With the remarkable development and widespread applications of large language
models (LLMs), the use of machine-generated text (MGT) is becoming increasingly
common. This trend brings potential risks, particularly to the quality and
completeness of information in fields such as news and education. Current
research predominantly addresses the detection of pure MGT without adequately
addressing mixed scenarios including AI-revised Human-Written Text (HWT) or
human-revised MGT. To confront this challenge, we introduce mixcase, a novel
concept representing a hybrid text form involving both machine-generated and
human-generated content. We collected mixcase instances generated from multiple
daily text-editing scenarios and composed MixSet, the first dataset dedicated
to studying these mixed modification scenarios. We conduct experiments to
evaluate the efficacy of popular MGT detectors, assessing their effectiveness,
robustness, and generalization performance. Our findings reveal that existing
detectors struggle to identify mixcase as a separate class or MGT, particularly
in dealing with subtle modifications and style adaptability. This research
underscores the urgent need for more fine-grain detectors tailored for mixcase,
offering valuable insights for future research. Code and Models are available
at https://github.com/Dongping-Chen/MixSet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05967">Block-Diagonal Orthogonal Relation and Matrix Entity for Knowledge Graph Embedding. (arXiv:2401.05967v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yihua Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shimodaira_H/0/1/0/all/0/1">Hidetoshi Shimodaira</a></p>
<p>The primary aim of Knowledge Graph embeddings (KGE) is to learn
low-dimensional representations of entities and relations for predicting
missing facts. While rotation-based methods like RotatE and QuatE perform well
in KGE, they face two challenges: limited model flexibility requiring
proportional increases in relation size with entity dimension, and difficulties
in generalizing the model for higher-dimensional rotations. To address these
issues, we introduce OrthogonalE, a novel KGE model employing matrices for
entities and block-diagonal orthogonal matrices with Riemannian optimization
for relations. This approach enhances the generality and flexibility of KGE
models. The experimental results indicate that our new KGE model, OrthogonalE,
is both general and flexible, significantly outperforming state-of-the-art KGE
models while substantially reducing the number of relation parameters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05998">Combating Adversarial Attacks with Multi-Agent Debate. (arXiv:2401.05998v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chern_S/0/1/0/all/0/1">Steffi Chern</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1">Zhen Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1">Andy Liu</a></p>
<p>While state-of-the-art language models have achieved impressive results, they
remain susceptible to inference-time adversarial attacks, such as adversarial
prompts generated by red teams <a href="/abs/2209.07858">arXiv:2209.07858</a>. One approach proposed to
improve the general quality of language model generations is multi-agent
debate, where language models self-evaluate through discussion and feedback
<a href="/abs/2305.14325">arXiv:2305.14325</a>. We implement multi-agent debate between current
state-of-the-art language models and evaluate models' susceptibility to red
team attacks in both single- and multi-agent settings. We find that multi-agent
debate can reduce model toxicity when jailbroken or less capable models are
forced to debate with non-jailbroken or more capable models. We also find
marginal improvements through the general usage of multi-agent interactions. We
further perform adversarial prompt content classification via embedding
clustering, and analyze the susceptibility of different models to different
types of attack topics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06034">LinguAlchemy: Fusing Typological and Geographical Elements for Unseen Language Generalization. (arXiv:2401.06034v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Adilazuarda_M/0/1/0/all/0/1">Muhammad Farid Adilazuarda</a>, <a href="http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1">Samuel Cahyawijaya</a>, <a href="http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1">Alham Fikri Aji</a>, <a href="http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1">Genta Indra Winata</a>, <a href="http://arxiv.org/find/cs/1/au:+Purwarianti_A/0/1/0/all/0/1">Ayu Purwarianti</a></p>
<p>Pretrained language models (PLMs) have shown remarkable generalization toward
multiple tasks and languages. Nonetheless, the generalization of PLMs towards
unseen languages is poor, resulting in significantly worse language
performance, or even generating nonsensical responses that are comparable to a
random baseline. This limitation has been a longstanding problem of PLMs
raising the problem of diversity and equal access to language modeling
technology. In this work, we solve this limitation by introducing LinguAlchemy,
a regularization technique that incorporates various aspects of languages
covering typological, geographical, and phylogenetic constraining the resulting
representation of PLMs to better characterize the corresponding linguistics
constraints. LinguAlchemy significantly improves the accuracy performance of
mBERT and XLM-R on unseen languages by ~18% and ~2%, respectively compared to
fully finetuned models and displaying a high degree of unseen language
generalization. We further introduce AlchemyScale and AlchemyTune, extension of
LinguAlchemy which adjusts the linguistic regularization weights automatically,
alleviating the need for hyperparameter search. LinguAlchemy enables better
cross-lingual generalization to unseen languages which is vital for better
inclusivity and accessibility of PLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06059">Investigating Data Contamination for Pre-training Language Models. (arXiv:2401.06059v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1">Minhao Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1">Ken Ziyu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1">Ming Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Schaeffer_R/0/1/0/all/0/1">Rylan Schaeffer</a>, <a href="http://arxiv.org/find/cs/1/au:+Ouyang_S/0/1/0/all/0/1">Siru Ouyang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Jiawei Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Koyejo_S/0/1/0/all/0/1">Sanmi Koyejo</a></p>
<p>Language models pre-trained on web-scale corpora demonstrate impressive
capabilities on diverse downstream tasks. However, there is increasing concern
whether such capabilities might arise from evaluation datasets being included
in the pre-training corpus -- a phenomenon known as \textit{data contamination}
-- in a manner that artificially increases performance. There has been little
understanding of how this potential contamination might influence LMs'
performance on downstream tasks. In this paper, we explore the impact of data
contamination at the pre-training stage by pre-training a series of GPT-2
models \textit{from scratch}. We highlight the effect of both text
contamination (\textit{i.e.}\ input text of the evaluation samples) and
ground-truth contamination (\textit{i.e.}\ the prompts asked on the input and
the desired outputs) from evaluation data. We also investigate the effects of
repeating contamination for various downstream tasks. Additionally, we examine
the prevailing n-gram-based definitions of contamination within current LLM
reports, pinpointing their limitations and inadequacy. Our findings offer new
insights into data contamination's effects on language model capabilities and
underscore the need for independent, comprehensive contamination assessments in
LLM studies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06066">DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models. (arXiv:2401.06066v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1">Damai Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1">Chengqi Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1">Chenggang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">R.X. Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1">Huazuo Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Deli Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiashi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1">Wangding Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1">Xingkai Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Y. Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1">Zhenda Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Y.K. Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1">Panpan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1">Fuli Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruan_C/0/1/0/all/0/1">Chong Ruan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1">Zhifang Sui</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1">Wenfeng Liang</a></p>
<p>In the era of large language models, Mixture-of-Experts (MoE) is a promising
architecture for managing computational costs when scaling up model parameters.
However, conventional MoE architectures like GShard, which activate the top-$K$
out of $N$ experts, face challenges in ensuring expert specialization, i.e.
each expert acquires non-overlapping and focused knowledge. In response, we
propose the DeepSeekMoE architecture towards ultimate expert specialization. It
involves two principal strategies: (1) finely segmenting the experts into $mN$
ones and activating $mK$ from them, allowing for a more flexible combination of
activated experts; (2) isolating $K_s$ experts as shared ones, aiming at
capturing common knowledge and mitigating redundancy in routed experts.
Starting from a modest scale with 2B parameters, we demonstrate that
DeepSeekMoE 2B achieves comparable performance with GShard 2.9B, which has 1.5
times the expert parameters and computation. In addition, DeepSeekMoE 2B nearly
approaches the performance of its dense counterpart with the same number of
total parameters, which set the upper bound of MoE models. Subsequently, we
scale up DeepSeekMoE to 16B parameters and show that it achieves comparable
performance with LLaMA2 7B, with only about 40% of computations. Further, our
preliminary efforts to scale up DeepSeekMoE to 145B parameters consistently
validate its substantial advantages over the GShard architecture, and show its
performance comparable with DeepSeek 67B, using only 28.5% (maybe even 18.2%)
of computations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06071">LEGO:Language Enhanced Multi-modal Grounding Model. (arXiv:2401.06071v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhaowei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1">Qi Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1">Hang Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1">Yiqing Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_Q/0/1/0/all/0/1">Qi Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1">Ran Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1">Junting Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zefeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Vu_V/0/1/0/all/0/1">Van Tu Vu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zhida Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tao Wang</a></p>
<p>Multi-modal large language models have demonstrated impressive performance
across various tasks in different modalities. However, existing multi-modal
models primarily emphasize capturing global information within each modality
while neglecting the importance of perceiving local information across
modalities. Consequently, these models lack the ability to effectively
understand the fine-grained details of input data, limiting their performance
in tasks that require a more nuanced understanding. To address this limitation,
there is a compelling need to develop models that enable fine-grained
understanding across multiple modalities, thereby enhancing their applicability
to a wide range of tasks. In this paper, we propose LEGO, a language enhanced
multi-modal grounding model. Beyond capturing global information like other
multi-modal models, our proposed model excels at tasks demanding a detailed
understanding of local information within the input. It demonstrates precise
identification and localization of specific regions in images or moments in
videos. To achieve this objective, we design a diversified dataset construction
pipeline, resulting in a multi-modal, multi-granularity dataset for model
training. The code, dataset, and demo of our model can be found at https:
//github.com/lzw-lzw/LEGO.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06072">Chain of History: Learning and Forecasting with LLMs for Temporal Knowledge Graph Completion. (arXiv:2401.06072v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1">Ruilin Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_T/0/1/0/all/0/1">Tianle Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Haoling Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Junzhe Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zicheng Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiayi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yujiu Yang</a></p>
<p>Temporal Knowledge Graph Completion (TKGC) is a challenging task of
predicting missing event links at future timestamps by leveraging established
temporal structural knowledge. Given the formidable generative capabilities
inherent in LLMs (LLMs), this paper proposes a novel approach to conceptualize
temporal link prediction as an event generation task within the context of a
historical event chain. We employ efficient fine-tuning methods to make LLMs
adapt to specific graph textual information and patterns discovered in temporal
timelines. Furthermore, we introduce structure-based historical data
augmentation and the integration of reverse knowledge to emphasize LLMs'
awareness of structural information, thereby enhancing their reasoning
capabilities. We conduct thorough experiments on multiple widely used datasets
and find that our fine-tuned model outperforms existing embedding-based models
on multiple metrics, achieving SOTA results. We also carry out sufficient
ablation experiments to explore the key influencing factors when LLMs perform
structured temporal knowledge inference tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06081">Improving Large Language Models via Fine-grained Reinforcement Learning with Minimum Editing Constraint. (arXiv:2401.06081v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhipeng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1">Kun Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Wayne Xin Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1">Junchen Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1">Fuzheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Di Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1">Ji-Rong Wen</a></p>
<p>Reinforcement learning (RL) has been widely used in training large language
models~(LLMs) for preventing unexpected outputs, \eg reducing harmfulness and
errors. However, existing RL methods mostly adopt the instance-level reward,
which is unable to provide fine-grained supervision for complex reasoning
tasks, and can not focus on the few key tokens that lead to the incorrectness.
To address it, we propose a new RL method named \textbf{RLMEC} that
incorporates a generative model as the reward model, which is trained by the
erroneous solution rewriting task under the minimum editing constraint, and can
produce token-level rewards for RL training. Based on the generative reward
model, we design the token-level RL objective for training and an
imitation-based regularization for stabilizing RL process. And the both
objectives focus on the learning of the key tokens for the erroneous solution,
reducing the effect of other unimportant tokens. The experiment results on
mathematical tasks and question-answering tasks have demonstrated the
effectiveness of our approach. Our code and data are available at
\url{https://github.com/RUCAIBox/RLMEC}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06088">Autocompletion of Chief Complaints in the Electronic Health Records using Large Language Models. (arXiv:2401.06088v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Islam_K/0/1/0/all/0/1">K M Sajjadul Islam</a>, <a href="http://arxiv.org/find/cs/1/au:+Nipu_A/0/1/0/all/0/1">Ayesha Siddika Nipu</a>, <a href="http://arxiv.org/find/cs/1/au:+Madiraju_P/0/1/0/all/0/1">Praveen Madiraju</a>, <a href="http://arxiv.org/find/cs/1/au:+Deshpande_P/0/1/0/all/0/1">Priya Deshpande</a></p>
<p>The Chief Complaint (CC) is a crucial component of a patient's medical record
as it describes the main reason or concern for seeking medical care. It
provides critical information for healthcare providers to make informed
decisions about patient care. However, documenting CCs can be time-consuming
for healthcare providers, especially in busy emergency departments. To address
this issue, an autocompletion tool that suggests accurate and well-formatted
phrases or sentences for clinical notes can be a valuable resource for triage
nurses. In this study, we utilized text generation techniques to develop
machine learning models using CC data. In our proposed work, we train a Long
Short-Term Memory (LSTM) model and fine-tune three different variants of
Biomedical Generative Pretrained Transformers (BioGPT), namely
microsoft/biogpt, microsoft/BioGPT-Large, and microsoft/BioGPT-Large-PubMedQA.
Additionally, we tune a prompt by incorporating exemplar CC sentences,
utilizing the OpenAI API of GPT-4. We evaluate the models' performance based on
the perplexity score, modified BERTScore, and cosine similarity score. The
results show that BioGPT-Large exhibits superior performance compared to the
other models. It consistently achieves a remarkably low perplexity score of
1.65 when generating CC, whereas the baseline LSTM model achieves the best
perplexity score of 170. Further, we evaluate and assess the proposed models'
performance and the outcome of GPT-4.0. Our study demonstrates that utilizing
LLMs such as BioGPT, leads to the development of an effective autocompletion
tool for generating CC documentation in healthcare settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06102">Patchscope: A Unifying Framework for Inspecting Hidden Representations of Language Models. (arXiv:2401.06102v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ghandeharioun_A/0/1/0/all/0/1">Asma Ghandeharioun</a>, <a href="http://arxiv.org/find/cs/1/au:+Caciularu_A/0/1/0/all/0/1">Avi Caciularu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pearce_A/0/1/0/all/0/1">Adam Pearce</a>, <a href="http://arxiv.org/find/cs/1/au:+Dixon_L/0/1/0/all/0/1">Lucas Dixon</a>, <a href="http://arxiv.org/find/cs/1/au:+Geva_M/0/1/0/all/0/1">Mor Geva</a></p>
<p>Inspecting the information encoded in hidden representations of large
language models (LLMs) can explain models' behavior and verify their alignment
with human values. Given the capabilities of LLMs in generating
human-understandable text, we propose leveraging the model itself to explain
its internal representations in natural language. We introduce a framework
called Patchscopes and show how it can be used to answer a wide range of
research questions about an LLM's computation. We show that prior
interpretability methods based on projecting representations into the
vocabulary space and intervening on the LLM computation, can be viewed as
special instances of this framework. Moreover, several of their shortcomings
such as failure in inspecting early layers or lack of expressivity can be
mitigated by a Patchscope. Beyond unifying prior inspection techniques,
Patchscopes also opens up new possibilities such as using a more capable model
to explain the representations of a smaller model, and unlocks new applications
such as self-correction in multi-hop reasoning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06104">Transformers are Multi-State RNNs. (arXiv:2401.06104v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oren_M/0/1/0/all/0/1">Matanel Oren</a>, <a href="http://arxiv.org/find/cs/1/au:+Hassid_M/0/1/0/all/0/1">Michael Hassid</a>, <a href="http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1">Yossi Adi</a>, <a href="http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1">Roy Schwartz</a></p>
<p>Transformers are considered conceptually different compared to the previous
generation of state-of-the-art NLP models - recurrent neural networks (RNNs).
In this work, we demonstrate that decoder-only transformers can in fact be
conceptualized as infinite multi-state RNNs - an RNN variant with unlimited
hidden state size. We further show that pretrained transformers can be
converted into $\textit{finite}$ multi-state RNNs by fixing the size of their
hidden state. We observe that several existing transformers cache compression
techniques can be framed as such conversion policies, and introduce a novel
policy, TOVA, which is simpler compared to these policies. Our experiments with
several long range tasks indicate that TOVA outperforms all other baseline
policies, while being nearly on par with the full (infinite) model, and using
in some cases only $\frac{1}{8}$ of the original cache size. Our results
indicate that transformer decoder LLMs often behave in practice as RNNs. They
also lay out the option of mitigating one of their most painful computational
bottlenecks - the size of their cache memory. We publicly release our code at
https://github.com/schwartz-lab-NLP/TOVA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06105">PALP: Prompt Aligned Personalization of Text-to-Image Models. (arXiv:2401.06105v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Arar_M/0/1/0/all/0/1">Moab Arar</a>, <a href="http://arxiv.org/find/cs/1/au:+Voynov_A/0/1/0/all/0/1">Andrey Voynov</a>, <a href="http://arxiv.org/find/cs/1/au:+Hertz_A/0/1/0/all/0/1">Amir Hertz</a>, <a href="http://arxiv.org/find/cs/1/au:+Avrahami_O/0/1/0/all/0/1">Omri Avrahami</a>, <a href="http://arxiv.org/find/cs/1/au:+Fruchter_S/0/1/0/all/0/1">Shlomi Fruchter</a>, <a href="http://arxiv.org/find/cs/1/au:+Pritch_Y/0/1/0/all/0/1">Yael Pritch</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1">Daniel Cohen-Or</a>, <a href="http://arxiv.org/find/cs/1/au:+Shamir_A/0/1/0/all/0/1">Ariel Shamir</a></p>
<p>Content creators often aim to create personalized images using personal
subjects that go beyond the capabilities of conventional text-to-image models.
Additionally, they may want the resulting image to encompass a specific
location, style, ambiance, and more. Existing personalization methods may
compromise personalization ability or the alignment to complex textual prompts.
This trade-off can impede the fulfillment of user prompts and subject fidelity.
We propose a new approach focusing on personalization methods for a
\emph{single} prompt to address this issue. We term our approach prompt-aligned
personalization. While this may seem restrictive, our method excels in
improving text alignment, enabling the creation of images with complex and
intricate prompts, which may pose a challenge for current techniques. In
particular, our method keeps the personalized model aligned with a target
prompt using an additional score distillation sampling term. We demonstrate the
versatility of our method in multi- and single-shot settings and further show
that it can compose multiple subjects or use inspiration from reference images,
such as artworks. We compare our approach quantitatively and qualitatively with
existing baselines and state-of-the-art techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06112">Axis Tour: Word Tour Determines the Order of Axes in ICA-transformed Embeddings. (arXiv:2401.06112v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yamagiwa_H/0/1/0/all/0/1">Hiroaki Yamagiwa</a>, <a href="http://arxiv.org/find/cs/1/au:+Takase_Y/0/1/0/all/0/1">Yusuke Takase</a>, <a href="http://arxiv.org/find/cs/1/au:+Shimodaira_H/0/1/0/all/0/1">Hidetoshi Shimodaira</a></p>
<p>Word embedding is one of the most important components in natural language
processing, but interpreting high-dimensional embeddings remains a challenging
problem. To address this problem, Independent Component Analysis (ICA) is
identified as an effective solution. ICA-transformed word embeddings reveal
interpretable semantic axes; however, the order of these axes are arbitrary. In
this study, we focus on this property and propose a novel method, Axis Tour,
which optimizes the order of the axes. Inspired by Word Tour, a one-dimensional
word embedding method, we aim to improve the clarity of the word embedding
space by maximizing the semantic continuity of the axes. Furthermore, we show
through experiments on downstream tasks that Axis Tour constructs better
low-dimensional embeddings compared to both PCA and ICA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06118">Extreme Compression of Large Language Models via Additive Quantization. (arXiv:2401.06118v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Egiazarian_V/0/1/0/all/0/1">Vage Egiazarian</a>, <a href="http://arxiv.org/find/cs/1/au:+Panferov_A/0/1/0/all/0/1">Andrei Panferov</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuznedelev_D/0/1/0/all/0/1">Denis Kuznedelev</a>, <a href="http://arxiv.org/find/cs/1/au:+Frantar_E/0/1/0/all/0/1">Elias Frantar</a>, <a href="http://arxiv.org/find/cs/1/au:+Babenko_A/0/1/0/all/0/1">Artem Babenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1">Dan Alistarh</a></p>
<p>The emergence of accurate open large language models (LLMs) has led to a race
towards quantization techniques for such models enabling execution on end-user
devices. In this paper, we revisit the problem of "extreme" LLM
compression--defined as targeting extremely low bit counts, such as 2 to 3 bits
per parameter, from the point of view of classic methods in Multi-Codebook
Quantization (MCQ). Our work builds on top of Additive Quantization, a classic
algorithm from the MCQ family, and adapts it to the quantization of language
models. The resulting algorithm advances the state-of-the-art in LLM
compression, outperforming all recently-proposed techniques in terms of
accuracy at a given compression budget. For instance, when compressing Llama 2
models to 2 bits per parameter, our algorithm quantizes the 7B model to 6.93
perplexity (a 1.29 improvement relative to the best prior work, and 1.81 points
from FP16), the 13B model to 5.70 perplexity (a .36 improvement) and the 70B
model to 3.94 perplexity (a .22 improvement) on WikiText2. We release our
implementation of Additive Quantization for Language Models AQLM as a baseline
to facilitate future research in LLM quantization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06121">TOFU: A Task of Fictitious Unlearning for LLMs. (arXiv:2401.06121v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maini_P/0/1/0/all/0/1">Pratyush Maini</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1">Zhili Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Schwarzschild_A/0/1/0/all/0/1">Avi Schwarzschild</a>, <a href="http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1">Zachary C. Lipton</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1">J. Zico Kolter</a></p>
<p>Large language models trained on massive corpora of data from the web can
memorize and reproduce sensitive or private data raising both legal and ethical
concerns. Unlearning, or tuning models to forget information present in their
training data, provides us with a way to protect private data after training.
Although several methods exist for such unlearning, it is unclear to what
extent they result in models equivalent to those where the data to be forgotten
was never learned in the first place. To address this challenge, we present
TOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen
our understanding of unlearning. We offer a dataset of 200 diverse synthetic
author profiles, each consisting of 20 question-answer pairs, and a subset of
these profiles called the forget set that serves as the target for unlearning.
We compile a suite of metrics that work together to provide a holistic picture
of unlearning efficacy. Finally, we provide a set of baseline results from
existing unlearning algorithms. Importantly, none of the baselines we consider
show effective unlearning motivating continued efforts to develop approaches
for unlearning that effectively tune models so that they truly behave as if
they were never trained on the forget data at all.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.14383">Linear Spaces of Meanings: Compositional Structures in Vision-Language Models. (arXiv:2302.14383v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Trager_M/0/1/0/all/0/1">Matthew Trager</a>, <a href="http://arxiv.org/find/cs/1/au:+Perera_P/0/1/0/all/0/1">Pramuditha Perera</a>, <a href="http://arxiv.org/find/cs/1/au:+Zancato_L/0/1/0/all/0/1">Luca Zancato</a>, <a href="http://arxiv.org/find/cs/1/au:+Achille_A/0/1/0/all/0/1">Alessandro Achille</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhatia_P/0/1/0/all/0/1">Parminder Bhatia</a>, <a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1">Stefano Soatto</a></p>
<p>We investigate compositional structures in data embeddings from pre-trained
vision-language models (VLMs). Traditionally, compositionality has been
associated with algebraic operations on embeddings of words from a pre-existing
vocabulary. In contrast, we seek to approximate representations from an encoder
as combinations of a smaller set of vectors in the embedding space. These
vectors can be seen as "ideal words" for generating concepts directly within
the embedding space of the model. We first present a framework for
understanding compositional structures from a geometric perspective. We then
explain what these compositional structures entail probabilistically in the
case of VLM embeddings, providing intuitions for why they arise in practice.
Finally, we empirically explore these structures in CLIP's embeddings and we
evaluate their usefulness for solving different vision-language tasks such as
classification, debiasing, and retrieval. Our results show that simple linear
algebraic operations on embedding vectors can be used as compositional and
interpretable methods for regulating the behavior of VLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.05352">A Taxonomy of Foundation Model based Systems through the Lens of Software Architecture. (arXiv:2305.05352v5 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1">Qinghua Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1">Liming Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiwei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yue Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1">Zhenchang Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Whittle_J/0/1/0/all/0/1">Jon Whittle</a></p>
<p>The recent release of large language model (LLM) based chatbots, such as
ChatGPT, has attracted huge interest in foundation models. It is widely
believed that foundation models will serve as the fundamental building blocks
for future AI systems. As foundation models are in their early stages, the
design of foundation model based systems has not yet been systematically
explored. There is limited understanding about the impact of introducing
foundation models in software architecture. Therefore, in this paper, we
propose a taxonomy of foundation model based systems, which classifies and
compares the characteristics of foundation models and design options of
foundation model based systems. Our taxonomy comprises three categories: the
pretraining and adaptation of foundation models, the architecture design of
foundation model based systems, and responsible-AI-by-design. This taxonomy can
serve as concrete guidance for making major architectural design decisions when
designing foundation model based systems and highlights trade-offs arising from
design decisions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.11731">Persian Typographical Error Type Detection Using Deep Neural Networks on Algorithmically-Generated Misspellings. (arXiv:2305.11731v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1">Mohammad Dehghani</a>, <a href="http://arxiv.org/find/cs/1/au:+Faili_H/0/1/0/all/0/1">Heshaam Faili</a></p>
<p>Spelling correction is a remarkable challenge in the field of natural
language processing. The objective of spelling correction tasks is to recognize
and rectify spelling errors automatically. The development of applications that
can effectually diagnose and correct Persian spelling and grammatical errors
has become more important in order to improve the quality of Persian text. The
Typographical Error Type Detection in Persian is a relatively understudied
area. Therefore, this paper presents a compelling approach for detecting
typographical errors in Persian texts. Our work includes the presentation of a
publicly available dataset called FarsTypo, which comprises 3.4 million words
arranged in chronological order and tagged with their corresponding
part-of-speech. These words cover a wide range of topics and linguistic styles.
We develop an algorithm designed to apply Persian-specific errors to a scalable
portion of these words, resulting in a parallel dataset of correct and
incorrect words. By leveraging FarsTypo, we establish a strong foundation and
conduct a thorough comparison of various methodologies employing different
architectures. Additionally, we introduce a groundbreaking Deep Sequential
Neural Network that utilizes both word and character embeddings, along with
bidirectional LSTM layers, for token classification aimed at detecting
typographical errors across 51 distinct classes. Our approach is contrasted
with highly advanced industrial systems that, unlike this study, have been
developed using a diverse range of resources. The outcomes of our final method
proved to be highly competitive, achieving an accuracy of 97.62%, precision of
98.83%, recall of 98.61%, and surpassing others in terms of speed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.17147">Heterogeneous Value Alignment Evaluation for Large Language Models. (arXiv:2305.17147v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhaowei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Ceyao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1">Nian Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_S/0/1/0/all/0/1">Siyuan Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rong_Z/0/1/0/all/0/1">Ziqi Rong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1">Song-Chun Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1">Shuguang Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yaodong Yang</a></p>
<p>The emergent capabilities of Large Language Models (LLMs) have made it
crucial to align their values with those of humans. However, current
methodologies typically attempt to assign value as an attribute to LLMs, yet
lack attention to the ability to pursue value and the importance of
transferring heterogeneous values in specific practical applications. In this
paper, we propose a Heterogeneous Value Alignment Evaluation (HVAE) system,
designed to assess the success of aligning LLMs with heterogeneous values.
Specifically, our approach first brings the Social Value Orientation (SVO)
framework from social psychology, which corresponds to how much weight a person
attaches to the welfare of others in relation to their own. We then assign the
LLMs with different social values and measure whether their behaviors align
with the inducing values. We conduct evaluations with new auto-metric
\textit{value rationality} to represent the ability of LLMs to align with
specific values. Evaluating the value rationality of five mainstream LLMs, we
discern a propensity in LLMs towards neutral values over pronounced personal
values. By examining the behavior of these LLMs, we contribute to a deeper
insight into the value alignment of LLMs within a heterogeneous value system.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.17333">Fine-Tuning Language Models with Just Forward Passes. (arXiv:2305.17333v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Malladi_S/0/1/0/all/0/1">Sadhika Malladi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1">Tianyu Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Nichani_E/0/1/0/all/0/1">Eshaan Nichani</a>, <a href="http://arxiv.org/find/cs/1/au:+Damian_A/0/1/0/all/0/1">Alex Damian</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jason D. Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Danqi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1">Sanjeev Arora</a></p>
<p>Fine-tuning language models (LMs) has yielded success on diverse downstream
tasks, but as LMs grow in size, backpropagation requires a prohibitively large
amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients
using only two forward passes but are theorized to be catastrophically slow for
optimizing large models. In this work, we propose a memory-efficient
zerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate
in-place, thereby fine-tuning LMs with the same memory footprint as inference.
For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter
model, whereas fine-tuning with backpropagation can train only a 2.7B LM with
the same budget. We conduct comprehensive experiments across model types
(masked and autoregressive LMs), model scales (up to 66B), and downstream tasks
(classification, multiple-choice, and generation). Our results demonstrate that
(1) MeZO significantly outperforms in-context learning and linear probing; (2)
MeZO achieves comparable performance to fine-tuning with backpropagation across
multiple tasks, with up to 12x memory reduction and up to 2x GPU-hour reduction
in our implementation; (3) MeZO is compatible with both full-parameter and
parameter-efficient tuning techniques such as LoRA and prefix tuning; (4) MeZO
can effectively optimize non-differentiable objectives (e.g., maximizing
accuracy or F1). We support our empirical findings with theoretical insights,
highlighting how adequate pre-training and task prompts enable MeZO to
fine-tune huge models, despite classical ZO analyses suggesting otherwise.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03234">Automated Distractor and Feedback Generation for Math Multiple-choice Questions via In-context Learning. (arXiv:2308.03234v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+McNichols_H/0/1/0/all/0/1">Hunter McNichols</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1">Wanyong Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jaewook Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Scarlatos_A/0/1/0/all/0/1">Alexander Scarlatos</a>, <a href="http://arxiv.org/find/cs/1/au:+Smith_D/0/1/0/all/0/1">Digory Smith</a>, <a href="http://arxiv.org/find/cs/1/au:+Woodhead_S/0/1/0/all/0/1">Simon Woodhead</a>, <a href="http://arxiv.org/find/cs/1/au:+Lan_A/0/1/0/all/0/1">Andrew Lan</a></p>
<p>Multiple-choice questions (MCQs) are ubiquitous in almost all levels of
education since they are easy to administer, grade, and are a reliable form of
assessment. An important aspect of MCQs is the distractors, i.e., incorrect
options that are designed to target specific misconceptions or insufficient
knowledge among students. To date, the task of crafting high-quality
distractors has largely remained a labor-intensive process for teachers and
learning content designers, which has limited scalability. In this work, we
explore the task of automated distractor and corresponding feedback message
generation in math MCQs using large language models. We establish a formulation
of these two tasks and propose a simple, in-context learning-based solution.
Moreover, we propose generative AI-based metrics for evaluating the quality of
the feedback messages. We conduct extensive experiments on these tasks using a
real-world MCQ dataset. Our findings suggest that there is a lot of room for
improvement in automated distractor and feedback generation; based on these
findings, we outline several directions for future work.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03853">CORAL: Expert-Curated medical Oncology Reports to Advance Language Model Inference. (arXiv:2308.03853v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sushil_M/0/1/0/all/0/1">Madhumita Sushil</a>, <a href="http://arxiv.org/find/cs/1/au:+Kennedy_V/0/1/0/all/0/1">Vanessa E. Kennedy</a>, <a href="http://arxiv.org/find/cs/1/au:+Mandair_D/0/1/0/all/0/1">Divneet Mandair</a>, <a href="http://arxiv.org/find/cs/1/au:+Miao_B/0/1/0/all/0/1">Brenda Y. Miao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zack_T/0/1/0/all/0/1">Travis Zack</a>, <a href="http://arxiv.org/find/cs/1/au:+Butte_A/0/1/0/all/0/1">Atul J. Butte</a></p>
<p>Both medical care and observational studies in oncology require a thorough
understanding of a patient's disease progression and treatment history, often
elaborately documented in clinical notes. Despite their vital role, no current
oncology information representation and annotation schema fully encapsulates
the diversity of information recorded within these notes. Although large
language models (LLMs) have recently exhibited impressive performance on
various medical natural language processing tasks, due to the current lack of
comprehensively annotated oncology datasets, an extensive evaluation of LLMs in
extracting and reasoning with the complex rhetoric in oncology notes remains
understudied. We developed a detailed schema for annotating textual oncology
information, encompassing patient characteristics, tumor characteristics,
tests, treatments, and temporality. Using a corpus of 40 de-identified breast
and pancreatic cancer progress notes at University of California, San
Francisco, we applied this schema to assess the zero-shot abilities of three
recent LLMs (GPT-4, GPT-3.5-turbo, and FLAN-UL2) to extract detailed
oncological history from two narrative sections of clinical progress notes. Our
team annotated 9028 entities, 9986 modifiers, and 5312 relationships. The GPT-4
model exhibited overall best performance, with an average BLEU score of 0.73,
an average ROUGE score of 0.72, an exact-match F1-score of 0.51, and an average
accuracy of 68% on complex tasks (expert manual evaluation on subset). Notably,
it was proficient in tumor characteristic and medication extraction, and
demonstrated superior performance in relational inference like adverse event
detection. However, further improvements are needed before using it to reliably
extract important facts from cancer progress notes needed for clinical
research, complex population management, and documenting quality patient care.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.09832">Task Selection and Assignment for Multi-modal Multi-task Dialogue Act Classification with Non-stationary Multi-armed Bandits. (arXiv:2309.09832v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xiangheng He</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Junjie Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1">Bj&#xf6;rn W. Schuller</a></p>
<p>Multi-task learning (MTL) aims to improve the performance of a primary task
by jointly learning with related auxiliary tasks. Traditional MTL methods
select tasks randomly during training. However, both previous studies and our
results suggest that such a random selection of tasks may not be helpful, and
can even be harmful to performance. Therefore, new strategies for task
selection and assignment in MTL need to be explored. This paper studies the
multi-modal, multi-task dialogue act classification task, and proposes a method
for selecting and assigning tasks based on non-stationary multi-armed bandits
(MAB) with discounted Thompson Sampling (TS) using Gaussian priors. Our
experimental results show that in different training stages, different tasks
have different utility. Our proposed method can effectively identify the task
utility, actively avoid useless or harmful tasks, and realise the task
assignment during training. Our proposed method is significantly superior in
terms of UAR and F1 to the single-task and multi-task baselines with p-values &lt;
0.05. Further analysis of experiments indicates that for the dataset with the
data imbalance problem, our proposed method has significantly higher stability
and can obtain consistent and decent performance for minority classes. Our
proposed method is superior to the current state-of-the-art model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.13183">Breaking through Deterministic Barriers: Randomized Pruning Mask Generation and Selection. (arXiv:2310.13183v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jianwei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1">Weizhi Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_Q/0/1/0/all/0/1">Qi Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1">Dongkuan Xu</a></p>
<p>It is widely acknowledged that large and sparse models have higher accuracy
than small and dense models under the same model size constraints. This
motivates us to train a large model and then remove its redundant neurons or
weights by pruning. Most existing works pruned the networks in a deterministic
way, the performance of which solely depends on a single pruning criterion and
thus lacks variety. Instead, in this paper, we propose a model pruning strategy
that first generates several pruning masks in a designed random way.
Subsequently, along with an effective mask-selection rule, the optimal mask is
chosen from the pool of mask candidates. To further enhance efficiency, we
introduce an early mask evaluation strategy, mitigating the overhead associated
with training multiple masks. Our extensive experiments demonstrate that this
approach achieves state-of-the-art performance across eight datasets from GLUE,
particularly excelling at high levels of sparsity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.13191">Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy for Language Models. (arXiv:2310.13191v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jianwei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_Q/0/1/0/all/0/1">Qi Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1">Wei Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1">Dongkuan Xu</a></p>
<p>The pruning objective has recently extended beyond accuracy and sparsity to
robustness in language models. Despite this, existing methods struggle to
enhance robustness against adversarial attacks when continually increasing
model sparsity and require a retraining process. As humans step into the era of
large language models, these issues become increasingly prominent. This paper
proposes that the robustness of language models is proportional to the extent
of pre-trained knowledge they encompass. Accordingly, we introduce a
post-training pruning strategy designed to faithfully replicate the embedding
space and feature space of dense language models, aiming to conserve more
pre-trained knowledge during the pruning process. In this setup, each layer's
reconstruction error not only originates from itself but also includes
cumulative error from preceding layers, followed by an adaptive rectification.
Compared to other state-of-art baselines, our approach demonstrates a superior
balance between accuracy, sparsity, robustness, and pruning cost with BERT on
datasets SST2, IMDB, and AGNews, marking a significant stride towards robust
pruning in language models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00541">An Embedded Diachronic Sense Change Model with a Case Study from Ancient Greek. (arXiv:2311.00541v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zafar_S/0/1/0/all/0/1">Schyan Zafar</a>, <a href="http://arxiv.org/find/cs/1/au:+Nicholls_G/0/1/0/all/0/1">Geoff K. Nicholls</a></p>
<p>Word meanings change over time, and word senses evolve, emerge or die out in
the process. For ancient languages, where the corpora are often small and
sparse, modelling such changes accurately proves challenging, and quantifying
uncertainty in sense-change estimates consequently becomes important. GASC
(Genre-Aware Semantic Change) and DiSC (Diachronic Sense Change) are existing
generative models that have been used to analyse sense change for target words
from an ancient Greek text corpus, using unsupervised learning without the help
of any pre-training. These models represent the senses of a given target word
such as "kosmos" (meaning decoration, order or world) as distributions over
context words, and sense prevalence as a distribution over senses. The models
are fitted using Markov Chain Monte Carlo (MCMC) methods to measure temporal
changes in these representations. In this paper, we introduce EDiSC, an
Embedded DiSC model, which combines word embeddings with DiSC to provide
superior model performance. We show empirically that EDiSC offers improved
predictive accuracy, ground-truth recovery and uncertainty quantification, as
well as better sampling efficiency and scalability properties with MCMC
methods. We also discuss the challenges of fitting these models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02790">CausalCite: A Causal Formulation of Paper Citations. (arXiv:2311.02790v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kumar_I/0/1/0/all/0/1">Ishan Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1">Zhijing Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Mokhtarian_E/0/1/0/all/0/1">Ehsan Mokhtarian</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1">Siyuan Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1">Mrinmaya Sachan</a>, <a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1">Bernhard Sch&#xf6;lkopf</a></p>
<p>Evaluating the significance of a paper is pivotal yet challenging for the
scientific community. While the citation count is the most commonly used proxy
for this purpose, they are widely criticized for failing to accurately reflect
a paper's true impact. In this work, we propose a causal inference method,
TextMatch, which adapts the traditional matching framework to high-dimensional
text embeddings. Specifically, we encode each paper using the text embeddings
by large language models (LLMs), extract similar samples by cosine similarity,
and synthesize a counterfactual sample by the weighted average of similar
papers according to their similarity values. We apply the resulting metric,
called CausalCite, as a causal formulation of paper citations. We show its
effectiveness on various criteria, such as high correlation with paper impact
as reported by scientific experts on a previous dataset of 1K papers,
(test-of-time) awards for past papers, and its stability across various
sub-fields of AI. We also provide a set of findings that can serve as suggested
ways for future researchers to use our metric for a better understanding of a
paper's quality. Our code and data are at
https://github.com/causalNLP/causal-cite.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03220">ALYMPICS: Language Agents Meet Game Theory -- Exploring Strategic Decision-Making with AI Agents. (arXiv:2311.03220v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1">Shaoguang Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1">Yuzhe Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1">Yan Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1">Wenshan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Fengyi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1">Tao Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1">Furu Wei</a></p>
<p>This paper introduces Alympics (Olympics for Agents), a systematic simulation
framework utilizing Large Language Model (LLM) agents for game theory research.
Alympics creates a versatile platform for studying complex game theory
problems, bridging the gap between theoretical game theory and empirical
investigations by providing a controlled environment for simulating human-like
strategic interactions with LLM agents. In our pilot case study, the "Water
Allocation Challenge," we explore Alympics through a challenging strategic game
focused on the multi-round auction on scarce survival resources. This study
demonstrates the framework's ability to qualitatively and quantitatively
analyze game determinants, strategies, and outcomes. Additionally, we conduct a
comprehensive human assessment and an in-depth evaluation of LLM agents in
strategic decision-making scenarios. Our findings not only expand the
understanding of LLM agents' proficiency in emulating human strategic behavior
but also highlight their potential in advancing game theory knowledge, thereby
enriching our understanding of both game theory and empowering further research
into strategic decision-making domains with LLM agents. Codes, prompts, and all
related resources are available at https://github.com/microsoft/Alympics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07102">Fovea Transformer: Efficient Long-Context Modeling with Structured Fine-to-Coarse Attention. (arXiv:2311.07102v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1">Ziwei He</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1">Jian Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1">Le Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Leng_J/0/1/0/all/0/1">Jingwen Leng</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1">Bo Jiang</a></p>
<p>The quadratic complexity of self-attention in Transformers has hindered the
processing of long text. To alleviate this problem, previous works have
proposed to sparsify the attention matrix, taking advantage of the observation
that crucial information about a token can be derived from its neighbors. These
methods typically combine one or another form of local attention and global
attention. Such combinations introduce abrupt changes in contextual granularity
when going from local to global, which may be undesirable. We believe that a
smoother transition could potentially enhance model's ability to capture
long-context dependencies. In this study, we introduce Fovea Transformer, a
long-context focused transformer that addresses the challenges of capturing
global dependencies while maintaining computational efficiency. To achieve
this, we construct a multi-scale tree from the input sequence, and use
representations of context tokens with a progressively coarser granularity in
the tree, as their distance to the query token increases. We evaluate our model
on three long-context summarization tasks\footnote{Our code is publicly
available at: \textit{https://github.com/ZiweiHe/Fovea-Transformer}}. It
achieves state-of-the-art performance on two of them, and competitive results
on the third with mixed improvement and setback of the evaluation metrics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05720">Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer Inputs of Language Models in Federated Learning. (arXiv:2312.05720v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jianwei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Sheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_Q/0/1/0/all/0/1">Qi Lei</a></p>
<p>Federated learning (FL) emphasizes decentralized training by storing data
locally and sending only model updates, underlining user privacy. Recently, a
line of works on privacy attacks impairs user privacy by extracting sensitive
training text from language models in the context of FL. Yet, these attack
techniques face distinct hurdles: some work chiefly with limited batch sizes
(e.g., batch size of 1), and others are easily detectable. This paper
introduces an innovative approach that is challenging to detect, significantly
enhancing the recovery rate of text in various batch-size settings. Building on
fundamental gradient matching and domain prior knowledge, we enhance the attack
by recovering the input of the Pooler layer of language models, which enables
us to provide additional supervised signals at the feature level. Unlike
gradient data, these signals do not average across sentences and tokens,
thereby offering more nuanced and effective insights. We benchmark our method
using text classification tasks on datasets such as CoLA, SST-2, and Rotten
Tomatoes. Across different batch sizes and models, our approach consistently
outperforms previous state-of-the-art results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06499">TaCo: Targeted Concept Removal in Output Embeddings for NLP via Information Theory and Explainability. (arXiv:2312.06499v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jourdan_F/0/1/0/all/0/1">Fanny Jourdan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bethune_L/0/1/0/all/0/1">Louis B&#xe9;thune</a>, <a href="http://arxiv.org/find/cs/1/au:+Picard_A/0/1/0/all/0/1">Agustin Picard</a>, <a href="http://arxiv.org/find/cs/1/au:+Risser_L/0/1/0/all/0/1">Laurent Risser</a>, <a href="http://arxiv.org/find/cs/1/au:+Asher_N/0/1/0/all/0/1">Nicholas Asher</a></p>
<p>The fairness of Natural Language Processing (NLP) models has emerged as a
crucial concern. Information theory indicates that to achieve fairness, a model
should not be able to predict sensitive variables, such as gender, ethnicity,
and age. However, information related to these variables often appears
implicitly in language, posing a challenge in identifying and mitigating biases
effectively. To tackle this issue, we present a novel approach that operates at
the embedding level of an NLP model, independent of the specific architecture.
Our method leverages insights from recent advances in XAI techniques and
employs an embedding transformation to eliminate implicit information from a
selected variable. By directly manipulating the embeddings in the final layer,
our approach enables a seamless integration into existing models without
requiring significant modifications or retraining. In evaluation, we show that
the proposed post-hoc approach significantly reduces gender-related
associations in NLP models while preserving the overall performance and
functionality of the models. An implementation of our method is available:
https://github.com/fanny-jourdan/TaCo
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08079">Extending Whisper with prompt tuning to target-speaker ASR. (arXiv:2312.08079v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1">Hao Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1">Zhiyuan Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_M/0/1/0/all/0/1">Mingjie Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jing Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Ju Liu</a></p>
<p>Target-speaker automatic speech recognition (ASR) aims to transcribe the
desired speech of a target speaker from multi-talker overlapped utterances.
Most of the existing target-speaker ASR (TS-ASR) methods involve either
training from scratch or fully fine-tuning a pre-trained model, leading to
significant training costs and becoming inapplicable to large foundation
models. This work leverages prompt tuning, a parameter-efficient fine-tuning
approach, to extend Whisper, a large-scale single-talker ASR model, to TS-ASR.
Variants of prompt tuning approaches along with their configurations are
explored and optimized for TS-ASR.Experimental results show that prompt tuning
can achieve performance comparable to state-of-the-art full training approaches
while only requiring about 1\% of task-specific model parameters. Notably, the
original Whisper's features, such as inverse text normalization and timestamp
tagging, are retained in target-speaker ASR, keeping the generated
transcriptions natural and informative.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.09075">Towards Verifiable Text Generation with Evolving Memory and Self-Reflection. (arXiv:2312.09075v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Hao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1">Hengyi Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1">Yingyan Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1">Xiaochi Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuaiqiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1">Dawei Yin</a></p>
<p>Despite the remarkable ability of large language models (LLMs) in language
comprehension and generation, they often suffer from producing factually
incorrect information, also known as hallucination. A promising solution to
this issue is verifiable text generation, which prompts LLMs to generate
content with citations for accuracy verification. However, verifiable text
generation is non-trivial due to the focus-shifting phenomenon, the intricate
reasoning needed to align the claim with correct citations, and the dilemma
between the precision and breadth of retrieved documents. In this paper, we
present VTG, an innovative framework for Verifiable Text Generation with
evolving memory and self-reflection. VTG introduces evolving long short-term
memory to retain both valuable documents and recent documents. A two-tier
verifier equipped with an evidence finder is proposed to rethink and reflect on
the relationship between the claim and citations. Furthermore, active retrieval
and diverse query generation are utilized to enhance both the precision and
breadth of the retrieved documents. We conduct extensive experiments on five
datasets across three knowledge-intensive tasks and the results reveal that VTG
significantly outperforms baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10813">Re-parameterized Low-rank Prompt: Generalize a Vision-Language Model within 0.5K Parameters. (arXiv:2312.10813v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hao_T/0/1/0/all/0/1">Tianxiang Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1">Mengyao Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hui Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1">Sicheng Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Jungong Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1">Guiguang Ding</a></p>
<p>With the development of large pre-trained vision-language models, how to
effectively transfer the knowledge of such foundational models to downstream
tasks becomes a hot topic, especially in a data-deficient scenario. Recently,
prompt tuning has become a popular solution. When adapting the vision-language
models, researchers freeze the parameters in the backbone and only design and
tune the prompts. On the one hand, the delicate design of prompt tuning
exhibits strong performance. On the other hand, complicated structures and
update rules largely increase the computation and storage cost. Motivated by
the observation that the evolution pattern of the generalization capability in
visual-language models aligns harmoniously with the trend of rank variations in
the prompt matrix during adaptation, we design a new type of prompt,
Re-parameterized Low-rank Prompt (RLP), for both efficient and effective
adaptation. Our method could largely reduce the number of tunable parameters
and storage space, which is quite beneficial in resource-limited scenarios.
Extensive experiments further demonstrate the superiority of RLP. In
particular, RLP shows comparable or even stronger performance than the latest
state-of-the-art methods with an extremely small number of parameters. On a
series of tasks over 11 datasets, RLP significantly increases the average
downstream accuracy of classic prompt tuning by up to 5.25% using merely 0.5K
parameters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12269">Automated speech audiometry: Can it work using open-source pre-trained Kaldi-NL automatic speech recognition?. (arXiv:2312.12269v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Araiza_Illan_G/0/1/0/all/0/1">Gloria Araiza-Illan</a>, <a href="http://arxiv.org/find/cs/1/au:+Meyer_L/0/1/0/all/0/1">Luke Meyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Truong_K/0/1/0/all/0/1">Khiet P. Truong</a>, <a href="http://arxiv.org/find/cs/1/au:+Baskent_D/0/1/0/all/0/1">Deniz Baskent</a></p>
<p>A practical speech audiometry tool is the digits-in-noise (DIN) test for
hearing screening of populations of varying ages and hearing status. The test
is usually conducted by a human supervisor (e.g., clinician), who scores the
responses spoken by the listener, or online, where a software scores the
responses entered by the listener. The test has 24 digit-triplets presented in
an adaptive staircase procedure, resulting in a speech reception threshold
(SRT). We propose an alternative automated DIN test setup that can evaluate
spoken responses whilst conducted without a human supervisor, using the
open-source automatic speech recognition toolkit, Kaldi-NL. Thirty
self-reported normal-hearing Dutch adults (19-64 years) completed one
DIN+Kaldi-NL test. Their spoken responses were recorded, and used for
evaluating the transcript of decoded responses by Kaldi-NL. Study 1 evaluated
the Kaldi-NL performance through its word error rate (WER), percentage of
summed decoding errors regarding only digits found in the transcript compared
to the total number of digits present in the spoken responses. Average WER
across participants was 5.0% (range 0 - 48%, SD = 8.8%), with average decoding
errors in three triplets per participant. Study 2 analysed the effect that
triplets with decoding errors from Kaldi-NL had on the DIN test output (SRT),
using bootstrapping simulations. Previous research indicated 0.70 dB as the
typical within-subject SRT variability for normal-hearing adults. Study 2
showed that up to four triplets with decoding errors produce SRT variations
within this range, suggesting that our proposed setup could be feasible for
clinical applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14187">WaveCoder: Widespread And Versatile Enhanced Instruction Tuning with Refined Data Generation. (arXiv:2312.14187v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Zhaojian Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_N/0/1/0/all/0/1">Ning Shang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yangyu Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Can Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yishujie Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1">Wenxiang Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_Q/0/1/0/all/0/1">Qiufeng Yin</a></p>
<p>Recent work demonstrates that, after being fine-tuned on a high-quality
instruction dataset, the resulting model can obtain impressive capabilities to
address a wide range of tasks. However, existing methods for instruction data
generation often produce duplicate data and are not controllable enough on data
quality. In this paper, we extend the generalization of instruction tuning by
classifying the instruction data to 4 code-related tasks and propose a
LLM-based Generator-Discriminator data process framework to generate diverse,
high-quality instruction data from open source code. Hence, we introduce
CodeOcean, a dataset comprising 20,000 instruction instances across 4 universal
code-related tasks,which is aimed at augmenting the effectiveness of
instruction tuning and improving the generalization ability of fine-tuned
model. Subsequently, we present WaveCoder, a fine-tuned Code LLM with
Widespread And Versatile Enhanced instruction tuning. This model is
specifically designed for enhancing instruction tuning of Code Language Models
(LLMs). Our experiments demonstrate that Wavecoder models outperform other
open-source models in terms of generalization ability across different
code-related tasks at the same level of fine-tuning scale. Moreover, Wavecoder
exhibits high efficiency in previous code generation tasks. This paper thus
offers a significant contribution to the field of instruction data generation
and fine-tuning models, providing new insights and tools for enhancing
performance in code-related tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16148">The Media Bias Taxonomy: A Systematic Literature Review on the Forms and Automated Detection of Media Bias. (arXiv:2312.16148v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Spinde_T/0/1/0/all/0/1">Timo Spinde</a>, <a href="http://arxiv.org/find/cs/1/au:+Hinterreiter_S/0/1/0/all/0/1">Smi Hinterreiter</a>, <a href="http://arxiv.org/find/cs/1/au:+Haak_F/0/1/0/all/0/1">Fabian Haak</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruas_T/0/1/0/all/0/1">Terry Ruas</a>, <a href="http://arxiv.org/find/cs/1/au:+Giese_H/0/1/0/all/0/1">Helge Giese</a>, <a href="http://arxiv.org/find/cs/1/au:+Meuschke_N/0/1/0/all/0/1">Norman Meuschke</a>, <a href="http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1">Bela Gipp</a></p>
<p>The way the media presents events can significantly affect public perception,
which in turn can alter people's beliefs and views. Media bias describes a
one-sided or polarizing perspective on a topic. This article summarizes the
research on computational methods to detect media bias by systematically
reviewing 3140 research papers published between 2019 and 2022. To structure
our review and support a mutual understanding of bias across research domains,
we introduce the Media Bias Taxonomy, which provides a coherent overview of the
current state of research on media bias from different perspectives. We show
that media bias detection is a highly active research field, in which
transformer-based classification approaches have led to significant
improvements in recent years. These improvements include higher classification
accuracy and the ability to detect more fine-granular types of bias. However,
we have identified a lack of interdisciplinarity in existing projects, and a
need for more awareness of the various types of media bias to support
methodologically thorough performance evaluations of media bias detection
systems. Concluding from our analysis, we see the integration of recent machine
learning advancements with reliable and diverse bias assessment strategies from
other research areas as the most promising area for future research
contributions in the field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.04621">DebugBench: Evaluating Debugging Capability of Large Language Models. (arXiv:2401.04621v2 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tian_R/0/1/0/all/0/1">Runchu Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1">Yining Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1">Yujia Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Cong_X/0/1/0/all/0/1">Xin Cong</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yankai Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1">Yinxu Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yesai Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhiyuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1">Maosong Sun</a></p>
<p>Large Language Models (LLMs) have demonstrated exceptional coding capability.
However, as another critical component of programming proficiency, the
debugging capability of LLMs remains relatively unexplored. Previous
evaluations of LLMs' debugging ability are significantly limited by the risk of
data leakage, the scale of the dataset, and the variety of tested bugs. To
overcome these deficiencies, we introduce `DebugBench', an LLM debugging
benchmark consisting of 4,253 instances. It covers four major bug categories
and 18 minor types in C++, Java, and Python. To construct DebugBench, we
collect code snippets from the LeetCode community, implant bugs into source
data with GPT-4, and assure rigorous quality checks. We evaluate two commercial
and three open-source models in a zero-shot scenario. We find that (1) while
closed-source models like GPT-4 exhibit inferior debugging performance compared
to humans, open-source models such as Code Llama fail to attain any pass rate
scores; (2) the complexity of debugging notably fluctuates depending on the bug
category; (3) incorporating runtime feedback has a clear impact on debugging
performance which is not always helpful. As an extension, we also compare LLM
debugging and code generation, revealing a strong correlation between them for
closed-source models. These findings will benefit the development of LLMs in
debugging.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.04679">RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation. (arXiv:2401.04679v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nikdan_M/0/1/0/all/0/1">Mahdi Nikdan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tabesh_S/0/1/0/all/0/1">Soroush Tabesh</a>, <a href="http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1">Dan Alistarh</a></p>
<p>We investigate parameter-efficient fine-tuning (PEFT) methods that can
provide good accuracy under limited computational and memory budgets in the
context of large language models (LLMs). We present a new PEFT method called
Robust Adaptation (RoSA) inspired by robust principal component analysis (PCA)
that jointly trains $\textit{low-rank}$ and $\textit{highly-sparse}$ components
on top of a set of fixed pretrained weights to efficiently approximate the
performance of a full-fine-tuning (FFT) solution. Across a series of
challenging generative tasks such as grade-school math and SQL query
generation, which require fine-tuning for good performance, we show that RoSA
outperforms both LoRA and pure sparse fine-tuning, at the same parameter
budget. We provide system support for RoSA to complement the training
algorithm, specifically in the form of sparse GPU kernels which enable memory-
and computationally-efficient training. Our code will be made available at
$\href{https://github.com/IST-DASLab/RoSA}{\text{our github page}}$.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05254">Language-based Valence and Arousal Expressions between the United States and China: a Cross-Cultural Examination. (arXiv:2401.05254v2 [cs.CY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1">Young-Min Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Pang_D/0/1/0/all/0/1">Dandan Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Thapa_S/0/1/0/all/0/1">Stuti Thapa</a>, <a href="http://arxiv.org/find/cs/1/au:+Sherman_G/0/1/0/all/0/1">Garrick Sherman</a>, <a href="http://arxiv.org/find/cs/1/au:+Ungar_L/0/1/0/all/0/1">Lyle Ungar</a>, <a href="http://arxiv.org/find/cs/1/au:+Tay_L/0/1/0/all/0/1">Louis Tay</a>, <a href="http://arxiv.org/find/cs/1/au:+Guntuku_S/0/1/0/all/0/1">Sharath Chandra Guntuku</a></p>
<p>Although affective expressions of individuals have been extensively studied
using social media, research has primarily focused on the Western context.
There are substantial differences among cultures that contribute to their
affective expressions. This paper examines the differences between Twitter (X)
in the United States and Sina Weibo posts in China on two primary dimensions of
affect - valence and arousal. We study the difference in the functional
relationship between arousal and valence (so-called V-shaped) among individuals
in the US and China and explore the associated content differences.
Furthermore, we correlate word usage and topics in both platforms to interpret
their differences. We observe that for Twitter users, the variation in
emotional intensity is less distinct between negative and positive emotions
compared to Weibo users, and there is a sharper escalation in arousal
corresponding with heightened emotions. From language features, we discover
that affective expressions are associated with personal life and feelings on
Twitter, while on Weibo such discussions are about socio-political topics in
the society. These results suggest a West-East difference in the V-shaped
relationship between valence and arousal of affective expressions on social
media influenced by content differences. Our findings have implications for
applications and theories related to cultural differences in affective
expressions.
</p>
</p>
</div>

    </div>
    </body>
    