<!DOCTYPE html>
<html>
<head>
<title>2023-11-28-cs-cv</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2311.13603">Cross-layer scheme for low latency multiple description video streaming over Vehicular Ad-hoc NETworks (VANETs). (arXiv:2311.13603v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Labiod_M/0/1/0/all/0/1">Mohamed Aymen Labiod</a>, <a href="http://arxiv.org/find/cs/1/au:+Gharbi_M/0/1/0/all/0/1">Mohamed Gharbi</a>, <a href="http://arxiv.org/find/cs/1/au:+Coudoux_F/0/1/0/all/0/1">Francois-Xavier Coudoux</a>, <a href="http://arxiv.org/find/cs/1/au:+Corlay_P/0/1/0/all/0/1">Patrick Corlay</a>, <a href="http://arxiv.org/find/cs/1/au:+Doghmane_N/0/1/0/all/0/1">Noureddine Doghmane</a></p>
<p>There is nowadays a growing demand in vehicular communications for real-time
applications requiring video assistance. The new state-of-the-art
high-efficiency video coding (HEVC) standard is very promising for real-time
video streaming. It offers high coding efficiency, as well as dedicated low
delay coding structures. Among these, the all intra (AI) coding structure
guarantees minimal coding time at the expense of higher video bitrates, which
therefore penalizes transmission performances. In this work, we propose an
original cross-layer system in order to enhance received video quality in
vehicular communications. The system is low complex and relies on a multiple
description coding (MDC) approach. It is based on an adaptive mapping mechanism
applied at the IEEE 802.11p standard medium access control (MAC) layer.
Simulation results in a realistic vehicular environment demonstrate that for
low delay video communications, the proposed method provides significant video
quality improvements on the receiver side.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13608">Breathing Life Into Sketches Using Text-to-Video Priors. (arXiv:2311.13608v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gal_R/0/1/0/all/0/1">Rinon Gal</a>, <a href="http://arxiv.org/find/cs/1/au:+Vinker_Y/0/1/0/all/0/1">Yael Vinker</a>, <a href="http://arxiv.org/find/cs/1/au:+Alaluf_Y/0/1/0/all/0/1">Yuval Alaluf</a>, <a href="http://arxiv.org/find/cs/1/au:+Bermano_A/0/1/0/all/0/1">Amit H. Bermano</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1">Daniel Cohen-Or</a>, <a href="http://arxiv.org/find/cs/1/au:+Shamir_A/0/1/0/all/0/1">Ariel Shamir</a>, <a href="http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1">Gal Chechik</a></p>
<p>A sketch is one of the most intuitive and versatile tools humans use to
convey their ideas visually. An animated sketch opens another dimension to the
expression of ideas and is widely used by designers for a variety of purposes.
Animating sketches is a laborious process, requiring extensive experience and
professional design skills. In this work, we present a method that
automatically adds motion to a single-subject sketch (hence, "breathing life
into it"), merely by providing a text prompt indicating the desired motion. The
output is a short animation provided in vector representation, which can be
easily edited. Our method does not require extensive training, but instead
leverages the motion prior of a large pretrained text-to-video diffusion model
using a score-distillation loss to guide the placement of strokes. To promote
natural and smooth motion and to better preserve the sketch's appearance, we
model the learned motion through two components. The first governs small local
deformations and the second controls global affine transformations.
Surprisingly, we find that even models that struggle to generate sketch videos
on their own can still serve as a useful backbone for animating abstract
representations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13610">TRIDENT: The Nonlinear Trilogy for Implicit Neural Representations. (arXiv:2311.13610v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1">Zhenda Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1">Yanqi Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_R/0/1/0/all/0/1">Raymond H. Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1">Pietro Li&#xf2;</a>, <a href="http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1">Carola-Bibiane Sch&#xf6;nlieb</a>, <a href="http://arxiv.org/find/cs/1/au:+Aviles_Rivero_A/0/1/0/all/0/1">Angelica I Aviles-Rivero</a></p>
<p>Implicit neural representations (INRs) have garnered significant interest
recently for their ability to model complex, high-dimensional data without
explicit parameterisation. In this work, we introduce TRIDENT, a novel function
for implicit neural representations characterised by a trilogy of
nonlinearities. Firstly, it is designed to represent high-order features
through order compactness. Secondly, TRIDENT efficiently captures frequency
information, a feature called frequency compactness. Thirdly, it has the
capability to represent signals or images such that most of its energy is
concentrated in a limited spatial region, denoting spatial compactness. We
demonstrated through extensive experiments on various inverse problems that our
proposed function outperforms existing implicit neural representation
functions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13612">Descriptor and Word Soups: Overcoming the Parameter Efficiency Accuracy Tradeoff for Out-of-Distribution Few-shot Learning. (arXiv:2311.13612v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liao_C/0/1/0/all/0/1">Christopher Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsiligkaridis_T/0/1/0/all/0/1">Theodoros Tsiligkaridis</a>, <a href="http://arxiv.org/find/cs/1/au:+Kulis_B/0/1/0/all/0/1">Brian Kulis</a></p>
<p>Over the past year, a large body of multimodal research has emerged around
zero-shot evaluation using GPT descriptors. These studies boost the zero-shot
accuracy of pretrained VL models with an ensemble of label-specific text
generated by GPT. A recent study, WaffleCLIP, demonstrated that similar
zero-shot accuracy can be achieved with an ensemble of random descriptors.
However, both zero-shot methods are un-trainable and consequently sub-optimal
when some few-shot out-of-distribution (OOD) training data is available.
Inspired by these prior works, we present two more flexible methods called
descriptor and word soups, which do not require an LLM at test time and can
leverage training data to increase OOD target accuracy. Descriptor soup
greedily selects a small set of textual descriptors using generic few-shot
training data, then calculates robust class embeddings using the selected
descriptors. Word soup greedily assembles a chain of words in a similar manner.
Compared to existing few-shot soft prompt tuning methods, word soup requires
fewer parameters by construction and less GPU memory, since it does not require
backpropagation. Both soups outperform current published few-shot methods, even
when combined with SoTA zero-shot methods, on cross-dataset and domain
generalization benchmarks. Compared with SoTA prompt and descriptor ensembling
methods, such as ProDA and WaffleCLIP, word soup achieves higher OOD accuracy
with fewer ensemble members. Please checkout our code:
github.com/Chris210634/word_soups
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13613">Spanning Training Progress: Temporal Dual-Depth Scoring (TDDS) for Enhanced Dataset Pruning. (arXiv:2311.13613v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1">Jiawei Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yunsong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1">Weiying Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Joey Tianyi Zhou</a></p>
<p>Dataset pruning aims to construct a coreset capable of achieving performance
comparable to the original, full dataset. Most existing dataset pruning methods
rely on snapshot-based criteria to identify representative samples, often
resulting in poor generalization across various pruning and cross-architecture
scenarios. Recent studies have addressed this issue by expanding the scope of
training dynamics considered, including factors such as forgetting event and
probability change, typically using an averaging approach. However, these works
struggle to integrate a broader range of training dynamics without overlooking
well-generalized samples, which may not be sufficiently highlighted in an
averaging manner. In this study, we propose a novel dataset pruning method
termed as Temporal Dual-Depth Scoring (TDDS), to tackle this problem. TDDS
utilizes a dual-depth strategy to achieve a balance between incorporating
extensive training dynamics and identifying representative samples for dataset
pruning. In the first depth, we estimate the series of each sample's individual
contributions spanning the training progress, ensuring comprehensive
integration of training dynamics. In the second depth, we focus on the
variability of the sample-wise contributions identified in the first depth to
highlight well-generalized samples. Extensive experiments conducted on CIFAR
and ImageNet datasets verify the superiority of TDDS over previous SOTA
methods. Specifically on CIFAR-100, our method achieves 54.51% accuracy with
only 10% training data, surpassing random selection by 7.83% and other
comparison methods by at least 12.69%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13614">HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction Data. (arXiv:2311.13614v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1">Qifan Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Juncheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1">Longhui Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1">Liang Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1">Wentao Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1">Bosheng Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1">Siliang Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1">Qi Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1">Yueting Zhuang</a></p>
<p>Multi-modal Large Language Models (MLLMs) tuned on machine-generated
instruction-following data have demonstrated remarkable performance in various
multi-modal understanding and generation tasks. However, the hallucinations
inherent in machine-generated data, which could lead to hallucinatory outputs
in MLLMs, remain under-explored. This work aims to investigate various
hallucinations (i.e., object, relation, attribute hallucinations) and mitigate
those hallucinatory toxicities in large-scale machine-generated visual
instruction datasets. Drawing on the human ability to identify factual errors,
we present a novel hallucination detection and elimination framework,
HalluciDoctor, based on the cross-checking paradigm. We use our framework to
identify and eliminate hallucinations in the training data automatically.
Interestingly, HalluciDoctor also indicates that spurious correlations arising
from long-tail object co-occurrences contribute to hallucinations. Based on
that, we execute counterfactual visual instruction expansion to balance data
distribution, thereby enhancing MLLMs' resistance to hallucinations.
Comprehensive experiments on hallucination evaluation benchmarks show that our
method successfully mitigates 44.6% hallucinations relatively and maintains
competitive performance compared to LLaVA.The source code will be released at
\url{https://github.com/Yuqifan1117/HalluciDoctor}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13615">HEViTPose: High-Efficiency Vision Transformer for Human Pose Estimation. (arXiv:2311.13615v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1">Chengpeng Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_G/0/1/0/all/0/1">Guangxing Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chunyu Li</a></p>
<p>Human pose estimation in complicated situations has always been a challenging
task. Many Transformer-based pose networks have been proposed recently,
achieving encouraging progress in improving performance. However, the
remarkable performance of pose networks is always accompanied by heavy
computation costs and large network scale. In order to deal with this problem,
this paper proposes a High-Efficiency Vision Transformer for Human Pose
Estimation (HEViTPose). In HEViTPose, a Cascaded Group Spatial Reduction
Multi-Head Attention Module (CGSR-MHA) is proposed, which reduces the
computational cost through feature grouping and spatial degradation mechanisms,
while preserving feature diversity through multiple low-dimensional attention
heads. Moreover, a concept of Patch Embedded Overlap Width (PEOW) is defined to
help understand the relationship between the amount of overlap and local
continuity. By optimising PEOW, our model gains improvements in performance,
parameters and GFLOPs.
</p>
<p>Comprehensive experiments on two benchmark datasets (MPII and COCO)
demonstrate that the small and large HEViTPose models are on par with
state-of-the-art models while being more lightweight. Specifically, HEViTPose-B
achieves 90.7 PCK@0.5 on the MPII test set and 72.6 AP on the COCO test-dev2017
set. Compared with HRNet-W32 and Swin-S, our HEViTPose-B significantly reducing
Params ($\downarrow$62.1%,$\downarrow$80.4%,) and GFLOPs
($\downarrow$43.4%,$\downarrow$63.8%,). Code and models are available at
\url{here}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13616">Online Video Quality Enhancement with Spatial-Temporal Look-up Tables. (arXiv:2311.13616v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Qu_Z/0/1/0/all/0/1">Zefan Qu</a>, <a href="http://arxiv.org/find/eess/1/au:+Jiang_X/0/1/0/all/0/1">Xinyang Jiang</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1">Yifan Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_D/0/1/0/all/0/1">Dongsheng Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhao_C/0/1/0/all/0/1">Cairong Zhao</a></p>
<p>Low latency rates are crucial for online video-based applications, such as
video conferencing and cloud gaming, which make improving video quality in
online scenarios increasingly important. However, existing quality enhancement
methods are limited by slow inference speed and the requirement for temporal
information contained in future frames, making it challenging to deploy them
directly in online tasks. In this paper, we propose a novel method, STLVQE,
specifically designed to address the rarely studied online video quality
enhancement (Online-VQE) problem. Our STLVQE designs a new VQE framework which
contains a Module-Agnostic Feature Extractor that greatly reduces the redundant
computations and redesign the propagation, alignment, and enhancement module of
the network. A Spatial-Temporal Look-up Tables (STL) is proposed, which
extracts spatial-temporal information in videos while saving substantial
inference time. To the best of our knowledge, we are the first to exploit the
LUT structure to extract temporal information in video tasks. Extensive
experiments on the MFQE 2.0 dataset demonstrate that our STLVQE achieves a
satisfactory performance-speed trade-off.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13617">Boosting3D: High-Fidelity Image-to-3D by Boosting 2D Diffusion Prior to 3D Prior with Progressive Learning. (arXiv:2311.13617v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1">Kai Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jinlin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_M/0/1/0/all/0/1">Mengyang Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_M/0/1/0/all/0/1">Miaomiao Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xuansong Xie</a></p>
<p>We present Boosting3D, a multi-stage single image-to-3D generation method
that can robustly generate reasonable 3D objects in different data domains. The
point of this work is to solve the view consistency problem in single
image-guided 3D generation by modeling a reasonable geometric structure. For
this purpose, we propose to utilize better 3D prior to training the NeRF. More
specifically, we train an object-level LoRA for the target object using
original image and the rendering output of NeRF. And then we train the LoRA and
NeRF using a progressive training strategy. The LoRA and NeRF will boost each
other while training. After the progressive training, the LoRA learns the 3D
information of the generated object and eventually turns to an object-level 3D
prior. In the final stage, we extract the mesh from the trained NeRF and use
the trained LoRA to optimize the structure and appearance of the mesh. The
experiments demonstrate the effectiveness of the proposed method. Boosting3D
learns object-specific 3D prior which is beyond the ability of pre-trained
diffusion priors and achieves state-of-the-art performance in the single
image-to-3d generation task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13619">Steal My Artworks for Fine-tuning? A Watermarking Framework for Detecting Art Theft Mimicry in Text-to-Image Models. (arXiv:2311.13619v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1">Ge Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Junqiang Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Manman Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1">Zhenxing Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Sheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xinpeng Zhang</a></p>
<p>The advancement in text-to-image models has led to astonishing artistic
performances. However, several studios and websites illegally fine-tune these
models using artists' artworks to mimic their styles for profit, which violates
the copyrights of artists and diminishes their motivation to produce original
works. Currently, there is a notable lack of research focusing on this issue.
In this paper, we propose a novel watermarking framework that detects mimicry
in text-to-image models through fine-tuning. This framework embeds subtle
watermarks into digital artworks to protect their copyrights while still
preserving the artist's visual expression. If someone takes watermarked
artworks as training data to mimic an artist's style, these watermarks can
serve as detectable indicators. By analyzing the distribution of these
watermarks in a series of generated images, acts of fine-tuning mimicry using
stolen victim data will be exposed. In various fine-tune scenarios and against
watermark attack methods, our research confirms that analyzing the distribution
of watermarks in artificially generated images reliably detects unauthorized
mimicry.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13620">The Challenges of Image Generation Models in Generating Multi-Component Images. (arXiv:2311.13620v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Foong_T/0/1/0/all/0/1">Tham Yik Foong</a>, <a href="http://arxiv.org/find/cs/1/au:+Kotyan_S/0/1/0/all/0/1">Shashank Kotyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_P/0/1/0/all/0/1">Po Yuan Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Vargas_D/0/1/0/all/0/1">Danilo Vasconcellos Vargas</a></p>
<p>Recent advances in text-to-image generators have led to substantial
capabilities in image generation. However, the complexity of prompts acts as a
bottleneck in the quality of images generated. A particular under-explored
facet is the ability of generative models to create high-quality images
comprising multiple components given as a prior. In this paper, we propose and
validate a metric called Components Inclusion Score (CIS) to evaluate the
extent to which a model can correctly generate multiple components. Our results
reveal that the evaluated models struggle to incorporate all the visual
elements from prompts with multiple components (8.53% drop in CIS per component
for all evaluated models). We also identify a significant decline in the
quality of the images and context awareness within an image as the number of
components increased (15.91% decrease in inception Score and 9.62% increase in
Frechet Inception Distance). To remedy this issue, we fine-tuned Stable
Diffusion V2 on a custom-created test dataset with multiple components,
outperforming its vanilla counterpart. To conclude, these findings reveal a
critical limitation in existing text-to-image generators, shedding light on the
challenge of generating multiple components within a single image using a
complex prompt.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13621">Knowledge From the Dark Side: Entropy-Reweighted Knowledge Distillation for Balanced Knowledge Transfer. (arXiv:2311.13621v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1">Chi-Ping Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Tseng_C/0/1/0/all/0/1">Ching-Hsun Tseng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Shin-Jye Lee</a></p>
<p>Knowledge Distillation (KD) transfers knowledge from a larger "teacher" model
to a compact "student" model, guiding the student with the "dark knowledge"
$\unicode{x2014}$ the implicit insights present in the teacher's soft
predictions. Although existing KDs have shown the potential of transferring
knowledge, the gap between the two parties still exists. With a series of
investigations, we argue the gap is the result of the student's overconfidence
in prediction, signaling an imbalanced focus on pronounced features while
overlooking the subtle yet crucial dark knowledge. To overcome this, we
introduce the Entropy-Reweighted Knowledge Distillation (ER-KD), a novel
approach that leverages the entropy in the teacher's predictions to reweight
the KD loss on a sample-wise basis. ER-KD precisely refocuses the student on
challenging instances rich in the teacher's nuanced insights while reducing the
emphasis on simpler cases, enabling a more balanced knowledge transfer.
Consequently, ER-KD not only demonstrates compatibility with various
state-of-the-art KD methods but also further enhances their performance at
negligible cost. This approach offers a streamlined and effective strategy to
refine the knowledge transfer process in KD, setting a new paradigm in the
meticulous handling of dark knowledge. Our code is available at
https://github.com/cpsu00/ER-KD.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13622">TDiffDe: A Truncated Diffusion Model for Remote Sensing Hyperspectral Image Denoising. (arXiv:2311.13622v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Jiang He</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yajie Li</a>, <a href="http://arxiv.org/find/cs/1/au:+L_J/0/1/0/all/0/1">Jie L</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_Q/0/1/0/all/0/1">Qiangqiang Yuan</a></p>
<p>Hyperspectral images play a crucial role in precision agriculture,
environmental monitoring or ecological analysis. However, due to sensor
equipment and the imaging environment, the observed hyperspectral images are
often inevitably corrupted by various noise. In this study, we proposed a
truncated diffusion model, called TDiffDe, to recover the useful information in
hyperspectral images gradually. Rather than starting from a pure noise, the
input data contains image information in hyperspectral image denoising. Thus,
we cut the trained diffusion model from small steps to avoid the destroy of
valid information.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13623">Density Distribution-based Learning Framework for Addressing Online Continual Learning Challenges. (arXiv:2311.13623v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shilin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiahui Wang</a></p>
<p>In this paper, we address the challenges of online Continual Learning (CL) by
introducing a density distribution-based learning framework. CL, especially the
Class Incremental Learning, enables adaptation to new test distributions while
continuously learning from a single-pass training data stream, which is more in
line with the practical application requirements of real-world scenarios.
However, existing CL methods often suffer from catastrophic forgetting and
higher computing costs due to complex algorithm designs, limiting their
practical use. Our proposed framework overcomes these limitations by achieving
superior average accuracy and time-space efficiency, bridging the performance
gap between CL and classical machine learning. Specifically, we adopt an
independent Generative Kernel Density Estimation (GKDE) model for each CL task.
During the testing stage, the GKDEs utilize a self-reported max probability
density value to determine which one is responsible for predicting incoming
test instances. A GKDE-based learning objective can ensure that samples with
the same label are grouped together, while dissimilar instances are pushed
farther apart. Extensive experiments conducted on multiple CL datasets validate
the effectiveness of our proposed framework. Our method outperforms popular CL
approaches by a significant margin, while maintaining competitive time-space
efficiency, making our framework suitable for real-world applications. Code
will be available at https://github.com/xxxx/xxxx.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13627">Vamos: Versatile Action Models for Video Understanding. (arXiv:2311.13627v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shijie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1">Qi Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Do_M/0/1/0/all/0/1">Minh Quan Do</a>, <a href="http://arxiv.org/find/cs/1/au:+Agarwal_N/0/1/0/all/0/1">Nakul Agarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kwonjoon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1">Chen Sun</a></p>
<p>What makes good video representations for video understanding, such as
anticipating future activities, or answering video-conditioned questions? While
earlier approaches focus on end-to-end learning directly from video pixels, we
propose to revisit text-based representations, such as discrete action labels,
or free-form video captions, which are interpretable and can be directly
consumed by large language models (LLMs). Intuitively, different video
understanding tasks may require representations that are complementary and at
different granularities. To this end, we propose versatile action models
(Vamos), a learning framework powered by a large language model as the
"reasoner", and can flexibly leverage visual embeddings, action labels, and
free-form descriptions extracted from videos as its input. We evaluate Vamos on
four complementary video understanding benchmarks, Ego4D, Next-QA, IntentQA,
and EgoSchema, on its capability to model temporal dynamics, encode visual
history, and perform reasoning. Surprisingly, we observe that text-based
representations consistently achieve competitive performance on all benchmarks,
and that visual embeddings provide marginal or no performance improvement,
demonstrating the effectiveness of text-based video representation in the LLM
era. We perform extensive ablation study and qualitative analysis to support
our observations, and achieve state-of-the-art performance on three benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13629">Diffusion models meet image counter-forensics. (arXiv:2311.13629v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tailanian_M/0/1/0/all/0/1">Mat&#xed;as Tailanian</a>, <a href="http://arxiv.org/find/cs/1/au:+Gardella_M/0/1/0/all/0/1">Marina Gardella</a>, <a href="http://arxiv.org/find/cs/1/au:+Pardo_A/0/1/0/all/0/1">&#xc1;lvaro Pardo</a>, <a href="http://arxiv.org/find/cs/1/au:+Muse_P/0/1/0/all/0/1">Pablo Mus&#xe9;</a></p>
<p>From its acquisition in the camera sensors to its storage, different
operations are performed to generate the final image. This pipeline imprints
specific traces into the image to form a natural watermark. Tampering with an
image disturbs these traces; these disruptions are clues that are used by most
methods to detect and locate forgeries. In this article, we assess the
capabilities of diffusion models to erase the traces left by forgers and,
therefore, deceive forensics methods. Such an approach has been recently
introduced for adversarial purification, achieving significant performance. We
show that diffusion purification methods are well suited for counter-forensics
tasks. Such approaches outperform already existing counter-forensics techniques
both in deceiving forensics methods and in preserving the natural look of the
purified images. The source code is publicly available at
https://github.com/mtailanian/diff-cf.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13655">GAN-Avatar: Controllable Personalized GAN-based Human Head Avatar. (arXiv:2311.13655v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kabadayi_B/0/1/0/all/0/1">Berna Kabadayi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zielonka_W/0/1/0/all/0/1">Wojciech Zielonka</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhatnagar_B/0/1/0/all/0/1">Bharat Lal Bhatnagar</a>, <a href="http://arxiv.org/find/cs/1/au:+Pons_Moll_G/0/1/0/all/0/1">Gerard Pons-Moll</a>, <a href="http://arxiv.org/find/cs/1/au:+Thies_J/0/1/0/all/0/1">Justus Thies</a></p>
<p>Digital humans and, especially, 3D facial avatars have raised a lot of
attention in the past years, as they are the backbone of several applications
like immersive telepresence in AR or VR. Despite the progress, facial avatars
reconstructed from commodity hardware are incomplete and miss out on parts of
the side and back of the head, severely limiting the usability of the avatar.
This limitation in prior work stems from their requirement of face tracking,
which fails for profile and back views. To address this issue, we propose to
learn person-specific animatable avatars from images without assuming to have
access to precise facial expression tracking. At the core of our method, we
leverage a 3D-aware generative model that is trained to reproduce the
distribution of facial expressions from the training data. To train this
appearance model, we only assume to have a collection of 2D images with the
corresponding camera parameters. For controlling the model, we learn a mapping
from 3DMM facial expression parameters to the latent space of the generative
model. This mapping can be learned by sampling the latent space of the
appearance model and reconstructing the facial parameters from a normalized
frontal view, where facial expression estimation performs well. With this
scheme, we decouple 3D appearance reconstruction and animation control to
achieve high fidelity in image synthesis. In a series of experiments, we
compare our proposed technique to state-of-the-art monocular methods and show
superior quality while not requiring expression tracking of the training data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13656">Panda or not Panda? Understanding Adversarial Attacks with Interactive Visualization. (arXiv:2311.13656v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1">Yuzhe You</a>, <a href="http://arxiv.org/find/cs/1/au:+Tse_J/0/1/0/all/0/1">Jarvis Tse</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jian Zhao</a></p>
<p>Adversarial machine learning (AML) studies attacks that can fool machine
learning algorithms into generating incorrect outcomes as well as the defenses
against worst-case attacks to strengthen model robustness. Specifically for
image classification, it is challenging to understand adversarial attacks due
to their use of subtle perturbations that are not human-interpretable, as well
as the variability of attack impacts influenced by diverse methodologies,
instance differences, and model architectures. Through a design study with AML
learners and teachers, we introduce AdvEx, a multi-level interactive
visualization system that comprehensively presents the properties and impacts
of evasion attacks on different image classifiers for novice AML learners. We
quantitatively and qualitatively assessed AdvEx in a two-part evaluation
including user studies and expert interviews. Our results show that AdvEx is
not only highly effective as a visualization tool for understanding AML
mechanisms, but also provides an engaging and enjoyable learning experience,
thus demonstrating its overall benefits for AML learners.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13661">BenthIQ: a Transformer-Based Benthic Classification Model for Coral Restoration. (arXiv:2311.13661v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kurinchi_Vendhan_R/0/1/0/all/0/1">Rupa Kurinchi-Vendhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Gray_D/0/1/0/all/0/1">Drew Gray</a>, <a href="http://arxiv.org/find/cs/1/au:+Cole_E/0/1/0/all/0/1">Elijah Cole</a></p>
<p>Coral reefs are vital for marine biodiversity, coastal protection, and
supporting human livelihoods globally. However, they are increasingly
threatened by mass bleaching events, pollution, and unsustainable practices
with the advent of climate change. Monitoring the health of these ecosystems is
crucial for effective restoration and management. Current methods for creating
benthic composition maps often compromise between spatial coverage and
resolution. In this paper, we introduce BenthIQ, a multi-label semantic
segmentation network designed for high-precision classification of underwater
substrates, including live coral, algae, rock, and sand. Although commonly
deployed CNNs are limited in learning long-range semantic information,
transformer-based models have recently achieved state-of-the-art performance in
vision tasks such as object detection and image classification. We integrate
the hierarchical Swin Transformer as the backbone of a U-shaped encoder-decoder
architecture for local-global semantic feature learning. Using a real-world
case study in French Polynesia, we demonstrate that our approach outperforms
traditional CNN and attention-based models on pixel-wise classification of
shallow reef imagery.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13664">Sample as You Infer: Predictive Coding With Langevin Dynamics. (arXiv:2311.13664v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zahid_U/0/1/0/all/0/1">Umais Zahid</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1">Qinghai Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Fountas_Z/0/1/0/all/0/1">Zafeirios Fountas</a></p>
<p>We present a novel algorithm for parameter learning in generic deep
generative models that builds upon the predictive coding (PC) framework of
computational neuroscience. Our approach modifies the standard PC algorithm to
bring performance on-par and exceeding that obtained from standard variational
auto-encoder (VAE) training. By injecting Gaussian noise into the PC inference
procedure we re-envision it as an overdamped Langevin sampling, which
facilitates optimisation with respect to a tight evidence lower bound (ELBO).
We improve the resultant encoder-free training method by incorporating an
encoder network to provide an amortised warm-start to our Langevin sampling and
test three different objectives for doing so. Finally, to increase robustness
to the sampling step size and reduce sensitivity to curvature, we validate a
lightweight and easily computable form of preconditioning, inspired by Riemann
Manifold Langevin and adaptive optimizers from the SGD literature. We compare
against VAEs by training like-for-like generative models using our technique
against those trained with standard reparameterisation-trick-based ELBOs. We
observe our method out-performs or matches performance across a number of
metrics, including sample quality, while converging in a fraction of the number
of SGD training iterations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13668">MAIRA-1: A specialised large multimodal model for radiology report generation. (arXiv:2311.13668v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hyland_S/0/1/0/all/0/1">Stephanie L. Hyland</a>, <a href="http://arxiv.org/find/cs/1/au:+Bannur_S/0/1/0/all/0/1">Shruthi Bannur</a>, <a href="http://arxiv.org/find/cs/1/au:+Bouzid_K/0/1/0/all/0/1">Kenza Bouzid</a>, <a href="http://arxiv.org/find/cs/1/au:+Castro_D/0/1/0/all/0/1">Daniel C. Castro</a>, <a href="http://arxiv.org/find/cs/1/au:+Ranjit_M/0/1/0/all/0/1">Mercy Ranjit</a>, <a href="http://arxiv.org/find/cs/1/au:+Schwaighofer_A/0/1/0/all/0/1">Anton Schwaighofer</a>, <a href="http://arxiv.org/find/cs/1/au:+Perez_Garcia_F/0/1/0/all/0/1">Fernando P&#xe9;rez-Garc&#xed;a</a>, <a href="http://arxiv.org/find/cs/1/au:+Salvatelli_V/0/1/0/all/0/1">Valentina Salvatelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Srivastav_S/0/1/0/all/0/1">Shaury Srivastav</a>, <a href="http://arxiv.org/find/cs/1/au:+Thieme_A/0/1/0/all/0/1">Anja Thieme</a>, <a href="http://arxiv.org/find/cs/1/au:+Codella_N/0/1/0/all/0/1">Noel Codella</a>, <a href="http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1">Matthew P. Lungren</a>, <a href="http://arxiv.org/find/cs/1/au:+Wetscherek_M/0/1/0/all/0/1">Maria Teodora Wetscherek</a>, <a href="http://arxiv.org/find/cs/1/au:+Oktay_O/0/1/0/all/0/1">Ozan Oktay</a>, <a href="http://arxiv.org/find/cs/1/au:+Alvarez_Valle_J/0/1/0/all/0/1">Javier Alvarez-Valle</a></p>
<p>We present a radiology-specific multimodal model for the task for generating
radiological reports from chest X-rays (CXRs). Our work builds on the idea that
large language model(s) can be equipped with multimodal capabilities through
alignment with pre-trained vision encoders. On natural images, this has been
shown to allow multimodal models to gain image understanding and description
capabilities. Our proposed model (MAIRA-1) leverages a CXR-specific image
encoder in conjunction with a fine-tuned large language model based on
Vicuna-7B, and text-based data augmentation, to produce reports with
state-of-the-art quality. In particular, MAIRA-1 significantly improves on the
radiologist-aligned RadCliQ metric and across all lexical metrics considered.
Manual review of model outputs demonstrates promising fluency and accuracy of
generated reports while uncovering failure modes not captured by existing
evaluation practices. More information and resources can be found on the
project website: https://aka.ms/maira.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13681">Compact 3D Gaussian Representation for Radiance Field. (arXiv:2311.13681v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Joo Chan Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Rho_D/0/1/0/all/0/1">Daniel Rho</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xiangyu Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Ko_J/0/1/0/all/0/1">Jong Hwan Ko</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_E/0/1/0/all/0/1">Eunbyung Park</a></p>
<p>Neural Radiance Fields (NeRFs) have demonstrated remarkable potential in
capturing complex 3D scenes with high fidelity. However, one persistent
challenge that hinders the widespread adoption of NeRFs is the computational
bottleneck due to the volumetric rendering. On the other hand, 3D Gaussian
splatting (3DGS) has recently emerged as an alternative representation that
leverages a 3D Gaussisan-based representation and adopts the rasterization
pipeline to render the images rather than volumetric rendering, achieving very
fast rendering speed and promising image quality. However, a significant
drawback arises as 3DGS entails a substantial number of 3D Gaussians to
maintain the high fidelity of the rendered images, which requires a large
amount of memory and storage. To address this critical issue, we place a
specific emphasis on two key objectives: reducing the number of Gaussian points
without sacrificing performance and compressing the Gaussian attributes, such
as view-dependent color and covariance. To this end, we propose a learnable
mask strategy that significantly reduces the number of Gaussians while
preserving high performance. In addition, we propose a compact but effective
representation of view-dependent color by employing a grid-based neural field
rather than relying on spherical harmonics. Finally, we learn codebooks to
compactly represent the geometric attributes of Gaussian by vector
quantization. In our extensive experiments, we consistently show over
10$\times$ reduced storage and enhanced rendering speed, while maintaining the
quality of the scene representation, compared to 3DGS. Our work provides a
comprehensive framework for 3D scene representation, achieving high
performance, fast training, compactness, and real-time rendering. Our project
page is available at https://maincold2.github.io/c3dgs/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13682">Single-Shot Plug-and-Play Methods for Inverse Problems. (arXiv:2311.13682v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1">Yanqi Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lipei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1">Zhenda Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shujun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1">Lequan Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_R/0/1/0/all/0/1">Raymond H. Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1">Carola-Bibiane Sch&#xf6;nlieb</a>, <a href="http://arxiv.org/find/cs/1/au:+Aviles_Rivero_A/0/1/0/all/0/1">Angelica I Aviles-Rivero</a></p>
<p>The utilisation of Plug-and-Play (PnP) priors in inverse problems has become
increasingly prominent in recent years. This preference is based on the
mathematical equivalence between the general proximal operator and the
regularised denoiser, facilitating the adaptation of various off-the-shelf
denoiser priors to a wide range of inverse problems. However, existing PnP
models predominantly rely on pre-trained denoisers using large datasets. In
this work, we introduce Single-Shot PnP methods (SS-PnP), shifting the focus to
solving inverse problems with minimal data. First, we integrate Single-Shot
proximal denoisers into iterative methods, enabling training with single
instances. Second, we propose implicit neural priors based on a novel function
that preserves relevant frequencies to capture fine details while avoiding the
issue of vanishing gradients. We demonstrate, through extensive numerical and
visual experiments, that our method leads to better approximations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13688">Masked Conditional Diffusion Models for Image Analysis with Application to Radiographic Diagnosis of Infant Abuse. (arXiv:2311.13688v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Wu_S/0/1/0/all/0/1">Shaoju Wu</a>, <a href="http://arxiv.org/find/eess/1/au:+Kurugol_S/0/1/0/all/0/1">Sila Kurugol</a>, <a href="http://arxiv.org/find/eess/1/au:+Tsai_A/0/1/0/all/0/1">Andy Tsai</a></p>
<p>The classic metaphyseal lesion (CML) is a distinct injury that is highly
specific for infant abuse. It commonly occurs in the distal tibia. To aid
radiologists detect these subtle fractures, we need to develop a model that can
flag abnormal distal tibial radiographs (i.e. those with CMLs). Unfortunately,
the development of such a model requires a large and diverse training database,
which is often not available. To address this limitation, we propose a novel
generative model for data augmentation. Unlike previous models that fail to
generate data that span the diverse radiographic appearance of the distal
tibial CML, our proposed masked conditional diffusion model (MaC-DM) not only
generates realistic-appearing and wide-ranging synthetic images of the distal
tibial radiographs with and without CMLs, it also generates their associated
segmentation labels. To achieve these tasks, MaC-DM combines the weighted
segmentation masks of the tibias and the CML fracture sites as additional
conditions for classifier guidance. The augmented images from our model
improved the performances of ResNet-34 in classifying normal radiographs and
those with CMLs. Further, the augmented images and their associated
segmentation masks enhanced the performance of the U-Net in labeling areas of
the CMLs on distal tibial radiographs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13706">Multi-view Hybrid Graph Convolutional Network for Volume-to-mesh Reconstruction in Cardiovascular MRI. (arXiv:2311.13706v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Gaggion_N/0/1/0/all/0/1">Nicol&#xe1;s Gaggion</a>, <a href="http://arxiv.org/find/eess/1/au:+Matheson_B/0/1/0/all/0/1">Benjamin A. Matheson</a>, <a href="http://arxiv.org/find/eess/1/au:+Xia_Y/0/1/0/all/0/1">Yan Xia</a>, <a href="http://arxiv.org/find/eess/1/au:+Bonazzola_R/0/1/0/all/0/1">Rodrigo Bonazzola</a>, <a href="http://arxiv.org/find/eess/1/au:+Ravikumar_N/0/1/0/all/0/1">Nishant Ravikumar</a>, <a href="http://arxiv.org/find/eess/1/au:+Taylor_Z/0/1/0/all/0/1">Zeike A. Taylor</a>, <a href="http://arxiv.org/find/eess/1/au:+Milone_D/0/1/0/all/0/1">Diego H. Milone</a>, <a href="http://arxiv.org/find/eess/1/au:+Frangi_A/0/1/0/all/0/1">Alejandro F. Frangi</a>, <a href="http://arxiv.org/find/eess/1/au:+Ferrante_E/0/1/0/all/0/1">Enzo Ferrante</a></p>
<p>Cardiovascular magnetic resonance imaging is emerging as a crucial tool to
examine cardiac morphology and function. Essential to this endeavour are
anatomical 3D surface and volumetric meshes derived from CMR images, which
facilitate computational anatomy studies, biomarker discovery, and in-silico
simulations. However, conventional surface mesh generation methods, such as
active shape models and multi-atlas segmentation, are highly time-consuming and
require complex processing pipelines to generate simulation-ready 3D meshes. In
response, we introduce HybridVNet, a novel architecture for direct
image-to-mesh extraction seamlessly integrating standard convolutional neural
networks with graph convolutions, which we prove can efficiently handle surface
and volumetric meshes by encoding them as graph structures. To further enhance
accuracy, we propose a multiview HybridVNet architecture which processes both
long axis and short axis CMR, showing that it can increase the performance of
cardiac MR mesh generation. Our model combines traditional convolutional
networks with variational graph generative models, deep supervision and
mesh-specific regularisation. Experiments on a comprehensive dataset from the
UK Biobank confirm the potential of HybridVNet to significantly advance cardiac
imaging and computational cardiology by efficiently generating high-fidelity
and simulation ready meshes from CMR images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13710">A Comprehensive Review of Artificial Intelligence Applications in Major Retinal Conditions. (arXiv:2311.13710v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Raja_H/0/1/0/all/0/1">Hina Raja</a>, <a href="http://arxiv.org/find/eess/1/au:+Hassan_T/0/1/0/all/0/1">Taimur Hassan</a>, <a href="http://arxiv.org/find/eess/1/au:+Hassan_B/0/1/0/all/0/1">Bilal Hassan</a>, <a href="http://arxiv.org/find/eess/1/au:+Akram_M/0/1/0/all/0/1">Muhammad Usman Akram</a>, <a href="http://arxiv.org/find/eess/1/au:+Raja_H/0/1/0/all/0/1">Hira Raja</a>, <a href="http://arxiv.org/find/eess/1/au:+Abd_alrazaq_A/0/1/0/all/0/1">Alaa A Abd-alrazaq</a>, <a href="http://arxiv.org/find/eess/1/au:+Yousefi_S/0/1/0/all/0/1">Siamak Yousefi</a>, <a href="http://arxiv.org/find/eess/1/au:+Werghi_N/0/1/0/all/0/1">Naoufel Werghi</a></p>
<p>This paper provides a systematic survey of retinal diseases that cause visual
impairments or blindness, emphasizing the importance of early detection for
effective treatment. It covers both clinical and automated approaches for
detecting retinal disease, focusing on studies from the past decade. The survey
evaluates various algorithms for identifying structural abnormalities and
diagnosing retinal diseases, and it identifies future research directions based
on a critical analysis of existing literature. This comprehensive study, which
reviews both clinical and automated detection methods using different
modalities, appears to be unique in its scope. Additionally, the survey serves
as a helpful guide for researchers interested in digital retinopathy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13713">A Somewhat Robust Image Watermark against Diffusion-based Editing Models. (arXiv:2311.13713v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1">Mingtian Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tianhao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1">Somesh Jha</a></p>
<p>Recently, diffusion models (DMs) have become the state-of-the-art method for
image synthesis. Editing models based on DMs, known for their high fidelity and
precision, have inadvertently introduced new challenges related to image
copyright infringement and malicious editing. Our work is the first to
formalize and address this issue. After assessing and attempting to enhance
traditional image watermarking techniques, we recognize their limitations in
this emerging context. In response, we develop a novel technique, RIW (Robust
Invisible Watermarking), to embed invisible watermarks leveraging adversarial
example techniques. Our technique ensures a high extraction accuracy of $96\%$
for the invisible watermark after editing, compared to the $0\%$ offered by
conventional methods. We provide access to our code at
https://github.com/BennyTMT/RIW.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13716">DiverseNet: Decision Diversified Semi-supervised Semantic Segmentation Networks for Remote Sensing Imagery. (arXiv:2311.13716v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1">Wanli Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Karakus_O/0/1/0/all/0/1">Oktay Karakus</a>, <a href="http://arxiv.org/find/cs/1/au:+Rosin_P/0/1/0/all/0/1">Paul L. Rosin</a></p>
<p>Semi-supervised learning is designed to help reduce the cost of the manual
labelling process by exploiting the use of useful features from a large
quantity of unlabelled data during training. Since pixel-level manual labelling
in large-scale remote sensing imagery is expensive, semi-supervised learning
becomes an appropriate solution to this. However, most of the existing
semi-supervised learning methods still lack efficient perturbation methods to
promote diversity of features and the precision of pseudo labels during
training. In order to fill this gap, we propose DiverseNet architectures which
explore multi-head and multi-model semi-supervised learning algorithms by
simultaneously promoting precision and diversity during training. The two
proposed methods of DiverseNet, namely the DiverseHead and DiverseModel,
achieve the highest semantic segmentation performance in four widely utilised
remote sensing imagery data sets compared to state-of-the-art semi-supervised
learning methods. Meanwhile, the proposed DiverseHead architecture is
relatively lightweight in terms of parameter space compared to the
state-of-the-art methods whilst reaching high-performance results for all the
tested data sets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13717">Importance of Feature Extraction in the Calculation of Fr\&#x27;echet Distance for Medical Imaging. (arXiv:2311.13717v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Woodland_M/0/1/0/all/0/1">McKell Woodland</a> (1 and 2), <a href="http://arxiv.org/find/cs/1/au:+Taie_M/0/1/0/all/0/1">Mais Al Taie</a>, <a href="http://arxiv.org/find/cs/1/au:+Silva_J/0/1/0/all/0/1">Jessica Albuquerque Marques Silva</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Eltaher_M/0/1/0/all/0/1">Mohamed Eltaher</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Mohn_F/0/1/0/all/0/1">Frank Mohn</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Shieh_A/0/1/0/all/0/1">Alexander Shieh</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Castelo_A/0/1/0/all/0/1">Austin Castelo</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Kundu_S/0/1/0/all/0/1">Suprateek Kundu</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Yung_J/0/1/0/all/0/1">Joshua P. Yung</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1">Ankit B. Patel</a> (2 and 3), <a href="http://arxiv.org/find/cs/1/au:+Brock_K/0/1/0/all/0/1">Kristy K. Brock</a> (1) ((1) The University of Texas MD Anderson Cancer Center, (2) Rice University, (3) Baylor College of Medicine)</p>
<p>Fr\'echet Inception Distance is a widely used metric for evaluating synthetic
image quality that utilizes an ImageNet-trained InceptionV3 network as a
feature extractor. However, its application in medical imaging lacks a standard
feature extractor, leading to biased and inconsistent comparisons. This study
aimed to compare state-of-the-art feature extractors for computing Fr\'echet
Distances (FDs) in medical imaging. A StyleGAN2 network was trained with data
augmentation techniques tailored for limited data domains on datasets
comprising three medical imaging modalities and four anatomical locations.
Human evaluation of generative quality (via a visual Turing test) was compared
to FDs calculated using ImageNet-trained InceptionV3, ResNet50, SwAV, DINO, and
Swin Transformer architectures, in addition to an InceptionV3 network trained
on a large medical dataset, RadImageNet. All ImageNet-based extractors were
consistent with each other, but only SwAV was significantly correlated with
medical expert judgment. The RadImageNet-based FD showed volatility and lacked
correlation with human judgment. Caution is advised when using medical
image-trained extraction networks in the FD calculation. These networks should
be rigorously evaluated on the imaging modality under consideration and
publicly released. ImageNet-based extractors, while imperfect, are consistent
and widely understood. Training extraction networks with SwAV is a promising
approach for synthetic medical image evaluation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13719">Deep learning-based instance segmentation for the precise automated quantification of digital breast cancer immunohistochemistry images. (arXiv:2311.13719v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Priego_Torresa_B/0/1/0/all/0/1">Blanca Maria Priego-Torresa</a>, <a href="http://arxiv.org/find/eess/1/au:+Lobato_Delgado_B/0/1/0/all/0/1">Barbara Lobato-Delgado</a>, <a href="http://arxiv.org/find/eess/1/au:+Atienza_Cuevas_L/0/1/0/all/0/1">Lidia Atienza-Cuevas</a>, <a href="http://arxiv.org/find/eess/1/au:+Sanchez_Morillo_D/0/1/0/all/0/1">Daniel Sanchez-Morillo</a></p>
<p>The quantification of biomarkers on immunohistochemistry breast cancer images
is essential for defining appropriate therapy for breast cancer patients, as
well as for extracting relevant information on disease prognosis. This is an
arduous and time-consuming task that may introduce a bias in the results due to
intra- and inter-observer variability which could be alleviated by making use
of automatic quantification tools. However, this is not a simple processing
task given the heterogeneity of breast tumors that results in non-uniformly
distributed tumor cells exhibiting different staining colors and intensity,
size, shape, and texture, of the nucleus, cytoplasm and membrane. In this
research work, we demonstrate the feasibility of using a deep learning-based
instance segmentation architecture for the automatic quantification of both
nuclear and membrane biomarkers applied to IHC-stained slides. We have solved
the cumbersome task of training set generation with the design and
implementation of a web platform, which has served as a hub for communication
and feedback between researchers and pathologists as well as a system for the
validation of the automatic image processing models. Through this tool, we have
collected annotations over samples of HE, ER and Ki-67 (nuclear biomarkers) and
HER2 (membrane biomarker) IHC-stained images. Using the same deep learning
network architecture, we have trained two models, so-called nuclei- and
membrane-aware segmentation models, which, once successfully validated, have
revealed to be a promising method to segment nuclei instances in IHC-stained
images. The quantification method proposed in this work has been integrated
into the developed web platform and is currently being used as a
decision-support tool by pathologists.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13745">Sample-Efficient Training for Diffusion. (arXiv:2311.13745v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1">Shivam Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Parulekar_A/0/1/0/all/0/1">Aditya Parulekar</a>, <a href="http://arxiv.org/find/cs/1/au:+Price_E/0/1/0/all/0/1">Eric Price</a>, <a href="http://arxiv.org/find/cs/1/au:+Xun_Z/0/1/0/all/0/1">Zhiyang Xun</a></p>
<p>Score-based diffusion models have become the most popular approach to deep
generative modeling of images, largely due to their empirical performance and
reliability. Recently, a number of theoretical works \citep{chen2022,
Chen2022ImprovedAO, Chenetal23flowode, benton2023linear} have shown that
diffusion models can efficiently sample, assuming $L^2$-accurate score
estimates. The score-matching objective naturally approximates the true score
in $L^2$, but the sample complexity of existing bounds depends
\emph{polynomially} on the data radius and desired Wasserstein accuracy. By
contrast, the time complexity of sampling is only logarithmic in these
parameters. We show that estimating the score in $L^2$ \emph{requires} this
polynomial dependence, but that a number of samples that scales
polylogarithmically in the Wasserstein accuracy actually do suffice for
sampling. We show that with a polylogarithmic number of samples, the ERM of the
score-matching objective is $L^2$ accurate on all but a probability $\delta$
fraction of the true distribution, and that this weaker guarantee is sufficient
for efficient sampling.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13750">Towards Transferable Multi-modal Perception Representation Learning for Autonomy: NeRF-Supervised Masked AutoEncoder. (arXiv:2311.13750v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiaohao Xu</a></p>
<p>This work proposes a unified self-supervised pre-training framework for
transferable multi-modal perception representation learning via masked
multi-modal reconstruction in Neural Radiance Field (NeRF), namely
NeRF-Supervised Masked AutoEncoder (NS-MAE). Specifically, conditioned on
certain view directions and locations, multi-modal embeddings extracted from
corrupted multi-modal input signals, i.e., Lidar point clouds and images, are
rendered into projected multi-modal feature maps via neural rendering. Then,
original multi-modal signals serve as reconstruction targets for the rendered
multi-modal feature maps to enable self-supervised representation learning.
Extensive experiments show that the representation learned via NS-MAE shows
promising transferability for diverse multi-modal and single-modal (camera-only
and Lidar-only) perception models on diverse 3D perception downstream tasks (3D
object detection and BEV map segmentation) with diverse amounts of fine-tuning
labeled data. Moreover, we empirically find that NS-MAE enjoys the synergy of
both the mechanism of masked autoencoder and neural radiance field. Our code
shall be released upon acceptance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13752">3D-MIR: A Benchmark and Empirical Study on 3D Medical Image Retrieval in Radiology. (arXiv:2311.13752v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abacha_A/0/1/0/all/0/1">Asma Ben Abacha</a>, <a href="http://arxiv.org/find/cs/1/au:+Santamaria_Pang_A/0/1/0/all/0/1">Alberto Santamaria-Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Ho Hin Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Merkow_J/0/1/0/all/0/1">Jameson Merkow</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_Q/0/1/0/all/0/1">Qin Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Devarakonda_S/0/1/0/all/0/1">Surya Teja Devarakonda</a>, <a href="http://arxiv.org/find/cs/1/au:+Islam_A/0/1/0/all/0/1">Abdullah Islam</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_J/0/1/0/all/0/1">Julia Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1">Matthew P. Lungren</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1">Thomas Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Codella_N/0/1/0/all/0/1">Noel C Codella</a>, <a href="http://arxiv.org/find/cs/1/au:+Tarapov_I/0/1/0/all/0/1">Ivan Tarapov</a></p>
<p>The increasing use of medical imaging in healthcare settings presents a
significant challenge due to the increasing workload for radiologists, yet it
also offers opportunity for enhancing healthcare outcomes if effectively
leveraged. 3D image retrieval holds potential to reduce radiologist workloads
by enabling clinicians to efficiently search through diagnostically similar or
otherwise relevant cases, resulting in faster and more precise diagnoses.
However, the field of 3D medical image retrieval is still emerging, lacking
established evaluation benchmarks, comprehensive datasets, and thorough
studies. This paper attempts to bridge this gap by introducing a novel
benchmark for 3D Medical Image Retrieval (3D-MIR) that encompasses four
different anatomies imaged with computed tomography. Using this benchmark, we
explore a diverse set of search strategies that use aggregated 2D slices, 3D
volumes, and multi-modal embeddings from popular multi-modal foundation models
as queries. Quantitative and qualitative assessments of each approach are
provided alongside an in-depth discussion that offers insight for future
research. To promote the advancement of this field, our benchmark, dataset, and
code are made publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13777">GS-Pose: Category-Level Object Pose Estimation via Geometric and Semantic Correspondence. (arXiv:2311.13777v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Pengyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ikeda_T/0/1/0/all/0/1">Takuya Ikeda</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1">Robert Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Nishiwaki_K/0/1/0/all/0/1">Koichi Nishiwaki</a></p>
<p>Category-level pose estimation is a challenging task with many potential
applications in computer vision and robotics. Recently, deep-learning-based
approaches have made great progress, but are typically hindered by the need for
large datasets of either pose-labelled real images or carefully tuned
photorealistic simulators. This can be avoided by using only geometry inputs
such as depth images to reduce the domain-gap but these approaches suffer from
a lack of semantic information, which can be vital in the pose estimation
problem. To resolve this conflict, we propose to utilize both geometric and
semantic features obtained from a pre-trained foundation model.Our approach
projects 2D features from this foundation model into 3D for a single object
model per category, and then performs matching against this for new single view
observations of unseen object instances with a trained matching network. This
requires significantly less data to train than prior methods since the semantic
features are robust to object texture and appearance. We demonstrate this with
a rich evaluation, showing improved performance over prior methods with a
fraction of the data required.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13779">Detection and Identification Accuracy of PCA-Accelerated Real-Time Processing of Hyperspectral Imagery. (arXiv:2311.13779v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Basener_A/0/1/0/all/0/1">Abigail Basener</a>, <a href="http://arxiv.org/find/eess/1/au:+Herald_M/0/1/0/all/0/1">Meagan Herald</a></p>
<p>Real-time or near real-time hyperspectral detection and identification are
extremely useful and needed in many fields. These data sets can be quite large,
and the algorithms can require numerous computations that slow the process
down. A common way of speeding up the process is to use principal component
analysis (PCA) for dimension reduction. In the reduced dimensional space,
provided by a subset of the principal components, fewer computations are needed
to process the data resulting in a faster run time. In this paper, we propose a
way to further decrease the time required to use PCA by investigating how many
principal components may be omitted with minimal impact on the detection rate.
Using ACE to perform the detection, and then probability, and spectral fit for
identification, we find that the number of principal components can be reduced
by a substantial amount before seeing a noticeable change in detection rates.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13781">Dynamic Compositional Graph Convolutional Network for Efficient Composite Human Motion Prediction. (arXiv:2311.13781v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wanying Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1">Shen Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1">Fanyang Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Songtao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Mengyuan Liu</a></p>
<p>With potential applications in fields including intelligent surveillance and
human-robot interaction, the human motion prediction task has become a hot
research topic and also has achieved high success, especially using the recent
Graph Convolutional Network (GCN). Current human motion prediction task usually
focuses on predicting human motions for atomic actions. Observing that atomic
actions can happen at the same time and thus formulating the composite actions,
we propose the composite human motion prediction task. To handle this task, we
first present a Composite Action Generation (CAG) module to generate synthetic
composite actions for training, thus avoiding the laborious work of collecting
composite action samples. Moreover, we alleviate the effect of composite
actions on demand for a more complicated model by presenting a Dynamic
Compositional Graph Convolutional Network (DC-GCN). Extensive experiments on
the Human3.6M dataset and our newly collected CHAMP dataset consistently verify
the efficiency of our DC-GCN method, which achieves state-of-the-art motion
prediction accuracies and meanwhile needs few extra computational costs than
traditional GCN-based human motion methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13793">Evidential Active Recognition: Intelligent and Prudent Open-World Embodied Perception. (arXiv:2311.13793v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1">Lei Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1">Mingfu Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yunxuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1">Gang Hua</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Ying Wu</a></p>
<p>Active recognition enables robots to intelligently explore novel
observations, thereby acquiring more information while circumventing undesired
viewing conditions. Recent approaches favor learning policies from simulated or
collected data, wherein appropriate actions are more frequently selected when
the recognition is accurate. However, most recognition modules are developed
under the closed-world assumption, which makes them ill-equipped to handle
unexpected inputs, such as the absence of the target object in the current
observation. To address this issue, we propose treating active recognition as a
sequential evidence-gathering process, providing by-step uncertainty
quantification and reliable prediction under the evidence combination theory.
Additionally, the reward function developed in this paper effectively
characterizes the merit of actions when operating in open-world environments.
To evaluate the performance, we collect a dataset from an indoor simulator,
encompassing various recognition challenges such as distance, occlusion levels,
and visibility. Through a series of experiments on recognition and robustness
analysis, we demonstrate the necessity of introducing uncertainties to active
recognition and the superior performance of the proposed method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13831">Posterior Distillation Sampling. (arXiv:2311.13831v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Koo_J/0/1/0/all/0/1">Juil Koo</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1">Chanho Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Sung_M/0/1/0/all/0/1">Minhyuk Sung</a></p>
<p>We introduce Posterior Distillation Sampling (PDS), a novel optimization
method for parametric image editing based on diffusion models. Existing
optimization-based methods, which leverage the powerful 2D prior of diffusion
models to handle various parametric images, have mainly focused on generation.
Unlike generation, editing requires a balance between conforming to the target
attribute and preserving the identity of the source content. Recent 2D image
editing methods have achieved this balance by leveraging the stochastic latent
encoded in the generative process of diffusion models. To extend the editing
capabilities of diffusion models shown in pixel space to parameter space, we
reformulate the 2D image editing method into an optimization form named PDS.
PDS matches the stochastic latents of the source and the target, enabling the
sampling of targets in diverse parameter spaces that align with a desired
attribute while maintaining the source's identity. We demonstrate that this
optimization resembles running a generative process with the target attribute,
but aligning this process with the trajectory of the source's generative
process. Extensive editing results in Neural Radiance Fields and Scalable
Vector Graphics representations demonstrate that PDS is capable of sampling
targets to fulfill the aforementioned balance across various parameter spaces.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13833">Lego: Learning to Disentangle and Invert Concepts Beyond Object Appearance in Text-to-Image Diffusion Models. (arXiv:2311.13833v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Motamed_S/0/1/0/all/0/1">Saman Motamed</a>, <a href="http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1">Danda Pani Paudel</a>, <a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1">Luc Van Gool</a></p>
<p>Diffusion models have revolutionized generative content creation and
text-to-image (T2I) diffusion models in particular have increased the creative
freedom of users by allowing scene synthesis using natural language. T2I models
excel at synthesizing concepts such as nouns, appearances, and styles. To
enable customized content creation based on a few example images of a concept,
methods such as Textual Inversion and DreamBooth invert the desired concept and
enable synthesizing it in new scenes. However, inverting more general concepts
that go beyond object appearance and style (adjectives and verbs) through
natural language, remains a challenge. Two key characteristics of these
concepts contribute to the limitations of current inversion methods. 1)
Adjectives and verbs are entangled with nouns (subject) and can hinder
appearance-based inversion methods, where the subject appearance leaks into the
concept embedding and 2) describing such concepts often extends beyond single
word embeddings (being frozen in ice, walking on a tightrope, etc.) that
current methods do not handle.
</p>
<p>In this study, we introduce Lego, a textual inversion method designed to
invert subject entangled concepts from a few example images. Lego disentangles
concepts from their associated subjects using a simple yet effective Subject
Separation step and employs a Context Loss that guides the inversion of
single/multi-embedding concepts. In a thorough user study, Lego-generated
concepts were preferred over 70% of the time when compared to the baseline.
Additionally, visual question answering using a large language model suggested
Lego-generated concepts are better aligned with the text description of the
concept.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13846">Progressive Learning with Visual Prompt Tuning for Variable-Rate Image Compression. (arXiv:2311.13846v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qin_S/0/1/0/all/0/1">Shiyu Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yimin Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jinpeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Bin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1">Baoyi An</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_T/0/1/0/all/0/1">Tao Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1">Shu-Tao Xia</a></p>
<p>In this paper, we propose a progressive learning paradigm for
transformer-based variable-rate image compression. Our approach covers a wide
range of compression rates with the assistance of the Layer-adaptive Prompt
Module (LPM). Inspired by visual prompt tuning, we use LPM to extract prompts
for input images and hidden features at the encoder side and decoder side,
respectively, which are fed as additional information into the Swin Transformer
layer of a pre-trained transformer-based image compression model to affect the
allocation of attention region and the bits, which in turn changes the target
compression ratio of the model. To ensure the network is more lightweight, we
involves the integration of prompt networks with less convolutional layers.
Exhaustive experiments show that compared to methods based on multiple models,
which are optimized separately for different target rates, the proposed method
arrives at the same performance with 80% savings in parameter storage and 90%
savings in datasets. Meanwhile, our model outperforms all current variable
bitrate image methods in terms of rate-distortion performance and approaches
the state-of-the-art fixed bitrate image compression methods trained from
scratch.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13847">Perceptual Image Compression with Cooperative Cross-Modal Side Information. (arXiv:2311.13847v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qin_S/0/1/0/all/0/1">Shiyu Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Bin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yujun Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1">Baoyi An</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_T/0/1/0/all/0/1">Tao Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Via_S/0/1/0/all/0/1">Shu-Tao Via</a></p>
<p>The explosion of data has resulted in more and more associated text being
transmitted along with images. Inspired by from distributed source coding, many
works utilize image side information to enhance image compression. However,
existing methods generally do not consider using text as side information to
enhance perceptual compression of images, even though the benefits of
multimodal synergy have been widely demonstrated in research. This begs the
following question: How can we effectively transfer text-level semantic
dependencies to help image compression, which is only available to the decoder?
In this work, we propose a novel deep image compression method with text-guided
side information to achieve a better rate-perception-distortion tradeoff.
Specifically, we employ the CLIP text encoder and an effective Semantic-Spatial
Aware block to fuse the text and image features. This is done by predicting a
semantic mask to guide the learned text-adaptive affine transformation at the
pixel level. Furthermore, we design a text-conditional generative adversarial
networks to improve the perceptual quality of reconstructed images. Extensive
experiments involving four datasets and ten image quality assessment metrics
demonstrate that the proposed approach achieves superior results in terms of
rate-perception trade-off and semantic distortion.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13865">Language-guided Few-shot Semantic Segmentation. (arXiv:2311.13865v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jing Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1">Qiang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Fan Wang</a></p>
<p>Few-shot learning is a promising way for reducing the label cost in new
categories adaptation with the guidance of a small, well labeled support set.
But for few-shot semantic segmentation, the pixel-level annotations of support
images are still expensive. In this paper, we propose an innovative solution to
tackle the challenge of few-shot semantic segmentation using only language
information, i.e.image-level text labels. Our approach involves a
vision-language-driven mask distillation scheme, which contains a
vision-language pretraining (VLP) model and a mask refiner, to generate high
quality pseudo-semantic masks from text prompts. We additionally introduce a
distributed prototype supervision method and complementary correlation matching
module to guide the model in digging precise semantic relations among support
and query images. The experiments on two benchmark datasets demonstrate that
our method establishes a new baseline for language-guided few-shot semantic
segmentation and achieves competitive results to recent vision-guided methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13880">PointPCA+: Extending PointPCA objective quality assessment metric. (arXiv:2311.13880v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xuemei Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Alexiou_E/0/1/0/all/0/1">Evangelos Alexiou</a>, <a href="http://arxiv.org/find/cs/1/au:+Viola_I/0/1/0/all/0/1">Irene Viola</a>, <a href="http://arxiv.org/find/cs/1/au:+Cesar_P/0/1/0/all/0/1">Pablo Cesar</a></p>
<p>A computationally-simplified and descriptor-richer Point Cloud Quality
Assessment (PCQA) metric, namely PointPCA+, is proposed in this paper, which is
an extension of PointPCA. PointPCA proposed a set of perceptually-relevant
descriptors based on PCA decomposition that were applied to both the geometry
and texture data of point clouds for full reference PCQA. PointPCA+ employs PCA
only on the geometry data while enriching existing geometry and texture
descriptors, that are computed more efficiently. Similarly to PointPCA, a total
quality score is obtained through a learning-based fusion of individual
predictions from geometry and texture descriptors that capture local shape and
appearance properties, respectively. Before feature fusion, a feature selection
module is introduced to choose the most effective features from a proposed
super-set. Experimental results show that PointPCA+ achieves high predictive
performance against subjective ground truth scores obtained from publicly
available datasets. The code is available at
\url{https://github.com/cwi-dis/pointpca_suite/}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13895">Query by Activity Video in the Wild. (arXiv:2311.13895v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1">Tao Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Thong_W/0/1/0/all/0/1">William Thong</a>, <a href="http://arxiv.org/find/cs/1/au:+Mettes_P/0/1/0/all/0/1">Pascal Mettes</a>, <a href="http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1">Cees G.M. Snoek</a></p>
<p>This paper focuses on activity retrieval from a video query in an imbalanced
scenario. In current query-by-activity-video literature, a common assumption is
that all activities have sufficient labelled examples when learning an
embedding. This assumption does however practically not hold, as only a portion
of activities have many examples, while other activities are only described by
few examples. In this paper, we propose a visual-semantic embedding network
that explicitly deals with the imbalanced scenario for activity retrieval. Our
network contains two novel modules. The visual alignment module performs a
global alignment between the input video and fixed-sized visual bank
representations for all activities. The semantic module performs an alignment
between the input video and fixed-sized semantic activity representations. By
matching videos with both visual and semantic activity representations that are
of equal size over all activities, we no longer ignore infrequent activities
during retrieval. Experiments on a new imbalanced activity retrieval benchmark
show the effectiveness of our approach for all types of activities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13912">Expanding the deep-learning model to diagnosis LVNC: Limitations and trade-offs. (arXiv:2311.13912v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Bernabe_G/0/1/0/all/0/1">Gregorio Bernab&#xe9;</a>, <a href="http://arxiv.org/find/eess/1/au:+Gonzalez_Ferez_P/0/1/0/all/0/1">Pilar Gonz&#xe1;lez-F&#xe9;rez</a>, <a href="http://arxiv.org/find/eess/1/au:+Garcia_J/0/1/0/all/0/1">Jos&#xe9; M. Garc&#xed;a</a>, <a href="http://arxiv.org/find/eess/1/au:+Casas_G/0/1/0/all/0/1">Guillem Casas</a>, <a href="http://arxiv.org/find/eess/1/au:+Gonzalez_Carrillo_J/0/1/0/all/0/1">Josefa Gonz&#xe1;lez-Carrillo</a></p>
<p>Hyper-trabeculation or non-compaction in the left ventricle of the myocardium
(LVNC) is a recently classified form of cardiomyopathy. Several methods have
been proposed to quantify the trabeculae accurately in the left ventricle, but
there is no general agreement in the medical community to use a particular
approach. In previous work, we proposed DL-LVTQ, a deep learning approach for
left ventricular trabecular quantification based on a U-Net CNN architecture.
DL-LVTQ was an automatic diagnosis tool developed from a dataset of patients
with the same cardiomyopathy (hypertrophic cardiomyopathy).
</p>
<p>In this work, we have extended and adapted DL-LVTQ to cope with patients with
different cardiomyopathies. The dataset consists of up 379 patients in three
groups with different particularities and cardiomyopathies. Patient images were
taken from different scanners and hospitals. We have modified and adapted the
U-Net convolutional neural network to account for the different particularities
of a heterogeneous group of patients with various unclassifiable or mixed and
inherited cardiomyopathies.
</p>
<p>The inclusion of new groups of patients has increased the accuracy,
specificity and kappa values while maintaining the sensitivity of the automatic
deep learning method proposed. Therefore, a better-prepared diagnosis tool is
ready for various cardiomyopathies with different characteristics.
Cardiologists have considered that 98.9% of the evaluated outputs are verified
clinically for diagnosis. Therefore, the high precision to segment the
different cardiac structures allows us to make a robust diagnostic system
objective and faster, decreasing human error and time spent.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13925">Predicting Recovery or Decease of COVID-19 Patients with Clinical and RT-PCR Using Machine Learning Classification Algorithms. (arXiv:2311.13925v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Dehghani_M/0/1/0/all/0/1">Mohammad Dehghani</a>, <a href="http://arxiv.org/find/eess/1/au:+Yazdanparast_Z/0/1/0/all/0/1">Zahra Yazdanparast</a></p>
<p>The COVID-19 pandemic has disrupted the global economy and people's daily
lives in unprecedented ways. To make appropriate decisions, it is necessary to
diagnose COVID-19 rapidly and accurately. Clinical decision making is
influenced by data collected from patients. With the aid of artificial
intelligence, COVID-19 has been diagnosed quickly by analyzing symptoms,
polymerase chain reaction (PCR), computed tomography scans, chest X-rays,
routine laboratory blood tests and even cough sounds. Furthermore, these data
can be used to predict a patient's morality, although there is a question about
which data makes the most accurate predictions. Therefore, this study consists
of two parts. Our first objective is to examine whether machine learning
algorithms can predict the outcome of COVID-19 cases (recovery or death), based
on the features present in the dataset. In the second part of the research, we
investigated the impact of clinical and RT-PCR on prediction of recovery and
decease to determine which one is more reliable. We defined four stages with
different feature sets and use six machine learning methods to build prediction
model. With an accuracy of 78.7%, random forest showed promising results for
predicting death and recovery of patients. Based on this, it appears that
recovery and decease of patients are predictable using machine learning. For
second objective, results indicate that clinical alone (without using RT-PCR),
trained with AdaBoost algorithm, is the most accurate with an accuracy of
82.1%. This study can provide guidance for medical professionals in the event
of a crisis or outbreak similar to COVID-19.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13928">Parameter Exchange for Robust Dynamic Domain Generalization. (arXiv:2311.13928v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1">Luojun Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1">Zhifeng Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1">Zhishu Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yuanlong Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Weijie Chen</a></p>
<p>Agnostic domain shift is the main reason of model degradation on the unknown
target domains, which brings an urgent need to develop Domain Generalization
(DG). Recent advances at DG use dynamic networks to achieve training-free
adaptation on the unknown target domains, termed Dynamic Domain Generalization
(DDG), which compensates for the lack of self-adaptability in static models
with fixed weights. The parameters of dynamic networks can be decoupled into a
static and a dynamic component, which are designed to learn domain-invariant
and domain-specific features, respectively. Based on the existing arts, in this
work, we try to push the limits of DDG by disentangling the static and dynamic
components more thoroughly from an optimization perspective. Our main
consideration is that we can enable the static component to learn
domain-invariant features more comprehensively by augmenting the
domain-specific information. As a result, the more comprehensive
domain-invariant features learned by the static component can then enforce the
dynamic component to focus more on learning adaptive domain-specific features.
To this end, we propose a simple yet effective Parameter Exchange (PE) method
to perturb the combination between the static and dynamic components. We
optimize the model using the gradients from both the perturbed and
non-perturbed feed-forward jointly to implicitly achieve the aforementioned
disentanglement. In this way, the two components can be optimized in a
mutually-beneficial manner, which can resist the agnostic domain shifts and
improve the self-adaptability on the unknown target domain. Extensive
experiments show that PE can be easily plugged into existing dynamic networks
to improve their generalization ability without bells and whistles.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13929">MetaFBP: Learning to Learn High-Order Predictor for Personalized Facial Beauty Prediction. (arXiv:2311.13929v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1">Luojun Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1">Zhifeng Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1">Jia-Li Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qipeng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yuanlong Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Weijie Chen</a></p>
<p>Predicting individual aesthetic preferences holds significant practical
applications and academic implications for human society. However, existing
studies mainly focus on learning and predicting the commonality of facial
attractiveness, with little attention given to Personalized Facial Beauty
Prediction (PFBP). PFBP aims to develop a machine that can adapt to individual
aesthetic preferences with only a few images rated by each user. In this paper,
we formulate this task from a meta-learning perspective that each user
corresponds to a meta-task. To address such PFBP task, we draw inspiration from
the human aesthetic mechanism that visual aesthetics in society follows a
Gaussian distribution, which motivates us to disentangle user preferences into
a commonality and an individuality part. To this end, we propose a novel
MetaFBP framework, in which we devise a universal feature extractor to capture
the aesthetic commonality and then optimize to adapt the aesthetic
individuality by shifting the decision boundary of the predictor via a
meta-learning mechanism. Unlike conventional meta-learning methods that may
struggle with slow adaptation or overfitting to tiny support sets, we propose a
novel approach that optimizes a high-order predictor for fast adaptation. In
order to validate the performance of the proposed method, we build several PFBP
benchmarks by using existing facial beauty prediction datasets rated by
numerous users. Extensive experiments on these benchmarks demonstrate the
effectiveness of the proposed MetaFBP method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13930">Periodically Exchange Teacher-Student for Source-Free Object Detection. (arXiv:2311.13930v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qipeng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1">Luojun Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1">Zhifeng Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhifeng Yang</a></p>
<p>Source-free object detection (SFOD) aims to adapt the source detector to
unlabeled target domain data in the absence of source domain data. Most SFOD
methods follow the same self-training paradigm using mean-teacher (MT)
framework where the student model is guided by only one single teacher model.
However, such paradigm can easily fall into a training instability problem that
when the teacher model collapses uncontrollably due to the domain shift, the
student model also suffers drastic performance degradation. To address this
issue, we propose the Periodically Exchange Teacher-Student (PETS) method, a
simple yet novel approach that introduces a multiple-teacher framework
consisting of a static teacher, a dynamic teacher, and a student model. During
the training phase, we periodically exchange the weights between the static
teacher and the student model. Then, we update the dynamic teacher using the
moving average of the student model that has already been exchanged by the
static teacher. In this way, the dynamic teacher can integrate knowledge from
past periods, effectively reducing error accumulation and enabling a more
stable training process within the MT-based framework. Further, we develop a
consensus mechanism to merge the predictions of two teacher models to provide
higher-quality pseudo labels for student model. Extensive experiments on
multiple SFOD benchmarks show that the proposed method achieves
state-of-the-art performance compared with other related methods, demonstrating
the effectiveness and superiority of our method on SFOD task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13934">Robustness-Reinforced Knowledge Distillation with Correlation Distance and Network Pruning. (arXiv:2311.13934v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Seonghak Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Ham_G/0/1/0/all/0/1">Gyeongdo Ham</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1">Yucheol Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1">Daeshik Kim</a></p>
<p>The improvement in the performance of efficient and lightweight models (i.e.,
the student model) is achieved through knowledge distillation (KD), which
involves transferring knowledge from more complex models (i.e., the teacher
model). However, most existing KD techniques rely on Kullback-Leibler (KL)
divergence, which has certain limitations. First, if the teacher distribution
has high entropy, the KL divergence's mode-averaging nature hinders the
transfer of sufficient target information. Second, when the teacher
distribution has low entropy, the KL divergence tends to excessively focus on
specific modes, which fails to convey an abundant amount of valuable knowledge
to the student. Consequently, when dealing with datasets that contain numerous
confounding or challenging samples, student models may struggle to acquire
sufficient knowledge, resulting in subpar performance. Furthermore, in previous
KD approaches, we observed that data augmentation, a technique aimed at
enhancing a model's generalization, can have an adverse impact. Therefore, we
propose a Robustness-Reinforced Knowledge Distillation (R2KD) that leverages
correlation distance and network pruning. This approach enables KD to
effectively incorporate data augmentation for performance improvement.
Extensive experiments on various datasets, including CIFAR-100, FGVR,
TinyImagenet, and ImageNet, demonstrate our method's superiority over current
state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13954">Electric Network Frequency Optical Sensing Devices. (arXiv:2311.13954v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Moysiadis_C/0/1/0/all/0/1">Christos Moysiadis</a>, <a href="http://arxiv.org/find/cs/1/au:+Karantaidis_G/0/1/0/all/0/1">Georgios Karantaidis</a>, <a href="http://arxiv.org/find/cs/1/au:+Kotropoulos_C/0/1/0/all/0/1">Constantine Kotropoulos</a></p>
<p>Electric Network Frequency (ENF) acts as a fingerprint in multimedia
forensics applications. In indoor environments, ENF variations affect the
intensity of light sources connected to power mains. Accordingly, the light
intensity variations captured by sensing devices can be exploited to estimate
the ENF. A first optical sensing device based on a photodiode is developed for
capturing ENF variations in indoor lighting environments. In addition, a device
that captures the ENF directly from power mains is implemented. This device
serves as a ground truth ENF collector. Video recordings captured by a camera
are also employed to estimate the ENF. The camera serves as a second optical
sensor. The factors affecting the ENF estimation are thoroughly studied. The
maximum correlation coefficient between the ENF estimated by the two optical
sensors and that estimated directly from power mains is used to measure the
estimation accuracy. The paper's major contribution is in the disclosure of
extensive experimental evidence on ENF estimation in scenes ranging from static
ones capturing a white wall to non-static ones, including human activity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13958">High-Order Tensor Recovery with A Tensor $U_1$ Norm. (arXiv:2311.13958v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Zheng_J/0/1/0/all/0/1">Jingjing Zheng</a>, <a href="http://arxiv.org/find/stat/1/au:+Wang_W/0/1/0/all/0/1">Wenzhe Wang</a>, <a href="http://arxiv.org/find/stat/1/au:+Zhang_X/0/1/0/all/0/1">Xiaoqin Zhang</a>, <a href="http://arxiv.org/find/stat/1/au:+Cao_Y/0/1/0/all/0/1">Yankai Cao</a>, <a href="http://arxiv.org/find/stat/1/au:+Jiang_X/0/1/0/all/0/1">Xianta Jiang</a></p>
<p>Recently, numerous tensor SVD (t-SVD)-based tensor recovery methods have
emerged, showing promise in processing visual data. However, these methods
often suffer from performance degradation when confronted with high-order
tensor data exhibiting non-smooth changes, commonly observed in real-world
scenarios but ignored by the traditional t-SVD-based methods. Our objective in
this study is to provide an effective tensor recovery technique for handling
non-smooth changes in tensor data and efficiently explore the correlations of
high-order tensor data across its various dimensions without introducing
numerous variables and weights. To this end, we introduce a new tensor
decomposition and a new tensor norm called the Tensor $U_1$ norm. We utilize
these novel techniques in solving the problem of high-order tensor completion
problem and provide theoretical guarantees for the exact recovery of the
resulting tensor completion models. An optimization algorithm is proposed to
solve the resulting tensor completion model iteratively by combining the
proximal algorithm with the Alternating Direction Method of Multipliers.
Theoretical analysis showed the convergence of the algorithm to the
Karush-Kuhn-Tucker (KKT) point of the optimization problem. Numerical
experiments demonstrated the effectiveness of the proposed method in high-order
tensor completion, especially for tensor data with non-smooth changes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13959">RankFeat\&amp;RankWeight: Rank-1 Feature/Weight Removal for Out-of-distribution Detection. (arXiv:2311.13959v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Yue Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1">Nicu Sebe</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wei Wang</a></p>
<p>The task of out-of-distribution (OOD) detection is crucial for deploying
machine learning models in real-world settings. In this paper, we observe that
the singular value distributions of the in-distribution (ID) and OOD features
are quite different: the OOD feature matrix tends to have a larger dominant
singular value than the ID feature, and the class predictions of OOD samples
are largely determined by it. This observation motivates us to propose
\texttt{RankFeat}, a simple yet effective \emph{post hoc} approach for OOD
detection by removing the rank-1 matrix composed of the largest singular value
and the associated singular vectors from the high-level feature.
\texttt{RankFeat} achieves \emph{state-of-the-art} performance and reduces the
average false positive rate (FPR95) by 17.90\% compared with the previous best
method. The success of \texttt{RankFeat} motivates us to investigate whether a
similar phenomenon would exist in the parameter matrices of neural networks. We
thus propose \texttt{RankWeight} which removes the rank-1 weight from the
parameter matrices of a single deep layer. Our \texttt{RankWeight}is also
\emph{post hoc} and only requires computing the rank-1 matrix once. As a
standalone approach, \texttt{RankWeight} has very competitive performance
against other methods across various backbones. Moreover, \texttt{RankWeight}
enjoys flexible compatibility with a wide range of OOD detection methods. The
combination of \texttt{RankWeight} and \texttt{RankFeat} refreshes the new
\emph{state-of-the-art} performance, achieving the FPR95 as low as 16.13\% on
the ImageNet-1k benchmark. Extensive ablation studies and comprehensive
theoretical analyses are presented to support the empirical results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13963">Investigating the use of publicly available natural videos to learn Dynamic MR image reconstruction. (arXiv:2311.13963v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Jaubert_O/0/1/0/all/0/1">Olivier Jaubert</a>, <a href="http://arxiv.org/find/eess/1/au:+Pascale_M/0/1/0/all/0/1">Michele Pascale</a>, <a href="http://arxiv.org/find/eess/1/au:+Montalt_Tordera_J/0/1/0/all/0/1">Javier Montalt-Tordera</a>, <a href="http://arxiv.org/find/eess/1/au:+Akesson_J/0/1/0/all/0/1">Julius Akesson</a>, <a href="http://arxiv.org/find/eess/1/au:+Virsinskaite_R/0/1/0/all/0/1">Ruta Virsinskaite</a>, <a href="http://arxiv.org/find/eess/1/au:+Knight_D/0/1/0/all/0/1">Daniel Knight</a>, <a href="http://arxiv.org/find/eess/1/au:+Arridge_S/0/1/0/all/0/1">Simon Arridge</a>, <a href="http://arxiv.org/find/eess/1/au:+Steeden_J/0/1/0/all/0/1">Jennifer Steeden</a>, <a href="http://arxiv.org/find/eess/1/au:+Muthurangu_V/0/1/0/all/0/1">Vivek Muthurangu</a></p>
<p>Purpose: To develop and assess a deep learning (DL) pipeline to learn dynamic
MR image reconstruction from publicly available natural videos (Inter4K).
</p>
<p>Materials and Methods: Learning was performed for a range of DL architectures
(VarNet, 3D UNet, FastDVDNet) and corresponding sampling patterns (Cartesian,
radial, spiral) either from true multi-coil cardiac MR data (N=692) or from
pseudo-MR data simulated from Inter4K natural videos (N=692). Real-time
undersampled dynamic MR images were reconstructed using DL networks trained
with cardiac data and natural videos, and compressed sensing (CS). Differences
were assessed in simulations (N=104 datasets) in terms of MSE, PSNR, and SSIM
and prospectively for cardiac (short axis, four chambers, N=20) and speech
(N=10) data in terms of subjective image quality ranking, SNR and Edge
sharpness. Friedman Chi Square tests with post-hoc Nemenyi analysis were
performed to assess statistical significance.
</p>
<p>Results: For all simulation metrics, DL networks trained with cardiac data
outperformed DL networks trained with natural videos, which outperformed CS
(p&lt;0.05). However, in prospective experiments DL reconstructions using both
training datasets were ranked similarly (and higher than CS) and presented no
statistical differences in SNR and Edge Sharpness for most conditions.
Additionally, high SSIM was measured between the DL methods with cardiac data
and natural videos (SSIM&gt;0.85).
</p>
<p>Conclusion: The developed pipeline enabled learning dynamic MR reconstruction
from natural videos preserving DL reconstruction advantages such as high
quality fast and ultra-fast reconstructions while overcoming some limitations
(data scarcity or sharing). The natural video dataset, code and pre-trained
networks are made readily available on github.
</p>
<p>Key Words: real-time; dynamic MRI; deep learning; image reconstruction;
machine learning;
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13964">Deep Interactive Segmentation of Medical Images: A Systematic Review and Taxonomy. (arXiv:2311.13964v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Marinov_Z/0/1/0/all/0/1">Zdravko Marinov</a>, <a href="http://arxiv.org/find/eess/1/au:+Jager_P/0/1/0/all/0/1">Paul F. J&#xe4;ger</a>, <a href="http://arxiv.org/find/eess/1/au:+Egger_J/0/1/0/all/0/1">Jan Egger</a>, <a href="http://arxiv.org/find/eess/1/au:+Kleesiek_J/0/1/0/all/0/1">Jens Kleesiek</a>, <a href="http://arxiv.org/find/eess/1/au:+Stiefelhagen_R/0/1/0/all/0/1">Rainer Stiefelhagen</a></p>
<p>Interactive segmentation is a crucial research area in medical image analysis
aiming to boost the efficiency of costly annotations by incorporating human
feedback. This feedback takes the form of clicks, scribbles, or masks and
allows for iterative refinement of the model output so as to efficiently guide
the system towards the desired behavior. In recent years, deep learning-based
approaches have propelled results to a new level causing a rapid growth in the
field with 121 methods proposed in the medical imaging domain alone. In this
review, we provide a structured overview of this emerging field featuring a
comprehensive taxonomy, a systematic review of existing methods, and an
in-depth analysis of current practices. Based on these contributions, we
discuss the challenges and opportunities in the field. For instance, we find
that there is a severe lack of comparison across methods which needs to be
tackled by standardized baselines and benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13976">Low Latency Instance Segmentation by Continuous Clustering for Rotating LiDAR Sensors. (arXiv:2311.13976v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Reich_A/0/1/0/all/0/1">Andreas Reich</a>, <a href="http://arxiv.org/find/cs/1/au:+Wuensche_H/0/1/0/all/0/1">Hans-Joachim Wuensche</a></p>
<p>Low-latency instance segmentation of LiDAR point clouds is crucial in
real-world applications because it serves as an initial and frequently-used
building block in a robot's perception pipeline, where every task adds further
delay. Particularly in dynamic environments, this total delay can result in
significant positional offsets of dynamic objects, as seen in highway
scenarios. To address this issue, we employ continuous clustering of obstacle
points in order to obtain an instance-segmented point cloud. Unlike most
existing approaches, which use a full revolution of the LiDAR sensor, we
process the data stream in a continuous and seamless fashion. More
specifically, each column of a range image is processed as soon it is
available. Obstacle points are clustered to existing instances in real-time and
it is checked at a high-frequency which instances are completed and are ready
to be published. An additional advantage is that no problematic discontinuities
between the points of the start and the end of a scan are observed. In this
work we describe the two-layered data structure and the corresponding algorithm
for continuous clustering, which is able to cluster the incoming data in real
time. We explain the importance of a large perceptive field of view.
Furthermore, we describe and evaluate important architectural design choices,
which could be relevant to design an architecture for deep learning based
low-latency instance segmentation. We are publishing the source code at
https://github.com/UniBwTAS/continuous_clustering.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13986">FViT-Grasp: Grasping Objects With Using Fast Vision Transformers. (arXiv:2311.13986v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yenicesu_A/0/1/0/all/0/1">Arda Sarp Yenicesu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cicek_B/0/1/0/all/0/1">Berk Cicek</a>, <a href="http://arxiv.org/find/cs/1/au:+Oguz_O/0/1/0/all/0/1">Ozgur S.Oguz</a></p>
<p>This study addresses the challenge of manipulation, a prominent issue in
robotics. We have devised a novel methodology for swiftly and precisely
identifying the optimal grasp point for a robot to manipulate an object. Our
approach leverages a Fast Vision Transformer (FViT), a type of neural network
designed for processing visual data and predicting the most suitable grasp
location. Demonstrating state-of-the-art performance in terms of speed while
maintaining a high level of accuracy, our method holds promise for potential
deployment in real-time robotic grasping applications. We believe that this
study provides a baseline for future research in vision-based robotic grasp
applications. Its high speed and accuracy bring researchers closer to real-life
applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13993">EIGEN: Expert-Informed Joint Learning Aggregation for High-Fidelity Information Extraction from Document Images. (arXiv:2311.13993v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1">Abhishek Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Subramanian_V/0/1/0/all/0/1">Venkatapathy Subramanian</a>, <a href="http://arxiv.org/find/cs/1/au:+Maheshwari_A/0/1/0/all/0/1">Ayush Maheshwari</a>, <a href="http://arxiv.org/find/cs/1/au:+Narayan_P/0/1/0/all/0/1">Pradeep Narayan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shetty_D/0/1/0/all/0/1">Devi Prasad Shetty</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1">Ganesh Ramakrishnan</a></p>
<p>Information Extraction (IE) from document images is challenging due to the
high variability of layout formats. Deep models such as LayoutLM and BROS have
been proposed to address this problem and have shown promising results.
However, they still require a large amount of field-level annotations for
training these models. Other approaches using rule-based methods have also been
proposed based on the understanding of the layout and semantics of a form such
as geometric position, or type of the fields, etc. In this work, we propose a
novel approach, EIGEN (Expert-Informed Joint Learning aGgrEatioN), which
combines rule-based methods with deep learning models using data programming
approaches to circumvent the requirement of annotation of large amounts of
training data. Specifically, EIGEN consolidates weak labels induced from
multiple heuristics through generative models and use them along with a small
number of annotated labels to jointly train a deep model. In our framework, we
propose the use of labeling functions that include incorporating contextual
information thus capturing the visual and language context of a word for
accurate categorization. We empirically show that our EIGEN framework can
significantly improve the performance of state-of-the-art deep models with the
availability of very few labeled data instances. The source code is available
at
https://github.com/ayushayush591/EIGEN-High-Fidelity-Extraction-Document-Images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13997">GRJointNET: Synergistic Completion and Part Segmentation on 3D Incomplete Point Clouds. (arXiv:2311.13997v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gurses_Y/0/1/0/all/0/1">Yigit Gurses</a>, <a href="http://arxiv.org/find/cs/1/au:+Taspinar_M/0/1/0/all/0/1">Melisa Taspinar</a>, <a href="http://arxiv.org/find/cs/1/au:+Yurt_M/0/1/0/all/0/1">Mahmut Yurt</a>, <a href="http://arxiv.org/find/cs/1/au:+Ozer_S/0/1/0/all/0/1">Sedat Ozer</a></p>
<p>Segmentation of three-dimensional (3D) point clouds is an important task for
autonomous systems. However, success of segmentation algorithms depends greatly
on the quality of the underlying point clouds (resolution, completeness etc.).
In particular, incomplete point clouds might reduce a downstream model's
performance. GRNet is proposed as a novel and recent deep learning solution to
complete point clouds, but it is not capable of part segmentation. On the other
hand, our proposed solution, GRJointNet, is an architecture that can perform
joint completion and segmentation on point clouds as a successor of GRNet.
Features extracted for the two tasks are also utilized by each other to
increase the overall performance. We evaluated our proposed network on the
ShapeNet-Part dataset and compared its performance to GRNet. Our results
demonstrate GRJointNet can outperform GRNet on point completion. It should also
be noted that GRNet is not capable of segmentation while GRJointNet is. This
study1, therefore, holds a promise to enhance practicality and utility of point
clouds in 3D vision for autonomous systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14006">High-resolution Population Maps Derived from Sentinel-1 and Sentinel-2. (arXiv:2311.14006v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Metzger_N/0/1/0/all/0/1">Nando Metzger</a>, <a href="http://arxiv.org/find/cs/1/au:+Daudt_R/0/1/0/all/0/1">Rodrigo Caye Daudt</a>, <a href="http://arxiv.org/find/cs/1/au:+Tuia_D/0/1/0/all/0/1">Devis Tuia</a>, <a href="http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1">Konrad Schindler</a></p>
<p>Detailed population maps play an important role in diverse fields ranging
from humanitarian action to urban planning. Generating such maps in a timely
and scalable manner presents a challenge, especially in data-scarce regions. To
address it we have developed POPCORN, a population mapping method whose only
inputs are free, globally available satellite images from Sentinel-1 and
Sentinel-2; and a small number of aggregate population counts over coarse
census districts for calibration. Despite the minimal data requirements our
approach surpasses the mapping accuracy of existing schemes, including several
that rely on building footprints derived from high-resolution imagery. E.g., we
were able to produce population maps for Rwanda with 100m GSD based on less
than 400 regional census counts. In Kigali, those maps reach an $R^2$ score of
66% w.r.t. a ground truth reference map, with an average error of only $\pm$10
inhabitants/ha. Conveniently, POPCORN retrieves explicit maps of built-up areas
and of local building occupancy rates, making the mapping process interpretable
and offering additional insights, for instance about the distribution of
built-up, but unpopulated areas, e.g., industrial warehouses. Moreover, we find
that, once trained, the model can be applied repeatedly to track population
changes; and that it can be transferred to geographically similar regions,
e.g., from Uganda to Rwanda). With our work we aim to democratize access to
up-to-date and high-resolution population maps, recognizing that some regions
faced with particularly strong population dynamics may lack the resources for
costly micro-census campaigns.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14012">Shadow: A Novel Loss Function for Efficient Training in Siamese Networks. (arXiv:2311.14012v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1">Alif Elham Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1">Mohammad Junayed Hasan</a>, <a href="http://arxiv.org/find/cs/1/au:+Anjum_H/0/1/0/all/0/1">Humayra Anjum</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohammed_N/0/1/0/all/0/1">Nabeel Mohammed</a></p>
<p>Despite significant recent advances in similarity detection tasks, existing
approaches pose substantial challenges under memory constraints. One of the
primary reasons for this is the use of computationally expensive metric
learning loss functions such as Triplet Loss in Siamese networks. In this
paper, we present a novel loss function called Shadow Loss that compresses the
dimensions of an embedding space during loss calculation without loss of
performance. The distance between the projections of the embeddings is learned
from inputs on a compact projection space where distances directly correspond
to a measure of class similarity. Projecting on a lower-dimension projection
space, our loss function converges faster, and the resulting classified image
clusters have higher inter-class and smaller intra-class distances. Shadow Loss
not only reduces embedding dimensions favoring memory constraint devices but
also consistently performs better than the state-of-the-art Triplet Margin Loss
by an accuracy of 5\%-10\% across diverse datasets. The proposed loss function
is also model agnostic, upholding its performance across several tested models.
Its effectiveness and robustness across balanced, imbalanced, medical, and
non-medical image datasets suggests that it is not specific to a particular
model or dataset but demonstrates superior performance consistently while using
less memory and computation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14024">Creating and Benchmarking a Synthetic Dataset for Cloud Optical Thickness Estimation. (arXiv:2311.14024v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pirinen_A/0/1/0/all/0/1">Aleksis Pirinen</a>, <a href="http://arxiv.org/find/cs/1/au:+Abid_N/0/1/0/all/0/1">Nosheen Abid</a>, <a href="http://arxiv.org/find/cs/1/au:+Paszkowsky_N/0/1/0/all/0/1">Nuria Agues Paszkowsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Timoudas_T/0/1/0/all/0/1">Thomas Ohlson Timoudas</a>, <a href="http://arxiv.org/find/cs/1/au:+Scheirer_R/0/1/0/all/0/1">Ronald Scheirer</a>, <a href="http://arxiv.org/find/cs/1/au:+Ceccobello_C/0/1/0/all/0/1">Chiara Ceccobello</a>, <a href="http://arxiv.org/find/cs/1/au:+Kovacs_G/0/1/0/all/0/1">Gy&#xf6;rgy Kov&#xe1;cs</a>, <a href="http://arxiv.org/find/cs/1/au:+Persson_A/0/1/0/all/0/1">Anders Persson</a></p>
<p>Cloud formations often obscure optical satellite-based monitoring of the
Earth's surface, thus limiting Earth observation (EO) activities such as land
cover mapping, ocean color analysis, and cropland monitoring. The integration
of machine learning (ML) methods within the remote sensing domain has
significantly improved performance on a wide range of EO tasks, including cloud
detection and filtering, but there is still much room for improvement. A key
bottleneck is that ML methods typically depend on large amounts of annotated
data for training, which is often difficult to come by in EO contexts. This is
especially true for the task of cloud optical thickness (COT) estimation. A
reliable estimation of COT enables more fine-grained and application-dependent
control compared to using pre-specified cloud categories, as is commonly done
in practice. To alleviate the COT data scarcity problem, in this work we
propose a novel synthetic dataset for COT estimation, where top-of-atmosphere
radiances have been simulated for 12 of the spectral bands of the
Multi-Spectral Instrument (MSI) sensor onboard Sentinel-2 platforms. These data
points have been simulated under consideration of different cloud types, COTs,
and ground surface and atmospheric profiles. Extensive experimentation of
training several ML models to predict COT from the measured reflectivity of the
spectral bands demonstrates the usefulness of our proposed dataset.
Generalization to real data is also demonstrated on two satellite image
datasets -- one that is publicly available, and one which we have collected and
annotated. The synthetic data, the newly collected real dataset, code and
models have been made publicly available at
https://github.com/aleksispi/ml-cloud-opt-thick.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14028">Continual Learning of Diffusion Models with Generative Distillation. (arXiv:2311.14028v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Masip_S/0/1/0/all/0/1">Sergi Masip</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_P/0/1/0/all/0/1">Pau Rodriguez</a>, <a href="http://arxiv.org/find/cs/1/au:+Tuytelaars_T/0/1/0/all/0/1">Tinne Tuytelaars</a>, <a href="http://arxiv.org/find/cs/1/au:+Ven_G/0/1/0/all/0/1">Gido M. van de Ven</a></p>
<p>Diffusion models are powerful generative models that achieve state-of-the-art
performance in tasks such as image synthesis. However, training them demands
substantial amounts of data and computational resources. Continual learning
would allow for incrementally learning new tasks and accumulating knowledge,
thus reusing already trained models would be possible. One potentially suitable
approach is generative replay, where a copy of a generative model trained on
previous tasks produces synthetic data that are interleaved with data from the
current task. However, standard generative replay applied to diffusion models
results in a catastrophic loss in denoising capabilities. In this paper, we
propose generative distillation, an approach that distils the entire reverse
process of a diffusion model. We demonstrate that our approach significantly
improves the continual learning performance of generative replay with only a
moderate increase in the computational costs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14029">Understanding the Vulnerability of CLIP to Image Compression. (arXiv:2311.14029v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Cangxiong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1">Vinay P. Namboodiri</a>, <a href="http://arxiv.org/find/cs/1/au:+Padget_J/0/1/0/all/0/1">Julian Padget</a></p>
<p>CLIP is a widely used foundational vision-language model that is used for
zero-shot image recognition and other image-text alignment tasks. We
demonstrate that CLIP is vulnerable to change in image quality under
compression. This surprising result is further analysed using an attribution
method-Integrated Gradients. Using this attribution method, we are able to
better understand both quantitatively and qualitatively exactly the nature in
which the compression affects the zero-shot recognition accuracy of this model.
We evaluate this extensively on CIFAR-10 and STL-10. Our work provides the
basis to understand this vulnerability of CLIP and can help us develop more
effective methods to improve the robustness of CLIP and other vision-language
models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14049">Assessment of Deep Learning Segmentation for Real-Time Free-Breathing Cardiac Magnetic Resonance Imaging. (arXiv:2311.14049v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Schilling_M/0/1/0/all/0/1">Martin Schilling</a>, <a href="http://arxiv.org/find/eess/1/au:+Unterberg_Buchwald_C/0/1/0/all/0/1">Christina Unterberg-Buchwald</a>, <a href="http://arxiv.org/find/eess/1/au:+Lotz_J/0/1/0/all/0/1">Joachim Lotz</a>, <a href="http://arxiv.org/find/eess/1/au:+Uecker_M/0/1/0/all/0/1">Martin Uecker</a></p>
<p>In recent years, a variety of deep learning networks for cardiac MRI (CMR)
segmentation have been developed and analyzed. However, nearly all of them are
focused on cine CMR under breathold. In this work, accuracy of deep learning
methods is assessed for volumetric analysis (via segmentation) of the left
ventricle in real-time free-breathing CMR at rest and under exercise stress.
Data from healthy volunteers (n=15) for cine and real-time free-breathing CMR
were analyzed retrospectively. Segmentations of a commercial software (comDL)
and a freely available neural network (nnU-Net), were compared to a reference
created via the manual correction of comDL segmentation. Segmentation of left
ventricular endocardium (LV), left ventricular myocardium (MYO), and right
ventricle (RV) is evaluated for both end-systolic and end-diastolic phases and
analyzed with Dice's coefficient (DC). The volumetric analysis includes LV
end-diastolic volume (EDV), LV end-systolic volume (ESV), and LV ejection
fraction (EF). For cine CMR, nnU-Net and comDL achieve a DC above 0.95 for LV
and 0.9 for MYO, and RV. For real-time CMR, the accuracy of nnU-Net exceeds
that of comDL overall. For real-time CMR at rest, nnU-Net achieves a DC of 0.94
for LV, 0.89 for MYO, and 0.90 for RV; mean absolute differences between
nnU-Net and reference are 2.9mL for EDV, 3.5mL for ESV and 2.6% for EF. For
real-time CMR under exercise stress, nnU-Net achieves a DC of 0.92 for LV, 0.85
for MYO, and 0.83 for RV; mean absolute differences between nnU-Net and
reference are 11.4mL for EDV, 2.9mL for ESV and 3.6% for EF. Deep learning
methods designed or trained for cine CMR segmentation can perform well on
real-time CMR. For real-time free-breathing CMR at rest, the performance of
deep learning methods is comparable to inter-observer variability in cine CMR
and is usable or fully automatic segmentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14062">Hardware Resilience Properties of Text-Guided Image Classifiers. (arXiv:2311.14062v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wasim_S/0/1/0/all/0/1">Syed Talal Wasim</a>, <a href="http://arxiv.org/find/cs/1/au:+Saboka_K/0/1/0/all/0/1">Kabila Haile Saboka</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahmoud_A/0/1/0/all/0/1">Abdulrahman Mahmoud</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1">Salman Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Brooks_D/0/1/0/all/0/1">David Brooks</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_G/0/1/0/all/0/1">Gu-Yeon Wei</a></p>
<p>This paper presents a novel method to enhance the reliability of image
classification models during deployment in the face of transient hardware
errors. By utilizing enriched text embeddings derived from GPT-3 with question
prompts per class and CLIP pretrained text encoder, we investigate their impact
as an initialization for the classification layer. Our approach achieves a
remarkable $5.5\times$ average increase in hardware reliability (and up to 14x)
across various architectures in the most critical layer, with minimal accuracy
drop (0.3% on average) compared to baseline PyTorch models. Furthermore, our
method seamlessly integrates with any image classification backbone, showcases
results across various network architectures, decreases parameter and FLOPs
overhead, and follows a consistent training recipe. This research offers a
practical and efficient solution to bolster the robustness of image
classification models against hardware failures, with potential implications
for future studies in this domain. Our code and models are released at
https://github.com/TalalWasim/TextGuidedResilience.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14063">Do VSR Models Generalize Beyond LRS3?. (arXiv:2311.14063v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Djilali_Y/0/1/0/all/0/1">Yasser Abdelaziz Dahou Djilali</a>, <a href="http://arxiv.org/find/cs/1/au:+Narayan_S/0/1/0/all/0/1">Sanath Narayan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bihan_E/0/1/0/all/0/1">Eustache Le Bihan</a>, <a href="http://arxiv.org/find/cs/1/au:+Boussaid_H/0/1/0/all/0/1">Haithem Boussaid</a>, <a href="http://arxiv.org/find/cs/1/au:+Almazrouei_E/0/1/0/all/0/1">Ebtessam Almazrouei</a>, <a href="http://arxiv.org/find/cs/1/au:+Debbah_M/0/1/0/all/0/1">Merouane Debbah</a></p>
<p>The Lip Reading Sentences-3 (LRS3) benchmark has primarily been the focus of
intense research in visual speech recognition (VSR) during the last few years.
As a result, there is an increased risk of overfitting to its excessively used
test set, which is only one hour duration. To alleviate this issue, we build a
new VSR test set named WildVSR, by closely following the LRS3 dataset creation
processes. We then evaluate and analyse the extent to which the current VSR
models generalize to the new test data. We evaluate a broad range of publicly
available VSR models and find significant drops in performance on our test set,
compared to their corresponding LRS3 results. Our results suggest that the
increase in word error rates is caused by the models inability to generalize to
slightly harder and in the wild lip sequences than those found in the LRS3 test
set. Our new test benchmark is made public in order to enable future research
towards more robust VSR models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14064">HGCLIP: Exploring Vision-Language Models with Graph Representations for Hierarchical Understanding. (arXiv:2311.14064v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xia_P/0/1/0/all/0/1">Peng Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1">Xingtong Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1">Ming Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ju_L/0/1/0/all/0/1">Lie Ju</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhiyong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Duan_P/0/1/0/all/0/1">Peibo Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1">Zongyuan Ge</a></p>
<p>Object categories are typically organized into a multi-granularity taxonomic
hierarchy. When classifying categories at different hierarchy levels,
traditional uni-modal approaches focus primarily on image features, revealing
limitations in complex scenarios. Recent studies integrating Vision-Language
Models (VLMs) with class hierarchies have shown promise, yet they fall short of
fully exploiting the hierarchical relationships. These efforts are constrained
by their inability to perform effectively across varied granularity of
categories. To tackle this issue, we propose a novel framework (HGCLIP) that
effectively combines CLIP with a deeper exploitation of the Hierarchical class
structure via Graph representation learning. We explore constructing the class
hierarchy into a graph, with its nodes representing the textual or image
features of each category. After passing through a graph encoder, the textual
features incorporate hierarchical structure information, while the image
features emphasize class-aware features derived from prototypes through the
attention mechanism. Our approach demonstrates significant improvements on both
generic and fine-grained visual recognition benchmarks. Our codes are fully
available at https://github.com/richard-peng-xia/HGCLIP.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14073">Learning Saliency From Fixations. (arXiv:2311.14073v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Djilali_Y/0/1/0/all/0/1">Yasser Abdelaziz Dahou Djilali</a>, <a href="http://arxiv.org/find/cs/1/au:+McGuiness_K/0/1/0/all/0/1">Kevin McGuiness</a>, <a href="http://arxiv.org/find/cs/1/au:+OConnor_N/0/1/0/all/0/1">Noel O&#x27;Connor</a></p>
<p>We present a novel approach for saliency prediction in images, leveraging
parallel decoding in transformers to learn saliency solely from fixation maps.
Models typically rely on continuous saliency maps, to overcome the difficulty
of optimizing for the discrete fixation map. We attempt to replicate the
experimental setup that generates saliency datasets. Our approach treats
saliency prediction as a direct set prediction problem, via a global loss that
enforces unique fixations prediction through bipartite matching and a
transformer encoder-decoder architecture. By utilizing a fixed set of learned
fixation queries, the cross-attention reasons over the image features to
directly output the fixation points, distinguishing it from other modern
saliency predictors. Our approach, named Saliency TRansformer (SalTR), achieves
metric scores on par with state-of-the-art approaches on the Salicon and MIT300
benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14081">You Only Explain Once. (arXiv:2311.14081v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kelly_D/0/1/0/all/0/1">David A. Kelly</a>, <a href="http://arxiv.org/find/cs/1/au:+Chockler_H/0/1/0/all/0/1">Hana Chockler</a>, <a href="http://arxiv.org/find/cs/1/au:+Kroening_D/0/1/0/all/0/1">Daniel Kroening</a>, <a href="http://arxiv.org/find/cs/1/au:+Blake_N/0/1/0/all/0/1">Nathan Blake</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramaswamy_A/0/1/0/all/0/1">Aditi Ramaswamy</a>, <a href="http://arxiv.org/find/cs/1/au:+Navaratnarajah_M/0/1/0/all/0/1">Melane Navaratnarajah</a>, <a href="http://arxiv.org/find/cs/1/au:+Shivakumar_A/0/1/0/all/0/1">Aaditya Shivakumar</a></p>
<p>In this paper, we propose a new black-box explainability algorithm and tool,
YO-ReX, for efficient explanation of the outputs of object detectors. The new
algorithm computes explanations for all objects detected in the image
simultaneously. Hence, compared to the baseline, the new algorithm reduces the
number of queries by a factor of 10X for the case of ten detected objects. The
speedup increases further with with the number of objects. Our experimental
results demonstrate that YO-ReX can explain the outputs of YOLO with a
negligible overhead over the running time of YOLO. We also demonstrate similar
results for explaining SSD and Faster R-CNN. The speedup is achieved by
avoiding backtracking by combining aggressive pruning with a causal analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14084">AI-Generated Images Introduce Invisible Relevance Bias to Text-Image Retrieval. (arXiv:2311.14084v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Shicheng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_D/0/1/0/all/0/1">Danyang Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1">Liang Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1">Jingcheng Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jun Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1">Huawei Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1">Xueqi Cheng</a></p>
<p>With the advancement of generation models, AI-generated content (AIGC) is
becoming more realistic, flooding the Internet. A recent study suggests that
this phenomenon has elevated the issue of source bias in text retrieval for web
searches. Specifically, neural retrieval models tend to rank generated texts
higher than human-written texts. In this paper, we extend the study of this
bias to cross-modal retrieval. Firstly, we successfully construct a suitable
benchmark to explore the existence of the bias. Subsequent extensive
experiments on this benchmark reveal that AI-generated images introduce an
invisible relevance bias to text-image retrieval models. Specifically, our
experiments show that text-image retrieval models tend to rank the AI-generated
images higher than the real images, even though the AI-generated images do not
exhibit more visually relevant features to the query than real images. This
invisible relevance bias is prevalent across retrieval models with varying
training data and architectures. Furthermore, our subsequent exploration
reveals that the inclusion of AI-generated images in the training data of the
retrieval models exacerbates the invisible relevance bias. The above phenomenon
triggers a vicious cycle, which makes the invisible relevance bias become more
and more serious. To elucidate the potential causes of invisible relevance and
address the aforementioned issues, we introduce an effective training method
aimed at alleviating the invisible relevance bias. Subsequently, we apply our
proposed debiasing method to retroactively identify the causes of invisible
relevance, revealing that the AI-generated images induce the image encoder to
embed additional information into their representation. This information
exhibits a certain consistency across generated images with different semantics
and can make the retriever estimate a higher relevance score.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14086">Brain MRI Screening Tool with Federated Learning. (arXiv:2311.14086v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Stoklasa_R/0/1/0/all/0/1">Roman Stoklasa</a>, <a href="http://arxiv.org/find/eess/1/au:+Stathopoulos_I/0/1/0/all/0/1">Ioannis Stathopoulos</a>, <a href="http://arxiv.org/find/eess/1/au:+Karavasilis_E/0/1/0/all/0/1">Efstratios Karavasilis</a>, <a href="http://arxiv.org/find/eess/1/au:+Efstathopoulos_E/0/1/0/all/0/1">Efstathios Efstathopoulos</a>, <a href="http://arxiv.org/find/eess/1/au:+Dostal_M/0/1/0/all/0/1">Marek Dost&#xe1;l</a>, <a href="http://arxiv.org/find/eess/1/au:+Kerkovsky_M/0/1/0/all/0/1">Milo&#x161; Ke&#x159;kovsk&#xfd;</a>, <a href="http://arxiv.org/find/eess/1/au:+Kozubek_M/0/1/0/all/0/1">Michal Kozubek</a>, <a href="http://arxiv.org/find/eess/1/au:+Serio_L/0/1/0/all/0/1">Luigi Serio</a></p>
<p>In clinical practice, we often see significant delays between MRI scans and
the diagnosis made by radiologists, even for severe cases. In some cases, this
may be caused by the lack of additional information and clues, so even the
severe cases need to wait in the queue for diagnosis. This can be avoided if
there is an automatic software tool, which would supplement additional
information, alerting radiologists that the particular patient may be a severe
case.
</p>
<p>We are presenting an automatic brain MRI Screening Tool and we are
demonstrating its capabilities for detecting tumor-like pathologies. It is the
first version on the path toward a robust multi-pathology screening solution.
The tool supports Federated Learning, so multiple institutions may contribute
to the model without disclosing their private data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14090">Class Uncertainty: A Measure to Mitigate Class Imbalance. (arXiv:2311.14090v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Baltaci_Z/0/1/0/all/0/1">Z. S. Baltaci</a>, <a href="http://arxiv.org/find/cs/1/au:+Oksuz_K/0/1/0/all/0/1">K. Oksuz</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuzucu_S/0/1/0/all/0/1">S. Kuzucu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tezoren_K/0/1/0/all/0/1">K. Tezoren</a>, <a href="http://arxiv.org/find/cs/1/au:+Konar_B/0/1/0/all/0/1">B. K. Konar</a>, <a href="http://arxiv.org/find/cs/1/au:+Ozkan_A/0/1/0/all/0/1">A. Ozkan</a>, <a href="http://arxiv.org/find/cs/1/au:+Akbas_E/0/1/0/all/0/1">E. Akbas</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalkan_S/0/1/0/all/0/1">S. Kalkan</a></p>
<p>Class-wise characteristics of training examples affect the performance of
deep classifiers. A well-studied example is when the number of training
examples of classes follows a long-tailed distribution, a situation that is
likely to yield sub-optimal performance for under-represented classes. This
class imbalance problem is conventionally addressed by approaches relying on
the class-wise cardinality of training examples, such as data resampling. In
this paper, we demonstrate that considering solely the cardinality of classes
does not cover all issues causing class imbalance. To measure class imbalance,
we propose "Class Uncertainty" as the average predictive uncertainty of the
training examples, and we show that this novel measure captures the differences
across classes better than cardinality. We also curate SVCI-20 as a novel
dataset in which the classes have equal number of training examples but they
differ in terms of their hardness; thereby causing a type of class imbalance
which cannot be addressed by the approaches relying on cardinality. We
incorporate our "Class Uncertainty" measure into a diverse set of ten class
imbalance mitigation methods to demonstrate its effectiveness on long-tailed
datasets as well as on our SVCI-20. Code and datasets will be made available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14095">Video Anomaly Detection using GAN. (arXiv:2311.14095v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sethi_A/0/1/0/all/0/1">Anikeit Sethi</a>, <a href="http://arxiv.org/find/cs/1/au:+Saini_K/0/1/0/all/0/1">Krishanu Saini</a>, <a href="http://arxiv.org/find/cs/1/au:+Mididoddi_S/0/1/0/all/0/1">Sai Mounika Mididoddi</a></p>
<p>Accounting for the increased concern for public safety, automatic abnormal
event detection and recognition in a surveillance scene is crucial. It is a
current open study subject because of its intricacy and utility. The
identification of aberrant events automatically, it's a difficult undertaking
because everyone's idea of abnormality is different. A typical occurrence in
one circumstance could be seen as aberrant in another. Automatic anomaly
identification becomes particularly challenging in the surveillance footage
with a large crowd due to congestion and high occlusion. With the use of
machine learning techniques, this thesis study aims to offer the solution for
this use case so that human resources won't be required to keep an eye out for
any unusual activity in the surveillance system records. We have developed a
novel generative adversarial network (GAN) based anomaly detection model. This
model is trained such that it learns together about constructing a high
dimensional picture space and determining the latent space from the video's
context. The generator uses a residual Autoencoder architecture made up of a
multi-stage channel attention-based decoder and a two-stream, deep
convolutional encoder that can realise both spatial and temporal data. We have
also offered a technique for refining the GAN model that reduces training time
while also generalising the model by utilising transfer learning between
datasets. Using a variety of assessment measures, we compare our model to the
current state-of-the-art techniques on four benchmark datasets. The empirical
findings indicate that, in comparison to existing techniques, our network
performs favourably on all datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14097">ACT: Adversarial Consistency Models. (arXiv:2311.14097v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kong_F/0/1/0/all/0/1">Fei Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1">Jinhao Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Lichao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1">Hao Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">Renjing Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1">Hengtao Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiaofeng Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1">Xiaoshuang Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Kaidi Xu</a></p>
<p>Though diffusion models excel in image generation, their step-by-step
denoising leads to slow generation speeds. Consistency training addresses this
issue with single-step sampling but often produces lower-quality generations
and requires high training costs. In this paper, we show that optimizing
consistency training loss minimizes the Wasserstein distance between target and
generated distributions. As timestep increases, the upper bound accumulates
previous consistency training losses. Therefore, larger batch sizes are needed
to reduce both current and accumulated losses. We propose Adversarial
Consistency Training (ACT), which directly minimizes the Jensen-Shannon (JS)
divergence between distributions at each timestep using a discriminator.
Theoretically, ACT enhances generation quality, and convergence. By
incorporating a discriminator into the consistency training framework, our
method achieves improved FID scores on CIFAR10 and ImageNet 64$\times$64,
retains zero-shot image inpainting capabilities, and uses less than $1/6$ of
the original batch size and fewer than $1/2$ of the model parameters and
training steps compared to the baseline method, this leads to a substantial
reduction in resource consumption.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14146">Class Balanced Dynamic Acquisition for Domain Adaptive Semantic Segmentation using Active Learning. (arXiv:2311.14146v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schachtsiek_M/0/1/0/all/0/1">Marc Schachtsiek</a>, <a href="http://arxiv.org/find/cs/1/au:+Rossi_S/0/1/0/all/0/1">Simone Rossi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hannagan_T/0/1/0/all/0/1">Thomas Hannagan</a></p>
<p>Domain adaptive active learning is leading the charge in label-efficient
training of neural networks. For semantic segmentation, state-of-the-art models
jointly use two criteria of uncertainty and diversity to select training
labels, combined with a pixel-wise acquisition strategy. However, we show that
such methods currently suffer from a class imbalance issue which degrades their
performance for larger active learning budgets. We then introduce Class
Balanced Dynamic Acquisition (CBDA), a novel active learning method that
mitigates this issue, especially in high-budget regimes. The more balanced
labels increase minority class performance, which in turn allows the model to
outperform the previous baseline by 0.6, 1.7, and 2.4 mIoU for budgets of 5%,
10%, and 20%, respectively. Additionally, the focus on minority classes leads
to improvements of the minimum class performance of 0.5, 2.9, and 4.6 IoU
respectively. The top-performing model even exceeds the fully supervised
baseline, showing that a more balanced label than the entire ground truth can
be beneficial.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14148">Automated 3D Tumor Segmentation using Temporal Cubic PatchGAN (TCuP-GAN). (arXiv:2311.14148v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Mantha_K/0/1/0/all/0/1">Kameswara Bharadwaj Mantha</a>, <a href="http://arxiv.org/find/eess/1/au:+Sankar_R/0/1/0/all/0/1">Ramanakumar Sankar</a>, <a href="http://arxiv.org/find/eess/1/au:+Fortson_L/0/1/0/all/0/1">Lucy Fortson</a></p>
<p>Development of robust general purpose 3D segmentation frameworks using the
latest deep learning techniques is one of the active topics in various
bio-medical domains. In this work, we introduce Temporal Cubic PatchGAN
(TCuP-GAN), a volume-to-volume translational model that marries the concepts of
a generative feature learning framework with Convolutional Long Short-Term
Memory Networks (LSTMs), for the task of 3D segmentation. We demonstrate the
capabilities of our TCuP-GAN on the data from four segmentation challenges
(Adult Glioma, Meningioma, Pediatric Tumors, and Sub-Saharan Africa subset)
featured within the 2023 Brain Tumor Segmentation (BraTS) Challenge and
quantify its performance using LesionWise Dice similarity and $95\%$ Hausdorff
Distance metrics. We demonstrate the successful learning of our framework to
predict robust multi-class segmentation masks across all the challenges. This
benchmarking work serves as a stepping stone for future efforts towards
applying TCuP-GAN on other multi-class tasks such as multi-organelle
segmentation in electron microscopy imaging.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14155">GigaPose: Fast and Robust Novel Object Pose Estimation via One Correspondence. (arXiv:2311.14155v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1">Van Nguyen Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Groueix_T/0/1/0/all/0/1">Thibault Groueix</a>, <a href="http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1">Mathieu Salzmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Lepetit_V/0/1/0/all/0/1">Vincent Lepetit</a></p>
<p>We present GigaPose, a fast, robust, and accurate method for CAD-based novel
object pose estimation in RGB images. GigaPose first leverages discriminative
templates, rendered images of the CAD models, to recover the out-of-plane
rotation and then uses patch correspondences to estimate the four remaining
parameters. Our approach samples templates in only a two-degrees-of-freedom
space instead of the usual three and matches the input image to the templates
using fast nearest neighbor search in feature space, results in a speedup
factor of 38x compared to the state of the art. Moreover, GigaPose is
significantly more robust to segmentation errors. Our extensive evaluation on
the seven core datasets of the BOP challenge demonstrates that it achieves
state-of-the-art accuracy and can be seamlessly integrated with a refinement
method. Additionally, we show the potential of GigaPose with 3D models
predicted by recent work on 3D reconstruction from a single image, relaxing the
need for CAD models and making 6D pose object estimation much more convenient.
Our source code and trained models are publicly available at
https://github.com/nv-nguyen/gigaPose
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14175">Appearance-based gaze estimation enhanced with synthetic images using deep neural networks. (arXiv:2311.14175v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Herashchenko_D/0/1/0/all/0/1">Dmytro Herashchenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Farkas_I/0/1/0/all/0/1">Igor Farka&#x161;</a></p>
<p>Human eye gaze estimation is an important cognitive ingredient for successful
human-robot interaction, enabling the robot to read and predict human behavior.
We approach this problem using artificial neural networks and build a modular
system estimating gaze from separately cropped eyes, taking advantage of
existing well-functioning components for face detection (RetinaFace) and head
pose estimation (6DRepNet). Our proposed method does not require any special
hardware or infrared filters but uses a standard notebook-builtin RGB camera,
as often approached with appearance-based methods. Using the MetaHuman tool, we
also generated a large synthetic dataset of more than 57,000 human faces and
made it publicly available. The inclusion of this dataset (with eye gaze and
head pose information) on top of the standard Columbia Gaze dataset into
training the model led to better accuracy with a mean average error below two
degrees in eye pitch and yaw directions, which compares favourably to related
methods. We also verified the feasibility of our model by its preliminary
testing in real-world setting using the builtin 4K camera in NICO semi-humanoid
robot's eye.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14177">TCuPGAN: A novel framework developed for optimizing human-machine interactions in citizen science. (arXiv:2311.14177v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sankar_R/0/1/0/all/0/1">Ramanakumar Sankar</a>, <a href="http://arxiv.org/find/cs/1/au:+Mantha_K/0/1/0/all/0/1">Kameswara Mantha</a>, <a href="http://arxiv.org/find/cs/1/au:+Fortson_L/0/1/0/all/0/1">Lucy Fortson</a>, <a href="http://arxiv.org/find/cs/1/au:+Spiers_H/0/1/0/all/0/1">Helen Spiers</a>, <a href="http://arxiv.org/find/cs/1/au:+Pengo_T/0/1/0/all/0/1">Thomas Pengo</a>, <a href="http://arxiv.org/find/cs/1/au:+Mashek_D/0/1/0/all/0/1">Douglas Mashek</a>, <a href="http://arxiv.org/find/cs/1/au:+Mo_M/0/1/0/all/0/1">Myat Mo</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanders_M/0/1/0/all/0/1">Mark Sanders</a>, <a href="http://arxiv.org/find/cs/1/au:+Christensen_T/0/1/0/all/0/1">Trace Christensen</a>, <a href="http://arxiv.org/find/cs/1/au:+Salisbury_J/0/1/0/all/0/1">Jeffrey Salisbury</a>, <a href="http://arxiv.org/find/cs/1/au:+Trouille_L/0/1/0/all/0/1">Laura Trouille</a></p>
<p>In the era of big data in scientific research, there is a necessity to
leverage techniques which reduce human effort in labeling and categorizing
large datasets by involving sophisticated machine tools. To combat this
problem, we present a novel, general purpose model for 3D segmentation that
leverages patch-wise adversariality and Long Short-Term Memory to encode
sequential information. Using this model alongside citizen science projects
which use 3D datasets (image cubes) on the Zooniverse platforms, we propose an
iterative human-machine optimization framework where only a fraction of the 2D
slices from these cubes are seen by the volunteers. We leverage the patch-wise
discriminator in our model to provide an estimate of which slices within these
image cubes have poorly generalized feature representations, and
correspondingly poor machine performance. These images with corresponding
machine proposals would be presented to volunteers on Zooniverse for
correction, leading to a drastic reduction in the volunteer effort on citizen
science projects. We trained our model on ~2300 liver tissue 3D electron
micrographs. Lipid droplets were segmented within these images through human
annotation via the `Etch A Cell - Fat Checker' citizen science project, hosted
on the Zooniverse platform. In this work, we demonstrate this framework and the
selection methodology which resulted in a measured reduction in volunteer
effort by more than 60%. We envision this type of joint human-machine
partnership will be of great use on future Zooniverse projects.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2103.01208">Mind the box: $l_1$-APGD for sparse adversarial attacks on image classifiers. (arXiv:2103.01208v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Croce_F/0/1/0/all/0/1">Francesco Croce</a>, <a href="http://arxiv.org/find/cs/1/au:+Hein_M/0/1/0/all/0/1">Matthias Hein</a></p>
<p>We show that when taking into account also the image domain $[0,1]^d$,
established $l_1$-projected gradient descent (PGD) attacks are suboptimal as
they do not consider that the effective threat model is the intersection of the
$l_1$-ball and $[0,1]^d$. We study the expected sparsity of the steepest
descent step for this effective threat model and show that the exact projection
onto this set is computationally feasible and yields better performance.
Moreover, we propose an adaptive form of PGD which is highly effective even
with a small budget of iterations. Our resulting $l_1$-APGD is a strong
white-box attack showing that prior works overestimated their $l_1$-robustness.
Using $l_1$-APGD for adversarial training we get a robust classifier with SOTA
$l_1$-robustness. Finally, we combine $l_1$-APGD and an adaptation of the
Square Attack to $l_1$ into $l_1$-AutoAttack, an ensemble of attacks which
reliably assesses adversarial robustness for the threat model of $l_1$-ball
intersected with $[0,1]^d$.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.02205">Evaluating Object (mis)Detection from a Safety and Reliability Perspective: Discussion and Measures. (arXiv:2203.02205v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ceccarelli_A/0/1/0/all/0/1">Andrea Ceccarelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Montecchi_L/0/1/0/all/0/1">Leonardo Montecchi</a></p>
<p>We argue that object detectors in the safety critical domain should
prioritize detection of objects that are most likely to interfere with the
actions of the autonomous actor. Especially, this applies to objects that can
impact the actor's safety and reliability. To quantify the impact of object
(mis)detection on safety and reliability in the context of autonomous driving,
we propose new object detection measures that reward the correct identification
of objects that are most dangerous and most likely to affect driving decisions.
To achieve this, we build an object criticality model to reward the detection
of the objects based on proximity, orientation, and relative velocity with
respect to the subject vehicle. Then, we apply our model on the recent
autonomous driving dataset nuScenes, and we compare nine object detectors.
Results show that, in several settings, object detectors that perform best
according to the nuScenes ranking are not the preferable ones when the focus is
shifted on safety and reliability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.04838">CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation with Transformers. (arXiv:2203.04838v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiaming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Huayao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1">Kailun Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xinxin Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1">Ruiping Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1">Rainer Stiefelhagen</a></p>
<p>Scene understanding based on image segmentation is a crucial component of
autonomous vehicles. Pixel-wise semantic segmentation of RGB images can be
advanced by exploiting complementary features from the supplementary modality
(X-modality). However, covering a wide variety of sensors with a
modality-agnostic model remains an unresolved problem due to variations in
sensor characteristics among different modalities. Unlike previous
modality-specific methods, in this work, we propose a unified fusion framework,
CMX, for RGB-X semantic segmentation. To generalize well across different
modalities, that often include supplements as well as uncertainties, a unified
cross-modal interaction is crucial for modality fusion. Specifically, we design
a Cross-Modal Feature Rectification Module (CM-FRM) to calibrate bi-modal
features by leveraging the features from one modality to rectify the features
of the other modality. With rectified feature pairs, we deploy a Feature Fusion
Module (FFM) to perform sufficient exchange of long-range contexts before
mixing. To verify CMX, for the first time, we unify five modalities
complementary to RGB, i.e., depth, thermal, polarization, event, and LiDAR.
Extensive experiments show that CMX generalizes well to diverse multi-modal
fusion, achieving state-of-the-art performances on five RGB-Depth benchmarks,
as well as RGB-Thermal, RGB-Polarization, and RGB-LiDAR datasets. Besides, to
investigate the generalizability to dense-sparse data fusion, we establish an
RGB-Event semantic segmentation benchmark based on the EventScape dataset, on
which CMX sets the new state-of-the-art. The source code of CMX is publicly
available at https://github.com/huaaaliu/RGBX_Semantic_Segmentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2204.11291">Large Scale Time-Series Representation Learning via Simultaneous Low and High Frequency Feature Bootstrapping. (arXiv:2204.11291v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gorade_V/0/1/0/all/0/1">Vandan Gorade</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1">Azad Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishra_D/0/1/0/all/0/1">Deepak Mishra</a></p>
<p>Learning representation from unlabeled time series data is a challenging
problem. Most existing self-supervised and unsupervised approaches in the
time-series domain do not capture low and high-frequency features at the same
time. Further, some of these methods employ large scale models like
transformers or rely on computationally expensive techniques such as
contrastive learning. To tackle these problems, we propose a non-contrastive
self-supervised learning approach efficiently captures low and high-frequency
time-varying features in a cost-effective manner. Our method takes raw time
series data as input and creates two different augmented views for two branches
of the model, by randomly sampling the augmentations from same family.
Following the terminology of BYOL, the two branches are called online and
target network which allows bootstrapping of the latent representation. In
contrast to BYOL, where a backbone encoder is followed by multilayer perceptron
(MLP) heads, the proposed model contains additional temporal convolutional
network (TCN) heads. As the augmented views are passed through large kernel
convolution blocks of the encoder, the subsequent combination of MLP and TCN
enables an effective representation of low as well as high-frequency
time-varying features due to the varying receptive fields. The two modules (MLP
and TCN) act in a complementary manner. We train an online network where each
module learns to predict the outcome of the respective module of target network
branch. To demonstrate the robustness of our model we performed extensive
experiments and ablation studies on five real-world time-series datasets. Our
method achieved state-of-art performance on all five real-world datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.11952">3D helical CT Reconstruction with a Memory Efficient Learned Primal-Dual Architecture. (arXiv:2205.11952v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Rudzusika_J/0/1/0/all/0/1">Jevgenija Rudzusika</a>, <a href="http://arxiv.org/find/eess/1/au:+Bajic_B/0/1/0/all/0/1">Buda Baji&#x107;</a>, <a href="http://arxiv.org/find/eess/1/au:+Koehler_T/0/1/0/all/0/1">Thomas Koehler</a>, <a href="http://arxiv.org/find/eess/1/au:+Oktem_O/0/1/0/all/0/1">Ozan &#xd6;ktem</a></p>
<p>Deep learning based computed tomography (CT) reconstruction has demonstrated
outstanding performance on simulated 2D low-dose CT data. This applies in
particular to domain adapted neural networks, which incorporate a handcrafted
physics model for CT imaging. Empirical evidence shows that employing such
architectures reduces the demand for training data and improves upon
generalisation. However, their training requires large computational resources
that quickly become prohibitive in 3D helical CT, which is the most common
acquisition geometry used for medical imaging. Furthermore, clinical data also
comes with other challenges not accounted for in simulations, like errors in
flux measurement, resolution mismatch and, most importantly, the absence of the
real ground truth. The necessity to have a computationally feasible training
combined with the need to address these issues has made it difficult to
evaluate deep learning based reconstruction on clinical 3D helical CT. This
paper modifies a domain adapted neural network architecture, the Learned
Primal-Dual (LPD), so that it can be trained and applied to reconstruction in
this setting. We achieve this by splitting the helical trajectory into sections
and applying the unrolled LPD iterations to those sections sequentially. To the
best of our knowledge, this work is the first to apply an unrolled deep
learning architecture for reconstruction on full-sized clinical data, like
those in the Low dose CT image and projection data set (LDCT). Moreover,
training and testing is done on a single GPU card with 24GB of memory.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.01072">Dynamic Sub-Cluster-Aware Network for Few-Shot Skin Disease Classification. (arXiv:2207.01072v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+LI_S/0/1/0/all/0/1">Shuhan LI</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaomeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiaowei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1">Kwang-Ting Cheng</a></p>
<p>This paper addresses the problem of few-shot skin disease classification by
introducing a novel approach called the Sub-Cluster-Aware Network (SCAN) that
enhances accuracy in diagnosing rare skin diseases. The key insight motivating
the design of SCAN is the observation that skin disease images within a class
often exhibit multiple sub-clusters, characterized by distinct variations in
appearance. To improve the performance of few-shot learning, we focus on
learning a high-quality feature encoder that captures the unique sub-clustered
representations within each disease class, enabling better characterization of
feature distributions. Specifically, SCAN follows a dual-branch framework,
where the first branch learns class-wise features to distinguish different skin
diseases, and the second branch aims to learn features which can effectively
partition each class into several groups so as to preserve the sub-clustered
structure within each class. To achieve the objective of the second branch, we
present a cluster loss to learn image similarities via unsupervised clustering.
To ensure that the samples in each sub-cluster are from the same class, we
further design a purity loss to refine the unsupervised clustering results. We
evaluate the proposed approach on two public datasets for few-shot skin disease
classification. The experimental results validate that our framework
outperforms the state-of-the-art methods by around 2% to 5% in terms of
sensitivity, specificity, accuracy, and F1-score on the SD-198 and Derm7pt
datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.11209">Divide and Conquer: 3D Point Cloud Instance Segmentation With Point-Wise Binarization. (arXiv:2207.11209v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Weiguang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1">Yuyao Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Chaolong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1">Jianan Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1">Kaizhu Huang</a></p>
<p>Instance segmentation on point clouds is crucially important for 3D scene
understanding. Most SOTAs adopt distance clustering, which is typically
effective but does not perform well in segmenting adjacent objects with the
same semantic label (especially when they share neighboring points). Due to the
uneven distribution of offset points, these existing methods can hardly cluster
all instance points. To this end, we design a novel divide-and-conquer strategy
named PBNet that binarizes each point and clusters them separately to segment
instances. Our binary clustering divides offset instance points into two
categories: high and low density points (HPs vs. LPs). Adjacent objects can be
clearly separated by removing LPs, and then be completed and refined by
assigning LPs via a neighbor voting method. To suppress potential
over-segmentation, we propose to construct local scenes with the weight mask
for each instance. As a plug-in, the proposed binary clustering can replace
traditional distance clustering and lead to consistent performance gains on
many mainstream baselines. A series of experiments on ScanNetV2 and S3DIS
datasets indicate the superiority of our model. In particular, PBNet ranks
first on the ScanNetV2 official benchmark challenge, achieving the highest mAP.
Code will be available publicly at https://github.com/weiguangzhao/PBNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.05954">Automatically Score Tissue Images Like a Pathologist by Transfer Learning. (arXiv:2209.05954v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yan_I/0/1/0/all/0/1">Iris Yan</a></p>
<p>Cancer is the second leading cause of death in the world. Diagnosing cancer
early on can save many lives. Pathologists have to look at tissue microarray
(TMA) images manually to identify tumors, which can be time-consuming,
inconsistent and subjective. Existing automatic algorithms either have not
achieved the accuracy level of a pathologist or require substantial human
involvements. A major challenge is that TMA images with different shapes,
sizes, and locations can have the same score. Learning staining patterns in TMA
images requires a huge number of images, which are severely limited due to
privacy and regulation concerns in medical organizations. TMA images from
different cancer types may share certain common characteristics, but combining
them directly harms the accuracy due to heterogeneity in their staining
patterns. Transfer learning is an emerging learning paradigm that allows
borrowing strength from similar problems. However, existing approaches
typically require a large sample from similar learning problems, while TMA
images of different cancer types are often available in small sample size and
further existing algorithms are limited to transfer learning from one similar
problem. We propose a new transfer learning algorithm that could learn from
multiple related problems, where each problem has a small sample and can have a
substantially different distribution from the original one. The proposed
algorithm has made it possible to break the critical accuracy barrier (the 75%
accuracy level of pathologists), with a reported accuracy of 75.9% on breast
cancer TMA images from the Stanford Tissue Microarray Database. It is supported
by recent developments in transfer learning theory and empirical evidence in
clustering technology. This will allow pathologists to confidently adopt
automatic algorithms in recognizing tumors consistently with a higher accuracy
in real time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.13552">Perceptual Image Enhancement for Smartphone Real-Time Applications. (arXiv:2210.13552v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Conde_M/0/1/0/all/0/1">Marcos V. Conde</a>, <a href="http://arxiv.org/find/cs/1/au:+Vasluianu_F/0/1/0/all/0/1">Florin Vasluianu</a>, <a href="http://arxiv.org/find/cs/1/au:+Vazquez_Corral_J/0/1/0/all/0/1">Javier Vazquez-Corral</a>, <a href="http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1">Radu Timofte</a></p>
<p>Recent advances in camera designs and imaging pipelines allow us to capture
high-quality images using smartphones. However, due to the small size and lens
limitations of the smartphone cameras, we commonly find artifacts or
degradation in the processed images. The most common unpleasant effects are
noise artifacts, diffraction artifacts, blur, and HDR overexposure. Deep
learning methods for image restoration can successfully remove these artifacts.
However, most approaches are not suitable for real-time applications on mobile
devices due to their heavy computation and memory requirements. In this paper,
we propose LPIENet, a lightweight network for perceptual image enhancement,
with the focus on deploying it on smartphones. Our experiments show that, with
much fewer parameters and operations, our model can deal with the mentioned
artifacts and achieve competitive performance compared with state-of-the-art
methods on standard benchmarks. Moreover, to prove the efficiency and
reliability of our approach, we deployed the model directly on commercial
smartphones and evaluated its performance. Our model can process 2K resolution
images under 1 second in mid-level commercial smartphones.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.04927">DeepDC: Deep Distance Correlation as a Perceptual Image Quality Evaluator. (arXiv:2211.04927v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1">Hanwei Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Baoliang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1">Lingyu Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shiqi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1">Weisi Lin</a></p>
<p>ImageNet pre-trained deep neural networks (DNNs) show notable transferability
for building effective image quality assessment (IQA) models. Such a remarkable
byproduct has often been identified as an emergent property in previous
studies. In this work, we attribute such capability to the intrinsic
texture-sensitive characteristic that classifies images using texture features.
We fully exploit this characteristic to develop a novel full-reference IQA
(FR-IQA) model based exclusively on pre-trained DNN features. Specifically, we
compute the distance correlation, a highly promising yet relatively
under-investigated statistic, between reference and distorted images in the
deep feature domain. In addition, the distance correlation quantifies both
linear and nonlinear feature relationships, which is far beyond the widely used
first-order and second-order statistics in the feature space. We conduct
comprehensive experiments to demonstrate the superiority of the proposed
quality model on five standard IQA datasets, one perceptual similarity dataset,
two texture similarity datasets, and one geometric transformation dataset.
Moreover, we optimize the proposed model to generate a broad spectrum of
texture patterns, by treating the model as the style loss function for neural
style transfer (NST). Extensive experiments demonstrate that the proposed
texture synthesis and NST methods achieve the best quantitative and qualitative
results. We release our code at https://github.com/h4nwei/DeepDC.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.11744">Visual Dexterity: In-Hand Reorientation of Novel and Complex Object Shapes. (arXiv:2211.11744v3 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tippur_M/0/1/0/all/0/1">Megha Tippur</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Siyang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1">Vikash Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Adelson_E/0/1/0/all/0/1">Edward Adelson</a>, <a href="http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1">Pulkit Agrawal</a></p>
<p>In-hand object reorientation is necessary for performing many dexterous
manipulation tasks, such as tool use in less structured environments that
remain beyond the reach of current robots. Prior works built reorientation
systems assuming one or many of the following: reorienting only specific
objects with simple shapes, limited range of reorientation, slow or quasistatic
manipulation, simulation-only results, the need for specialized and costly
sensor suites, and other constraints which make the system infeasible for
real-world deployment. We present a general object reorientation controller
that does not make these assumptions. It uses readings from a single commodity
depth camera to dynamically reorient complex and new object shapes by any
rotation in real-time, with the median reorientation time being close to seven
seconds. The controller is trained using reinforcement learning in simulation
and evaluated in the real world on new object shapes not used for training,
including the most challenging scenario of reorienting objects held in the air
by a downward-facing hand that must counteract gravity during reorientation.
Our hardware platform only uses open-source components that cost less than five
thousand dollars. Although we demonstrate the ability to overcome assumptions
in prior work, there is ample scope for improving absolute performance. For
instance, the challenging duck-shaped object not used for training was dropped
in 56 percent of the trials. When it was not dropped, our controller reoriented
the object within 0.4 radians (23 degrees) 75 percent of the time. Videos are
available at: https://taochenshh.github.io/projects/visual-dexterity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.05911">Adaptive Self-Training for Object Detection. (arXiv:2212.05911v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vandeghen_R/0/1/0/all/0/1">Renaud Vandeghen</a>, <a href="http://arxiv.org/find/cs/1/au:+Louppe_G/0/1/0/all/0/1">Gilles Louppe</a>, <a href="http://arxiv.org/find/cs/1/au:+Droogenbroeck_M/0/1/0/all/0/1">Marc Van Droogenbroeck</a></p>
<p>Deep learning has emerged as an effective solution for solving the task of
object detection in images but at the cost of requiring large labeled datasets.
To mitigate this cost, semi-supervised object detection methods, which consist
in leveraging abundant unlabeled data, have been proposed and have already
shown impressive results. However, most of these methods require linking a
pseudo-label to a ground-truth object by thresholding. In previous works, this
threshold value is usually determined empirically, which is time consuming, and
only done for a single data distribution. When the domain, and thus the data
distribution, changes, a new and costly parameter search is necessary. In this
work, we introduce our method Adaptive Self-Training for Object Detection
(ASTOD), which is a simple yet effective teacher-student method. ASTOD
determines without cost a threshold value based directly on the ground value of
the score histogram. To improve the quality of the teacher predictions, we also
propose a novel pseudo-labeling procedure. We use different views of the
unlabeled images during the pseudo-labeling step to reduce the number of missed
predictions and thus obtain better candidate labels. Our teacher and our
student are trained separately, and our method can be used in an iterative
fashion by replacing the teacher by the student. On the MS-COCO dataset, our
method consistently performs favorably against state-of-the-art methods that do
not require a threshold parameter, and shows competitive results with methods
that require a parameter sweep search. Additional experiments with respect to a
supervised baseline on the DIOR dataset containing satellite images lead to
similar conclusions, and prove that it is possible to adapt the score threshold
automatically in self-training, regardless of the data distribution. The code
is available at https:// github.com/rvandeghen/ASTOD
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.13766">OVO: One-shot Vision Transformer Search with Online distillation. (arXiv:2212.13766v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1">Zimian Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1">Hengyue Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Niu_X/0/1/0/all/0/1">Xin Niu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dongsheng Li</a></p>
<p>Pure transformers have shown great potential for vision tasks recently.
However, their accuracy in small or medium datasets is not satisfactory.
Although some existing methods introduce a CNN as a teacher to guide the
training process by distillation, the gap between teacher and student networks
would lead to sub-optimal performance. In this work, we propose a new One-shot
Vision transformer search framework with Online distillation, namely OVO. OVO
samples sub-nets for both teacher and student networks for better distillation
results. Benefiting from the online distillation, thousands of subnets in the
supernet are well-trained without extra finetuning or retraining. In
experiments, OVO-Ti achieves 73.32% top-1 accuracy on ImageNet and 75.2% on
CIFAR-100, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.14460">Interpretable and intervenable ultrasonography-based machine learning models for pediatric appendicitis. (arXiv:2302.14460v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Marcinkevics_R/0/1/0/all/0/1">Ri&#x10d;ards Marcinkevi&#x10d;s</a>, <a href="http://arxiv.org/find/cs/1/au:+Wolfertstetter_P/0/1/0/all/0/1">Patricia Reis Wolfertstetter</a>, <a href="http://arxiv.org/find/cs/1/au:+Klimiene_U/0/1/0/all/0/1">Ugne Klimiene</a>, <a href="http://arxiv.org/find/cs/1/au:+Chin_Cheong_K/0/1/0/all/0/1">Kieran Chin-Cheong</a>, <a href="http://arxiv.org/find/cs/1/au:+Paschke_A/0/1/0/all/0/1">Alyssia Paschke</a>, <a href="http://arxiv.org/find/cs/1/au:+Zerres_J/0/1/0/all/0/1">Julia Zerres</a>, <a href="http://arxiv.org/find/cs/1/au:+Denzinger_M/0/1/0/all/0/1">Markus Denzinger</a>, <a href="http://arxiv.org/find/cs/1/au:+Niederberger_D/0/1/0/all/0/1">David Niederberger</a>, <a href="http://arxiv.org/find/cs/1/au:+Wellmann_S/0/1/0/all/0/1">Sven Wellmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Ozkan_E/0/1/0/all/0/1">Ece Ozkan</a>, <a href="http://arxiv.org/find/cs/1/au:+Knorr_C/0/1/0/all/0/1">Christian Knorr</a>, <a href="http://arxiv.org/find/cs/1/au:+Vogt_J/0/1/0/all/0/1">Julia E. Vogt</a></p>
<p>Appendicitis is among the most frequent reasons for pediatric abdominal
surgeries. Previous decision support systems for appendicitis have focused on
clinical, laboratory, scoring, and computed tomography data and have ignored
abdominal ultrasound, despite its noninvasive nature and widespread
availability. In this work, we present interpretable machine learning models
for predicting the diagnosis, management and severity of suspected appendicitis
using ultrasound images. Our approach utilizes concept bottleneck models (CBM)
that facilitate interpretation and interaction with high-level concepts
understandable to clinicians. Furthermore, we extend CBMs to prediction
problems with multiple views and incomplete concept sets. Our models were
trained on a dataset comprising 579 pediatric patients with 1709 ultrasound
images accompanied by clinical and laboratory data. Results show that our
proposed method enables clinicians to utilize a human-understandable and
intervenable predictive model without compromising performance or requiring
time-consuming image annotation when deployed. For predicting the diagnosis,
the extended multiview CBM attained an AUROC of 0.80 and an AUPR of 0.92,
performing comparably to similar black-box neural networks trained and tested
on the same dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.01538">Feature Perturbation Augmentation for Reliable Evaluation of Importance Estimators in Neural Networks. (arXiv:2303.01538v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Brocki_L/0/1/0/all/0/1">Lennart Brocki</a>, <a href="http://arxiv.org/find/cs/1/au:+Chung_N/0/1/0/all/0/1">Neo Christopher Chung</a></p>
<p>Post-hoc explanation methods attempt to make the inner workings of deep
neural networks more interpretable. However, since a ground truth is in general
lacking, local post-hoc interpretability methods, which assign importance
scores to input features, are challenging to evaluate. One of the most popular
evaluation frameworks is to perturb features deemed important by an
interpretability method and to measure the change in prediction accuracy.
Intuitively, a large decrease in prediction accuracy would indicate that the
explanation has correctly quantified the importance of features with respect to
the prediction outcome (e.g., logits). However, the change in the prediction
outcome may stem from perturbation artifacts, since perturbed samples in the
test dataset are out of distribution (OOD) compared to the training dataset and
can therefore potentially disturb the model in an unexpected manner. To
overcome this challenge, we propose feature perturbation augmentation (FPA)
which creates and adds perturbed images during the model training. Through
extensive computational experiments, we demonstrate that FPA makes deep neural
networks (DNNs) more robust against perturbations. Furthermore, training DNNs
with FPA demonstrate that the sign of importance scores may explain the model
more meaningfully than has previously been assumed. Overall, FPA is an
intuitive data augmentation technique that improves the evaluation of post-hoc
interpretability methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.00933">Knowledge Accumulation in Continually Learned Representations and the Issue of Feature Forgetting. (arXiv:2304.00933v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hess_T/0/1/0/all/0/1">Timm Hess</a>, <a href="http://arxiv.org/find/cs/1/au:+Verwimp_E/0/1/0/all/0/1">Eli Verwimp</a>, <a href="http://arxiv.org/find/cs/1/au:+Ven_G/0/1/0/all/0/1">Gido M. van de Ven</a>, <a href="http://arxiv.org/find/cs/1/au:+Tuytelaars_T/0/1/0/all/0/1">Tinne Tuytelaars</a></p>
<p>While it is established that neural networks suffer from catastrophic
forgetting ``at the output level'', it is debated whether this is also the case
at the level of representations. Some studies ascribe a certain level of innate
robustness to representations, that they only forget minimally and no critical
information, while others claim that representations are also severely affected
by forgetting. To settle this debate, we first discuss how this apparent
disagreement might stem from the coexistence of two phenomena that affect the
quality of continually learned representations: knowledge accumulation and
feature forgetting. We then show that, even though it is true that feature
forgetting can be small in absolute terms, newly learned information is
forgotten just as catastrophically at the level of representations as it is at
the output level. Next we show that this feature forgetting is problematic as
it substantially slows down knowledge accumulation. We further show that
representations that are continually learned through both supervised and
self-supervised learning suffer from feature forgetting. Finally, we study how
feature forgetting and knowledge accumulation are affected by different types
of continual learning methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.05390">HRS-Bench: Holistic, Reliable and Scalable Benchmark for Text-to-Image Models. (arXiv:2304.05390v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bakr_E/0/1/0/all/0/1">Eslam Mohamed Bakr</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1">Pengzhan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1">Xiaoqian Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1">Faizan Farooq Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Li Erran Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1">Mohamed Elhoseiny</a></p>
<p>In recent years, Text-to-Image (T2I) models have been extensively studied,
especially with the emergence of diffusion models that achieve state-of-the-art
results on T2I synthesis tasks. However, existing benchmarks heavily rely on
subjective human evaluation, limiting their ability to holistically assess the
model's capabilities. Furthermore, there is a significant gap between efforts
in developing new T2I architectures and those in evaluation. To address this,
we introduce HRS-Bench, a concrete evaluation benchmark for T2I models that is
Holistic, Reliable, and Scalable. Unlike existing bench-marks that focus on
limited aspects, HRS-Bench measures 13 skills that can be categorized into five
major categories: accuracy, robustness, generalization, fairness, and bias. In
addition, HRS-Bench covers 50 scenarios, including fashion, animals,
transportation, food, and clothes. We evaluate nine recent large-scale T2I
models using metrics that cover a wide range of skills. A human evaluation
aligned with 95% of our evaluations on average was conducted to probe the
effectiveness of HRS-Bench. Our experiments demonstrate that existing models
often struggle to generate images with the desired count of objects, visual
text, or grounded emotions. We hope that our benchmark help ease future
text-to-image generation research. The code and data are available at
https://eslambakr.github.io/hrsbench.github.io
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.01569">Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation. (arXiv:2305.01569v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kirstain_Y/0/1/0/all/0/1">Yuval Kirstain</a>, <a href="http://arxiv.org/find/cs/1/au:+Polyak_A/0/1/0/all/0/1">Adam Polyak</a>, <a href="http://arxiv.org/find/cs/1/au:+Singer_U/0/1/0/all/0/1">Uriel Singer</a>, <a href="http://arxiv.org/find/cs/1/au:+Matiana_S/0/1/0/all/0/1">Shahbuland Matiana</a>, <a href="http://arxiv.org/find/cs/1/au:+Penna_J/0/1/0/all/0/1">Joe Penna</a>, <a href="http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1">Omer Levy</a></p>
<p>The ability to collect a large dataset of human preferences from
text-to-image users is usually limited to companies, making such datasets
inaccessible to the public. To address this issue, we create a web app that
enables text-to-image users to generate images and specify their preferences.
Using this web app we build Pick-a-Pic, a large, open dataset of text-to-image
prompts and real users' preferences over generated images. We leverage this
dataset to train a CLIP-based scoring function, PickScore, which exhibits
superhuman performance on the task of predicting human preferences. Then, we
test PickScore's ability to perform model evaluation and observe that it
correlates better with human rankings than other automatic evaluation metrics.
Therefore, we recommend using PickScore for evaluating future text-to-image
generation models, and using Pick-a-Pic prompts as a more relevant dataset than
MS-COCO. Finally, we demonstrate how PickScore can enhance existing
text-to-image models via ranking.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.12476">Zero-shot Visual Relation Detection via Composite Visual Cues from Large Language Models. (arXiv:2305.12476v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1">Jun Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1">Guikun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1">Jian Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1">Yueting Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Long Chen</a></p>
<p>Pretrained vision-language models, such as CLIP, have demonstrated strong
generalization capabilities, making them promising tools in the realm of
zero-shot visual recognition. Visual relation detection (VRD) is a typical task
that identifies relationship (or interaction) types between object pairs within
an image. However, naively utilizing CLIP with prevalent class-based prompts
for zero-shot VRD has several weaknesses, e.g., it struggles to distinguish
between different fine-grained relation types and it neglects essential spatial
information of two objects. To this end, we propose a novel method for
zero-shot VRD: RECODE, which solves RElation detection via COmposite
DEscription prompts. Specifically, RECODE first decomposes each predicate
category into subject, object, and spatial components. Then, it leverages large
language models (LLMs) to generate description-based prompts (or visual cues)
for each component. Different visual cues enhance the discriminability of
similar relation categories from different perspectives, which significantly
boosts performance in VRD. To dynamically fuse different cues, we further
introduce a chain-of-thought method that prompts LLMs to generate reasonable
weights for different visual cues. Extensive experiments on four VRD benchmarks
have demonstrated the effectiveness and interpretability of RECODE.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.17455">CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers. (arXiv:2305.17455v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1">Dachuan Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1">Chaofan Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1">Anyi Rao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhendong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1">Chun Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiaqi Wang</a></p>
<p>Recent vision-language models have achieved tremendous progress far beyond
what we ever expected. However, their computational costs are also dramatically
growing with rapid development, especially for the large models. It makes model
acceleration exceedingly critical in a scenario of limited resources. Although
extensively studied for unimodal models, the acceleration for multimodal
models, especially the vision-language Transformers, is relatively
under-explored. To pursue more efficient and accessible vision-language
Transformers, this paper introduces \textbf{Cross}-\textbf{G}uided
\textbf{E}nsemble of \textbf{T}okens (\textbf{\emph{CrossGET}}), a universal
acceleration framework for vision-language Transformers. This framework
adaptively combines tokens through real-time, cross-modal guidance, thereby
achieving substantial acceleration while keeping high performance.
\textit{CrossGET} has two key innovations: 1) \textit{Cross-Guided Matching and
Ensemble}. \textit{CrossGET} incorporates cross-modal guided token matching and
ensemble to exploit cross-modal information effectively, only introducing
cross-modal tokens with negligible extra parameters. 2) \textit{Complete-Graph
Soft Matching}. In contrast to the existing bipartite soft matching approach,
\textit{CrossGET} introduces a complete-graph soft matching policy to achieve
more reliable token-matching results while maintaining parallelizability and
high efficiency. Extensive experiments are conducted on various vision-language
tasks, including image-text retrieval, visual reasoning, image captioning, and
visual question answering. Performance on both classic multimodal architectures
and emerging multimodal LLMs demonstrate the effectiveness and versatility of
the proposed \textit{CrossGET} framework. The code will be at
\url{https://github.com/sdc17/CrossGET}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19621">XTransCT: Ultra-Fast Volumetric CT Reconstruction using Two Orthogonal X-Ray Projections for Image-guided Radiation Therapy via a Transformer Network. (arXiv:2305.19621v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1">Chulong Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_L/0/1/0/all/0/1">Lin Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Dai_J/0/1/0/all/0/1">Jingjing Dai</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1">Xuan Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+He_W/0/1/0/all/0/1">Wenfeng He</a>, <a href="http://arxiv.org/find/eess/1/au:+Chan_Y/0/1/0/all/0/1">Yinping Chan</a>, <a href="http://arxiv.org/find/eess/1/au:+Xie_Y/0/1/0/all/0/1">Yaoqin Xie</a>, <a href="http://arxiv.org/find/eess/1/au:+Chi_F/0/1/0/all/0/1">Feng Chi</a>, <a href="http://arxiv.org/find/eess/1/au:+Liang_X/0/1/0/all/0/1">Xiaokun Liang</a></p>
<p>Computed tomography (CT) scans offer a detailed, three-dimensional
representation of patients' internal organs. However, conventional CT
reconstruction techniques necessitate acquiring hundreds or thousands of x-ray
projections through a complete rotational scan of the body, making navigation
or positioning during surgery infeasible. In image-guided radiation therapy, a
method that reconstructs ultra-sparse X-ray projections into CT images, we can
exploit the substantially reduced radiation dose and minimize equipment burden
for localization and navigation. In this study, we introduce a novel
Transformer architecture, termed XTransCT, devised to facilitate real-time
reconstruction of CT images from two-dimensional X-ray images. We assess our
approach regarding image quality and structural reliability using a dataset of
fifty patients, supplied by a hospital, as well as the larger public dataset
LIDC-IDRI, which encompasses thousands of patients. Additionally, we validated
our algorithm's generalizability on the LNDb dataset. Our findings indicate
that our algorithm surpasses other methods in image quality, structural
precision, and generalizability. Moreover, in comparison to previous 3D
convolution-based approaches, we note a substantial speed increase of
approximately 300 %, achieving 44 ms per 3D image reconstruction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.10988">Tame a Wild Camera: In-the-Wild Monocular Camera Calibration. (arXiv:2306.10988v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1">Shengjie Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1">Abhinav Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1">Masa Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiaoming Liu</a></p>
<p>3D sensing for monocular in-the-wild images, e.g., depth estimation and 3D
object detection, has become increasingly important. However, the unknown
intrinsic parameter hinders their development and deployment. Previous methods
for the monocular camera calibration rely on specific 3D objects or strong
geometry prior, such as using a checkerboard or imposing a Manhattan World
assumption. This work solves the problem from the other perspective by
exploiting the monocular 3D prior. Our method is assumption-free and calibrates
the complete $4$ Degree-of-Freedom (DoF) intrinsic parameters. First, we
demonstrate intrinsic is solved from two well-studied monocular priors, i.e.,
monocular depthmap, and surface normal map. However, this solution imposes a
low-bias and low-variance requirement for depth estimation. Alternatively, we
introduce a novel monocular 3D prior, the incidence field, defined as the
incidence rays between points in 3D space and pixels in the 2D imaging plane.
The incidence field is a pixel-wise parametrization of the intrinsic invariant
to image cropping and resizing. With the estimated incidence field, a robust
RANSAC algorithm recovers intrinsic. We demonstrate the effectiveness of our
method by showing superior performance on synthetic and zero-shot testing
datasets. Beyond calibration, we demonstrate downstream applications in image
manipulation detection &amp; restoration, uncalibrated two-view pose estimation,
and 3D sensing. Codes, models, and data will be held in
https://github.com/ShngJZ/WildCamera.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.12070">Task-Robust Pre-Training for Worst-Case Downstream Adaptation. (arXiv:2306.12070v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jianghui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xingyu Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1">Cong Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zhouchen Lin</a></p>
<p>Pre-training has achieved remarkable success when transferred to downstream
tasks. In machine learning, we care about not only the good performance of a
model but also its behavior under reasonable shifts of condition. The same
philosophy holds when pre-training a foundation model. However, the foundation
model may not uniformly behave well for a series of related downstream tasks.
This happens, for example, when conducting mask recovery regression where the
recovery ability or the training instances diverge like pattern features are
extracted dominantly on pre-training, but semantic features are also required
on a downstream task. This paper considers pre-training a model that guarantees
a uniformly good performance over the downstream tasks. We call this goal as
$\textit{downstream-task robustness}$. Our method first separates the upstream
task into several representative ones and applies a simple minimax loss for
pre-training. We then design an efficient algorithm to solve the minimax loss
and prove its convergence in the convex setting. In the experiments, we show
both on large-scale natural language processing and computer vision datasets
our method increases the metrics on worse-case downstream tasks. Additionally,
some theoretical explanations for why our loss is beneficial are provided.
Specifically, we show fewer samples are inherently required for the most
challenging downstream task in some cases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.15064">Self-Supervised Visual Acoustic Matching. (arXiv:2307.15064v2 [cs.MM] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Somayazulu_A/0/1/0/all/0/1">Arjun Somayazulu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Changan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1">Kristen Grauman</a></p>
<p>Acoustic matching aims to re-synthesize an audio clip to sound as if it were
recorded in a target acoustic environment. Existing methods assume access to
paired training data, where the audio is observed in both source and target
environments, but this limits the diversity of training data or requires the
use of simulated data or heuristics to create paired samples. We propose a
self-supervised approach to visual acoustic matching where training samples
include only the target scene image and audio -- without acoustically
mismatched source audio for reference. Our approach jointly learns to
disentangle room acoustics and re-synthesize audio into the target environment,
via a conditional GAN framework and a novel metric that quantifies the level of
residual acoustic information in the de-biased audio. Training with either
in-the-wild web data or simulated data, we demonstrate it outperforms the
state-of-the-art on multiple challenging datasets and a wide variety of
real-world audio and environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.16449">MovieChat: From Dense Token to Sparse Memory for Long Video Understanding. (arXiv:2307.16449v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_E/0/1/0/all/0/1">Enxin Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Chai_W/0/1/0/all/0/1">Wenhao Chai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guanhong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yucheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Haoyang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1">Feiyang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chi_H/0/1/0/all/0/1">Haozhe Chi</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1">Xun Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1">Tian Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yanting Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yan Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1">Jenq-Neng Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Gaoang Wang</a></p>
<p>Recently, integrating video foundation models and large language models to
build a video understanding system can overcome the limitations of specific
pre-defined vision tasks. Yet, existing systems can only handle videos with
very few frames. For long videos, the computation complexity, memory cost, and
long-term temporal connection impose additional challenges. Taking advantage of
the Atkinson-Shiffrin memory model, with tokens in Transformers being employed
as the carriers of memory in combination with our specially designed memory
mechanism, we propose the MovieChat to overcome these challenges. MovieChat
achieves state-of-the-art performance in long video understanding, along with
the released MovieChat-1K benchmark with 1K long video and 14K manual
annotations for validation of the effectiveness of our method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05303">Multi-Visual-Inertial System: Analysis, Calibration and Estimation. (arXiv:2308.05303v3 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yulin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Geneva_P/0/1/0/all/0/1">Patrick Geneva</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1">Guoquan Huang</a></p>
<p>In this paper, we study state estimation of multi-visual-inertial systems
(MVIS) and develop sensor fusion algorithms to optimally fuse an arbitrary
number of asynchronous inertial measurement units (IMUs) or gyroscopes and
global and(or) rolling shutter cameras. We are especially interested in the
full calibration of the associated visual-inertial sensors, including the IMU
or camera intrinsics and the IMU-IMU(or camera) spatiotemporal extrinsics as
well as the image readout time of rolling-shutter cameras (if used). To this
end, we develop a new analytic combined IMU integration with intrinsics-termed
ACI3-to preintegrate IMU measurements, which is leveraged to fuse auxiliary
IMUs and(or) gyroscopes alongside a base IMU. We model the multi-inertial
measurements to include all the necessary inertial intrinsic and IMU-IMU
spatiotemporal extrinsic parameters, while leveraging IMU-IMU rigid-body
constraints to eliminate the necessity of auxiliary inertial poses and thus
reducing computational complexity. By performing observability analysis of
MVIS, we prove that the standard four unobservable directions remain - no
matter how many inertial sensors are used, and also identify, for the first
time, degenerate motions for IMU-IMU spatiotemporal extrinsics and auxiliary
inertial intrinsics. In addition to the extensive simulations that validate our
analysis and algorithms, we have built our own MVIS sensor rig and collected
over 25 real-world datasets to experimentally verify the proposed calibration
against the state-of-the-art calibration method such as Kalibr. We show that
the proposed MVIS calibration is able to achieve competing accuracy with
improved convergence and repeatability, which is open sourced to better benefit
the community.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.10192">EDDense-Net: Fully Dense Encoder Decoder Network for Joint Segmentation of Optic Cup and Disc. (arXiv:2308.10192v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Mehmood_M/0/1/0/all/0/1">Mehwish Mehmood</a>, <a href="http://arxiv.org/find/eess/1/au:+Naveed_K/0/1/0/all/0/1">Khuram Naveed</a>, <a href="http://arxiv.org/find/eess/1/au:+Aurangzeb_K/0/1/0/all/0/1">Khursheed Aurangzeb</a>, <a href="http://arxiv.org/find/eess/1/au:+Khan_H/0/1/0/all/0/1">Haroon Ahmed Khan</a>, <a href="http://arxiv.org/find/eess/1/au:+Alhussein_M/0/1/0/all/0/1">Musaed Alhussein</a>, <a href="http://arxiv.org/find/eess/1/au:+Naqvi_S/0/1/0/all/0/1">Syed Saud Naqvi</a></p>
<p>Glaucoma is an eye disease that causes damage to the optic nerve, which can
lead to visual loss and permanent blindness. Early glaucoma detection is
therefore critical in order to avoid permanent blindness. The estimation of the
cup-to-disc ratio (CDR) during an examination of the optical disc (OD) is used
for the diagnosis of glaucoma. In this paper, we present the EDDense-Net
segmentation network for the joint segmentation of OC and OD. The encoder and
decoder in this network are made up of dense blocks with a grouped
convolutional layer in each block, allowing the network to acquire and convey
spatial information from the image while simultaneously reducing the network's
complexity. To reduce spatial information loss, the optimal number of filters
in all convolution layers were utilised. In semantic segmentation, dice pixel
classification is employed in the decoder to alleviate the problem of class
imbalance. The proposed network was evaluated on two publicly available
datasets where it outperformed existing state-of-the-art methods in terms of
accuracy and efficiency. For the diagnosis and analysis of glaucoma, this
method can be used as a second opinion system to assist medical
ophthalmologists.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12319">RemovalNet: DNN Fingerprint Removal Attacks. (arXiv:2308.12319v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1">Hongwei Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1">Kunzhe Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1">Jian Lou</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1">Zhan Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1">Kui Ren</a></p>
<p>With the performance of deep neural networks (DNNs) remarkably improving,
DNNs have been widely used in many areas. Consequently, the DNN model has
become a valuable asset, and its intellectual property is safeguarded by
ownership verification techniques (e.g., DNN fingerprinting). However, the
feasibility of the DNN fingerprint removal attack and its potential influence
remains an open problem. In this paper, we perform the first comprehensive
investigation of DNN fingerprint removal attacks. Generally, the knowledge
contained in a DNN model can be categorized into general semantic and
fingerprint-specific knowledge. To this end, we propose a min-max bilevel
optimization-based DNN fingerprint removal attack named RemovalNet, to evade
model ownership verification. The lower-level optimization is designed to
remove fingerprint-specific knowledge. While in the upper-level optimization,
we distill the victim model's general semantic knowledge to maintain the
surrogate model's performance. We conduct extensive experiments to evaluate the
fidelity, effectiveness, and efficiency of the RemovalNet against four advanced
defense methods on six metrics. The empirical results demonstrate that (1) the
RemovalNet is effective. After our DNN fingerprint removal attack, the model
distance between the target and surrogate models is x100 times higher than that
of the baseline attacks, (2) the RemovalNet is efficient. It uses only 0.2%
(400 samples) of the substitute dataset and 1,000 iterations to conduct our
attack. Besides, compared with advanced model stealing attacks, the RemovalNet
saves nearly 85% of computational resources at most, (3) the RemovalNet
achieves high fidelity that the created surrogate model maintains high accuracy
after the DNN fingerprint removal process. Our code is available at:
https://github.com/grasses/RemovalNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.14936">Auto-Prompting SAM for Mobile Friendly 3D Medical Image Segmentation. (arXiv:2308.14936v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chengyin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Khanduri_P/0/1/0/all/0/1">Prashant Khanduri</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiang_Y/0/1/0/all/0/1">Yao Qiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sultan_R/0/1/0/all/0/1">Rafi Ibn Sultan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chetty_I/0/1/0/all/0/1">Indrin Chetty</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1">Dongxiao Zhu</a></p>
<p>Segment Anything Model (SAM) has rapidly been adopted for segmenting a wide
range of natural images. However, recent studies have indicated that SAM
exhibits subpar performance on 3D medical image segmentation tasks. In addition
to the domain gaps between natural and medical images, disparities in the
spatial arrangement between 2D and 3D images, the substantial computational
burden imposed by powerful GPU servers, and the time-consuming manual prompt
generation impede the extension of SAM to a broader spectrum of medical image
segmentation applications. To mitigate these challenges, we introduce a novel
method, AutoSAM Adapter, designed specifically for 3D multi-organ CT-based
segmentation. This approach utilizes parameter-efficient adaptation techniques
and an automatic prompt learning paradigm, transforming SAM's capabilities for
3D medical image segmentation. It eliminates the need for manual prompts and
achieves SOTA performance in CT-based multi-organ segmentation tasks.
Furthermore, we successfully transfer the acquired knowledge of the AutoSAM
Adapter to other lightweight models tailored for 3D medical image analysis with
enhanced performance. Through extensive experiments, the AutoSAM Adapter has
been demonstrated as an effective method to adapt the foundational SAM-based 2D
natural image segmentation model for 3D medical image segmentation tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.00168">Pose-Graph Attentional Graph Neural Network for Lidar Place Recognition. (arXiv:2309.00168v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ramezani_M/0/1/0/all/0/1">Milad Ramezani</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Liang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Knights_J/0/1/0/all/0/1">Joshua Knights</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhibin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Pounds_P/0/1/0/all/0/1">Pauline Pounds</a>, <a href="http://arxiv.org/find/cs/1/au:+Moghadam_P/0/1/0/all/0/1">Peyman Moghadam</a></p>
<p>This paper proposes a pose-graph attentional graph neural network, called
P-GAT, which compares (key)nodes between sequential and non-sequential
sub-graphs for place recognition tasks as opposed to a common frame-to-frame
retrieval problem formulation currently implemented in SOTA place recognition
methods. P-GAT uses the maximum spatial and temporal information between
neighbour cloud descriptors -- generated by an existing encoder -- utilising
the concept of pose-graph SLAM. Leveraging intra- and inter-attention and graph
neural network, P-GAT relates point clouds captured in nearby locations in
Euclidean space and their embeddings in feature space. Experimental results on
the large-scale publically available datasets demonstrate the effectiveness of
our approach in scenes lacking distinct features and when training and testing
environments have different distributions (domain adaptation). Further, an
exhaustive comparison with the state-of-the-art shows improvements in
performance gains. Code is available at
https://github.com/csiro-robotics/P-GAT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.02301">CIEM: Contrastive Instruction Evaluation Method for Better Instruction Tuning. (arXiv:2309.02301v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1">Hongyu Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiyuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1">Minyi Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1">Zhenbang Sun</a></p>
<p>Nowadays, the research on Large Vision-Language Models (LVLMs) has been
significantly promoted thanks to the success of Large Language Models (LLM).
Nevertheless, these Vision-Language Models (VLMs) are suffering from the
drawback of hallucination -- due to insufficient understanding of vision and
language modalities, VLMs may generate incorrect perception information when
doing downstream applications, for example, captioning a non-existent entity.
To address the hallucination phenomenon, on the one hand, we introduce a
Contrastive Instruction Evaluation Method (CIEM), which is an automatic
pipeline that leverages an annotated image-text dataset coupled with an LLM to
generate factual/contrastive question-answer pairs for the evaluation of the
hallucination of VLMs. On the other hand, based on CIEM, we further propose a
new instruction tuning method called CIT (the abbreviation of Contrastive
Instruction Tuning) to alleviate the hallucination of VLMs by automatically
producing high-quality factual/contrastive question-answer pairs and
corresponding justifications for model tuning. Through extensive experiments on
CIEM and CIT, we pinpoint the hallucination issues commonly present in existing
VLMs, the disability of the current instruction-tuning dataset to handle the
hallucination phenomenon and the superiority of CIT-tuned VLMs over both CIEM
and public datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.03493">SAM3D: Segment Anything Model in Volumetric Medical Images. (arXiv:2309.03493v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Bui_N/0/1/0/all/0/1">Nhat-Tan Bui</a>, <a href="http://arxiv.org/find/eess/1/au:+Hoang_D/0/1/0/all/0/1">Dinh-Hieu Hoang</a>, <a href="http://arxiv.org/find/eess/1/au:+Tran_M/0/1/0/all/0/1">Minh-Triet Tran</a>, <a href="http://arxiv.org/find/eess/1/au:+Doretto_G/0/1/0/all/0/1">Gianfranco Doretto</a>, <a href="http://arxiv.org/find/eess/1/au:+Adjeroh_D/0/1/0/all/0/1">Donald Adjeroh</a>, <a href="http://arxiv.org/find/eess/1/au:+Patel_B/0/1/0/all/0/1">Brijesh Patel</a>, <a href="http://arxiv.org/find/eess/1/au:+Choudhary_A/0/1/0/all/0/1">Arabinda Choudhary</a>, <a href="http://arxiv.org/find/eess/1/au:+Le_N/0/1/0/all/0/1">Ngan Le</a></p>
<p>Image segmentation remains a pivotal component in medical image analysis,
aiding in the extraction of critical information for precise diagnostic
practices. With the advent of deep learning, automated image segmentation
methods have risen to prominence, showcasing exceptional proficiency in
processing medical imagery. Motivated by the Segment Anything Model (SAM)-a
foundational model renowned for its remarkable precision and robust
generalization capabilities in segmenting 2D natural images-we introduce SAM3D,
an innovative adaptation tailored for 3D volumetric medical image analysis.
Unlike current SAM-based methods that segment volumetric data by converting the
volume into separate 2D slices for individual analysis, our SAM3D model
processes the entire 3D volume image in a unified approach. Extensive
experiments are conducted on multiple medical image datasets to demonstrate
that our network attains competitive results compared with other
state-of-the-art methods in 3D medical segmentation tasks while being
significantly efficient in terms of parameters. Code and checkpoints are
available at https://github.com/UARK-AICV/SAM3D.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.08273">Unsupervised Disentangling of Facial Representations with 3D-aware Latent Diffusion Models. (arXiv:2309.08273v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1">Ruian He</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1">Zhen Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1">Weimin Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1">Bo Yan</a></p>
<p>Unsupervised learning of facial representations has gained increasing
attention for face understanding ability without heavily relying on large-scale
annotated datasets. However, it remains unsolved due to the coupling of facial
identities, expressions, and external factors like pose and light. Prior
methods primarily focus on 2D factors and pixel-level consistency, leading to
incomplete disentangling and suboptimal performance in downstream tasks. In
this paper, we propose LatentFace, a novel unsupervised disentangling framework
for facial expression and identity representation. We suggest the disentangling
problem should be performed in latent space and propose the solution using a
3D-aware latent diffusion model. First, we introduce a 3D-aware autoencoder to
encode face images into 3D latent embeddings. Second, we propose a novel
representation diffusion model (RDM) to disentangle 3D latent into facial
identity and expression. Consequently, our method achieves state-of-the-art
performance in facial expression recognition and face verification among
unsupervised facial representation learning models. Codes are available at
\url{https://github.com/ryanhe312/LatentFace}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.08745">Improved Breast Cancer Diagnosis through Transfer Learning on Hematoxylin and Eosin Stained Histology Images. (arXiv:2309.08745v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ahmed_F/0/1/0/all/0/1">Fahad Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdel_Salam_R/0/1/0/all/0/1">Reem Abdel-Salam</a>, <a href="http://arxiv.org/find/cs/1/au:+Hamnett_L/0/1/0/all/0/1">Leon Hamnett</a>, <a href="http://arxiv.org/find/cs/1/au:+Adewunmi_M/0/1/0/all/0/1">Mary Adewunmi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ayano_T/0/1/0/all/0/1">Temitope Ayano</a></p>
<p>Breast cancer is one of the leading causes of death for women worldwide.
Early screening is essential for early identification, but the chance of
survival declines as the cancer progresses into advanced stages. For this
study, the most recent BRACS dataset of histological (H\&amp;E) stained images was
used to classify breast cancer tumours, which contains both the whole-slide
images (WSI) and region-of-interest (ROI) images, however, for our study we
have considered ROI images. We have experimented using different pre-trained
deep learning models, such as Xception, EfficientNet, ResNet50, and
InceptionResNet, pre-trained on the ImageNet weights. We pre-processed the
BRACS ROI along with image augmentation, upsampling, and dataset split
strategies. For the default dataset split, the best results were obtained by
ResNet50 achieving 66% f1-score. For the custom dataset split, the best results
were obtained by performing upsampling and image augmentation which results in
96.2% f1-score. Our second approach also reduced the number of false positive
and false negative classifications to less than 3% for each class. We believe
that our study significantly impacts the early diagnosis and identification of
breast cancer tumors and their subtypes, especially atypical and malignant
tumors, thus improving patient outcomes and reducing patient mortality rates.
Overall, this study has primarily focused on identifying seven (7) breast
cancer tumor subtypes, and we believe that the experimental models can be
fine-tuned further to generalize over previous breast cancer histology datasets
as well.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12862">Associative Transformer Is A Sparse Representation Learner. (arXiv:2309.12862v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yuwei Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Ochiai_H/0/1/0/all/0/1">Hideya Ochiai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhirong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1">Stephen Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Kanai_R/0/1/0/all/0/1">Ryota Kanai</a></p>
<p>Emerging from the monolithic pairwise attention mechanism in conventional
Transformer models, there is a growing interest in leveraging sparse
interactions that align more closely with biological principles. Approaches
including the Set Transformer and the Perceiver employ cross-attention
consolidated with a latent space that forms an attention bottleneck with
limited capacity. Building upon recent neuroscience studies of Global Workspace
Theory and associative memory, we propose the Associative Transformer (AiT).
AiT induces low-rank explicit memory that serves as both priors to guide
bottleneck attention in the shared workspace and attractors within associative
memory of a Hopfield network. Through joint end-to-end training, these priors
naturally develop module specialization, each contributing a distinct inductive
bias to form attention bottlenecks. A bottleneck can foster competition among
inputs for writing information into the memory. We show that AiT is a sparse
representation learner, learning distinct priors through the bottlenecks that
are complexity-invariant to input quantities and dimensions. AiT demonstrates
its superiority over methods such as the Set Transformer, Vision Transformer,
and Coordination in various vision tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.17389">Prompt-based test-time real image dehazing: a novel pipeline. (arXiv:2309.17389v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zixuan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1">Zewei He</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Ziqian Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xuecheng Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Zhe-Ming Lu</a></p>
<p>Existing methods attempt to improve models' generalization ability on
real-world hazy images by exploring well-designed training schemes (e.g.,
CycleGAN, prior loss). However, most of them need very complicated training
procedures to achieve satisfactory results. In this work, we present a totally
novel testing pipeline called Prompt-based Test-Time Dehazing (PTTD) to help
generate visually pleasing results of real-captured hazy images during the
inference phase. We experimentally find that given a dehazing model trained on
synthetic data, by fine-tuning the statistics (i.e., mean and standard
deviation) of encoding features, PTTD is able to narrow the domain gap,
boosting the performance of real image dehazing. Accordingly, we first apply a
prompt generation module (PGM) to generate a visual prompt, which is the source
of appropriate statistical perturbations for mean and standard deviation. And
then, we employ the feature adaptation module (FAM) into the existing dehazing
models for adjusting the original statistics with the guidance of the generated
prompt. Note that, PTTD is model-agnostic and can be equipped with various
state-of-the-art dehazing models trained on synthetic hazy-clean pairs.
Extensive experimental results demonstrate that our PTTD is flexible meanwhile
achieves superior performance against state-of-the-art dehazing methods in
real-world scenarios. The source code of our PTTD will be made available at
https://github.com/cecret3350/PTTD-Dehazing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.06214">CoT3DRef: Chain-of-Thoughts Data-Efficient 3D Visual Grounding. (arXiv:2310.06214v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bakr_E/0/1/0/all/0/1">Eslam Mohamed Bakr</a>, <a href="http://arxiv.org/find/cs/1/au:+Ayman_M/0/1/0/all/0/1">Mohamed Ayman</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_M/0/1/0/all/0/1">Mahmoud Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Slim_H/0/1/0/all/0/1">Habib Slim</a>, <a href="http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1">Mohamed Elhoseiny</a></p>
<p>3D visual grounding is the ability to localize objects in 3D scenes
conditioned by utterances. Most existing methods devote the referring head to
localize the referred object directly, causing failure in complex scenarios. In
addition, it does not illustrate how and why the network reaches the final
decision. In this paper, we address this question Can we design an
interpretable 3D visual grounding framework that has the potential to mimic the
human perception system?. To this end, we formulate the 3D visual grounding
problem as a sequence-to-sequence task by first predicting a chain of anchors
and then the final target. Interpretability not only improves the overall
performance but also helps us identify failure cases. Following the chain of
thoughts approach enables us to decompose the referring task into interpretable
intermediate steps, boosting the performance and making our framework extremely
data-efficient. Moreover, our proposed framework can be easily integrated into
any existing architecture. We validate our approach through comprehensive
experiments on the Nr3D, Sr3D, and Scanrefer benchmarks and show consistent
performance gains compared to existing methods without requiring manually
annotated data. Furthermore, our proposed framework, dubbed CoT3DRef, is
significantly data-efficient, whereas on the Sr3D dataset, when trained only on
10% of the data, we match the SOTA performance that trained on the entire data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08537">XAI Benchmark for Visual Explanation. (arXiv:2310.08537v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yifei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1">Siyi Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1">James Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_B/0/1/0/all/0/1">Bo Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_G/0/1/0/all/0/1">Guangji Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1">Liang Zhao</a></p>
<p>The rise of deep learning has ushered in significant progress in computer
vision (CV) tasks, yet the "black box" nature of these models often precludes
interpretability. This challenge has spurred the development of Explainable
Artificial Intelligence (XAI) by generating explanations to AI's
decision-making process. An explanation is aimed to not only faithfully reflect
the true reasoning process (i.e., faithfulness) but also align with humans'
reasoning (i.e., alignment). Within XAI, visual explanations employ visual cues
to elucidate the reasoning behind machine learning models, particularly in
image processing, by highlighting images' critical areas important to
predictions. Despite the considerable body of research in visual explanations,
standardized benchmarks for evaluating them are seriously underdeveloped. In
particular, to evaluate alignment, existing works usually merely illustrate a
few images' visual explanations, or hire some referees to report the
explanation quality under ad-hoc questionnaires. However, this cannot achieve a
standardized, quantitative, and comprehensive evaluation. To address this
issue, we develop a benchmark for visual explanation, consisting of eight
datasets with human explanation annotations from various domains, accommodating
both post-hoc and intrinsic visual explanation methods. Additionally, we devise
a visual explanation pipeline that includes data loading, explanation
generation, and method evaluation. Our proposed benchmarks facilitate a fair
evaluation and comparison of visual explanation methods. Building on our
curated collection of datasets, we benchmarked eight existing visual
explanation methods and conducted a thorough comparison across four selected
datasets using six alignment-based and causality-based metrics. Our benchmark
will be accessible through our website https://xaidataset.github.io.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.09600">Hawkeye: A PyTorch-based Library for Fine-Grained Image Recognition with Deep Learning. (arXiv:2310.09600v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Jiabei He</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yang Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1">Xiu-Shen Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Ye Wu</a></p>
<p>Fine-Grained Image Recognition (FGIR) is a fundamental and challenging task
in computer vision and multimedia that plays a crucial role in Intellectual
Economy and Industrial Internet applications. However, the absence of a unified
open-source software library covering various paradigms in FGIR poses a
significant challenge for researchers and practitioners in the field. To
address this gap, we present Hawkeye, a PyTorch-based library for FGIR with
deep learning. Hawkeye is designed with a modular architecture, emphasizing
high-quality code and human-readable configuration, providing a comprehensive
solution for FGIR tasks. In Hawkeye, we have implemented 16 state-of-the-art
fine-grained methods, covering 6 different paradigms, enabling users to explore
various approaches for FGIR. To the best of our knowledge, Hawkeye represents
the first open-source PyTorch-based library dedicated to FGIR. It is publicly
available at https://github.com/Hawkeye-FineGrained/Hawkeye/, providing
researchers and practitioners with a powerful tool to advance their research
and development in the field of FGIR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.14356">Cultural and Linguistic Diversity Improves Visual Representations. (arXiv:2310.14356v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ye_A/0/1/0/all/0/1">Andre Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Santy_S/0/1/0/all/0/1">Sebastin Santy</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1">Jena D. Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1">Amy X. Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1">Ranjay Krishna</a></p>
<p>Computer vision often treats perception as objective, and this assumption
gets reflected in the way that datasets are collected and models are trained.
For instance, image descriptions in different languages are typically assumed
to be translations of the same semantic content. However, work in
cross-cultural psychology and linguistics has shown that individuals differ in
their visual perception depending on their cultural background and the language
they speak. In this paper, we demonstrate significant differences in semantic
content across languages in both dataset and model-produced captions. When data
is multilingual as opposed to monolingual, captions have higher semantic
coverage on average, as measured by scene graph, embedding, and linguistic
complexity. For example, multilingual captions have on average 21.8% more
objects, 24.5% more relations, and 27.1% more attributes than a set of
monolingual captions. Moreover, models trained on content from different
languages perform best against test data from those languages, while those
trained on multilingual content perform consistently well across all evaluation
data compositions. Our research provides implications for how diverse modes of
perception can improve image understanding.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17294">Scale-Adaptive Feature Aggregation for Efficient Space-Time Video Super-Resolution. (arXiv:2310.17294v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zhewei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_A/0/1/0/all/0/1">Ailin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xiaotao Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1">Chen Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jun Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1">Shuchang Zhou</a></p>
<p>The Space-Time Video Super-Resolution (STVSR) task aims to enhance the visual
quality of videos, by simultaneously performing video frame interpolation (VFI)
and video super-resolution (VSR). However, facing the challenge of the
additional temporal dimension and scale inconsistency, most existing STVSR
methods are complex and inflexible in dynamically modeling different motion
amplitudes. In this work, we find that choosing an appropriate processing scale
achieves remarkable benefits in flow-based feature propagation. We propose a
novel Scale-Adaptive Feature Aggregation (SAFA) network that adaptively selects
sub-networks with different processing scales for individual samples.
Experiments on four public STVSR benchmarks demonstrate that SAFA achieves
state-of-the-art performance. Our SAFA network outperforms recent
state-of-the-art methods such as TMNet and VideoINR by an average improvement
of over 0.5dB on PSNR, while requiring less than half the number of parameters
and only 1/3 computational costs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19653">Upgrading VAE Training With Unlimited Data Plans Provided by Diffusion Models. (arXiv:2310.19653v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Xiao_T/0/1/0/all/0/1">Tim Z. Xiao</a>, <a href="http://arxiv.org/find/stat/1/au:+Zenn_J/0/1/0/all/0/1">Johannes Zenn</a>, <a href="http://arxiv.org/find/stat/1/au:+Bamler_R/0/1/0/all/0/1">Robert Bamler</a></p>
<p>Variational autoencoders (VAEs) are popular models for representation
learning but their encoders are susceptible to overfitting (Cremer et al.,
2018) because they are trained on a finite training set instead of the true
(continuous) data distribution $p_{\mathrm{data}}(\mathbf{x})$. Diffusion
models, on the other hand, avoid this issue by keeping the encoder fixed. This
makes their representations less interpretable, but it simplifies training,
enabling accurate and continuous approximations of
$p_{\mathrm{data}}(\mathbf{x})$. In this paper, we show that overfitting
encoders in VAEs can be effectively mitigated by training on samples from a
pre-trained diffusion model. These results are somewhat unexpected as recent
findings (Alemohammad et al., 2023; Shumailov et al., 2023) observe a decay in
generative performance when models are trained on data generated by another
generative model. We analyze generalization performance, amortization gap, and
robustness of VAEs trained with our proposed method on three different data
sets. We find improvements in all metrics compared to both normal training and
conventional data augmentation methods, and we show that a modest amount of
samples from the diffusion model suffices to obtain these gains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00996">VCISR: Blind Single Image Super-Resolution with Video Compression Synthetic Data. (arXiv:2311.00996v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Wang_B/0/1/0/all/0/1">Boyang Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_B/0/1/0/all/0/1">Bowen Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1">Shiyu Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_F/0/1/0/all/0/1">Fengyu Yang</a></p>
<p>In the blind single image super-resolution (SISR) task, existing works have
been successful in restoring image-level unknown degradations. However, when a
single video frame becomes the input, these works usually fail to address
degradations caused by video compression, such as mosquito noise, ringing,
blockiness, and staircase noise. In this work, we for the first time, present a
video compression-based degradation model to synthesize low-resolution image
data in the blind SISR task. Our proposed image synthesizing method is widely
applicable to existing image datasets, so that a single degraded image can
contain distortions caused by the lossy video compression algorithms. This
overcomes the leak of feature diversity in video data and thus retains the
training efficiency. By introducing video coding artifacts to SISR degradation
models, neural networks can super-resolve images with the ability to restore
video compression degradations, and achieve better results on restoring generic
distortions caused by image compression as well. Our proposed approach achieves
superior performance in SOTA no-reference Image Quality Assessment, and shows
better visual quality on various datasets. In addition, we evaluate the SISR
neural network trained with our degradation model on video super-resolution
(VSR) datasets. Compared to architectures specifically designed for the VSR
purpose, our method exhibits similar or better performance, evidencing that the
presented strategy on infusing video-based degradation is generalizable to
address more complicated compression artifacts even without temporal cues.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01017">Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion. (arXiv:2311.01017v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lunjun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1">Yuwen Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Ze Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Casas_S/0/1/0/all/0/1">Sergio Casas</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1">Rui Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1">Raquel Urtasun</a></p>
<p>Learning world models can teach an agent how the world works in an
unsupervised manner. Even though it can be viewed as a special case of sequence
modeling, progress for scaling world models on robotic applications such as
autonomous driving has been somewhat less rapid than scaling language models
with Generative Pre-trained Transformers (GPT). We identify two reasons as
major bottlenecks: dealing with complex and unstructured observation space, and
having a scalable generative model. Consequently, we propose a novel world
modeling approach that first tokenizes sensor observations with VQVAE, then
predicts the future via discrete diffusion. To efficiently decode and denoise
tokens in parallel, we recast Masked Generative Image Transformer into the
discrete diffusion framework with a few simple changes, resulting in notable
improvement. When applied to learning world models on point cloud observations,
our model reduces prior SOTA Chamfer distance by more than 65% for 1s
prediction, and more than 50% for 3s prediction, across NuScenes, KITTI
Odometry, and Argoverse2 datasets. Our results demonstrate that discrete
diffusion on tokenized agent experience can unlock the power of GPT-like
unsupervised learning for robotic agents.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03561">Sea You Later: Metadata-Guided Long-Term Re-Identification for UAV-Based Multi-Object Tracking. (arXiv:2311.03561v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Cheng-Yen Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Hsiang-Wei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1">Zhongyu Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuo_H/0/1/0/all/0/1">Heng-Cheng Kuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Mei_J/0/1/0/all/0/1">Jie Mei</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1">Chung-I Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1">Jenq-Neng Hwang</a></p>
<p>Re-identification (ReID) in multi-object tracking (MOT) for UAVs in maritime
computer vision has been challenging for several reasons. More specifically,
short-term re-identification (ReID) is difficult due to the nature of the
characteristics of small targets and the sudden movement of the drone's gimbal.
Long-term ReID suffers from the lack of useful appearance diversity. In
response to these challenges, we present an adaptable motion-based MOT
algorithm, called Metadata Guided MOT (MG-MOT). This algorithm effectively
merges short-term tracking data into coherent long-term tracks, harnessing
crucial metadata from UAVs, including GPS position, drone altitude, and camera
orientations. Extensive experiments are conducted to validate the efficacy of
our MOT algorithm. Utilizing the challenging SeaDroneSee tracking dataset,
which encompasses the aforementioned scenarios, we achieve a much-improved
performance in the latest edition of the UAV-based Maritime Object Tracking
Challenge with a state-of-the-art HOTA of 69.5% and an IDF1 of 85.9% on the
testing split.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05784">Are &quot;Hierarchical&quot; Visual Representations Hierarchical?. (arXiv:2311.05784v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shen_E/0/1/0/all/0/1">Ethan Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1">Ali Farhadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kusupati_A/0/1/0/all/0/1">Aditya Kusupati</a></p>
<p>Learned visual representations often capture large amounts of semantic
information for accurate downstream applications. Human understanding of the
world is fundamentally grounded in hierarchy. To mimic this and further improve
representation capabilities, the community has explored "hierarchical" visual
representations that aim at modeling the underlying hierarchy of the visual
world. In this work, we set out to investigate if hierarchical visual
representations truly capture the human perceived hierarchy better than
standard learned representations. To this end, we create HierNet, a suite of 12
datasets spanning 3 kinds of hierarchy from the BREEDs subset of ImageNet.
After extensive evaluation of Hyperbolic and Matryoshka Representations across
training setups, we conclude that they do not capture hierarchy any better than
the standard representations but can assist in other aspects like search
efficiency and interpretability. Our benchmark and the datasets are
open-sourced at https://github.com/ethanlshen/HierNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06031">Diagonal Hierarchical Consistency Learning for Semi-supervised Medical Image Segmentation. (arXiv:2311.06031v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Koo_H/0/1/0/all/0/1">Heejoon Koo</a></p>
<p>Medical image segmentation, which is essential for many clinical
applications, has achieved almost human-level performance via data-driven deep
learning technologies. Nevertheless, its performance is predicated upon the
costly process of manually annotating a vast amount of medical images. To this
end, we propose a novel framework for robust semi-supervised medical image
segmentation using diagonal hierarchical consistency learning (DiHC-Net).
First, it is composed of multiple sub-models with identical multi-scale
architecture but with distinct sub-layers, such as up-sampling and
normalisation layers. Second, with mutual consistency, a novel consistency
regularisation is enforced between one model's intermediate and final
prediction and soft pseudo labels from other models in a diagonal hierarchical
fashion. A series of experiments verifies the efficacy of our simple framework,
outperforming all previous approaches on public Left Atrium (LA) dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06214">Instant3D: Fast Text-to-3D with Sparse-View Generation and Large Reconstruction Model. (arXiv:2311.06214v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiahao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1">Hao Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zexiang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Luan_F/0/1/0/all/0/1">Fujun Luan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yinghao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1">Yicong Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Sunkavalli_K/0/1/0/all/0/1">Kalyan Sunkavalli</a>, <a href="http://arxiv.org/find/cs/1/au:+Shakhnarovich_G/0/1/0/all/0/1">Greg Shakhnarovich</a>, <a href="http://arxiv.org/find/cs/1/au:+Bi_S/0/1/0/all/0/1">Sai Bi</a></p>
<p>Text-to-3D with diffusion models has achieved remarkable progress in recent
years. However, existing methods either rely on score distillation-based
optimization which suffer from slow inference, low diversity and Janus
problems, or are feed-forward methods that generate low-quality results due to
the scarcity of 3D training data. In this paper, we propose Instant3D, a novel
method that generates high-quality and diverse 3D assets from text prompts in a
feed-forward manner. We adopt a two-stage paradigm, which first generates a
sparse set of four structured and consistent views from text in one shot with a
fine-tuned 2D text-to-image diffusion model, and then directly regresses the
NeRF from the generated images with a novel transformer-based sparse-view
reconstructor. Through extensive experiments, we demonstrate that our method
can generate diverse 3D assets of high visual quality within 20 seconds, which
is two orders of magnitude faster than previous optimization-based methods that
can take 1 to 10 hours. Our project webpage: https://jiahao.ai/instant3d/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06607">Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models. (arXiv:2311.06607v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1">Biao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qiang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1">Zhiyin Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shuo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jingxu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yabo Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuliang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1">Xiang Bai</a></p>
<p>Large Multimodal Models (LMMs) have shown promise in vision-language tasks
but struggle with high-resolution input and detailed scene understanding.
Addressing these challenges, we introduce Monkey to enhance LMM capabilities.
Firstly, Monkey processes input images by dividing them into uniform patches,
each matching the size (e.g., 448x448) used in the original training of the
well-trained vision encoder. Equipped with individual adapter for each patch,
Monkey can handle higher resolutions up to 1344x896 pixels, enabling the
detailed capture of complex visual information. Secondly, it employs a
multi-level description generation method, enriching the context for
scene-object associations. This two-part strategy ensures more effective
learning from generated data: the higher resolution allows for a more detailed
capture of visuals, which in turn enhances the effectiveness of comprehensive
descriptions. Extensive ablative results validate the effectiveness of our
designs. Additionally, experiments on 18 datasets further demonstrate that
Monkey surpasses existing LMMs in many tasks like Image Captioning and various
Visual Question Answering formats. Specially, in qualitative tests focused on
dense text question answering, Monkey has exhibited encouraging results
compared with GPT4V. Code is available at
https://github.com/Yuliang-Liu/Monkey.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07075">GazeForensics: DeepFake Detection via Gaze-guided Spatial Inconsistency Learning. (arXiv:2311.07075v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1">Qinlin He</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1">Chunlei Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1">Decheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1">Nannan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xinbo Gao</a></p>
<p>DeepFake detection is pivotal in personal privacy and public safety. With the
iterative advancement of DeepFake techniques, high-quality forged videos and
images are becoming increasingly deceptive. Prior research has seen numerous
attempts by scholars to incorporate biometric features into the field of
DeepFake detection. However, traditional biometric-based approaches tend to
segregate biometric features from general ones and freeze the biometric feature
extractor. These approaches resulted in the exclusion of valuable general
features, potentially leading to a performance decline and, consequently, a
failure to fully exploit the potential of biometric information in assisting
DeepFake detection. Moreover, insufficient attention has been dedicated to
scrutinizing gaze authenticity within the realm of DeepFake detection in recent
years. In this paper, we introduce GazeForensics, an innovative DeepFake
detection method that utilizes gaze representation obtained from a 3D gaze
estimation model to regularize the corresponding representation within our
DeepFake detection model, while concurrently integrating general features to
further enhance the performance of our model. Experiment results reveal that
our proposed GazeForensics outperforms the current state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09178">RBPGAN: Recurrent Back-Projection GAN for Video Super Resolution. (arXiv:2311.09178v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sulaiman_M/0/1/0/all/0/1">Marwah Sulaiman</a>, <a href="http://arxiv.org/find/cs/1/au:+Shehabeldin_Z/0/1/0/all/0/1">Zahraa Shehabeldin</a>, <a href="http://arxiv.org/find/cs/1/au:+Fahmy_I/0/1/0/all/0/1">Israa Fahmy</a>, <a href="http://arxiv.org/find/cs/1/au:+Barakat_M/0/1/0/all/0/1">Mohammed Barakat</a>, <a href="http://arxiv.org/find/cs/1/au:+El_Naggar_M/0/1/0/all/0/1">Mohammed El-Naggar</a>, <a href="http://arxiv.org/find/cs/1/au:+Hussein_D/0/1/0/all/0/1">Dareen Hussein</a>, <a href="http://arxiv.org/find/cs/1/au:+Youssef_M/0/1/0/all/0/1">Moustafa Youssef</a>, <a href="http://arxiv.org/find/cs/1/au:+Eraqi_H/0/1/0/all/0/1">Hesham Eraqi</a></p>
<p>Recently, video super resolution (VSR) has become a very impactful task in
the area of Computer Vision due to its various applications. In this paper, we
propose Recurrent Back-Projection Generative Adversarial Network (RBPGAN) for
VSR in an attempt to generate temporally coherent solutions while preserving
spatial details. RBPGAN integrates two state-of-the-art models to get the best
in both worlds without compromising the accuracy of produced video. The
generator of the model is inspired by RBPN system, while the discriminator is
inspired by TecoGAN. We also utilize Ping-Pong loss to increase temporal
consistency over time. Our contribution together results in a model that
outperforms earlier work in terms of temporally consistent details, as we will
demonstrate qualitatively and quantitatively using different datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10543">Joint covariance property under geometric image transformations for spatio-temporal receptive fields according to the generalized Gaussian derivative model for visual receptive fields. (arXiv:2311.10543v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lindeberg_T/0/1/0/all/0/1">Tony Lindeberg</a></p>
<p>The influence of natural image transformations on receptive field responses
is crucial for modelling visual operations in computer vision and biological
vision. In this regard, covariance properties with respect to geometric image
transformations in the earliest layers of the visual hierarchy are essential
for expressing robust image operations and for formulating invariant visual
operations at higher levels. This paper defines and proves a joint covariance
property under compositions of spatial scaling transformations, spatial affine
transformations, Galilean transformations and temporal scaling transformations,
which makes it possible to characterize how different types of image
transformations interact with each other. Specifically, the derived relations
show how the receptive field parameters need to be transformed, in order to
match the output from spatio-temporal receptive fields with the underlying
spatio-temporal image transformations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10754">A Recent Survey of the Advancements in Deep Learning Techniques for Monkeypox Disease Detection. (arXiv:2311.10754v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Khan_S/0/1/0/all/0/1">Saddam Hussain Khan</a>, <a href="http://arxiv.org/find/eess/1/au:+Iqbal_R/0/1/0/all/0/1">Rashid Iqbal</a>, <a href="http://arxiv.org/find/eess/1/au:+Naz_S/0/1/0/all/0/1">Saeeda Naz</a> (Artifical Intelligence Lab, Department of Computer Systems Engineering, University of Engineering and Applied Science (UEAS), Swat, Pakistan)</p>
<p>Monkeypox (MPox) is a zoonotic infectious disease induced by the MPox Virus,
part of the poxviridae orthopoxvirus group initially discovered in Africa and
gained global attention in mid-2022 with cases reported outside endemic areas.
Symptoms include headaches, chills, fever, smallpox, measles, and
chickenpox-like skin manifestations and the WHO officially announced MPox as a
global public health pandemic, in July 2022.Traditionally, PCR testing of skin
lesions is considered a benchmark for the primary diagnosis by WHO, with
symptom management as the primary treatment and antiviral drugs like
tecovirimat for severe cases. However, manual analysis within hospitals poses a
substantial challenge including the substantial burden on healthcare
professionals, limited facilities, availability and fatigue among doctors, and
human error during public health emergencies. Therefore, this survey paper
provides an extensive and efficient analysis of deep learning (DL) methods for
the automatic detection of MPox in skin lesion images. These DL techniques are
broadly grouped into categories, including deep CNN, Deep CNNs ensemble, deep
hybrid learning, the newly developed, and Vision transformer for diagnosing
MPox. Moreover, this study offers a systematic exploration of the evolutionary
progression of DL techniques and identifies, and addresses limitations in
previous methods while highlighting the valuable contributions and innovation.
Additionally, the paper addresses benchmark datasets and their collection from
various authentic sources, pre-processing techniques, and evaluation metrics.
The survey also briefly delves into emerging concepts, identifies research
gaps, limitations, and applications, and outlines challenges in the diagnosis
process. This survey furnishes valuable insights into the prospective areas of
DL innovative ideas and is anticipated to serve as a path for researchers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12024">PF-LRM: Pose-Free Large Reconstruction Model for Joint Pose and Shape Prediction. (arXiv:2311.12024v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Peng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1">Hao Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bi_S/0/1/0/all/0/1">Sai Bi</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yinghao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Luan_F/0/1/0/all/0/1">Fujun Luan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sunkavalli_K/0/1/0/all/0/1">Kalyan Sunkavalli</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenping Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zexiang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kai Zhang</a></p>
<p>We propose a Pose-Free Large Reconstruction Model (PF-LRM) for reconstructing
a 3D object from a few unposed images even with little visual overlap, while
simultaneously estimating the relative camera poses in ~1.3 seconds on a single
A100 GPU. PF-LRM is a highly scalable method utilizing the self-attention
blocks to exchange information between 3D object tokens and 2D image tokens; we
predict a coarse point cloud for each view, and then use a differentiable
Perspective-n-Point (PnP) solver to obtain camera poses. When trained on a huge
amount of multi-view posed data of ~1M objects, PF-LRM shows strong
cross-dataset generalization ability, and outperforms baseline methods by a
large margin in terms of pose prediction accuracy and 3D reconstruction quality
on various unseen evaluation datasets. We also demonstrate our model's
applicability in downstream text/image-to-3D task with fast feed-forward
inference. Our project website is at: https://totoro97.github.io/pf-lrm .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12144">Applications of Large Scale Foundation Models for Autonomous Driving. (arXiv:2311.12144v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yu Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yue Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhu Li</a></p>
<p>Since DARPA Grand Challenges (rural) in 2004/05 and Urban Challenges in 2007,
autonomous driving has been the most active field of AI applications. Recently
powered by large language models (LLMs), chat systems, such as chatGPT and
PaLM, emerge and rapidly become a promising direction to achieve artificial
general intelligence (AGI) in natural language processing (NLP). There comes a
natural thinking that we could employ these abilities to reformulate autonomous
driving. By combining LLM with foundation models, it is possible to utilize the
human knowledge, commonsense and reasoning to rebuild autonomous driving
systems from the current long-tailed AI dilemma. In this paper, we investigate
the techniques of foundation models and LLMs applied for autonomous driving,
categorized as simulation, world model, data annotation and planning or E2E
solutions etc.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12401">CASR: Refining Action Segmentation via Magrinalizing Frame-levle Causal Relationships. (arXiv:2311.12401v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Du_K/0/1/0/all/0/1">Keqing Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xinyu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hang Chen</a></p>
<p>Integrating deep learning and causal discovery has increased the
interpretability of Temporal Action Segmentation (TAS) tasks. However,
frame-level causal relationships exist many complicated noises outside the
segment-level, making it infeasible to directly express macro action semantics.
Thus, we propose Causal Abstraction Segmentation Refiner (CASR), which can
refine TAS results from various models by enhancing video causality in
marginalizing frame-level casual relationships. Specifically, we define the
equivalent frame-level casual model and segment-level causal model, so that the
causal adjacency matrix constructed from marginalized frame-level causal
relationships has the ability to represent the segmnet-level causal
relationships. CASR works out by reducing the difference in the causal
adjacency matrix between we constructed and pre-segmentation results of
backbone models. In addition, we propose a novel evaluation metric Causal Edit
Distance (CED) to evaluate the causal interpretability. Extensive experimental
results on mainstream datasets indicate that CASR significantly surpasses
existing various methods in action segmentation performance, as well as in
causal explainability and generalization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12553">&quot;HoVer-UNet&quot;: Accelerating HoVerNet with UNet-based multi-class nuclei segmentation via knowledge distillation. (arXiv:2311.12553v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Tommasino_C/0/1/0/all/0/1">Cristian Tommasino</a>, <a href="http://arxiv.org/find/eess/1/au:+Russo_C/0/1/0/all/0/1">Cristiano Russo</a>, <a href="http://arxiv.org/find/eess/1/au:+Rinaldi_A/0/1/0/all/0/1">Antonio Maria Rinaldi</a>, <a href="http://arxiv.org/find/eess/1/au:+Ciompi_F/0/1/0/all/0/1">Francesco Ciompi</a></p>
<p>We present "HoVer-UNet", an approach to distill the knowledge of the
multi-branch HoVerNet framework for nuclei instance segmentation and
classification in histopathology. We propose a compact, streamlined single UNet
network with a Mix Vision Transformer backbone, and equip it with a custom loss
function to optimally encode the distilled knowledge of HoVerNet, reducing
computational requirements without compromising performances. We show that our
model achieved results comparable to HoVerNet on the public PanNuke and Consep
datasets with a three-fold reduction in inference time. We make the code of our
model publicly available at https://github.com/DIAGNijmegen/HoVer-UNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12651">Mobile-Seed: Joint Semantic Segmentation and Boundary Detection for Mobile Robots. (arXiv:2311.12651v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1">Youqi Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1">Shuhao Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jianping Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1">Zhen Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1">Bisheng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xieyuanli Chen</a></p>
<p>Precise and rapid delineation of sharp boundaries and robust semantics is
essential for numerous downstream robotic tasks, such as robot grasping and
manipulation, real-time semantic mapping, and online sensor calibration
performed on edge computing units. Although boundary detection and semantic
segmentation are complementary tasks, most studies focus on lightweight models
for semantic segmentation but overlook the critical role of boundary detection.
In this work, we introduce Mobile-Seed, a lightweight, dual-task framework
tailored for simultaneous semantic segmentation and boundary detection. Our
framework features a two-stream encoder, an active fusion decoder (AFD) and a
dual-task regularization approach. The encoder is divided into two pathways:
one captures category-aware semantic information, while the other discerns
boundaries from multi-scale features. The AFD module dynamically adapts the
fusion of semantic and boundary information by learning channel-wise
relationships, allowing for precise weight assignment of each channel.
Furthermore, we introduce a regularization loss to mitigate the conflicts in
dual-task learning and deep diversity supervision. Compared to existing
methods, the proposed Mobile-Seed offers a lightweight framework to
simultaneously improve semantic segmentation performance and accurately locate
object boundaries. Experiments on the Cityscapes dataset have shown that
Mobile-Seed achieves notable improvement over the state-of-the-art (SOTA)
baseline by 2.2 percentage points (pp) in mIoU and 4.2 pp in mF-score, while
maintaining an online inference speed of 23.9 frames-per-second (FPS) with
1024x2048 resolution input on an RTX 2080 Ti GPU. Additional experiments on
CamVid and PASCAL Context datasets confirm our method's generalizability. Code
and additional results are publicly available at
https://whu-usi3dv.github.io/Mobile-Seed/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13110">White-Box Transformers via Sparse Rate Reduction: Compression Is All There Is?. (arXiv:2311.13110v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yaodong Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Buchanan_S/0/1/0/all/0/1">Sam Buchanan</a>, <a href="http://arxiv.org/find/cs/1/au:+Pai_D/0/1/0/all/0/1">Druv Pai</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_T/0/1/0/all/0/1">Tianzhe Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Ziyang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tong_S/0/1/0/all/0/1">Shengbang Tong</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1">Hao Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1">Yuexiang Zhai</a>, <a href="http://arxiv.org/find/cs/1/au:+Haeffele_B/0/1/0/all/0/1">Benjamin D. Haeffele</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yi Ma</a></p>
<p>In this paper, we contend that a natural objective of representation learning
is to compress and transform the distribution of the data, say sets of tokens,
towards a low-dimensional Gaussian mixture supported on incoherent subspaces.
The goodness of such a representation can be evaluated by a principled measure,
called sparse rate reduction, that simultaneously maximizes the intrinsic
information gain and extrinsic sparsity of the learned representation. From
this perspective, popular deep network architectures, including transformers,
can be viewed as realizing iterative schemes to optimize this measure.
Particularly, we derive a transformer block from alternating optimization on
parts of this objective: the multi-head self-attention operator compresses the
representation by implementing an approximate gradient descent step on the
coding rate of the features, and the subsequent multi-layer perceptron
sparsifies the features. This leads to a family of white-box transformer-like
deep network architectures, named CRATE, which are mathematically fully
interpretable. We show, by way of a novel connection between denoising and
compression, that the inverse to the aforementioned compressive encoding can be
realized by the same class of CRATE architectures. Thus, the so-derived
white-box architectures are universal to both encoders and decoders.
Experiments show that these networks, despite their simplicity, indeed learn to
compress and sparsify representations of large-scale real-world image and text
datasets, and achieve performance very close to highly engineered
transformer-based models: ViT, MAE, DINO, BERT, and GPT2. We believe the
proposed computational framework demonstrates great potential in bridging the
gap between theory and practice of deep learning, from a unified perspective of
data compression. Code is available at: https://ma-lab-berkeley.github.io/CRATE .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13120">Multi-modal In-Context Learning Makes an Ego-evolving Scene Text Recognizer. (arXiv:2311.13120v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zhen Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jingqun Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1">Chunhui Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1">Binghong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhizhong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1">Xin Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1">Can Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1">Yuan Xie</a></p>
<p>Scene text recognition (STR) in the wild frequently encounters challenges
when coping with domain variations, font diversity, shape deformations, etc. A
straightforward solution is performing model fine-tuning tailored to a specific
scenario, but it is computationally intensive and requires multiple model
copies for various scenarios. Recent studies indicate that large language
models (LLMs) can learn from a few demonstration examples in a training-free
manner, termed "In-Context Learning" (ICL). Nevertheless, applying LLMs as a
text recognizer is unacceptably resource-consuming. Moreover, our pilot
experiments on LLMs show that ICL fails in STR, mainly attributed to the
insufficient incorporation of contextual information from diverse samples in
the training stage. To this end, we introduce E$^2$STR, a STR model trained
with context-rich scene text sequences, where the sequences are generated via
our proposed in-context training strategy. E$^2$STR demonstrates that a
regular-sized model is sufficient to achieve effective ICL capabilities in STR.
Extensive experiments show that E$^2$STR exhibits remarkable training-free
adaptation in various scenarios and outperforms even the fine-tuned
state-of-the-art approaches on public benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13199">DRIFu: Differentiable Rendering and Implicit Function-based Single-View 3D Reconstruction. (arXiv:2311.13199v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kuang_Z/0/1/0/all/0/1">Zijian Kuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ying_L/0/1/0/all/0/1">Lihang Ying</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1">Shi Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1">Li Cheng</a></p>
<p>The Differentiable Rendering and Implicit Function-based model (DRIFu) draws
its roots from the Pixel-aligned Implicit Function (PIFU), a pioneering 3D
digitization technique initially designed for clothed human bodies. PIFU excels
in capturing nuanced body shape variations within a low-dimensional space and
has been extensively trained on human 3D scans. However, the application of
PIFU to live animals poses significant challenges, primarily due to the
inherent difficulty in obtaining the cooperation of animals for 3D scanning. In
response to this challenge, we introduce the DRIFu model, specifically tailored
for animal digitization. To train DRIFu, we employ a curated set of synthetic
3D animal models, encompassing diverse shapes, sizes, and even accounting for
variations such as baby birds. Our innovative alignment tools play a pivotal
role in mapping these diverse synthetic animal models onto a unified template,
facilitating precise predictions of animal shape and texture. Crucially, our
template alignment strategy establishes a shared shape space, allowing for the
seamless sampling of new animal shapes, posing them realistically, animating
them, and aligning them with real-world data. This groundbreaking approach
revolutionizes our capacity to comprehensively understand and represent avian
forms. For further details and access to the project, the project website can
be found at https://github.com/kuangzijian/drifu-for-animals
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13231">Using Human Feedback to Fine-tune Diffusion Models without Any Reward Model. (arXiv:2311.13231v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1">Kai Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_J/0/1/0/all/0/1">Jian Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1">Jiafei Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_C/0/1/0/all/0/1">Chunjiang Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiaxin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qimai Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1">Weihan Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiaolong Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiu Li</a></p>
<p>Using reinforcement learning with human feedback (RLHF) has shown significant
promise in fine-tuning diffusion models. Previous methods start by training a
reward model that aligns with human preferences, then leverage RL techniques to
fine-tune the underlying models. However, crafting an efficient reward model
demands extensive datasets, optimal architecture, and manual hyperparameter
tuning, making the process both time and cost-intensive. The direct preference
optimization (DPO) method, effective in fine-tuning large language models,
eliminates the necessity for a reward model. However, the extensive GPU memory
requirement of the diffusion model's denoising process hinders the direct
application of the DPO method. To address this issue, we introduce the Direct
Preference for Denoising Diffusion Policy Optimization (D3PO) method to
directly fine-tune diffusion models. The theoretical analysis demonstrates that
although D3PO omits training a reward model, it effectively functions as the
optimal reward model trained using human feedback data to guide the learning
process. This approach requires no training of a reward model, proving to be
more direct, cost-effective, and minimizing computational overhead. In
experiments, our method uses the relative scale of objectives as a proxy for
human preference, delivering comparable results to methods using ground-truth
rewards. Moreover, D3PO demonstrates the ability to reduce image distortion
rates and generate safer images, overcoming challenges lacking robust reward
models. Our code is publicly available in
https://github.com/yk7333/D3PO/tree/main.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13384">LucidDreamer: Domain-free Generation of 3D Gaussian Splatting Scenes. (arXiv:2311.13384v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1">Jaeyoung Chung</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Suyoung Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Nam_H/0/1/0/all/0/1">Hyeongjin Nam</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jaerin Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kyoung Mu Lee</a></p>
<p>With the widespread usage of VR devices and contents, demands for 3D scene
generation techniques become more popular. Existing 3D scene generation models,
however, limit the target scene to specific domain, primarily due to their
training strategies using 3D scan dataset that is far from the real-world. To
address such limitation, we propose LucidDreamer, a domain-free scene
generation pipeline by fully leveraging the power of existing large-scale
diffusion-based generative model. Our LucidDreamer has two alternate steps:
Dreaming and Alignment. First, to generate multi-view consistent images from
inputs, we set the point cloud as a geometrical guideline for each image
generation. Specifically, we project a portion of point cloud to the desired
view and provide the projection as a guidance for inpainting using the
generative model. The inpainted images are lifted to 3D space with estimated
depth maps, composing a new points. Second, to aggregate the new points into
the 3D scene, we propose an aligning algorithm which harmoniously integrates
the portions of newly generated 3D scenes. The finally obtained 3D scene serves
as initial points for optimizing Gaussian splats. LucidDreamer produces
Gaussian splats that are highly-detailed compared to the previous 3D scene
generation methods, with no constraint on domain of the target scene. Project
page: https://luciddreamer-cvlab.github.io/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13559">Transfer Learning-based Real-time Handgun Detection. (arXiv:2311.13559v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Elmir_Y/0/1/0/all/0/1">Youssef Elmir</a>, <a href="http://arxiv.org/find/cs/1/au:+Laouar_S/0/1/0/all/0/1">Sid Ahmed Laouar</a>, <a href="http://arxiv.org/find/cs/1/au:+Hamdaoui_L/0/1/0/all/0/1">Larbi Hamdaoui</a></p>
<p>Traditional surveillance systems rely on human attention, limiting their
effectiveness. This study employs convolutional neural networks and transfer
learning to develop a real-time computer vision system for automatic handgun
detection. Comprehensive analysis of online handgun detection methods is
conducted, emphasizing reducing false positives and learning time. Transfer
learning is demonstrated as an effective approach. Despite technical
challenges, the proposed system achieves a precision rate of 84.74%,
demonstrating promising performance comparable to related works, enabling
faster learning and accurate automatic handgun detection for enhanced security.
This research advances security measures by reducing human monitoring
dependence, showcasing the potential of transfer learning-based approaches for
efficient and reliable handgun detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12850">Meticulously Selecting 1% of the Dataset for Pre-training! Generating Differentially Private Images Data with Semantics Query. (arXiv:2311.12850v1 [cs.CV] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Kecen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1">Chen Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhixiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yuzhong Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1">Xinwen Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tianhao Wang</a></p>
<p>Differential Privacy (DP) image data synthesis, which leverages the DP
technique to generate synthetic data to replace the sensitive data, allowing
organizations to share and utilize synthetic images without privacy concerns.
Previous methods incorporate the advanced techniques of generative models and
pre-training on a public dataset to produce exceptional DP image data, but
suffer from problems of unstable training and massive computational resource
demands. This paper proposes a novel DP image synthesis method, termed
PRIVIMAGE, which meticulously selects pre-training data, promoting the
efficient creation of DP datasets with high fidelity and utility. PRIVIMAGE
first establishes a semantic query function using a public dataset. Then, this
function assists in querying the semantic distribution of the sensitive
dataset, facilitating the selection of data from the public dataset with
analogous semantics for pre-training. Finally, we pre-train an image generative
model using the selected data and then fine-tune this model on the sensitive
dataset using Differentially Private Stochastic Gradient Descent (DP-SGD).
PRIVIMAGE allows us to train a lightly parameterized generative model, reducing
the noise in the gradient during DP-SGD training and enhancing training
stability. Extensive experiments demonstrate that PRIVIMAGE uses only 1% of the
public dataset for pre-training and 7.6% of the parameters in the generative
model compared to the state-of-the-art method, whereas achieves superior
synthetic performance and conserves more computational resources. On average,
PRIVIMAGE achieves 30.1% lower FID and 12.6% higher Classification Accuracy
than the state-of-the-art method. The replication package and datasets can be
accessed online.
</p>
</p>
</div>

    </div>
    </body>
    