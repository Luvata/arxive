<!DOCTYPE html>
<html>
<head>
<title>2023-09-23-cs-ai</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2309.11506">Matching Table Metadata with Business Glossaries Using Large Language Models. (arXiv:2309.11506v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lobo_E/0/1/0/all/0/1">Elita Lobo</a>, <a href="http://arxiv.org/find/cs/1/au:+Hassanzadeh_O/0/1/0/all/0/1">Oktie Hassanzadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Pham_N/0/1/0/all/0/1">Nhan Pham</a>, <a href="http://arxiv.org/find/cs/1/au:+Mihindukulasooriya_N/0/1/0/all/0/1">Nandana Mihindukulasooriya</a>, <a href="http://arxiv.org/find/cs/1/au:+Subramanian_D/0/1/0/all/0/1">Dharmashankar Subramanian</a>, <a href="http://arxiv.org/find/cs/1/au:+Samulowitz_H/0/1/0/all/0/1">Horst Samulowitz</a></p>
<p>Enterprises often own large collections of structured data in the form of
large databases or an enterprise data lake. Such data collections come with
limited metadata and strict access policies that could limit access to the data
contents and, therefore, limit the application of classic retrieval and
analysis solutions. As a result, there is a need for solutions that can
effectively utilize the available metadata. In this paper, we study the problem
of matching table metadata to a business glossary containing data labels and
descriptions. The resulting matching enables the use of an available or curated
business glossary for retrieval and analysis without or before requesting
access to the data contents. One solution to this problem is to use
manually-defined rules or similarity measures on column names and glossary
descriptions (or their vector embeddings) to find the closest match. However,
such approaches need to be tuned through manual labeling and cannot handle many
business glossaries that contain a combination of simple as well as complex and
long descriptions. In this work, we leverage the power of large language models
(LLMs) to design generic matching methods that do not require manual tuning and
can identify complex relations between column names and glossaries. We propose
methods that utilize LLMs in two ways: a) by generating additional context for
column names that can aid with matching b) by using LLMs to directly infer if
there is a relation between column names and glossary descriptions. Our
preliminary experimental results show the effectiveness of our proposed
methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11508">Towards LLM-based Autograding for Short Textual Answers. (arXiv:2309.11508v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1">Johannes Schneider</a>, <a href="http://arxiv.org/find/cs/1/au:+Schenk_B/0/1/0/all/0/1">Bernd Schenk</a>, <a href="http://arxiv.org/find/cs/1/au:+Niklaus_C/0/1/0/all/0/1">Christina Niklaus</a>, <a href="http://arxiv.org/find/cs/1/au:+Vlachos_M/0/1/0/all/0/1">Michaelis Vlachos</a></p>
<p>Grading of exams is an important, labor intensive, subjective, repetitive and
frequently challenging task. The feasibility of autograding textual responses
has greatly increased thanks to the availability of large language models
(LLMs) such as ChatGPT and because of the substantial influx of data brought
about by digitalization. However, entrusting AI models with decision-making
roles raises ethical considerations, mainly stemming from potential biases and
issues related to generating false information. Thus, in this manuscript we
provide an evaluation of a large language model for the purpose of autograding,
while also highlighting how LLMs can support educators in validating their
grading procedures. Our evaluation is targeted towards automatic short textual
answers grading (ASAG), spanning various languages and examinations from two
distinct courses. Our findings suggest that while "out-of-the-box" LLMs provide
a valuable tool to provide a complementary perspective, their readiness for
independent automated grading remains a work in progress, necessitating human
oversight.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11510">When is a Foundation Model a Foundation Model. (arXiv:2309.11510v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alfasly_S/0/1/0/all/0/1">Saghir Alfasly</a>, <a href="http://arxiv.org/find/cs/1/au:+Nejat_P/0/1/0/all/0/1">Peyman Nejat</a>, <a href="http://arxiv.org/find/cs/1/au:+Hemati_S/0/1/0/all/0/1">Sobhan Hemati</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_J/0/1/0/all/0/1">Jibran Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lahr_I/0/1/0/all/0/1">Isaiah Lahr</a>, <a href="http://arxiv.org/find/cs/1/au:+Alsaafin_A/0/1/0/all/0/1">Areej Alsaafin</a>, <a href="http://arxiv.org/find/cs/1/au:+Shafique_A/0/1/0/all/0/1">Abubakr Shafique</a>, <a href="http://arxiv.org/find/cs/1/au:+Comfere_N/0/1/0/all/0/1">Nneka Comfere</a>, <a href="http://arxiv.org/find/cs/1/au:+Murphree_D/0/1/0/all/0/1">Dennis Murphree</a>, <a href="http://arxiv.org/find/cs/1/au:+Meroueh_C/0/1/0/all/0/1">Chady Meroueh</a>, <a href="http://arxiv.org/find/cs/1/au:+Yasir_S/0/1/0/all/0/1">Saba Yasir</a>, <a href="http://arxiv.org/find/cs/1/au:+Mangold_A/0/1/0/all/0/1">Aaron Mangold</a>, <a href="http://arxiv.org/find/cs/1/au:+Boardman_L/0/1/0/all/0/1">Lisa Boardman</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_V/0/1/0/all/0/1">Vijay Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Garcia_J/0/1/0/all/0/1">Joaquin J. Garcia</a>, <a href="http://arxiv.org/find/cs/1/au:+Tizhoosh_H/0/1/0/all/0/1">H.R. Tizhoosh</a></p>
<p>Recently, several studies have reported on the fine-tuning of foundation
models for image-text modeling in the field of medicine, utilizing images from
online data sources such as Twitter and PubMed. Foundation models are large,
deep artificial neural networks capable of learning the context of a specific
domain through training on exceptionally extensive datasets. Through
validation, we have observed that the representations generated by such models
exhibit inferior performance in retrieval tasks within digital pathology when
compared to those generated by significantly smaller, conventional deep
networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11526">Likelihood-based Sensor Calibration for Expert-Supported Distributed Learning Algorithms in IoT Systems. (arXiv:2309.11526v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Machhamer_R/0/1/0/all/0/1">R&#xfc;diger Machhamer</a>, <a href="http://arxiv.org/find/cs/1/au:+Fazlic_L/0/1/0/all/0/1">Lejla Begic Fazlic</a>, <a href="http://arxiv.org/find/cs/1/au:+Guven_E/0/1/0/all/0/1">Eray Guven</a>, <a href="http://arxiv.org/find/cs/1/au:+Junk_D/0/1/0/all/0/1">David Junk</a>, <a href="http://arxiv.org/find/cs/1/au:+Kurt_G/0/1/0/all/0/1">Gunes Karabulut Kurt</a>, <a href="http://arxiv.org/find/cs/1/au:+Naumann_S/0/1/0/all/0/1">Stefan Naumann</a>, <a href="http://arxiv.org/find/cs/1/au:+Didas_S/0/1/0/all/0/1">Stephan Didas</a>, <a href="http://arxiv.org/find/cs/1/au:+Gollmer_K/0/1/0/all/0/1">Klaus-Uwe Gollmer</a>, <a href="http://arxiv.org/find/cs/1/au:+Bergmann_R/0/1/0/all/0/1">Ralph Bergmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Timm_I/0/1/0/all/0/1">Ingo J. Timm</a>, <a href="http://arxiv.org/find/cs/1/au:+Dartmann_G/0/1/0/all/0/1">Guido Dartmann</a></p>
<p>An important task in the field of sensor technology is the efficient
implementation of adaptation procedures of measurements from one sensor to
another sensor of identical design. One idea is to use the estimation of an
affine transformation between different systems, which can be improved by the
knowledge of experts. This paper presents an improved solution from Glacier
Research that was published back in 1973. It is shown that this solution can be
adapted for software calibration of sensors, implementation of expert-based
adaptation, and federated learning methods. We evaluate our research with
simulations and also with real measured data of a multi-sensor board with 8
identical sensors. The results show an improvement for both the simulation and
the experiments with real data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11527">TrueLearn: A Python Library for Personalised Informational Recommendations with (Implicit) Feedback. (arXiv:2309.11527v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1">Yuxiang Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Djemili_K/0/1/0/all/0/1">Karim Djemili</a>, <a href="http://arxiv.org/find/cs/1/au:+Elezi_D/0/1/0/all/0/1">Denis Elezi</a>, <a href="http://arxiv.org/find/cs/1/au:+Shalman_A/0/1/0/all/0/1">Aaneel Shalman</a>, <a href="http://arxiv.org/find/cs/1/au:+Perez_Ortiz_M/0/1/0/all/0/1">Mar&#xed;a P&#xe9;rez-Ortiz</a>, <a href="http://arxiv.org/find/cs/1/au:+Bulathwela_S/0/1/0/all/0/1">Sahan Bulathwela</a></p>
<p>This work describes the TrueLearn Python library, which contains a family of
online learning Bayesian models for building educational (or more generally,
informational) recommendation systems. This family of models was designed
following the "open learner" concept, using humanly-intuitive user
representations. For the sake of interpretability and putting the user in
control, the TrueLearn library also contains different representations to help
end-users visualise the learner models, which may in the future facilitate user
interaction with their own models. Together with the library, we include a
previously publicly released implicit feedback educational dataset with
evaluation metrics to measure the performance of the models. The extensive
documentation and coding examples make the library highly accessible to both
machine learning developers and educational data mining and learning analytic
practitioners. The library and the support documentation with examples are
available at https://truelearn.readthedocs.io/en/latest.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11528">Learning Complete Topology-Aware Correlations Between Relations for Inductive Link Prediction. (arXiv:2309.11528v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hanzhu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_Q/0/1/0/all/0/1">Qitan Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1">Zhihao Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiajun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1">Huarui He</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1">Hongtao Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yongdong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1">Feng Wu</a></p>
<p>Inductive link prediction -- where entities during training and inference
stages can be different -- has shown great potential for completing evolving
knowledge graphs in an entity-independent manner. Many popular methods mainly
focus on modeling graph-level features, while the edge-level interactions --
especially the semantic correlations between relations -- have been less
explored. However, we notice a desirable property of semantic correlations
between relations is that they are inherently edge-level and
entity-independent. This implies the great potential of the semantic
correlations for the entity-independent inductive link prediction task.
Inspired by this observation, we propose a novel subgraph-based method, namely
TACO, to model Topology-Aware COrrelations between relations that are highly
correlated to their topological structures within subgraphs. Specifically, we
prove that semantic correlations between any two relations can be categorized
into seven topological patterns, and then proposes Relational Correlation
Network (RCN) to learn the importance of each pattern. To further exploit the
potential of RCN, we propose Complete Common Neighbor induced subgraph that can
effectively preserve complete topological patterns within the subgraph.
Extensive experiments demonstrate that TACO effectively unifies the graph-level
information and edge-level interactions to jointly perform reasoning, leading
to a superior performance over existing state-of-the-art methods for the
inductive link prediction task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11555">Limitations in odour recognition and generalisation in a neuromorphic olfactory circuit. (arXiv:2309.11555v1 [cs.NE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dennler_N/0/1/0/all/0/1">Nik Dennler</a>, <a href="http://arxiv.org/find/cs/1/au:+Schaik_A/0/1/0/all/0/1">Andr&#xe9; van Schaik</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmuker_M/0/1/0/all/0/1">Michael Schmuker</a></p>
<p>Neuromorphic computing is one of the few current approaches that have the
potential to significantly reduce power consumption in Machine Learning and
Artificial Intelligence. Imam &amp; Cleland presented an odour-learning algorithm
that runs on a neuromorphic architecture and is inspired by circuits described
in the mammalian olfactory bulb. They assess the algorithm's performance in
"rapid online learning and identification" of gaseous odorants and odorless
gases (short "gases") using a set of gas sensor recordings of different odour
presentations and corrupting them by impulse noise. We replicated parts of the
study and discovered limitations that affect some of the conclusions drawn.
First, the dataset used suffers from sensor drift and a non-randomised
measurement protocol, rendering it of limited use for odour identification
benchmarks. Second, we found that the model is restricted in its ability to
generalise over repeated presentations of the same gas. We demonstrate that the
task the study refers to can be solved with a simple hash table approach,
matching or exceeding the reported results in accuracy and runtime. Therefore,
a validation of the model that goes beyond restoring a learned data sample
remains to be shown, in particular its suitability to odour identification
tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11568">BTLM-3B-8K: 7B Parameter Performance in a 3B Parameter Model. (arXiv:2309.11568v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dey_N/0/1/0/all/0/1">Nolan Dey</a>, <a href="http://arxiv.org/find/cs/1/au:+Soboleva_D/0/1/0/all/0/1">Daria Soboleva</a>, <a href="http://arxiv.org/find/cs/1/au:+Al_Khateeb_F/0/1/0/all/0/1">Faisal Al-Khateeb</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1">Bowen Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pathria_R/0/1/0/all/0/1">Ribhu Pathria</a>, <a href="http://arxiv.org/find/cs/1/au:+Khachane_H/0/1/0/all/0/1">Hemant Khachane</a>, <a href="http://arxiv.org/find/cs/1/au:+Muhammad_S/0/1/0/all/0/1">Shaheer Muhammad</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhiming/0/1/0/all/0/1">Zhiming</a> (Charles) <a href="http://arxiv.org/find/cs/1/au:+Chen/0/1/0/all/0/1">Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Myers_R/0/1/0/all/0/1">Robert Myers</a>, <a href="http://arxiv.org/find/cs/1/au:+Steeves_J/0/1/0/all/0/1">Jacob Robert Steeves</a>, <a href="http://arxiv.org/find/cs/1/au:+Vassilieva_N/0/1/0/all/0/1">Natalia Vassilieva</a>, <a href="http://arxiv.org/find/cs/1/au:+Tom_M/0/1/0/all/0/1">Marvin Tom</a>, <a href="http://arxiv.org/find/cs/1/au:+Hestness_J/0/1/0/all/0/1">Joel Hestness</a></p>
<p>We introduce the Bittensor Language Model, called "BTLM-3B-8K", a new
state-of-the-art 3 billion parameter open-source language model. BTLM-3B-8K was
trained on 627B tokens from the SlimPajama dataset with a mixture of 2,048 and
8,192 context lengths. BTLM-3B-8K outperforms all existing 3B parameter models
by 2-5.5% across downstream tasks. BTLM-3B-8K is even competitive with some 7B
parameter models. Additionally, BTLM-3B-8K provides excellent long context
performance, outperforming MPT-7B-8K and XGen-7B-8K on tasks up to 8,192
context length. We trained the model on a cleaned and deduplicated SlimPajama
dataset; aggressively tuned the \textmu P hyperparameters and schedule; used
ALiBi position embeddings; and adopted the SwiGLU nonlinearity.
</p>
<p>On Hugging Face, the most popular models have 7B parameters, indicating that
users prefer the quality-size ratio of 7B models. Compacting the 7B parameter
model to one with 3B parameters, with little performance impact, is an
important milestone. BTLM-3B-8K needs only 3GB of memory with 4-bit precision
and takes 2.5x less inference compute than 7B models, helping to open up access
to a powerful language model on mobile and edge devices. BTLM-3B-8K is
available under an Apache 2.0 license on Hugging Face:
https://huggingface.co/cerebras/btlm-3b-8k-base.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11575">Distilling Adversarial Prompts from Safety Benchmarks: Report for the Adversarial Nibbler Challenge. (arXiv:2309.11575v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Brack_M/0/1/0/all/0/1">Manuel Brack</a>, <a href="http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1">Patrick Schramowski</a>, <a href="http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1">Kristian Kersting</a></p>
<p>Text-conditioned image generation models have recently achieved astonishing
image quality and alignment results. Consequently, they are employed in a
fast-growing number of applications. Since they are highly data-driven, relying
on billion-sized datasets randomly scraped from the web, they also produce
unsafe content. As a contribution to the Adversarial Nibbler challenge, we
distill a large set of over 1,000 potential adversarial inputs from existing
safety benchmarks. Our analysis of the gathered prompts and corresponding
images demonstrates the fragility of input filters and provides further
insights into systematic safety issues in current generative image models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11587">CATS: Conditional Adversarial Trajectory Synthesis for Privacy-Preserving Trajectory Data Publication Using Deep Learning Approaches. (arXiv:2309.11587v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1">Jinmeng Rao</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1">Song Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1">Sijia Zhu</a></p>
<p>The prevalence of ubiquitous location-aware devices and mobile Internet
enables us to collect massive individual-level trajectory dataset from users.
Such trajectory big data bring new opportunities to human mobility research but
also raise public concerns with regard to location privacy. In this work, we
present the Conditional Adversarial Trajectory Synthesis (CATS), a
deep-learning-based GeoAI methodological framework for privacy-preserving
trajectory data generation and publication. CATS applies K-anonymity to the
underlying spatiotemporal distributions of human movements, which provides a
distributional-level strong privacy guarantee. By leveraging conditional
adversarial training on K-anonymized human mobility matrices, trajectory global
context learning using the attention-based mechanism, and recurrent bipartite
graph matching of adjacent trajectory points, CATS is able to reconstruct
trajectory topology from conditionally sampled locations and generate
high-quality individual-level synthetic trajectory data, which can serve as
supplements or alternatives to raw data for privacy-preserving trajectory data
publication. The experiment results on over 90k GPS trajectories show that our
method has a better performance in privacy preservation, spatiotemporal
characteristic preservation, and downstream utility compared with baseline
methods, which brings new insights into privacy-preserving human mobility
research using generative AI techniques and explores data ethics issues in
GIScience.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11608">Dataset Factory: A Toolchain For Generative Computer Vision Datasets. (arXiv:2309.11608v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kharitonov_D/0/1/0/all/0/1">Daniel Kharitonov</a>, <a href="http://arxiv.org/find/cs/1/au:+Turner_R/0/1/0/all/0/1">Ryan Turner</a></p>
<p>Generative AI workflows heavily rely on data-centric tasks - such as
filtering samples by annotation fields, vector distances, or scores produced by
custom classifiers. At the same time, computer vision datasets are quickly
approaching petabyte volumes, rendering data wrangling difficult. In addition,
the iterative nature of data preparation necessitates robust dataset sharing
and versioning mechanisms, both of which are hard to implement ad-hoc. To solve
these challenges, we propose a "dataset factory" approach that separates the
storage and processing of samples from metadata and enables data-centric
operations at scale for machine learning teams and individual researchers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11610">Hand Gesture Recognition with Two Stage Approach Using Transfer Learning and Deep Ensemble Learning. (arXiv:2309.11610v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Savas_S/0/1/0/all/0/1">Serkan Sava&#x15f;</a>, <a href="http://arxiv.org/find/cs/1/au:+Erguzen_A/0/1/0/all/0/1">Atilla Erg&#xfc;zen</a></p>
<p>Human-Computer Interaction (HCI) has been the subject of research for many
years, and recent studies have focused on improving its performance through
various techniques. In the past decade, deep learning studies have shown high
performance in various research areas, leading researchers to explore their
application to HCI. Convolutional neural networks can be used to recognize hand
gestures from images using deep architectures. In this study, we evaluated
pre-trained high-performance deep architectures on the HG14 dataset, which
consists of 14 different hand gesture classes. Among 22 different models,
versions of the VGGNet and MobileNet models attained the highest accuracy
rates. Specifically, the VGG16 and VGG19 models achieved accuracy rates of
94.64% and 94.36%, respectively, while the MobileNet and MobileNetV2 models
achieved accuracy rates of 96.79% and 94.43%, respectively. We performed hand
gesture recognition on the dataset using an ensemble learning technique, which
combined the four most successful models. By utilizing these models as base
learners and applying the Dirichlet ensemble technique, we achieved an accuracy
rate of 98.88%. These results demonstrate the effectiveness of the deep
ensemble learning technique for HCI and its potential applications in areas
such as augmented reality, virtual reality, and game technologies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11619">Cloud-Based Hierarchical Imitation Learning for Scalable Transfer of Construction Skills from Human Workers to Assisting Robots. (arXiv:2309.11619v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Hongrui Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kamat_V/0/1/0/all/0/1">Vineet R. Kamat</a>, <a href="http://arxiv.org/find/cs/1/au:+Menassa_C/0/1/0/all/0/1">Carol C. Menassa</a></p>
<p>Assigning repetitive and physically-demanding construction tasks to robots
can alleviate human workers's exposure to occupational injuries. Transferring
necessary dexterous and adaptive artisanal construction craft skills from
workers to robots is crucial for the successful delegation of construction
tasks and achieving high-quality robot-constructed work. Predefined motion
planning scripts tend to generate rigid and collision-prone robotic behaviors
in unstructured construction site environments. In contrast, Imitation Learning
(IL) offers a more robust and flexible skill transfer scheme. However, the
majority of IL algorithms rely on human workers to repeatedly demonstrate task
performance at full scale, which can be counterproductive and infeasible in the
case of construction work. To address this concern, this paper proposes an
immersive, cloud robotics-based virtual demonstration framework that serves two
primary purposes. First, it digitalizes the demonstration process, eliminating
the need for repetitive physical manipulation of heavy construction objects.
Second, it employs a federated collection of reusable demonstrations that are
transferable for similar tasks in the future and can thus reduce the
requirement for repetitive illustration of tasks by human agents. Additionally,
to enhance the trustworthiness, explainability, and ethical soundness of the
robot training, this framework utilizes a Hierarchical Imitation Learning (HIL)
model to decompose human manipulation skills into sequential and reactive
sub-skills. These two layers of skills are represented by deep generative
models, enabling adaptive control of robot actions. By delegating the physical
strains of construction work to human-trained robots, this framework promotes
the inclusion of workers with diverse physical capabilities and educational
backgrounds within the construction industry.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11638">A survey on the semantics of sequential patterns with negation. (arXiv:2309.11638v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guyet_T/0/1/0/all/0/1">Thomas Guyet</a></p>
<p>A sequential pattern with negation, or negative sequential pattern, takes the
form of a sequential pattern for which the negation symbol may be used in front
of some of the pattern's itemsets. Intuitively, such a pattern occurs in a
sequence if negated itemsets are absent in the sequence. Recent work has shown
that different semantics can be attributed to these pattern forms, and that
state-of-the-art algorithms do not extract the same sets of patterns. This
raises the important question of the interpretability of sequential pattern
with negation. In this study, our focus is on exploring how potential users
perceive negation in sequential patterns. Our aim is to determine whether
specific semantics are more "intuitive" than others and whether these align
with the semantics employed by one or more state-of-the-art algorithms. To
achieve this, we designed a questionnaire to reveal the semantics' intuition of
each user. This article presents both the design of the questionnaire and an
in-depth analysis of the 124 responses obtained. The outcomes indicate that two
of the semantics are predominantly intuitive; however, neither of them aligns
with the semantics of the primary state-of-the-art algorithms. As a result, we
provide recommendations to account for this disparity in the conclusions drawn.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11641">Attentive VQ-VAE. (arXiv:2309.11641v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rivera_M/0/1/0/all/0/1">Mariano Rivera</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoyos_A/0/1/0/all/0/1">Angello Hoyos</a></p>
<p>We present a novel approach to enhance the capabilities of VQVAE models
through the integration of an Attentive Residual Encoder (AREN) and a Residual
Pixel Attention layer. The objective of our research is to improve the
performance of VQVAE while maintaining practical parameter levels. The AREN
encoder is designed to operate effectively at multiple levels, accommodating
diverse architectural complexities. The key innovation is the integration of an
inter-pixel auto-attention mechanism into the AREN encoder. This approach
allows us to efficiently capture and utilize contextual information across
latent vectors. Additionally, our models uses additional encoding levels to
further enhance the model's representational power. Our attention layer employs
a minimal parameter approach, ensuring that latent vectors are modified only
when pertinent information from other pixels is available. Experimental results
demonstrate that our proposed modifications lead to significant improvements in
data representation and generation, making VQVAEs even more suitable for a wide
range of applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11648">Orbital AI-based Autonomous Refuelling Solution. (arXiv:2309.11648v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rondao_D/0/1/0/all/0/1">Duarte Rondao</a>, <a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1">Lei He</a>, <a href="http://arxiv.org/find/cs/1/au:+Aouf_N/0/1/0/all/0/1">Nabil Aouf</a></p>
<p>Cameras are rapidly becoming the choice for on-board sensors towards space
rendezvous due to their small form factor and inexpensive power, mass, and
volume costs. When it comes to docking, however, they typically serve a
secondary role, whereas the main work is done by active sensors such as lidar.
This paper documents the development of a proposed AI-based (artificial
intelligence) navigation algorithm intending to mature the use of on-board
visible wavelength cameras as a main sensor for docking and on-orbit servicing
(OOS), reducing the dependency on lidar and greatly reducing costs.
Specifically, the use of AI enables the expansion of the relative navigation
solution towards multiple classes of scenarios, e.g., in terms of targets or
illumination conditions, which would otherwise have to be crafted on a
case-by-case manner using classical image processing methods. Multiple
convolutional neural network (CNN) backbone architectures are benchmarked on
synthetically generated data of docking manoeuvres with the International Space
Station (ISS), achieving position and attitude estimates close to 1%
range-normalised and 1 deg, respectively. The integration of the solution with
a physical prototype of the refuelling mechanism is validated in laboratory
using a robotic arm to simulate a berthing procedure.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11653">&quot;It&#x27;s a Fair Game&#x27;&#x27;, or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational Agents. (arXiv:2309.11653v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhiping Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_M/0/1/0/all/0/1">Michelle Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Hao-Ping/0/1/0/all/0/1">Hao-Ping</a> (Hank)Lee, <a href="http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1">Bingsheng Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1">Sauvik Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Lerner_A/0/1/0/all/0/1">Ada Lerner</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dakuo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1">Tianshi Li</a></p>
<p>The widespread use of Large Language Model (LLM)-based conversational agents
(CAs), especially in high-stakes domains, raises many privacy concerns.
Building ethical LLM-based CAs that respect user privacy requires an in-depth
understanding of the privacy risks that concern users the most. However,
existing research, primarily model-centered, does not provide insight into
users' perspectives. To bridge this gap, we analyzed sensitive disclosures in
real-world ChatGPT conversations and conducted semi-structured interviews with
19 LLM-based CA users. We found that users are constantly faced with trade-offs
between privacy, utility, and convenience when using LLM-based CAs. However,
users' erroneous mental models and the dark patterns in system design limited
their awareness and comprehension of the privacy risks. Additionally, the
human-like interactions encouraged more sensitive disclosures, which
complicated users' ability to navigate the trade-offs. We discuss practical
design guidelines and the needs for paradigmatic shifts to protect the privacy
of LLM-based CA users.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11672">Generative AI in Mafia-like Game Simulation. (arXiv:2309.11672v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1">Munyeong Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Sungsu Kim</a></p>
<p>In this research, we explore the efficacy and potential of Generative AI
models, specifically focusing on their application in role-playing simulations
exemplified through Spyfall, a renowned mafia-style game. By leveraging GPT-4's
advanced capabilities, the study aimed to showcase the model's potential in
understanding, decision-making, and interaction during game scenarios.
Comparative analyses between GPT-4 and its predecessor, GPT-3.5-turbo,
demonstrated GPT-4's enhanced adaptability to the game environment, with
significant improvements in posing relevant questions and forming human-like
responses. However, challenges such as the model;s limitations in bluffing and
predicting opponent moves emerged. Reflections on game development, financial
constraints, and non-verbal limitations of the study were also discussed. The
findings suggest that while GPT-4 exhibits promising advancements over earlier
models, there remains potential for further development, especially in
instilling more human-like attributes in AI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11680">Federated Learning with Neural Graphical Models. (arXiv:2309.11680v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chajewska_U/0/1/0/all/0/1">Urszula Chajewska</a>, <a href="http://arxiv.org/find/cs/1/au:+Shrivastava_H/0/1/0/all/0/1">Harsh Shrivastava</a></p>
<p>Federated Learning (FL) addresses the need to create models based on
proprietary data in such a way that multiple clients retain exclusive control
over their data, while all benefit from improved model accuracy due to pooled
resources. Recently proposed Neural Graphical Models (NGMs) are Probabilistic
Graphical models that utilize the expressive power of neural networks to learn
complex non-linear dependencies between the input features. They learn to
capture the underlying data distribution and have efficient algorithms for
inference and sampling. We develop a FL framework which maintains a global NGM
model that learns the averaged information from the local NGM models while
keeping the training data within the client's environment. Our design, FedNGMs,
avoids the pitfalls and shortcomings of neuron matching frameworks like
Federated Matched Averaging that suffers from model parameter explosion. Our
global model size remains constant throughout the process. In the cases where
clients have local variables that are not part of the combined global
distribution, we propose a `Stitching' algorithm, which personalizes the global
NGM models by merging the additional variables using the client's data. FedNGM
is robust to data heterogeneity, large number of participants, and limited
communication bandwidth.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11682">Dr. FERMI: A Stochastic Distributionally Robust Fair Empirical Risk Minimization Framework. (arXiv:2309.11682v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Baharlouei_S/0/1/0/all/0/1">Sina Baharlouei</a>, <a href="http://arxiv.org/find/cs/1/au:+Razaviyayn_M/0/1/0/all/0/1">Meisam Razaviyayn</a></p>
<p>While training fair machine learning models has been studied extensively in
recent years, most developed methods rely on the assumption that the training
and test data have similar distributions. In the presence of distribution
shifts, fair models may behave unfairly on test data. There have been some
developments for fair learning robust to distribution shifts to address this
shortcoming. However, most proposed solutions are based on the assumption of
having access to the causal graph describing the interaction of different
features. Moreover, existing algorithms require full access to data and cannot
be used when small batches are used (stochastic/batch implementation). This
paper proposes the first stochastic distributionally robust fairness framework
with convergence guarantees that do not require knowledge of the causal graph.
More specifically, we formulate the fair inference in the presence of the
distribution shift as a distributionally robust optimization problem under
$L_p$ norm uncertainty sets with respect to the Exponential Renyi Mutual
Information (ERMI) as the measure of fairness violation. We then discuss how
the proposed method can be implemented in a stochastic fashion. We have
evaluated the presented framework's performance and efficiency through
extensive experiments on real datasets consisting of distribution shifts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11688">LLM Guided Inductive Inference for Solving Compositional Problems. (arXiv:2309.11688v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sodani_A/0/1/0/all/0/1">Abhigya Sodani</a>, <a href="http://arxiv.org/find/cs/1/au:+Moos_L/0/1/0/all/0/1">Lauren Moos</a>, <a href="http://arxiv.org/find/cs/1/au:+Mirman_M/0/1/0/all/0/1">Matthew Mirman</a></p>
<p>While large language models (LLMs) have demonstrated impressive performance
in question-answering tasks, their performance is limited when the questions
require knowledge that is not included in the model's training data and can
only be acquired through direct observation or interaction with the real world.
Existing methods decompose reasoning tasks through the use of modules invoked
sequentially, limiting their ability to answer deep reasoning tasks. We
introduce a method, Recursion based extensible LLM (REBEL), which handles
open-world, deep reasoning tasks by employing automated reasoning techniques
like dynamic planning and forward-chaining strategies. REBEL allows LLMs to
reason via recursive problem decomposition and utilization of external tools.
The tools that REBEL uses are specified only by natural language description.
We further demonstrate REBEL capabilities on a set of problems that require a
deeply nested use of external tools in a compositional and conversational
setting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11691">RAI4IoE: Responsible AI for Enabling the Internet of Energy. (arXiv:2309.11691v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1">Minhui Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Nepal_S/0/1/0/all/0/1">Surya Nepal</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Ling Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sethuvenkatraman_S/0/1/0/all/0/1">Subbu Sethuvenkatraman</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1">Xingliang Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Rudolph_C/0/1/0/all/0/1">Carsten Rudolph</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1">Ruoxi Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Eisenhauer_G/0/1/0/all/0/1">Greg Eisenhauer</a></p>
<p>This paper plans to develop an Equitable and Responsible AI framework with
enabling techniques and algorithms for the Internet of Energy (IoE), in short,
RAI4IoE. The energy sector is going through substantial changes fueled by two
key drivers: building a zero-carbon energy sector and the digital
transformation of the energy infrastructure. We expect to see the convergence
of these two drivers resulting in the IoE, where renewable distributed energy
resources (DERs), such as electric cars, storage batteries, wind turbines and
photovoltaics (PV), can be connected and integrated for reliable energy
distribution by leveraging advanced 5G-6G networks and AI technology. This
allows DER owners as prosumers to participate in the energy market and derive
economic incentives. DERs are inherently asset-driven and face equitable
challenges (i.e., fair, diverse and inclusive). Without equitable access,
privileged individuals, groups and organizations can participate and benefit at
the cost of disadvantaged groups. The real-time management of DER resources not
only brings out the equity problem to the IoE, it also collects highly
sensitive location, time, activity dependent data, which requires to be handled
responsibly (e.g., privacy, security and safety), for AI-enhanced predictions,
optimization and prioritization services, and automated management of flexible
resources. The vision of our project is to ensure equitable participation of
the community members and responsible use of their data in IoE so that it could
reap the benefits of advances in AI to provide safe, reliable and sustainable
energy services.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11714">A Dynamic Domain Adaptation Deep Learning Network for EEG-based Motor Imagery Classification. (arXiv:2309.11714v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Jiao_J/0/1/0/all/0/1">Jie Jiao</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_M/0/1/0/all/0/1">Meiyan Xu</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_Q/0/1/0/all/0/1">Qingqing Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_H/0/1/0/all/0/1">Hefan Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_W/0/1/0/all/0/1">Wangliang Zhou</a></p>
<p>There is a correlation between adjacent channels of electroencephalogram
(EEG), and how to represent this correlation is an issue that is currently
being explored. In addition, due to inter-individual differences in EEG
signals, this discrepancy results in new subjects need spend a amount of
calibration time for EEG-based motor imagery brain-computer interface. In order
to solve the above problems, we propose a Dynamic Domain Adaptation Based Deep
Learning Network (DADL-Net). First, the EEG data is mapped to the
three-dimensional geometric space and its temporal-spatial features are learned
through the 3D convolution module, and then the spatial-channel attention
mechanism is used to strengthen the features, and the final convolution module
can further learn the spatial-temporal information of the features. Finally, to
account for inter-subject and cross-sessions differences, we employ a dynamic
domain-adaptive strategy, the distance between features is reduced by
introducing a Maximum Mean Discrepancy loss function, and the classification
layer is fine-tuned by using part of the target domain data. We verify the
performance of the proposed method on BCI competition IV 2a and OpenBMI
datasets. Under the intra-subject experiment, the accuracy rates of 70.42% and
73.91% were achieved on the OpenBMI and BCIC IV 2a datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11724">Emotion-Aware Prosodic Phrasing for Expressive Text-to-Speech. (arXiv:2309.11724v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1">Rui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Haizhou Li</a></p>
<p>Prosodic phrasing is crucial to the naturalness and intelligibility of
end-to-end Text-to-Speech (TTS). There exist both linguistic and emotional
prosody in natural speech. As the study of prosodic phrasing has been
linguistically motivated, prosodic phrasing for expressive emotion rendering
has not been well studied. In this paper, we propose an emotion-aware prosodic
phrasing model, termed \textit{EmoPP}, to mine the emotional cues of utterance
accurately and predict appropriate phrase breaks. We first conduct objective
observations on the ESD dataset to validate the strong correlation between
emotion and prosodic phrasing. Then the objective and subjective evaluations
show that the EmoPP outperforms all baselines and achieves remarkable
performance in terms of emotion expressiveness. The audio samples and the code
are available at \url{https://github.com/AI-S2-Lab/EmoPP}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11725">FluentEditor: Text-based Speech Editing by Considering Acoustic and Prosody Consistency. (arXiv:2309.11725v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1">Rui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xi_J/0/1/0/all/0/1">Jiatian Xi</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1">Ziyue Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Haizhou Li</a></p>
<p>Text-based speech editing (TSE) techniques are designed to enable users to
edit the output audio by modifying the input text transcript instead of the
audio itself. Despite much progress in neural network-based TSE techniques, the
current techniques have focused on reducing the difference between the
generated speech segment and the reference target in the editing region,
ignoring its local and global fluency in the context and original utterance. To
maintain the speech fluency, we propose a fluency speech editing model, termed
\textit{FluentEditor}, by considering fluency-aware training criterion in the
TSE training. Specifically, the \textit{acoustic consistency constraint} aims
to smooth the transition between the edited region and its neighboring acoustic
segments consistent with the ground truth, while the \textit{prosody
consistency constraint} seeks to ensure that the prosody attributes within the
edited regions remain consistent with the overall style of the original
utterance. The subjective and objective experimental results on VCTK
demonstrate that our \textit{FluentEditor} outperforms all advanced baselines
in terms of naturalness and fluency. The audio samples and code are available
at \url{https://github.com/Ai-S2-Lab/FluentEditor}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11737">Choice-75: A Dataset on Decision Branching in Script Learning. (arXiv:2309.11737v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1">Zhaoyi Joey Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Li Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1">Chris Callison-Burch</a></p>
<p>Script learning studies how daily events unfold. Previous works tend to
consider a script as a linear sequence of events while ignoring the potential
branches that arise due to people's circumstantial choices. We hence propose
Choice-75, the first benchmark that challenges intelligent systems to predict
decisions given descriptive scenarios, containing 75 scripts and more than 600
scenarios. While large language models demonstrate overall decent performances,
there is still notable room for improvement in many hard scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11751">How Robust is Google&#x27;s Bard to Adversarial Image Attacks?. (arXiv:2309.11751v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1">Yinpeng Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huanran Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiawei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1">Zhengwei Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xiao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yichi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yu Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1">Hang Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jun Zhu</a></p>
<p>Multimodal Large Language Models (MLLMs) that integrate text and other
modalities (especially vision) have achieved unprecedented performance in
various multimodal tasks. However, due to the unsolved adversarial robustness
problem of vision models, MLLMs can have more severe safety and security risks
by introducing the vision inputs. In this work, we study the adversarial
robustness of Google's Bard, a competitive chatbot to ChatGPT that released its
multimodal capability recently, to better understand the vulnerabilities of
commercial MLLMs. By attacking white-box surrogate vision encoders or MLLMs,
the generated adversarial examples can mislead Bard to output wrong image
descriptions with a 22% success rate based solely on the transferability. We
show that the adversarial examples can also attack other MLLMs, e.g., a 26%
attack success rate against Bing Chat and a 86% attack success rate against
ERNIE bot. Moreover, we identify two defense mechanisms of Bard, including face
detection and toxicity detection of images. We design corresponding attacks to
evade these defenses, demonstrating that the current defenses of Bard are also
vulnerable. We hope this work can deepen our understanding on the robustness of
MLLMs and facilitate future research on defenses. Our code is available at
https://github.com/thu-ml/Attack-Bard.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11753">Improve the efficiency of deep reinforcement learning through semantic exploration guided by natural language. (arXiv:2309.11753v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Zhourui Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_M/0/1/0/all/0/1">Meng Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_Q/0/1/0/all/0/1">Qiyue Yin</a></p>
<p>Reinforcement learning is a powerful technique for learning from trial and
error, but it often requires a large number of interactions to achieve good
performance. In some domains, such as sparse-reward tasks, an oracle that can
provide useful feedback or guidance to the agent during the learning process is
really of great importance. However, querying the oracle too frequently may be
costly or impractical, and the oracle may not always have a clear answer for
every situation. Therefore, we propose a novel method for interacting with the
oracle in a selective and efficient way, using a retrieval-based approach. We
assume that the interaction can be modeled as a sequence of templated questions
and answers, and that there is a large corpus of previous interactions
available. We use a neural network to encode the current state of the agent and
the oracle, and retrieve the most relevant question from the corpus to ask the
oracle. We then use the oracle's answer to update the agent's policy and value
function. We evaluate our method on an object manipulation task. We show that
our method can significantly improve the efficiency of RL by reducing the
number of interactions needed to reach a certain level of performance, compared
to baselines that do not use the oracle or use it in a naive way.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11755">2DDATA: 2D Detection Annotations Transmittable Aggregation for Semantic Segmentation on Point Cloud. (arXiv:2309.11755v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1">Guan-Cheng Lee</a></p>
<p>Recently, multi-modality models have been introduced because of the
complementary information from different sensors such as LiDAR and cameras. It
requires paired data along with precise calibrations for all modalities, the
complicated calibration among modalities hugely increases the cost of
collecting such high-quality datasets, and hinder it from being applied to
practical scenarios. Inherit from the previous works, we not only fuse the
information from multi-modality without above issues, and also exhaust the
information in the RGB modality. We introduced the 2D Detection Annotations
Transmittable Aggregation(\textbf{2DDATA}), designing a data-specific branch,
called \textbf{Local Object Branch}, which aims to deal with points in a
certain bounding box, because of its easiness of acquiring 2D bounding box
annotations. We demonstrate that our simple design can transmit bounding box
prior information to the 3D encoder model, proving the feasibility of large
multi-modality models fused with modality-specific data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11782">DimCL: Dimensional Contrastive Learning For Improving Self-Supervised Learning. (arXiv:2309.11782v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Thanh Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1">Trung Pham</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chaoning Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luu_T/0/1/0/all/0/1">Tung Luu</a>, <a href="http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1">Thang Vu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoo_C/0/1/0/all/0/1">Chang D. Yoo</a></p>
<p>Self-supervised learning (SSL) has gained remarkable success, for which
contrastive learning (CL) plays a key role. However, the recent development of
new non-CL frameworks has achieved comparable or better performance with high
improvement potential, prompting researchers to enhance these frameworks
further. Assimilating CL into non-CL frameworks has been thought to be
beneficial, but empirical evidence indicates no visible improvements. In view
of that, this paper proposes a strategy of performing CL along the dimensional
direction instead of along the batch direction as done in conventional
contrastive learning, named Dimensional Contrastive Learning (DimCL). DimCL
aims to enhance the feature diversity, and it can serve as a regularizer to
prior SSL frameworks. DimCL has been found to be effective, and the
hardness-aware property is identified as a critical reason for its success.
Extensive experimental results reveal that assimilating DimCL into SSL
frameworks leads to performance improvement by a non-trivial margin on various
datasets and backbone architectures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11805">JobRecoGPT -- Explainable job recommendations using LLMs. (arXiv:2309.11805v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ghosh_P/0/1/0/all/0/1">Preetam Ghosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Sadaphal_V/0/1/0/all/0/1">Vaishali Sadaphal</a></p>
<p>In today's rapidly evolving job market, finding the right opportunity can be
a daunting challenge. With advancements in the field of AI, computers can now
recommend suitable jobs to candidates. However, the task of recommending jobs
is not same as recommending movies to viewers. Apart from must-have criteria,
like skills and experience, there are many subtle aspects to a job which can
decide if it is a good fit or not for a given candidate. Traditional approaches
can capture the quantifiable aspects of jobs and candidates, but a substantial
portion of the data that is present in unstructured form in the job
descriptions and resumes is lost in the process of conversion to structured
format. As of late, Large Language Models (LLMs) have taken over the AI field
by storm with extraordinary performance in fields where text-based data is
available. Inspired by the superior performance of LLMs, we leverage their
capability to understand natural language for capturing the information that
was previously getting lost during the conversion of unstructured data to
structured form. To this end, we compare performance of four different
approaches for job recommendations namely, (i) Content based deterministic,
(ii) LLM guided, (iii) LLM unguided, and (iv) Hybrid. In this study, we present
advantages and limitations of each method and evaluate their performance in
terms of time requirements.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11811">Multimodal Transformers for Wireless Communications: A Case Study in Beam Prediction. (arXiv:2309.11811v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Tian_Y/0/1/0/all/0/1">Yu Tian</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhao_Q/0/1/0/all/0/1">Qiyang Zhao</a>, <a href="http://arxiv.org/find/eess/1/au:+Kherroubi_Z/0/1/0/all/0/1">Zine el abidine Kherroubi</a>, <a href="http://arxiv.org/find/eess/1/au:+Boukhalfa_F/0/1/0/all/0/1">Fouzi Boukhalfa</a>, <a href="http://arxiv.org/find/eess/1/au:+Wu_K/0/1/0/all/0/1">Kebin Wu</a>, <a href="http://arxiv.org/find/eess/1/au:+Bader_F/0/1/0/all/0/1">Faouzi Bader</a></p>
<p>Wireless communications at high-frequency bands with large antenna arrays
face challenges in beam management, which can potentially be improved by
multimodality sensing information from cameras, LiDAR, radar, and GPS. In this
paper, we present a multimodal transformer deep learning framework for
sensing-assisted beam prediction. We employ a convolutional neural network to
extract the features from a sequence of images, point clouds, and radar raw
data sampled over time. At each convolutional layer, we use transformer
encoders to learn the hidden relations between feature tokens from different
modalities and time instances over abstraction space and produce encoded
vectors for the next-level feature extraction. We train the model on a
combination of different modalities with supervised learning. We try to enhance
the model over imbalanced data by utilizing focal loss and exponential moving
average. We also evaluate data processing and augmentation techniques such as
image enhancement, segmentation, background filtering, multimodal data
flipping, radar signal transformation, and GPS angle calibration. Experimental
results show that our solution trained on image and GPS data produces the best
distance-based accuracy of predicted beams at 78.44%, with effective
generalization to unseen day scenarios near 73% and night scenarios over 84%.
This outperforms using other modalities and arbitrary data processing
techniques, which demonstrates the effectiveness of transformers with feature
fusion in performing radio beam prediction from images and GPS. Furthermore,
our solution could be pretrained from large sequences of multimodality wireless
data, on fine-tuning for multiple downstream radio network tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11838">Evaluating Large Language Models for Document-grounded Response Generation in Information-Seeking Dialogues. (arXiv:2309.11838v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Braunschweiler_N/0/1/0/all/0/1">Norbert Braunschweiler</a>, <a href="http://arxiv.org/find/cs/1/au:+Doddipatla_R/0/1/0/all/0/1">Rama Doddipatla</a>, <a href="http://arxiv.org/find/cs/1/au:+Keizer_S/0/1/0/all/0/1">Simon Keizer</a>, <a href="http://arxiv.org/find/cs/1/au:+Stoyanchev_S/0/1/0/all/0/1">Svetlana Stoyanchev</a></p>
<p>In this paper, we investigate the use of large language models (LLMs) like
ChatGPT for document-grounded response generation in the context of
information-seeking dialogues. For evaluation, we use the MultiDoc2Dial corpus
of task-oriented dialogues in four social service domains previously used in
the DialDoc 2022 Shared Task. Information-seeking dialogue turns are grounded
in multiple documents providing relevant information. We generate dialogue
completion responses by prompting a ChatGPT model, using two methods:
Chat-Completion and LlamaIndex. ChatCompletion uses knowledge from ChatGPT
model pretraining while LlamaIndex also extracts relevant information from
documents. Observing that document-grounded response generation via LLMs cannot
be adequately assessed by automatic evaluation metrics as they are
significantly more verbose, we perform a human evaluation where annotators rate
the output of the shared task winning system, the two Chat-GPT variants
outputs, and human responses. While both ChatGPT variants are more likely to
include information not present in the relevant segments, possibly including a
presence of hallucinations, they are rated higher than both the shared task
winning system and human responses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11853">BitCoin: Bidirectional Tagging and Supervised Contrastive Learning based Joint Relational Triple Extraction Framework. (arXiv:2309.11853v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1">Luyao He</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhongbao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1">Sen Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuxin Chen</a></p>
<p>Relation triple extraction (RTE) is an essential task in information
extraction and knowledge graph construction. Despite recent advancements,
existing methods still exhibit certain limitations. They just employ
generalized pre-trained models and do not consider the specificity of RTE
tasks. Moreover, existing tagging-based approaches typically decompose the RTE
task into two subtasks, initially identifying subjects and subsequently
identifying objects and relations. They solely focus on extracting relational
triples from subject to object, neglecting that once the extraction of a
subject fails, it fails in extracting all triples associated with that subject.
To address these issues, we propose BitCoin, an innovative Bidirectional
tagging and supervised Contrastive learning based joint relational triple
extraction framework. Specifically, we design a supervised contrastive learning
method that considers multiple positives per anchor rather than restricting it
to just one positive. Furthermore, a penalty term is introduced to prevent
excessive similarity between the subject and object. Our framework implements
taggers in two directions, enabling triples extraction from subject to object
and object to subject. Experimental results show that BitCoin achieves
state-of-the-art results on the benchmark datasets and significantly improves
the F1 score on Normal, SEO, EPO, and multiple relation extraction tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11858">OSNet &amp; MNetO: Two Types of General Reconstruction Architectures for Linear Computed Tomography in Multi-Scenarios. (arXiv:2309.11858v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhisheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1">Zihan Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1">Fenglin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yixing Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Haijun Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1">Junning Cui</a></p>
<p>Recently, linear computed tomography (LCT) systems have actively attracted
attention. To weaken projection truncation and image the region of interest
(ROI) for LCT, the backprojection filtration (BPF) algorithm is an effective
solution. However, in BPF for LCT, it is difficult to achieve stable interior
reconstruction, and for differentiated backprojection (DBP) images of LCT,
multiple rotation-finite inversion of Hilbert transform (Hilbert
filtering)-inverse rotation operations will blur the image. To satisfy multiple
reconstruction scenarios for LCT, including interior ROI, complete object, and
exterior region beyond field-of-view (FOV), and avoid the rotation operations
of Hilbert filtering, we propose two types of reconstruction architectures. The
first overlays multiple DBP images to obtain a complete DBP image, then uses a
network to learn the overlying Hilbert filtering function, referred to as the
Overlay-Single Network (OSNet). The second uses multiple networks to train
different directional Hilbert filtering models for DBP images of multiple
linear scannings, respectively, and then overlays the reconstructed results,
i.e., Multiple Networks Overlaying (MNetO). In two architectures, we introduce
a Swin Transformer (ST) block to the generator of pix2pixGAN to extract both
local and global features from DBP images at the same time. We investigate two
architectures from different networks, FOV sizes, pixel sizes, number of
projections, geometric magnification, and processing time. Experimental results
show that two architectures can both recover images. OSNet outperforms BPF in
various scenarios. For the different networks, ST-pix2pixGAN is superior to
pix2pixGAN and CycleGAN. MNetO exhibits a few artifacts due to the differences
among the multiple models, but any one of its models is suitable for imaging
the exterior edge in a certain direction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11875">Stochastic stiffness identification and response estimation of Timoshenko beams via physics-informed Gaussian processes. (arXiv:2309.11875v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tondo_G/0/1/0/all/0/1">Gledson Rodrigo Tondo</a>, <a href="http://arxiv.org/find/cs/1/au:+Rau_S/0/1/0/all/0/1">Sebastian Rau</a>, <a href="http://arxiv.org/find/cs/1/au:+Kavrakov_I/0/1/0/all/0/1">Igor Kavrakov</a>, <a href="http://arxiv.org/find/cs/1/au:+Morgenthal_G/0/1/0/all/0/1">Guido Morgenthal</a></p>
<p>Machine learning models trained with structural health monitoring data have
become a powerful tool for system identification. This paper presents a
physics-informed Gaussian process (GP) model for Timoshenko beam elements. The
model is constructed as a multi-output GP with covariance and cross-covariance
kernels analytically derived based on the differential equations for
deflections, rotations, strains, bending moments, shear forces and applied
loads. Stiffness identification is performed in a Bayesian format by maximising
a posterior model through a Markov chain Monte Carlo method, yielding a
stochastic model for the structural parameters. The optimised GP model is
further employed for probabilistic predictions of unobserved responses.
Additionally, an entropy-based method for physics-informed sensor placement
optimisation is presented, exploiting heterogeneous sensor position information
and structural boundary conditions built into the GP model. Results demonstrate
that the proposed approach is effective at identifying structural parameters
and is capable of fusing data from heterogeneous and multi-fidelity sensors.
Probabilistic predictions of structural responses and internal forces are in
closer agreement with measured data. We validate our model with an experimental
setup and discuss the quality and uncertainty of the obtained results. The
proposed approach has potential applications in the field of structural health
monitoring (SHM) for both mechanical and structural systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11876">Multi-level Asymmetric Contrastive Learning for Medical Image Segmentation Pre-training. (arXiv:2309.11876v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1">Shuang Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1">Lei Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xinliang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1">Zifeng Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Qian Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1">Lujia Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiayi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yanye Lu</a></p>
<p>Contrastive learning, which is a powerful technique for learning image-level
representations from unlabeled data, leads a promising direction to dealing
with the dilemma between large-scale pre-training and limited labeled data.
However, most existing contrastive learning strategies are designed mainly for
downstream tasks of natural images, therefore they are sub-optimal and even
worse than learning from scratch when directly applied to medical images whose
downstream tasks are usually segmentation. In this work, we propose a novel
asymmetric contrastive learning framework named JCL for medical image
segmentation with self-supervised pre-training. Specifically, (1) A novel
asymmetric contrastive learning strategy is proposed to pre-train both encoder
and decoder simultaneously in one-stage to provide better initialization for
segmentation models. (2) A multi-level contrastive loss is designed to take the
correspondence among feature-level, image-level and pixel-level projections,
respectively into account to make sure multi-level representations can be
learned by the encoder and decoder during pre-training. (3) Experiments on
multiple medical image datasets indicate our JCL framework outperforms existing
SOTA contrastive learning strategies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11895">Audio Contrastive based Fine-tuning. (arXiv:2309.11895v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_Q/0/1/0/all/0/1">Qibin Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Chenghao Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yizhi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Moubayed_N/0/1/0/all/0/1">Noura Al Moubayed</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1">Chenghua Lin</a></p>
<p>Audio classification plays a crucial role in speech and sound processing
tasks with a wide range of applications. There still remains a challenge of
striking the right balance between fitting the model to the training data
(avoiding overfitting) and enabling it to generalise well to a new domain.
Leveraging the transferability of contrastive learning, we introduce Audio
Contrastive-based Fine-tuning (AudioConFit), an efficient approach
characterised by robust generalisability. Empirical experiments on a variety of
audio classification tasks demonstrate the effectiveness and robustness of our
approach, which achieves state-of-the-art results in various settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11899">Unlocking the Heart Using Adaptive Locked Agnostic Networks. (arXiv:2309.11899v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Majchrowska_S/0/1/0/all/0/1">Sylwia Majchrowska</a>, <a href="http://arxiv.org/find/cs/1/au:+Hildeman_A/0/1/0/all/0/1">Anders Hildeman</a>, <a href="http://arxiv.org/find/cs/1/au:+Teare_P/0/1/0/all/0/1">Philip Teare</a>, <a href="http://arxiv.org/find/cs/1/au:+Diethe_T/0/1/0/all/0/1">Tom Diethe</a></p>
<p>Supervised training of deep learning models for medical imaging applications
requires a significant amount of labeled data. This is posing a challenge as
the images are required to be annotated by medical professionals. To address
this limitation, we introduce the Adaptive Locked Agnostic Network (ALAN), a
concept involving self-supervised visual feature extraction using a large
backbone model to produce anatomically robust semantic self-segmentation. In
the ALAN methodology, this self-supervised training occurs only once on a large
and diverse dataset. Due to the intuitive interpretability of the segmentation,
downstream models tailored for specific tasks can be easily designed using
white-box models with few parameters. This, in turn, opens up the possibility
of communicating the inner workings of a model with domain experts and
introducing prior knowledge into it. It also means that the downstream models
become less data-hungry compared to fully supervised approaches. These
characteristics make ALAN particularly well-suited for resource-scarce
scenarios, such as costly clinical trials and rare diseases. In this paper, we
apply the ALAN approach to three publicly available echocardiography datasets:
EchoNet-Dynamic, CAMUS, and TMED-2. Our findings demonstrate that the
self-supervised backbone model robustly identifies anatomical subregions of the
heart in an apical four-chamber view. Building upon this, we design two
downstream models, one for segmenting a target anatomical region, and a second
for echocardiogram view classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11907">Learning to Recover for Safe Reinforcement Learning. (arXiv:2309.11907v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haoyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1">Xin Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_Q/0/1/0/all/0/1">Qinqing Ren</a></p>
<p>Safety controllers is widely used to achieve safe reinforcement learning.
Most methods that apply a safety controller are using handcrafted safety
constraints to construct the safety controller. However, when the environment
dynamics are sophisticated, handcrafted safety constraints become unavailable.
Therefore, it worth to research on constructing safety controllers by learning
algorithms. We propose a three-stage architecture for safe reinforcement
learning, namely TU-Recovery Architecture. A safety critic and a recovery
policy is learned before task training. They form a safety controller to ensure
safety in task training. Then a phenomenon induced by disagreement between task
policy and recovery policy, called adversarial phenomenon, which reduces
learning efficiency and model performance, is described. Auxiliary reward is
proposed to mitigate adversarial phenomenon, while help the task policy to
learn to recover from high-risk states. A series of experiments are conducted
in a robot navigation environment. Experiments demonstrate that TU-Recovery
outperforms unconstrained counterpart in both reward gaining and constraint
violations during task training, and auxiliary reward further improve
TU-Recovery in reward-to-cost ratio by significantly reduce constraint
violations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11937">On the Definition of Appropriate Trust and the Tools that Come with it. (arXiv:2309.11937v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lofstrom_H/0/1/0/all/0/1">Helena L&#xf6;fstr&#xf6;m</a></p>
<p>Evaluating the efficiency of human-AI interactions is challenging, including
subjective and objective quality aspects. With the focus on the human
experience of the explanations, evaluations of explanation methods have become
mostly subjective, making comparative evaluations almost impossible and highly
linked to the individual user. However, it is commonly agreed that one aspect
of explanation quality is how effectively the user can detect if the
predictions are trustworthy and correct, i.e., if the explanations can increase
the user's appropriate trust in the model. This paper starts with the
definitions of appropriate trust from the literature. It compares the
definitions with model performance evaluation, showing the strong similarities
between appropriate trust and model performance evaluation. The paper's main
contribution is a novel approach to evaluating appropriate trust by taking
advantage of the likenesses between definitions. The paper offers several
straightforward evaluation methods for different aspects of user performance,
including suggesting a method for measuring uncertainty and appropriate trust
in regression.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11960">A Comprehensive Review on Financial Explainable AI. (arXiv:2309.11960v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yeo_W/0/1/0/all/0/1">Wei Jie Yeo</a>, <a href="http://arxiv.org/find/cs/1/au:+Heever_W/0/1/0/all/0/1">Wihan van der Heever</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_R/0/1/0/all/0/1">Rui Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1">Erik Cambria</a>, <a href="http://arxiv.org/find/cs/1/au:+Satapathy_R/0/1/0/all/0/1">Ranjan Satapathy</a>, <a href="http://arxiv.org/find/cs/1/au:+Mengaldo_G/0/1/0/all/0/1">Gianmarco Mengaldo</a></p>
<p>The success of artificial intelligence (AI), and deep learning models in
particular, has led to their widespread adoption across various industries due
to their ability to process huge amounts of data and learn complex patterns.
However, due to their lack of explainability, there are significant concerns
regarding their use in critical sectors, such as finance and healthcare, where
decision-making transparency is of paramount importance. In this paper, we
provide a comparative survey of methods that aim to improve the explainability
of deep learning models within the context of finance. We categorize the
collection of explainable AI methods according to their corresponding
characteristics, and we review the concerns and challenges of adopting
explainable AI methods, together with future directions we deemed appropriate
and important.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11975">Inferring Capabilities from Task Performance with Bayesian Triangulation. (arXiv:2309.11975v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Burden_J/0/1/0/all/0/1">John Burden</a>, <a href="http://arxiv.org/find/cs/1/au:+Voudouris_K/0/1/0/all/0/1">Konstantinos Voudouris</a>, <a href="http://arxiv.org/find/cs/1/au:+Burnell_R/0/1/0/all/0/1">Ryan Burnell</a>, <a href="http://arxiv.org/find/cs/1/au:+Rutar_D/0/1/0/all/0/1">Danaja Rutar</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheke_L/0/1/0/all/0/1">Lucy Cheke</a>, <a href="http://arxiv.org/find/cs/1/au:+Hernandez_Orallo_J/0/1/0/all/0/1">Jos&#xe9; Hern&#xe1;ndez-Orallo</a></p>
<p>As machine learning models become more general, we need to characterise them
in richer, more meaningful ways. We describe a method to infer the cognitive
profile of a system from diverse experimental data. To do so, we introduce
measurement layouts that model how task-instance features interact with system
capabilities to affect performance. These features must be triangulated in
complex ways to be able to infer capabilities from non-populational data -- a
challenge for traditional psychometric and inferential tools. Using the
Bayesian probabilistic programming library PyMC, we infer different cognitive
profiles for agents in two scenarios: 68 actual contestants in the AnimalAI
Olympics and 30 synthetic agents for O-PIAAGETS, an object permanence battery.
We showcase the potential for capability-oriented evaluation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11981">Rethinking the Evaluating Framework for Natural Language Understanding in AI Systems: Language Acquisition as a Core for Future Metrics. (arXiv:2309.11981v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vera_P/0/1/0/all/0/1">Patricio Vera</a>, <a href="http://arxiv.org/find/cs/1/au:+Moya_P/0/1/0/all/0/1">Pedro Moya</a>, <a href="http://arxiv.org/find/cs/1/au:+Barraza_L/0/1/0/all/0/1">Lisa Barraza</a></p>
<p>In the burgeoning field of artificial intelligence (AI), the unprecedented
progress of large language models (LLMs) in natural language processing (NLP)
offers an opportunity to revisit the entire approach of traditional metrics of
machine intelligence, both in form and content. As the realm of machine
cognitive evaluation has already reached Imitation, the next step is an
efficient Language Acquisition and Understanding. Our paper proposes a paradigm
shift from the established Turing Test towards an all-embracing framework that
hinges on language acquisition, taking inspiration from the recent advancements
in LLMs. The present contribution is deeply tributary of the excellent work
from various disciplines, point out the need to keep interdisciplinary bridges
open, and delineates a more robust and sustainable approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11984">Representation Abstractions as Incentives for Reinforcement Learning Agents: A Robotic Grasping Case Study. (arXiv:2309.11984v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Petropoulakis_P/0/1/0/all/0/1">Panagiotis Petropoulakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Graf_L/0/1/0/all/0/1">Ludwig Gr&#xe4;f</a>, <a href="http://arxiv.org/find/cs/1/au:+Josifovski_J/0/1/0/all/0/1">Josip Josifovski</a>, <a href="http://arxiv.org/find/cs/1/au:+Malmir_M/0/1/0/all/0/1">Mohammadhossein Malmir</a>, <a href="http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1">Alois Knoll</a></p>
<p>Choosing an appropriate representation of the environment for the underlying
decision-making process of the \gls{RL} agent is not always straightforward.
The state representation should be inclusive enough to allow the agent to
informatively decide on its actions and compact enough to increase sample
efficiency for policy training. Given this outlook, this work examines the
effect of various state representations in incentivizing the agent to solve a
specific robotic task: antipodal and planar object grasping. A continuum of
state representation abstractions is defined, starting from a model-based
approach with complete system knowledge, through hand-crafted numerical, to
image-based representations with decreasing level of induced task-specific
knowledge. We examine the effects of each representation in the ability of the
agent to solve the task in simulation and the transferability of the learned
policy to the real robot. The results show that RL agents using numerical
states can perform on par with non-learning baselines. Furthermore, we find
that agents using image-based representations from pre-trained environment
embedding vectors perform better than end-to-end trained agents, and
hypothesize that task-specific knowledge is necessary for achieving convergence
and high success rates in robot control. Supplementary material can be found at
the project webpage: https://github.com/PetropoulakisPanagiotis/igae.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11987">Predictability and Comprehensibility in Post-Hoc XAI Methods: A User-Centered Analysis. (arXiv:2309.11987v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jalali_A/0/1/0/all/0/1">Anahid Jalali</a>, <a href="http://arxiv.org/find/cs/1/au:+Haslhofer_B/0/1/0/all/0/1">Bernhard Haslhofer</a>, <a href="http://arxiv.org/find/cs/1/au:+Kriglstein_S/0/1/0/all/0/1">Simone Kriglstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Rauber_A/0/1/0/all/0/1">Andreas Rauber</a></p>
<p>Post-hoc explainability methods aim to clarify predictions of black-box
machine learning models. However, it is still largely unclear how well users
comprehend the provided explanations and whether these increase the users
ability to predict the model behavior. We approach this question by conducting
a user study to evaluate comprehensibility and predictability in two widely
used tools: LIME and SHAP. Moreover, we investigate the effect of
counterfactual explanations and misclassifications on users ability to
understand and predict the model behavior. We find that the comprehensibility
of SHAP is significantly reduced when explanations are provided for samples
near a model's decision boundary. Furthermore, we find that counterfactual
explanations and misclassifications can significantly increase the users
understanding of how a machine learning model is making decisions. Based on our
findings, we also derive design recommendations for future post-hoc
explainability methods with increased comprehensibility and predictability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11998">LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset. (arXiv:2309.11998v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1">Lianmin Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chiang_W/0/1/0/all/0/1">Wei-Lin Chiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sheng_Y/0/1/0/all/0/1">Ying Sheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1">Tianle Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_S/0/1/0/all/0/1">Siyuan Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhanghao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1">Yonghao Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhuohan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zi Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1">Eric. P Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1">Joseph E. Gonzalez</a>, <a href="http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1">Ion Stoica</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hao Zhang</a></p>
<p>Studying how people interact with large language models (LLMs) in real-world
scenarios is increasingly important due to their widespread use in various
applications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset
containing one million real-world conversations with 25 state-of-the-art LLMs.
This dataset is collected from 210K unique IP addresses in the wild on our
Vicuna demo and Chatbot Arena website. We offer an overview of the dataset's
content, including its curation process, basic statistics, and topic
distribution, highlighting its diversity, originality, and scale. We
demonstrate its versatility through four use cases: developing content
moderation models that perform similarly to GPT-4, building a safety benchmark,
training instruction-following models that perform similarly to Vicuna, and
creating challenging benchmark questions. We believe that this dataset will
serve as a valuable resource for understanding and advancing LLM capabilities.
The dataset is publicly available at
\url{https://huggingface.co/datasets/lmsys/lmsys-chat-1m}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12004">Safe Hierarchical Reinforcement Learning for CubeSat Task Scheduling Based on Energy Consumption. (arXiv:2309.12004v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ramezani_M/0/1/0/all/0/1">Mahya Ramezani</a>, <a href="http://arxiv.org/find/cs/1/au:+Alandihallaj_M/0/1/0/all/0/1">M. Amin Alandihallaj</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanchez_Lopez_J/0/1/0/all/0/1">Jose Luis Sanchez-Lopez</a>, <a href="http://arxiv.org/find/cs/1/au:+Hein_A/0/1/0/all/0/1">Andreas Hein</a></p>
<p>This paper presents a Hierarchical Reinforcement Learning methodology
tailored for optimizing CubeSat task scheduling in Low Earth Orbits (LEO).
Incorporating a high-level policy for global task distribution and a low-level
policy for real-time adaptations as a safety mechanism, our approach integrates
the Similarity Attention-based Encoder (SABE) for task prioritization and an
MLP estimator for energy consumption forecasting. Integrating this mechanism
creates a safe and fault-tolerant system for CubeSat task scheduling.
Simulation results validate the Hierarchical Reinforcement Learning superior
convergence and task success rate, outperforming both the MADDPG model and
traditional random scheduling across multiple CubeSat configurations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12022">Demystifying Visual Features of Movie Posters for Multi-Label Genre Identification. (arXiv:2309.12022v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nareti_U/0/1/0/all/0/1">Utsav Kumar Nareti</a>, <a href="http://arxiv.org/find/cs/1/au:+Adak_C/0/1/0/all/0/1">Chandranath Adak</a>, <a href="http://arxiv.org/find/cs/1/au:+Chattopadhyay_S/0/1/0/all/0/1">Soumi Chattopadhyay</a></p>
<p>In the film industry, movie posters have been an essential part of
advertising and marketing for many decades, and continue to play a vital role
even today in the form of digital posters through online, social media and OTT
platforms. Typically, movie posters can effectively promote and communicate the
essence of a film, such as its genre, visual style/ tone, vibe and storyline
cue/ theme, which are essential to attract potential viewers. Identifying the
genres of a movie often has significant practical applications in recommending
the film to target audiences. Previous studies on movie genre identification
are limited to subtitles, plot synopses, and movie scenes that are mostly
accessible after the movie release. Posters usually contain pre-release
implicit information to generate mass interest. In this paper, we work for
automated multi-label genre identification only from movie poster images,
without any aid of additional textual/meta-data information about movies, which
is one of the earliest attempts of its kind. Here, we present a deep
transformer network with a probabilistic module to identify the movie genres
exclusively from the poster. For experimental analysis, we procured 13882
number of posters of 13 genres from the Internet Movie Database (IMDb), where
our model performances were encouraging and even outperformed some major
contemporary architectures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12028">Dynamic Hypergraph Structure Learning for Traffic Flow Forecasting. (arXiv:2309.12028v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yusheng Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1">Xiao Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Ju_W/0/1/0/all/0/1">Wei Ju</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1">Xian-Sheng Hua</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Ming Zhang</a></p>
<p>This paper studies the problem of traffic flow forecasting, which aims to
predict future traffic conditions on the basis of road networks and traffic
conditions in the past. The problem is typically solved by modeling complex
spatio-temporal correlations in traffic data using spatio-temporal graph neural
networks (GNNs). However, the performance of these methods is still far from
satisfactory since GNNs usually have limited representation capacity when it
comes to complex traffic networks. Graphs, by nature, fall short in capturing
non-pairwise relations. Even worse, existing methods follow the paradigm of
message passing that aggregates neighborhood information linearly, which fails
to capture complicated spatio-temporal high-order interactions. To tackle these
issues, in this paper, we propose a novel model named Dynamic Hypergraph
Structure Learning (DyHSL) for traffic flow prediction. To learn non-pairwise
relationships, our DyHSL extracts hypergraph structural information to model
dynamics in the traffic networks, and updates each node representation by
aggregating messages from its associated hyperedges. Additionally, to capture
high-order spatio-temporal relations in the road network, we introduce an
interactive graph convolution block, which further models the neighborhood
interaction for each node. Finally, we integrate these two views into a
holistic multi-scale correlation extraction module, which conducts temporal
pooling with different scales to model different temporal patterns. Extensive
experiments on four popular traffic benchmark datasets demonstrate the
effectiveness of our proposed DyHSL compared with a broad range of competing
baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12038">Uncertainty-driven Exploration Strategies for Online Grasp Learning. (arXiv:2309.12038v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yitian Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Schillinger_P/0/1/0/all/0/1">Philipp Schillinger</a>, <a href="http://arxiv.org/find/cs/1/au:+Gabriel_M/0/1/0/all/0/1">Miroslav Gabriel</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuss_A/0/1/0/all/0/1">Alexander Kuss</a>, <a href="http://arxiv.org/find/cs/1/au:+Feldman_Z/0/1/0/all/0/1">Zohar Feldman</a>, <a href="http://arxiv.org/find/cs/1/au:+Ziesche_H/0/1/0/all/0/1">Hanna Ziesche</a>, <a href="http://arxiv.org/find/cs/1/au:+Vien_N/0/1/0/all/0/1">Ngo Anh Vien</a></p>
<p>Existing grasp prediction approaches are mostly based on offline learning,
while, ignored the exploratory grasp learning during online adaptation to new
picking scenarios, i.e., unseen object portfolio, camera and bin settings etc.
In this paper, we present a novel method for online learning of grasp
predictions for robotic bin picking in a principled way. Existing grasp
prediction approaches are mostly based on offline learning, while, ignored the
exploratory grasp learning during online adaptation to new picking scenarios,
i.e., unseen object portfolio, camera and bin settings etc. In this paper, we
present a novel method for online learning of grasp predictions for robotic bin
picking in a principled way. Specifically, the online learning algorithm with
an effective exploration strategy can significantly improve its adaptation
performance to unseen environment settings. To this end, we first propose to
formulate online grasp learning as a RL problem that will allow to adapt both
grasp reward prediction and grasp poses. We propose various uncertainty
estimation schemes based on Bayesian Uncertainty Quantification and
Distributional Ensembles. We carry out evaluations on real-world bin picking
scenes of varying difficulty. The objects in the bin have various challenging
physical and perceptual characteristics that can be characterized by semi- or
total transparency, and irregular or curved surfaces. The results of our
experiments demonstrate a notable improvement in the suggested approach
compared to conventional online learning methods which incorporate only naive
exploration strategies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12056">BELT:Bootstrapping Electroencephalography-to-Language Decoding and Zero-Shot Sentiment Classification by Natural Language Supervision. (arXiv:2309.12056v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jinzhao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1">Yiqun Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1">Yu-Cheng Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu-Kai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1">Chin-Teng Lin</a></p>
<p>This paper presents BELT, a novel model and learning framework for the
pivotal topic of brain-to-language translation research. The translation from
noninvasive brain signals into readable natural language has the potential to
promote the application scenario as well as the development of brain-computer
interfaces (BCI) as a whole. The critical problem in brain signal decoding or
brain-to-language translation is the acquisition of semantically appropriate
and discriminative EEG representation from a dataset of limited scale and
quality. The proposed BELT method is a generic and efficient framework that
bootstraps EEG representation learning using off-the-shelf large-scale
pretrained language models (LMs). With a large LM's capacity for understanding
semantic information and zero-shot generalization, BELT utilizes large LMs
trained on Internet-scale datasets to bring significant improvements to the
understanding of EEG signals.
</p>
<p>In particular, the BELT model is composed of a deep conformer encoder and a
vector quantization encoder. Semantical EEG representation is achieved by a
contrastive learning step that provides natural language supervision. We
achieve state-of-the-art results on two featuring brain decoding tasks
including the brain-to-language translation and zero-shot sentiment
classification. Specifically, our model surpasses the baseline model on both
tasks by 5.45% and over 10% and archives a 42.31% BLEU-1 score and 67.32%
precision on the main evaluation metrics for translation and zero-shot
sentiment classification respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12058">An Efficient Consolidation of Word Embedding and Deep Learning Techniques for Classifying Anticancer Peptides: FastText+BiLSTM. (arXiv:2309.12058v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Karakaya_O/0/1/0/all/0/1">Onur Karakaya</a>, <a href="http://arxiv.org/find/cs/1/au:+Kilimci_Z/0/1/0/all/0/1">Zeynep Hilal Kilimci</a></p>
<p>Anticancer peptides (ACPs) are a group of peptides that exhibite
antineoplastic properties. The utilization of ACPs in cancer prevention can
present a viable substitute for conventional cancer therapeutics, as they
possess a higher degree of selectivity and safety. Recent scientific
advancements generate an interest in peptide-based therapies which offer the
advantage of efficiently treating intended cells without negatively impacting
normal cells. However, as the number of peptide sequences continues to increase
rapidly, developing a reliable and precise prediction model becomes a
challenging task. In this work, our motivation is to advance an efficient model
for categorizing anticancer peptides employing the consolidation of word
embedding and deep learning models. First, Word2Vec and FastText are evaluated
as word embedding techniques for the purpose of extracting peptide sequences.
Then, the output of word embedding models are fed into deep learning approaches
CNN, LSTM, BiLSTM. To demonstrate the contribution of proposed framework,
extensive experiments are carried on widely-used datasets in the literature,
ACPs250 and Independent. Experiment results show the usage of proposed model
enhances classification accuracy when compared to the state-of-the-art studies.
The proposed combination, FastText+BiLSTM, exhibits 92.50% of accuracy for
ACPs250 dataset, and 96.15% of accuracy for Independent dataset, thence
determining new state-of-the-art.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12067">Survey of Action Recognition, Spotting and Spatio-Temporal Localization in Soccer -- Current Trends and Research Perspectives. (arXiv:2309.12067v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Seweryn_K/0/1/0/all/0/1">Karolina Seweryn</a>, <a href="http://arxiv.org/find/cs/1/au:+Wroblewska_A/0/1/0/all/0/1">Anna Wr&#xf3;blewska</a>, <a href="http://arxiv.org/find/cs/1/au:+Lukasik_S/0/1/0/all/0/1">Szymon &#x141;ukasik</a></p>
<p>Action scene understanding in soccer is a challenging task due to the complex
and dynamic nature of the game, as well as the interactions between players.
This article provides a comprehensive overview of this task divided into action
recognition, spotting, and spatio-temporal action localization, with a
particular emphasis on the modalities used and multimodal methods. We explore
the publicly available data sources and metrics used to evaluate models'
performance. The article reviews recent state-of-the-art methods that leverage
deep learning techniques and traditional methods. We focus on multimodal
methods, which integrate information from multiple sources, such as video and
audio data, and also those that represent one source in various ways. The
advantages and limitations of methods are discussed, along with their potential
for improving the accuracy and robustness of models. Finally, the article
highlights some of the open research questions and future directions in the
field of soccer action recognition, including the potential for multimodal
methods to advance this field. Overall, this survey provides a valuable
resource for researchers interested in the field of action scene understanding
in soccer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12071">Benchmarking quantized LLaMa-based models on the Brazilian Secondary School Exam. (arXiv:2309.12071v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Santos_M/0/1/0/all/0/1">Matheus L. O. Santos</a>, <a href="http://arxiv.org/find/cs/1/au:+Campelo_C/0/1/0/all/0/1">Cl&#xe1;udio E. C. Campelo</a></p>
<p>Although Large Language Models (LLMs) represent a revolution in the way we
interact with computers, allowing the construction of complex questions and the
ability to reason over a sequence of statements, their use is restricted due to
the need for dedicated hardware for execution. In this study, we evaluate the
performance of LLMs based on the 7 and 13 billion LLaMA models, subjected to a
quantization process and run on home hardware. The models considered were
Alpaca, Koala, and Vicuna. To evaluate the effectiveness of these models, we
developed a database containing 1,006 questions from the ENEM (Brazilian
National Secondary School Exam). Our analysis revealed that the best performing
models achieved an accuracy of approximately 46% for the original texts of the
Portuguese questions and 49% on their English translations. In addition, we
evaluated the computational efficiency of the models by measuring the time
required for execution. On average, the 7 and 13 billion LLMs took
approximately 20 and 50 seconds, respectively, to process the queries on a
machine equipped with an AMD Ryzen 5 3600x processor
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12075">Accelerating Thematic Investment with Prompt Tuned Pretrained Language Models. (arXiv:2309.12075v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Buchner_V/0/1/0/all/0/1">Valentin Leonhard Buchner</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1">Lele Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalo_J/0/1/0/all/0/1">Jan-Christoph Kalo</a></p>
<p>Prompt Tuning is emerging as a scalable and cost-effective method to
fine-tune Pretrained Language Models (PLMs). This study benchmarks the
performance and computational efficiency of Prompt Tuning and baseline methods
on a multi-label text classification task. This is applied to the use case of
classifying companies into an investment firm's proprietary industry taxonomy,
supporting their thematic investment strategy. Text-to-text classification with
PLMs is frequently reported to outperform classification with a classification
head, but has several limitations when applied to a multi-label classification
problem where each label consists of multiple tokens: (a) Generated labels may
not match any label in the industry taxonomy; (b) During fine-tuning, multiple
labels must be provided in an arbitrary order; (c) The model provides a binary
decision for each label, rather than an appropriate confidence score.
Limitation (a) is addressed by applying constrained decoding using Trie Search,
which slightly improves classification performance. All limitations (a), (b),
and (c) are addressed by replacing the PLM's language head with a
classification head. This improves performance significantly, while also
reducing computational costs during inference. The results indicate the
continuing need to adapt state-of-the-art methods to domain-specific tasks,
even in the era of PLMs with strong generalization abilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12109">PEFTT: Parameter-Efficient Fine-Tuning for low-resource Tibetan pre-trained language models. (arXiv:2309.12109v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mingjun_Z/0/1/0/all/0/1">Zhou Mingjun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuoma_D/0/1/0/all/0/1">Daiqing Zhuoma</a>, <a href="http://arxiv.org/find/cs/1/au:+Nuo_Q/0/1/0/all/0/1">Qun Nuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Tashi_N/0/1/0/all/0/1">Nyima Tashi</a></p>
<p>In this era of large language models (LLMs), the traditional training of
models has become increasingly unimaginable for regular users and institutions.
The exploration of efficient fine-tuning for high-resource languages on these
models is an undeniable trend that is gradually gaining popularity. However,
there has been very little exploration for various low-resource languages, such
as Tibetan. Research in Tibetan NLP is inherently scarce and limited. While
there is currently no existing large language model for Tibetan due to its
low-resource nature, that day will undoubtedly arrive. Therefore, research on
efficient fine-tuning for low-resource language models like Tibetan is highly
necessary. Our research can serve as a reference to fill this crucial gap.
Efficient fine-tuning strategies for pre-trained language models (PLMs) in
Tibetan have seen minimal exploration. We conducted three types of efficient
fine-tuning experiments on the publicly available TNCC-title dataset:
"prompt-tuning," "Adapter lightweight fine-tuning," and "prompt-tuning +
Adapter fine-tuning." The experimental results demonstrate significant
improvements using these methods, providing valuable insights for advancing
Tibetan language applications in the context of pre-trained models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12113">Incentivizing Massive Unknown Workers for Budget-Limited Crowdsensing: From Off-Line and On-Line Perspectives. (arXiv:2309.12113v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1">Feng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1">Yuqi Chai</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Huan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1">Pengfei Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1">Lingjie Duan</a></p>
<p>Although the uncertainties of the workers can be addressed by the standard
Combinatorial Multi-Armed Bandit (CMAB) framework in existing proposals through
a trade-off between exploration and exploitation, we may not have sufficient
budget to enable the trade-off among the individual workers, especially when
the number of the workers is huge while the budget is limited. Moreover, the
standard CMAB usually assumes the workers always stay in the system, whereas
the workers may join in or depart from the system over time, such that what we
have learnt for an individual worker cannot be applied after the worker leaves.
To address the above challenging issues, in this paper, we first propose an
off-line Context-Aware CMAB-based Incentive (CACI) mechanism. We innovate in
leveraging the exploration-exploitation trade-off in a elaborately partitioned
context space instead of the individual workers, to effectively incentivize the
massive unknown workers with very limited budget. We also extend the above
basic idea to the on-line setting where unknown workers may join in or depart
from the systems dynamically, and propose an on-line version of the CACI
mechanism. Specifically, by the exploitation-exploration trade-off in the
context space, we learn to estimate the sensing ability of any unknown worker
(even it never appeared in the system before) according to its context
information. We perform rigorous theoretical analysis to reveal the upper
bounds on the regrets of our CACI mechanisms and to prove their truthfulness
and individual rationality, respectively. Extensive experiments on both
synthetic and real datasets are also conducted to verify the efficacy of our
mechanisms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12132">A knowledge representation approach for construction contract knowledge modeling. (arXiv:2309.12132v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1">Chunmo Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_S/0/1/0/all/0/1">Saika Wong</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1">Xing Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1">Yinqiu Tang</a></p>
<p>The emergence of large language models (LLMs) presents an unprecedented
opportunity to automate construction contract management, reducing human errors
and saving significant time and costs. However, LLMs may produce convincing yet
inaccurate and misleading content due to a lack of domain expertise. To address
this issue, expert-driven contract knowledge can be represented in a structured
manner to constrain the automatic contract management process. This paper
introduces the Nested Contract Knowledge Graph (NCKG), a knowledge
representation approach that captures the complexity of contract knowledge
using a nested structure. It includes a nested knowledge representation
framework, a NCKG ontology built on the framework, and an implementation
method. Furthermore, we present the LLM-assisted contract review pipeline
enhanced with external knowledge in NCKG. Our pipeline achieves a promising
performance in contract risk reviewing, shedding light on the combination of
LLM and KG towards more reliable and interpretable contract management.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12137">OSN-MDAD: Machine Translation Dataset for Arabic Multi-Dialectal Conversations on Online Social Media. (arXiv:2309.12137v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alzamzami_F/0/1/0/all/0/1">Fatimah Alzamzami</a>, <a href="http://arxiv.org/find/cs/1/au:+Saddik_A/0/1/0/all/0/1">Abdulmotaleb El Saddik</a></p>
<p>While resources for English language are fairly sufficient to understand
content on social media, similar resources in Arabic are still immature. The
main reason that the resources in Arabic are insufficient is that Arabic has
many dialects in addition to the standard version (MSA). Arabs do not use MSA
in their daily communications; rather, they use dialectal versions.
Unfortunately, social users transfer this phenomenon into their use of social
media platforms, which in turn has raised an urgent need for building suitable
AI models for language-dependent applications. Existing machine translation
(MT) systems designed for MSA fail to work well with Arabic dialects. In light
of this, it is necessary to adapt to the informal nature of communication on
social networks by developing MT systems that can effectively handle the
various dialects of Arabic. Unlike for MSA that shows advanced progress in MT
systems, little effort has been exerted to utilize Arabic dialects for MT
systems. While few attempts have been made to build translation datasets for
dialectal Arabic, they are domain dependent and are not OSN cultural-language
friendly. In this work, we attempt to alleviate these limitations by proposing
an online social network-based multidialect Arabic dataset that is crafted by
contextually translating English tweets into four Arabic dialects: Gulf,
Yemeni, Iraqi, and Levantine. To perform the translation, we followed our
proposed guideline framework for content translation, which could be
universally applicable for translation between foreign languages and local
dialects. We validated the authenticity of our proposed dataset by developing
neural MT models for four Arabic dialects. Our results have shown a superior
performance of our NMT models trained using our dataset. We believe that our
dataset can reliably serve as an Arabic multidialectal translation dataset for
informal MT tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12139">On the relationship between Benchmarking, Standards and Certification in Robotics and AI. (arXiv:2309.12139v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Winfield_A/0/1/0/all/0/1">Alan F.T. Winfield</a>, <a href="http://arxiv.org/find/cs/1/au:+Studley_M/0/1/0/all/0/1">Matthew Studley</a></p>
<p>Benchmarking, standards and certification are closely related processes.
Standards can provide normative requirements that robotics and AI systems may
or may not conform to. Certification generally relies upon conformance with one
or more standards as the key determinant of granting a certificate to operate.
And benchmarks are sets of standardised tests against which robots and AI
systems can be measured. Benchmarks therefore can be thought of as informal
standards. In this paper we will develop these themes with examples from
benchmarking, standards and certification, and argue that these three linked
processes are not only useful but vital to the broader practice of Responsible
Innovation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12140">Unsupervised Domain Adaptation for Self-Driving from Past Traversal Features. (arXiv:2309.12140v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Travis Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_K/0/1/0/all/0/1">Katie Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Phoo_C/0/1/0/all/0/1">Cheng Perng Phoo</a>, <a href="http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1">Yurong You</a>, <a href="http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1">Wei-Lun Chao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hariharan_B/0/1/0/all/0/1">Bharath Hariharan</a>, <a href="http://arxiv.org/find/cs/1/au:+Campbell_M/0/1/0/all/0/1">Mark Campbell</a>, <a href="http://arxiv.org/find/cs/1/au:+Weinberger_K/0/1/0/all/0/1">Kilian Q. Weinberger</a></p>
<p>The rapid development of 3D object detection systems for self-driving cars
has significantly improved accuracy. However, these systems struggle to
generalize across diverse driving environments, which can lead to
safety-critical failures in detecting traffic participants. To address this, we
propose a method that utilizes unlabeled repeated traversals of multiple
locations to adapt object detectors to new driving environments. By
incorporating statistics computed from repeated LiDAR scans, we guide the
adaptation process effectively. Our approach enhances LiDAR-based detection
models using spatial quantized historical features and introduces a lightweight
regression head to leverage the statistics for feature regularization.
Additionally, we leverage the statistics for a novel self-training process to
stabilize the training. The framework is detector model-agnostic and
experiments on real-world datasets demonstrate significant improvements,
achieving up to a 20-point performance gain, especially in detecting
pedestrians and distant objects. Code is available at
https://github.com/zhangtravis/Hist-DA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12177">Explainable Artificial Intelligence for Drug Discovery and Development -- A Comprehensive Survey. (arXiv:2309.12177v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alizadehsani_R/0/1/0/all/0/1">Roohallah Alizadehsani</a>, <a href="http://arxiv.org/find/cs/1/au:+Hussain_S/0/1/0/all/0/1">Sadiq Hussain</a>, <a href="http://arxiv.org/find/cs/1/au:+Calixto_R/0/1/0/all/0/1">Rene Ripardo Calixto</a>, <a href="http://arxiv.org/find/cs/1/au:+Albuquerque_V/0/1/0/all/0/1">Victor Hugo C. de Albuquerque</a>, <a href="http://arxiv.org/find/cs/1/au:+Roshanzamir_M/0/1/0/all/0/1">Mohamad Roshanzamir</a>, <a href="http://arxiv.org/find/cs/1/au:+Rahouti_M/0/1/0/all/0/1">Mohamed Rahouti</a>, <a href="http://arxiv.org/find/cs/1/au:+Jagatheesaperumal_S/0/1/0/all/0/1">Senthil Kumar Jagatheesaperumal</a></p>
<p>The field of drug discovery has experienced a remarkable transformation with
the advent of artificial intelligence (AI) and machine learning (ML)
technologies. However, as these AI and ML models are becoming more complex,
there is a growing need for transparency and interpretability of the models.
Explainable Artificial Intelligence (XAI) is a novel approach that addresses
this issue and provides a more interpretable understanding of the predictions
made by machine learning models. In recent years, there has been an increasing
interest in the application of XAI techniques to drug discovery. This review
article provides a comprehensive overview of the current state-of-the-art in
XAI for drug discovery, including various XAI methods, their application in
drug discovery, and the challenges and limitations of XAI techniques in drug
discovery. The article also covers the application of XAI in drug discovery,
including target identification, compound design, and toxicity prediction.
Furthermore, the article suggests potential future research directions for the
application of XAI in drug discovery. The aim of this review article is to
provide a comprehensive understanding of the current state of XAI in drug
discovery and its potential to transform the field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12201">Electroencephalogram Sensor Data Compression Using An Asymmetrical Sparse Autoencoder With A Discrete Cosine Transform Layer. (arXiv:2309.12201v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Zhu_X/0/1/0/all/0/1">Xin Zhu</a>, <a href="http://arxiv.org/find/eess/1/au:+Pan_H/0/1/0/all/0/1">Hongyi Pan</a>, <a href="http://arxiv.org/find/eess/1/au:+Rong_S/0/1/0/all/0/1">Shuaiang Rong</a>, <a href="http://arxiv.org/find/eess/1/au:+Cetin_A/0/1/0/all/0/1">Ahmet Enis Cetin</a></p>
<p>Electroencephalogram (EEG) data compression is necessary for wireless
recording applications to reduce the amount of data that needs to be
transmitted. In this paper, an asymmetrical sparse autoencoder with a discrete
cosine transform (DCT) layer is proposed to compress EEG signals. The encoder
module of the autoencoder has a combination of a fully connected linear layer
and the DCT layer to reduce redundant data using hard-thresholding
nonlinearity. Furthermore, the DCT layer includes trainable hard-thresholding
parameters and scaling layers to give emphasis or de-emphasis on individual DCT
coefficients. Finally, the one-by-one convolutional layer generates the latent
space. The sparsity penalty-based cost function is employed to keep the feature
map as sparse as possible in the latent space. The latent space data is
transmitted to the receiver. The decoder module of the autoencoder is designed
using the inverse DCT and two fully connected linear layers to improve the
accuracy of data reconstruction. In comparison to other state-of-the-art
methods, the proposed method significantly improves the average quality score
in various data compression experiments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12244">ChaCha: Leveraging Large Language Models to Prompt Children to Share Their Emotions about Personal Events. (arXiv:2309.12244v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Seo_W/0/1/0/all/0/1">Woosuk Seo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Chanmo Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Young-Ho Kim</a></p>
<p>Children typically learn to identify and express emotions through sharing
their stories and feelings with others, particularly their family. However, it
is challenging for parents or siblings to have emotional communication with
children since children are still developing their communication skills. We
present ChaCha, a chatbot that encourages and guides children to share personal
events and associated emotions. ChaCha combines a state machine and large
language models (LLMs) to keep the dialogue on track while carrying on
free-form conversations. Through an exploratory study with 20 children (aged
8-12), we examine how ChaCha prompts children to share personal events and
guides them to describe associated emotions. Participants perceived ChaCha as a
close friend and shared their stories on various topics, such as family trips
and personal achievements. Based on the quantitative and qualitative findings,
we discuss opportunities for leveraging LLMs to design child-friendly chatbots
to support children in sharing their emotions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12247">Bad Actor, Good Advisor: Exploring the Role of Large Language Models in Fake News Detection. (arXiv:2309.12247v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1">Beizhe Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sheng_Q/0/1/0/all/0/1">Qiang Sheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1">Juan Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yuhui Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Danding Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_P/0/1/0/all/0/1">Peng Qi</a></p>
<p>Detecting fake news requires both a delicate sense of diverse clues and a
profound understanding of the real-world background, which remains challenging
for detectors based on small language models (SLMs) due to their knowledge and
capability limitations. Recent advances in large language models (LLMs) have
shown remarkable performance in various tasks, but whether and how LLMs could
help with fake news detection remains underexplored. In this paper, we
investigate the potential of LLMs in fake news detection. First, we conduct an
empirical study and find that a sophisticated LLM such as GPT 3.5 could
generally expose fake news and provide desirable multi-perspective rationales
but still underperforms the basic SLM, fine-tuned BERT. Our subsequent analysis
attributes such a gap to the LLM's inability to select and integrate rationales
properly to conclude. Based on these findings, we propose that current LLMs may
not substitute fine-tuned SLMs in fake news detection but can be a good advisor
for SLMs by providing multi-perspective instructive rationales. To instantiate
this proposal, we design an adaptive rationale guidance network for fake news
detection (ARG), in which SLMs selectively acquire insights on news analysis
from the LLMs' rationales. We further derive a rationale-free version of ARG by
distillation, namely ARG-D, which services cost-sensitive scenarios without
inquiring LLMs. Experiments on two real-world datasets demonstrate that ARG and
ARG-D outperform three types of baseline methods, including SLM-based,
LLM-based, and combinations of small and large language models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12253">SALSA-CLRS: A Sparse and Scalable Benchmark for Algorithmic Reasoning. (arXiv:2309.12253v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Minder_J/0/1/0/all/0/1">Julian Minder</a>, <a href="http://arxiv.org/find/cs/1/au:+Grotschla_F/0/1/0/all/0/1">Florian Gr&#xf6;tschla</a>, <a href="http://arxiv.org/find/cs/1/au:+Mathys_J/0/1/0/all/0/1">Jo&#xeb;l Mathys</a>, <a href="http://arxiv.org/find/cs/1/au:+Wattenhofer_R/0/1/0/all/0/1">Roger Wattenhofer</a></p>
<p>We introduce an extension to the CLRS algorithmic learning benchmark,
prioritizing scalability and the utilization of sparse representations. Many
algorithms in CLRS require global memory or information exchange, mirrored in
its execution model, which constructs fully connected (not sparse) graphs based
on the underlying problem. Despite CLRS's aim of assessing how effectively
learned algorithms can generalize to larger instances, the existing execution
model becomes a significant constraint due to its demanding memory requirements
and runtime (hard to scale). However, many important algorithms do not demand a
fully connected graph; these algorithms, primarily distributed in nature, align
closely with the message-passing paradigm employed by Graph Neural Networks.
Hence, we propose SALSA-CLRS, an extension of the current CLRS benchmark
specifically with scalability and sparseness in mind. Our approach includes
adapted algorithms from the original CLRS benchmark and introduces new problems
from distributed and randomized algorithms. Moreover, we perform a thorough
empirical evaluation of our benchmark. Code is publicly available at
https://github.com/jkminder/SALSA-CLRS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12267">Enabling Quartile-based Estimated-Mean Gradient Aggregation As Baseline for Federated Image Classifications. (arXiv:2309.12267v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yusen Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1">Jamie Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1">Phuong Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yesha_Y/0/1/0/all/0/1">Yelena Yesha</a></p>
<p>Federated Learning (FL) has revolutionized how we train deep neural networks
by enabling decentralized collaboration while safeguarding sensitive data and
improving model performance. However, FL faces two crucial challenges: the
diverse nature of data held by individual clients and the vulnerability of the
FL system to security breaches. This paper introduces an innovative solution
named Estimated Mean Aggregation (EMA) that not only addresses these challenges
but also provides a fundamental reference point as a $\mathsf{baseline}$ for
advanced aggregation techniques in FL systems. EMA's significance lies in its
dual role: enhancing model security by effectively handling malicious outliers
through trimmed means and uncovering data heterogeneity to ensure that trained
models are adaptable across various client datasets. Through a wealth of
experiments, EMA consistently demonstrates high accuracy and area under the
curve (AUC) compared to alternative methods, establishing itself as a robust
baseline for evaluating the effectiveness and security of FL aggregation
methods. EMA's contributions thus offer a crucial step forward in advancing the
efficiency, security, and versatility of decentralized deep learning in the
context of FL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12276">LLMR: Real-time Prompting of Interactive Worlds using Large Language Models. (arXiv:2309.12276v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Torre_F/0/1/0/all/0/1">Fernanda De La Torre</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1">Cathy Mengying Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Han Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Banburski_Fahey_A/0/1/0/all/0/1">Andrzej Banburski-Fahey</a>, <a href="http://arxiv.org/find/cs/1/au:+Fernandez_J/0/1/0/all/0/1">Judith Amores Fernandez</a>, <a href="http://arxiv.org/find/cs/1/au:+Lanier_J/0/1/0/all/0/1">Jaron Lanier</a></p>
<p>We present Large Language Model for Mixed Reality (LLMR), a framework for the
real-time creation and modification of interactive Mixed Reality experiences
using LLMs. LLMR leverages novel strategies to tackle difficult cases where
ideal training data is scarce, or where the design goal requires the synthesis
of internal dynamics, intuitive analysis, or advanced interactivity. Our
framework relies on text interaction and the Unity game engine. By
incorporating techniques for scene understanding, task planning,
self-debugging, and memory management, LLMR outperforms the standard GPT-4 by
4x in average error rate. We demonstrate LLMR's cross-platform interoperability
with several example worlds, and evaluate it on a variety of creation and
modification tasks to show that it can produce and edit diverse objects, tools,
and scenes. Finally, we conducted a usability study (N=11) with a diverse set
that revealed participants had positive experiences with the system and would
use it again.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12284">MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models. (arXiv:2309.12284v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1">Longhui Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1">Weisen Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1">Han Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Jincheng Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhengying Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwok_J/0/1/0/all/0/1">James T. Kwok</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhenguo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1">Adrian Weller</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Weiyang Liu</a></p>
<p>Large language models (LLMs) have pushed the limits of natural language
understanding and exhibited excellent problem-solving ability. Despite the
great success, most existing open-source LLMs (\eg, LLaMA-2) are still far away
from satisfactory for solving mathematical problem due to the complex reasoning
procedures. To bridge this gap, we propose \emph{MetaMath}, a fine-tuned
language model that specializes in mathematical reasoning. Specifically, we
start by bootstrapping mathematical questions by rewriting the question from
multiple perspectives without extra knowledge, which results in a new dataset
called {MetaMathQA}. Then we fine-tune the LLaMA-2 models on MetaMathQA.
Experimental results on two popular benchmarks (\ie, GSM8K and MATH) for
mathematical reasoning demonstrate that MetaMath outperforms a suite of
open-source LLMs by a significant margin. Our MetaMath-7B model achieves
$66.4\%$ on GSM8K and $19.4\%$ on MATH, exceeding the state-of-the-art models
of the same size by $11.5\%$ and $8.7\%$. Particularly, {MetaMath-70B} achieves
an accuracy of $82.3\%$ on {GSM8K}, slightly better than {GPT-3.5-Turbo}. We
release the {MetaMathQA} dataset, the {MetaMath} models with different model
sizes and the training code for public use.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12288">The Reversal Curse: LLMs trained on &quot;A is B&quot; fail to learn &quot;B is A&quot;. (arXiv:2309.12288v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Berglund_L/0/1/0/all/0/1">Lukas Berglund</a>, <a href="http://arxiv.org/find/cs/1/au:+Tong_M/0/1/0/all/0/1">Meg Tong</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaufmann_M/0/1/0/all/0/1">Max Kaufmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Balesni_M/0/1/0/all/0/1">Mikita Balesni</a>, <a href="http://arxiv.org/find/cs/1/au:+Stickland_A/0/1/0/all/0/1">Asa Cooper Stickland</a>, <a href="http://arxiv.org/find/cs/1/au:+Korbak_T/0/1/0/all/0/1">Tomasz Korbak</a>, <a href="http://arxiv.org/find/cs/1/au:+Evans_O/0/1/0/all/0/1">Owain Evans</a></p>
<p>We expose a surprising failure of generalization in auto-regressive large
language models (LLMs). If a model is trained on a sentence of the form "A is
B", it will not automatically generalize to the reverse direction "B is A".
This is the Reversal Curse. For instance, if a model is trained on "Olaf Scholz
was the ninth Chancellor of Germany", it will not automatically be able to
answer the question, "Who was the ninth Chancellor of Germany?". Moreover, the
likelihood of the correct answer ("Olaf Scholz") will not be higher than for a
random name. Thus, models exhibit a basic failure of logical deduction and do
not generalize a prevalent pattern in their training set (i.e. if "A is B''
occurs, "B is A" is more likely to occur). We provide evidence for the Reversal
Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as "Uriah
Hawthorne is the composer of 'Abyssal Melodies'" and showing that they fail to
correctly answer "Who composed 'Abyssal Melodies?'". The Reversal Curse is
robust across model sizes and model families and is not alleviated by data
augmentation. We also evaluate ChatGPT (GPT-3.5 and GPT-4) on questions about
real-world celebrities, such as "Who is Tom Cruise's mother? [A: Mary Lee
Pfeiffer]" and the reverse "Who is Mary Lee Pfeiffer's son?". GPT-4 correctly
answers questions like the former 79% of the time, compared to 33% for the
latter. This shows a failure of logical deduction that we hypothesize is caused
by the Reversal Curse. Code is available at
https://github.com/lukasberglund/reversal_curse.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12295">Learning to Drive Anywhere. (arXiv:2309.12295v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1">Ruizhao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1">Peng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ohn_Bar_E/0/1/0/all/0/1">Eshed Ohn-Bar</a>, <a href="http://arxiv.org/find/cs/1/au:+Saligrama_V/0/1/0/all/0/1">Venkatesh Saligrama</a></p>
<p>Human drivers can seamlessly adapt their driving decisions across
geographical locations with diverse conditions and rules of the road, e.g.,
left vs. right-hand traffic. In contrast, existing models for autonomous
driving have been thus far only deployed within restricted operational domains,
i.e., without accounting for varying driving behaviors across locations or
model scalability. In this work, we propose AnyD, a single geographically-aware
conditional imitation learning (CIL) model that can efficiently learn from
heterogeneous and globally distributed data with dynamic environmental,
traffic, and social characteristics. Our key insight is to introduce a
high-capacity geo-location-based channel attention mechanism that effectively
adapts to local nuances while also flexibly modeling similarities among regions
in a data-driven manner. By optimizing a contrastive imitation objective, our
proposed approach can efficiently scale across inherently imbalanced data
distributions and location-dependent events. We demonstrate the benefits of our
AnyD agent across multiple datasets, cities, and scalable deployment paradigms,
i.e., centralized, semi-supervised, and distributed agent training.
Specifically, AnyD outperforms CIL baselines by over 14% in open-loop
evaluation and 30% in closed-loop testing on CARLA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12300">See to Touch: Learning Tactile Dexterity through Visual Incentives. (arXiv:2309.12300v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guzey_I/0/1/0/all/0/1">Irmak Guzey</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1">Yinlong Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Evans_B/0/1/0/all/0/1">Ben Evans</a>, <a href="http://arxiv.org/find/cs/1/au:+Chintala_S/0/1/0/all/0/1">Soumith Chintala</a>, <a href="http://arxiv.org/find/cs/1/au:+Pinto_L/0/1/0/all/0/1">Lerrel Pinto</a></p>
<p>Equipping multi-fingered robots with tactile sensing is crucial for achieving
the precise, contact-rich, and dexterous manipulation that humans excel at.
However, relying solely on tactile sensing fails to provide adequate cues for
reasoning about objects' spatial configurations, limiting the ability to
correct errors and adapt to changing situations. In this paper, we present
Tactile Adaptation from Visual Incentives (TAVI), a new framework that enhances
tactile-based dexterity by optimizing dexterous policies using vision-based
rewards. First, we use a contrastive-based objective to learn visual
representations. Next, we construct a reward function using these visual
representations through optimal-transport based matching on one human
demonstration. Finally, we use online reinforcement learning on our robot to
optimize tactile-based policies that maximize the visual reward. On six
challenging tasks, such as peg pick-and-place, unstacking bowls, and flipping
slender objects, TAVI achieves a success rate of 73% using our four-fingered
Allegro robot hand. The increase in performance is 108% higher than policies
using tactile and vision-based rewards and 135% higher than policies without
tactile observational input. Robot videos are best viewed on our project
website: https://see-to-touch.github.io/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12301">Environment-biased Feature Ranking for Novelty Detection Robustness. (arXiv:2309.12301v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Smeu_S/0/1/0/all/0/1">Stefan Smeu</a>, <a href="http://arxiv.org/find/cs/1/au:+Burceanu_E/0/1/0/all/0/1">Elena Burceanu</a>, <a href="http://arxiv.org/find/cs/1/au:+Haller_E/0/1/0/all/0/1">Emanuela Haller</a>, <a href="http://arxiv.org/find/cs/1/au:+Nicolicioiu_A/0/1/0/all/0/1">Andrei Liviu Nicolicioiu</a></p>
<p>We tackle the problem of robust novelty detection, where we aim to detect
novelties in terms of semantic content while being invariant to changes in
other, irrelevant factors. Specifically, we operate in a setup with multiple
environments, where we determine the set of features that are associated more
with the environments, rather than to the content relevant for the task. Thus,
we propose a method that starts with a pretrained embedding and a multi-env
setup and manages to rank the features based on their environment-focus. First,
we compute a per-feature score based on the feature distribution variance
between envs. Next, we show that by dropping the highly scored ones, we manage
to remove spurious correlations and improve the overall performance by up to
6%, both in covariance and sub-population shift cases, both for a real and a
synthetic benchmark, that we introduce for this task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12307">LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models. (arXiv:2309.12307v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yukang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1">Shengju Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1">Haotian Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lai_X/0/1/0/all/0/1">Xin Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhijian Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1">Song Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1">Jiaya Jia</a></p>
<p>We present LongLoRA, an efficient fine-tuning approach that extends the
context sizes of pre-trained large language models (LLMs), with limited
computation cost. Typically, training LLMs with long context sizes is
computationally expensive, requiring extensive training hours and GPU
resources. For example, training on the context length of 8192 needs 16x
computational costs in self-attention layers as that of 2048. In this paper, we
speed up the context extension of LLMs in two aspects. On the one hand,
although dense global attention is needed during inference, fine-tuning the
model can be effectively and efficiently done by sparse local attention. The
proposed shift short attention effectively enables context extension, leading
to non-trivial computation saving with similar performance to fine-tuning with
vanilla attention. Particularly, it can be implemented with only two lines of
code in training, while being optional in inference. On the other hand, we
revisit the parameter-efficient fine-tuning regime for context expansion.
Notably, we find that LoRA for context extension works well under the premise
of trainable embedding and normalization. LongLoRA demonstrates strong
empirical results on various tasks on LLaMA2 models from 7B/13B to 70B.
LongLoRA adopts LLaMA2 7B from 4k context to 100k, or LLaMA2 70B to 32k on a
single 8x A100 machine. LongLoRA extends models' context while retaining their
original architectures, and is compatible with most existing techniques, like
FlashAttention-2. In addition, to make LongLoRA practical, we collect a
dataset, LongQA, for supervised fine-tuning. It contains more than 3k long
context question-answer pairs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12309">Rehearsal: Simulating Conflict to Teach Conflict Resolution. (arXiv:2309.12309v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shaikh_O/0/1/0/all/0/1">Omar Shaikh</a>, <a href="http://arxiv.org/find/cs/1/au:+Chai_V/0/1/0/all/0/1">Valentino Chai</a>, <a href="http://arxiv.org/find/cs/1/au:+Gelfand_M/0/1/0/all/0/1">Michele J. Gelfand</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1">Diyi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bernstein_M/0/1/0/all/0/1">Michael S. Bernstein</a></p>
<p>Interpersonal conflict is an uncomfortable but unavoidable fact of life.
Navigating conflict successfully is a skill -- one that can be learned through
deliberate practice -- but few have access to effective training or feedback.
To expand this access, we introduce Rehearsal, a system that allows users to
rehearse conflicts with a believable simulated interlocutor, explore
counterfactual "what if?" scenarios to identify alternative conversational
paths, and learn through feedback on how and when to apply specific conflict
strategies. Users can utilize Rehearsal to practice handling a variety of
predefined conflict scenarios, from office disputes to relationship issues, or
they can choose to create their own. To enable Rehearsal, we develop IRP
prompting, a method of conditioning output of a large language model on the
influential Interest-Rights-Power (IRP) theory from conflict resolution.
Rehearsal uses IRP to generate utterances grounded in conflict resolution
theory, guiding users towards counterfactual conflict resolution strategies
that help de-escalate difficult conversations. In a between-subjects
evaluation, 40 participants engaged in an actual conflict with a confederate
after training. Compared to a control group with lecture material covering the
same IRP theory, participants with simulated training from Rehearsal
significantly improved their performance in the unaided conflict: they reduced
their use of escalating competitive strategies by an average of 67%, while
doubling their use of cooperative strategies. Overall, Rehearsal highlights the
potential effectiveness of language models as tools for learning and practicing
interpersonal skills.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12311">LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent. (arXiv:2309.12311v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jianing Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xuweiyi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1">Shengyi Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Madaan_N/0/1/0/all/0/1">Nikhil Madaan</a>, <a href="http://arxiv.org/find/cs/1/au:+Iyengar_M/0/1/0/all/0/1">Madhavan Iyengar</a>, <a href="http://arxiv.org/find/cs/1/au:+Fouhey_D/0/1/0/all/0/1">David F. Fouhey</a>, <a href="http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1">Joyce Chai</a></p>
<p>3D visual grounding is a critical skill for household robots, enabling them
to navigate, manipulate objects, and answer questions based on their
environment. While existing approaches often rely on extensive labeled data or
exhibit limitations in handling complex language queries, we propose
LLM-Grounder, a novel zero-shot, open-vocabulary, Large Language Model
(LLM)-based 3D visual grounding pipeline. LLM-Grounder utilizes an LLM to
decompose complex natural language queries into semantic constituents and
employs a visual grounding tool, such as OpenScene or LERF, to identify objects
in a 3D scene. The LLM then evaluates the spatial and commonsense relations
among the proposed objects to make a final grounding decision. Our method does
not require any labeled training data and can generalize to novel 3D scenes and
arbitrary text queries. We evaluate LLM-Grounder on the ScanRefer benchmark and
demonstrate state-of-the-art zero-shot grounding accuracy. Our findings
indicate that LLMs significantly improve the grounding capability, especially
for complex language queries, making LLM-Grounder an effective approach for 3D
vision-language tasks in robotics. Videos and interactive demos can be found on
the project website https://chat-with-nerf.github.io/ .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12312">ForceSight: Text-Guided Mobile Manipulation with Visual-Force Goals. (arXiv:2309.12312v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Collins_J/0/1/0/all/0/1">Jeremy A. Collins</a>, <a href="http://arxiv.org/find/cs/1/au:+Houff_C/0/1/0/all/0/1">Cody Houff</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1">You Liang Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kemp_C/0/1/0/all/0/1">Charles C. Kemp</a></p>
<p>We present ForceSight, a system for text-guided mobile manipulation that
predicts visual-force goals using a deep neural network. Given a single RGBD
image combined with a text prompt, ForceSight determines a target end-effector
pose in the camera frame (kinematic goal) and the associated forces (force
goal). Together, these two components form a visual-force goal. Prior work has
demonstrated that deep models outputting human-interpretable kinematic goals
can enable dexterous manipulation by real robots. Forces are critical to
manipulation, yet have typically been relegated to lower-level execution in
these systems. When deployed on a mobile manipulator equipped with an
eye-in-hand RGBD camera, ForceSight performed tasks such as precision grasps,
drawer opening, and object handovers with an 81% success rate in unseen
environments with object instances that differed significantly from the
training data. In a separate experiment, relying exclusively on visual servoing
and ignoring force goals dropped the success rate from 90% to 45%,
demonstrating that force goals can significantly enhance performance. The
appendix, videos, code, and trained models are available at
https://force-sight.github.io/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2108.05015">VisEvent: Reliable Object Tracking via Collaboration of Frame and Event Flows. (arXiv:2108.05015v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jianing Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1">Lin Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhipeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhe Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yaowei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yonghong Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1">Feng Wu</a></p>
<p>Different from visible cameras which record intensity images frame by frame,
the biologically inspired event camera produces a stream of asynchronous and
sparse events with much lower latency. In practice, visible cameras can better
perceive texture details and slow motion, while event cameras can be free from
motion blurs and have a larger dynamic range which enables them to work well
under fast motion and low illumination. Therefore, the two sensors can
cooperate with each other to achieve more reliable object tracking. In this
work, we propose a large-scale Visible-Event benchmark (termed VisEvent) due to
the lack of a realistic and scaled dataset for this task. Our dataset consists
of 820 video pairs captured under low illumination, high speed, and background
clutter scenarios, and it is divided into a training and a testing subset, each
of which contains 500 and 320 videos, respectively. Based on VisEvent, we
transform the event flows into event images and construct more than 30 baseline
methods by extending current single-modality trackers into dual-modality
versions. More importantly, we further build a simple but effective tracking
algorithm by proposing a cross-modality transformer, to achieve more effective
feature fusion between visible and event data. Extensive experiments on the
proposed VisEvent dataset, FE108, COESOT, and two simulated datasets (i.e.,
OTB-DVS and VOT-DVS), validated the effectiveness of our model. The dataset and
source code have been released on:
\url{https://github.com/wangxiao5791509/VisEvent_SOT_Benchmark}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2204.05232">Survey of Aspect-based Sentiment Analysis Datasets. (arXiv:2204.05232v5 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chebolu_S/0/1/0/all/0/1">Siva Uday Sampreeth Chebolu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1">Franck Dernoncourt</a>, <a href="http://arxiv.org/find/cs/1/au:+Lipka_N/0/1/0/all/0/1">Nedim Lipka</a>, <a href="http://arxiv.org/find/cs/1/au:+Solorio_T/0/1/0/all/0/1">Thamar Solorio</a></p>
<p>Aspect-based sentiment analysis (ABSA) is a natural language processing
problem that requires analyzing user-generated reviews to determine: a) The
target entity being reviewed, b) The high-level aspect to which it belongs, and
c) The sentiment expressed toward the targets and the aspects. Numerous yet
scattered corpora for ABSA make it difficult for researchers to identify
corpora best suited for a specific ABSA subtask quickly. This study aims to
present a database of corpora that can be used to train and assess autonomous
ABSA systems. Additionally, we provide an overview of the major corpora for
ABSA and its subtasks and highlight several features that researchers should
consider when selecting a corpus. Finally, we discuss the advantages and
disadvantages of current collection approaches and make recommendations for
future corpora creation. This survey examines 65 publicly available ABSA
datasets covering over 25 domains, including 45 English and 20 other languages
datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.02998">Optimal Propagation for Graph Neural Networks. (arXiv:2205.02998v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1">Beidi Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1">Boxin Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zhe Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Liangyue Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tong_H/0/1/0/all/0/1">Hanghang Tong</a></p>
<p>Graph Neural Networks (GNNs) have achieved tremendous success in a variety of
real-world applications by relying on the fixed graph data as input. However,
the initial input graph might not be optimal in terms of specific downstream
tasks, because of information scarcity, noise, adversarial attacks, or
discrepancies between the distribution in graph topology, features, and
groundtruth labels. In this paper, we propose a bi-level optimization approach
for learning the optimal graph structure via directly learning the Personalized
PageRank propagation matrix as well as the downstream semi-supervised node
classification simultaneously. We also explore a low-rank approximation model
for further reducing the time complexity. Empirical evaluations show the
superior efficacy and robustness of the proposed model over all baseline
methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.04952">Read the Room: Adapting a Robot&#x27;s Voice to Ambient and Social Contexts. (arXiv:2205.04952v3 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tuttosi_P/0/1/0/all/0/1">Paige Tuttosi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hughson_E/0/1/0/all/0/1">Emma Hughson</a>, <a href="http://arxiv.org/find/cs/1/au:+Matsufuji_A/0/1/0/all/0/1">Akihiro Matsufuji</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_A/0/1/0/all/0/1">Angelica Lim</a></p>
<p>How should a robot speak in a formal, quiet and dark, or a bright, lively and
noisy environment? By designing robots to speak in a more social and
ambient-appropriate manner we can improve perceived awareness and intelligence
for these agents. We describe a process and results toward selecting robot
voice styles for perceived social appropriateness and ambiance awareness.
Understanding how humans adapt their voices in different acoustic settings can
be challenging due to difficulties in voice capture in the wild. Our approach
includes 3 steps: (a) Collecting and validating voice data interactions in
virtual Zoom ambiances, (b) Exploration and clustering human vocal utterances
to identify primary voice styles, and (c) Testing robot voice styles in
recreated ambiances using projections, lighting and sound. We focus on food
service scenarios as a proof-of-concept setting. We provide results using the
Pepper robot's voice with different styles, towards robots that speak in a
contextually appropriate and adaptive manner. Our results with N=120
participants provide evidence that the choice of voice style in different
ambiances impacted a robot's perceived intelligence in several factors
including: social appropriateness, comfort, awareness, human-likeness and
competency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.05355">Analysis and Comparison of Classification Metrics. (arXiv:2209.05355v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ferrer_L/0/1/0/all/0/1">Luciana Ferrer</a></p>
<p>A variety of different performance metrics are commonly used in the machine
learning literature for the evaluation of classification systems. Some of the
most common ones for measuring quality of hard decisions are standard and
balanced accuracy, standard and balanced error rate, F-beta score, and Matthews
correlation coefficient (MCC). In this document, we review the definition of
these and other metrics and compare them with the expected cost (EC), a metric
introduced in every statistical learning course but rarely used in the machine
learning literature. We show that both the standard and balanced error rates
are special cases of the EC. Further, we show its relation with F-beta score
and MCC and argue that EC is superior to these traditional metrics for being
based on first principles from statistics, and for being more general,
interpretable, and adaptable to any application scenario. The metrics mentioned
above measure the quality of hard decisions. Yet, most modern classification
systems output continuous scores for the classes which we may want to evaluate
directly. Metrics for measuring the quality of system scores include the area
under the ROC curve, equal error rate, cross-entropy, Brier score, and Bayes EC
or Bayes risk, among others. The last three metrics are special cases of a
family of metrics given by the expected value of proper scoring rules (PSRs).
We review the theory behind these metrics, showing that they are a principled
way to measure the quality of the posterior probabilities produced by a system.
Finally, we show how to use these metrics to compute a system's calibration
loss and compare this metric with the widely-used expected calibration error
(ECE), arguing that calibration loss based on PSRs is superior to the ECE for
being more interpretable, more general, and directly applicable to the
multi-class case, among other reasons.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.11084">Towards Cooperative Flight Control Using Visual-Attention. (arXiv:2212.11084v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1">Lianhao Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chahine_M/0/1/0/all/0/1">Makram Chahine</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tsun-Hsuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Seyde_T/0/1/0/all/0/1">Tim Seyde</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lechner_M/0/1/0/all/0/1">Mathias Lechner</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasani_R/0/1/0/all/0/1">Ramin Hasani</a>, <a href="http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1">Daniela Rus</a></p>
<p>The cooperation of a human pilot with an autonomous agent during flight
control realizes parallel autonomy. We propose an air-guardian system that
facilitates cooperation between a pilot with eye tracking and a parallel
end-to-end neural control system. Our vision-based air-guardian system combines
a causal continuous-depth neural network model with a cooperation layer to
enable parallel autonomy between a pilot and a control system based on
perceived differences in their attention profiles. The attention profiles for
neural networks are obtained by computing the networks' saliency maps (feature
importance) through the VisualBackProp algorithm, while the attention profiles
for humans are either obtained by eye tracking of human pilots or saliency maps
of networks trained to imitate human pilots. When the attention profile of the
pilot and guardian agents align, the pilot makes control decisions. Otherwise,
the air-guardian makes interventions and takes over the control of the
aircraft. We show that our attention-based air-guardian system can balance the
trade-off between its level of involvement in the flight and the pilot's
expertise and attention. The guardian system is particularly effective in
situations where the pilot was distracted due to information overload. We
demonstrate the effectiveness of our method for navigating flight scenarios in
simulation with a fixed-wing aircraft and on hardware with a quadrotor
platform.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.03044">A Survey on Transformers in Reinforcement Learning. (arXiv:2301.03044v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wenzhe Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1">Hao Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zichuan Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chongjie Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Zongqing Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_D/0/1/0/all/0/1">Deheng Ye</a></p>
<p>Transformer has been considered the dominating neural architecture in NLP and
CV, mostly under supervised settings. Recently, a similar surge of using
Transformers has appeared in the domain of reinforcement learning (RL), but it
is faced with unique design choices and challenges brought by the nature of RL.
However, the evolution of Transformers in RL has not yet been well unraveled.
In this paper, we seek to systematically review motivations and progress on
using Transformers in RL, provide a taxonomy on existing works, discuss each
sub-field, and summarize future prospects.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.04456">ERNIE-Music: Text-to-Waveform Music Generation with Diffusion Models. (arXiv:2302.04456v2 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1">Pengfei Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pang_C/0/1/0/all/0/1">Chao Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1">Yekun Chai</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuohuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yu Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1">Hao Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Hua Wu</a></p>
<p>In recent years, the burgeoning interest in diffusion models has led to
significant advances in image and speech generation. Nevertheless, the direct
synthesis of music waveforms from unrestricted textual prompts remains a
relatively underexplored domain. In response to this lacuna, this paper
introduces a pioneering contribution in the form of a text-to-waveform music
generation model, underpinned by the utilization of diffusion models. Our
methodology hinges on the innovative incorporation of free-form textual prompts
as conditional factors to guide the waveform generation process within the
diffusion model framework. Addressing the challenge of limited text-music
parallel data, we undertake the creation of a dataset by harnessing web
resources, a task facilitated by weak supervision techniques. Furthermore, a
rigorous empirical inquiry is undertaken to contrast the efficacy of two
distinct prompt formats for text conditioning, namely, music tags and
unconstrained textual descriptions. The outcomes of this comparative analysis
affirm the superior performance of our proposed model in terms of enhancing
text-music relevance. Finally, our work culminates in a demonstrative
exhibition of the excellent capabilities of our model in text-to-music
generation. We further demonstrate that our generated music in the waveform
domain outperforms previous works by a large margin in terms of diversity,
quality, and text-music relevance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.13773">Graph Neural Networks for the Offline Nanosatellite Task Scheduling Problem. (arXiv:2303.13773v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pacheco_B/0/1/0/all/0/1">Bruno Machado Pacheco</a>, <a href="http://arxiv.org/find/cs/1/au:+Seman_L/0/1/0/all/0/1">Laio Oriel Seman</a>, <a href="http://arxiv.org/find/cs/1/au:+Rigo_C/0/1/0/all/0/1">Cezar Antonio Rigo</a>, <a href="http://arxiv.org/find/cs/1/au:+Camponogara_E/0/1/0/all/0/1">Eduardo Camponogara</a>, <a href="http://arxiv.org/find/cs/1/au:+Bezerra_E/0/1/0/all/0/1">Eduardo Augusto Bezerra</a>, <a href="http://arxiv.org/find/cs/1/au:+Coelho_L/0/1/0/all/0/1">Leandro dos Santos Coelho</a></p>
<p>This study investigates how to schedule nanosatellite tasks more efficiently
using Graph Neural Networks (GNNs). In the Offline Nanosatellite Task
Scheduling (ONTS) problem, the goal is to find the optimal schedule for tasks
to be carried out in orbit while taking into account Quality-of-Service (QoS)
considerations such as priority, minimum and maximum activation events,
execution time-frames, periods, and execution windows, as well as constraints
on the satellite's power resources and the complexity of energy harvesting and
management. The ONTS problem has been approached using conventional
mathematical formulations and exact methods, but their applicability to
challenging cases of the problem is limited. This study examines the use of
GNNs in this context, which has been effectively applied to optimization
problems such as the traveling salesman, scheduling, and facility placement
problems. More specifically, we investigate whether GNNs can learn the complex
structure of the ONTS problem with respect to feasibility and optimality of
candidate solutions. Furthermore, we evaluate using GNN-based heuristic
solutions to provide better solutions (w.r.t. the objective value) to the ONTS
problem and reduce the optimization cost. Our experiments show that GNNs are
not only able to learn feasibility and optimality for instances of the ONTS
problem, but they can generalize to harder instances than those seen during
training. Furthermore, the GNN-based heuristics improved the expected objective
value of the best solution found under the time limit in 45%, and reduced the
expected time to find a feasible solution in 35%, when compared to the SCIP
(Solving Constraint Integer Programs) solver in its off-the-shelf configuration
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.11530">Ensuring Trustworthy Medical Artificial Intelligence through Ethical and Philosophical Principles. (arXiv:2304.11530v4 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jha_D/0/1/0/all/0/1">Debesh Jha</a>, <a href="http://arxiv.org/find/cs/1/au:+Rauniyar_A/0/1/0/all/0/1">Ashish Rauniyar</a>, <a href="http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1">Abhiskek Srivastava</a>, <a href="http://arxiv.org/find/cs/1/au:+Hagos_D/0/1/0/all/0/1">Desta Haileselassie Hagos</a>, <a href="http://arxiv.org/find/cs/1/au:+Tomar_N/0/1/0/all/0/1">Nikhil Kumar Tomar</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_V/0/1/0/all/0/1">Vanshali Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Keles_E/0/1/0/all/0/1">Elif Keles</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zheyuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Demir_U/0/1/0/all/0/1">Ugur Demir</a>, <a href="http://arxiv.org/find/cs/1/au:+Topcu_A/0/1/0/all/0/1">Ahmet Topcu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yazidi_A/0/1/0/all/0/1">Anis Yazidi</a>, <a href="http://arxiv.org/find/cs/1/au:+Haaakegaard_J/0/1/0/all/0/1">Jan Erik H&#xe5;akeg&#xe5;rd</a>, <a href="http://arxiv.org/find/cs/1/au:+Bagci_U/0/1/0/all/0/1">Ulas Bagci</a></p>
<p>Artificial intelligence (AI) methods hold immense potential to revolutionize
numerous medical care by enhancing the experience of medical experts and
patients. AI-based computer-assisted diagnosis and treatment tools can
democratize healthcare by matching the clinical level or surpassing clinical
experts. As a result, advanced healthcare services can be affordable to all
populations, irrespective of demographics, race, or socioeconomic background.
The democratization of such AI tools can reduce the cost of care, optimize
resource allocation, and improve the quality of care. In contrast to humans, AI
can uncover complex relations in the data from a large set of inputs and even
lead to new evidence-based knowledge in medicine. However, integrating AI into
healthcare raises several ethical and philosophical concerns, such as bias,
transparency, autonomy, responsibility, and accountability. Here, we emphasize
recent advances in AI-assisted medical image analysis, existing standards, and
the significance of comprehending ethical issues and best practices for
clinical settings. We cover the technical and ethical challenges and
implications of deploying AI in hospitals and public organizations. We also
discuss key measures and techniques to address ethical challenges, data
scarcity, racial bias, lack of transparency, and algorithmic bias and provide
recommendations and future directions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.12654">CoDi: Co-evolving Contrastive Diffusion Models for Mixed-type Tabular Synthesis. (arXiv:2304.12654v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1">Chaejeong Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jayoung Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1">Noseong Park</a></p>
<p>With growing attention to tabular data these days, the attempt to apply a
synthetic table to various tasks has been expanded toward various scenarios.
Owing to the recent advances in generative modeling, fake data generated by
tabular data synthesis models become sophisticated and realistic. However,
there still exists a difficulty in modeling discrete variables (columns) of
tabular data. In this work, we propose to process continuous and discrete
variables separately (but being conditioned on each other) by two diffusion
models. The two diffusion models are co-evolved during training by reading
conditions from each other. In order to further bind the diffusion models,
moreover, we introduce a contrastive learning method with a negative sampling
method. In our experiments with 11 real-world tabular datasets and 8 baseline
methods, we prove the efficacy of the proposed method, called CoDi.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.06908">CoMoSpeech: One-Step Speech and Singing Voice Synthesis via Consistency Model. (arXiv:2305.06908v3 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1">Zhen Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1">Wei Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1">Xu Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jie Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qifeng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yike Guo</a></p>
<p>Denoising diffusion probabilistic models (DDPMs) have shown promising
performance for speech synthesis. However, a large number of iterative steps
are required to achieve high sample quality, which restricts the inference
speed. Maintaining sample quality while increasing sampling speed has become a
challenging task. In this paper, we propose a "Co"nsistency "Mo"del-based
"Speech" synthesis method, CoMoSpeech, which achieve speech synthesis through a
single diffusion sampling step while achieving high audio quality. The
consistency constraint is applied to distill a consistency model from a
well-designed diffusion-based teacher model, which ultimately yields superior
performances in the distilled CoMoSpeech. Our experiments show that by
generating audio recordings by a single sampling step, the CoMoSpeech achieves
an inference speed more than 150 times faster than real-time on a single NVIDIA
A100 GPU, which is comparable to FastSpeech2, making diffusion-sampling based
speech synthesis truly practical. Meanwhile, objective and subjective
evaluations on text-to-speech and singing voice synthesis show that the
proposed teacher models yield the best audio quality, and the one-step sampling
based CoMoSpeech achieves the best inference speed with better or comparable
audio quality to other conventional multi-step diffusion model baselines. Audio
samples are available at https://comospeech.github.io/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.07712">Poisson-Gaussian Holographic Phase Retrieval with Score-based Image Prior. (arXiv:2305.07712v2 [eess.SP] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1">Zongyu Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Hu_J/0/1/0/all/0/1">Jason Hu</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1">Xiaojian Xu</a>, <a href="http://arxiv.org/find/eess/1/au:+Shen_L/0/1/0/all/0/1">Liyue Shen</a>, <a href="http://arxiv.org/find/eess/1/au:+Fessler_J/0/1/0/all/0/1">Jeffrey A. Fessler</a></p>
<p>Phase retrieval (PR) is a crucial problem in many imaging applications. This
study focuses on resolving the holographic phase retrieval problem in
situations where the measurements are affected by a combination of Poisson and
Gaussian noise, which commonly occurs in optical imaging systems. To address
this problem, we propose a new algorithm called "AWFS" that uses the
accelerated Wirtinger flow (AWF) with a score function as generative prior.
Specifically, we formulate the PR problem as an optimization problem that
incorporates both data fidelity and regularization terms. We calculate the
gradient of the log-likelihood function for PR and determine its corresponding
Lipschitz constant. Additionally, we introduce a generative prior in our
regularization framework by using score matching to capture information about
the gradient of image prior distributions. We provide theoretical analysis that
establishes a critical-point convergence guarantee for the proposed algorithm.
The results of our simulation experiments on three different datasets show the
following: 1) By using the PG likelihood model, the proposed algorithm improves
reconstruction compared to algorithms based solely on Gaussian or Poisson
likelihood. 2) The proposed score-based image prior method, performs better
than the method based on denoising diffusion probabilistic model (DDPM), as
well as plug-and-play alternating direction method of multipliers (PnP-ADMM)
and regularization by denoising (RED).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13135">Transforming Geospatial Ontologies by Homomorphisms. (arXiv:2305.13135v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1">Xiuzhan Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1">Wei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1">Min Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Rangarajan_P/0/1/0/all/0/1">Priya Rangarajan</a></p>
<p>In this paper, we study the geospatial ontologies that we are interested in
together as a geospatial ontology system, consisting of a set of the geospatial
ontologies and a set of geospatial ontology operations, without any internal
details of the geospatial ontologies and their operations being needed,
algebraically. A homomorphism between two geospatial ontology systems is a
function between two sets of geospatial ontologies in the systems, which
preserves the geospatial ontology operations. We view clustering a set of the
ontologies as partitioning the set or defining an equivalence relation on the
set or forming a quotient set of the set or obtaining the surjective image of
the set. Each geospatial ontology system homomorphism can be factored as a
surjective clustering to a quotient space, followed by an embedding. Geospatial
ontology merging systems, natural partial orders on the systems, and geospatial
ontology merging closures in the systems are then transformed under geospatial
ontology system homomorphisms that are given by quotients and embeddings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13706">Semantic-aware Transmission Scheduling: a Monotonicity-driven Deep Reinforcement Learning Approach. (arXiv:2305.13706v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiazheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wanchun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Quevedo_D/0/1/0/all/0/1">Daniel Quevedo</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yonghui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Vucetic_B/0/1/0/all/0/1">Branka Vucetic</a></p>
<p>For cyber-physical systems in the 6G era, semantic communications connecting
distributed devices for dynamic control and remote state estimation are
required to guarantee application-level performance, not merely focus on
communication-centric performance. Semantics here is a measure of the
usefulness of information transmissions. Semantic-aware transmission scheduling
of a large system often involves a large decision-making space, and the optimal
policy cannot be obtained by existing algorithms effectively. In this paper, we
first investigate the fundamental properties of the optimal semantic-aware
scheduling policy and then develop advanced deep reinforcement learning (DRL)
algorithms by leveraging the theoretical guidelines. Our numerical results show
that the proposed algorithms can substantially reduce training time and enhance
training performance compared to benchmark algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16755">Can large language models generate salient negative statements?. (arXiv:2305.16755v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Arnaout_H/0/1/0/all/0/1">Hiba Arnaout</a>, <a href="http://arxiv.org/find/cs/1/au:+Razniewski_S/0/1/0/all/0/1">Simon Razniewski</a></p>
<p>We examine the ability of large language models (LLMs) to generate salient
(interesting) negative statements about real-world entities; an emerging
research topic of the last few years. We probe the LLMs using zero- and k-shot
unconstrained probes, and compare with traditional methods for negation
generation, i.e., pattern-based textual extractions and knowledge-graph-based
inferences, as well as crowdsourced gold statements. We measure the correctness
and salience of the generated lists about subjects from different domains. Our
evaluation shows that guided probes do in fact improve the quality of generated
negatives, compared to the zero-shot variant. Nevertheless, using both prompts,
LLMs still struggle with the notion of factuality of negatives, frequently
generating many ambiguous statements, or statements with negative keywords but
a positive meaning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.06101">Prodigy: An Expeditiously Adaptive Parameter-Free Learner. (arXiv:2306.06101v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mishchenko_K/0/1/0/all/0/1">Konstantin Mishchenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Defazio_A/0/1/0/all/0/1">Aaron Defazio</a></p>
<p>We consider the problem of estimating the learning rate in adaptive methods,
such as Adagrad and Adam. We describe two techniques, Prodigy and Resetting, to
provably estimate the distance to the solution $D$, which is needed to set the
learning rate optimally. Our techniques are modifications of the D-Adaptation
method for learning-rate-free learning. Our methods improve upon the
convergence rate of D-Adaptation by a factor of $O(\sqrt{\log(D/d_0)})$, where
$d_0$ is the initial estimate of $D$. We test our methods on 12 common
logistic-regression benchmark datasets, VGG11 and ResNet-50 training on
CIFAR10, ViT training on Imagenet, LSTM training on IWSLT14, DLRM training on
Criteo dataset, VarNet on Knee MRI dataset, as well as RoBERTa and GPT
transformer training on BookWiki. Our experimental results show that our
approaches consistently outperform D-Adaptation and reach test accuracy values
close to that of hand-tuned Adam.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.10308">Achilles&#x27; Heels: Vulnerable Record Identification in Synthetic Data Publishing. (arXiv:2306.10308v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Meeus_M/0/1/0/all/0/1">Matthieu Meeus</a>, <a href="http://arxiv.org/find/cs/1/au:+Guepin_F/0/1/0/all/0/1">Florent Gu&#xe9;pin</a>, <a href="http://arxiv.org/find/cs/1/au:+Cretu_A/0/1/0/all/0/1">Ana-Maria Cretu</a>, <a href="http://arxiv.org/find/cs/1/au:+Montjoye_Y/0/1/0/all/0/1">Yves-Alexandre de Montjoye</a></p>
<p>Synthetic data is seen as the most promising solution to share
individual-level data while preserving privacy. Shadow modeling-based
Membership Inference Attacks (MIAs) have become the standard approach to
evaluate the privacy risk of synthetic data. While very effective, they require
a large number of datasets to be created and models trained to evaluate the
risk posed by a single record. The privacy risk of a dataset is thus currently
evaluated by running MIAs on a handful of records selected using ad-hoc
methods. We here propose what is, to the best of our knowledge, the first
principled vulnerable record identification technique for synthetic data
publishing, leveraging the distance to a record's closest neighbors. We show
our method to strongly outperform previous ad-hoc methods across datasets and
generators. We also show evidence of our method to be robust to the choice of
MIA and to specific choice of parameters. Finally, we show it to accurately
identify vulnerable records when synthetic data generators are made
differentially private. The choice of vulnerable records is as important as
more accurate MIAs when evaluating the privacy of synthetic data releases,
including from a legal perspective. We here propose a simple yet highly
effective method to do so. We hope our method will enable practitioners to
better estimate the risk posed by synthetic data publishing and researchers to
fairly compare ever improving MIAs on synthetic data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.14437">A Self-supervised Contrastive Learning Method for Grasp Outcomes Prediction. (arXiv:2306.14437v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chengliang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1">Binhua Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yiwen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1">Yuanzhe Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Mai_K/0/1/0/all/0/1">Ke Mai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yupo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_Z/0/1/0/all/0/1">Zhengkun Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xinyu Wu</a></p>
<p>In this paper, we investigate the effectiveness of contrastive learning
methods for predicting grasp outcomes in an unsupervised manner. By utilizing a
publicly available dataset, we demonstrate that contrastive learning methods
perform well on the task of grasp outcomes prediction. Specifically, the
dynamic-dictionary-based method with the momentum updating technique achieves a
satisfactory accuracy of 81.83% using data from one single tactile sensor,
outperforming other unsupervised methods. Our results reveal the potential of
contrastive learning methods for applications in the field of robot grasping
and highlight the importance of accurate grasp prediction for achieving stable
grasps.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.16524">Hyena Neural Operator for Partial Differential Equations. (arXiv:2306.16524v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Patil_S/0/1/0/all/0/1">Saurabh Patil</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zijie Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Farimani_A/0/1/0/all/0/1">Amir Barati Farimani</a></p>
<p>Numerically solving partial differential equations typically requires fine
discretization to resolve necessary spatiotemporal scales, which can be
computationally expensive. Recent advances in deep learning have provided a new
approach to solving partial differential equations that involves the use of
neural operators. Neural operators are neural network architectures that learn
mappings between function spaces and have the capability to solve partial
differential equations based on data. This study utilizes a novel neural
operator called Hyena, which employs a long convolutional filter that is
parameterized by a multilayer perceptron. The Hyena operator is an operation
that enjoys sub-quadratic complexity and state space model to parameterize long
convolution that enjoys a global receptive field. This mechanism enhances the
model's comprehension of the input's context and enables data-dependent weight
for different partial differential equations instances. To measure how
effective the layers are in solving partial differential equations, we conduct
experiments on Diffusion-Reaction equation and Navier Stokes equation. Our
findings indicate Hyena Neural operator can serve as an efficient and accurate
model for learning partial differential equations solution operator. The data
and code used can be found at:
https://github.com/Saupatil07/Hyena-Neural-Operator
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.17366">$\lambda$-AC: Learning latent decision-aware models for reinforcement learning in continuous state-spaces. (arXiv:2306.17366v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Voelcker_C/0/1/0/all/0/1">Claas A Voelcker</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmadian_A/0/1/0/all/0/1">Arash Ahmadian</a>, <a href="http://arxiv.org/find/cs/1/au:+Abachi_R/0/1/0/all/0/1">Romina Abachi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gilitschenski_I/0/1/0/all/0/1">Igor Gilitschenski</a>, <a href="http://arxiv.org/find/cs/1/au:+Farahmand_A/0/1/0/all/0/1">Amir-massoud Farahmand</a></p>
<p>The idea of decision-aware model learning, that models should be accurate
where it matters for decision-making, has gained prominence in model-based
reinforcement learning. While promising theoretical results have been
established, the empirical performance of algorithms leveraging a
decision-aware loss has been lacking, especially in continuous control
problems. In this paper, we present a study on the necessary components for
decision-aware reinforcement learning models and we showcase design choices
that enable well-performing algorithms. To this end, we provide a theoretical
and empirical investigation into prominent algorithmic ideas in the field. We
highlight that empirical design decisions established in the MuZero line of
works are vital to achieving good performance for related algorithms, and we
showcase differences in behavior between different instantiations of
value-aware algorithms in stochastic environments. Using these insights, we
propose the Latent Model-Based Decision-Aware Actor-Critic framework
($\lambda$-AC) for decision-aware model-based reinforcement learning in
continuous state-spaces and highlight important design choices in different
environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01701">Synthetic is all you need: removing the auxiliary data assumption for membership inference attacks against synthetic data. (arXiv:2307.01701v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guepin_F/0/1/0/all/0/1">Florent Gu&#xe9;pin</a>, <a href="http://arxiv.org/find/cs/1/au:+Meeus_M/0/1/0/all/0/1">Matthieu Meeus</a>, <a href="http://arxiv.org/find/cs/1/au:+Cretu_A/0/1/0/all/0/1">Ana-Maria Cretu</a>, <a href="http://arxiv.org/find/cs/1/au:+Montjoye_Y/0/1/0/all/0/1">Yves-Alexandre de Montjoye</a></p>
<p>Synthetic data is emerging as one of the most promising solutions to share
individual-level data while safeguarding privacy. While membership inference
attacks (MIAs), based on shadow modeling, have become the standard to evaluate
the privacy of synthetic data, they currently assume the attacker to have
access to an auxiliary dataset sampled from a similar distribution as the
training dataset. This is often seen as a very strong assumption in practice,
especially as the proposed main use cases for synthetic tabular data (e.g.
medical data, financial transactions) are very specific and don't have any
reference datasets directly available. We here show how this assumption can be
removed, allowing for MIAs to be performed using only the synthetic data.
Specifically, we developed three different scenarios: (S1) Black-box access to
the generator, (S2) only access to the released synthetic dataset and (S3) a
theoretical setup as upper bound for the attack performance using only
synthetic data. Our results show that MIAs are still successful, across two
real-world datasets and two synthetic data generators. These results show how
the strong hypothesis made when auditing synthetic data releases - access to an
auxiliary dataset - can be relaxed, making the attacks more realistic in
practice.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09683">PubMed and Beyond: Biomedical Literature Search in the Age of Artificial Intelligence. (arXiv:2307.09683v3 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1">Qiao Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Leaman_R/0/1/0/all/0/1">Robert Leaman</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Zhiyong Lu</a></p>
<p>Biomedical research yields a wealth of information, much of which is only
accessible through the literature. Consequently, literature search is an
essential tool for building on prior knowledge in clinical and biomedical
research. Although recent improvements in artificial intelligence have expanded
functionality beyond keyword-based search, these advances may be unfamiliar to
clinicians and researchers. In response, we present a survey of literature
search tools tailored to both general and specific information needs in
biomedicine, with the objective of helping readers efficiently fulfill their
information needs. We first examine the widely used PubMed search engine,
discussing recent improvements and continued challenges. We then describe
literature search tools catering to five specific information needs: 1.
Identifying high-quality clinical research for evidence-based medicine. 2.
Retrieving gene-related information for precision medicine and genomics. 3.
Searching by meaning, including natural language questions. 4. Locating related
articles with literature recommendation. 5. Mining literature to discover
associations between concepts such as diseases and genetic variants.
Additionally, we cover practical considerations and best practices for choosing
and using these tools. Finally, we provide a perspective on the future of
literature search engines, considering recent breakthroughs in large language
models such as ChatGPT. In summary, our survey provides a comprehensive view of
biomedical literature search functionalities with 36 publicly available tools.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.07037">Bayesian Flow Networks. (arXiv:2308.07037v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Graves_A/0/1/0/all/0/1">Alex Graves</a>, <a href="http://arxiv.org/find/cs/1/au:+Srivastava_R/0/1/0/all/0/1">Rupesh Kumar Srivastava</a>, <a href="http://arxiv.org/find/cs/1/au:+Atkinson_T/0/1/0/all/0/1">Timothy Atkinson</a>, <a href="http://arxiv.org/find/cs/1/au:+Gomez_F/0/1/0/all/0/1">Faustino Gomez</a></p>
<p>This paper introduces Bayesian Flow Networks (BFNs), a new class of
generative model in which the parameters of a set of independent distributions
are modified with Bayesian inference in the light of noisy data samples, then
passed as input to a neural network that outputs a second, interdependent
distribution. Starting from a simple prior and iteratively updating the two
distributions yields a generative procedure similar to the reverse process of
diffusion models; however it is conceptually simpler in that no forward process
is required. Discrete and continuous-time loss functions are derived for
continuous, discretised and discrete data, along with sample generation
procedures. Notably, the network inputs for discrete data lie on the
probability simplex, and are therefore natively differentiable, paving the way
for gradient-based sample guidance and few-step generation in discrete domains
such as language modelling. The loss function directly optimises data
compression and places no restrictions on the network architecture. In our
experiments BFNs achieve competitive log-likelihoods for image modelling on
dynamically binarized MNIST and CIFAR-10, and outperform all known discrete
diffusion models on the text8 character-level language modelling task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.10792">Instruction Tuning for Large Language Models: A Survey. (arXiv:2308.10792v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shengyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1">Linfeng Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaoya Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Sen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xiaofei Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuhe Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiwei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1">Runyi Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tianwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1">Fei Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guoyin Wang</a></p>
<p>This paper surveys research works in the quickly advancing field of
instruction tuning (IT), a crucial technique to enhance the capabilities and
controllability of large language models (LLMs). Instruction tuning refers to
the process of further training LLMs on a dataset consisting of
\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the
gap between the next-word prediction objective of LLMs and the users' objective
of having LLMs adhere to human instructions. In this work, we make a systematic
review of the literature, including the general methodology of IT, the
construction of IT datasets, the training of IT models, and applications to
different modalities, domains and applications, along with an analysis on
aspects that influence the outcome of IT (e.g., generation of instruction
outputs, size of the instruction dataset, etc). We also review the potential
pitfalls of IT along with criticism against it, along with efforts pointing out
current deficiencies of existing strategies and suggest some avenues for
fruitful research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.03992">ConDA: Contrastive Domain Adaptation for AI-generated Text Detection. (arXiv:2309.03992v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bhattacharjee_A/0/1/0/all/0/1">Amrita Bhattacharjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumarage_T/0/1/0/all/0/1">Tharindu Kumarage</a>, <a href="http://arxiv.org/find/cs/1/au:+Moraffah_R/0/1/0/all/0/1">Raha Moraffah</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Huan Liu</a></p>
<p>Large language models (LLMs) are increasingly being used for generating text
in a variety of use cases, including journalistic news articles. Given the
potential malicious nature in which these LLMs can be used to generate
disinformation at scale, it is important to build effective detectors for such
AI-generated text. Given the surge in development of new LLMs, acquiring
labeled training data for supervised detectors is a bottleneck. However, there
might be plenty of unlabeled text data available, without information on which
generator it came from. In this work we tackle this data problem, in detecting
AI-generated news text, and frame the problem as an unsupervised domain
adaptation task. Here the domains are the different text generators, i.e. LLMs,
and we assume we have access to only the labeled source data and unlabeled
target data. We develop a Contrastive Domain Adaptation framework, called
ConDA, that blends standard domain adaptation techniques with the
representation power of contrastive learning to learn domain invariant
representations that are effective for the final unsupervised detection task.
Our experiments demonstrate the effectiveness of our framework, resulting in
average performance gains of 31.7% from the best performing baselines, and
within 0.8% margin of a fully supervised detector. All our code and data is
available at https://github.com/AmritaBh/ConDA-gen-text-detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.08587">Compositional Foundation Models for Hierarchical Planning. (arXiv:2309.08587v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ajay_A/0/1/0/all/0/1">Anurag Ajay</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1">Seungwook Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1">Yilun Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shuang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1">Abhi Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1">Tommi Jaakkola</a>, <a href="http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1">Josh Tenenbaum</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaelbling_L/0/1/0/all/0/1">Leslie Kaelbling</a>, <a href="http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1">Akash Srivastava</a>, <a href="http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1">Pulkit Agrawal</a></p>
<p>To make effective decisions in novel environments with long-horizon goals, it
is crucial to engage in hierarchical reasoning across spatial and temporal
scales. This entails planning abstract subgoal sequences, visually reasoning
about the underlying plans, and executing actions in accordance with the
devised plan through visual-motor control. We propose Compositional Foundation
Models for Hierarchical Planning (HiP), a foundation model which leverages
multiple expert foundation model trained on language, vision and action data
individually jointly together to solve long-horizon tasks. We use a large
language model to construct symbolic plans that are grounded in the environment
through a large video diffusion model. Generated video plans are then grounded
to visual-motor control, through an inverse dynamics model that infers actions
from generated videos. To enable effective reasoning within this hierarchy, we
enforce consistency between the models via iterative refinement. We illustrate
the efficacy and adaptability of our approach in three different long-horizon
table-top manipulation tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.09517">FedGKD: Unleashing the Power of Collaboration in Federated Graph Neural Networks. (arXiv:2309.09517v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pan_Q/0/1/0/all/0/1">Qiying Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1">Ruofan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tengfei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tianyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yifei Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Weiqiang Wang</a></p>
<p>Federated training of Graph Neural Networks (GNN) has become popular in
recent years due to its ability to perform graph-related tasks under data
isolation scenarios while preserving data privacy. However, graph heterogeneity
issues in federated GNN systems continue to pose challenges. Existing
frameworks address the problem by representing local tasks using different
statistics and relating them through a simple aggregation mechanism. However,
these approaches suffer from limited efficiency from two aspects: low quality
of task-relatedness quantification and inefficacy of exploiting the
collaboration structure. To address these issues, we propose FedGKD, a novel
federated GNN framework that utilizes a novel client-side graph dataset
distillation method to extract task features that better describe
task-relatedness, and introduces a novel server-side aggregation mechanism that
is aware of the global collaboration structure. We conduct extensive
experiments on six real-world datasets of different scales, demonstrating our
framework's outperformance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.09553">Causal-Story: Local Causal Attention Utilizing Parameter-Efficient Tuning For Visual Story Synthesis. (arXiv:2309.09553v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_T/0/1/0/all/0/1">Tianyi Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1">Jiuxin Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaofeng Zhang</a></p>
<p>The excellent text-to-image synthesis capability of diffusion models has
driven progress in synthesizing coherent visual stories. The current
state-of-the-art method combines the features of historical captions,
historical frames, and the current captions as conditions for generating the
current frame. However, this method treats each historical frame and caption as
the same contribution. It connects them in order with equal weights, ignoring
that not all historical conditions are associated with the generation of the
current frame. To address this issue, we propose Causal-Story. This model
incorporates a local causal attention mechanism that considers the causal
relationship between previous captions, frames, and current captions. By
assigning weights based on this relationship, Causal-Story generates the
current frame, thereby improving the global consistency of story generation. We
evaluated our model on the PororoSV and FlintstonesSV datasets and obtained
state-of-the-art FID scores, and the generated frames also demonstrate better
storytelling in visuals.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.10532">A Cognitively-Inspired Neural Architecture for Visual Abstract Reasoning Using Contrastive Perceptual and Conceptual Processing. (arXiv:2309.10532v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yuan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanyal_D/0/1/0/all/0/1">Deepayan Sanyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Ainooson_J/0/1/0/all/0/1">James Ainooson</a>, <a href="http://arxiv.org/find/cs/1/au:+Michelson_J/0/1/0/all/0/1">Joel Michelson</a>, <a href="http://arxiv.org/find/cs/1/au:+Farhana_E/0/1/0/all/0/1">Effat Farhana</a>, <a href="http://arxiv.org/find/cs/1/au:+Kunda_M/0/1/0/all/0/1">Maithilee Kunda</a></p>
<p>We introduce a new neural architecture for solving visual abstract reasoning
tasks inspired by human cognition, specifically by observations that human
abstract reasoning often interleaves perceptual and conceptual processing as
part of a flexible, iterative, and dynamic cognitive process. Inspired by this
principle, our architecture models visual abstract reasoning as an iterative,
self-contrasting learning process that pursues consistency between perceptual
and conceptual processing of visual stimuli. We explain how this new
Contrastive Perceptual-Conceptual Network (CPCNet) works using matrix reasoning
problems in the style of the well-known Raven's Progressive Matrices
intelligence test. Experiments on the machine learning dataset RAVEN show that
CPCNet achieves higher accuracy than all previously published models while also
using the weakest inductive bias. We also point out a substantial and
previously unremarked class imbalance in the original RAVEN dataset, and we
propose a new variant of RAVEN -- AB-RAVEN -- that is more balanced in terms of
abstract concepts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11206">Retrieve-Rewrite-Answer: A KG-to-Text Enhanced LLMs Framework for Knowledge Graph Question Answering. (arXiv:2309.11206v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yike Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_N/0/1/0/all/0/1">Nan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bi_S/0/1/0/all/0/1">Sheng Bi</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1">Guilin Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1">Jie Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_A/0/1/0/all/0/1">Anhuan Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1">Wei Song</a></p>
<p>Despite their competitive performance on knowledge-intensive tasks, large
language models (LLMs) still have limitations in memorizing all world knowledge
especially long tail knowledge. In this paper, we study the KG-augmented
language model approach for solving the knowledge graph question answering
(KGQA) task that requires rich world knowledge. Existing work has shown that
retrieving KG knowledge to enhance LLMs prompting can significantly improve
LLMs performance in KGQA. However, their approaches lack a well-formed
verbalization of KG knowledge, i.e., they ignore the gap between KG
representations and textual representations. To this end, we propose an
answer-sensitive KG-to-Text approach that can transform KG knowledge into
well-textualized statements most informative for KGQA. Based on this approach,
we propose a KG-to-Text enhanced LLMs framework for solving the KGQA task.
Experiments on several KGQA benchmarks show that the proposed KG-to-Text
augmented LLMs approach outperforms previous KG-augmented LLMs approaches
regarding answer accuracy and usefulness of knowledge statements.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11331">Gold-YOLO: Efficient Object Detector via Gather-and-Distribute Mechanism. (arXiv:2309.11331v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chengcheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1">Wei He</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1">Ying Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jianyuan Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chuanjian Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1">Kai Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yunhe Wang</a></p>
<p>In the past years, YOLO-series models have emerged as the leading approaches
in the area of real-time object detection. Many studies pushed up the baseline
to a higher level by modifying the architecture, augmenting data and designing
new losses. However, we find previous models still suffer from information
fusion problem, although Feature Pyramid Network (FPN) and Path Aggregation
Network (PANet) have alleviated this. Therefore, this study provides an
advanced Gatherand-Distribute mechanism (GD) mechanism, which is realized with
convolution and self-attention operations. This new designed model named as
Gold-YOLO, which boosts the multi-scale feature fusion capabilities and
achieves an ideal balance between latency and accuracy across all model scales.
Additionally, we implement MAE-style pretraining in the YOLO-series for the
first time, allowing YOLOseries models could be to benefit from unsupervised
pretraining. Gold-YOLO-N attains an outstanding 39.9% AP on the COCO val2017
datasets and 1030 FPS on a T4 GPU, which outperforms the previous SOTA model
YOLOv6-3.0-N with similar FPS by +2.4%. The PyTorch code is available at
https://github.com/huawei-noah/Efficient-Computing/tree/master/Detection/Gold-YOLO,
and the MindSpore code is available at
https://gitee.com/mindspore/models/tree/master/research/cv/Gold_YOLO.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11436">You Only Look at Screens: Multimodal Chain-of-Action Agents. (arXiv:2309.11436v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhuosheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1">Aston Zhang</a></p>
<p>Autonomous user interface (UI) agents aim to facilitate task automation by
interacting with the user interface without manual intervention. Recent studies
have investigated eliciting the capabilities of large language models (LLMs)
for effective engagement in diverse environments. To align with the
input-output requirement of LLMs, existing approaches are developed under a
sandbox setting where they rely on external tools and application-specific APIs
to parse the environment into textual elements and interpret the predicted
actions. Consequently, those approaches often grapple with inference
inefficiency and error propagation risks. To mitigate the challenges, we
introduce Auto-UI, a multimodal solution that directly interacts with the
interface, bypassing the need for environment parsing or reliance on
application-dependent APIs. Moreover, we propose a chain-of-action technique --
leveraging a series of intermediate previous action histories and future action
plans -- to help the agent decide what action to execute. We evaluate our
approach on a new device-control benchmark AITW with 30K unique instructions,
spanning multi-step tasks such as application operation, web searching, and web
shopping. Experimental results show that Auto-UI achieves state-of-the-art
performance with an action type prediction accuracy of 90% and an overall
action success rate of 74%. Code is publicly available at
https://github.com/cooelf/Auto-UI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11489">Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning. (arXiv:2309.11489v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1">Tianbao Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1">Siheng Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1">Chen Henry Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yitao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Q/0/1/0/all/0/1">Qian Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_V/0/1/0/all/0/1">Victor Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yanchao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1">Tao Yu</a></p>
<p>Designing reward functions is a longstanding challenge in reinforcement
learning (RL); it requires specialized knowledge or domain data, leading to
high costs for development. To address this, we introduce Text2Reward, a
data-free framework that automates the generation of dense reward functions
based on large language models (LLMs). Given a goal described in natural
language, Text2Reward generates dense reward functions as an executable program
grounded in a compact representation of the environment. Unlike inverse RL and
recent work that uses LLMs to write sparse reward codes, Text2Reward produces
interpretable, free-form dense reward codes that cover a wide range of tasks,
utilize existing packages, and allow iterative refinement with human feedback.
We evaluate Text2Reward on two robotic manipulation benchmarks (ManiSkill2,
MetaWorld) and two locomotion environments of MuJoCo. On 13 of the 17
manipulation tasks, policies trained with generated reward codes achieve
similar or better task success rates and convergence speed than expert-written
reward codes. For locomotion tasks, our method learns six novel locomotion
behaviors with a success rate exceeding 94%. Furthermore, we show that the
policies trained in the simulator with our method can be deployed in the real
world. Finally, Text2Reward further improves the policies by refining their
reward functions with human feedback. Video results are available at
https://text-to-reward.github.io
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11132">Contrastive Pseudo Learning for Open-World DeepFake Attribution. (arXiv:2309.11132v1 [cs.CV] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1">Zhimin Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1">Taiping Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1">Bangjie Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_R/0/1/0/all/0/1">Ran Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1">Shouhong Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1">Lizhuang Ma</a></p>
<p>The challenge in sourcing attribution for forgery faces has gained widespread
attention due to the rapid development of generative techniques. While many
recent works have taken essential steps on GAN-generated faces, more
threatening attacks related to identity swapping or expression transferring are
still overlooked. And the forgery traces hidden in unknown attacks from the
open-world unlabeled faces still remain under-explored. To push the related
frontier research, we introduce a new benchmark called Open-World DeepFake
Attribution (OW-DFA), which aims to evaluate attribution performance against
various types of fake faces under open-world scenarios. Meanwhile, we propose a
novel framework named Contrastive Pseudo Learning (CPL) for the OW-DFA task
through 1) introducing a Global-Local Voting module to guide the feature
alignment of forged faces with different manipulated regions, 2) designing a
Confidence-based Soft Pseudo-label strategy to mitigate the pseudo-noise caused
by similar methods in unlabeled set. In addition, we extend the CPL framework
with a multi-stage paradigm that leverages pre-train technique and iterative
learning to further enhance traceability performance. Extensive experiments
verify the superiority of our proposed method on the OW-DFA and also
demonstrate the interpretability of deepfake attribution task and its impact on
improving the security of deepfake detection area.
</p>
</p>
</div>

    </div>
    </body>
    