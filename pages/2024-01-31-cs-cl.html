<!DOCTYPE html>
<html>
<head>
<title>2024-01-31-cs-cl</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2401.15170">Scalable Qualitative Coding with LLMs: Chain-of-Thought Reasoning Matches Human Performance in Some Hermeneutic Tasks. (arXiv:2401.15170v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dunivin_Z/0/1/0/all/0/1">Zackary Okun Dunivin</a></p>
<p>Qualitative coding, or content analysis, extracts meaning from text to
discern quantitative patterns across a corpus of texts. Recently, advances in
the interpretive abilities of large language models (LLMs) offer potential for
automating the coding process (applying category labels to texts), thereby
enabling human researchers to concentrate on more creative research aspects,
while delegating these interpretive tasks to AI. Our case study comprises a set
of socio-historical codes on dense, paragraph-long passages representative of a
humanistic study. We show that GPT-4 is capable of human-equivalent
interpretations, whereas GPT-3.5 is not. Compared to our human-derived gold
standard, GPT-4 delivers excellent intercoder reliability (Cohen's $\kappa \geq
0.79$) for 3 of 9 codes, and substantial reliability ($\kappa \geq 0.6$) for 8
of 9 codes. In contrast, GPT-3.5 greatly underperforms for all codes
($mean(\kappa) = 0.34$; $max(\kappa) = 0.55$). Importantly, we find that coding
fidelity improves considerably when the LLM is prompted to give rationale
justifying its coding decisions (chain-of-thought reasoning). We present these
and other findings along with a set of best practices for adapting traditional
codebooks for LLMs. Our results indicate that for certain codebooks,
state-of-the-art LLMs are already adept at large-scale content analysis.
Furthermore, they suggest the next generation of models will likely render AI
coding a viable option for a majority of codebooks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15222">Transfer Learning for the Prediction of Entity Modifiers in Clinical Text: Application to Opioid Use Disorder Case Detection. (arXiv:2401.15222v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Almudaifer_A/0/1/0/all/0/1">Abdullateef I. Almudaifer</a>, <a href="http://arxiv.org/find/cs/1/au:+O%60Leary_T/0/1/0/all/0/1">Tobias O`Leary</a>, <a href="http://arxiv.org/find/cs/1/au:+Covington_W/0/1/0/all/0/1">Whitney Covington</a>, <a href="http://arxiv.org/find/cs/1/au:+Hairston_J/0/1/0/all/0/1">JaMor Hairston</a>, <a href="http://arxiv.org/find/cs/1/au:+Deitch_Z/0/1/0/all/0/1">Zachary Deitch</a>, <a href="http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1">Ankit Anand</a>, <a href="http://arxiv.org/find/cs/1/au:+Carroll_C/0/1/0/all/0/1">Caleb M. Carroll</a>, <a href="http://arxiv.org/find/cs/1/au:+Crisan_E/0/1/0/all/0/1">Estera Crisan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bradford_W/0/1/0/all/0/1">William Bradford</a>, <a href="http://arxiv.org/find/cs/1/au:+Walter_L/0/1/0/all/0/1">Lauren Walter</a>, <a href="http://arxiv.org/find/cs/1/au:+Ellen_E/0/1/0/all/0/1">Eaton Ellen</a>, <a href="http://arxiv.org/find/cs/1/au:+Feldman_S/0/1/0/all/0/1">Sue S. Feldman</a>, <a href="http://arxiv.org/find/cs/1/au:+Osborne_J/0/1/0/all/0/1">John D. Osborne</a></p>
<p>Background: The semantics of entities extracted from a clinical text can be
dramatically altered by modifiers, including entity negation, uncertainty,
conditionality, severity, and subject. Existing models for determining
modifiers of clinical entities involve regular expression or features weights
that are trained independently for each modifier.
</p>
<p>Methods: We develop and evaluate a multi-task transformer architecture design
where modifiers are learned and predicted jointly using the publicly available
SemEval 2015 Task 14 corpus and a new Opioid Use Disorder (OUD) data set that
contains modifiers shared with SemEval as well as novel modifiers specific for
OUD. We evaluate the effectiveness of our multi-task learning approach versus
previously published systems and assess the feasibility of transfer learning
for clinical entity modifiers when only a portion of clinical modifiers are
shared.
</p>
<p>Results: Our approach achieved state-of-the-art results on the ShARe corpus
from SemEval 2015 Task 14, showing an increase of 1.1% on weighted accuracy,
1.7% on unweighted accuracy, and 10% on micro F1 scores.
</p>
<p>Conclusions: We show that learned weights from our shared model can be
effectively transferred to a new partially matched data set, validating the use
of transfer learning for clinical text modifiers
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15241">Unlearning Reveals the Influential Training Data of Language Models. (arXiv:2401.15241v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Isonuma_M/0/1/0/all/0/1">Masaru Isonuma</a>, <a href="http://arxiv.org/find/cs/1/au:+Titov_I/0/1/0/all/0/1">Ivan Titov</a></p>
<p>In order to enhance the performance of language models while mitigating the
risks of generating harmful content, it is crucial to identify which training
dataset affects the model's outputs. Ideally, we can measure the influence of
each dataset by removing it from training; however, it is prohibitively
expensive to retrain a model multiple times. This paper presents UnTrac, which
estimates the influence of a training dataset by unlearning it from the trained
model. UnTrac is extremely simple; each training dataset is unlearned by
gradient ascent, and we evaluate how much the model's predictions change after
unlearning. We empirically examine if our methods can assess the influence of
pretraining datasets on generating toxic, biased, and untruthful content.
Experimental results demonstrate that our method estimates their influence much
more accurately than existing methods while requiring neither excessive memory
space nor multiple model checkpoints.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15269">Improving Medical Reasoning through Retrieval and Self-Reflection with Retrieval-Augmented Large Language Models. (arXiv:2401.15269v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jeong_M/0/1/0/all/0/1">Minbyul Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Sohn_J/0/1/0/all/0/1">Jiwoong Sohn</a>, <a href="http://arxiv.org/find/cs/1/au:+Sung_M/0/1/0/all/0/1">Mujeen Sung</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1">Jaewoo Kang</a></p>
<p>Recent proprietary large language models (LLMs), such as GPT-4, have achieved
a milestone in tackling diverse challenges in the biomedical domain, ranging
from multiple-choice questions to long-form generations. To address challenges
that still cannot be handled with the encoded knowledge of LLMs, various
retrieval-augmented generation (RAG) methods have been developed by searching
documents from the knowledge corpus and appending them unconditionally or
selectively to the input of LLMs for generation. However, when applying
existing methods to different domain-specific problems, poor generalization
becomes apparent, leading to fetching incorrect documents or making inaccurate
judgments. In this paper, we introduce Self-BioRAG, a framework reliable for
biomedical text that specializes in generating explanations, retrieving
domain-specific documents, and self-reflecting generated responses. We utilize
84k filtered biomedical instruction sets to train Self-BioRAG that can assess
its generated explanations with customized reflective tokens. Our work proves
that domain-specific components, such as a retriever, domain-related document
corpus, and instruction sets are necessary for adhering to domain-related
instructions. Using three major medical question-answering benchmark datasets,
experimental results of Self-BioRAG demonstrate significant performance gains
by achieving a 7.2% absolute improvement on average over the state-of-the-art
open-foundation model with a parameter size of 7B or less. Overall, we analyze
that Self-BioRAG finds the clues in the question, retrieves relevant documents
if needed, and understands how to answer with information from retrieved
documents and encoded knowledge as a medical expert does. We release our data
and code for training our framework components and model weights (7B and 13B)
to enhance capabilities in biomedical and clinical domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15312">How We Refute Claims: Automatic Fact-Checking through Flaw Identification and Explanation. (arXiv:2401.15312v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kao_W/0/1/0/all/0/1">Wei-Yu Kao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yen_A/0/1/0/all/0/1">An-Zi Yen</a></p>
<p>Automated fact-checking is a crucial task in the governance of internet
content. Although various studies utilize advanced models to tackle this issue,
a significant gap persists in addressing complex real-world rumors and
deceptive claims. To address this challenge, this paper explores the novel task
of flaw-oriented fact-checking, including aspect generation and flaw
identification. We also introduce RefuteClaim, a new framework designed
specifically for this task. Given the absence of an existing dataset, we
present FlawCheck, a dataset created by extracting and transforming insights
from expert reviews into relevant aspects and identified flaws. The
experimental results underscore the efficacy of RefuteClaim, particularly in
classifying and elucidating false claims.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15316">UNSEE: Unsupervised Non-contrastive Sentence Embeddings. (arXiv:2401.15316v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cagatan_O/0/1/0/all/0/1">&#xd6;mer Veysel &#xc7;a&#x11f;atan</a></p>
<p>We present UNSEE: Unsupervised Non-Contrastive Sentence Embeddings, a novel
approach that outperforms SimCSE in the Massive Text Embedding benchmark. Our
exploration begins by addressing the challenge of representation collapse, a
phenomenon observed when contrastive objectives in SimCSE are replaced with
non-contrastive objectives. To counter this issue, we propose a straightforward
solution known as the target network, effectively mitigating representation
collapse. The introduction of the target network allows us to leverage
non-contrastive objectives, maintaining training stability while achieving
performance improvements comparable to contrastive objectives. Our method has
achieved peak performance in non-contrastive sentence embeddings through
meticulous fine-tuning and optimization. This comprehensive effort has yielded
superior sentence representation models, showcasing the effectiveness of our
approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15328">Equipping Language Models with Tool Use Capability for Tabular Data Analysis in Finance. (arXiv:2401.15328v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Theuma_A/0/1/0/all/0/1">Adrian Theuma</a>, <a href="http://arxiv.org/find/cs/1/au:+Shareghi_E/0/1/0/all/0/1">Ehsan Shareghi</a></p>
<p>Large language models (LLMs) have exhibited an array of reasoning
capabilities but face challenges like error propagation and hallucination,
particularly in specialised areas like finance, where data is heterogeneous,
and precision is paramount. We explore the potential of language model
augmentation with external tools to mitigate these limitations and offload
certain reasoning steps to external tools that are more suited for the task,
instead of solely depending on the LLM's inherent abilities. More concretely,
using financial domain question-answering datasets, we apply supervised
fine-tuning on a LLaMA-2 13B Chat model to act both as a 'task router' and
'task solver'. The 'task router' dynamically directs a question to either be
answered internally by the LLM or externally via the right tool from the tool
set. Our tool-equipped SFT model, Raven, demonstrates an improvement of 35.2%
and 5.06% over the base model and SFT-only baselines, respectively, and is
highly competitive with strong GPT-3.5 results. To the best of our knowledge,
our work is the first that investigates tool augmentation of language models
for the finance domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15347">A Comprehensive Survey of Compression Algorithms for Language Models. (arXiv:2401.15347v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1">Seungcheol Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1">Jaehyeon Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Sojin Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_U/0/1/0/all/0/1">U Kang</a></p>
<p>How can we compress language models without sacrificing accuracy? The number
of compression algorithms for language models is rapidly growing to benefit
from remarkable advances of recent language models without side effects due to
the gigantic size of language models, such as increased carbon emissions and
expensive maintenance fees. While numerous compression algorithms have shown
remarkable progress in compressing language models, it ironically becomes
challenging to capture emerging trends and identify the fundamental concepts
underlying them due to the excessive number of algorithms. In this paper, we
survey and summarize diverse compression algorithms including pruning,
quantization, knowledge distillation, low-rank approximation, parameter
sharing, and efficient architecture design. We not only summarize the overall
trend of diverse compression algorithms but also select representative
algorithms and provide in-depth analyses of them. We discuss the value of each
category of compression algorithms, and the desired properties of low-cost
compression algorithms which have a significant impact due to the emergence of
large language models. Finally, we introduce promising future research topics
based on our survey results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15351">A Survey on Neural Topic Models: Methods, Applications, and Challenges. (arXiv:2401.15351v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xiaobao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Thong Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Luu_A/0/1/0/all/0/1">Anh Tuan Luu</a></p>
<p>Topic models have been prevalent for decades to discover latent topics and
infer topic proportions of documents in an unsupervised fashion. They have been
widely used in various applications like text analysis and context
recommendation. Recently, the rise of neural networks has facilitated the
emergence of a new research field -- Neural Topic Models (NTMs). Different from
conventional topic models, NTMs directly optimize parameters without requiring
model-specific derivations. This endows NTMs with better scalability and
flexibility, resulting in significant research attention and plentiful new
methods and applications. In this paper, we present a comprehensive survey on
neural topic models concerning methods, applications, and challenges.
Specifically, we systematically organize current NTM methods according to their
network structures and introduce the NTMs for various scenarios like short
texts and cross-lingual documents. We also discuss a wide range of popular
applications built on NTMs. Finally, we highlight the challenges confronted by
NTMs to inspire future research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15360">Importance-Aware Data Augmentation for Document-Level Neural Machine Translation. (arXiv:2401.15360v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1">Minghao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yufei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Foster_G/0/1/0/all/0/1">George Foster</a>, <a href="http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1">Lizhen Qu</a>, <a href="http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1">Gholamreza Haffari</a></p>
<p>Document-level neural machine translation (DocNMT) aims to generate
translations that are both coherent and cohesive, in contrast to its
sentence-level counterpart. However, due to its longer input length and limited
availability of training data, DocNMT often faces the challenge of data
sparsity. To overcome this issue, we propose a novel Importance-Aware Data
Augmentation (IADA) algorithm for DocNMT that augments the training data based
on token importance information estimated by the norm of hidden states and
training gradients. We conduct comprehensive experiments on three widely-used
DocNMT benchmarks. Our empirical results show that our proposed IADA
outperforms strong DocNMT baselines as well as several data augmentation
approaches, with statistical significance on both sentence-level and
document-level BLEU.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15371">LegalDuet: Learning Effective Representations for Legal Judgment Prediction through a Dual-View Legal Clue Reasoning. (arXiv:2401.15371v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1">Pengjie Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhenghao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1">Xiaoyuan Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Liner Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1">Yu Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1">Ge Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xing Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Shuang-hua Yang</a></p>
<p>Most existing Legal Judgment Prediction (LJP) models focus on discovering the
legal triggers in the criminal fact description. However, in real-world
scenarios, a professional judge not only needs to assimilate the law case
experience that thrives on past sentenced legal judgments but also depends on
the professional legal grounded reasoning that learned from professional legal
knowledge. In this paper, we propose a LegalDuet model, which pretrains
language models to learn a tailored embedding space for making legal judgments.
It proposes a dual-view legal clue reasoning mechanism, which derives from two
reasoning chains of judges: 1) Law Case Reasoning, which makes legal judgments
according to the judgment experiences learned from analogy/confusing legal
cases; 2) Legal Ground Reasoning, which lies in matching the legal clues
between criminal cases and legal decisions. Our experiments show that LegalDuet
achieves state-of-the-art performance on the CAIL2018 dataset and outperforms
baselines with about 4% improvements on average. Our dual-view reasoning based
pretraining can capture critical legal clues to learn a tailored embedding
space to distinguish criminal cases. It reduces LegalDuet's uncertainty during
prediction and brings pretraining advances to the confusing/low frequent
charges. All codes are available at https://github.com/NEUIR/LegalDuet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15378">A RAG-based Question Answering System Proposal for Understanding Islam: MufassirQAS LLM. (arXiv:2401.15378v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alan_A/0/1/0/all/0/1">Ahmet Yusuf Alan</a>, <a href="http://arxiv.org/find/cs/1/au:+Karaarslan_E/0/1/0/all/0/1">Enis Karaarslan</a>, <a href="http://arxiv.org/find/cs/1/au:+Aydin_O/0/1/0/all/0/1">Omer Aydin</a></p>
<p>There exist challenges in learning and understanding religions as the
presence of complexity and depth of religious doctrines and teachings. Chatbots
as question-answering systems can help in solving these challenges. LLM
chatbots use NLP techniques to establish connections between topics and
accurately respond to complex questions. These capabilities make it perfect to
be used in enlightenment on religion as a question answering chatbot. However,
LLMs also have a tendency to generate false information, known as
hallucination. The responses of the chatbots can include content that insults
personal religious beliefs, interfaith conflicts, and controversial or
sensitive topics. It needs to avoid such cases without promoting hate speech or
offending certain groups of people or their beliefs. This study uses a vector
database-based Retrieval Augmented Generation (RAG) approach to enhance the
accuracy and transparency of LLMs. Our question-answering system is called as
"MufassirQAS". We created a vector database with several open-access books that
include Turkish context. These are Turkish translations, and interpretations on
Islam. We worked on creating system prompts with care, ensuring they provide
instructions that prevent harmful, offensive, or disrespectful responses. We
also tested the MufassirQAS and ChatGPT with sensitive questions. We got better
performance with our system. Study and enhancements are still in progress.
Results and future works are given.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15385">Towards Event Extraction from Speech with Contextual Clues. (arXiv:2401.15385v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1">Jingqi Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1">Tongtong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jinming Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guitao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1">Guilin Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuan-Fang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1">Gholamreza Haffari</a></p>
<p>While text-based event extraction has been an active research area and has
seen successful application in many domains, extracting semantic events from
speech directly is an under-explored problem. In this paper, we introduce the
Speech Event Extraction (SpeechEE) task and construct three synthetic training
sets and one human-spoken test set. Compared to event extraction from text,
SpeechEE poses greater challenges mainly due to complex speech signals that are
continuous and have no word boundaries. Additionally, unlike perceptible sound
events, semantic events are more subtle and require a deeper understanding. To
tackle these challenges, we introduce a sequence-to-structure generation
paradigm that can produce events from speech signals in an end-to-end manner,
together with a conditioned generation method that utilizes speech recognition
transcripts as the contextual clue. We further propose to represent events with
a flat format to make outputs more natural language-like. Our experimental
results show that our method brings significant improvements on all datasets,
achieving a maximum F1 gain of 10.7%. The code and datasets are released on
https://github.com/jodie-kang/SpeechEE.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15391">MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries. (arXiv:2401.15391v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1">Yixuan Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yi Yang</a></p>
<p>Retrieval-augmented generation (RAG) augments large language models (LLM) by
retrieving relevant knowledge, showing promising potential in mitigating LLM
hallucinations and enhancing response quality, thereby facilitating the great
adoption of LLMs in practice. However, we find that existing RAG systems are
inadequate in answering multi-hop queries, which require retrieving and
reasoning over multiple pieces of supporting evidence. Furthermore, to our
knowledge, no existing RAG benchmarking dataset focuses on multi-hop queries.
In this paper, we develop a novel dataset, MultiHop-RAG, which consists of a
knowledge base, a large collection of multi-hop queries, their ground-truth
answers, and the associated supporting evidence. We detail the procedure of
building the dataset, utilizing an English news article dataset as the
underlying RAG knowledge base. We demonstrate the benchmarking utility of
MultiHop-RAG in two experiments. The first experiment compares different
embedding models for retrieving evidence for multi-hop queries. In the second
experiment, we examine the capabilities of various state-of-the-art LLMs,
including GPT-4, PaLM, and Llama2-70B, in reasoning and answering multi-hop
queries given the evidence. Both experiments reveal that existing RAG methods
perform unsatisfactorily in retrieving and answering multi-hop queries. We hope
MultiHop-RAG will be a valuable resource for the community in developing
effective RAG systems, thereby facilitating greater adoption of LLMs in
practice. The MultiHop-RAG and implemented RAG system is publicly available at
https://github.com/yixuantt/MultiHop-RAG/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15393">Semantics of Multiword Expressions in Transformer-Based Models: A Survey. (arXiv:2401.15393v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Miletic_F/0/1/0/all/0/1">Filip Mileti&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Walde_S/0/1/0/all/0/1">Sabine Schulte im Walde</a></p>
<p>Multiword expressions (MWEs) are composed of multiple words and exhibit
variable degrees of compositionality. As such, their meanings are notoriously
difficult to model, and it is unclear to what extent this issue affects
transformer architectures. Addressing this gap, we provide the first in-depth
survey of MWE processing with transformer models. We overall find that they
capture MWE semantics inconsistently, as shown by reliance on surface patterns
and memorized information. MWE meaning is also strongly localized,
predominantly in early layers of the architecture. Representations benefit from
specific linguistic properties, such as lower semantic idiosyncrasy and
ambiguity of target expressions. Our findings overall question the ability of
transformer models to robustly capture fine-grained semantics. Furthermore, we
highlight the need for more directly comparable evaluation setups.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15400">Indexing Portuguese NLP Resources with PT-Pump-Up. (arXiv:2401.15400v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Almeida_R/0/1/0/all/0/1">R&#xfa;ben Almeida</a>, <a href="http://arxiv.org/find/cs/1/au:+Campos_R/0/1/0/all/0/1">Ricardo Campos</a>, <a href="http://arxiv.org/find/cs/1/au:+Jorge_A/0/1/0/all/0/1">Al&#xed;pio Jorge</a>, <a href="http://arxiv.org/find/cs/1/au:+Nunes_S/0/1/0/all/0/1">S&#xe9;rgio Nunes</a></p>
<p>The recent advances in natural language processing (NLP) are linked to
training processes that require vast amounts of corpora. Access to this data is
commonly not a trivial process due to resource dispersion and the need to
maintain these infrastructures online and up-to-date. New developments in NLP
are often compromised due to the scarcity of data or lack of a shared
repository that works as an entry point to the community. This is especially
true in low and mid-resource languages, such as Portuguese, which lack data and
proper resource management infrastructures. In this work, we propose
PT-Pump-Up, a set of tools that aim to reduce resource dispersion and improve
the accessibility to Portuguese NLP resources. Our proposal is divided into
four software components: a) a web platform to list the available resources; b)
a client-side Python package to simplify the loading of Portuguese NLP
resources; c) an administrative Python package to manage the platform and d) a
public GitHub repository to foster future collaboration and contributions. All
four components are accessible using: https://linktr.ee/pt_pump_up
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15422">A Survey on Data Augmentation in Large Model Era. (arXiv:2401.15422v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yue Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1">Chenlu Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1">Yi Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yuan Wu</a></p>
<p>Large models, encompassing large language and diffusion models, have shown
exceptional promise in approximating human-level intelligence, garnering
significant interest from both academic and industrial spheres. However, the
training of these large models necessitates vast quantities of high-quality
data, and with continuous updates to these models, the existing reservoir of
high-quality data may soon be depleted. This challenge has catalyzed a surge in
research focused on data augmentation methods. Leveraging large models, these
data augmentation techniques have outperformed traditional approaches. This
paper offers an exhaustive review of large model-driven data augmentation
methods, adopting a comprehensive perspective. We begin by establishing a
classification of relevant studies into three main categories: image
augmentation, text augmentation, and paired data augmentation. Following this,
we delve into various data post-processing techniques pertinent to large
model-based data augmentation. Our discussion then expands to encompass the
array of applications for these data augmentation methods within natural
language processing, computer vision, and audio signal processing. We proceed
to evaluate the successes and limitations of large model-based data
augmentation across different scenarios. Concluding our review, we highlight
prospective challenges and avenues for future exploration in the field of data
augmentation. Our objective is to furnish researchers with critical insights,
ultimately contributing to the advancement of more sophisticated large models.
We consistently maintain the related open-source materials at:
https://github.com/MLGroup-JLU/LLM-data-aug-survey.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15439">Pre-training and Diagnosing Knowledge Base Completion Models. (arXiv:2401.15439v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kocijan_V/0/1/0/all/0/1">Vid Kocijan</a>, <a href="http://arxiv.org/find/cs/1/au:+Jang_M/0/1/0/all/0/1">Myeongjun Erik Jang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1">Thomas Lukasiewicz</a></p>
<p>In this work, we introduce and analyze an approach to knowledge transfer from
one collection of facts to another without the need for entity or relation
matching. The method works for both canonicalized knowledge bases and
uncanonicalized or open knowledge bases, i.e., knowledge bases where more than
one copy of a real-world entity or relation may exist. The main contribution is
a method that can make use of large-scale pre-training on facts, which were
collected from unstructured text, to improve predictions on structured data
from a specific domain. The introduced method is most impactful on small
datasets such as ReVerb20k, where a 6% absolute increase of mean reciprocal
rank and 65% relative decrease of mean rank over the previously best method was
achieved, despite not relying on large pre-trained models like Bert. To
understand the obtained pre-trained models better, we then introduce a novel
dataset for the analysis of pre-trained models for Open Knowledge Base
Completion, called Doge (Diagnostics of Open knowledge Graph Embeddings). It
consists of 6 subsets and is designed to measure multiple properties of a
pre-trained model: robustness against synonyms, ability to perform deductive
reasoning, presence of gender stereotypes, consistency with reverse relations,
and coverage of different areas of general knowledge. Using the introduced
dataset, we show that the existing OKBC models lack consistency in the presence
of synonyms and inverse relations and are unable to perform deductive
reasoning. Moreover, their predictions often align with gender stereotypes,
which persist even when presented with counterevidence. We additionally
investigate the role of pre-trained word embeddings and demonstrate that
avoiding biased word embeddings is not a sufficient measure to prevent biased
behavior of OKBC models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15449">Learning to Trust Your Feelings: Leveraging Self-awareness in LLMs for Hallucination Mitigation. (arXiv:2401.15449v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1">Yuxin Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1">Zhuoyang Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiaxing Zhang</a></p>
<p>We evaluate the ability of Large Language Models (LLMs) to discern and
express their internal knowledge state, a key factor in countering factual
hallucination and ensuring reliable application of LLMs. We observe a robust
self-awareness of internal knowledge state in LLMs, evidenced by over 85%
accuracy in knowledge probing. However, LLMs often fail to express their
internal knowledge during generation, leading to factual hallucinations. We
develop an automated hallucination annotation tool, Dreamcatcher, which merges
knowledge probing and consistency checking methods to rank factual preference
data. Using knowledge preference as reward, We propose a Reinforcement Learning
from Knowledge Feedback (RLKF) training framework, leveraging reinforcement
learning to enhance the factuality and honesty of LLMs. Our experiments across
multiple models show that RLKF training effectively enhances the ability of
models to utilize their internal knowledge state, boosting performance in a
variety of knowledge-based and honesty-related tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.08565">Geographic Adaptation of Pretrained Language Models. (arXiv:2203.08565v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hofmann_V/0/1/0/all/0/1">Valentin Hofmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1">Goran Glava&#x161;</a>, <a href="http://arxiv.org/find/cs/1/au:+Ljubesic_N/0/1/0/all/0/1">Nikola Ljube&#x161;i&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Pierrehumbert_J/0/1/0/all/0/1">Janet B. Pierrehumbert</a>, <a href="http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1">Hinrich Sch&#xfc;tze</a></p>
<p>While pretrained language models (PLMs) have been shown to possess a plethora
of linguistic knowledge, the existing body of research has largely neglected
extralinguistic knowledge, which is generally difficult to obtain by
pretraining on text alone. Here, we contribute to closing this gap by examining
geolinguistic knowledge, i.e., knowledge about geographic variation in
language. We introduce geoadaptation, an intermediate training step that
couples language modeling with geolocation prediction in a multi-task learning
setup. We geoadapt four PLMs, covering language groups from three geographic
areas, and evaluate them on five different tasks: fine-tuned (i.e., supervised)
geolocation prediction, zero-shot (i.e., unsupervised) geolocation prediction,
fine-tuned language identification, zero-shot language identification, and
zero-shot prediction of dialect features. Geoadaptation is very successful at
injecting geolinguistic knowledge into the PLMs: the geoadapted PLMs
consistently outperform PLMs adapted using only language modeling (by
especially wide margins on zero-shot prediction tasks), and we obtain new
state-of-the-art results on two benchmarks for geolocation prediction and
language identification. Furthermore, we show that the effectiveness of
geoadaptation stems from its ability to geographically retrofit the
representation space of the PLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.14570">MiniDisc: Minimal Distillation Schedule for Language Model Compression. (arXiv:2205.14570v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qifan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiahao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jingang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1">Wei Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1">Dawei Song</a></p>
<p>Recent studies have uncovered that language model distillation is less
effective when facing a large capacity gap between the teacher and the student,
and introduced teacher assistant-based distillation to bridge the gap. As a
connection, the scale and the performance of the teacher assistant is of vital
importance to bring the knowledge from the teacher to the student. However,
existing teacher assistant-based methods require maximally many trials before
scheduling an optimal teacher assistant. To this end, we propose a minimal
distillation schedule (MiniDisc) for scheduling the optimal teacher assistant
in minimally one trial. In particular, motivated by the finding that the
performance of the student is positively correlated to the scale-performance
tradeoff of the teacher assistant, MiniDisc is designed with a
$\lambda$-tradeoff to measure the optimality of the teacher assistant without
trial distillation to the student. MiniDisc then can schedule the optimal
teacher assistant with the best $\lambda$-tradeoff in a sandwich framework.
MiniDisc is evaluated with an extensive set of experiments on GLUE.
Experimental results demonstrate the improved efficiency our MiniDisc compared
to several state-of-the-art baselines. We further apply MiniDisc to a language
model with billions of parameters and show its scalability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.01079">DiSCoMaT: Distantly Supervised Composition Extraction from Tables in Materials Science Articles. (arXiv:2207.01079v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gupta_T/0/1/0/all/0/1">Tanishq Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Zaki_M/0/1/0/all/0/1">Mohd Zaki</a>, <a href="http://arxiv.org/find/cs/1/au:+Khatsuriya_D/0/1/0/all/0/1">Devanshi Khatsuriya</a>, <a href="http://arxiv.org/find/cs/1/au:+Hira_K/0/1/0/all/0/1">Kausik Hira</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishnan_N/0/1/0/all/0/1">N. M. Anoop Krishnan</a>, <a href="http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1">Mausam</a></p>
<p>A crucial component in the curation of KB for a scientific domain (e.g.,
materials science, foods &amp; nutrition, fuels) is information extraction from
tables in the domain's published research articles. To facilitate research in
this direction, we define a novel NLP task of extracting compositions of
materials (e.g., glasses) from tables in materials science papers. The task
involves solving several challenges in concert, such as tables that mention
compositions have highly varying structures; text in captions and full paper
needs to be incorporated along with data in tables; and regular languages for
numbers, chemical compounds and composition expressions must be integrated into
the model. We release a training dataset comprising 4,408 distantly supervised
tables, along with 1,475 manually annotated dev and test tables. We also
present a strong baseline DISCOMAT, that combines multiple graph neural
networks with several task-specific regular expressions, features, and
constraints. We show that DISCOMAT outperforms recent table processing
architectures by significant margins.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.07661">On the Relation between Sensitivity and Accuracy in In-context Learning. (arXiv:2209.07661v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yanda Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1">Chen Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Zhou Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1">Kathleen McKeown</a>, <a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1">He He</a></p>
<p>In-context learning (ICL) suffers from oversensitivity to the prompt, making
it unreliable in real-world scenarios. We study the sensitivity of ICL with
respect to multiple perturbation types. First, we find that label bias obscures
the true sensitivity, and therefore prior work may have significantly
underestimated ICL sensitivity. Second, we observe a strong negative
correlation between ICL sensitivity and accuracy: predictions sensitive to
perturbations are less likely to be correct. Motivated by these findings, we
propose \textsc{SenSel}, a few-shot selective prediction method that abstains
from sensitive predictions. Experiments on ten classification datasets show
that \textsc{SenSel} consistently outperforms two commonly used
confidence-based and entropy-based baselines on abstention decisions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.12273">Graphemic Normalization of the Perso-Arabic Script. (arXiv:2210.12273v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Doctor_R/0/1/0/all/0/1">Raiomond Doctor</a>, <a href="http://arxiv.org/find/cs/1/au:+Gutkin_A/0/1/0/all/0/1">Alexander Gutkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Johny_C/0/1/0/all/0/1">Cibu Johny</a>, <a href="http://arxiv.org/find/cs/1/au:+Roark_B/0/1/0/all/0/1">Brian Roark</a>, <a href="http://arxiv.org/find/cs/1/au:+Sproat_R/0/1/0/all/0/1">Richard Sproat</a></p>
<p>Since its original appearance in 1991, the Perso-Arabic script representation
in Unicode has grown from 169 to over 440 atomic isolated characters spread
over several code pages representing standard letters, various diacritics and
punctuation for the original Arabic and numerous other regional orthographic
traditions. This paper documents the challenges that Perso-Arabic presents
beyond the best-documented languages, such as Arabic and Persian, building on
earlier work by the expert community. We particularly focus on the situation in
natural language processing (NLP), which is affected by multiple, often
neglected, issues such as the use of visually ambiguous yet canonically
nonequivalent letters and the mixing of letters from different orthographies.
Among the contributing conflating factors are the lack of input methods, the
instability of modern orthographies, insufficient literacy, and loss or lack of
orthographic tradition. We evaluate the effects of script normalization on
eight languages from diverse language families in the Perso-Arabic script
diaspora on machine translation and statistical language modeling tasks. Our
results indicate statistically significant improvements in performance in most
conditions for all the languages considered when normalization is applied. We
argue that better understanding and representation of Perso-Arabic script
variation within regional orthographic traditions, where those are present, is
crucial for further progress of modern computational NLP techniques especially
for languages with a paucity of resources.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.09949">Compressing Transformer-based self-supervised models for speech processing. (arXiv:2211.09949v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1">Tzu-Quan Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1">Tsung-Huan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1">Chun-Yao Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Kuang-Ming Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_T/0/1/0/all/0/1">Tzu-hsun Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hung-yi Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1">Hao Tang</a></p>
<p>Despite the success of Transformers in self- supervised learning with
applications to various downstream tasks, the computational cost of training
and inference remains a major challenge for applying these models to a wide
spectrum of devices. Several isolated attempts have been made to compress
Transformers, but the settings and metrics are different across studies.
Trade-off at various compression rates are also largely missing in prior work,
making it difficult to compare compression techniques. In this work, we aim to
provide context for the isolated results, studying several commonly used
compression techniques, including weight pruning, head pruning, low-rank
approximation, and knowledge distillation. We report trade- off at various
compression rate, including wall-clock time, the number of parameters, and the
number of multiply-accumulate operations. Our results show that compared to
recent approaches, basic compression techniques are strong baselines. We
further present several applications of our results, revealing properties of
Transformers, such as the significance of diagonal attention heads. In
addition, our results lead to a simple combination of compression techniques
that improves trade-off over recent approaches. We hope the results would
promote more diverse comparisons among model compression techniques and promote
the use of model compression as a tool for analyzing models. Our code of
compressing speech self-supervised model is available at
https://github.com/nervjack2/Speech-SSL-Compression/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.12132">AutoPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning. (arXiv:2301.12132v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Han Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1">Xingchen Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1">Ivan Vuli&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1">Anna Korhonen</a></p>
<p>Large pretrained language models are widely used in downstream NLP tasks via
task-specific fine-tuning, but such procedures can be costly. Recently,
Parameter-Efficient Fine-Tuning (PEFT) methods have achieved strong task
performance while updating much fewer parameters than full model fine-tuning
(FFT). However, it is non-trivial to make informed design choices on the PEFT
configurations, such as their architecture, the number of tunable parameters,
and even the layers in which the PEFT modules are inserted. Consequently, it is
highly likely that the current, manually designed configurations are suboptimal
in terms of their performance-efficiency trade-off. Inspired by advances in
neural architecture search, we propose AutoPEFT for automatic PEFT
configuration selection: we first design an expressive configuration search
space with multiple representative PEFT modules as building blocks. Using
multi-objective Bayesian optimisation in a low-cost setup, we then discover a
Pareto-optimal set of configurations with strong performance-cost trade-offs
across different numbers of parameters that are also highly transferable across
different tasks. Empirically, on GLUE and SuperGLUE tasks, we show that
AutoPEFT-discovered configurations significantly outperform existing PEFT
methods and are on par or better than FFT without incurring substantial
training efficiency costs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.02759">Detecting Reddit Users with Depression Using a Hybrid Neural Network SBERT-CNN. (arXiv:2302.02759v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Ziyi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1">Ren Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1">Sunyang Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zong_N/0/1/0/all/0/1">Nansu Zong</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hongfang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1">Ming Huang</a></p>
<p>Depression is a widespread mental health issue, affecting an estimated 3.8%
of the global population. It is also one of the main contributors to disability
worldwide. Recently it is becoming popular for individuals to use social media
platforms (e.g., Reddit) to express their difficulties and health issues (e.g.,
depression) and seek support from other users in online communities. It opens
great opportunities to automatically identify social media users with
depression by parsing millions of posts for potential interventions. Deep
learning methods have begun to dominate in the field of machine learning and
natural language processing (NLP) because of their ease of use, efficient
processing, and state-of-the-art results on many NLP tasks. In this work, we
propose a hybrid deep learning model which combines a pretrained sentence BERT
(SBERT) and convolutional neural network (CNN) to detect individuals with
depression with their Reddit posts. The sentence BERT is used to learn the
meaningful representation of semantic information in each post. CNN enables the
further transformation of those embeddings and the temporal identification of
behavioral patterns of users. We trained and evaluated the model performance to
identify Reddit users with depression by utilizing the Self-reported Mental
Health Diagnoses (SMHD) data. The hybrid deep learning model achieved an
accuracy of 0.86 and an F1 score of 0.86 and outperformed the state-of-the-art
documented result (F1 score of 0.79) by other machine learning models in the
literature. The results show the feasibility of the hybrid model to identify
individuals with depression. Although the hybrid model is validated to detect
depression with Reddit posts, it can be easily tuned and applied to other text
classification tasks and different clinical applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.04914">Flexible, Model-Agnostic Method for Materials Data Extraction from Text Using General Purpose Language Models. (arXiv:2302.04914v2 [cond-mat.mtrl-sci] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Polak_M/0/1/0/all/0/1">Maciej P. Polak</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Modi_S/0/1/0/all/0/1">Shrey Modi</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Latosinska_A/0/1/0/all/0/1">Anna Latosinska</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Zhang_J/0/1/0/all/0/1">Jinming Zhang</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Wang_C/0/1/0/all/0/1">Ching-Wen Wang</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Wang_S/0/1/0/all/0/1">Shanonan Wang</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Hazra_A/0/1/0/all/0/1">Ayan Deep Hazra</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Morgan_D/0/1/0/all/0/1">Dane Morgan</a></p>
<p>Accurate and comprehensive material databases extracted from research papers
are critical for materials science and engineering but require significant
human effort to develop. In this paper we present a simple method of extracting
materials data from full texts of research papers suitable for quickly
developing modest-sized databases. The method requires minimal to no coding,
prior knowledge about the extracted property, or model training, and provides
high recall and almost perfect precision in the resultant database. The method
is fully automated except for one human-assisted step, which typically requires
just a few hours of human labor. The method builds on top of natural language
processing and large general language models but can work with almost any such
model. The language models GPT-3/3.5, bart and DeBERTaV3 are evaluated here for
comparison. We provide a detailed detailed analysis of the methods performance
in extracting bulk modulus data, obtaining up to 90% precision at 96% recall,
depending on the amount of human effort involved. We then demonstrate the
methods broader effectiveness by developing a database of critical cooling
rates for metallic glasses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.08957">Like a Good Nearest Neighbor: Practical Content Moderation and Text Classification. (arXiv:2302.08957v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bates_L/0/1/0/all/0/1">Luke Bates</a>, <a href="http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1">Iryna Gurevych</a></p>
<p>Few-shot text classification systems have impressive capabilities but are
infeasible to deploy and use reliably due to their dependence on prompting and
billion-parameter language models. SetFit (Tunstall et al., 2022) is a recent,
practical approach that fine-tunes a Sentence Transformer under a contrastive
learning paradigm and achieves similar results to more unwieldy systems.
Inexpensive text classification is important for addressing the problem of
domain drift in all classification tasks, and especially in detecting harmful
content, which plagues social media platforms. Here, we propose Like a Good
Nearest Neighbor (LaGoNN), a modification to SetFit that introduces no
learnable parameters but alters input text with information from its nearest
neighbor, for example, the label and text, in the training data, making novel
data appear similar to an instance on which the model was optimized. LaGoNN is
effective at flagging undesirable content and text classification, and improves
the performance of SetFit. To demonstrate the value of LaGoNN, we conduct a
thorough study of text classification systems in the context of content
moderation under four label distributions, and in general and multilingual
classification settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.01295">Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks using Prompt-Tuning. (arXiv:2304.01295v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tu_L/0/1/0/all/0/1">Lifu Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qu_J/0/1/0/all/0/1">Jin Qu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yavuz_S/0/1/0/all/0/1">Semih Yavuz</a>, <a href="http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1">Shafiq Joty</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wenhao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1">Caiming Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yingbo Zhou</a></p>
<p>Cross-lingual transfer of language models trained on high-resource languages
like English has been widely studied for many NLP tasks, but focus on
conversational tasks has been rather limited. This is partly due to the high
cost of obtaining non-English conversational data, which results in limited
coverage. In this work, we introduce XSGD for cross-lingual alignment
pretraining, a parallel and large-scale multilingual conversation dataset that
we created by translating the English-only Schema-Guided Dialogue (SGD) dataset
(Rastogi et al., 2020) into 105 other languages. XSGD contains approximately
330k utterances per language. To facilitate aligned cross-lingual
representations, we develop an efficient prompt-tuning-based method for
learning alignment prompts. We also investigate two different classifiers:
NLI-based and vanilla classifiers, and test cross-lingual capability enabled by
the aligned prompts. We evaluate our model's cross-lingual generalization
capabilities on two conversation tasks: slot-filling and intent classification.
Our results demonstrate the strong and efficient modeling ability of NLI-based
classifiers and the large cross-lingual transfer improvements achieved by our
aligned prompts, particularly in few-shot settings. In addition, we highlight
the nice results of our approach compared to LLMs such as text-davinci-003 and
ChatGPT in both zero-shot and few-shot settings. While LLMs exhibit impressive
performance in English, their cross-lingual capabilities in other languages,
particularly low-resource languages, are limited.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.14402">LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions. (arXiv:2304.14402v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1">Minghao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Waheed_A/0/1/0/all/0/1">Abdul Waheed</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chiyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1">Muhammad Abdul-Mageed</a>, <a href="http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1">Alham Fikri Aji</a></p>
<p>Large language models (LLMs) with instruction fine-tuning demonstrate
superior generative capabilities. However, these models are resource-intensive.
To alleviate this issue, we explore distilling knowledge from instruction-tuned
LLMs into much smaller ones. To this end, we carefully develop a large set of
2.58M instructions based on both existing and newly-generated instructions. In
addition to being sizable, we design our instructions to cover a broad set of
topics to ensure diversity. Extensive analysis of our instruction dataset
confirms its diversity, and we generate responses for these instructions using
gpt-3.5-turbo. Leveraging these instructions, we fine-tune a diverse herd of
models, collectively referred to as LaMini-LM, which includes models from both
the encoder-decoder and decoder-only families, with varying sizes. We evaluate
the performance of our models using automatic metrics on 15 different natural
language processing (NLP) benchmarks, as well as through human assessment. The
results demonstrate that our proposed LaMini-LM models are comparable to
competitive baselines, while being much smaller in size.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.05644">Towards Building the Federated GPT: Federated Instruction Tuning. (arXiv:2305.05644v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jianyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Vahidian_S/0/1/0/all/0/1">Saeed Vahidian</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuo_M/0/1/0/all/0/1">Martin Kuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chunyuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Ruiyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1">Tong Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yufan Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guoyin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yiran Chen</a></p>
<p>While "instruction-tuned" generative large language models (LLMs) have
demonstrated an impressive ability to generalize to new tasks, the training
phases heavily rely on large amounts of diverse and high-quality instruction
data (such as ChatGPT and GPT-4). Unfortunately, acquiring high-quality data,
especially when it comes to human-written data, can pose significant challenges
both in terms of cost and accessibility. Moreover, concerns related to privacy
can further limit access to such data, making the process of obtaining it a
complex and nuanced undertaking. Consequently, this hinders the generality of
the tuned models and may restrict their effectiveness in certain contexts. To
tackle this issue, our study introduces a new approach called Federated
Instruction Tuning (FedIT), which leverages federated learning (FL) as the
learning framework for the instruction tuning of LLMs. This marks the first
exploration of FL-based instruction tuning for LLMs. This is especially
important since text data is predominantly generated by end users. Therefore,
it is imperative to design and adapt FL approaches to effectively leverage
these users' diverse instructions stored on local devices, while preserving
privacy and ensuring data security. In the current paper, by conducting widely
used GPT-4 auto-evaluation, we demonstrate that by exploiting the heterogeneous
and diverse sets of instructions on the client's end with the proposed
framework FedIT, we improved the performance of LLMs compared to centralized
training with only limited local instructions. Further, in this paper, we
developed a Github repository named Shepherd. This repository offers a
foundational framework for exploring federated fine-tuning of LLMs using
heterogeneous instructions across diverse categories.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.07984">SCENE: Self-Labeled Counterfactuals for Extrapolating to Negative Examples. (arXiv:2305.07984v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1">Deqing Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Godbole_A/0/1/0/all/0/1">Ameya Godbole</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1">Robin Jia</a></p>
<p>Detecting negatives (such as non-entailment relationships, unanswerable
questions, and false claims) is an important and challenging aspect of many
natural language understanding tasks. Though manually collecting challenging
negative examples can help models detect them, it is both costly and
domain-specific. In this work, we propose Self-labeled Counterfactuals for
Extrapolating to Negative Examples (SCENE), an automatic method for
synthesizing training data that greatly improves models' ability to detect
challenging negative examples. In contrast with standard data augmentation,
which synthesizes new examples for existing labels, SCENE can synthesize
negative examples zero-shot from only positive ones. Given a positive example,
SCENE perturbs it with a mask infilling model, then determines whether the
resulting example is negative based on a self-training heuristic. With access
to only answerable training examples, SCENE can close 69.6% of the performance
gap on SQuAD 2.0, a dataset where half of the evaluation examples are
unanswerable, compared to a model trained on SQuAD 2.0. Our method also extends
to boolean question answering and recognizing textual entailment, and improves
generalization from SQuAD to ACE-whQA, an out-of-domain extractive QA
benchmark.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.10163">Large Language Models Leverage External Knowledge to Extend Clinical Insight Beyond Language Boundaries. (arXiv:2305.10163v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jiageng Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xian Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1">Zhaopeng Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Minghui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yingying Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1">Yefeng Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jie Yang</a></p>
<p>Objectives: Large Language Models (LLMs) such as ChatGPT and Med-PaLM have
excelled in various medical question-answering tasks. However, these
English-centric models encounter challenges in non-English clinical settings,
primarily due to limited clinical knowledge in respective languages, a
consequence of imbalanced training corpora. We systematically evaluate LLMs in
the Chinese medical context and develop a novel in-context learning framework
to enhance their performance.
</p>
<p>Materials and Methods: The latest China National Medical Licensing
Examination (CNMLE-2022) served as the benchmark. We collected 53 medical books
and 381,149 medical questions to construct the medical knowledge base and
question bank. The proposed Knowledge and Few-shot Enhancement In-context
Learning (KFE) framework leverages the in-context learning ability of LLMs to
integrate diverse external clinical knowledge sources. We evaluated KFE with
ChatGPT(GPT3.5), GPT4, Baichuan2-7b, and Baichuan2-13B in CNMLE-2022 and
further investigated the effectiveness of different pathways for incorporating
LLMs with medical knowledge from seven distinct perspectives.
</p>
<p>Results: Directly applying ChatGPT failed to qualify for the CNMLE-2022 at a
score of 51. Cooperated with the KFE framework, the LLMs with varying sizes
yielded consistent and significant improvements. The ChatGPT's performance
surged to 70.04 and GPT-4 achieved the highest score of 82.59. This surpasses
the qualification threshold (60) and exceeds the average human score of 68.70,
affirming the effectiveness and robustness of the framework. It also enabled a
smaller Baichuan2-13B to pass the examination, showcasing the great potential
in low-resource settings. This study shed light on the optimal practices to
enhance the capabilities of LLMs in non-English medical scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.11789">Solving NLP Problems through Human-System Collaboration: A Discussion-based Approach. (arXiv:2305.11789v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kaneko_M/0/1/0/all/0/1">Masahiro Kaneko</a>, <a href="http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1">Graham Neubig</a>, <a href="http://arxiv.org/find/cs/1/au:+Okazaki_N/0/1/0/all/0/1">Naoaki Okazaki</a></p>
<p>Humans work together to solve common problems by having discussions,
explaining, and agreeing or disagreeing with each other. Similarly, if a system
can have discussions with humans when solving tasks, it can improve the
system's performance and reliability. In previous research on explainability,
it has only been possible for the system to make predictions and for humans to
ask questions about them rather than having a mutual exchange of opinions. This
research aims to create a dataset and computational framework for systems that
discuss and refine their predictions through dialogue. Through experiments, we
show that the proposed system can have beneficial discussions with humans
improving the accuracy by up to 25 points in the natural language inference
task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.12924">EnCore: Fine-Grained Entity Typing by Pre-Training Entity Encoders on Coreference Chains. (arXiv:2305.12924v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mtumbuka_F/0/1/0/all/0/1">Frank Mtumbuka</a>, <a href="http://arxiv.org/find/cs/1/au:+Schockaert_S/0/1/0/all/0/1">Steven Schockaert</a></p>
<p>Entity typing is the task of assigning semantic types to the entities that
are mentioned in a text. In the case of fine-grained entity typing (FET), a
large set of candidate type labels is considered. Since obtaining sufficient
amounts of manual annotations is then prohibitively expensive, FET models are
typically trained using distant supervision. In this paper, we propose to
improve on this process by pre-training an entity encoder such that embeddings
of coreferring entities are more similar to each other than to the embeddings
of other entities. The main problem with this strategy, which helps to explain
why it has not previously been considered, is that predicted coreference links
are often too noisy. We show that this problem can be addressed by using a
simple trick: we only consider coreference links that are predicted by two
different off-the-shelf systems. With this prudent use of coreference links,
our pre-training strategy allows us to improve the state-of-the-art in
benchmarks on fine-grained entity typing, as well as traditional entity
extraction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13521">CEO: Corpus-based Open-Domain Event Ontology Induction. (arXiv:2305.13521v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1">Nan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jianshu Chen</a></p>
<p>Existing event-centric NLP models often only apply to the pre-defined
ontology, which significantly restricts their generalization capabilities. This
paper presents CEO, a novel Corpus-based Event Ontology induction model to
relax the restriction imposed by pre-defined event ontologies. Without direct
supervision, CEO leverages distant supervision from available summary datasets
to detect corpus-wise salient events and exploits external event knowledge to
force events within a short distance to have close embeddings. Experiments on
three popular event datasets show that the schema induced by CEO has better
coverage and higher accuracy than previous methods. Moreover, CEO is the first
event ontology induction model that can induce a hierarchical event ontology
with meaningful names on eleven open-domain corpora, making the induced schema
more trustworthy and easier to be further curated.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13684">mPLM-Sim: Better Cross-Lingual Similarity and Transfer in Multilingual Pretrained Language Models. (arXiv:2305.13684v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1">Peiqin Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1">Chengzhi Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zheyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1">Andr&#xe9; F. T. Martins</a>, <a href="http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1">Hinrich Sch&#xfc;tze</a></p>
<p>Recent multilingual pretrained language models (mPLMs) have been shown to
encode strong language-specific signals, which are not explicitly provided
during pretraining. It remains an open question whether it is feasible to
employ mPLMs to measure language similarity, and subsequently use the
similarity results to select source languages for boosting cross-lingual
transfer. To investigate this, we propose mPLMSim, a language similarity
measure that induces the similarities across languages from mPLMs using
multi-parallel corpora. Our study shows that mPLM-Sim exhibits moderately high
correlations with linguistic similarity measures, such as lexicostatistics,
genealogical language family, and geographical sprachbund. We also conduct a
case study on languages with low correlation and observe that mPLM-Sim yields
more accurate similarity results. Additionally, we find that similarity results
vary across different mPLMs and different layers within an mPLM. We further
investigate whether mPLMSim is effective for zero-shot cross-lingual transfer
by conducting experiments on both low-level syntactic tasks and high-level
semantic tasks. The experimental results demonstrate that mPLM-Sim is capable
of selecting better source languages than linguistic measures, resulting in a
1%-2% improvement in zero-shot cross-lingual transfer performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15645">ConvGQR: Generative Query Reformulation for Conversational Search. (arXiv:2305.15645v3 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mo_F/0/1/0/all/0/1">Fengran Mo</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_K/0/1/0/all/0/1">Kelong Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yutao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yihong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1">Kaiyu Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1">Jian-Yun Nie</a></p>
<p>In conversational search, the user's real search intent for the current turn
is dependent on the previous conversation history. It is challenging to
determine a good search query from the whole conversation context. To avoid the
expensive re-training of the query encoder, most existing methods try to learn
a rewriting model to de-contextualize the current query by mimicking the manual
query rewriting. However, manually rewritten queries are not always the best
search queries. Training a rewriting model on them would limit the model's
ability to produce good search queries. Another useful hint is the potential
answer to the question. In this paper, we propose ConvGQR, a new framework to
reformulate conversational queries based on generative pre-trained language
models (PLMs), one for query rewriting and another for generating potential
answers. By combining both, ConvGQR can produce better search queries. In
addition, to relate query reformulation to retrieval performance, we propose a
knowledge infusion mechanism to optimize both query reformulation and
retrieval. Extensive experiments on four conversational search datasets
demonstrate the effectiveness of ConvGQR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16128">Give Me More Details: Improving Fact-Checking with Latent Retrieval. (arXiv:2305.16128v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xuming Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Junzhe Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Zhijiang Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1">Philip S. Yu</a></p>
<p>Evidence plays a crucial role in automated fact-checking. When verifying
real-world claims, existing fact-checking systems either assume the evidence
sentences are given or use the search snippets returned by the search engine.
Such methods ignore the challenges of collecting evidence and may not provide
sufficient information to verify real-world claims. Aiming at building a better
fact-checking system, we propose to incorporate full text from source documents
as evidence and introduce two enriched datasets. The first one is a
multilingual dataset, while the second one is monolingual (English). We further
develop a latent variable model to jointly extract evidence sentences from
documents and perform claim verification. Experiments indicate that including
source documents can provide sufficient contextual clues even when gold
evidence sentences are not annotated. The proposed system is able to achieve
significant improvements upon best-reported models under different settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.00168">Measuring the Robustness of NLP Models to Domain Shifts. (arXiv:2306.00168v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Calderon_N/0/1/0/all/0/1">Nitay Calderon</a>, <a href="http://arxiv.org/find/cs/1/au:+Porat_N/0/1/0/all/0/1">Naveh Porat</a>, <a href="http://arxiv.org/find/cs/1/au:+Ben_David_E/0/1/0/all/0/1">Eyal Ben-David</a>, <a href="http://arxiv.org/find/cs/1/au:+Chapanin_A/0/1/0/all/0/1">Alexander Chapanin</a>, <a href="http://arxiv.org/find/cs/1/au:+Gekhman_Z/0/1/0/all/0/1">Zorik Gekhman</a>, <a href="http://arxiv.org/find/cs/1/au:+Oved_N/0/1/0/all/0/1">Nadav Oved</a>, <a href="http://arxiv.org/find/cs/1/au:+Shalumov_V/0/1/0/all/0/1">Vitaly Shalumov</a>, <a href="http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1">Roi Reichart</a></p>
<p>Existing research on Domain Robustness (DR) suffers from disparate setups,
lack of task variety, and scarce research on recent capabilities such as
few-shot learning. Furthermore, we claim that the common practice of measuring
DR might further obscure the picture. Current research focuses on challenge
sets and relies solely on the Source Drop (SD): Using the source in-domain
performance as a reference point for degradation. However, the Target Drop
(TD), which measures degradation from the target in-domain performance, should
be used as a complementary point of view. In this study, we developed a
benchmark comprised of seven NLP tasks, including classification, QA, and
generation. Our benchmark focuses on natural topical domain shifts and enables
measuring both the SD and the TD. Our comprehensive study, involving over
14,000 domain shifts across 18 fine-tuned and few-shot models, shows that both
model types suffer from drops upon domain shifts. While fine-tuned models excel
in-domain, few-shot LLMs often surpass them cross-domain, showing better
robustness. In addition, we found that a large SD can be explained by shifting
to a harder domain rather than by a genuine DR challenge. Thus, the TD is a
more reliable metric for assessing DR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.12896">Corrections of Zipf&#x27;s and Heaps&#x27; Laws Derived from Hapax Rate Models. (arXiv:2307.12896v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Debowski_L/0/1/0/all/0/1">&#x141;ukasz D&#x119;bowski</a></p>
<p>The article introduces corrections to Zipf's and Heaps' laws based on
systematic models of the proportion of hapaxes, i.e., words that occur once.
The derivation rests on two assumptions: The first one is the standard urn
model which predicts that marginal frequency distributions for shorter texts
look as if word tokens were sampled blindly from a given longer text. The
second assumption posits that the hapax rate is a simple function of the text
length. Four such functions are discussed: the constant model, the Davis model,
the linear model, and the logistic model. It is shown that the logistic model
yields the best fit.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14385">Mental-LLM: Leveraging Large Language Models for Mental Health Prediction via Online Text Data. (arXiv:2307.14385v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xuhai Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1">Bingsheng Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1">Yuanzhe Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Gabriel_S/0/1/0/all/0/1">Saadia Gabriel</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Hong Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hendler_J/0/1/0/all/0/1">James Hendler</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghassemi_M/0/1/0/all/0/1">Marzyeh Ghassemi</a>, <a href="http://arxiv.org/find/cs/1/au:+Dey_A/0/1/0/all/0/1">Anind K. Dey</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dakuo Wang</a></p>
<p>Advances in large language models (LLMs) have empowered a variety of
applications. However, there is still a significant gap in research when it
comes to understanding and enhancing the capabilities of LLMs in the field of
mental health. In this work, we present a comprehensive evaluation of multiple
LLMs on various mental health prediction tasks via online text data, including
Alpaca, Alpaca-LoRA, FLAN-T5, GPT-3.5, and GPT-4. We conduct a broad range of
experiments, covering zero-shot prompting, few-shot prompting, and instruction
fine-tuning. The results indicate a promising yet limited performance of LLMs
with zero-shot and few-shot prompt designs for mental health tasks. More
importantly, our experiments show that instruction finetuning can significantly
boost the performance of LLMs for all tasks simultaneously. Our best-finetuned
models, Mental-Alpaca and Mental-FLAN-T5, outperform the best prompt design of
GPT-3.5 (25 and 15 times bigger) by 10.9% on balanced accuracy and the best of
GPT-4 (250 and 150 times bigger) by 4.8%. They further perform on par with the
state-of-the-art task-specific language model. We also conduct an exploratory
case study on LLMs' capability on mental health reasoning tasks, illustrating
the promising capability of certain models such as GPT-4. We summarize our
findings into a set of action guidelines for potential methods to enhance LLMs'
capability for mental health tasks. Meanwhile, we also emphasize the important
limitations before achieving deployability in real-world mental health
settings, such as known racial and gender bias. We highlight the important
ethical risks accompanying this line of research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.08628">Learning the meanings of function words from grounded language using a visual question answering model. (arXiv:2308.08628v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Portelance_E/0/1/0/all/0/1">Eva Portelance</a>, <a href="http://arxiv.org/find/cs/1/au:+Frank_M/0/1/0/all/0/1">Michael C. Frank</a>, <a href="http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1">Dan Jurafsky</a></p>
<p>Interpreting a seemingly-simple function word like "or", "behind", or "more"
can require logical, numerical, and relational reasoning. How are such words
learned by children? Prior acquisition theories have often relied on positing a
foundation of innate knowledge. Yet recent neural-network based visual question
answering models apparently can learn to use function words as part of
answering questions about complex visual scenes. In this paper, we study what
these models learn about function words, in the hope of better understanding
how the meanings of these words can be learnt by both models and children. We
show that recurrent models trained on visually grounded language learn gradient
semantics for function words requiring spacial and numerical reasoning.
Furthermore, we find that these models can learn the meanings of logical
connectives "and" and "or" without any prior knowledge of logical reasoning, as
well as early evidence that they are sensitive to alternative expressions when
interpreting language. Finally, we show that word learning difficulty is
dependent on frequency in models' input. Our findings offer proof-of-concept
evidence that it is possible to learn the nuanced interpretations of function
words in visually grounded context by using non-symbolic general statistical
learning algorithms, without any prior knowledge of linguistic meaning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.10335">Can ChatGPT replace StackOverflow? A Study on Robustness and Reliability of Large Language Model Code Generation. (arXiv:2308.10335v5 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1">Li Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zilong Wang</a></p>
<p>Recently, the large language models (LLMs) have shown extraordinary ability
in understanding natural language and generating programming code. It has been
a common practice of software engineers to consult LLMs when encountering
coding questions. Although efforts have been made to avoid syntax errors and
align the code with the intended semantics, the reliability and robustness of
the code generationfrom LLMs have not yet been thoroughly studied. The
executable code is not equivalent to the reliable and robust code, especially
in the context of real-world software development. The misuse of APIs in the
generated code could lead to severe problem, such as resource leaks, program
crashes. To make things worse, the users of LLM code generation services are
actually the developers that are most vulnerable to these code that seems right
-- They are always novice developers that are not familiar with the APIs that
LLMs generate code for them. Therefore, they could hardly tell the misuse in
the code generated by LLMs, which further facilitates the incorrect code
applied in real-world software. Existing code evaluation benchmark and datasets
focus on crafting small tasks such as programming questions in coding
interviews, which however deviates from the problem that developers would ask
LLM for real-world coding help. To fill the missing piece, in this work, we
propose a dataset RobustAPI for evaluating the reliability and robustness of
code generated by LLMs. We collect 1208 coding questions from StackOverflow on
24 representative Java APIs. We summarize thecommon misuse patterns of these
APIs and evaluate them oncurrent popular LLMs. The evaluation results show that
evenfor GPT-4, 62% of the generated code contains API misuses,which would cause
unexpected consequences if the code isintroduced into real-world software.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.04174">Manifold-based Verbalizer Space Re-embedding for Tuning-free Prompt-based Classification. (arXiv:2309.04174v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haochun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1">Sendong Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xi_N/0/1/0/all/0/1">Nuwa Xi</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_M/0/1/0/all/0/1">Muzhen Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1">Bing Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Ting Liu</a></p>
<p>Prompt-based classification adapts tasks to a cloze question format utilizing
the [MASK] token and the filled tokens are then mapped to labels through
pre-defined verbalizers. Recent studies have explored the use of verbalizer
embeddings to reduce labor in this process. However, all existing studies
require a tuning process for either the pre-trained models or additional
trainable embeddings. Meanwhile, the distance between high-dimensional
verbalizer embeddings should not be measured by Euclidean distance due to the
potential for non-linear manifolds in the representation space. In this study,
we propose a tuning-free manifold-based space re-embedding method called
Locally Linear Embedding with Intra-class Neighborhood Constraint (LLE-INC) for
verbalizer embeddings, which preserves local properties within the same class
as guidance for classification. Experimental results indicate that even without
tuning any parameters, our LLE-INC is on par with automated verbalizers with
parameter tuning. And with the parameter updating, our approach further
enhances prompt-based tuning by up to 3.2%. Furthermore, experiments with the
LLaMA-7B&amp;13B indicate that LLE-INC is an efficient tuning-free classification
approach for the hyper-scale language models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.06054">Breaking through the learning plateaus of in-context learning in Transformer. (arXiv:2309.06054v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1">Jingwen Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1">Tao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuwang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yan Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1">Nanning Zheng</a></p>
<p>In-context learning, i.e., learning from context examples, is an impressive
ability of Transformer. Training Transformers to possess this in-context
learning skill is computationally intensive due to the occurrence of learning
plateaus, which are periods within the training process where there is minimal
or no enhancement in the model's in-context learning capability. To study the
mechanism behind the learning plateaus, we conceptually seperate a component
within the model's internal representation that is exclusively affected by the
model's weights. We call this the "weights component", and the remainder is
identified as the "context component". By conducting meticulous and controlled
experiments on synthetic tasks, we note that the persistence of learning
plateaus correlates with compromised functionality of the weights component.
Recognizing the impaired performance of the weights component as a fundamental
behavior drives learning plateaus, we have developed three strategies to
expedite the learning of Transformers. The effectiveness of these strategies is
further confirmed in natural language processing tasks. In conclusion, our
research demonstrates the feasibility of cultivating a powerful in-context
learning ability within AI systems in an eco-friendly manner.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07098">Mitigating Hallucinations and Off-target Machine Translation with Source-Contrastive and Language-Contrastive Decoding. (arXiv:2309.07098v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1">Rico Sennrich</a>, <a href="http://arxiv.org/find/cs/1/au:+Vamvas_J/0/1/0/all/0/1">Jannis Vamvas</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohammadshahi_A/0/1/0/all/0/1">Alireza Mohammadshahi</a></p>
<p>Hallucinations and off-target translation remain unsolved problems in MT,
especially for low-resource languages and massively multilingual models. In
this paper, we introduce two related methods to mitigate these failure cases
with a modified decoding objective, without either requiring retraining or
external models. In source-contrastive decoding, we search for a translation
that is probable given the correct input, but improbable given a random input
segment. In language-contrastive decoding, we search for a translation that is
probable, but improbable given the wrong language indicator token. Experiments
on the massively multilingual models M2M-100 (418M) and SMaLL-100 show that
these methods suppress hallucinations and off-target translations, reducing the
number of translations with segment-level chrF2 below 10 by 67-83% on average,
and the number of translations with oscillatory hallucinations by 75-92% on
average, across 57 tested translation directions. In a proof of concept on
out-of-English translation, we also show that we can suppress off-target
translations with large language models. We release our source code at
https://github.com/ZurichNLP/ContraDecode.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.08609">Media of Langue: The dictionary that visualizes Inter-Lingual Semantic Network/Space. (arXiv:2309.08609v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Muramoto_G/0/1/0/all/0/1">Goki Muramoto</a>, <a href="http://arxiv.org/find/cs/1/au:+Sato_A/0/1/0/all/0/1">Atsuki Sato</a>, <a href="http://arxiv.org/find/cs/1/au:+Koyama_T/0/1/0/all/0/1">Takayoshi Koyama</a></p>
<p>This paper introduces "Media of Langue," a novel dictionary visualizing
Inter-lingual semantic network/space. Our proposed Inter-lingual semantic
network/space is formed solely from the accumulation of translation practices
between two or more language systems, in contrast to existing semantic
networks/spaces that explicitly use "intra"-lingual relations. By visualizing
this network/space for humans, an Inter-lingual dictionary can be realized that
points to the semantic place of many words at once with a chain of mutual
translation, which also contains the functions of existing dictionaries such as
bilingual and synonym dictionaries. We implemented and published this interface
as a web application, focusing on seven language pairs. In this paper, we first
describe Inter-lingual semantic network/space with its basic features and the
way to develop it from bilingual corpora, then details the design of "Media of
Langue," with a quick analysis and illustrative examples of use cases. Our
website is www.media-of-langue.org. A demonstration video is available at
https://youtu.be/98lXuX4yjsU.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.13340">Towards LLM-guided Causal Explainability for Black-box Text Classifiers. (arXiv:2309.13340v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bhattacharjee_A/0/1/0/all/0/1">Amrita Bhattacharjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Moraffah_R/0/1/0/all/0/1">Raha Moraffah</a>, <a href="http://arxiv.org/find/cs/1/au:+Garland_J/0/1/0/all/0/1">Joshua Garland</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Huan Liu</a></p>
<p>With the advent of larger and more complex deep learning models, such as in
Natural Language Processing (NLP), model qualities like explainability and
interpretability, albeit highly desirable, are becoming harder challenges to
tackle and solve. For example, state-of-the-art models in text classification
are black-box by design. Although standard explanation methods provide some
degree of explainability, these are mostly correlation-based methods and do not
provide much insight into the model. The alternative of causal explainability
is more desirable to achieve but extremely challenging in NLP due to a variety
of reasons. Inspired by recent endeavors to utilize Large Language Models
(LLMs) as experts, in this work, we aim to leverage the instruction-following
and textual understanding capabilities of recent state-of-the-art LLMs to
facilitate causal explainability via counterfactual explanation generation for
black-box text classifiers. To do this, we propose a three-step pipeline via
which, we use an off-the-shelf LLM to: (1) identify the latent or unobserved
features in the input text, (2) identify the input features associated with the
latent features, and finally (3) use the identified input features to generate
a counterfactual explanation. We experiment with our pipeline on multiple NLP
text classification datasets, with several recent LLMs, and present interesting
and promising findings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00863">Syllable-level lyrics generation from melody exploiting character-level language model. (arXiv:2310.00863v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhe Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lasocki_K/0/1/0/all/0/1">Karol Lasocki</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yi Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Takasu_A/0/1/0/all/0/1">Atsuhiro Takasu</a></p>
<p>The generation of lyrics tightly connected to accompanying melodies involves
establishing a mapping between musical notes and syllables of lyrics. This
process requires a deep understanding of music constraints and semantic
patterns at syllable-level, word-level, and sentence-level semantic meanings.
However, pre-trained language models specifically designed at the syllable
level are publicly unavailable. To solve these challenging issues, we propose
to exploit fine-tuning character-level language models for syllable-level
lyrics generation from symbolic melody. In particular, our method endeavors to
incorporate linguistic knowledge of the language model into the beam search
process of a syllable-level Transformer generator network. Additionally, by
exploring ChatGPT-based evaluation for generated lyrics, along with human
subjective evaluation, we demonstrate that our approach enhances the coherence
and correctness of the generated lyrics, eliminating the need to train
expensive new language models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01801">Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs. (arXiv:2310.01801v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1">Suyu Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yunan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Liyuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Minjia Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Jiawei Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jianfeng Gao</a></p>
<p>In this study, we introduce adaptive KV cache compression, a plug-and-play
method that reduces the memory footprint of generative inference for Large
Language Models (LLMs). Different from the conventional KV cache that retains
key and value vectors for all context tokens, we conduct targeted profiling to
discern the intrinsic structure of attention modules. Based on the recognized
structure, we then construct the KV cache in an adaptive manner: evicting
long-range contexts on attention heads emphasizing local contexts, discarding
non-special tokens on attention heads centered on special tokens, and only
employing the standard KV cache for attention heads that broadly attend to all
tokens. Moreover, with the lightweight attention profiling used to guide the
construction of the adaptive KV cache, FastGen can be deployed without
resource-intensive fine-tuning or re-training. In our experiments across
various asks, FastGen demonstrates substantial reduction on GPU memory
consumption with negligible generation quality loss. We will release our code
and the compatible CUDA kernel for reproducibility.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02446">Low-Resource Languages Jailbreak GPT-4. (arXiv:2310.02446v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yong_Z/0/1/0/all/0/1">Zheng-Xin Yong</a>, <a href="http://arxiv.org/find/cs/1/au:+Menghini_C/0/1/0/all/0/1">Cristina Menghini</a>, <a href="http://arxiv.org/find/cs/1/au:+Bach_S/0/1/0/all/0/1">Stephen H. Bach</a></p>
<p>AI safety training and red-teaming of large language models (LLMs) are
measures to mitigate the generation of unsafe content. Our work exposes the
inherent cross-lingual vulnerability of these safety mechanisms, resulting from
the linguistic inequality of safety training data, by successfully
circumventing GPT-4's safeguard through translating unsafe English inputs into
low-resource languages. On the AdvBenchmark, GPT-4 engages with the unsafe
translated inputs and provides actionable items that can get the users towards
their harmful goals 79% of the time, which is on par with or even surpassing
state-of-the-art jailbreaking attacks. Other high-/mid-resource languages have
significantly lower attack success rate, which suggests that the cross-lingual
vulnerability mainly applies to low-resource languages. Previously, limited
training on low-resource languages primarily affects speakers of those
languages, causing technological disparities. However, our work highlights a
crucial shift: this deficiency now poses a risk to all LLMs users. Publicly
available translation APIs enable anyone to exploit LLMs' safety
vulnerabilities. Therefore, our work calls for a more holistic red-teaming
efforts to develop robust multilingual safeguards with wide language coverage.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07276">BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations. (arXiv:2310.07276v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pei_Q/0/1/0/all/0/1">Qizhi Pei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jinhua Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1">Kehan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1">Kaiyuan Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1">Lijun Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1">Yingce Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1">Rui Yan</a></p>
<p>Recent advancements in biological research leverage the integration of
molecules, proteins, and natural language to enhance drug discovery. However,
current models exhibit several limitations, such as the generation of invalid
molecular SMILES, underutilization of contextual information, and equal
treatment of structured and unstructured knowledge. To address these issues, we
propose $\mathbf{BioT5}$, a comprehensive pre-training framework that enriches
cross-modal integration in biology with chemical knowledge and natural language
associations. $\mathbf{BioT5}$ utilizes SELFIES for $100%$ robust molecular
representations and extracts knowledge from the surrounding context of
bio-entities in unstructured biological literature. Furthermore,
$\mathbf{BioT5}$ distinguishes between structured and unstructured knowledge,
leading to more effective utilization of information. After fine-tuning, BioT5
shows superior performance across a wide range of tasks, demonstrating its
strong capability of capturing underlying relations and properties of
bio-entities. Our code is available at
$\href{https://github.com/QizhiPei/BioT5}{Github}$.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08383">Reconstructing Materials Tetrahedron: Challenges in Materials Information Extraction. (arXiv:2310.08383v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hira_K/0/1/0/all/0/1">Kausik Hira</a>, <a href="http://arxiv.org/find/cs/1/au:+Zaki_M/0/1/0/all/0/1">Mohd Zaki</a>, <a href="http://arxiv.org/find/cs/1/au:+Sheth_D/0/1/0/all/0/1">Dhruvil Sheth</a>, <a href="http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1">Mausam</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishnan_N/0/1/0/all/0/1">N M Anoop Krishnan</a></p>
<p>Discovery of new materials has a documented history of propelling human
progress for centuries and more. The behaviour of a material is a function of
its composition, structure, and properties, which further depend on its
processing and testing conditions. Recent developments in deep learning and
natural language processing have enabled information extraction at scale from
published literature such as peer-reviewed publications, books, and patents.
However, this information is spread in multiple formats, such as tables, text,
and images, and with little or no uniformity in reporting style giving rise to
several machine learning challenges. Here, we discuss, quantify, and document
these challenges in automated information extraction (IE) from materials
science literature towards the creation of a large materials science knowledge
base. Specifically, we focus on IE from text and tables and outline several
challenges with examples. We hope the present work inspires researchers to
address the challenges in a coherent fashion, providing a fillip to IE towards
a materials knowledge base.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.14703">Establishing Vocabulary Tests as a Benchmark for Evaluating Large Language Models. (arXiv:2310.14703v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Martinez_G/0/1/0/all/0/1">Gonzalo Mart&#xed;nez</a>, <a href="http://arxiv.org/find/cs/1/au:+Conde_J/0/1/0/all/0/1">Javier Conde</a>, <a href="http://arxiv.org/find/cs/1/au:+Merino_Gomez_E/0/1/0/all/0/1">Elena Merino-G&#xf3;mez</a>, <a href="http://arxiv.org/find/cs/1/au:+Bermudez_Margaretto_B/0/1/0/all/0/1">Beatriz Berm&#xfa;dez-Margaretto</a>, <a href="http://arxiv.org/find/cs/1/au:+Hernandez_J/0/1/0/all/0/1">Jos&#xe9; Alberto Hern&#xe1;ndez</a>, <a href="http://arxiv.org/find/cs/1/au:+Reviriego_P/0/1/0/all/0/1">Pedro Reviriego</a>, <a href="http://arxiv.org/find/cs/1/au:+Brysbaert_M/0/1/0/all/0/1">Marc Brysbaert</a></p>
<p>Vocabulary tests, once a cornerstone of language modeling evaluation, have
been largely overlooked in the current landscape of Large Language Models
(LLMs) like Llama, Mistral, and GPT. While most LLM evaluation benchmarks focus
on specific tasks or domain-specific knowledge, they often neglect the
fundamental linguistic aspects of language understanding and production. In
this paper, we advocate for the revival of vocabulary tests as a valuable tool
for assessing LLM performance. We evaluate seven LLMs using two vocabulary test
formats across two languages and uncover surprising gaps in their lexical
knowledge. These findings shed light on the intricacies of LLM word
representations, their learning mechanisms, and performance variations across
models and languages. Moreover, the ability to automatically generate and
perform vocabulary tests offers new opportunities to expand the approach and
provide a more complete picture of LLMs' language skills.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17589">An Open Source Data Contamination Report for Large Language Models. (arXiv:2310.17589v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yucheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Guerin_F/0/1/0/all/0/1">Frank Guerin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1">Chenghua Lin</a></p>
<p>Data contamination in model evaluation has become increasingly prevalent with
the growing popularity of large language models. It allows models to "cheat"
via memorisation instead of displaying true capabilities. Therefore,
contamination analysis has become an crucial part of reliable model evaluation
to validate results. However, existing contamination analysis is usually
conducted internally by large language model developers and often lacks
transparency and completeness. This paper presents an extensive data
contamination report for over 15 popular large language models across six
popular multiple-choice QA benchmarks. We also introduce an open-source
pipeline that enables the community to perform contamination analysis on
customised data and models. Our experiments reveal varying contamination levels
ranging from 1\% to 45\% across benchmarks, with the contamination degree
increasing rapidly over time. Performance analysis of large language models
indicates that data contamination does not necessarily lead to increased model
metrics: while significant accuracy boosts of up to 14\% and 7\% are observed
on contaminated C-Eval and Hellaswag benchmarks, only a minimal increase is
noted on contaminated MMLU. We also find larger models seem able to gain more
advantages than smaller models on contaminated test sets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01927">GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling. (arXiv:2311.01927v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Katsch_T/0/1/0/all/0/1">Tobias Katsch</a></p>
<p>Linear Recurrence has proven to be a powerful tool for modeling long
sequences efficiently. In this work, we show that existing models fail to take
full advantage of its potential. Motivated by this finding, we develop
GateLoop, a foundational sequence model that generalizes linear recurrent
models such as S4, S5, LRU and RetNet, by employing data-controlled state
transitions. Utilizing this theoretical advance, GateLoop empirically
outperforms existing models for auto-regressive language modeling. Our method
comes with a low-cost $O(l)$ recurrent mode and an efficient $O(l \log_{2} l)$
parallel mode making use of highly optimized associative scan implementations.
Furthermore, we derive an $O(l^2)$ surrogate attention mode, revealing
remarkable implications for Transformer and recently proposed architectures.
Specifically, we prove that our approach can be interpreted as providing
data-controlled relative-positional information to Attention. While many
existing models solely rely on data-controlled cumulative sums for context
aggregation, our findings suggest that incorporating data-controlled complex
cumulative products may be a crucial step towards more powerful sequence
models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04507">Conversation Understanding using Relational Temporal Graph Neural Networks with Auxiliary Cross-Modality Interaction. (arXiv:2311.04507v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1">Cam-Van Thi Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Mai_A/0/1/0/all/0/1">Anh-Tuan Mai</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1">The-Son Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Kieu_H/0/1/0/all/0/1">Hai-Dang Kieu</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1">Duc-Trong Le</a></p>
<p>Emotion recognition is a crucial task for human conversation understanding.
It becomes more challenging with the notion of multimodal data, e.g., language,
voice, and facial expressions. As a typical solution, the global- and the local
context information are exploited to predict the emotional label for every
single sentence, i.e., utterance, in the dialogue. Specifically, the global
representation could be captured via modeling of cross-modal interactions at
the conversation level. The local one is often inferred using the temporal
information of speakers or emotional shifts, which neglects vital factors at
the utterance level. Additionally, most existing approaches take fused features
of multiple modalities in an unified input without leveraging modality-specific
representations. Motivating from these problems, we propose the Relational
Temporal Graph Neural Network with Auxiliary Cross-Modality Interaction
(CORECT), an novel neural network framework that effectively captures
conversation-level cross-modality interactions and utterance-level temporal
dependencies with the modality-specific manner for conversation understanding.
Extensive experiments demonstrate the effectiveness of CORECT via its
state-of-the-art results on the IEMOCAP and CMU-MOSEI datasets for the
multimodal ERC task Implementation available at:
https://github.com/leson502/CORECT\_EMNLP2023
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04892">Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs. (arXiv:2311.04892v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1">Shashank Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Shrivastava_V/0/1/0/all/0/1">Vaishnavi Shrivastava</a>, <a href="http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1">Ameet Deshpande</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalyan_A/0/1/0/all/0/1">Ashwin Kalyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1">Peter Clark</a>, <a href="http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1">Ashish Sabharwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Khot_T/0/1/0/all/0/1">Tushar Khot</a></p>
<p>Recent works have showcased the ability of LLMs to embody diverse personas in
their responses, exemplified by prompts like 'You are Yoda. Explain the Theory
of Relativity.' While this ability allows personalization of LLMs and enables
human behavior simulation, its effect on LLMs' capabilities remains unclear. To
fill this gap, we present the first extensive study of the unintended
side-effects of persona assignment on the ability of LLMs to perform basic
reasoning tasks. Our study covers 24 reasoning datasets, 4 LLMs, and 19 diverse
personas (e.g. an Asian person) spanning 5 socio-demographic groups. Our
experiments unveil that LLMs harbor deep rooted bias against various
socio-demographics underneath a veneer of fairness. While they overtly reject
stereotypes when explicitly asked ('Are Black people less skilled at
mathematics?'), they manifest stereotypical and erroneous presumptions when
asked to answer questions while adopting a persona. These can be observed as
abstentions in responses, e.g., 'As a Black person, I can't answer this
question as it requires math knowledge', and generally result in a substantial
performance drop. Our experiments with ChatGPT-3.5 show that this bias is
ubiquitous - 80% of our personas demonstrate bias; it is significant - some
datasets show performance drops of 70%+; and can be especially harmful for
certain groups - some personas suffer statistically significant drops on 80%+
of the datasets. Overall, all 4 LLMs exhibit this bias to varying extents, with
GPT-4-Turbo showing the least but still a problematic amount of bias (evident
in 42% of the personas). Further analysis shows that these persona-induced
errors can be hard-to-discern and hard-to-avoid. Our findings serve as a
cautionary tale that the practice of assigning personas to LLMs - a trend on
the rise - can surface their deep-rooted biases and have unforeseeable and
detrimental side-effects.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07536">A Comprehensive Evaluation of GPT-4V on Knowledge-Intensive Visual Question Answering. (arXiv:2311.07536v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yunxin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Longyue Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1">Baotian Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xinyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1">Wanqi Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1">Chenyang Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Min Zhang</a></p>
<p>The emergence of multimodal large models (MLMs) has significantly advanced
the field of visual understanding, offering remarkable capabilities in the
realm of visual question answering (VQA). Yet, the true challenge lies in the
domain of knowledge-intensive VQA tasks, which necessitate not just recognition
of visual elements, but also a deep comprehension of the visual information in
conjunction with a vast repository of learned knowledge. To uncover such
capabilities of MLMs, particularly the newly introduced GPT-4V, we provide an
in-depth evaluation from three perspectives: 1) Commonsense Knowledge, which
assesses how well models can understand visual cues and connect to general
knowledge; 2) Fine-grained World Knowledge, which tests the model's skill in
reasoning out specific knowledge from images, showcasing their proficiency
across various specialized fields; 3) Comprehensive Knowledge with
Decision-making Rationales, which examines model's capability to provide
logical explanations for its inference, facilitating a deeper analysis from the
interpretability perspective. Extensive experiments indicate that GPT-4V
achieves SOTA performance on above three tasks. Interestingly, we find that: a)
GPT-4V demonstrates enhanced reasoning and explanation when using composite
images as few-shot; b) GPT-4V produces severe hallucinations when dealing with
world knowledge, highlighting the future need for advancements in this research
direction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08724">Knowledge Graph Construction in Power Distribution Networks. (arXiv:2311.08724v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Che Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bing Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Sizhe Li</a></p>
<p>In this paper, we propose a method for knowledge graph construction in power
distribution networks. This method leverages entity features, which involve
their semantic, phonetic, and syntactic characteristics, in both the knowledge
graph of distribution network and the dispatching texts. An enhanced model
based on Convolutional Neural Network, is utilized for effectively matching
dispatch text entities with those in the knowledge graph. The effectiveness of
this model is evaluated through experiments in real-world power distribution
dispatch scenarios. The results indicate that, compared with the baselines, the
proposed model excels in linking a variety of entity types, demonstrating high
overall accuracy in power distribution knowledge graph construction task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09335">Investigating Hallucinations in Pruned Large Language Models for Abstractive Summarization. (arXiv:2311.09335v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chrysostomou_G/0/1/0/all/0/1">George Chrysostomou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zhixue Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Williams_M/0/1/0/all/0/1">Miles Williams</a>, <a href="http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1">Nikolaos Aletras</a></p>
<p>Despite the remarkable performance of generative large language models (LLMs)
on abstractive summarization, they face two significant challenges: their
considerable size and tendency to hallucinate. Hallucinations are concerning
because they erode reliability and raise safety issues. Pruning is a technique
that reduces model size by removing redundant weights, enabling more efficient
sparse inference. Pruned models yield downstream task performance comparable to
the original, making them ideal alternatives when operating on a limited
budget. However, the effect that pruning has upon hallucinations in abstractive
summarization with LLMs has yet to be explored. In this paper, we provide an
extensive empirical study across five summarization datasets, two
state-of-the-art pruning methods, and five instruction-tuned LLMs.
Surprisingly, we find that hallucinations from pruned LLMs are less prevalent
than the original models. Our analysis suggests that pruned models tend to
depend more on the source document for summary generation. This leads to a
higher lexical overlap between the generated summary and the source document,
which could be a reason for the reduction in hallucination risk.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13708">Dynamic Fault Analysis in Substations Based on Knowledge Graphs. (arXiv:2311.13708v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Weiwei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Sizhe Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1">Hui Fan</a></p>
<p>To address the challenge of identifying hidden danger in substations from
unstructured text, a novel dynamic analysis method is proposed. We first
extract relevant information from the unstructured text, and then leverages a
flexible distributed search engine built on Elastic-Search to handle the data.
Following this, the hidden Markov model is employed to train the data within
the engine. The Viterbi algorithm is integrated to decipher the hidden state
sequences, facilitating the segmentation and labeling of entities related to
hidden dangers. The final step involves using the Neo4j graph database to
dynamically create a knowledge graph that visualizes hidden dangers in the
substation. The effectiveness of the proposed method is demonstrated through a
case analysis from a specific substation with hidden dangers revealed in the
text records.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15480">Automatic Time Signature Determination for New Scores Using Lyrics for Latent Rhythmic Structure. (arXiv:2311.15480v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liao_C/0/1/0/all/0/1">Callie C. Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_D/0/1/0/all/0/1">Duoduo Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Guessford_J/0/1/0/all/0/1">Jesse Guessford</a></p>
<p>There has recently been a sharp increase in interest in Artificial
Intelligence-Generated Content (AIGC). Despite this, musical components such as
time signatures have not been studied sufficiently to form an algorithmic
determination approach for new compositions, especially lyrical songs. This is
likely because of the neglect of musical details, which is critical for
constructing a robust framework. Specifically, time signatures establish the
fundamental rhythmic structure for almost all aspects of a song, including the
phrases and notes. In this paper, we propose a novel approach that only uses
lyrics as input to automatically generate a fitting time signature for lyrical
songs and uncover the latent rhythmic structure utilizing explainable machine
learning models. In particular, we devise multiple methods that are associated
with discovering lyrical patterns and creating new features that simultaneously
contain lyrical, rhythmic, and statistical information. In this approach, the
best of our experimental results reveal a 97.6% F1 score and a 0.996 Area Under
the Curve (AUC) of the Receiver Operating Characteristic (ROC) score. In
conclusion, our research directly generates time signatures from lyrics
automatically for new scores utilizing machine learning, which is an innovative
idea that approaches an understudied component of musicology and therefore
contributes significantly to the future of Artificial Intelligence (AI) music
generation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16522">Dynamic Fault Characteristics Evaluation in Power Grid. (arXiv:2311.16522v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pei_H/0/1/0/all/0/1">Hao Pei</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1">Si Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chuanfu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Che Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Haoming Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Sizhe Li</a></p>
<p>To enhance the intelligence degree in operation and maintenance, a novel
method for fault detection in power grids is proposed. The proposed GNN-based
approach first identifies fault nodes through a specialized feature extraction
method coupled with a knowledge graph. By incorporating temporal data, the
method leverages the status of nodes from preceding and subsequent time periods
to help current fault detection. To validate the effectiveness of the node
features, a correlation analysis of the output features from each node was
conducted. The results from experiments show that this method can accurately
locate fault nodes in simulation scenarios with a remarkable accuracy.
Additionally, the graph neural network based feature modeling allows for a
qualitative examination of how faults spread across nodes, which provides
valuable insights for analyzing fault nodes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18727">Automatic Functional Differentiation in JAX. (arXiv:2311.18727v2 [cs.PL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1">Min Lin</a></p>
<p>We extend JAX with the capability to automatically differentiate higher-order
functions (functionals and operators). By representing functions as a
generalization of arrays, we seamlessly use JAX's existing primitive system to
implement higher-order functions. We present a set of primitive operators that
serve as foundational building blocks for constructing several key types of
functionals. For every introduced primitive operator, we derive and implement
both linearization and transposition rules, aligning with JAX's internal
protocols for forward and reverse mode automatic differentiation. This
enhancement allows for functional differentiation in the same syntax
traditionally use for functions. The resulting functional gradients are
themselves functions ready to be invoked in python. We showcase this tool's
efficacy and simplicity through applications where functional derivatives are
indispensable. The source code of this work is released at
https://github.com/sail-sg/autofd .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03731">MultiGPrompt for Multi-Task Pre-Training and Prompting on Graphs. (arXiv:2312.03731v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1">Xingtong Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1">Chang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1">Yuan Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xinming Zhang</a></p>
<p>Graphs can inherently model interconnected objects on the Web, thereby
facilitating a series of Web applications, such as web analyzing and content
recommendation. Recently, Graph Neural Networks (GNNs) have emerged as a
mainstream technique for graph representation learning. However, their efficacy
within an end-to-end supervised framework is significantly tied to the
availabilityof task-specific labels. To mitigate labeling costs and enhance
robustness in few-shot settings, pre-training on self-supervised tasks has
emerged as a promising method, while prompting has been proposed to further
narrow the objective gap between pretext and downstream tasks. Although there
has been some initial exploration of prompt-based learning on graphs, they
primarily leverage a single pretext task, resulting in a limited subset of
general knowledge that could be learned from the pre-training data. Hence, in
this paper, we propose MultiGPrompt, a novel multi-task pre-training and
prompting framework to exploit multiple pretext tasks for more comprehensive
pre-trained knowledge. First, in pre-training, we design a set of pretext
tokens to synergize multiple pretext tasks. Second, we propose a dual-prompt
mechanism consisting of composed and open prompts to leverage task-specific and
global pre-training knowledge, to guide downstream tasks in few-shot settings.
Finally, we conduct extensive experiments on six public datasets to evaluate
and analyze MultiGPrompt.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03905">A Pseudo-Semantic Loss for Autoregressive Models with Logical Constraints. (arXiv:2312.03905v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ahmed_K/0/1/0/all/0/1">Kareem Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1">Kai-Wei Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Broeck_G/0/1/0/all/0/1">Guy Van den Broeck</a></p>
<p>Neuro-symbolic AI bridges the gap between purely symbolic and neural
approaches to learning. This often requires maximizing the likelihood of a
symbolic constraint w.r.t the neural network's output distribution. Such output
distributions are typically assumed to be fully-factorized. This limits the
applicability of neuro-symbolic learning to the more expressive autoregressive
distributions, e.g., transformers. Under such distributions, computing the
likelihood of even simple constraints is #P-hard. Instead of attempting to
enforce the constraint on the entire output distribution, we propose to do so
on a random, local approximation thereof. More precisely, we optimize the
likelihood of the constraint under a pseudolikelihood-based approximation
centered around a model sample. Our approximation is factorized, allowing the
reuse of solutions to sub-problems, a main tenet for efficiently computing
neuro-symbolic losses. Moreover, it is a local, high-fidelity approximation of
the likelihood, exhibiting low entropy and KL-divergence around the model
sample. We evaluate our approach on Sudoku and shortest-path prediction cast as
autoregressive generation, and observe that we greatly improve upon the base
model's ability to predict logically-consistent outputs. We also evaluate on
the task of detoxifying large language models. Using a simple constraint
disallowing a list of toxic words, we are able to steer the model's outputs
away from toxic generations, achieving SoTA detoxification compared to previous
approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10048">Knowledge Graph Enhanced Aspect-Level Sentiment Analysis. (arXiv:2312.10048v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sharma_K/0/1/0/all/0/1">Kavita Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Patel_R/0/1/0/all/0/1">Ritu Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Iyer_S/0/1/0/all/0/1">Sunita Iyer</a></p>
<p>In this paper, we propose a novel method to enhance sentiment analysis by
addressing the challenge of context-specific word meanings. It combines the
advantages of a BERT model with a knowledge graph based synonym data. This
synergy leverages a dynamic attention mechanism to develop a knowledge-driven
state vector. For classifying sentiments linked to specific aspects, the
approach constructs a memory bank integrating positional data. The data are
then analyzed using a DCGRU to pinpoint sentiment characteristics related to
specific aspect terms. Experiments on three widely used datasets demonstrate
the superior performance of our method in sentiment classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15816">TEILP: Time Prediction over Knowledge Graphs via Logical Reasoning. (arXiv:2312.15816v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiong_S/0/1/0/all/0/1">Siheng Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yuan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Payani_A/0/1/0/all/0/1">Ali Payani</a>, <a href="http://arxiv.org/find/cs/1/au:+Kerce_J/0/1/0/all/0/1">James C Kerce</a>, <a href="http://arxiv.org/find/cs/1/au:+Fekri_F/0/1/0/all/0/1">Faramarz Fekri</a></p>
<p>Conventional embedding-based models approach event time prediction in
temporal knowledge graphs (TKGs) as a ranking problem. However, they often fall
short in capturing essential temporal relationships such as order and distance.
In this paper, we propose TEILP, a logical reasoning framework that naturally
integrates such temporal elements into knowledge graph predictions. We first
convert TKGs into a temporal event knowledge graph (TEKG) which has a more
explicit representation of time in term of nodes of the graph. The TEKG equips
us to develop a differentiable random walk approach to time prediction.
Finally, we introduce conditional probability density functions, associated
with the logical rules involving the query interval, using which we arrive at
the time prediction. We compare TEILP with state-of-the-art methods on five
benchmark datasets. We show that our model achieves a significant improvement
over baselines while providing interpretable explanations. In particular, we
consider several scenarios where training samples are limited, event types are
imbalanced, and forecasting the time of future events based on only past events
is desired. In all these cases, TEILP outperforms state-of-the-art methods in
terms of robustness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02968">Rule-Guided Joint Embedding Learning over Knowledge Graphs. (arXiv:2401.02968v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qisong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Ji Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1">Sijia Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1">Neng Liu</a></p>
<p>Recent studies focus on embedding learning over knowledge graphs, which map
entities and relations in knowledge graphs into low-dimensional vector spaces.
While existing models mainly consider the aspect of graph structure, there
exists a wealth of contextual and literal information that can be utilized for
more effective embedding learning. This paper introduces a novel model that
incorporates both contextual and literal information into entity and relation
embeddings by utilizing graph convolutional networks. Specifically, for
contextual information, we assess its significance through confidence and
relatedness metrics. In addition, a unique rule-based method is developed to
calculate the confidence metric, and the relatedness metric is derived from the
literal information's representations. We validate our model performance with
thorough experiments on two established benchmark datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.03591">Text Classification Based on Knowledge Graphs and Improved Attention Mechanism. (arXiv:2401.03591v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Siyu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1">Chenwei Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xinyi Liu</a></p>
<p>To resolve the semantic ambiguity in texts, we propose a model, which
innovatively combines a knowledge graph with an improved attention mechanism.
An existing knowledge base is utilized to enrich the text with relevant
contextual concepts. The model operates at both character and word levels to
deepen its understanding by integrating the concepts. We first adopt
information gain to select import words. Then an encoder-decoder framework is
used to encode the text along with the related concepts. The local attention
mechanism adjusts the weight of each concept, reducing the influence of
irrelevant or noisy concepts during classification. We improve the calculation
formula for attention scores in the local self-attention mechanism, ensuring
that words with different frequencies of occurrence in the text receive higher
attention scores. Finally, the model employs a Bi-directional Gated Recurrent
Unit (Bi-GRU), which is effective in feature extraction from texts for improved
classification accuracy. Its performance is demonstrated on datasets such as
AGNews, Ohsumed, and TagMyNews, achieving accuracy of 75.1%, 58.7%, and 68.5%
respectively, showing its effectiveness in classifying tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06772">Semantic Parsing for Question Answering over Knowledge Graphs. (arXiv:2401.06772v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1">Sijia Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenwen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qisong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jiang Zhao</a></p>
<p>In this paper, we introduce a novel method with graph-to-segment mapping for
question answering over knowledge graphs, which helps understanding question
utterances. This method centers on semantic parsing, a key approach for
interpreting these utterances. The challenges lie in comprehending implicit
entities, relationships, and complex constraints like time, ordinality, and
aggregation within questions, contextualized by the knowledge graph. Our
framework employs a combination of rule-based and neural-based techniques to
parse and construct highly accurate and comprehensive semantic segment
sequences. These sequences form semantic query graphs, effectively representing
question utterances. We approach question semantic parsing as a sequence
generation task, utilizing an encoder-decoder neural network to transform
natural language questions into semantic segments. Moreover, to enhance the
parsing of implicit entities and relations, we incorporate a graph neural
network that leverages the context of the knowledge graph to better understand
question representations. Our experimental evaluations on two datasets
demonstrate the effectiveness and superior performance of our model in semantic
parsing for question answering.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.10244">Knowledge Graph Driven Recommendation System Algorithm. (arXiv:2401.10244v2 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chaoyang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yanan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1">Siwei Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wei Li</a></p>
<p>In this paper, we propose a novel graph neural network-based recommendation
model called KGLN, which leverages Knowledge Graph (KG) information to enhance
the accuracy and effectiveness of personalized recommendations. We first use a
single-layer neural network to merge individual node features in the graph, and
then adjust the aggregation weights of neighboring entities by incorporating
influence factors. The model evolves from a single layer to multiple layers
through iteration, enabling entities to access extensive multi-order associated
entity information. The final step involves integrating features of entities
and users to produce a recommendation score. The model performance was
evaluated by comparing its effects on various aggregation methods and influence
factors. In tests over the MovieLen-1M and Book-Crossing datasets, KGLN shows
an Area Under the ROC curve (AUC) improvement of 0.3% to 5.9% and 1.1% to 8.2%,
respectively, which is better than existing benchmark methods like LibFM,
DeepFM, Wide&amp;Deep, and RippleNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.10711">Weakly Supervised Gaussian Contrastive Grounding with Large Multimodal Models for Video Question Answering. (arXiv:2401.10711v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haibo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1">Chenghang Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yixuan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1">Weifeng Ge</a></p>
<p>Video Question Answering (VideoQA) aims to answer natural language questions
based on the information observed in videos. Despite the recent success of
Large Multimodal Models (LMMs) in image-language understanding and reasoning,
they deal with VideoQA insufficiently by simply taking uniformly sampled frames
as visual inputs, which ignores question-relevant visual clues. Moreover, there
are no human annotations for question-critical timestamps in existing VideoQA
datasets. In light of this, we propose a novel weakly supervised framework to
enforce the LMMs to reason out the answers with question-critical moments as
visual inputs. Specifically, we fuse the question and answer pairs as event
descriptions to find multiple keyframes as target moments, which will be
pseudo-labels. With these pseudo-labels as additionally weak supervision, we
devise a lightweight Gaussian-based Contrastive Grounding (GCG) module. GCG
learns multiple Gaussian functions to characterize the temporal structure of
the video, and sample question-critical frames as positive moments to be the
visual inputs of LMMs. Extensive experiments on several VideoQA benchmarks
verify the effectiveness of our framework, and we achieve substantial
improvements compared to previous state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.10768">Mitigating Hallucinations of Large Language Models via Knowledge Consistent Alignment. (arXiv:2401.10768v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wan_F/0/1/0/all/0/1">Fanqi Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xinting Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1">Leyang Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Quan_X/0/1/0/all/0/1">Xiaojun Quan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bi_W/0/1/0/all/0/1">Wei Bi</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1">Shuming Shi</a></p>
<p>While Large Language Models (LLMs) have proven to be exceptional on a variety
of tasks after alignment, they may still produce responses that contradict the
context or world knowledge confidently, a phenomenon known as
``hallucination''. In this paper, we demonstrate that reducing the
inconsistency between the external knowledge encapsulated in the training data
and the intrinsic knowledge inherited in the pretraining corpus could mitigate
hallucination in alignment. Specifically, we introduce a novel knowledge
consistent alignment (KCA) approach, which involves automatically formulating
examinations based on external knowledge for accessing the comprehension of
LLMs. For data encompassing knowledge inconsistency, KCA implements several
simple yet efficient strategies for processing. We illustrate the superior
performance of the proposed KCA approach in mitigating hallucinations across
six benchmarks using LLMs of different backbones and scales. Furthermore, we
confirm the correlation between knowledge inconsistency and hallucination,
signifying the effectiveness of reducing knowledge inconsistency in alleviating
hallucinations. Our code, model weights, and data are public at
\url{https://github.com/fanqiwan/KCA}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.10893">Location Sensitive Embedding for Knowledge Graph Reasoning. (arXiv:2401.10893v2 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Banerjee_D/0/1/0/all/0/1">Deepak Banerjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Ishaan_A/0/1/0/all/0/1">Anjali Ishaan</a></p>
<p>Embedding methods transform the knowledge graph into a continuous,
low-dimensional space, facilitating inference and completion tasks. Existing
methods are mainly divided into two types: translational distance models and
semantic matching models. A key challenge in translational distance models is
their inability to effectively differentiate between 'head' and 'tail' entities
in graphs. To address this problem, a novel location-sensitive embedding (LSE)
method has been developed. LSE innovatively modifies the head entity using
relation-specific mappings, conceptualizing relations as linear transformations
rather than mere translations. The theoretical foundations of LSE, including
its representational capabilities and its connections to existing models, have
been thoroughly examined. A more streamlined variant, LSE-d, which employs a
diagonal matrix for transformations to enhance practical efficiency, is also
proposed. Experiments conducted on four large-scale KG datasets for link
prediction show that LSEd either outperforms or is competitive with
state-of-the-art related works.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11864">Improving Small Language Models&#x27; Mathematical Reasoning via Equation-of-Thought Distillation. (arXiv:2401.11864v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xunyu Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jian Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1">Can Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Weiping Wang</a></p>
<p>This work addresses the challenge of democratizing advanced Large Language
Models (LLMs) by compressing their mathematical reasoning capabilities into
sub-billion parameter Small Language Models (SLMs) without compromising
performance. We introduce Equation-of-Thought Distillation (EoTD), a novel
technique that encapsulates the reasoning process into equation-based
representations to construct an EoTD dataset for fine-tuning SLMs.
Additionally, we propose the Ensemble Thoughts Distillation (ETD) framework to
enhance the reasoning performance of SLMs. This involves creating a reasoning
dataset with multiple thought processes, including Chain-of-Thought (CoT),
Program-of-Thought (PoT), and Equation-of-Thought (EoT), and using it for
fine-tuning. Our experimental findings demonstrate that EoTD significantly
boosts the reasoning abilities of SLMs, while ETD enables these models to
achieve state-of-the-art reasoning performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13165">Misgendering and Assuming Gender in Machine Translation when Working with Low-Resource Languages. (arXiv:2401.13165v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1">Sourojit Ghosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Chatterjee_S/0/1/0/all/0/1">Srishti Chatterjee</a></p>
<p>This chapter focuses on gender-related errors in machine translation (MT) in
the context of low-resource languages. We begin by explaining what low-resource
languages are, examining the inseparable social and computational factors that
create such linguistic hierarchies. We demonstrate through a case study of our
mother tongue Bengali, a global language spoken by almost 300 million people
but still classified as low-resource, how gender is assumed and inferred in
translations to and from the high(est)-resource English when no such
information is provided in source texts. We discuss the postcolonial and
societal impacts of such errors leading to linguistic erasure and
representational harms, and conclude by discussing potential solutions towards
uplifting languages by providing them more agency in MT conversations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13275">Can AI Assistants Know What They Don&#x27;t Know?. (arXiv:2401.13275v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1">Qinyuan Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1">Tianxiang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiangyang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1">Zhangyue Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shimin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Linyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1">Zhengfu He</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Kai Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1">Xipeng Qiu</a></p>
<p>Recently, AI assistants based on large language models (LLMs) show surprising
performance in many tasks, such as dialogue, solving math problems, writing
code, and using tools. Although LLMs possess intensive world knowledge, they
still make factual errors when facing some knowledge intensive tasks, like
open-domain question answering. These untruthful responses from the AI
assistant may cause significant risks in practical applications. We believe
that an AI assistant's refusal to answer questions it does not know is a
crucial method for reducing hallucinations and making the assistant truthful.
Therefore, in this paper, we ask the question "Can AI assistants know what they
don't know and express them through natural language?" To answer this question,
we construct a model-specific "I don't know" (Idk) dataset for an assistant,
which contains its known and unknown questions, based on existing open-domain
question answering datasets. Then we align the assistant with its corresponding
Idk dataset and observe whether it can refuse to answer its unknown questions
after alignment. Experimental results show that after alignment with Idk
datasets, the assistant can refuse to answer most its unknown questions. For
questions they attempt to answer, the accuracy is significantly higher than
before the alignment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13565">Large Malaysian Language Model Based on Mistral for Enhanced Local Language Understanding. (arXiv:2401.13565v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zolkepli_H/0/1/0/all/0/1">Husein Zolkepli</a>, <a href="http://arxiv.org/find/cs/1/au:+Razak_A/0/1/0/all/0/1">Aisyah Razak</a>, <a href="http://arxiv.org/find/cs/1/au:+Adha_K/0/1/0/all/0/1">Kamarul Adha</a>, <a href="http://arxiv.org/find/cs/1/au:+Nazhan_A/0/1/0/all/0/1">Ariff Nazhan</a></p>
<p>In this paper, we present significant advancements in the pretraining of
Mistral 7B, a large-scale language model, using a dataset of 32.6 GB,
equivalent to 1.1 billion tokens. We explore the impact of extending the
context length, releasing models with context lengths of 4096 and 32768 tokens,
and further refining performance with a specialized 16384 context length
instruction-tuned model, we called it Malaysian Mistral.
</p>
<p>Our experiments demonstrate the efficacy of continue pretraining and the
influence of extended context lengths on Mistral 7B's language understanding
capabilities. Additionally, we release a model specifically tuned with a 16384
context length instruction, showcasing its potential for capturing nuanced
language intricacies.
</p>
<p>Furthermore, our research contributes to the benchmarking of Malaysian
Mistral against prominent language models, including ChatGPT3.5 and Claude 2.
We present compelling results indicating Malaysian Mistral's superior
performance on Tatabahasa (Malay grammar) test set, particularly when
fine-tuned with instructions.
</p>
<p>All models released at
https://huggingface.co/collections/mesolitica/malaysian-mistral-7b-6528f2ec825f4bba46c1700c
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13802">Investigating the Efficacy of Large Language Models for Code Clone Detection. (arXiv:2401.13802v2 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khajezade_M/0/1/0/all/0/1">Mohamad Khajezade</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jie JW Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fard_F/0/1/0/all/0/1">Fatemeh Hendijani Fard</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_Perez_G/0/1/0/all/0/1">Gema Rodr&#xed;guez-P&#xe9;rez</a>, <a href="http://arxiv.org/find/cs/1/au:+Shehata_M/0/1/0/all/0/1">Mohamed Sami Shehata</a></p>
<p>Large Language Models (LLMs) have demonstrated remarkable success in various
natural language processing and software engineering tasks, such as code
generation. The LLMs are mainly utilized in the prompt-based zero/few-shot
paradigm to guide the model in accomplishing the task. GPT-based models are one
of the popular ones studied for tasks such as code comment generation or test
generation. These tasks are `generative' tasks. However, there is limited
research on the usage of LLMs for `non-generative' tasks such as classification
using the prompt-based paradigm. In this preliminary exploratory study, we
investigated the applicability of LLMs for Code Clone Detection (CCD), a
non-generative task. By building a mono-lingual and cross-lingual CCD dataset
derived from CodeNet, we first investigated two different prompts using ChatGPT
to detect Type-4 code clones in Java-Java and Java-Ruby pairs in a zero-shot
setting. We then conducted an analysis to understand the strengths and
weaknesses of ChatGPT in CCD. ChatGPT surpasses the baselines in cross-language
CCD attaining an F1-score of 0.877 and achieves comparable performance to fully
fine-tuned models for mono-lingual CCD, with an F1-score of 0.878. Also, the
prompt and the difficulty level of the problems has an impact on the
performance of ChatGPT. Finally we provide insights and future directions based
on our initial analysis
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13919">WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models. (arXiv:2401.13919v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1">Hongliang He</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1">Wenlin Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1">Kaixin Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1">Wenhao Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1">Yong Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1">Zhenzhong Lan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1">Dong Yu</a></p>
<p>The advancement of large language models (LLMs) leads to a new era marked by
the development of autonomous applications in the real world, which drives
innovation in the creation of advanced web-based agents. Existing web agents
typically only handle one input modality and are evaluated only in simplified
web simulators or static web snapshots, greatly limiting their applicability in
real-world scenarios. To bridge this gap, we introduce WebVoyager, an
innovative Large Multimodal Model (LMM) powered web agent that can complete
user instructions end-to-end by interacting with real-world websites. Moreover,
we propose a new evaluation protocol for web agents to address the challenges
of automatic evaluation of open-ended web agent tasks, leveraging the robust
multimodal comprehension capabilities of GPT-4V. We create a new benchmark by
gathering real-world tasks from 15 widely used websites to evaluate our agents.
We show that WebVoyager achieves a 55.7% task success rate, significantly
surpassing the performance of both GPT-4 (All Tools) and the WebVoyager
(text-only) setups, underscoring the exceptional capability of WebVoyager in
practical applications. We found that our proposed automatic evaluation
achieves 85.3% agreement with human judgment, paving the way for further
development of web agents in a real-world setting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14166">BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on Few-shot Inference via Debiased Domain Abstraction. (arXiv:2401.14166v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiangmeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_F/0/1/0/all/0/1">Fei Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1">Yifan Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiang_W/0/1/0/all/0/1">Wenwen Qiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1">Changwen Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1">Fuchun Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1">Hui Xiong</a></p>
<p>As a novel and effective fine-tuning paradigm based on large-scale
pre-trained language models (PLMs), prompt-tuning aims to reduce the gap
between downstream tasks and pre-training objectives. While prompt-tuning has
yielded continuous advancements in various tasks, such an approach still
remains a persistent defect: prompt-tuning methods fail to generalize to
specific few-shot patterns. From the perspective of distribution analyses, we
disclose that the intrinsic issues behind the phenomenon are the
over-multitudinous conceptual knowledge contained in PLMs and the abridged
knowledge for target downstream domains, which jointly result in that PLMs
mis-locate the knowledge distributions corresponding to the target domains in
the universal knowledge embedding space. To this end, we intuitively explore to
approximate the unabridged target domains of downstream tasks in a debiased
manner, and then abstract such domains to generate discriminative prompts,
thereby providing the de-ambiguous guidance for PLMs. Guided by such an
intuition, we propose a simple yet effective approach, namely BayesPrompt, to
learn prompts that contain the domain discriminative information against the
interference from domain-irrelevant knowledge. BayesPrompt primitively
leverages known distributions to approximate the debiased factual distributions
of target domains and further uniformly samples certain representative features
from the approximated distributions to generate the ultimate prompts for PLMs.
We provide theoretical insights with the connection to domain adaptation.
Empirically, our method achieves state-of-the-art performance on benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14215">Commonsense-augmented Memory Construction and Management in Long-term Conversations via Context-aware Persona Refinement. (arXiv:2401.14215v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">Hana Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Ong_K/0/1/0/all/0/1">Kai Tzu-iunn Ong</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Seoyeon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1">Dongha Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Yeo_J/0/1/0/all/0/1">Jinyoung Yeo</a></p>
<p>Memorizing and utilizing speakers' personas is a common practice for response
generation in long-term conversations. Yet, human-authored datasets often
provide uninformative persona sentences that hinder response quality. This
paper presents a novel framework that leverages commonsense-based persona
expansion to address such issues in long-term conversation. While prior work
focuses on not producing personas that contradict others, we focus on
transforming contradictory personas into sentences that contain rich speaker
information, by refining them based on their contextual backgrounds with
designed strategies. As the pioneer of persona expansion in multi-session
settings, our framework facilitates better response generation via human-like
persona refinement. The supplementary video of our work is available at
https://caffeine-15bbf.web.app/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14680">MaLLaM -- Malaysia Large Language Model. (arXiv:2401.14680v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zolkepli_H/0/1/0/all/0/1">Husein Zolkepli</a>, <a href="http://arxiv.org/find/cs/1/au:+Razak_A/0/1/0/all/0/1">Aisyah Razak</a>, <a href="http://arxiv.org/find/cs/1/au:+Adha_K/0/1/0/all/0/1">Kamarul Adha</a>, <a href="http://arxiv.org/find/cs/1/au:+Nazhan_A/0/1/0/all/0/1">Ariff Nazhan</a></p>
<p>Addressing the gap in Large Language Model pretrained from scratch with
Malaysian context, We trained models with 1.1 billion, 3 billion, and 5 billion
parameters on a substantial 349GB dataset, equivalent to 90 billion tokens
based on our pretrained Byte Pair Encoding (BPE) tokenizer for a single epoch.
MaLLaM contributes to enhanced natural language understanding and generation
tasks in the Malay language. Although trained on a smaller dataset of 90
billion tokens, our instruction-tuned MaLLaM models perform competitively. When
compared to ChatGPT3.5 and Malaysian Mistral, MaLLaM's instruction-tuned models
demonstrate notable proficiency, underscoring the effectiveness of our approach
in capturing and understanding the nuances of the Malaysian language. MaLLaM
models mark a significant contribution to the field, providing comprehensive
language representations grounded in Malaysian context. This endeavor aims to
pave the way for enhanced natural language understanding and generation tasks
specific to the linguistic nuances present in Malaysia. We discuss the training
methodology, dataset composition, and the potential impact of MaLLaM in
advancing the capabilities of large language models within the context of the
Malay language.
</p>
<p>All models released at
https://huggingface.co/collections/mesolitica/mallam-6577b59d1e0b436ae75f930f
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14887">The Power of Noise: Redefining Retrieval for RAG Systems. (arXiv:2401.14887v2 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cuconasu_F/0/1/0/all/0/1">Florin Cuconasu</a>, <a href="http://arxiv.org/find/cs/1/au:+Trappolini_G/0/1/0/all/0/1">Giovanni Trappolini</a>, <a href="http://arxiv.org/find/cs/1/au:+Siciliano_F/0/1/0/all/0/1">Federico Siciliano</a>, <a href="http://arxiv.org/find/cs/1/au:+Filice_S/0/1/0/all/0/1">Simone Filice</a>, <a href="http://arxiv.org/find/cs/1/au:+Campagnano_C/0/1/0/all/0/1">Cesare Campagnano</a>, <a href="http://arxiv.org/find/cs/1/au:+Maarek_Y/0/1/0/all/0/1">Yoelle Maarek</a>, <a href="http://arxiv.org/find/cs/1/au:+Tonellotto_N/0/1/0/all/0/1">Nicola Tonellotto</a>, <a href="http://arxiv.org/find/cs/1/au:+Silvestri_F/0/1/0/all/0/1">Fabrizio Silvestri</a></p>
<p>Retrieval-Augmented Generation (RAG) systems represent a significant
advancement over traditional Large Language Models (LLMs). RAG systems enhance
their generation ability by incorporating external data retrieved through an
Information Retrieval (IR) phase, overcoming the limitations of standard LLMs,
which are restricted to their pre-trained knowledge and limited context window.
Most research in this area has predominantly concentrated on the generative
aspect of LLMs within RAG systems. Our study fills this gap by thoroughly and
critically analyzing the influence of IR components on RAG systems. This paper
analyzes which characteristics a retriever should possess for an effective
RAG's prompt formulation, focusing on the type of documents that should be
retrieved. We evaluate various elements, such as the relevance of the documents
to the prompt, their position, and the number included in the context. Our
findings reveal, among other insights, that including irrelevant documents can
unexpectedly enhance performance by more than 30% in accuracy, contradicting
our initial assumption of diminished quality. These results underscore the need
for developing specialized strategies to integrate retrieval with language
generation models, thereby laying the groundwork for future research in this
field.
</p>
</p>
</div>

    </div>
    </body>
    