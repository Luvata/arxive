<!DOCTYPE html>
<html>
<head>
<title>2023-12-02-cs-lg</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2311.17924">Unrolling Virtual Worlds for Immersive Experiences. (arXiv:2311.17924v1 [cs.GR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1">Alexey Tikhonov</a>, <a href="http://arxiv.org/find/cs/1/au:+Repushko_A/0/1/0/all/0/1">Anton Repushko</a></p>
<p>This research pioneers a method for generating immersive worlds, drawing
inspiration from elements of vintage adventure games like Myst and employing
modern text-to-image models. We explore the intricate conversion of 2D
panoramas into 3D scenes using equirectangular projections, addressing the
distortions in perception that occur as observers navigate within the
encompassing sphere. Our approach employs a technique similar to "inpainting"
to rectify distorted projections, enabling the smooth construction of locally
coherent worlds. This provides extensive insight into the interrelation of
technology, perception, and experiential reality within human-computer
interaction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17929">New Online Communities: Graph Deep Learning on Anonymous Voting Networks to Identify Sybils in Polycentric Governance. (arXiv:2311.17929v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+DuPont_Q/0/1/0/all/0/1">Quinn DuPont</a></p>
<p>This research examines the polycentric governance of digital assets in
Decentralized Autonomous Organizations (DAOs). It offers a theoretical
framework and addresses a critical challenge facing decentralized governance by
developing a method to identify sybils, or spurious identities. The method uses
graph deep learning techniques to identify sybil activity in a DAO governance
dataset (snapshot.org). Specifically, a Graph Convolutional Neural Network
(GCNN) learned voting behaviours and a fast k-means vector clustering algorithm
(FAISS) used the high dimensional embeddings to identify similar nodes in a
graph. The results reveal that deep learning can effectively identify sybils,
reducing the voting graph by 2-5%. This research underscores the importance of
sybil resistance in DAOs and offers a novel perspective on decentralized
governance, informing future policy, regulation, and governance practices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17932">Generating Molecular Conformer Fields. (arXiv:2311.17932v1 [physics.chem-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Wang_Y/0/1/0/all/0/1">Yuyang Wang</a>, <a href="http://arxiv.org/find/physics/1/au:+Elhag_A/0/1/0/all/0/1">Ahmed A. Elhag</a>, <a href="http://arxiv.org/find/physics/1/au:+Jaitly_N/0/1/0/all/0/1">Navdeep Jaitly</a>, <a href="http://arxiv.org/find/physics/1/au:+Susskind_J/0/1/0/all/0/1">Joshua M. Susskind</a>, <a href="http://arxiv.org/find/physics/1/au:+Bautista_M/0/1/0/all/0/1">Miguel Angel Bautista</a></p>
<p>In this paper we tackle the problem of generating conformers of a molecule in
3D space given its molecular graph. We parameterize these conformers as
continuous functions that map elements from the molecular graph to points in 3D
space. We then formulate the problem of learning to generate conformers as
learning a distribution over these functions using a diffusion generative
model, called Molecular Conformer Fields (MCF). Our approach is simple and
scalable, and achieves state-of-the-art performance on challenging molecular
conformer generation benchmarks while making no assumptions about the explicit
structure of molecules (e.g. modeling torsional angles). MCF represents an
advance in extending diffusion models to handle complex scientific problems in
a conceptually simple, scalable and effective manner.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17941">Advancing Attack-Resilient Scheduling of Integrated Energy Systems with Demand Response via Deep Reinforcement Learning. (arXiv:2311.17941v1 [eess.SY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1">Yang Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Ma_W/0/1/0/all/0/1">Wenjie Ma</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1">Yuanzheng Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1">Sen Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1">Zhe Chen</a></p>
<p>Optimally scheduling multi-energy flow is an effective method to utilize
renewable energy sources (RES) and improve the stability and economy of
integrated energy systems (IES). However, the stable demand-supply of IES faces
challenges from uncertainties that arise from RES and loads, as well as the
increasing impact of cyber-attacks with advanced information and communication
technologies adoption. To address these challenges, this paper proposes an
innovative model-free resilience scheduling method based on state-adversarial
deep reinforcement learning (DRL) for integrated demand response (IDR)-enabled
IES. The proposed method designs an IDR program to explore the interaction
ability of electricity-gas-heat flexible loads. Additionally, a
state-adversarial Markov decision process (SA-MDP) model characterizes the
energy scheduling problem of IES under cyber-attack. The state-adversarial soft
actor-critic (SA-SAC) algorithm is proposed to mitigate the impact of
cyber-attacks on the scheduling strategy. Simulation results demonstrate that
our method is capable of adequately addressing the uncertainties resulting from
RES and loads, mitigating the impact of cyber-attacks on the scheduling
strategy, and ensuring a stable demand supply for various energy sources.
Moreover, the proposed method demonstrates resilience against cyber-attacks.
Compared to the original soft actor-critic (SAC) algorithm, it achieves a 10\%
improvement in economic performance under cyber-attack scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17943">LayerCollapse: Adaptive compression of neural networks. (arXiv:2311.17943v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shabgahi_S/0/1/0/all/0/1">Soheil Zibakhsh Shabgahi</a>, <a href="http://arxiv.org/find/cs/1/au:+Shariff_M/0/1/0/all/0/1">Mohammad Soheil Shariff</a>, <a href="http://arxiv.org/find/cs/1/au:+Koushanfar_F/0/1/0/all/0/1">Farinaz Koushanfar</a></p>
<p>Handling the ever-increasing scale of contemporary deep learning and
transformer-based models poses a significant challenge. Although great strides
have been made in optimizing model compression techniques such as model
architecture search and knowledge distillation, the availability of data and
computational resources remains a considerable hurdle for these optimizations.
This paper introduces LayerCollapse, a novel alternative adaptive model
compression methodology. LayerCollapse works by eliminating non-linearities
within the network and collapsing two consecutive fully connected layers into a
single linear transformation. This approach simultaneously reduces both the
number of layers and the parameter count, thereby enhancing model efficiency.
We also introduce a compression aware regularizer, which compresses the model
in alignment with the dataset quality and model expressiveness, consequently
reducing overfitting across tasks. Our results demonstrate LayerCollapse's
effective compression and regularization capabilities in multiple fine-grained
classification benchmarks, achieving up to 74% post training compression with
minimal accuracy loss. We compare this method with knowledge distillation on
the same target network, showcasing a five-fold increase in computational
efficiency and 8% improvement in overall accuracy on the ImageNet dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17951">C3Net: Compound Conditioned ControlNet for Multimodal Content Generation. (arXiv:2311.17951v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Juntao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuehuai Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1">Yu-Wing Tai</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1">Chi-Keung Tang</a></p>
<p>We present Compound Conditioned ControlNet, C3Net, a novel generative neural
architecture taking conditions from multiple modalities and synthesizing
multimodal contents simultaneously (e.g., image, text, audio). C3Net adapts the
ControlNet architecture to jointly train and make inferences on a
production-ready diffusion model and its trainable copies. Specifically, C3Net
first aligns the conditions from multi-modalities to the same semantic latent
space using modality-specific encoders based on contrastive training. Then, it
generates multimodal outputs based on the aligned latent space, whose semantic
information is combined using a ControlNet-like architecture called Control
C3-UNet. Correspondingly, with this system design, our model offers an improved
solution for joint-modality generation through learning and explaining
multimodal conditions instead of simply taking linear interpolations on the
latent space. Meanwhile, as we align conditions to a unified latent space,
C3Net only requires one trainable Control C3-UNet to work on multimodal
semantic information. Furthermore, our model employs unimodal pretraining on
the condition alignment stage, outperforming the non-pretrained alignment even
on relatively scarce training data and thus demonstrating high-quality compound
condition generation. We contribute the first high-quality tri-modal validation
set to validate quantitatively that C3Net outperforms or is on par with first
and contemporary state-of-the-art multimodal generation. Our codes and
tri-modal dataset will be released.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17956">QuadraNet: Improving High-Order Neural Interaction Efficiency with Hardware-Aware Quadratic Neural Networks. (arXiv:2311.17956v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Chenhui Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1">Fuxun Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zirui Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chenchen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1">Jinjun Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xiang Chen</a></p>
<p>Recent progress in computer vision-oriented neural network designs is mostly
driven by capturing high-order neural interactions among inputs and features.
And there emerged a variety of approaches to accomplish this, such as
Transformers and its variants. However, these interactions generate a large
amount of intermediate state and/or strong data dependency, leading to
considerable memory consumption and computing cost, and therefore compromising
the overall runtime performance. To address this challenge, we rethink the
high-order interactive neural network design with a quadratic computing
approach. Specifically, we propose QuadraNet -- a comprehensive model design
methodology from neuron reconstruction to structural block and eventually to
the overall neural network implementation. Leveraging quadratic neurons'
intrinsic high-order advantages and dedicated computation optimization schemes,
QuadraNet could effectively achieve optimal cognition and computation
performance. Incorporating state-of-the-art hardware-aware neural architecture
search and system integration techniques, QuadraNet could also be well
generalized in different hardware constraint settings and deployment scenarios.
The experiment shows thatQuadraNet achieves up to 1.5$\times$ throughput, 30%
less memory footprint, and similar cognition performance, compared with the
state-of-the-art high-order approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17958">CommunityAI: Towards Community-based Federated Learning. (arXiv:2311.17958v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Murturi_I/0/1/0/all/0/1">Ilir Murturi</a>, <a href="http://arxiv.org/find/cs/1/au:+Donta_P/0/1/0/all/0/1">Praveen Kumar Donta</a>, <a href="http://arxiv.org/find/cs/1/au:+Dustdar_S/0/1/0/all/0/1">Schahram Dustdar</a></p>
<p>Federated Learning (FL) has emerged as a promising paradigm to train machine
learning models collaboratively while preserving data privacy. However, its
widespread adoption faces several challenges, including scalability,
heterogeneous data and devices, resource constraints, and security concerns.
Despite its promise, FL has not been specifically adapted for community
domains, primarily due to the wide-ranging differences in data types and
context, devices and operational conditions, environmental factors, and
stakeholders. In response to these challenges, we present a novel framework for
Community-based Federated Learning called CommunityAI. CommunityAI enables
participants to be organized into communities based on their shared interests,
expertise, or data characteristics. Community participants collectively
contribute to training and refining learning models while maintaining data and
participant privacy within their respective groups. Within this paper, we
discuss the conceptual architecture, system requirements, processes, and future
challenges that must be solved. Finally, our goal within this paper is to
present our vision regarding enabling a collaborative learning process within
various communities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17959">Transformer Based Model for Predicting Rapid Impact Compaction Outcomes: A Case Study of Utapao International Airport. (arXiv:2311.17959v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Youwai_S/0/1/0/all/0/1">Sompote Youwai</a>, <a href="http://arxiv.org/find/cs/1/au:+Detcheewa_S/0/1/0/all/0/1">Sirasak Detcheewa</a></p>
<p>This paper introduces a novel deep learning approach to predict the
engineering properties of the ground improved by Rapid Impact Compaction (RIC),
which is a ground improvement technique that uses a drop hammer to compact the
soil and fill layers. The proposed approach uses transformer-based neural
networks to capture the complex nonlinear relationships between the input
features, such as the hammer energy, drop height, and number of blows, and the
output variables, such as the cone resistance. The approach is applied to a
real-world dataset from a trial test section for the new apron construction of
the Utapao International Airport in Thailand. The results show that the
proposed approach outperforms the existing methods in terms of prediction
accuracy and efficiency and provides interpretable attention maps that reveal
the importance of different features for RIC prediction. The paper also
discusses the limitations and future directions of applying deep learning
methods to RIC prediction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17961">Skilful Precipitation Nowcasting Using NowcastNet. (arXiv:2311.17961v1 [physics.ao-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Kumar_A/0/1/0/all/0/1">Ajitabh Kumar</a></p>
<p>Designing early warning system for precipitation requires accurate short-term
forecasting system. Climate change has led to an increase in frequency of
extreme weather events, and hence such systems can prevent disasters and loss
of life. Managing such events remain a challenge for both public and private
institutions. Precipitation nowcasting can help relevant institutions to better
prepare for such events as they impact agriculture, transport, public health
and safety, etc. Physics-based numerical weather prediction (NWP) is unable to
perform well for nowcasting because of large computational turn-around time.
Deep-learning based models on the other hand are able to give predictions
within seconds. We use recently proposed NowcastNet, a physics-conditioned deep
generative network, to forecast precipitation for different regions of Europe
using satellite images. Both spatial and temporal transfer learning is done by
forecasting for the unseen regions and year. Model makes realistic predictions
and is able to outperform baseline for such a prediction task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17964">Linear normalised hash function for clustering gene sequences and identifying reference sequences from multiple sequence alignments. (arXiv:2311.17964v1 [q-bio.GN])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Helal_M/0/1/0/all/0/1">Manal Helal</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Kong_F/0/1/0/all/0/1">Fanrong Kong</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Chen_S/0/1/0/all/0/1">Sharon C-A Chen</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Zhou_F/0/1/0/all/0/1">Fei Zhou</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Dwyer_D/0/1/0/all/0/1">Dominic E Dwyer</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Potter_J/0/1/0/all/0/1">John Potter</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Sintchenko_V/0/1/0/all/0/1">Vitali Sintchenko</a></p>
<p>The aim of this study was to develop a method that would identify the cluster
centroids and the optimal number of clusters for a given sensitivity level and
could work equally well for the different sequence datasets. A novel method
that combines the linear mapping hash function and multiple sequence alignment
(MSA) was developed. This method takes advantage of the already sorted by
similarity sequences from the MSA output, and identifies the optimal number of
clusters, clusters cut-offs, and clusters centroids that can represent
reference gene vouchers for the different species. The linear mapping hash
function can map an already ordered by similarity distance matrix to indices to
reveal gaps in the values around which the optimal cut-offs of the different
clusters can be identified. The method was evaluated using sets of closely
related (16S rRNA gene sequences of Nocardia species) and highly variable (VP1
genomic region of Enterovirus 71) sequences and outperformed existing
unsupervised machine learning clustering methods and dimensionality reduction
methods. This method does not require prior knowledge of the number of clusters
or the distance between clusters, handles clusters of different sizes and
shapes, and scales linearly with the dataset. The combination of MSA with the
linear mapping hash function is a computationally efficient way of gene
sequence clustering and can be a valuable tool for the assessment of
similarity, clustering of different microbial genomes, identifying reference
sequences, and for the study of evolution of bacteria and viruses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17965">Defining Reference Sequences for Nocardia Species by Similarity and Clustering Analyses of 16S rRNA Gene Sequence Data. (arXiv:2311.17965v1 [q-bio.GN])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Helal_M/0/1/0/all/0/1">Manal Helal</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Kong_F/0/1/0/all/0/1">Fanrong Kong</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Chen_S/0/1/0/all/0/1">Sharon C. A. Chen</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Bain_M/0/1/0/all/0/1">Michael Bain</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Christen_R/0/1/0/all/0/1">Richard Christen</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Sintchenko_V/0/1/0/all/0/1">Vitali Sintchenko</a></p>
<p>The intra- and inter-species genetic diversity of bacteria and the absence of
'reference', or the most representative, sequences of individual species
present a significant challenge for sequence-based identification. The aims of
this study were to determine the utility, and compare the performance of
several clustering and classification algorithms to identify the species of 364
sequences of 16S rRNA gene with a defined species in GenBank, and 110 sequences
of 16S rRNA gene with no defined species, all within the genus Nocardia. A
total of 364 16S rRNA gene sequences of Nocardia species were studied. In
addition, 110 16S rRNA gene sequences assigned only to the Nocardia genus level
at the time of submission to GenBank were used for machine learning
classification experiments. Different clustering algorithms were compared with
a novel algorithm or the linear mapping (LM) of the distance matrix. Principal
Components Analysis was used for the dimensionality reduction and
visualization. Results: The LM algorithm achieved the highest performance and
classified the set of 364 16S rRNA sequences into 80 clusters, the majority of
which (83.52%) corresponded with the original species. The most representative
16S rRNA sequences for individual Nocardia species have been identified as
'centroids' in respective clusters from which the distances to all other
sequences were minimized; 110 16S rRNA gene sequences with identifications
recorded only at the genus level were classified using machine learning
methods. Simple kNN machine learning demonstrated the highest performance and
classified Nocardia species sequences with an accuracy of 92.7% and a mean
frequency of 0.578.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17967">Discovering Galaxy Features via Dataset Distillation. (arXiv:2311.17967v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guan_H/0/1/0/all/0/1">Haowen Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xuan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zishi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhiyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Kempe_J/0/1/0/all/0/1">Julia Kempe</a></p>
<p>In many applications, Neural Nets (NNs) have classification performance on
par or even exceeding human capacity. Moreover, it is likely that NNs leverage
underlying features that might differ from those humans perceive to classify.
Can we "reverse-engineer" pertinent features to enhance our scientific
understanding? Here, we apply this idea to the notoriously difficult task of
galaxy classification: NNs have reached high performance for this task, but
what does a neural net (NN) "see" when it classifies galaxies? Are there
morphological features that the human eye might overlook that could help with
the task and provide new insights? Can we visualize tracers of early evolution,
or additionally incorporated spectral data? We present a novel way to summarize
and visualize galaxy morphology through the lens of neural networks, leveraging
Dataset Distillation, a recent deep-learning methodology with the primary
objective to distill knowledge from a large dataset and condense it into a
compact synthetic dataset, such that a model trained on this synthetic dataset
achieves performance comparable to a model trained on the full dataset. We
curate a class-balanced, medium-size high-confidence version of the Galaxy Zoo
2 dataset, and proceed with dataset distillation from our accurate
NN-classifier to create synthesized prototypical images of galaxy morphological
features, demonstrating its effectiveness. Of independent interest, we
introduce a self-adaptive version of the state-of-the-art Matching Trajectory
algorithm to automate the distillation process, and show enhanced performance
on computer vision benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17968">Latent Alignment with Deep Set EEG Decoders. (arXiv:2311.17968v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Bakas_S/0/1/0/all/0/1">Stylianos Bakas</a>, <a href="http://arxiv.org/find/eess/1/au:+Ludwig_S/0/1/0/all/0/1">Siegfried Ludwig</a>, <a href="http://arxiv.org/find/eess/1/au:+Adamos_D/0/1/0/all/0/1">Dimitrios A. Adamos</a>, <a href="http://arxiv.org/find/eess/1/au:+Laskaris_N/0/1/0/all/0/1">Nikolaos Laskaris</a>, <a href="http://arxiv.org/find/eess/1/au:+Panagakis_Y/0/1/0/all/0/1">Yannis Panagakis</a>, <a href="http://arxiv.org/find/eess/1/au:+Zafeiriou_S/0/1/0/all/0/1">Stefanos Zafeiriou</a></p>
<p>The variability in EEG signals between different individuals poses a
significant challenge when implementing brain-computer interfaces (BCI).
Commonly proposed solutions to this problem include deep learning models, due
to their increased capacity and generalization, as well as explicit domain
adaptation techniques. Here, we introduce the Latent Alignment method that won
the Benchmarks for EEG Transfer Learning (BEETL) competition and present its
formulation as a deep set applied on the set of trials from a given subject.
Its performance is compared to recent statistical domain adaptation techniques
under various conditions. The experimental paradigms include motor imagery
(MI), oddball event-related potentials (ERP) and sleep stage classification,
where different well-established deep learning models are applied on each task.
Our experimental results show that performing statistical distribution
alignment at later stages in a deep learning model is beneficial to the
classification accuracy, yielding the highest performance for our proposed
method. We further investigate practical considerations that arise in the
context of using deep learning and statistical alignment for EEG decoding. In
this regard, we study class-discriminative artifacts that can spuriously
improve results for deep learning models, as well as the impact of
class-imbalance on alignment. We delineate a trade-off relationship between
increased classification accuracy when alignment is performed at later modeling
stages, and susceptibility to class-imbalance in the set of trials that the
statistics are computed on.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17969">Generation of a Compendium of Transcription Factor Cascades and Identification of Potential Therapeutic Targets using Graph Machine Learning. (arXiv:2311.17969v1 [q-bio.MN])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Sivarajkumar_S/0/1/0/all/0/1">Sonish Sivarajkumar</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Tandale_P/0/1/0/all/0/1">Pratyush Tandale</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Bhardwaj_A/0/1/0/all/0/1">Ankit Bhardwaj</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Johnson_K/0/1/0/all/0/1">Kipp W. Johnson</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Titus_A/0/1/0/all/0/1">Anoop Titus</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Glicksberg_B/0/1/0/all/0/1">Benjamin S. Glicksberg</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Khader_S/0/1/0/all/0/1">Shameer Khader</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Yadav_K/0/1/0/all/0/1">Kamlesh K. Yadav</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Subramanian_L/0/1/0/all/0/1">Lakshminarayanan Subramanian</a></p>
<p>Transcription factors (TFs) play a vital role in the regulation of gene
expression thereby making them critical to many cellular processes. In this
study, we used graph machine learning methods to create a compendium of TF
cascades using data extracted from the STRING database. A TF cascade is a
sequence of TFs that regulate each other, forming a directed path in the TF
network. We constructed a knowledge graph of 81,488 unique TF cascades, with
the longest cascade consisting of 62 TFs. Our results highlight the complex and
intricate nature of TF interactions, where multiple TFs work together to
regulate gene expression. We also identified 10 TFs with the highest regulatory
influence based on centrality measurements, providing valuable information for
researchers interested in studying specific TFs. Furthermore, our pathway
enrichment analysis revealed significant enrichment of various pathways and
functional categories, including those involved in cancer and other diseases,
as well as those involved in development, differentiation, and cell signaling.
The enriched pathways identified in this study may have potential as targets
for therapeutic intervention in diseases associated with dysregulation of
transcription factors. We have released the dataset, knowledge graph, and
graphML methods for the TF cascades, and created a website to display the
results, which can be accessed by researchers interested in using this dataset.
Our study provides a valuable resource for understanding the complex network of
interactions between TFs and their regulatory roles in cellular processes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17970">Description Generation using Variational Auto-Encoders for precursor microRNA. (arXiv:2311.17970v1 [q-bio.QM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Petkovic_M/0/1/0/all/0/1">Marko Petkovi&#x107;</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Menkovski_V/0/1/0/all/0/1">Vlado Menkovski</a></p>
<p>Micro RNAs (miRNA) are a type of non-coding RNA, which are involved in gene
regulation and can be associated with diseases such as cancer, cardiovascular
and neurological diseases. As such, identifying the entire genome of miRNA can
be of great relevance. Since experimental methods for novel precursor miRNA
(pre-miRNA) detection are complex and expensive, computational detection using
ML could be useful. Existing ML methods are often complex black boxes, which do
not create an interpretable structural description of pre-miRNA. In this paper,
we propose a novel framework, which makes use of generative modeling through
Variational Auto-Encoders to uncover the generative factors of pre-miRNA. After
training the VAE, the pre-miRNA description is developed using a decision tree
on the lower dimensional latent space. Applying the framework to miRNA
classification, we obtain a high reconstruction and classification performance,
while also developing an accurate miRNA description.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17972">Self-Infilling Code Generation. (arXiv:2311.17972v1 [cs.PL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1">Lin Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1">Jianbo Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Hongxia Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1">Lingpeng Kong</a></p>
<p>This work introduces a general code generation framework that incorporates
infilling operations into auto-regressive decoding. Our approach capitalizes on
the observation that recent code language models with infilling capabilities
can perform \emph{self-infilling}: whereas infilling operations aim to fill in
the middle based on a predefined prefix and suffix, self-infilling sequentially
generates both such surrounding context and the infilled content. We utilize
this feature to develop an infilling-augmented decoding process that
facilitates non-monotonic generation. This approach allows for postponing the
generation of uncertain code snippets until a definitive suffix is established,
leading to improved control over the generation sequence. In addition, it
facilitates a looping mechanism, which can iteratively update and synchronize
each piece of generation in a cyclic manner. Extensive experiments are
conducted to demonstrate that our proposed decoding process is effective in
enhancing regularity and quality across several code generation benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17973">Homogeneous Artificial Neural Network. (arXiv:2311.17973v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Polyakov_A/0/1/0/all/0/1">Andrey Polyakov</a></p>
<p>The paper proposes an artificial neural network (ANN) being a global
approximator for a special class of functions, which are known as generalized
homogeneous. The homogeneity means a symmetry of a function with respect to a
group of transformations having topological characterization of a dilation. In
this paper, a class of the so-called linear dilations is considered. A
homogeneous universal approximation theorem is proven. Procedures for an
upgrade of an existing ANN to a homogeneous one are developed. Theoretical
results are supported by examples from the various domains (computer science,
systems theory and automatic control).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17978">AutArch: An AI-assisted workflow for object detection and automated recording in archaeological catalogues. (arXiv:2311.17978v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Klein_K/0/1/0/all/0/1">Kevin Klein</a>, <a href="http://arxiv.org/find/cs/1/au:+Wohde_A/0/1/0/all/0/1">Alyssa Wohde</a>, <a href="http://arxiv.org/find/cs/1/au:+Gorelik_A/0/1/0/all/0/1">Alexander V. Gorelik</a>, <a href="http://arxiv.org/find/cs/1/au:+Heyd_V/0/1/0/all/0/1">Volker Heyd</a>, <a href="http://arxiv.org/find/cs/1/au:+Diekmann_Y/0/1/0/all/0/1">Yoan Diekmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Brami_M/0/1/0/all/0/1">Maxime Brami</a></p>
<p>Compiling large datasets from published resources, such as archaeological
find catalogues presents fundamental challenges: identifying relevant content
and manually recording it is a time-consuming, repetitive and error-prone task.
For the data to be useful, it must be of comparable quality and adhere to the
same recording standards, which is hardly ever the case in archaeology. Here,
we present a new data collection method exploiting recent advances in
Artificial Intelligence. Our software uses an object detection neural network
combined with further classification networks to speed up, automate, and
standardise data collection from legacy resources, such as archaeological
drawings and photographs in large unsorted PDF files. The AI-assisted workflow
detects common objects found in archaeological catalogues, such as graves,
skeletons, ceramics, ornaments, stone tools and maps, and spatially relates and
analyses these objects on the page to extract real-life attributes, such as the
size and orientation of a grave based on the north arrow and the scale. A
graphical interface allows for and assists with manual validation. We
demonstrate the benefits of this approach by collecting a range of shapes and
numerical attributes from richly-illustrated archaeological catalogues, and
benchmark it in a real-world experiment with ten users. Moreover, we record
geometric whole-outlines through contour detection, an alternative to
landmark-based geometric morphometrics not achievable by hand.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17983">Improving Faithfulness for Vision Transformers. (arXiv:2311.17983v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1">Lijie Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yixin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1">Ninghao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huai_M/0/1/0/all/0/1">Mengdi Huai</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Lichao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Di Wang</a></p>
<p>Vision Transformers (ViTs) have achieved state-of-the-art performance for
various vision tasks. One reason behind the success lies in their ability to
provide plausible innate explanations for the behavior of neural architectures.
However, ViTs suffer from issues with explanation faithfulness, as their focal
points are fragile to adversarial attacks and can be easily changed with even
slight perturbations on the input image. In this paper, we propose a rigorous
approach to mitigate these issues by introducing Faithful ViTs (FViTs). Briefly
speaking, an FViT should have the following two properties: (1) The top-$k$
indices of its self-attention vector should remain mostly unchanged under input
perturbation, indicating stable explanations; (2) The prediction distribution
should be robust to perturbations. To achieve this, we propose a new method
called Denoised Diffusion Smoothing (DDS), which adopts randomized smoothing
and diffusion-based denoising. We theoretically prove that processing ViTs
directly with DDS can turn them into FViTs. We also show that Gaussian noise is
nearly optimal for both $\ell_2$ and $\ell_\infty$-norm cases. Finally, we
demonstrate the effectiveness of our approach through comprehensive experiments
and evaluations. Specifically, we compare our FViTs with other baselines
through visual interpretation and robustness accuracy under adversarial
attacks. Results show that FViTs are more robust against adversarial attacks
while maintaining the explainability of attention, indicating higher
faithfulness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18007">Towards out-of-distribution generalization in large-scale astronomical surveys: robust networks learn similar representations. (arXiv:2311.18007v1 [astro-ph.IM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/astro-ph/1/au:+Gondhalekar_Y/0/1/0/all/0/1">Yash Gondhalekar</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Hassan_S/0/1/0/all/0/1">Sultan Hassan</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Saphra_N/0/1/0/all/0/1">Naomi Saphra</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Andrianomena_S/0/1/0/all/0/1">Sambatra Andrianomena</a></p>
<p>The generalization of machine learning (ML) models to out-of-distribution
(OOD) examples remains a key challenge in extracting information from upcoming
astronomical surveys. Interpretability approaches are a natural way to gain
insights into the OOD generalization problem. We use Centered Kernel Alignment
(CKA), a similarity measure metric of neural network representations, to
examine the relationship between representation similarity and performance of
pre-trained Convolutional Neural Networks (CNNs) on the CAMELS Multifield
Dataset. We find that when models are robust to a distribution shift, they
produce substantially different representations across their layers on OOD
data. However, when they fail to generalize, these representations change less
from layer to layer on OOD data. We discuss the potential application of
similarity representation in guiding model design, training strategy, and
mitigating the OOD problem by incorporating CKA as an inductive bias during
training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18021">Understanding and Improving In-Context Learning on Vision-language Models. (arXiv:2311.18021v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shuo Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1">Zhen Han</a>, <a href="http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1">Bailan He</a>, <a href="http://arxiv.org/find/cs/1/au:+Buckley_M/0/1/0/all/0/1">Mark Buckley</a>, <a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1">Philip Torr</a>, <a href="http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1">Volker Tresp</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jindong Gu</a></p>
<p>Recently, in-context learning (ICL) on large language models (LLMs) has
received great attention, and this technique can also be applied to
vision-language models (VLMs) built upon LLMs. These VLMs can respond to
queries by conditioning responses on a series of multimodal demonstrations,
which comprise images, queries, and answers. Though ICL has been extensively
studied on LLMs, its research on VLMs remains limited. The inclusion of
additional visual information in the demonstrations motivates the following
research questions: which of the two modalities in the demonstration is more
significant? How can we select effective multimodal demonstrations to enhance
ICL performance? This study investigates the significance of both visual and
language information. Our findings indicate that ICL in VLMs is predominantly
driven by the textual information in the demonstrations whereas the visual
information in the demonstrations barely affects the ICL performance.
Subsequently, we provide an understanding of the findings by analyzing the
model information flow and comparing model inner states given different ICL
settings. Motivated by our analysis, we propose a simple yet effective
approach, termed Mixed Modality In-Context Example Selection (MMICES), which
considers both visual and language modalities when selecting demonstrations and
shows better ICL performance. Extensive experiments are conducted to support
our findings, understanding, and improvement of the ICL performance of VLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18022">A trainable manifold for accurate approximation with ReLU Networks. (arXiv:2311.18022v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Milkert_M/0/1/0/all/0/1">Max Milkert</a>, <a href="http://arxiv.org/find/cs/1/au:+Laine_F/0/1/0/all/0/1">Forrest Laine</a></p>
<p>We present a novel technique for exercising greater control of the weights of
ReLU activated neural networks to produce more accurate function
approximations. Many theoretical works encode complex operations into ReLU
networks using smaller base components. In these works, a common base component
is a constant width approximation to x^2, which has exponentially decaying
error with respect to depth. We extend this block to represent a greater range
of convex one-dimensional functions. We derive a manifold of weights such that
the output of these new networks utilizes exponentially many piecewise-linear
segments. This manifold guides their training process to overcome drawbacks
associated with random initialization and unassisted gradient descent. We train
these networks to approximate functions which do not necessarily lie on the
manifold, showing a significant reduction of error values over conventional
approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18025">A Probabilistic Method to Predict Classifier Accuracy on Larger Datasets given Small Pilot Data. (arXiv:2311.18025v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Harvey_E/0/1/0/all/0/1">Ethan Harvey</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wansu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Kent_D/0/1/0/all/0/1">David M. Kent</a>, <a href="http://arxiv.org/find/cs/1/au:+Hughes_M/0/1/0/all/0/1">Michael C. Hughes</a></p>
<p>Practitioners building classifiers often start with a smaller pilot dataset
and plan to grow to larger data in the near future. Such projects need a
toolkit for extrapolating how much classifier accuracy may improve from a 2x,
10x, or 50x increase in data size. While existing work has focused on finding a
single "best-fit" curve using various functional forms like power laws, we
argue that modeling and assessing the uncertainty of predictions is critical
yet has seen less attention. In this paper, we propose a Gaussian process model
to obtain probabilistic extrapolations of accuracy or similar performance
metrics as dataset size increases. We evaluate our approach in terms of error,
likelihood, and coverage across six datasets. Though we focus on medical tasks
and image modalities, our open source approach generalizes to any kind of
classifier.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18027">Enhancing Data-Assimilation in CFD using Graph Neural Networks. (arXiv:2311.18027v1 [physics.flu-dyn])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Quattromini_M/0/1/0/all/0/1">Michele Quattromini</a>, <a href="http://arxiv.org/find/physics/1/au:+Bucci_M/0/1/0/all/0/1">Michele Alessandro Bucci</a>, <a href="http://arxiv.org/find/physics/1/au:+Cherubini_S/0/1/0/all/0/1">Stefania Cherubini</a>, <a href="http://arxiv.org/find/physics/1/au:+Semeraro_O/0/1/0/all/0/1">Onofrio Semeraro</a></p>
<p>We present a novel machine learning approach for data assimilation applied in
fluid mechanics, based on adjoint-optimization augmented by Graph Neural
Networks (GNNs) models. We consider as baseline the Reynolds-Averaged
Navier-Stokes (RANS) equations, where the unknown is the meanflow and a closure
model based on the Reynolds-stress tensor is required for correctly computing
the solution. An end-to-end process is cast; first, we train a GNN model for
the closure term. Second, the GNN model is introduced in the training process
of data assimilation, where the RANS equations act as a physics constraint for
a consistent prediction. We obtain our results using direct numerical
simulations based on a Finite Element Method (FEM) solver; a two-fold interface
between the GNN model and the solver allows the GNN's predictions to be
incorporated into post-processing steps of the FEM analysis. The proposed
scheme provides an excellent reconstruction of the meanflow without any
features selection; preliminary results show promising generalization
properties over unseen flow configurations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18028">Filtered Semi-Markov CRF. (arXiv:2311.18028v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zaratiana_U/0/1/0/all/0/1">Urchade Zaratiana</a>, <a href="http://arxiv.org/find/cs/1/au:+Tomeh_N/0/1/0/all/0/1">Nadi Tomeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Khbir_N/0/1/0/all/0/1">Niama El Khbir</a>, <a href="http://arxiv.org/find/cs/1/au:+Holat_P/0/1/0/all/0/1">Pierre Holat</a>, <a href="http://arxiv.org/find/cs/1/au:+Charnois_T/0/1/0/all/0/1">Thierry Charnois</a></p>
<p>Semi-Markov CRF has been proposed as an alternative to the traditional Linear
Chain CRF for text segmentation tasks such as Named Entity Recognition (NER).
Unlike CRF, which treats text segmentation as token-level prediction, Semi-CRF
considers segments as the basic unit, making it more expressive. However,
Semi-CRF suffers from two major drawbacks: (1) quadratic complexity over
sequence length, as it operates on every span of the input sequence, and (2)
inferior performance compared to CRF for sequence labeling tasks like NER. In
this paper, we introduce Filtered Semi-Markov CRF, a variant of Semi-CRF that
addresses these issues by incorporating a filtering step to eliminate
irrelevant segments, reducing complexity and search space. Our approach is
evaluated on several NER benchmarks, where it outperforms both CRF and Semi-CRF
while being significantly faster. The implementation of our method is available
on \href{https://github.com/urchade/Filtered-Semi-Markov-CRF}{Github}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18029">A Bag of Receptive Fields for Time Series Extrinsic Predictions. (arXiv:2311.18029v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Spinnato_F/0/1/0/all/0/1">Francesco Spinnato</a>, <a href="http://arxiv.org/find/cs/1/au:+Guidotti_R/0/1/0/all/0/1">Riccardo Guidotti</a>, <a href="http://arxiv.org/find/cs/1/au:+Monreale_A/0/1/0/all/0/1">Anna Monreale</a>, <a href="http://arxiv.org/find/cs/1/au:+Nanni_M/0/1/0/all/0/1">Mirco Nanni</a></p>
<p>High-dimensional time series data poses challenges due to its dynamic nature,
varying lengths, and presence of missing values. This kind of data requires
extensive preprocessing, limiting the applicability of existing Time Series
Classification and Time Series Extrinsic Regression techniques. For this
reason, we propose BORF, a Bag-Of-Receptive-Fields model, which incorporates
notions from time series convolution and 1D-SAX to handle univariate and
multivariate time series with varying lengths and missing values. We evaluate
BORF on Time Series Classification and Time Series Extrinsic Regression tasks
using the full UEA and UCR repositories, demonstrating its competitive
performance against state-of-the-art methods. Finally, we outline how this
representation can naturally provide saliency and feature-based explanations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18035">TransOpt: Transformer-based Representation Learning for Optimization Problem Classification. (arXiv:2311.18035v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cenikj_G/0/1/0/all/0/1">Gjorgjina Cenikj</a>, <a href="http://arxiv.org/find/cs/1/au:+Petelin_G/0/1/0/all/0/1">Ga&#x161;per Petelin</a>, <a href="http://arxiv.org/find/cs/1/au:+Eftimov_T/0/1/0/all/0/1">Tome Eftimov</a></p>
<p>We propose a representation of optimization problem instances using a
transformer-based neural network architecture trained for the task of problem
classification of the 24 problem classes from the Black-box Optimization
Benchmarking (BBOB) benchmark. We show that transformer-based methods can be
trained to recognize problem classes with accuracies in the range of 70\%-80\%
for different problem dimensions, suggesting the possible application of
transformer architectures in acquiring representations for black-box
optimization problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18044">Transfer Learning in Robotics: An Upcoming Breakthrough? A Review of Promises and Challenges. (arXiv:2311.18044v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jaquier_N/0/1/0/all/0/1">No&#xe9;mie Jaquier</a>, <a href="http://arxiv.org/find/cs/1/au:+Welle_M/0/1/0/all/0/1">Michael C. Welle</a>, <a href="http://arxiv.org/find/cs/1/au:+Gams_A/0/1/0/all/0/1">Andrej Gams</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_K/0/1/0/all/0/1">Kunpeng Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Fichera_B/0/1/0/all/0/1">Bernardo Fichera</a>, <a href="http://arxiv.org/find/cs/1/au:+Billard_A/0/1/0/all/0/1">Aude Billard</a>, <a href="http://arxiv.org/find/cs/1/au:+Ude_A/0/1/0/all/0/1">Ale&#x161; Ude</a>, <a href="http://arxiv.org/find/cs/1/au:+Asfour_T/0/1/0/all/0/1">Tamim Asfour</a>, <a href="http://arxiv.org/find/cs/1/au:+Kragic_D/0/1/0/all/0/1">Danica Kragi&#x107;</a></p>
<p>Transfer learning is a conceptually-enticing paradigm in pursuit of truly
intelligent embodied agents. The core concept -- reusing prior knowledge to
learn in and from novel situations -- is successfully leveraged by humans to
handle novel situations. In recent years, transfer learning has received
renewed interest from the community from different perspectives, including
imitation learning, domain adaptation, and transfer of experience from
simulation to the real world, among others. In this paper, we unify the concept
of transfer learning in robotics and provide the first taxonomy of its kind
considering the key concepts of robot, task, and environment. Through a review
of the promises and challenges in the field, we identify the need of
transferring at different abstraction levels, the need of quantifying the
transfer gap and the quality of transfer, as well as the dangers of negative
transfer. Via this position paper, we hope to channel the effort of the
community towards the most significant roadblocks to realize the full potential
of transfer learning in robotics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18048">An Interventional Perspective on Identifiability in Gaussian LTI Systems with Independent Component Analysis. (arXiv:2311.18048v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rajendran_G/0/1/0/all/0/1">Goutham Rajendran</a>, <a href="http://arxiv.org/find/cs/1/au:+Reizinger_P/0/1/0/all/0/1">Patrik Reizinger</a>, <a href="http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1">Wieland Brendel</a>, <a href="http://arxiv.org/find/cs/1/au:+Ravikumar_P/0/1/0/all/0/1">Pradeep Ravikumar</a></p>
<p>We investigate the relationship between system identification and
intervention design in dynamical systems. While previous research demonstrated
how identifiable representation learning methods, such as Independent Component
Analysis (ICA), can reveal cause-effect relationships, it relied on a passive
perspective without considering how to collect data. Our work shows that in
Gaussian Linear Time-Invariant (LTI) systems, the system parameters can be
identified by introducing diverse intervention signals in a multi-environment
setting. By harnessing appropriate diversity assumptions motivated by the ICA
literature, our findings connect experiment design and representational
identifiability in dynamical systems. We corroborate our findings on synthetic
and (simulated) physical data. Additionally, we show that Hidden Markov Models,
in general, and (Gaussian) LTI systems, in particular, fulfil a generalization
of the Causal de Finetti theorem with continuous parameters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18061">TransNAS-TSAD: Harnessing Transformers for Multi-Objective Neural Architecture Search in Time Series Anomaly Detection. (arXiv:2311.18061v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Haq_I/0/1/0/all/0/1">Ijaz Ul Haq</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1">Byung Suk Lee</a></p>
<p>The surge in real-time data collection across various industries has
underscored the need for advanced anomaly detection in both univariate and
multivariate time series data. Traditional methods, while comprehensive, often
struggle to capture the complex interdependencies in such data. This paper
introduces TransNAS-TSAD, a novel framework that synergizes transformer
architecture with neural architecture search (NAS), enhanced through NSGA-II
algorithm optimization. This innovative approach effectively tackles the
complexities of both univariate and multivariate time series, balancing
computational efficiency with detection accuracy. Our evaluation reveals that
TransNAS-TSAD surpasses conventional anomaly detection models, demonstrating
marked improvements in diverse data scenarios. We also propose the
Efficiency-Accuracy-Complexity Score (EACS) as a new metric for assessing model
performance, emphasizing the crucial balance between accuracy and computational
resources. TransNAS-TSAD sets a new benchmark in time series anomaly detection,
offering a versatile, efficient solution for complex real-world applications.
This research paves the way for future developments in the field, highlighting
its potential in a wide range of industry applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18062">Understanding Your Agent: Leveraging Large Language Models for Behavior Explanation. (arXiv:2311.18062v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xijia Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yue Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Stepputtis_S/0/1/0/all/0/1">Simon Stepputtis</a>, <a href="http://arxiv.org/find/cs/1/au:+Sycara_K/0/1/0/all/0/1">Katia Sycara</a>, <a href="http://arxiv.org/find/cs/1/au:+Campbell_J/0/1/0/all/0/1">Joseph Campbell</a></p>
<p>Intelligent agents such as robots are increasingly deployed in real-world,
safety-critical settings. It is vital that these agents are able to explain the
reasoning behind their decisions to human counterparts; however, their behavior
is often produced by uninterpretable models such as deep neural networks. We
propose an approach to generate natural language explanations for an agent's
behavior based only on observations of states and actions, thus making our
method independent from the underlying model's representation. For such models,
we first learn a behavior representation and subsequently use it to produce
plausible explanations with minimal hallucination while affording user
interaction with a pre-trained large language model. We evaluate our method in
a multi-agent search-and-rescue environment and demonstrate the effectiveness
of our explanations for agents executing various behaviors. Through user
studies and empirical experiments, we show that our approach generates
explanations as helpful as those produced by a human domain expert while
enabling beneficial interactions such as clarification and counterfactual
queries.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18063">TurkishBERTweet: Fast and Reliable Large Language Model for Social Media Analysis. (arXiv:2311.18063v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Najafi_A/0/1/0/all/0/1">Ali Najafi</a>, <a href="http://arxiv.org/find/cs/1/au:+Varol_O/0/1/0/all/0/1">Onur Varol</a></p>
<p>Turkish is one of the most popular languages in the world. Wide us of this
language on social media platforms such as Twitter, Instagram, or Tiktok and
strategic position of the country in the world politics makes it appealing for
the social network researchers and industry. To address this need, we introduce
TurkishBERTweet, the first large scale pre-trained language model for Turkish
social media built using almost 900 million tweets. The model shares the same
architecture as base BERT model with smaller input length, making
TurkishBERTweet lighter than BERTurk and can have significantly lower inference
time. We trained our model using the same approach for RoBERTa model and
evaluated on two text classification tasks: Sentiment Classification and Hate
Speech Detection. We demonstrate that TurkishBERTweet outperforms the other
available alternatives on generalizability and its lower inference time gives
significant advantage to process large-scale datasets. We also compared our
models with the commercial OpenAI solutions in terms of cost and performance to
demonstrate TurkishBERTweet is scalable and cost-effective solution. As part of
our research, we released TurkishBERTweet and fine-tuned LoRA adapters for the
mentioned tasks under the MIT License to facilitate future research and
applications on Turkish social media. Our TurkishBERTweet model is available
at: https://github.com/ViralLab/TurkishBERTweet
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18072">Self-Supervised Learning for Large-Scale Preventive Security Constrained DC Optimal Power Flow. (arXiv:2311.18072v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1">Seonho Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Hentenryck_P/0/1/0/all/0/1">Pascal Van Hentenryck</a></p>
<p>Security-Constrained Optimal Power Flow (SCOPF) plays a crucial role in power
grid stability but becomes increasingly complex as systems grow. This paper
introduces PDL-SCOPF, a self-supervised end-to-end primal-dual learning
framework for producing near-optimal solutions to large-scale SCOPF problems in
milliseconds. Indeed, PDL-SCOPF remedies the limitations of supervised
counterparts that rely on training instances with their optimal solutions,
which becomes impractical for large-scale SCOPF problems. PDL-SCOPF mimics an
Augmented Lagrangian Method (ALM) for training primal and dual networks that
learn the primal solutions and the Lagrangian multipliers, respectively, to the
unconstrained optimizations. In addition, PDL-SCOPF incorporates a repair layer
to ensure the feasibility of the power balance in the nominal case, and a
binary search layer to compute, using the Automatic Primary Response (APR), the
generator dispatches in the contingencies. The resulting differentiable program
can then be trained end-to-end using the objective function of the SCOPF and
the power balance constraints of the contingencies. Experimental results
demonstrate that the PDL-SCOPF delivers accurate feasible solutions with
minimal optimality gaps. The framework underlying PDL-SCOPF aims at bridging
the gap between traditional optimization methods and machine learning,
highlighting the potential of self-supervised end-to-end primal-dual learning
for large-scale optimization tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18076">A Nystr\&quot;om method with missing distances. (arXiv:2311.18076v1 [cs.IT])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lichtenberg_S/0/1/0/all/0/1">Samuel Lichtenberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Tasissa_A/0/1/0/all/0/1">Abiy Tasissa</a></p>
<p>We study the problem of determining the configuration of $n$ points, referred
to as mobile nodes, by utilizing pairwise distances to $m$ fixed points known
as anchor nodes. In the standard setting, we have information about the
distances between anchors (anchor-anchor) and between anchors and mobile nodes
(anchor-mobile), but the distances between mobile nodes (mobile-mobile) are not
known. For this setup, the Nystr\"om method is a viable technique for
estimating the positions of the mobile nodes. This study focuses on the setting
where the anchor-mobile block of the distance matrix contains only partial
distance information. First, we establish a relationship between the columns of
the anchor-mobile block in the distance matrix and the columns of the
corresponding block in the Gram matrix via a graph Laplacian. Exploiting this
connection, we introduce a novel sampling model that frames the position
estimation problem as low-rank recovery of an inner product matrix, given a
subset of its expansion coefficients in a special non-orthogonal basis. This
basis and its dual basis--the central elements of our model--are explicitly
derived. Our analysis is grounded in a specific centering of the points that is
unique to the Nystr\"om method. With this in mind, we extend previous work in
Euclidean distance geometry by providing a general dual basis approach for
points centered anywhere.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18078">The Forecastability of Underlying Building Electricity Demand from Time Series Data. (arXiv:2311.18078v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khalil_M/0/1/0/all/0/1">Mohamad Khalil</a>, <a href="http://arxiv.org/find/cs/1/au:+McGough_A/0/1/0/all/0/1">A. Stephen McGough</a>, <a href="http://arxiv.org/find/cs/1/au:+Kazmi_H/0/1/0/all/0/1">Hussain Kazmi</a>, <a href="http://arxiv.org/find/cs/1/au:+Walker_S/0/1/0/all/0/1">Sara Walker</a></p>
<p>Forecasting building energy consumption has become a promising solution in
Building Energy Management Systems for energy saving and optimization.
Furthermore, it can play an important role in the efficient management of the
operation of a smart grid. Different data-driven approaches to forecast the
future energy demand of buildings at different scale, and over various time
horizons, can be found in the scientific literature, including extensive
Machine Learning and Deep Learning approaches. However, the identification of
the most accurate forecaster model which can be utilized to predict the energy
demand of such a building is still challenging.In this paper, the design and
implementation of a data-driven approach to predict how forecastable the future
energy demand of a building is, without first utilizing a data-driven
forecasting model, is presented. The investigation utilizes a historical
electricity consumption time series data set with a half-hour interval that has
been collected from a group of residential buildings located in the City of
London, United Kingdom
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18083">Meta Co-Training: Two Views are Better than One. (arXiv:2311.18083v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rothenberger_J/0/1/0/all/0/1">Jay C. Rothenberger</a>, <a href="http://arxiv.org/find/cs/1/au:+Diochnos_D/0/1/0/all/0/1">Dimitrios I. Diochnos</a></p>
<p>In many practical computer vision scenarios unlabeled data is plentiful, but
labels are scarce and difficult to obtain. As a result, semi-supervised
learning which leverages unlabeled data to boost the performance of supervised
classifiers have received significant attention in recent literature. One major
class of semi-supervised algorithms is co-training. In co-training two
different models leverage different independent and sufficient "views" of the
data to jointly make better predictions. During co-training each model creates
pseudo labels on unlabeled points which are used to improve the other model. We
show that in the common case when independent views are not available we can
construct such views inexpensively using pre-trained models. Co-training on the
constructed views yields a performance improvement over any of the individual
views we construct and performance comparable with recent approaches in
semi-supervised learning, but has some undesirable properties. To alleviate the
issues present with co-training we present Meta Co-Training which is an
extension of the successful Meta Pseudo Labels approach to multiple views. Our
method achieves new state-of-the-art performance on ImageNet-10% with very few
training resources, as well as outperforming prior semi-supervised work on
several other fine-grained image classification datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18094">Self-Driving Telescopes: Autonomous Scheduling of Astronomical Observation Campaigns with Offline Reinforcement Learning. (arXiv:2311.18094v1 [astro-ph.IM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/astro-ph/1/au:+Terranova_F/0/1/0/all/0/1">Franco Terranova</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Voetberg_M/0/1/0/all/0/1">M. Voetberg</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Nord_B/0/1/0/all/0/1">Brian Nord</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Pagul_A/0/1/0/all/0/1">Amanda Pagul</a></p>
<p>Modern astronomical experiments are designed to achieve multiple scientific
goals, from studies of galaxy evolution to cosmic acceleration. These goals
require data of many different classes of night-sky objects, each of which has
a particular set of observational needs. These observational needs are
typically in strong competition with one another. This poses a challenging
multi-objective optimization problem that remains unsolved. The effectiveness
of Reinforcement Learning (RL) as a valuable paradigm for training autonomous
systems has been well-demonstrated, and it may provide the basis for
self-driving telescopes capable of optimizing the scheduling for astronomy
campaigns. Simulated datasets containing examples of interactions between a
telescope and a discrete set of sky locations on the celestial sphere can be
used to train an RL model to sequentially gather data from these several
locations to maximize a cumulative reward as a measure of the quality of the
data gathered. We use simulated data to test and compare multiple
implementations of a Deep Q-Network (DQN) for the task of optimizing the
schedule of observations from the Stone Edge Observatory (SEO). We combine
multiple improvements on the DQN and adjustments to the dataset, showing that
DQNs can achieve an average reward of 87%+-6% of the maximum achievable reward
in each state on the test set. This is the first comparison of offline RL
algorithms for a particular astronomical challenge and the first open-source
framework for performing such a comparison and assessment task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18098">Adaptive Early Exiting for Collaborative Inference over Noisy Wireless Channels. (arXiv:2311.18098v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jankowski_M/0/1/0/all/0/1">Mikolaj Jankowski</a>, <a href="http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1">Deniz Gunduz</a>, <a href="http://arxiv.org/find/cs/1/au:+Mikolajczyk_K/0/1/0/all/0/1">Krystian Mikolajczyk</a></p>
<p>Collaborative inference systems are one of the emerging solutions for
deploying deep neural networks (DNNs) at the wireless network edge. Their main
idea is to divide a DNN into two parts, where the first is shallow enough to be
reliably executed at edge devices of limited computational power, while the
second part is executed at an edge server with higher computational
capabilities. The main advantage of such systems is that the input of the DNN
gets compressed as the subsequent layers of the shallow part extract only the
information necessary for the task. As a result, significant communication
savings can be achieved compared to transmitting raw input samples. In this
work, we study early exiting in the context of collaborative inference, which
allows obtaining inference results at the edge device for certain samples,
without the need to transmit the partially processed data to the edge server at
all, leading to further communication savings. The central part of our system
is the transmission-decision (TD) mechanism, which, given the information from
the early exit, and the wireless channel conditions, decides whether to keep
the early exit prediction or transmit the data to the edge server for further
processing. In this paper, we evaluate various TD mechanisms and show
experimentally, that for an image classification task over the wireless edge,
proper utilization of early exits can provide both performance gains and
significant communication savings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18102">PatchBMI-Net: Lightweight Facial Patch-based Ensemble for BMI Prediction. (arXiv:2311.18102v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aarotale_P/0/1/0/all/0/1">Parshuram N. Aarotale</a>, <a href="http://arxiv.org/find/cs/1/au:+Hill_T/0/1/0/all/0/1">Twyla Hill</a>, <a href="http://arxiv.org/find/cs/1/au:+Rattani_A/0/1/0/all/0/1">Ajita Rattani</a></p>
<p>Due to an alarming trend related to obesity affecting 93.3 million adults in
the United States alone, body mass index (BMI) and body weight have drawn
significant interest in various health monitoring applications. Consequently,
several studies have proposed self-diagnostic facial image-based BMI prediction
methods for healthy weight monitoring. These methods have mostly used
convolutional neural network (CNN) based regression baselines, such as VGG19,
ResNet50, and Efficient-NetB0, for BMI prediction from facial images. However,
the high computational requirement of these heavy-weight CNN models limits
their deployment to resource-constrained mobile devices, thus deterring weight
monitoring using smartphones. This paper aims to develop a lightweight facial
patch-based ensemble (PatchBMI-Net) for BMI prediction to facilitate the
deployment and weight monitoring using smartphones. Extensive experiments on
BMI-annotated facial image datasets suggest that our proposed PatchBMI-Net
model can obtain Mean Absolute Error (MAE) in the range [3.58, 6.51] with a
size of about 3.3 million parameters. On cross-comparison with heavyweight
models, such as ResNet-50 and Xception, trained for BMI prediction from facial
images, our proposed PatchBMI-Net obtains equivalent MAE along with the model
size reduction of about 5.4x and the average inference time reduction of about
3x when deployed on Apple-14 smartphone. Thus, demonstrating performance
efficiency as well as low latency for on-device deployment and weight
monitoring using smartphone applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18128">Dynamic Scheduling of a Multiclass Queue in the Halfin-Whitt Regime: A Computational Approach for High-Dimensional Problems. (arXiv:2311.18128v1 [eess.SY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ata_B/0/1/0/all/0/1">Bar&#x131;&#x15f; Ata</a>, <a href="http://arxiv.org/find/eess/1/au:+Kasikaralar_E/0/1/0/all/0/1">Ebru Ka&#x15f;&#x131;karalar</a></p>
<p>We consider a multi-class queueing model of a telephone call center, in which
a system manager dynamically allocates available servers to customer calls.
Calls can terminate through either service completion or customer abandonment,
and the manager strives to minimize the expected total of holding costs plus
abandonment costs over a finite horizon. Focusing on the Halfin-Whitt heavy
traffic regime, we derive an approximating diffusion control problem, and
building on earlier work by Han et al. (2018), develop a simulation-based
computational method for solution of such problems, one that relies heavily on
deep neural network technology. Using this computational method, we propose a
policy for the original (pre-limit) call center scheduling problem. Finally,
the performance of this policy is assessed using test problems based on
publicly available call center data. For the test problems considered so far,
our policy does as well as the best benchmark we could find. Moreover, our
method is computationally feasible at least up to dimension 100, that is, for
call centers with 100 or more distinct customer classes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18129">Mixed-Precision Quantization for Federated Learning on Resource-Constrained Heterogeneous Devices. (arXiv:2311.18129v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huancheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Vikalo_H/0/1/0/all/0/1">Haris Vikalo</a></p>
<p>While federated learning (FL) systems often utilize quantization to battle
communication and computational bottlenecks, they have heretofore been limited
to deploying fixed-precision quantization schemes. Meanwhile, the concept of
mixed-precision quantization (MPQ), where different layers of a deep learning
model are assigned varying bit-width, remains unexplored in the FL settings. We
present a novel FL algorithm, FedMPQ, which introduces mixed-precision
quantization to resource-heterogeneous FL systems. Specifically, local models,
quantized so as to satisfy bit-width constraint, are trained by optimizing an
objective function that includes a regularization term which promotes reduction
of precision in some of the layers without significant performance degradation.
The server collects local model updates, de-quantizes them into full-precision
models, and then aggregates them into a global model. To initialize the next
round of local training, the server relies on the information learned in the
previous training round to customize bit-width assignments of the models
delivered to different clients. In extensive benchmarking experiments on
several model architectures and different datasets in both iid and non-iid
settings, FedMPQ outperformed the baseline FL schemes that utilize
fixed-precision quantization while incurring only a minor computational
overhead on the participating devices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18130">The Trifecta: Three simple techniques for training deeper Forward-Forward networks. (arXiv:2311.18130v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dooms_T/0/1/0/all/0/1">Thomas Dooms</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1">Ing Jyh Tsang</a>, <a href="http://arxiv.org/find/cs/1/au:+Oramas_J/0/1/0/all/0/1">Jose Oramas</a></p>
<p>Modern machine learning models are able to outperform humans on a variety of
non-trivial tasks. However, as the complexity of the models increases, they
consume significant amounts of power and still struggle to generalize
effectively to unseen data. Local learning, which focuses on updating subsets
of a model's parameters at a time, has emerged as a promising technique to
address these issues. Recently, a novel local learning algorithm, called
Forward-Forward, has received widespread attention due to its innovative
approach to learning. Unfortunately, its application has been limited to
smaller datasets due to scalability issues. To this end, we propose The
Trifecta, a collection of three simple techniques that synergize exceptionally
well and drastically improve the Forward-Forward algorithm on deeper networks.
Our experiments demonstrate that our models are on par with similarly
structured, backpropagation-based models in both training speed and test
accuracy on simple datasets. This is achieved by the ability to learn
representations that are informative locally, on a layer-by-layer basis, and
retain their informativeness when propagated to deeper layers in the
architecture. This leads to around 84\% accuracy on CIFAR-10, a notable
improvement (25\%) over the original FF algorithm. These results highlight the
potential of Forward-Forward as a genuine competitor to backpropagation and as
a promising research avenue.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18144">Dynamical phase transition in quantum neural networks with large depth. (arXiv:2311.18144v1 [quant-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Zhang_B/0/1/0/all/0/1">Bingzhi Zhang</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Liu_J/0/1/0/all/0/1">Junyu Liu</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Wu_X/0/1/0/all/0/1">Xiao-Chuan Wu</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Jiang_L/0/1/0/all/0/1">Liang Jiang</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Zhuang_Q/0/1/0/all/0/1">Quntao Zhuang</a></p>
<p>Understanding the training dynamics of quantum neural networks is a
fundamental task in quantum information science with wide impact in physics,
chemistry and machine learning. In this work, we show that the late-time
training dynamics of quantum neural networks can be described by the
generalized Lotka-Volterra equations, which lead to a dynamical phase
transition. When the targeted value of cost function crosses the minimum
achievable value from above to below, the dynamics evolve from a frozen-kernel
phase to a frozen-error phase, showing a duality between the quantum neural
tangent kernel and the total error. In both phases, the convergence towards the
fixed point is exponential, while at the critical point becomes polynomial. Via
mapping the Hessian of the training dynamics to a Hamiltonian in the imaginary
time, we reveal the nature of the phase transition to be second-order with the
exponent $\nu=1$, where scale invariance and closing gap are observed at
critical point. We also provide a non-perturbative analytical theory to explain
the phase transition via a restricted Haar ensemble at late time, when the
output state approaches the steady state. The theory findings are verified
experimentally on IBM quantum devices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18168">Probabilistic Speech-Driven 3D Facial Motion Synthesis: New Benchmarks, Methods, and Applications. (arXiv:2311.18168v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1">Karren D. Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ranjan_A/0/1/0/all/0/1">Anurag Ranjan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1">Jen-Hao Rick Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Vemulapalli_R/0/1/0/all/0/1">Raviteja Vemulapalli</a>, <a href="http://arxiv.org/find/cs/1/au:+Tuzel_O/0/1/0/all/0/1">Oncel Tuzel</a></p>
<p>We consider the task of animating 3D facial geometry from speech signal.
Existing works are primarily deterministic, focusing on learning a one-to-one
mapping from speech signal to 3D face meshes on small datasets with limited
speakers. While these models can achieve high-quality lip articulation for
speakers in the training set, they are unable to capture the full and diverse
distribution of 3D facial motions that accompany speech in the real world.
Importantly, the relationship between speech and facial motion is one-to-many,
containing both inter-speaker and intra-speaker variations and necessitating a
probabilistic approach. In this paper, we identify and address key challenges
that have so far limited the development of probabilistic models: lack of
datasets and metrics that are suitable for training and evaluating them, as
well as the difficulty of designing a model that generates diverse results
while remaining faithful to a strong conditioning signal as speech. We first
propose large-scale benchmark datasets and metrics suitable for probabilistic
modeling. Then, we demonstrate a probabilistic model that achieves both
diversity and fidelity to speech, outperforming other methods across the
proposed benchmarks. Finally, we showcase useful applications of probabilistic
models trained on these large-scale datasets: we can generate diverse
speech-driven 3D facial motion that matches unseen speaker styles extracted
from reference clips; and our synthetic meshes can be used to improve the
performance of downstream audio-visual models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18174">Packrat: Automatic Reconfiguration for Latency Minimization in CPU-based DNN Serving. (arXiv:2311.18174v1 [cs.DC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bhardwaj_A/0/1/0/all/0/1">Ankit Bhardwaj</a>, <a href="http://arxiv.org/find/cs/1/au:+Phanishayee_A/0/1/0/all/0/1">Amar Phanishayee</a>, <a href="http://arxiv.org/find/cs/1/au:+Narayanan_D/0/1/0/all/0/1">Deepak Narayanan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tarta_M/0/1/0/all/0/1">Mihail Tarta</a>, <a href="http://arxiv.org/find/cs/1/au:+Stutsman_R/0/1/0/all/0/1">Ryan Stutsman</a></p>
<p>In this paper, we investigate how to push the performance limits of serving
Deep Neural Network (DNN) models on CPU-based servers. Specifically, we observe
that while intra-operator parallelism across multiple threads is an effective
way to reduce inference latency, it provides diminishing returns. Our primary
insight is that instead of running a single instance of a model with all
available threads on a server, running multiple instances each with smaller
batch sizes and fewer threads for intra-op parallelism can provide lower
inference latency. However, the right configuration is hard to determine
manually since it is workload- (DNN model and batch size used by the serving
system) and deployment-dependent (number of CPU cores on server). We present
Packrat, a new serving system for online inference that given a model and batch
size ($B$) algorithmically picks the optimal number of instances ($i$), the
number of threads each should be allocated ($t$), and the batch sizes each
should operate on ($b$) that minimizes latency. Packrat is built as an
extension to TorchServe and supports online reconfigurations to avoid serving
downtime. Averaged across a range of batch sizes, Packrat improves inference
latency by 1.43$\times$ to 1.83$\times$ on a range of commonly used DNNs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18177">An Effective Universal Polynomial Basis for Spectral Graph Neural Networks. (arXiv:2311.18177v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1">Keke Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1">Pietro Li&#xf2;</a></p>
<p>Spectral Graph Neural Networks (GNNs), also referred to as graph filters have
gained increasing prevalence for heterophily graphs. Optimal graph filters rely
on Laplacian eigendecomposition for Fourier transform. In an attempt to avert
the prohibitive computations, numerous polynomial filters by leveraging
distinct polynomials have been proposed to approximate the desired graph
filters. However, polynomials in the majority of polynomial filters are
predefined and remain fixed across all graphs, failing to accommodate the
diverse heterophily degrees across different graphs. To tackle this issue, we
first investigate the correlation between polynomial bases of desired graph
filters and the degrees of graph heterophily via a thorough theoretical
analysis. Afterward, we develop an adaptive heterophily basis by incorporating
graph heterophily degrees. Subsequently, we integrate this heterophily basis
with the homophily basis, creating a universal polynomial basis UniBasis. In
consequence, we devise a general polynomial filter UniFilter. Comprehensive
experiments on both real-world and synthetic datasets with varying heterophily
degrees significantly support the superiority of UniFilter, demonstrating the
effectiveness and generality of UniBasis, as well as its promising capability
as a new method for graph analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18188">Leveraging cache to enable SLU on tiny devices. (arXiv:2311.18188v1 [eess.AS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Benazir_A/0/1/0/all/0/1">Afsara Benazir</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1">Zhiming Xu</a>, <a href="http://arxiv.org/find/eess/1/au:+Lin_F/0/1/0/all/0/1">Felix Xiaozhu Lin</a> (University of Virginia)</p>
<p>This paper addresses spoken language understanding (SLU) on
microcontroller-like embedded devices, integrating on-device execution with
cloud offloading in a novel fashion. We exploit temporal locality in a device's
speech inputs and accordingly reuse recent SLU inferences. Our idea is simple:
let the device match new inputs against cached results, and only offload
unmatched inputs to the cloud for full inference. Realization of this idea,
however, is non-trivial: the device needs to compare acoustic features in a
robust, low-cost way. To this end, we present XYZ, a speech cache for tiny
devices. It matches speech inputs at two levels of representations: first by
clustered sequences of raw sound units, then as sequences of phonemes. Working
in tandem, the two representations offer complementary cost/accuracy tradeoffs.
To further boost accuracy, our cache is learning: with the mismatched and then
offloaded inputs, it continuously finetunes the device's feature extractors
(with the assistance of the cloud). We implement XYZ on an off-the-shelf STM32
microcontroller. The resultant implementation has a small memory footprint of
2MB. Evaluated on challenging speech benchmarks, our system resolves 45%--90%
of inputs on device, reducing the average latency by up to 80% compared to
offloading to popular cloud speech services. Our benefit is pronounced even in
adversarial settings -- noisy environments, cold cache, or one device shared by
a number of users.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18190">Toward the Tradeoffs between Privacy, Fairness and Utility in Federated Learning. (arXiv:2311.18190v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1">Kangkang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaojin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1">Xi Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1">Gaolei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jing Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jianhua Li</a></p>
<p>Federated Learning (FL) is a novel privacy-protection distributed machine
learning paradigm that guarantees user privacy and prevents the risk of data
leakage due to the advantage of the client's local training. Researchers have
struggled to design fair FL systems that ensure fairness of results. However,
the interplay between fairness and privacy has been less studied. Increasing
the fairness of FL systems can have an impact on user privacy, while an
increase in user privacy can affect fairness. In this work, on the client side,
we use fairness metrics, such as Demographic Parity (DemP), Equalized Odds
(EOs), and Disparate Impact (DI), to construct the local fair model. To protect
the privacy of the client model, we propose a privacy-protection fairness FL
method. The results show that the accuracy of the fair model with privacy
increases because privacy breaks the constraints of the fairness metrics. In
our experiments, we conclude the relationship between privacy, fairness and
utility, and there is a tradeoff between these.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18194">Positional Information Matters for Invariant In-Context Learning: A Case Study of Simple Function Classes. (arXiv:2311.18194v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yongqiang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_B/0/1/0/all/0/1">Binghui Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1">Kaiwen Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1">Bo Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Bian_Y/0/1/0/all/0/1">Yatao Bian</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1">James Cheng</a></p>
<p>In-context learning (ICL) refers to the ability of a model to condition on a
few in-context demonstrations (input-output examples of the underlying task) to
generate the answer for a new query input, without updating parameters. Despite
the impressive ICL ability of LLMs, it has also been found that ICL in LLMs is
sensitive to input demonstrations and limited to short context lengths. To
understand the limitations and principles for successful ICL, we conduct an
investigation with ICL linear regression of transformers. We characterize
several Out-of-Distribution (OOD) cases for ICL inspired by realistic LLM ICL
failures and compare transformers with DeepSet, a simple yet powerful
architecture for ICL. Surprisingly, DeepSet outperforms transformers across a
variety of distribution shifts, implying that preserving permutation invariance
symmetry to input demonstrations is crucial for OOD ICL. The phenomenon
specifies a fundamental requirement by ICL, which we termed as ICL invariance.
Nevertheless, the positional encodings in LLMs will break ICL invariance. To
this end, we further evaluate transformers with identical positional encodings
and find preserving ICL invariance in transformers achieves state-of-the-art
performance across various ICL distribution shifts
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18206">SCOPE-RL: A Python Library for Offline Reinforcement Learning and Off-Policy Evaluation. (arXiv:2311.18206v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kiyohara_H/0/1/0/all/0/1">Haruka Kiyohara</a>, <a href="http://arxiv.org/find/cs/1/au:+Kishimoto_R/0/1/0/all/0/1">Ren Kishimoto</a>, <a href="http://arxiv.org/find/cs/1/au:+Kawakami_K/0/1/0/all/0/1">Kosuke Kawakami</a>, <a href="http://arxiv.org/find/cs/1/au:+Kobayashi_K/0/1/0/all/0/1">Ken Kobayashi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nakata_K/0/1/0/all/0/1">Kazuhide Nakata</a>, <a href="http://arxiv.org/find/cs/1/au:+Saito_Y/0/1/0/all/0/1">Yuta Saito</a></p>
<p>This paper introduces SCOPE-RL, a comprehensive open-source Python software
designed for offline reinforcement learning (offline RL), off-policy evaluation
(OPE), and selection (OPS). Unlike most existing libraries that focus solely on
either policy learning or evaluation, SCOPE-RL seamlessly integrates these two
key aspects, facilitating flexible and complete implementations of both offline
RL and OPE processes. SCOPE-RL put particular emphasis on its OPE modules,
offering a range of OPE estimators and robust evaluation-of-OPE protocols. This
approach enables more in-depth and reliable OPE compared to other packages. For
instance, SCOPE-RL enhances OPE by estimating the entire reward distribution
under a policy rather than its mere point-wise expected value. Additionally,
SCOPE-RL provides a more thorough evaluation-of-OPE by presenting the
risk-return tradeoff in OPE results, extending beyond mere accuracy evaluations
in existing OPE literature. SCOPE-RL is designed with user accessibility in
mind. Its user-friendly APIs, comprehensive documentation, and a variety of
easy-to-follow examples assist researchers and practitioners in efficiently
implementing and experimenting with various offline RL methods and OPE
estimators, tailored to their specific problem contexts. The documentation of
SCOPE-RL is available at https://scope-rl.readthedocs.io/en/latest/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18207">Towards Assessing and Benchmarking Risk-Return Tradeoff of Off-Policy Evaluation. (arXiv:2311.18207v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kiyohara_H/0/1/0/all/0/1">Haruka Kiyohara</a>, <a href="http://arxiv.org/find/cs/1/au:+Kishimoto_R/0/1/0/all/0/1">Ren Kishimoto</a>, <a href="http://arxiv.org/find/cs/1/au:+Kawakami_K/0/1/0/all/0/1">Kosuke Kawakami</a>, <a href="http://arxiv.org/find/cs/1/au:+Kobayashi_K/0/1/0/all/0/1">Ken Kobayashi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nakata_K/0/1/0/all/0/1">Kazuhide Nakata</a>, <a href="http://arxiv.org/find/cs/1/au:+Saito_Y/0/1/0/all/0/1">Yuta Saito</a></p>
<p>Off-Policy Evaluation (OPE) aims to assess the effectiveness of
counterfactual policies using only offline logged data and is often used to
identify the top-k promising policies for deployment in online A/B tests.
Existing evaluation metrics for OPE estimators primarily focus on the
"accuracy" of OPE or that of downstream policy selection, neglecting
risk-return tradeoff in the subsequent online policy deployment. To address
this issue, we draw inspiration from portfolio evaluation in finance and
develop a new metric, called SharpeRatio@k, which measures the risk-return
tradeoff of policy portfolios formed by an OPE estimator under varying online
evaluation budgets (k). We validate our metric in two example scenarios,
demonstrating its ability to effectively distinguish between low-risk and
high-risk estimators and to accurately identify the most efficient estimator.
This efficient estimator is characterized by its capability to form the most
advantageous policy portfolios, maximizing returns while minimizing risks
during online deployment, a nuance that existing metrics typically overlook. To
facilitate a quick, accurate, and consistent evaluation of OPE via
SharpeRatio@k, we have also integrated this metric into an open-source
software, SCOPE-RL. Employing SharpeRatio@k and SCOPE-RL, we conduct
comprehensive benchmarking experiments on various estimators and RL tasks,
focusing on their risk-return tradeoff. These experiments offer several
interesting directions and suggestions for future OPE research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18208">SMaRt: Improving GANs with Score Matching Regularity. (arXiv:2311.18208v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1">Mengfei Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yujun Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Ceyuan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_R/0/1/0/all/0/1">Ran Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenping Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yong-jin Liu</a></p>
<p>Generative adversarial networks (GANs) usually struggle in learning from
highly diverse data, whose underlying manifold is complex. In this work, we
revisit the mathematical foundations of GANs, and theoretically reveal that the
native adversarial loss for GAN training is insufficient to fix the problem of
subsets with positive Lebesgue measure of the generated data manifold lying out
of the real data manifold. Instead, we find that score matching serves as a
valid solution to this issue thanks to its capability of persistently pushing
the generated data points towards the real data manifold. We thereby propose to
improve the optimization of GANs with score matching regularity (SMaRt).
Regarding the empirical evidences, we first design a toy example to show that
training GANs by the aid of a ground-truth score function can help reproduce
the real data distribution more accurately, and then confirm that our approach
can consistently boost the synthesis performance of various state-of-the-art
GANs on real-world datasets with pre-trained diffusion models acting as the
approximate score function. For instance, when training Aurora on the ImageNet
64x64 dataset, we manage to improve FID from 8.87 to 7.11, on par with the
performance of one-step consistency model. The source code will be made public.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18224">Reasoning with the Theory of Mind for Pragmatic Semantic Communication. (arXiv:2311.18224v1 [cs.IT])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Thomas_C/0/1/0/all/0/1">Christo Kurisummoottil Thomas</a>, <a href="http://arxiv.org/find/cs/1/au:+Strinati_E/0/1/0/all/0/1">Emilio Calvanese Strinati</a>, <a href="http://arxiv.org/find/cs/1/au:+Saad_W/0/1/0/all/0/1">Walid Saad</a></p>
<p>In this paper, a pragmatic semantic communication framework that enables
effective goal-oriented information sharing between two-intelligent agents is
proposed. In particular, semantics is defined as the causal state that
encapsulates the fundamental causal relationships and dependencies among
different features extracted from data. The proposed framework leverages the
emerging concept in machine learning (ML) called theory of mind (ToM). It
employs a dynamic two-level (wireless and semantic) feedback mechanism to
continuously fine-tune neural network components at the transmitter. Thanks to
the ToM, the transmitter mimics the actual mental state of the receiver's
reasoning neural network operating semantic interpretation. Then, the estimated
mental state at the receiver is dynamically updated thanks to the proposed
dynamic two-level feedback mechanism. At the lower level, conventional channel
quality metrics are used to optimize the channel encoding process based on the
wireless communication channel's quality, ensuring an efficient mapping of
semantic representations to a finite constellation. Additionally, a semantic
feedback level is introduced, providing information on the receiver's perceived
semantic effectiveness with minimal overhead. Numerical evaluations demonstrate
the framework's ability to achieve efficient communication with a reduced
amount of bits while maintaining the same semantics, outperforming conventional
systems that do not exploit the ToM-based reasoning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18232">LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language Models. (arXiv:2311.18232v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abdulhai_M/0/1/0/all/0/1">Marwa Abdulhai</a>, <a href="http://arxiv.org/find/cs/1/au:+White_I/0/1/0/all/0/1">Isadora White</a>, <a href="http://arxiv.org/find/cs/1/au:+Snell_C/0/1/0/all/0/1">Charlie Snell</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1">Charles Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1">Joey Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1">Yuexiang Zhai</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Kelvin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1">Sergey Levine</a></p>
<p>Large language models (LLMs) provide excellent text-generation capabilities,
but standard prompting and generation methods generally do not lead to
intentional or goal-directed agents and might necessitate considerable prompt
tuning. This becomes particularly apparent in multi-turn conversations: even
the best current LLMs rarely ask clarifying questions, engage in explicit
information gathering, or take actions now that lead to better decisions after
multiple turns. Reinforcement learning has the potential to leverage the
powerful modeling capabilities of LLMs, as well as their internal
representation of textual interactions, to create capable goal-directed
language agents. This can enable intentional and temporally extended
interactions, such as with humans, through coordinated persuasion and carefully
crafted questions, or in goal-directed play through text games to bring about
desired final outcomes. However, enabling this requires the community to
develop stable and reliable reinforcement learning algorithms that can
effectively train LLMs. Developing such algorithms requires tasks that can
gauge progress on algorithm design, provide accessible and reproducible
evaluations for multi-turn interactions, and cover a range of task properties
and challenges in improving reinforcement learning algorithms. Our paper
introduces the LMRL-Gym benchmark for evaluating multi-turn RL for LLMs,
together with an open-source research framework containing a basic toolkit for
getting started on multi-turn RL with offline value-based and policy-based RL
methods. Our benchmark consists of 8 different language tasks, which require
multiple rounds of language interaction and cover a range of tasks in
open-ended dialogue and text games.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18237">Label-efficient Training of Small Task-specific Models by Leveraging Vision Foundation Models. (arXiv:2311.18237v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vemulapalli_R/0/1/0/all/0/1">Raviteja Vemulapalli</a>, <a href="http://arxiv.org/find/cs/1/au:+Pouransari_H/0/1/0/all/0/1">Hadi Pouransari</a>, <a href="http://arxiv.org/find/cs/1/au:+Faghri_F/0/1/0/all/0/1">Fartash Faghri</a>, <a href="http://arxiv.org/find/cs/1/au:+Mehta_S/0/1/0/all/0/1">Sachin Mehta</a>, <a href="http://arxiv.org/find/cs/1/au:+Farajtabar_M/0/1/0/all/0/1">Mehrdad Farajtabar</a>, <a href="http://arxiv.org/find/cs/1/au:+Rastegari_M/0/1/0/all/0/1">Mohammad Rastegari</a>, <a href="http://arxiv.org/find/cs/1/au:+Tuzel_O/0/1/0/all/0/1">Oncel Tuzel</a></p>
<p>Large Vision Foundation Models (VFMs) pretrained on massive datasets exhibit
impressive performance on various downstream tasks, especially with limited
labeled target data. However, due to their high memory and compute
requirements, these models cannot be deployed in resource constrained settings.
This raises an important question: How can we utilize the knowledge from a
large VFM to train a small task-specific model for a new target task with
limited labeled training data? In this work, we answer this question by
proposing a simple and highly effective task-oriented knowledge transfer
approach to leverage pretrained VFMs for effective training of small
task-specific models. Our experimental results on four target tasks under
limited labeled data settings show that the proposed knowledge transfer
approach outperforms task-agnostic VFM distillation, web-scale CLIP pretraining
and supervised ImageNet pretraining by 1-10.5%, 2-22% and 2-14%, respectively.
We also show that the dataset used for transferring knowledge has a significant
effect on the final target task performance, and propose an image
retrieval-based approach for curating effective transfer sets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18243">DKiS: Decay weight invertible image steganography with private key. (arXiv:2311.18243v1 [cs.MM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Hang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yitian Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xuhua Liu</a></p>
<p>Image steganography, the practice of concealing information within another
image, traditionally faces security challenges when its methods become publicly
known. To counteract this, we introduce a novel private key-based image
steganography technique. This approach ensures the security of hidden
information, requiring a corresponding private key for access, irrespective of
the public knowledge of the steganography method. We present experimental
evidence demonstrating our method's effectiveness, showcasing its real-world
applicability. Additionally, we identified a critical challenge in the
invertible image steganography process: the transfer of non-essential, or
`garbage', information from the secret to the host pipeline. To address this,
we introduced the decay weight to control the information transfer, filtering
out irrelevant data and enhancing the performance of image steganography. Our
code is publicly accessible at https://github.com/yanghangAI/DKiS, and a
practical demonstration is available at <a href="http://yanghang.site/hidekey.">this http URL</a>
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18244">Poisoning Attacks Against Contrastive Recommender Systems. (arXiv:2311.18244v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zongwei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Junliang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1">Min Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1">Hongzhi Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1">Bin Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Sadiq_S/0/1/0/all/0/1">Shazia Sadiq</a></p>
<p>Contrastive learning (CL) has recently gained significant popularity in the
field of recommendation. Its ability to learn without heavy reliance on labeled
data is a natural antidote to the data sparsity issue. Previous research has
found that CL can not only enhance recommendation accuracy but also
inadvertently exhibit remarkable robustness against noise. However, this paper
identifies a vulnerability of CL-based recommender systems: Compared with their
non-CL counterparts, they are even more susceptible to poisoning attacks that
aim to promote target items. Our analysis points to the uniform dispersion of
representations led by the CL loss as the very factor that accounts for this
vulnerability. We further theoretically and empirically demonstrate that the
optimization of CL loss can lead to smooth spectral values of representations.
Based on these insights, we attempt to reveal the potential poisoning attacks
against CL-based recommender systems. The proposed attack encompasses a
dual-objective framework: One that induces a smoother spectral value
distribution to amplify the CL loss's inherent dispersion effect, named
dispersion promotion; and the other that directly elevates the visibility of
target items, named rank promotion. We validate the destructiveness of our
attack model through extensive experimentation on four datasets. By shedding
light on these vulnerabilities, we aim to facilitate the development of more
robust CL-based recommender systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18246">Combined Scheduling, Memory Allocation and Tensor Replacement for Minimizing Off-Chip Data Accesses of DNN Accelerators. (arXiv:2311.18246v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1">Aarti Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Malik_S/0/1/0/all/0/1">Sharad Malik</a></p>
<p>Specialized hardware accelerators have been extensively used for Deep Neural
Networks (DNNs) to provide power/performance benefits. These accelerators
contain specialized hardware that supports DNN operators, and scratchpad memory
for storing the tensor operands. Often, the size of the scratchpad is
insufficient to store all the tensors needed for the computation, and
additional data accesses are needed to move tensors back and forth from host
memory during the computation with significant power/performance overhead. The
volume of these additional data accesses depends on the operator schedule, and
memory allocation (specific locations selected for the tensors in the
scratchpad). We propose an optimization framework, named COSMA, for mapping
DNNs to an accelerator that finds the optimal operator schedule, memory
allocation and tensor replacement that minimizes the additional data accesses.
COSMA provides an Integer Linear Programming (ILP) formulation to generate the
optimal solution for mapping a DNN to the accelerator for a given scratchpad
size. We demonstrate that, using an off-the-shelf ILP solver, COSMA obtains the
optimal solution in seconds for a wide-range of state-of-the-art DNNs for
different applications. Further, it out-performs existing methods by reducing
on average 84% of the non-compulsory data accesses. We further propose a
divide-and-conquer heuristic to scale up to certain complex DNNs generated by
Neural Architecture Search, and this heuristic solution reduces on average 85%
data accesses compared with other works.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18252">Navigating Privacy and Copyright Challenges Across the Data Lifecycle of Generative AI. (arXiv:2311.18252v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dawen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_B/0/1/0/all/0/1">Boming Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yue Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiwei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1">Thong Hoang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1">Zhenchang Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Staples_M/0/1/0/all/0/1">Mark Staples</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1">Qinghua Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1">Liming Zhu</a></p>
<p>The advent of Generative AI has marked a significant milestone in artificial
intelligence, demonstrating remarkable capabilities in generating realistic
images, texts, and data patterns. However, these advancements come with
heightened concerns over data privacy and copyright infringement, primarily due
to the reliance on vast datasets for model training. Traditional approaches
like differential privacy, machine unlearning, and data poisoning only offer
fragmented solutions to these complex issues. Our paper delves into the
multifaceted challenges of privacy and copyright protection within the data
lifecycle. We advocate for integrated approaches that combines technical
innovation with ethical foresight, holistically addressing these concerns by
investigating and devising solutions that are informed by the lifecycle
perspective. This work aims to catalyze a broader discussion and inspire
concerted efforts towards data privacy and copyright integrity in Generative
AI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18257">Diffusion Models Without Attention. (arXiv:2311.18257v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1">Jing Nathan Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jiatao Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1">Alexander M. Rush</a></p>
<p>In recent advancements in high-fidelity image generation, Denoising Diffusion
Probabilistic Models (DDPMs) have emerged as a key player. However, their
application at high resolutions presents significant computational challenges.
Current methods, such as patchifying, expedite processes in UNet and
Transformer architectures but at the expense of representational capacity.
Addressing this, we introduce the Diffusion State Space Model (DiffuSSM), an
architecture that supplants attention mechanisms with a more scalable state
space model backbone. This approach effectively handles higher resolutions
without resorting to global compression, thus preserving detailed image
representation throughout the diffusion process. Our focus on FLOP-efficient
architectures in diffusion training marks a significant step forward.
Comprehensive evaluations on both ImageNet and LSUN datasets at two resolutions
demonstrate that DiffuSSMs are on par or even outperform existing diffusion
models with attention modules in FID and Inception Score metrics while
significantly reducing total FLOP usage.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18260">Consensus, dissensus and synergy between clinicians and specialist foundation models in radiology report generation. (arXiv:2311.18260v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Tanno_R/0/1/0/all/0/1">Ryutaro Tanno</a>, <a href="http://arxiv.org/find/eess/1/au:+Barrett_D/0/1/0/all/0/1">David G.T. Barrett</a>, <a href="http://arxiv.org/find/eess/1/au:+Sellergren_A/0/1/0/all/0/1">Andrew Sellergren</a>, <a href="http://arxiv.org/find/eess/1/au:+Ghaisas_S/0/1/0/all/0/1">Sumedh Ghaisas</a>, <a href="http://arxiv.org/find/eess/1/au:+Dathathri_S/0/1/0/all/0/1">Sumanth Dathathri</a>, <a href="http://arxiv.org/find/eess/1/au:+See_A/0/1/0/all/0/1">Abigail See</a>, <a href="http://arxiv.org/find/eess/1/au:+Welbl_J/0/1/0/all/0/1">Johannes Welbl</a>, <a href="http://arxiv.org/find/eess/1/au:+Singhal_K/0/1/0/all/0/1">Karan Singhal</a>, <a href="http://arxiv.org/find/eess/1/au:+Azizi_S/0/1/0/all/0/1">Shekoofeh Azizi</a>, <a href="http://arxiv.org/find/eess/1/au:+Tu_T/0/1/0/all/0/1">Tao Tu</a>, <a href="http://arxiv.org/find/eess/1/au:+Schaekermann_M/0/1/0/all/0/1">Mike Schaekermann</a>, <a href="http://arxiv.org/find/eess/1/au:+May_R/0/1/0/all/0/1">Rhys May</a>, <a href="http://arxiv.org/find/eess/1/au:+Lee_R/0/1/0/all/0/1">Roy Lee</a>, <a href="http://arxiv.org/find/eess/1/au:+Man_S/0/1/0/all/0/1">SiWai Man</a>, <a href="http://arxiv.org/find/eess/1/au:+Ahmed_Z/0/1/0/all/0/1">Zahra Ahmed</a>, <a href="http://arxiv.org/find/eess/1/au:+Mahdavi_S/0/1/0/all/0/1">Sara Mahdavi</a>, <a href="http://arxiv.org/find/eess/1/au:+Belgrave_D/0/1/0/all/0/1">Danielle Belgrave</a>, <a href="http://arxiv.org/find/eess/1/au:+Natarajan_V/0/1/0/all/0/1">Vivek Natarajan</a>, <a href="http://arxiv.org/find/eess/1/au:+Shetty_S/0/1/0/all/0/1">Shravya Shetty</a>, <a href="http://arxiv.org/find/eess/1/au:+Kohli_P/0/1/0/all/0/1">Pushmeet Kohli</a>, <a href="http://arxiv.org/find/eess/1/au:+Huang_P/0/1/0/all/0/1">Po-Sen Huang</a>, <a href="http://arxiv.org/find/eess/1/au:+Karthikesalingam_A/0/1/0/all/0/1">Alan Karthikesalingam</a>, <a href="http://arxiv.org/find/eess/1/au:+Ktena_I/0/1/0/all/0/1">Ira Ktena</a></p>
<p>Radiology reports are an instrumental part of modern medicine, informing key
clinical decisions such as diagnosis and treatment. The worldwide shortage of
radiologists, however, restricts access to expert care and imposes heavy
workloads, contributing to avoidable errors and delays in report delivery.
While recent progress in automated report generation with vision-language
models offer clear potential in ameliorating the situation, the path to
real-world adoption has been stymied by the challenge of evaluating the
clinical quality of AI-generated reports. In this study, we build a
state-of-the-art report generation system for chest radiographs, Flamingo-CXR,
by fine-tuning a well-known vision-language foundation model on radiology data.
To evaluate the quality of the AI-generated reports, a group of 16 certified
radiologists provide detailed evaluations of AI-generated and human written
reports for chest X-rays from an intensive care setting in the United States
and an inpatient setting in India. At least one radiologist (out of two per
case) preferred the AI report to the ground truth report in over 60$\%$ of
cases for both datasets. Amongst the subset of AI-generated reports that
contain errors, the most frequently cited reasons were related to the location
and finding, whereas for human written reports, most mistakes were related to
severity and finding. This disparity suggested potential complementarity
between our AI system and human experts, prompting us to develop an assistive
scenario in which Flamingo-CXR generates a first-draft report, which is
subsequently revised by a clinician. This is the first demonstration of
clinician-AI collaboration for report writing, and the resultant reports are
assessed to be equivalent or preferred by at least one radiologist to reports
written by experts alone in 80$\%$ of in-patient cases and 66$\%$ of intensive
care cases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18261">Learning Exactly Linearizable Deep Dynamics Models. (arXiv:2311.18261v1 [eess.SY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Moriyasu_R/0/1/0/all/0/1">Ryuta Moriyasu</a>, <a href="http://arxiv.org/find/eess/1/au:+Kusunoki_M/0/1/0/all/0/1">Masayuki Kusunoki</a>, <a href="http://arxiv.org/find/eess/1/au:+Kashima_K/0/1/0/all/0/1">Kenji Kashima</a></p>
<p>Research on control using models based on machine-learning methods has now
shifted to the practical engineering stage. Achieving high performance and
theoretically guaranteeing the safety of the system is critical for such
applications. In this paper, we propose a learning method for exactly
linearizable dynamical models that can easily apply various control theories to
ensure stability, reliability, etc., and to provide a high degree of freedom of
expression. As an example, we present a design that combines simple linear
control and control barrier functions. The proposed model is employed for the
real-time control of an automotive engine, and the results demonstrate good
predictive performance and stable control under constraints.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18274">Semiparametric Efficient Inference in Adaptive Experiments. (arXiv:2311.18274v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Cook_T/0/1/0/all/0/1">Thomas Cook</a>, <a href="http://arxiv.org/find/stat/1/au:+Mishler_A/0/1/0/all/0/1">Alan Mishler</a>, <a href="http://arxiv.org/find/stat/1/au:+Ramdas_A/0/1/0/all/0/1">Aaditya Ramdas</a></p>
<p>We consider the problem of efficient inference of the Average Treatment
Effect in a sequential experiment where the policy governing the assignment of
subjects to treatment or control can change over time. We first provide a
central limit theorem for the Adaptive Augmented Inverse-Probability Weighted
estimator, which is semiparametric efficient, under weaker assumptions than
those previously made in the literature. This central limit theorem enables
efficient inference at fixed sample sizes. We then consider a sequential
inference setting, deriving both asymptotic and nonasymptotic confidence
sequences that are considerably tighter than previous methods. These
anytime-valid methods enable inference under data-dependent stopping times
(sample sizes). Additionally, we use propensity score truncation techniques
from the recent off-policy estimation literature to reduce the finite sample
variance of our estimator without affecting the asymptotic variance. Empirical
results demonstrate that our methods yield narrower confidence sequences than
those previously developed in the literature while maintaining time-uniform
error control.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18306">PAUNet: Precipitation Attention-based U-Net for rain prediction from satellite radiance data. (arXiv:2311.18306v1 [physics.ao-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Reddy_P/0/1/0/all/0/1">P. Jyoteeshkumar Reddy</a>, <a href="http://arxiv.org/find/physics/1/au:+Baki_H/0/1/0/all/0/1">Harish Baki</a>, <a href="http://arxiv.org/find/physics/1/au:+Chinta_S/0/1/0/all/0/1">Sandeep Chinta</a>, <a href="http://arxiv.org/find/physics/1/au:+Matear_R/0/1/0/all/0/1">Richard Matear</a>, <a href="http://arxiv.org/find/physics/1/au:+Taylor_J/0/1/0/all/0/1">John Taylor</a></p>
<p>This paper introduces Precipitation Attention-based U-Net (PAUNet), a deep
learning architecture for predicting precipitation from satellite radiance
data, addressing the challenges of the Weather4cast 2023 competition. PAUNet is
a variant of U-Net and Res-Net, designed to effectively capture the large-scale
contextual information of multi-band satellite images in visible, water vapor,
and infrared bands through encoder convolutional layers with center cropping
and attention mechanisms. We built upon the Focal Precipitation Loss including
an exponential component (e-FPL), which further enhanced the importance across
different precipitation categories, particularly medium and heavy rain. Trained
on a substantial dataset from various European regions, PAUNet demonstrates
notable accuracy with a higher Critical Success Index (CSI) score than the
baseline model in predicting rainfall over multiple time slots. PAUNet's
architecture and training methodology showcase improvements in precipitation
forecasting, crucial for sectors like emergency services and retail and supply
chain management.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18307">Categorical Traffic Transformer: Interpretable and Diverse Behavior Prediction with Tokenized Latent. (arXiv:2311.18307v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuxiao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tonkens_S/0/1/0/all/0/1">Sander Tonkens</a>, <a href="http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1">Marco Pavone</a></p>
<p>Adept traffic models are critical to both planning and closed-loop simulation
for autonomous vehicles (AV), and key design objectives include accuracy,
diverse multimodal behaviors, interpretability, and downstream compatibility.
Recently, with the advent of large language models (LLMs), an additional
desirable feature for traffic models is LLM compatibility. We present
Categorical Traffic Transformer (CTT), a traffic model that outputs both
continuous trajectory predictions and tokenized categorical predictions (lane
modes, homotopies, etc.). The most outstanding feature of CTT is its fully
interpretable latent space, which enables direct supervision of the latent
variable from the ground truth during training and avoids mode collapse
completely. As a result, CTT can generate diverse behaviors conditioned on
different latent modes with semantic meanings while beating SOTA on prediction
accuracy. In addition, CTT's ability to input and output tokens enables
integration with LLMs for common-sense reasoning and zero-shot generalization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18313">Automatic Implementation of Neural Networks through Reaction Networks -- Part I: Circuit Design and Convergence Analysis. (arXiv:2311.18313v1 [math.DS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Fan_Y/0/1/0/all/0/1">Yuzhen Fan</a>, <a href="http://arxiv.org/find/math/1/au:+Zhang_X/0/1/0/all/0/1">Xiaoyu Zhang</a>, <a href="http://arxiv.org/find/math/1/au:+Gao_C/0/1/0/all/0/1">Chuanhou Gao</a>, <a href="http://arxiv.org/find/math/1/au:+Dochain_D/0/1/0/all/0/1">Denis Dochain</a></p>
<p>Information processing relying on biochemical interactions in the cellular
environment is essential for biological organisms. The implementation of
molecular computational systems holds significant interest and potential in the
fields of synthetic biology and molecular computation. This two-part article
aims to introduce a programmable biochemical reaction network (BCRN) system
endowed with mass action kinetics that realizes the fully connected neural
network (FCNN) and has the potential to act automatically in vivo. In part I,
the feedforward propagation computation, the backpropagation component, and all
bridging processes of FCNN are ingeniously designed as specific BCRN modules
based on their dynamics. This approach addresses a design gap in the
biochemical assignment module and judgment termination module and provides a
novel precise and robust realization of bi-molecular reactions for the learning
process. Through equilibrium approaching, we demonstrate that the designed BCRN
system achieves FCNN functionality with exponential convergence to target
computational results, thereby enhancing the theoretical support for such work.
Finally, the performance of this construction is further evaluated on two
typical logic classification problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18316">Learning for Semantic Knowledge Base-Guided Online Feature Transmission in Dynamic Channels. (arXiv:2311.18316v1 [cs.IT])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xiangyu Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yaping Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1">Dongyu Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiaodong Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1">Hao Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1">Shuguang Cui</a></p>
<p>With the proliferation of edge computing, efficient AI inference on edge
devices has become essential for intelligent applications such as autonomous
vehicles and VR/AR. In this context, we address the problem of efficient remote
object recognition by optimizing feature transmission between mobile devices
and edge servers. We propose an online optimization framework to address the
challenge of dynamic channel conditions and device mobility in an end-to-end
communication system. Our approach builds upon existing methods by leveraging a
semantic knowledge base to drive multi-level feature transmission, accounting
for temporal factors and dynamic elements throughout the transmission process.
To solve the online optimization problem, we design a novel soft
actor-critic-based deep reinforcement learning system with a carefully designed
reward function for real-time decision-making, overcoming the optimization
difficulty of the NP-hard problem and achieving the minimization of semantic
loss while respecting latency constraints. Numerical results showcase the
superiority of our approach compared to traditional greedy methods under
various system setups.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18341">Learning Robust Precipitation Forecaster by Temporal Frame Interpolation. (arXiv:2311.18341v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1">Lu Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xu-Yang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1">Han-Jia Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1">De-Chuan Zhan</a></p>
<p>Recent advancements in deep learning have propelled the field of weather
prediction models to new heights. Despite their progress, these models often
struggle with real-world application due to their sensitivity to
spatial-temporal shifts, a vulnerability particularly pronounced in weather
prediction tasks where overfitting to local and temporal variations is common.
This paper presents an investigation into the development of a robust
precipitation forecasting model that stands resilient to such shifts. We
introduce Temporal Frame Interpolation (TFI), an innovative technique designed
to fortify forecasting models against spatial-temporal discrepancies. TFI
operates by generating synthetic samples through the interpolation of adjacent
frames from satellite imagery and ground radar data, thereby enriching the
training dataset and bolstering the model's defense against noise on frames.
Additionally, we integrate a novel multi-level dice loss, which exploits the
ordinal nature of rainfall intensities to further refine model performance.
These methodologies have collectively advanced our model's forecasting
precision, achieving \textit{1st place} on the transfer learning leaderboard in
the \textit{Weather4Cast'23 competition}.It not only demonstrates the efficacy
of our approaches but also sets a new benchmark for deep learning applications
in meteorological forecasting. Our code and weights have been public on
\url{https://github.com/Secilia-Cxy/UNetTFI}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18348">Reconstructing Historical Climate Fields With Deep Learning. (arXiv:2311.18348v1 [physics.geo-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Bochow_N/0/1/0/all/0/1">Nils Bochow</a>, <a href="http://arxiv.org/find/physics/1/au:+Poltronieri_A/0/1/0/all/0/1">Anna Poltronieri</a>, <a href="http://arxiv.org/find/physics/1/au:+Rypdal_M/0/1/0/all/0/1">Martin Rypdal</a>, <a href="http://arxiv.org/find/physics/1/au:+Boers_N/0/1/0/all/0/1">Niklas Boers</a></p>
<p>Historical records of climate fields are often sparse due to missing
measurements, especially before the introduction of large-scale satellite
missions. Several statistical and model-based methods have been introduced to
fill gaps and reconstruct historical records. Here, we employ a recently
introduced deep-learning approach based on Fourier convolutions, trained on
numerical climate model output, to reconstruct historical climate fields. Using
this approach we are able to realistically reconstruct large and irregular
areas of missing data, as well as reconstruct known historical events such as
strong El Ni\~no and La Ni\~na with very little given information. Our method
outperforms the widely used statistical kriging method as well as other recent
machine learning approaches. The model generalizes to higher resolutions than
the ones it was trained on and can be used on a variety of climate fields.
Moreover, it allows inpainting of masks never seen before during the model
training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18356">Towards Comparable Active Learning. (arXiv:2311.18356v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Werner_T/0/1/0/all/0/1">Thorben Werner</a>, <a href="http://arxiv.org/find/cs/1/au:+Burchert_J/0/1/0/all/0/1">Johannes Burchert</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmidt_Thieme_L/0/1/0/all/0/1">Lars Schmidt-Thieme</a></p>
<p>Active Learning has received significant attention in the field of machine
learning for its potential in selecting the most informative samples for
labeling, thereby reducing data annotation costs. However, we show that the
reported lifts in recent literature generalize poorly to other domains leading
to an inconclusive landscape in Active Learning research. Furthermore, we
highlight overlooked problems for reproducing AL experiments that can lead to
unfair comparisons and increased variance in the results. This paper addresses
these issues by providing an Active Learning framework for a fair comparison of
algorithms across different tasks and domains, as well as a fast and performant
oracle algorithm for evaluation. To the best of our knowledge, we propose the
first AL benchmark that tests algorithms in 3 major domains: Tabular, Image,
and Text. We report empirical results for 6 widely used algorithms on 7
real-world and 2 synthetic datasets and aggregate them into a domain-specific
ranking of AL algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18364">Hubness Reduction Improves Sentence-BERT Semantic Spaces. (arXiv:2311.18364v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nielsen_B/0/1/0/all/0/1">Beatrix M. G. Nielsen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hansen_L/0/1/0/all/0/1">Lars Kai Hansen</a></p>
<p>Semantic representations of text, i.e. representations of natural language
which capture meaning by geometry, are essential for areas such as information
retrieval and document grouping. High-dimensional trained dense vectors have
received much attention in recent years as such representations. We investigate
the structure of semantic spaces that arise from embeddings made with
Sentence-BERT and find that the representations suffer from a well-known
problem in high dimensions called hubness. Hubness results in asymmetric
neighborhood relations, such that some texts (the hubs) are neighbours of many
other texts while most texts (so-called anti-hubs), are neighbours of few or no
other texts. We quantify the semantic quality of the embeddings using hubness
scores and error rate of a neighbourhood based classifier. We find that when
hubness is high, we can reduce error rate and hubness using hubness reduction
methods. We identify a combination of two methods as resulting in the best
reduction. For example, on one of the tested pretrained models, this combined
method can reduce hubness by about 75% and error rate by about 9%. Thus, we
argue that mitigating hubness in the embedding space provides better semantic
representations of text.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18376">Age Effects on Decision-Making, Drift Diffusion Model. (arXiv:2311.18376v1 [q-bio.NC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Kavian_Z/0/1/0/all/0/1">Zahra Kavian</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Hajisadeghi_K/0/1/0/all/0/1">Kimia Hajisadeghi</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Rezazadeh_Y/0/1/0/all/0/1">Yashar Rezazadeh</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Faraji_M/0/1/0/all/0/1">Mehrbod Faraji</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Ebrahimpour_R/0/1/0/all/0/1">Reza Ebrahimpour</a></p>
<p>Training can improve human decision-making performance. After several
training sessions, a person can quickly and accurately complete a task.
However, decision-making is always a trade-off between accuracy and response
time. Factors such as age and drug abuse can affect the decision-making
process. This study examines how training can improve the performance of
different age groups in completing a random dot motion (RDM) task. The
participants are divided into two groups: old and young. They undergo a
three-phase training and then repeat the same RDM task. The hierarchical
drift-diffusion model analyzes the subjects' responses and determines how the
model's parameters change after training for both age groups. The results show
that after training, the participants were able to accumulate sensory
information faster, and the model drift rate increased. However, their decision
boundary decreased as they became more confident and had a lower
decision-making threshold. Additionally, the old group had a higher boundary
and lower drift rate in both pre and post-training, and there was less
difference between the two group parameters after training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18377">Transfer Learning across Different Chemical Domains: Virtual Screening of Organic Materials with Deep Learning Models Pretrained on Small Molecule and Chemical Reaction Data. (arXiv:2311.18377v1 [physics.chem-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Zhang_C/0/1/0/all/0/1">Chengwei Zhang</a>, <a href="http://arxiv.org/find/physics/1/au:+Zhai_Y/0/1/0/all/0/1">Yushuang Zhai</a>, <a href="http://arxiv.org/find/physics/1/au:+Gong_Z/0/1/0/all/0/1">Ziyang Gong</a>, <a href="http://arxiv.org/find/physics/1/au:+She_Y/0/1/0/all/0/1">Yuan-Bin She</a>, <a href="http://arxiv.org/find/physics/1/au:+Yang_Y/0/1/0/all/0/1">Yun-Fang Yang</a>, <a href="http://arxiv.org/find/physics/1/au:+Su_A/0/1/0/all/0/1">An Su</a></p>
<p>Machine learning prediction of organic materials properties is an efficient
virtual screening method ahead of more expensive screening methods. However,
this approach has suffered from insufficient labeled data on organic materials
to train state-of-the-art machine learning models. In this study, we
demonstrate that drug-like small molecule and chemical reaction databases can
be used to pretrain the BERT model for the virtual screening of organic
materials. Among the BERT models fine-tuned by five virtual screening tasks on
organic materials, the USPTO-SMILES pretrained BERT model had R2 &gt; 0.90 for two
tasks and R2 &gt; 0.82 for one, which was generally superior to the same models
pretrained by the small molecule or organic materials databases, as well as to
the other three traditional machine learning models trained directly on the
virtual screening task data. The superior performance of the USPTO-SMILES
pretrained BERT model is due to the greater variety of organic building blocks
in the USPTO database and the broader coverage of the chemical space. The even
better performance of the BERT model pretrained externally from a chemical
reaction database with additional sources of chemical reactions strengthens our
proof of concept that transfer learning across different chemical domains is
practical for the virtual screening of organic materials.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18387">On Exact Inversion of DPM-Solvers. (arXiv:2311.18387v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1">Seongmin Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kyeonghyun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeon_S/0/1/0/all/0/1">Suh Yoon Jeon</a>, <a href="http://arxiv.org/find/cs/1/au:+Bae_H/0/1/0/all/0/1">Hyewon Bae</a>, <a href="http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1">Se Young Chun</a></p>
<p>Diffusion probabilistic models (DPMs) are a key component in modern
generative models. DPM-solvers have achieved reduced latency and enhanced
quality significantly, but have posed challenges to find the exact inverse
(i.e., finding the initial noise from the given image). Here we investigate the
exact inversions for DPM-solvers and propose algorithms to perform them when
samples are generated by the first-order as well as higher-order DPM-solvers.
For each explicit denoising step in DPM-solvers, we formulated the inversions
using implicit methods such as gradient descent or forward step method to
ensure the robustness to large classifier-free guidance unlike the prior
approach using fixed-point iteration. Experimental results demonstrated that
our proposed exact inversion methods significantly reduced the error of both
image and noise reconstructions, greatly enhanced the ability to distinguish
invisible watermarks and well prevented unintended background changes
consistently during image editing. Project page:
\url{https://smhongok.github.io/inv-dpm.html}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18393">Data-efficient Deep Reinforcement Learning for Vehicle Trajectory Control. (arXiv:2311.18393v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Frauenknecht_B/0/1/0/all/0/1">Bernd Frauenknecht</a>, <a href="http://arxiv.org/find/cs/1/au:+Ehlgen_T/0/1/0/all/0/1">Tobias Ehlgen</a>, <a href="http://arxiv.org/find/cs/1/au:+Trimpe_S/0/1/0/all/0/1">Sebastian Trimpe</a></p>
<p>Advanced vehicle control is a fundamental building block in the development
of autonomous driving systems. Reinforcement learning (RL) promises to achieve
control performance superior to classical approaches while keeping
computational demands low during deployment. However, standard RL approaches
like soft-actor critic (SAC) require extensive amounts of training data to be
collected and are thus impractical for real-world application. To address this
issue, we apply recently developed data-efficient deep RL methods to vehicle
trajectory control. Our investigation focuses on three methods, so far
unexplored for vehicle control: randomized ensemble double Q-learning (REDQ),
probabilistic ensembles with trajectory sampling and model predictive path
integral optimizer (PETS-MPPI), and model-based policy optimization (MBPO). We
find that in the case of trajectory control, the standard model-based RL
formulation used in approaches like PETS-MPPI and MBPO is not suitable. We,
therefore, propose a new formulation that splits dynamics prediction and
vehicle localization. Our benchmark study on the CARLA simulator reveals that
the three identified data-efficient deep RL approaches learn control strategies
on a par with or better than SAC, yet reduce the required number of environment
interactions by more than one order of magnitude.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18398">RainAI -- Precipitation Nowcasting from Satellite Data. (arXiv:2311.18398v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sarabia_R/0/1/0/all/0/1">Rafael Pablos Sarabia</a>, <a href="http://arxiv.org/find/cs/1/au:+Nyborg_J/0/1/0/all/0/1">Joachim Nyborg</a>, <a href="http://arxiv.org/find/cs/1/au:+Birk_M/0/1/0/all/0/1">Morten Birk</a>, <a href="http://arxiv.org/find/cs/1/au:+Assent_I/0/1/0/all/0/1">Ira Assent</a></p>
<p>This paper presents a solution to the Weather4Cast 2023 competition, where
the goal is to forecast high-resolution precipitation with an 8-hour lead time
using lower-resolution satellite radiance images. We propose a simple, yet
effective method for spatiotemporal feature learning using a 2D U-Net model,
that outperforms the official 3D U-Net baseline in both performance and
efficiency. We place emphasis on refining the dataset, through importance
sampling and dataset preparation, and show that such techniques have a
significant impact on performance. We further study an alternative
cross-entropy loss function that improves performance over the standard mean
squared error loss, while also enabling models to produce probabilistic
outputs. Additional techniques are explored regarding the generation of
predictions at different lead times, specifically through Conditioning Lead
Time. Lastly, to generate high-resolution forecasts, we evaluate standard and
learned upsampling methods. The code and trained parameters are available at
https://github.com/rafapablos/w4c23-rainai.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18426">Convergence Analysis of Fractional Gradient Descent. (arXiv:2311.18426v1 [math.OC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Aggarwal_A/0/1/0/all/0/1">Ashwani Aggarwal</a></p>
<p>Fractional derivatives are a well-studied generalization of integer order
derivatives. Naturally, for optimization, it is of interest to understand the
convergence properties of gradient descent using fractional derivatives.
Convergence analysis of fractional gradient descent is currently limited both
in the methods analyzed and the settings analyzed. This paper aims to fill in
these gaps by analyzing variations of fractional gradient descent in smooth and
convex, smooth and strongly convex, and smooth and non-convex settings. First,
novel bounds will be established bridging fractional and integer derivatives.
Then, these bounds will be applied to the aforementioned settings to prove
$O(1/T)$ convergence for smooth and convex functions and linear convergence for
smooth and strongly convex functions. Additionally, we prove $O(1/T)$
convergence for smooth and non-convex functions using an extended notion of
smoothness that is more natural for fractional derivatives. Finally, empirical
results will be presented on the potential speed up of fractional gradient
descent over standard gradient descent as well as the challenges of predicting
which will be faster in general.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18431">On the convergence of adaptive first order methods: proximal gradient and alternating minimization algorithms. (arXiv:2311.18431v1 [math.OC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Latafat_P/0/1/0/all/0/1">Puya Latafat</a>, <a href="http://arxiv.org/find/math/1/au:+Themelis_A/0/1/0/all/0/1">Andreas Themelis</a>, <a href="http://arxiv.org/find/math/1/au:+Patrinos_P/0/1/0/all/0/1">Panagiotis Patrinos</a></p>
<p>Building upon recent works on linesearch-free adaptive proximal gradient
methods, this paper proposes AdaPG$^{\pi,r}$, a framework that unifies and
extends existing results by providing larger stepsize policies and improved
lower bounds. Different choices of the parameters $\pi$ and $r$ are discussed
and the efficacy of the resulting methods is demonstrated through numerical
simulations. In an attempt to better understand the underlying theory, its
convergence is established in a more general setting that allows for
time-varying parameters. Finally, an adaptive alternating minimization
algorithm is presented by exploring the dual setting. This algorithm not only
incorporates additional adaptivity, but also expands its applicability beyond
standard strongly convex settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18434">Exploring the Temperature-Dependent Phase Transition in Modern Hopfield Networks. (arXiv:2311.18434v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Koulischer_F/0/1/0/all/0/1">Felix Koulischer</a>, <a href="http://arxiv.org/find/cs/1/au:+Goemaere_C/0/1/0/all/0/1">C&#xe9;dric Goemaere</a>, <a href="http://arxiv.org/find/cs/1/au:+Meersch_T/0/1/0/all/0/1">Tom van der Meersch</a>, <a href="http://arxiv.org/find/cs/1/au:+Deleu_J/0/1/0/all/0/1">Johannes Deleu</a>, <a href="http://arxiv.org/find/cs/1/au:+Demeester_T/0/1/0/all/0/1">Thomas Demeester</a></p>
<p>The recent discovery of a connection between Transformers and Modern Hopfield
Networks (MHNs) has reignited the study of neural networks from a physical
energy-based perspective. This paper focuses on the pivotal effect of the
inverse temperature hyperparameter $\beta$ on the distribution of energy minima
of the MHN. To achieve this, the distribution of energy minima is tracked in a
simplified MHN in which equidistant normalised patterns are stored. This
network demonstrates a phase transition at a critical temperature
$\beta_{\text{c}}$, from a single global attractor towards highly pattern
specific minima as $\beta$ is increased. Importantly, the dynamics are not
solely governed by the hyperparameter $\beta$ but are instead determined by an
effective inverse temperature $\beta_{\text{eff}}$ which also depends on the
distribution and size of the stored patterns. Recognizing the role of
hyperparameters in the MHN could, in the future, aid researchers in the domain
of Transformers to optimise their initial choices, potentially reducing the
necessity for time and energy expensive hyperparameter fine-tuning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18437">The Sliding Regret in Stochastic Bandits: Discriminating Index and Randomized Policies. (arXiv:2311.18437v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Boone_V/0/1/0/all/0/1">Victor Boone</a></p>
<p>This paper studies the one-shot behavior of no-regret algorithms for
stochastic bandits. Although many algorithms are known to be asymptotically
optimal with respect to the expected regret, over a single run, their
pseudo-regret seems to follow one of two tendencies: it is either smooth or
bumpy. To measure this tendency, we introduce a new notion: the sliding regret,
that measures the worst pseudo-regret over a time-window of fixed length
sliding to infinity. We show that randomized methods (e.g. Thompson Sampling
and MED) have optimal sliding regret, while index policies, although possibly
asymptotically optimal for the expected regret, have the worst possible sliding
regret under regularity conditions on their index (e.g. UCB, UCB-V, KL-UCB,
MOSS, IMED etc.). We further analyze the average bumpiness of the pseudo-regret
of index policies via the regret of exploration, that we show to be suboptimal
as well.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18451">How Much Is Hidden in the NAS Benchmarks? Few-Shot Adaptation of a NAS Predictor. (arXiv:2311.18451v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Loya_H/0/1/0/all/0/1">Hrushikesh Loya</a>, <a href="http://arxiv.org/find/cs/1/au:+Dudziak_L/0/1/0/all/0/1">&#x141;ukasz Dudziak</a>, <a href="http://arxiv.org/find/cs/1/au:+Mehrotra_A/0/1/0/all/0/1">Abhinav Mehrotra</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1">Royson Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Fernandez_Marques_J/0/1/0/all/0/1">Javier Fernandez-Marques</a>, <a href="http://arxiv.org/find/cs/1/au:+Lane_N/0/1/0/all/0/1">Nicholas D. Lane</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1">Hongkai Wen</a></p>
<p>Neural architecture search has proven to be a powerful approach to designing
and refining neural networks, often boosting their performance and efficiency
over manually-designed variations, but comes with computational overhead. While
there has been a considerable amount of research focused on lowering the cost
of NAS for mainstream tasks, such as image classification, a lot of those
improvements stem from the fact that those tasks are well-studied in the
broader context. Consequently, applicability of NAS to emerging and
under-represented domains is still associated with a relatively high cost
and/or uncertainty about the achievable gains. To address this issue, we turn
our focus towards the recent growth of publicly available NAS benchmarks in an
attempt to extract general NAS knowledge, transferable across different tasks
and search spaces. We borrow from the rich field of meta-learning for few-shot
adaptation and carefully study applicability of those methods to NAS, with a
special focus on the relationship between task-level correlation (domain shift)
and predictor transferability; which we deem critical for improving NAS on
diverse tasks. In our experiments, we use 6 NAS benchmarks in conjunction,
spanning in total 16 NAS settings -- our meta-learning approach not only shows
superior (or matching) performance in the cross-validation experiments but also
successful extrapolation to a new search space and tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18460">Causal Fairness under Unobserved Confounding: A Neural Sensitivity Framework. (arXiv:2311.18460v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schroder_M/0/1/0/all/0/1">Maresa Schr&#xf6;der</a>, <a href="http://arxiv.org/find/cs/1/au:+Frauen_D/0/1/0/all/0/1">Dennis Frauen</a>, <a href="http://arxiv.org/find/cs/1/au:+Feuerriegel_S/0/1/0/all/0/1">Stefan Feuerriegel</a></p>
<p>Fairness for machine learning predictions is widely required in practice for
legal, ethical, and societal reasons. Existing work typically focuses on
settings without unobserved confounding, even though unobserved confounding can
lead to severe violations of causal fairness and, thus, unfair predictions. In
this work, we analyze the sensitivity of causal fairness to unobserved
confounding. Our contributions are three-fold. First, we derive bounds for
causal fairness metrics under different sources of unobserved confounding. This
enables practitioners to examine the sensitivity of their machine learning
models to unobserved confounding in fairness-critical applications. Second, we
propose a novel neural framework for learning fair predictions, which allows us
to offer worst-case guarantees of the extent to which causal fairness can be
violated due to unobserved confounding. Third, we demonstrate the effectiveness
of our framework in a series of experiments, including a real-world case study
about predicting prison sentences. To the best of our knowledge, ours is the
first work to study causal fairness under unobserved confounding. To this end,
our work is of direct practical value as a refutation strategy to ensure the
fairness of predictions in high-stakes applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18491">ZeST-NeRF: Using temporal aggregation for Zero-Shot Temporal NeRFs. (arXiv:2311.18491v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gonzalez_V/0/1/0/all/0/1">Violeta Men&#xe9;ndez Gonz&#xe1;lez</a>, <a href="http://arxiv.org/find/cs/1/au:+Gilbert_A/0/1/0/all/0/1">Andrew Gilbert</a>, <a href="http://arxiv.org/find/cs/1/au:+Phillipson_G/0/1/0/all/0/1">Graeme Phillipson</a>, <a href="http://arxiv.org/find/cs/1/au:+Jolly_S/0/1/0/all/0/1">Stephen Jolly</a>, <a href="http://arxiv.org/find/cs/1/au:+Hadfield_S/0/1/0/all/0/1">Simon Hadfield</a></p>
<p>In the field of media production, video editing techniques play a pivotal
role. Recent approaches have had great success at performing novel view image
synthesis of static scenes. But adding temporal information adds an extra layer
of complexity. Previous models have focused on implicitly representing static
and dynamic scenes using NeRF. These models achieve impressive results but are
costly at training and inference time. They overfit an MLP to describe the
scene implicitly as a function of position. This paper proposes ZeST-NeRF, a
new approach that can produce temporal NeRFs for new scenes without retraining.
We can accurately reconstruct novel views using multi-view synthesis techniques
and scene flow-field estimation, trained only with unrelated scenes. We
demonstrate how existing state-of-the-art approaches from a range of fields
cannot adequately solve this new task and demonstrate the efficacy of our
solution. The resulting network improves quantitatively by 15% and produces
significantly better visual results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18495">Improving Adversarial Transferability via Model Alignment. (arXiv:2311.18495v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_A/0/1/0/all/0/1">Avery Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Farahmand_A/0/1/0/all/0/1">Amir-massoud Farahmand</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1">Yangchen Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1">Philip Torr</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jindong Gu</a></p>
<p>Neural networks are susceptible to adversarial perturbations that are
transferable across different models. In this paper, we introduce a novel model
alignment technique aimed at improving a given source model's ability in
generating transferable adversarial perturbations. During the alignment
process, the parameters of the source model are fine-tuned to minimize an
alignment loss. This loss measures the divergence in the predictions between
the source model and another, independently trained model, referred to as the
witness model. To understand the effect of model alignment, we conduct a
geometric anlaysis of the resulting changes in the loss landscape. Extensive
experiments on the ImageNet dataset, using a variety of model architectures,
demonstrate that perturbations generated from aligned source models exhibit
significantly higher transferability than those from the original source model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18498">Data-Agnostic Model Poisoning against Federated Learning: A Graph Autoencoder Approach. (arXiv:2311.18498v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Kai Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1">Jingjing Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1">Xin Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ni_W/0/1/0/all/0/1">Wei Ni</a>, <a href="http://arxiv.org/find/cs/1/au:+Akan_O/0/1/0/all/0/1">Ozgur B. Akan</a>, <a href="http://arxiv.org/find/cs/1/au:+Poor_H/0/1/0/all/0/1">H. Vincent Poor</a></p>
<p>This paper proposes a novel, data-agnostic, model poisoning attack on
Federated Learning (FL), by designing a new adversarial graph autoencoder
(GAE)-based framework. The attack requires no knowledge of FL training data and
achieves both effectiveness and undetectability. By listening to the benign
local models and the global model, the attacker extracts the graph structural
correlations among the benign local models and the training data features
substantiating the models. The attacker then adversarially regenerates the
graph structural correlations while maximizing the FL training loss, and
subsequently generates malicious local models using the adversarial graph
structure and the training data features of the benign ones. A new algorithm is
designed to iteratively train the malicious local models using GAE and
sub-gradient descent. The convergence of FL under attack is rigorously proved,
with a considerably large optimality gap. Experiments show that the FL accuracy
drops gradually under the proposed attack and existing defense mechanisms fail
to detect it. The attack can give rise to an infection across all benign
devices, making it a serious threat to FL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18506">Global Convergence of Online Identification for Mixed Linear Regression. (arXiv:2311.18506v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Liu_Y/0/1/0/all/0/1">Yujing Liu</a>, <a href="http://arxiv.org/find/stat/1/au:+Liu_Z/0/1/0/all/0/1">Zhixin Liu</a>, <a href="http://arxiv.org/find/stat/1/au:+Guo_L/0/1/0/all/0/1">Lei Guo</a></p>
<p>Mixed linear regression (MLR) is a powerful model for characterizing
nonlinear relationships by utilizing a mixture of linear regression sub-models.
The identification of MLR is a fundamental problem, where most of the existing
results focus on offline algorithms, rely on independent and identically
distributed (i.i.d) data assumptions, and provide local convergence results
only. This paper investigates the online identification and data clustering
problems for two basic classes of MLRs, by introducing two corresponding new
online identification algorithms based on the expectation-maximization (EM)
principle. It is shown that both algorithms will converge globally without
resorting to the traditional i.i.d data assumptions. The main challenge in our
investigation lies in the fact that the gradient of the maximum likelihood
function does not have a unique zero, and a key step in our analysis is to
establish the stability of the corresponding differential equation in order to
apply the celebrated Ljung's ODE method. It is also shown that the
within-cluster error and the probability that the new data is categorized into
the correct cluster are asymptotically the same as those in the case of known
parameters. Finally, numerical simulations are provided to verify the
effectiveness of our online algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18512">Revisiting Proposal-based Object Detection. (arXiv:2311.18512v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bhowmik_A/0/1/0/all/0/1">Aritra Bhowmik</a>, <a href="http://arxiv.org/find/cs/1/au:+Oswald_M/0/1/0/all/0/1">Martin R. Oswald</a>, <a href="http://arxiv.org/find/cs/1/au:+Mettes_P/0/1/0/all/0/1">Pascal Mettes</a>, <a href="http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1">Cees G. M. Snoek</a></p>
<p>This paper revisits the pipeline for detecting objects in images with
proposals. For any object detector, the obtained box proposals or queries need
to be classified and regressed towards ground truth boxes. The common solution
for the final predictions is to directly maximize the overlap between each
proposal and the ground truth box, followed by a winner-takes-all ranking or
non-maximum suppression. In this work, we propose a simple yet effective
alternative. For proposal regression, we solve a simpler problem where we
regress to the area of intersection between proposal and ground truth. In this
way, each proposal only specifies which part contains the object, avoiding a
blind inpainting problem where proposals need to be regressed beyond their
visual scope. In turn, we replace the winner-takes-all strategy and obtain the
final prediction by taking the union over the regressed intersections of a
proposal group surrounding an object. Our revisited approach comes with minimal
changes to the detection pipeline and can be plugged into any existing method.
We show that our approach directly improves canonical object detection and
instance segmentation architectures, highlighting the utility of
intersection-based regression and grouping.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18520">Calibration-free online test-time adaptation for electroencephalography motor imagery decoding. (arXiv:2311.18520v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wimpff_M/0/1/0/all/0/1">Martin Wimpff</a>, <a href="http://arxiv.org/find/cs/1/au:+Dobler_M/0/1/0/all/0/1">Mario D&#xf6;bler</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1">Bin Yang</a></p>
<p>Providing a promising pathway to link the human brain with external devices,
Brain-Computer Interfaces (BCIs) have seen notable advancements in decoding
capabilities, primarily driven by increasingly sophisticated techniques,
especially deep learning. However, achieving high accuracy in real-world
scenarios remains a challenge due to the distribution shift between sessions
and subjects. In this paper we will explore the concept of online test-time
adaptation (OTTA) to continuously adapt the model in an unsupervised fashion
during inference time. Our approach guarantees the preservation of privacy by
eliminating the requirement to access the source data during the adaptation
process. Additionally, OTTA achieves calibration-free operation by not
requiring any session- or subject-specific data. We will investigate the task
of electroencephalography (EEG) motor imagery decoding using a lightweight
architecture together with different OTTA techniques like alignment, adaptive
batch normalization, and entropy minimization. We examine two datasets and
three distinct data settings for a comprehensive analysis. Our adaptation
methods produce state-of-the-art results, potentially instigating a shift in
transfer learning for BCI decoding towards online adaptation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18521">Combining deep generative models with extreme value theory for synthetic hazard simulation: a multivariate and spatially coherent approach. (arXiv:2311.18521v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peard_A/0/1/0/all/0/1">Alison Peard</a>, <a href="http://arxiv.org/find/cs/1/au:+Hall_J/0/1/0/all/0/1">Jim Hall</a></p>
<p>Climate hazards can cause major disasters when they occur simultaneously as
compound hazards. To understand the distribution of climate risk and inform
adaptation policies, scientists need to simulate a large number of physically
realistic and spatially coherent events. Current methods are limited by
computational constraints and the probabilistic spatial distribution of
compound events is not given sufficient attention. The bottleneck in current
approaches lies in modelling the dependence structure between variables, as
inference on parametric models suffers from the curse of dimensionality.
Generative adversarial networks (GANs) are well-suited to such a problem due to
their ability to implicitly learn the distribution of data in high-dimensional
settings. We employ a GAN to model the dependence structure for daily maximum
wind speed, significant wave height, and total precipitation over the Bay of
Bengal, combining this with traditional extreme value theory for controlled
extrapolation of the tails. Once trained, the model can be used to efficiently
generate thousands of realistic compound hazard events, which can inform
climate risk assessments for climate adaptation and disaster preparedness. The
method developed is flexible and transferable to other multivariate and spatial
climate datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18525">Detecting Anomalous Network Communication Patterns Using Graph Convolutional Networks. (arXiv:2311.18525v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vaisman_Y/0/1/0/all/0/1">Yizhak Vaisman</a>, <a href="http://arxiv.org/find/cs/1/au:+Katz_G/0/1/0/all/0/1">Gilad Katz</a>, <a href="http://arxiv.org/find/cs/1/au:+Elovici_Y/0/1/0/all/0/1">Yuval Elovici</a>, <a href="http://arxiv.org/find/cs/1/au:+Shabtai_A/0/1/0/all/0/1">Asaf Shabtai</a></p>
<p>To protect an organizations' endpoints from sophisticated cyberattacks,
advanced detection methods are required. In this research, we present
GCNetOmaly: a graph convolutional network (GCN)-based variational autoencoder
(VAE) anomaly detector trained on data that include connection events among
internal and external machines. As input, the proposed GCN-based VAE model
receives two matrices: (i) the normalized adjacency matrix, which represents
the connections among the machines, and (ii) the feature matrix, which includes
various features (demographic, statistical, process-related, and Node2vec
structural features) that are used to profile the individual nodes/machines.
After training the model on data collected for a predefined time window, the
model is applied on the same data; the reconstruction score obtained by the
model for a given machine then serves as the machine's anomaly score.
GCNetOmaly was evaluated on real, large-scale data logged by Carbon Black EDR
from a large financial organization's automated teller machines (ATMs) as well
as communication with Active Directory (AD) servers in two setups: unsupervised
and supervised. The results of our evaluation demonstrate GCNetOmaly's
effectiveness in detecting anomalous behavior of machines on unsupervised data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18526">HOT: Higher-Order Dynamic Graph Representation Learning with Efficient Transformers. (arXiv:2311.18526v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Besta_M/0/1/0/all/0/1">Maciej Besta</a>, <a href="http://arxiv.org/find/cs/1/au:+Catarino_A/0/1/0/all/0/1">Afonso Claudino Catarino</a>, <a href="http://arxiv.org/find/cs/1/au:+Gianinazzi_L/0/1/0/all/0/1">Lukas Gianinazzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Blach_N/0/1/0/all/0/1">Nils Blach</a>, <a href="http://arxiv.org/find/cs/1/au:+Nyczyk_P/0/1/0/all/0/1">Piotr Nyczyk</a>, <a href="http://arxiv.org/find/cs/1/au:+Niewiadomski_H/0/1/0/all/0/1">Hubert Niewiadomski</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoefler_T/0/1/0/all/0/1">Torsten Hoefler</a></p>
<p>Many graph representation learning (GRL) problems are dynamic, with millions
of edges added or removed per second. A fundamental workload in this setting is
dynamic link prediction: using a history of graph updates to predict whether a
given pair of vertices will become connected. Recent schemes for link
prediction in such dynamic settings employ Transformers, modeling individual
graph updates as single tokens. In this work, we propose HOT: a model that
enhances this line of works by harnessing higher-order (HO) graph structures;
specifically, k-hop neighbors and more general subgraphs containing a given
pair of vertices. Harnessing such HO structures by encoding them into the
attention matrix of the underlying Transformer results in higher accuracy of
link prediction outcomes, but at the expense of increased memory pressure. To
alleviate this, we resort to a recent class of schemes that impose hierarchy on
the attention matrix, significantly reducing memory footprint. The final design
offers a sweetspot between high accuracy and low memory utilization. HOT
outperforms other dynamic GRL schemes, for example achieving 9%, 7%, and 15%
higher accuracy than - respectively - DyGFormer, TGN, and GraphMixer, for the
MOOC dataset. Our design can be seamlessly extended towards other dynamic GRL
workloads.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18531">Dataset Distillation via the Wasserstein Metric. (arXiv:2311.18531v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Haoyang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_T/0/1/0/all/0/1">Tiancheng Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Luwei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Dalal_V/0/1/0/all/0/1">Vibhu Dalal</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Jingrui He</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haohan Wang</a></p>
<p>Dataset distillation (DD) offers a compelling approach in computer vision,
with the goal of condensing extensive datasets into smaller synthetic versions
without sacrificing much of the model performance. In this paper, we continue
to study the methods for DD, by addressing its conceptually core objective: how
to capture the essential representation of extensive datasets in smaller,
synthetic forms.
</p>
<p>We propose a novel approach utilizing the Wasserstein distance, a metric
rooted in optimal transport theory, to enhance distribution matching in DD. Our
method leverages the Wasserstein barycenter, offering a geometrically
meaningful way to quantify distribution differences and effectively capture the
centroid of a set of distributions. Our approach retains the computational
benefits of distribution matching-based methods while achieving new
state-of-the-art performance on several benchmarks.
</p>
<p>To provide useful prior for learning the images, we embed the synthetic data
into the feature space of pretrained classification models to conduct
distribution matching. Extensive testing on various high-resolution datasets
confirms the effectiveness and adaptability of our method, indicating the
promising yet unexplored capabilities of Wasserstein metrics in dataset
distillation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18540">Match me if you can: Semantic Correspondence Learning with Unpaired Images. (arXiv:2311.18540v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jiwon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Heo_B/0/1/0/all/0/1">Byeongho Heo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1">Sangdoo Yun</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Seungryong Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1">Dongyoon Han</a></p>
<p>Recent approaches for semantic correspondence have focused on obtaining
high-quality correspondences using a complicated network, refining the
ambiguous or noisy matching points. Despite their performance improvements,
they remain constrained by the limited training pairs due to costly point-level
annotations. This paper proposes a simple yet effective method that performs
training with unlabeled pairs to complement both limited image pairs and sparse
point pairs, requiring neither extra labeled keypoints nor trainable modules.
We fundamentally extend the data quantity and variety by augmenting new
unannotated pairs not primitively provided as training pairs in benchmarks.
Using a simple teacher-student framework, we offer reliable pseudo
correspondences to the student network via machine supervision. Finally, the
performance of our network is steadily improved by the proposed iterative
training, putting back the student as a teacher to generate refined labels and
train a new student repeatedly. Our models outperform the milestone baselines,
including state-of-the-art methods on semantic correspondence benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18547">Real-Time Vibration-Based Bearing Fault Diagnosis Under Time-Varying Speed Conditions. (arXiv:2311.18547v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jalonen_T/0/1/0/all/0/1">Tuomas Jalonen</a>, <a href="http://arxiv.org/find/cs/1/au:+Al_Sad_M/0/1/0/all/0/1">Mohammad Al-Sa&#x27;d</a>, <a href="http://arxiv.org/find/cs/1/au:+Kiranyaz_S/0/1/0/all/0/1">Serkan Kiranyaz</a>, <a href="http://arxiv.org/find/cs/1/au:+Gabbouj_M/0/1/0/all/0/1">Moncef Gabbouj</a></p>
<p>Detection of rolling-element bearing faults is crucial for implementing
proactive maintenance strategies and for minimizing the economic and
operational consequences of unexpected failures. However, many existing
techniques are developed and tested under strictly controlled conditions,
limiting their adaptability to the diverse and dynamic settings encountered in
practical applications. This paper presents an efficient real-time
convolutional neural network (CNN) for diagnosing multiple bearing faults under
various noise levels and time-varying rotational speeds. Additionally, we
propose a novel Fisher-based spectral separability analysis (SSA) method to
elucidate the effectiveness of the designed CNN model. We conducted experiments
on both healthy bearings and bearings afflicted with inner race, outer race,
and roller ball faults. The experimental results show the superiority of our
model over the current state-of-the-art approach in three folds: it achieves
substantial accuracy gains of up to 15.8%, it is robust to noise with high
performance across various signal-to-noise ratios, and it runs in real-time
with processing durations five times less than acquisition. Additionally, by
using the proposed SSA technique, we offer insights into the model's
performance and underscore its effectiveness in tackling real-world challenges.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18553">Heterogeneous Graph-based Trajectory Prediction using Local Map Context and Social Interactions. (arXiv:2311.18553v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Grimm_D/0/1/0/all/0/1">Daniel Grimm</a>, <a href="http://arxiv.org/find/cs/1/au:+Zipfl_M/0/1/0/all/0/1">Maximilian Zipfl</a>, <a href="http://arxiv.org/find/cs/1/au:+Hertlein_F/0/1/0/all/0/1">Felix Hertlein</a>, <a href="http://arxiv.org/find/cs/1/au:+Naumann_A/0/1/0/all/0/1">Alexander Naumann</a>, <a href="http://arxiv.org/find/cs/1/au:+Luttin_J/0/1/0/all/0/1">J&#xfc;rgen L&#xfc;ttin</a>, <a href="http://arxiv.org/find/cs/1/au:+Thoma_S/0/1/0/all/0/1">Steffen Thoma</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmid_S/0/1/0/all/0/1">Stefan Schmid</a>, <a href="http://arxiv.org/find/cs/1/au:+Halilaj_L/0/1/0/all/0/1">Lavdim Halilaj</a>, <a href="http://arxiv.org/find/cs/1/au:+Rettinger_A/0/1/0/all/0/1">Achim Rettinger</a>, <a href="http://arxiv.org/find/cs/1/au:+Zollner_J/0/1/0/all/0/1">J. Marius Z&#xf6;llner</a></p>
<p>Precisely predicting the future trajectories of surrounding traffic
participants is a crucial but challenging problem in autonomous driving, due to
complex interactions between traffic agents, map context and traffic rules.
Vector-based approaches have recently shown to achieve among the best
performances on trajectory prediction benchmarks. These methods model simple
interactions between traffic agents but don't distinguish between relation-type
and attributes like their distance along the road. Furthermore, they represent
lanes only by sequences of vectors representing center lines and ignore context
information like lane dividers and other road elements. We present a novel
approach for vector-based trajectory prediction that addresses these
shortcomings by leveraging three crucial sources of information: First, we
model interactions between traffic agents by a semantic scene graph, that
accounts for the nature and important features of their relation. Second, we
extract agent-centric image-based map features to model the local map context.
Finally, we generate anchor paths to enforce the policy in multi-modal
prediction to permitted trajectories only. Each of these three enhancements
shows advantages over the baseline model HoliGraph.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18557">Can semi-supervised learning use all the data effectively? A lower bound perspective. (arXiv:2311.18557v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tifrea_A/0/1/0/all/0/1">Alexandru &#x162;ifrea</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuce_G/0/1/0/all/0/1">Gizem Y&#xfc;ce</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanyal_A/0/1/0/all/0/1">Amartya Sanyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1">Fanny Yang</a></p>
<p>Prior works have shown that semi-supervised learning algorithms can leverage
unlabeled data to improve over the labeled sample complexity of supervised
learning (SL) algorithms. However, existing theoretical analyses focus on
regimes where the unlabeled data is sufficient to learn a good decision
boundary using unsupervised learning (UL) alone. This begs the question: Can
SSL algorithms simultaneously improve upon both UL and SL? To this end, we
derive a tight lower bound for 2-Gaussian mixture models that explicitly
depends on the labeled and the unlabeled dataset size as well as the
signal-to-noise ratio of the mixture distribution. Surprisingly, our result
implies that no SSL algorithm can improve upon the minimax-optimal statistical
error rates of SL or UL algorithms for these distributions. Nevertheless, we
show empirically on real-world data that SSL algorithms can still outperform UL
and SL methods. Therefore, our work suggests that, while proving performance
gains for SSL algorithms is possible, it requires careful tracking of
constants.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18558">Learning Radio Environments by Differentiable Ray Tracing. (arXiv:2311.18558v1 [cs.IT])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hoydis_J/0/1/0/all/0/1">Jakob Hoydis</a>, <a href="http://arxiv.org/find/cs/1/au:+Aoudia_F/0/1/0/all/0/1">Fay&#xe7;al A&#xef;t Aoudia</a>, <a href="http://arxiv.org/find/cs/1/au:+Cammerer_S/0/1/0/all/0/1">Sebastian Cammerer</a>, <a href="http://arxiv.org/find/cs/1/au:+Euchner_F/0/1/0/all/0/1">Florian Euchner</a>, <a href="http://arxiv.org/find/cs/1/au:+Nimier_David_M/0/1/0/all/0/1">Merlin Nimier-David</a>, <a href="http://arxiv.org/find/cs/1/au:+Brink_S/0/1/0/all/0/1">Stephan ten Brink</a>, <a href="http://arxiv.org/find/cs/1/au:+Keller_A/0/1/0/all/0/1">Alexander Keller</a></p>
<p>Ray tracing (RT) is instrumental in 6G research in order to generate
spatially-consistent and environment-specific channel impulse responses (CIRs).
While acquiring accurate scene geometries is now relatively straightforward,
determining material characteristics requires precise calibration using channel
measurements. We therefore introduce a novel gradient-based calibration method,
complemented by differentiable parametrizations of material properties,
scattering and antenna patterns. Our method seamlessly integrates with
differentiable ray tracers that enable the computation of derivatives of CIRs
with respect to these parameters. Essentially, we approach field computation as
a large computational graph wherein parameters are trainable akin to weights of
a neural network (NN). We have validated our method using both synthetic data
and real-world indoor channel measurements, employing a distributed
multiple-input multiple-output (MIMO) channel sounder.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18559">FediOS: Decoupling Orthogonal Subspaces for Personalization in Feature-skew Federated Learning. (arXiv:2311.18559v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1">Lingzhi Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zexi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yang Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1">Chao Wu</a></p>
<p>Personalized federated learning (pFL) enables collaborative training among
multiple clients to enhance the capability of customized local models. In pFL,
clients may have heterogeneous (also known as non-IID) data, which poses a key
challenge in how to decouple the data knowledge into generic knowledge for
global sharing and personalized knowledge for preserving local personalization.
A typical way of pFL focuses on label distribution skew, and they adopt a
decoupling scheme where the model is split into a common feature extractor and
two prediction heads (generic and personalized). However, such a decoupling
scheme cannot solve the essential problem of feature skew heterogeneity,
because a common feature extractor cannot decouple the generic and personalized
features. Therefore, in this paper, we rethink the architecture decoupling
design for feature-skew pFL and propose an effective pFL method called FediOS.
In FediOS, we reformulate the decoupling into two feature extractors (generic
and personalized) and one shared prediction head. Orthogonal projections are
used for clients to map the generic features into one common subspace and
scatter the personalized features into different subspaces to achieve
decoupling for them. In addition, a shared prediction head is trained to
balance the importance of generic and personalized features during inference.
Extensive experiments on four vision datasets demonstrate our method reaches
state-of-the-art pFL performances under feature skew heterogeneity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18574">Multi-scale Iterative Refinement towards Robust and Versatile Molecular Docking. (arXiv:2311.18574v1 [q-bio.BM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Yan_J/0/1/0/all/0/1">Jiaxian Yan</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Zhang_Z/0/1/0/all/0/1">Zaixi Zhang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Zhang_K/0/1/0/all/0/1">Kai Zhang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Liu_Q/0/1/0/all/0/1">Qi Liu</a></p>
<p>Molecular docking is a key computational tool utilized to predict the binding
conformations of small molecules to protein targets, which is fundamental in
the design of novel drugs. Despite recent advancements in geometric deep
learning-based approaches leading to improvements in blind docking efficiency,
these methods have encountered notable challenges, such as limited
generalization performance on unseen proteins, the inability to concurrently
address the settings of blind docking and site-specific docking, and the
frequent occurrence of physical implausibilities such as inter-molecular steric
clash. In this study, we introduce DeltaDock, a robust and versatile framework
designed for efficient molecular docking to overcome these challenges.
DeltaDock operates in a two-step process: rapid initial complex structures
sampling followed by multi-scale iterative refinement of the initial
structures. In the initial stage, to sample accurate structures with high
efficiency, we develop a ligand-dependent binding site prediction model founded
on large protein models and graph neural networks. This model is then paired
with GPU-accelerated sampling algorithms. The sampled structures are updated
using a multi-scale iterative refinement module that captures both
protein-ligand atom-atom interactions and residue-atom interactions in the
following stage. Distinct from previous geometric deep learning methods that
are conditioned on the blind docking setting, DeltaDock demonstrates superior
performance in both blind docking and site-specific docking settings.
Comprehensive experimental results reveal that DeltaDock consistently surpasses
baseline methods in terms of docking accuracy. Furthermore, it displays
remarkable generalization capabilities and proficiency for predicting
physically valid structures, thereby attesting to its robustness and
reliability in various scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18575">Class Distribution Shifts in Zero-Shot Learning: Learning Robust Representations. (arXiv:2311.18575v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Slavutsky_Y/0/1/0/all/0/1">Yuli Slavutsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Benjamini_Y/0/1/0/all/0/1">Yuval Benjamini</a></p>
<p>Distribution shifts between training and deployment data often affect the
performance of machine learning models. In this paper, we explore a setting
where a hidden variable induces a shift in the distribution of classes. These
distribution shifts are particularly challenging for zero-shot classifiers, as
they rely on representations learned from training classes, but are deployed on
new, unseen ones. We introduce an algorithm to learn data representations that
are robust to such class distribution shifts in zero-shot verification tasks.
We show that our approach, which combines hierarchical data sampling with
out-of-distribution generalization techniques, improves generalization to
diverse class distributions in both simulations and real-world datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18578">Communication-Efficient Heterogeneous Federated Learning with Generalized Heavy-Ball Momentum. (arXiv:2311.18578v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zaccone_R/0/1/0/all/0/1">Riccardo Zaccone</a>, <a href="http://arxiv.org/find/cs/1/au:+Masone_C/0/1/0/all/0/1">Carlo Masone</a>, <a href="http://arxiv.org/find/cs/1/au:+Ciccone_M/0/1/0/all/0/1">Marco Ciccone</a></p>
<p>Federated Learning (FL) is the state-of-the-art approach for learning from
decentralized data in privacy-constrained scenarios. As the current literature
reports, the main problems associated with FL refer to system and statistical
challenges: the former ones demand for efficient learning from edge devices,
including lowering communication bandwidth and frequency, while the latter
require algorithms robust to non-iidness. State-of-art approaches either
guarantee convergence at increased communication cost or are not sufficiently
robust to handle extreme heterogeneous local distributions. In this work we
propose a novel generalization of the heavy-ball momentum, and present FedHBM
to effectively address statistical heterogeneity in FL without introducing any
communication overhead. We conduct extensive experimentation on common FL
vision and NLP datasets, showing that our FedHBM algorithm empirically yields
better model quality and higher convergence speed w.r.t. the state-of-art,
especially in pathological non-iid scenarios. While being designed for
cross-silo settings, we show how FedHBM is applicable in moderate-to-high
cross-device scenarios, and how good model initializations (e.g. pre-training)
can be exploited for prompt acceleration. Extended experimentation on
large-scale real-world federated datasets further corroborates the
effectiveness of our approach for real-world FL applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18587">Continuous 16-bit Training: Accelerating 32-bit Pre-Trained Neural Networks. (arXiv:2311.18587v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yun_J/0/1/0/all/0/1">Juyoung Yun</a></p>
<p>In the field of deep learning, the prevalence of models initially trained
with 32-bit precision is a testament to its robustness and accuracy. However,
the continuous evolution of these models often demands further training, which
can be resource-intensive. This study introduces a novel approach where we
continue the training of these pre-existing 32-bit models using 16-bit
precision. This technique not only caters to the need for efficiency in
computational resources but also significantly improves the speed of additional
training phases. By adopting 16-bit precision for ongoing training, we are able
to substantially decrease memory requirements and computational burden, thereby
accelerating the training process in a resource-limited setting. Our
experiments show that this method maintains the high standards of accuracy set
by the original 32-bit training while providing a much-needed boost in training
speed. This approach is especially pertinent in today's context, where most
models are initially trained in 32-bit and require periodic updates and
refinements. The findings from our research suggest that this strategy of
16-bit continuation training can be a key solution for sustainable and
efficient deep learning, offering a practical way to enhance pre-trained models
rapidly and in a resource-conscious manner.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18588">Optimizing ZX-Diagrams with Deep Reinforcement Learning. (arXiv:2311.18588v1 [quant-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Nagele_M/0/1/0/all/0/1">Maximilian N&#xe4;gele</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Marquardt_F/0/1/0/all/0/1">Florian Marquardt</a></p>
<p>ZX-diagrams are a powerful graphical language for the description of quantum
processes with applications in fundamental quantum mechanics, quantum circuit
optimization, tensor network simulation, and many more. The utility of
ZX-diagrams relies on a set of local transformation rules that can be applied
to them without changing the underlying quantum process they describe. These
rules can be exploited to optimize the structure of ZX-diagrams for a range of
applications. However, finding an optimal sequence of transformation rules is
generally an open problem. In this work, we bring together ZX-diagrams with
reinforcement learning, a machine learning technique designed to discover an
optimal sequence of actions in a decision-making problem and show that a
trained reinforcement learning agent can significantly outperform other
optimization techniques like a greedy strategy or simulated annealing. The use
of graph neural networks to encode the policy of the agent enables
generalization to diagrams much bigger than seen during the training phase.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18598">Generalisable Agents for Neural Network Optimisation. (arXiv:2311.18598v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tessera_K/0/1/0/all/0/1">Kale-ab Tessera</a>, <a href="http://arxiv.org/find/cs/1/au:+Tilbury_C/0/1/0/all/0/1">Callum Rhys Tilbury</a>, <a href="http://arxiv.org/find/cs/1/au:+Abramowitz_S/0/1/0/all/0/1">Sasha Abramowitz</a>, <a href="http://arxiv.org/find/cs/1/au:+Kock_R/0/1/0/all/0/1">Ruan de Kock</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahjoub_O/0/1/0/all/0/1">Omayma Mahjoub</a>, <a href="http://arxiv.org/find/cs/1/au:+Rosman_B/0/1/0/all/0/1">Benjamin Rosman</a>, <a href="http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1">Sara Hooker</a>, <a href="http://arxiv.org/find/cs/1/au:+Pretorius_A/0/1/0/all/0/1">Arnu Pretorius</a></p>
<p>Optimising deep neural networks is a challenging task due to complex training
dynamics, high computational requirements, and long training times. To address
this difficulty, we propose the framework of Generalisable Agents for Neural
Network Optimisation (GANNO) -- a multi-agent reinforcement learning (MARL)
approach that learns to improve neural network optimisation by dynamically and
responsively scheduling hyperparameters during training. GANNO utilises an
agent per layer that observes localised network dynamics and accordingly takes
actions to adjust these dynamics at a layerwise level to collectively improve
global performance. In this paper, we use GANNO to control the layerwise
learning rate and show that the framework can yield useful and responsive
schedules that are competitive with handcrafted heuristics. Furthermore, GANNO
is shown to perform robustly across a wide variety of unseen initial
conditions, and can successfully generalise to harder problems than it was
trained on. Our work presents an overview of the opportunities that this
paradigm offers for training neural networks, along with key challenges that
remain to be overcome.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18608">Contrastive Denoising Score for Text-guided Latent Diffusion Image Editing. (arXiv:2311.18608v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nam_H/0/1/0/all/0/1">Hyelin Nam</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwon_G/0/1/0/all/0/1">Gihyun Kwon</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_G/0/1/0/all/0/1">Geon Yeong Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1">Jong Chul Ye</a></p>
<p>With the remarkable advent of text-to-image diffusion models, image editing
methods have become more diverse and continue to evolve. A promising recent
approach in this realm is Delta Denoising Score (DDS) - an image editing
technique based on Score Distillation Sampling (SDS) framework that leverages
the rich generative prior of text-to-image diffusion models. However, relying
solely on the difference between scoring functions is insufficient for
preserving specific structural elements from the original image, a crucial
aspect of image editing. Inspired by the similarity and importance differences
between DDS and the contrastive learning for unpaired image-to-image
translation (CUT), here we present an embarrassingly simple yet very powerful
modification of DDS, called Contrastive Denoising Score (CDS), for latent
diffusion models (LDM). Specifically, to enforce structural correspondence
between the input and output while maintaining the controllability of contents,
we introduce a straightforward approach to regulate structural consistency
using CUT loss within the DDS framework. To calculate this loss, instead of
employing auxiliary networks, we utilize the intermediate features of LDM, in
particular, those from the self-attention layers, which possesses rich spatial
information. Our approach enables zero-shot image-to-image translation and
neural radiance field (NeRF) editing, achieving a well-balanced interplay
between maintaining the structural details and transforming content.
Qualitative results and comparisons demonstrates the effectiveness of our
proposed method. Project page with code is available at
https://hyelinnam.github.io/CDS/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18639">Targeted Reduction of Causal Models. (arXiv:2311.18639v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Kekic_A/0/1/0/all/0/1">Armin Keki&#x107;</a>, <a href="http://arxiv.org/find/stat/1/au:+Scholkopf_B/0/1/0/all/0/1">Bernhard Sch&#xf6;lkopf</a>, <a href="http://arxiv.org/find/stat/1/au:+Besserve_M/0/1/0/all/0/1">Michel Besserve</a></p>
<p>Why does a phenomenon occur? Addressing this question is central to most
scientific inquiries based on empirical observations, and often heavily relies
on simulations of scientific models. As models become more intricate,
deciphering the causes behind these phenomena in high-dimensional spaces of
interconnected variables becomes increasingly challenging. Causal machine
learning may assist scientists in the discovery of relevant and interpretable
patterns of causation in simulations. We introduce Targeted Causal Reduction
(TCR), a method for turning complex models into a concise set of causal factors
that explain a specific target phenomenon. We derive an information theoretic
objective to learn TCR from interventional data or simulations and propose
algorithms to optimize this objective efficiently. TCR's ability to generate
interpretable high-level explanations from complex models is demonstrated on
toy and mechanical systems, illustrating its potential to assist scientists in
the study of complex phenomena in a broad range of disciplines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18663">Choosing the parameter of the Fermat distance: navigating geometry and noise. (arXiv:2311.18663v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Chazal_F/0/1/0/all/0/1">Fr&#xe9;d&#xe9;ric Chazal</a>, <a href="http://arxiv.org/find/stat/1/au:+Ferraris_L/0/1/0/all/0/1">Laure Ferraris</a>, <a href="http://arxiv.org/find/stat/1/au:+Groisman_P/0/1/0/all/0/1">Pablo Groisman</a>, <a href="http://arxiv.org/find/stat/1/au:+Jonckheere_M/0/1/0/all/0/1">Matthieu Jonckheere</a>, <a href="http://arxiv.org/find/stat/1/au:+Pascal_F/0/1/0/all/0/1">Fr&#xe9;d&#xe9;ric Pascal</a>, <a href="http://arxiv.org/find/stat/1/au:+Sapienza_F/0/1/0/all/0/1">Facundo Sapienza</a></p>
<p>The Fermat distance has been recently established as a useful tool for
machine learning tasks when a natural distance is not directly available to the
practitioner or to improve the results given by Euclidean distances by
exploding the geometrical and statistical properties of the dataset. This
distance depends on a parameter $\alpha$ that greatly impacts the performance
of subsequent tasks. Ideally, the value of $\alpha$ should be large enough to
navigate the geometric intricacies inherent to the problem. At the same, it
should remain restrained enough to sidestep any deleterious ramifications
stemming from noise during the process of distance estimation. We study both
theoretically and through simulations how to select this parameter.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18672">A Comparison Between Invariant and Equivariant Classical and Quantum Graph Neural Networks. (arXiv:2311.18672v1 [quant-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Forestano_R/0/1/0/all/0/1">Roy T. Forestano</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Cara_M/0/1/0/all/0/1">Mar&#xe7;al Comajoan Cara</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Dahale_G/0/1/0/all/0/1">Gopal Ramesh Dahale</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Dong_Z/0/1/0/all/0/1">Zhongtian Dong</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Gleyzer_S/0/1/0/all/0/1">Sergei Gleyzer</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Justice_D/0/1/0/all/0/1">Daniel Justice</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Kong_K/0/1/0/all/0/1">Kyoungchul Kong</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Magorsch_T/0/1/0/all/0/1">Tom Magorsch</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Matchev_K/0/1/0/all/0/1">Konstantin T. Matchev</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Matcheva_K/0/1/0/all/0/1">Katia Matcheva</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Unlu_E/0/1/0/all/0/1">Eyup B. Unlu</a></p>
<p>Machine learning algorithms are heavily relied on to understand the vast
amounts of data from high-energy particle collisions at the CERN Large Hadron
Collider (LHC). The data from such collision events can naturally be
represented with graph structures. Therefore, deep geometric methods, such as
graph neural networks (GNNs), have been leveraged for various data analysis
tasks in high-energy physics. One typical task is jet tagging, where jets are
viewed as point clouds with distinct features and edge connections between
their constituent particles. The increasing size and complexity of the LHC
particle datasets, as well as the computational models used for their analysis,
greatly motivate the development of alternative fast and efficient
computational paradigms such as quantum computation. In addition, to enhance
the validity and robustness of deep networks, one can leverage the fundamental
symmetries present in the data through the use of invariant inputs and
equivariant layers. In this paper, we perform a fair and comprehensive
comparison between classical graph neural networks (GNNs) and equivariant graph
neural networks (EGNNs) and their quantum counterparts: quantum graph neural
networks (QGNNs) and equivariant quantum graph neural networks (EQGNN). The
four architectures were benchmarked on a binary classification task to classify
the parton-level particle initiating the jet. Based on their AUC scores, the
quantum networks were shown to outperform the classical networks. However,
seeing the computational advantage of the quantum networks in practice may have
to wait for the further development of quantum technology and its associated
APIs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18684">Handling Cost and Constraints with Off-Policy Deep Reinforcement Learning. (arXiv:2311.18684v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Markowitz_J/0/1/0/all/0/1">Jared Markowitz</a>, <a href="http://arxiv.org/find/cs/1/au:+Silverberg_J/0/1/0/all/0/1">Jesse Silverberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Collins_G/0/1/0/all/0/1">Gary Collins</a></p>
<p>By reusing data throughout training, off-policy deep reinforcement learning
algorithms offer improved sample efficiency relative to on-policy approaches.
For continuous action spaces, the most popular methods for off-policy learning
include policy improvement steps where a learned state-action ($Q$) value
function is maximized over selected batches of data. These updates are often
paired with regularization to combat associated overestimation of $Q$ values.
With an eye toward safety, we revisit this strategy in environments with
"mixed-sign" reward functions; that is, with reward functions that include
independent positive (incentive) and negative (cost) terms. This setting is
common in real-world applications, and may be addressed with or without
constraints on the cost terms. We find the combination of function
approximation and a term that maximizes $Q$ in the policy update to be
problematic in such environments, because systematic errors in value estimation
impact the contributions from the competing terms asymmetrically. This results
in overemphasis of either incentives or costs and may severely limit learning.
We explore two remedies to this issue. First, consistent with prior work, we
find that periodic resetting of $Q$ and policy networks can be used to reduce
value estimation error and improve learning in this setting. Second, we
formulate novel off-policy actor-critic methods for both unconstrained and
constrained learning that do not explicitly maximize $Q$ in the policy update.
We find that this second approach, when applied to continuous action spaces
with mixed-sign rewards, consistently and significantly outperforms
state-of-the-art methods augmented by resetting. We further find that our
approach produces agents that are both competitive with popular methods overall
and more reliably competent on frequently-studied control problems that do not
have mixed-sign rewards.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18694">Balancing Summarization and Change Detection in Graph Streams. (arXiv:2311.18694v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Fukushima_S/0/1/0/all/0/1">Shintaro Fukushima</a>, <a href="http://arxiv.org/find/stat/1/au:+Yamanishi_K/0/1/0/all/0/1">Kenji Yamanishi</a></p>
<p>This study addresses the issue of balancing graph summarization and graph
change detection. Graph summarization compresses large-scale graphs into a
smaller scale. However, the question remains: To what extent should the
original graph be compressed? This problem is solved from the perspective of
graph change detection, aiming to detect statistically significant changes
using a stream of summary graphs. If the compression rate is extremely high,
important changes can be ignored, whereas if the compression rate is extremely
low, false alarms may increase with more memory. This implies that there is a
trade-off between compression rate in graph summarization and accuracy in
change detection. We propose a novel quantitative methodology to balance this
trade-off to simultaneously realize reliable graph summarization and change
detection. We introduce a probabilistic structure of hierarchical latent
variable model into a graph, thereby designing a parameterized summary graph on
the basis of the minimum description length principle. The parameter specifying
the summary graph is then optimized so that the accuracy of change detection is
guaranteed to suppress Type I error probability (probability of raising false
alarms) to be less than a given confidence level. First, we provide a
theoretical framework for connecting graph summarization with change detection.
Then, we empirically demonstrate its effectiveness on synthetic and real
datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18695">Seg2Reg: Differentiable 2D Segmentation to 1D Regression Rendering for 360 Room Layout Reconstruction. (arXiv:2311.18695v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1">Cheng Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Tai_W/0/1/0/all/0/1">Wei-En Tai</a>, <a href="http://arxiv.org/find/cs/1/au:+Shih_Y/0/1/0/all/0/1">Yu-Lin Shih</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Kuan-Wei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Syu_Y/0/1/0/all/0/1">Yong-Jing Syu</a>, <a href="http://arxiv.org/find/cs/1/au:+The_K/0/1/0/all/0/1">Kent Selwyn The</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu-Chiang Frank Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hwann-Tzong Chen</a></p>
<p>State-of-the-art single-view 360-degree room layout reconstruction methods
formulate the problem as a high-level 1D (per-column) regression task. On the
other hand, traditional low-level 2D layout segmentation is simpler to learn
and can represent occluded regions, but it requires complex post-processing for
the targeting layout polygon and sacrifices accuracy. We present Seg2Reg to
render 1D layout depth regression from the 2D segmentation map in a
differentiable and occlusion-aware way, marrying the merits of both sides.
Specifically, our model predicts floor-plan density for the input
equirectangular 360-degree image. Formulating the 2D layout representation as a
density field enables us to employ `flattened' volume rendering to form 1D
layout depth regression. In addition, we propose a novel 3D warping
augmentation on layout to improve generalization. Finally, we re-implement
recent room layout reconstruction methods into our codebase for benchmarking
and explore modern backbones and training techniques to serve as the strong
baseline. Our model significantly outperforms previous arts. The code will be
made available upon publication.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18703">Predictable Reinforcement Learning Dynamics through Entropy Rate Minimization. (arXiv:2311.18703v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ornia_D/0/1/0/all/0/1">Daniel Jarne Ornia</a>, <a href="http://arxiv.org/find/cs/1/au:+Delimpaltadakis_G/0/1/0/all/0/1">Giannis Delimpaltadakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Kober_J/0/1/0/all/0/1">Jens Kober</a>, <a href="http://arxiv.org/find/cs/1/au:+Alonso_Mora_J/0/1/0/all/0/1">Javier Alonso-Mora</a></p>
<p>In Reinforcement Learning (RL), agents have no incentive to exhibit
predictable behaviors, and are often pushed (through e.g. policy entropy
regularization) to randomize their actions in favor of exploration. From a
human perspective, this makes RL agents hard to interpret and predict, and from
a safety perspective, even harder to formally verify. We propose a novel method
to induce predictable behavior in RL agents, referred to as
Predictability-Aware RL (PA-RL), which employs the state sequence entropy rate
as a predictability measure. We show how the entropy rate can be formulated as
an average reward objective, and since its entropy reward function is
policy-dependent, we introduce an action-dependent surrogate entropy enabling
the use of PG methods. We prove that deterministic policies minimizing the
average surrogate reward exist and also minimize the actual entropy rate, and
show how, given a learned dynamical model, we are able to approximate the value
function associated to the true entropy rate. Finally, we demonstrate the
effectiveness of the approach in RL tasks inspired by human-robot use-cases,
and show how it produces agents with more predictable behavior while achieving
near-optimal rewards.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18710">Meta-Prior: Meta learning for Adaptive Inverse Problem Solvers. (arXiv:2311.18710v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Terris_M/0/1/0/all/0/1">Matthieu Terris</a>, <a href="http://arxiv.org/find/cs/1/au:+Moreau_T/0/1/0/all/0/1">Thomas Moreau</a></p>
<p>Deep neural networks have become a foundational tool for addressing imaging
inverse problems. They are typically trained for a specific task, with a
supervised loss to learn a mapping from the observations to the image to
recover. However, real-world imaging challenges often lack ground truth data,
rendering traditional supervised approaches ineffective. Moreover, for each new
imaging task, a new model needs to be trained from scratch, wasting time and
resources. To overcome these limitations, we introduce a novel approach based
on meta-learning. Our method trains a meta-model on a diverse set of imaging
tasks that allows the model to be efficiently fine-tuned for specific tasks
with few fine-tuning steps. We show that the proposed method extends to the
unsupervised setting, where no ground truth data is available. In its bilevel
formulation, the outer level uses a supervised loss, that evaluates how well
the fine-tuned model performs, while the inner loss can be either supervised or
unsupervised, relying only on the measurement operator. This allows the
meta-model to leverage a few ground truth samples for each task while being
able to generalize to new imaging tasks. We show that in simple settings, this
approach recovers the Bayes optimal estimator, illustrating the soundness of
our approach. We also demonstrate our method's effectiveness on various tasks,
including image processing and magnetic resonance imaging.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18718">Steering Deep Feature Learning with Backward Aligned Feature Updates. (arXiv:2311.18718v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chizat_L/0/1/0/all/0/1">L&#xe9;na&#xef;c Chizat</a>, <a href="http://arxiv.org/find/cs/1/au:+Netrapalli_P/0/1/0/all/0/1">Praneeth Netrapalli</a></p>
<p>Deep learning succeeds by doing hierarchical feature learning, yet tuning
Hyper-Parameters (HP) such as initialization scales, learning rates etc., only
give indirect control over this behavior. In this paper, we propose the
alignment between the feature updates and the backward pass as a key notion to
predict, measure and control feature learning. On the one hand, we show that
when alignment holds, the magnitude of feature updates after one SGD step is
related to the magnitude of the forward and backward passes by a simple and
general formula. This leads to techniques to automatically adjust HPs
(initialization scales and learning rates) at initialization and throughout
training to attain a desired feature learning behavior. On the other hand, we
show that, at random initialization, this alignment is determined by the
spectrum of a certain kernel, and that well-conditioned layer-to-layer
Jacobians (aka dynamical isometry) implies alignment. Finally, we investigate
ReLU MLPs and ResNets in the large width-then-depth limit. Combining hints from
random matrix theory and numerical experiments, we show that (i) in MLP with
iid initializations, alignment degenerates with depth, making it impossible to
start training, and that (ii) in ResNets, the branch scale
$1/\sqrt{\text{depth}}$ is the only one maintaining non-trivial alignment at
infinite depth.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18725">AI in Pharma for Personalized Sequential Decision-Making: Methods, Applications and Opportunities. (arXiv:2311.18725v1 [stat.ME])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Li_Y/0/1/0/all/0/1">Yuhan Li</a>, <a href="http://arxiv.org/find/stat/1/au:+Zhang_H/0/1/0/all/0/1">Hongtao Zhang</a>, <a href="http://arxiv.org/find/stat/1/au:+Anderson_K/0/1/0/all/0/1">Keaven Anderson</a>, <a href="http://arxiv.org/find/stat/1/au:+Li_S/0/1/0/all/0/1">Songzi Li</a>, <a href="http://arxiv.org/find/stat/1/au:+Zhu_R/0/1/0/all/0/1">Ruoqing Zhu</a></p>
<p>In the pharmaceutical industry, the use of artificial intelligence (AI) has
seen consistent growth over the past decade. This rise is attributed to major
advancements in statistical machine learning methodologies, computational
capabilities and the increased availability of large datasets. AI techniques
are applied throughout different stages of drug development, ranging from drug
discovery to post-marketing benefit-risk assessment. Kolluri et al. provided a
review of several case studies that span these stages, featuring key
applications such as protein structure prediction, success probability
estimation, subgroup identification, and AI-assisted clinical trial monitoring.
From a regulatory standpoint, there was a notable uptick in submissions
incorporating AI components in 2021. The most prevalent therapeutic areas
leveraging AI were oncology (27%), psychiatry (15%), gastroenterology (12%),
and neurology (11%). The paradigm of personalized or precision medicine has
gained significant traction in recent research, partly due to advancements in
AI techniques \cite{hamburg2010path}. This shift has had a transformative
impact on the pharmaceutical industry. Departing from the traditional
"one-size-fits-all" model, personalized medicine incorporates various
individual factors, such as environmental conditions, lifestyle choices, and
health histories, to formulate customized treatment plans. By utilizing
sophisticated machine learning algorithms, clinicians and researchers are
better equipped to make informed decisions in areas such as disease prevention,
diagnosis, and treatment selection, thereby optimizing health outcomes for each
individual.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18727">Automatic Functional Differentiation in JAX. (arXiv:2311.18727v1 [cs.PL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1">Min Lin</a></p>
<p>We extend JAX with the capability to automatically differentiate higher-order
functions (functionals and operators). By representing functions as a
generalization of arrays, we seamlessly use JAX's existing primitive system to
implement higher-order functions. We present a set of primitive operators that
serve as foundational building blocks for constructing several key types of
functionals. For every introduced primitive operator, we derive and implement
both linearization and transposition rules, aligning with JAX's internal
protocols for forward and reverse mode automatic differentiation. This
enhancement allows for functional differentiation in the same syntax
traditionally use for functions. The resulting functional gradients are
themselves functions ready to be invoked in python. We showcase this tool's
efficacy and simplicity through applications where functional derivatives are
indispensable. The source code of this work is released at
https://github.com/sail-sg/autofd .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2009.09213">Dodging DeepFake Detection via Implicit Spatial-Domain Notch Filtering. (arXiv:2009.09213v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yihao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1">Felix Juefei-Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1">Qing Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pu_G/0/1/0/all/0/1">Geguang Pu</a></p>
<p>The current high-fidelity generation and high-precision detection of DeepFake
images are at an arms race. We believe that producing DeepFakes that are highly
realistic and 'detection evasive' can serve the ultimate goal of improving
future generation DeepFake detection capabilities. In this paper, we propose a
simple yet powerful pipeline to reduce the artifact patterns of fake images
without hurting image quality by performing implicit spatial-domain notch
filtering. We first demonstrate that frequency-domain notch filtering, although
famously shown to be effective in removing periodic noise in the spatial
domain, is infeasible for our task at hand due to the manual designs required
for the notch filters. We, therefore, resort to a learning-based approach to
reproduce the notch filtering effects, but solely in the spatial domain. We
adopt a combination of adding overwhelming spatial noise for breaking the
periodic noise pattern and deep image filtering to reconstruct the noise-free
fake images, and we name our method DeepNotch. Deep image filtering provides a
specialized filter for each pixel in the noisy image, producing filtered images
with high fidelity compared to their DeepFake counterparts. Moreover, we also
use the semantic information of the image to generate an adversarial guidance
map to add noise intelligently. Our large-scale evaluation on 3 representative
state-of-the-art DeepFake detection methods (tested on 16 types of DeepFakes)
has demonstrated that our technique significantly reduces the accuracy of these
3 fake image detection methods, 36.79% on average and up to 97.02% in the best
case.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2012.15408">Gated Ensemble of Spatio-temporal Mixture of Experts for Multi-task Learning in Ride-hailing System. (arXiv:2012.15408v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1">M. H. Rahman</a>, <a href="http://arxiv.org/find/cs/1/au:+Rifaat_S/0/1/0/all/0/1">S. M. Rifaat</a>, <a href="http://arxiv.org/find/cs/1/au:+Sadeek_S/0/1/0/all/0/1">S. N. Sadeek</a>, <a href="http://arxiv.org/find/cs/1/au:+Abrar_M/0/1/0/all/0/1">M. Abrar</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">D. Wang</a></p>
<p>Designing spatio-temporal forecasting models separately in a task-wise and
city-wise manner poses a burden for the expanding transportation network
companies. Therefore, a multi-task learning architecture is proposed in this
study by developing gated ensemble of spatio-temporal mixture of experts
network (GESME-Net) with convolutional recurrent neural network (CRNN),
convolutional neural network (CNN), and recurrent neural network (RNN) for
simultaneously forecasting spatio-temporal tasks in a city as well as across
different cities. Furthermore, a task adaptation layer is integrated with the
architecture for learning joint representation in multi-task learning and
revealing the contribution of the input features utilized in prediction. The
proposed architecture is tested with data from Didi Chuxing for: (i)
simultaneously forecasting demand and supply-demand gap in Beijing, and (ii)
simultaneously forecasting demand across Chengdu and Xian. In both scenarios,
models from our proposed architecture outperformed the single-task and
multi-task deep learning benchmarks and ensemble-based machine learning
algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2103.03808">Two-step reinforcement learning for model-free redesign of nonlinear optimal regulator. (arXiv:2103.03808v4 [eess.SY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Minami_M/0/1/0/all/0/1">Mei Minami</a>, <a href="http://arxiv.org/find/eess/1/au:+Masumoto_Y/0/1/0/all/0/1">Yuka Masumoto</a>, <a href="http://arxiv.org/find/eess/1/au:+Okawa_Y/0/1/0/all/0/1">Yoshihiro Okawa</a>, <a href="http://arxiv.org/find/eess/1/au:+Sasaki_T/0/1/0/all/0/1">Tomotake Sasaki</a>, <a href="http://arxiv.org/find/eess/1/au:+Hori_Y/0/1/0/all/0/1">Yutaka Hori</a></p>
<p>In many practical control applications, the performance level of a
closed-loop system degrades over time due to the change of plant
characteristics. Thus, there is a strong need for redesigning a controller
without going through the system modeling process, which is often difficult for
closed-loop systems. Reinforcement learning (RL) is one of the promising
approaches that enable model-free redesign of optimal controllers for nonlinear
dynamical systems based only on the measurement of the closed-loop system.
However, the learning process of RL usually requires a considerable number of
trial-and-error experiments using the poorly controlled system that may
accumulate wear on the plant. To overcome this limitation, we propose a
model-free two-step design approach that improves the transient learning
performance of RL in an optimal regulator redesign problem for unknown
nonlinear systems. Specifically, we first design a linear control law that
attains some degree of control performance in a model-free manner, and then,
train the nonlinear optimal control law with online RL by using the designed
linear control law in parallel. We introduce an offline RL algorithm for the
design of the linear control law and theoretically guarantee its convergence to
the LQR controller under mild assumptions. Numerical simulations show that the
proposed approach improves the transient learning performance and efficiency in
hyperparameter tuning of RL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2106.00599">ClustML: A Measure of Cluster Pattern Complexity in Scatterplots Learnt from Human-labeled Groupings. (arXiv:2106.00599v2 [cs.HC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abbas_M/0/1/0/all/0/1">Mostafa M. Abbas</a>, <a href="http://arxiv.org/find/cs/1/au:+Ullah_E/0/1/0/all/0/1">Ehsan Ullah</a>, <a href="http://arxiv.org/find/cs/1/au:+Baggag_A/0/1/0/all/0/1">Abdelkader Baggag</a>, <a href="http://arxiv.org/find/cs/1/au:+Bensmail_H/0/1/0/all/0/1">Halima Bensmail</a>, <a href="http://arxiv.org/find/cs/1/au:+Sedlmair_M/0/1/0/all/0/1">Michael Sedlmair</a>, <a href="http://arxiv.org/find/cs/1/au:+Aupetit_M/0/1/0/all/0/1">Micha&#xeb;l Aupetit</a></p>
<p>Visual quality measures (VQMs) are designed to support analysts by
automatically detecting and quantifying patterns in visualizations. We propose
a new VQM for visual grouping patterns in scatterplots, called ClustML, which
is trained on previously collected human subject judgments. Our model encodes
scatterplots in the parametric space of a Gaussian Mixture Model and uses a
classifier trained on human judgment data to estimate the perceptual complexity
of grouping patterns. The numbers of initial mixture components and final
combined groups. It improves on existing VQMs, first, by better estimating
human judgments on two-Gaussian cluster patterns and, second, by giving higher
accuracy when ranking general cluster patterns in scatterplots. We use it to
analyze kinship data for genome-wide association studies, in which experts rely
on the visual analysis of large sets of scatterplots. We make the benchmark
datasets and the new VQM available for practical use and further improvements.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2110.04996">A Survey of Learning Criteria Going Beyond the Usual Risk. (arXiv:2110.04996v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Holland_M/0/1/0/all/0/1">Matthew J. Holland</a>, <a href="http://arxiv.org/find/stat/1/au:+Tanabe_K/0/1/0/all/0/1">Kazuki Tanabe</a></p>
<p>Virtually all machine learning tasks are characterized using some form of
loss function, and "good performance" is typically stated in terms of a
sufficiently small average loss, taken over the random draw of test data. While
optimizing for performance on average is intuitive, convenient to analyze in
theory, and easy to implement in practice, such a choice brings about
trade-offs. In this work, we survey and introduce a wide variety of
non-traditional criteria used to design and evaluate machine learning
algorithms, place the classical paradigm within the proper historical context,
and propose a view of learning problems which emphasizes the question of "what
makes for a desirable loss distribution?" in place of tacit use of the expected
loss.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.05128">Fair Community Detection and Structure Learning in Heterogeneous Graphical Models. (arXiv:2112.05128v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Tarzanagh_D/0/1/0/all/0/1">Davoud Ataee Tarzanagh</a>, <a href="http://arxiv.org/find/stat/1/au:+Balzano_L/0/1/0/all/0/1">Laura Balzano</a>, <a href="http://arxiv.org/find/stat/1/au:+Hero_A/0/1/0/all/0/1">Alfred O. Hero</a></p>
<p>Inference of community structure in probabilistic graphical models may not be
consistent with fairness constraints when nodes have demographic attributes.
Certain demographics may be over-represented in some detected communities and
under-represented in others. This paper defines a novel $\ell_1$-regularized
pseudo-likelihood approach for fair graphical model selection. In particular,
we assume there is some community or clustering structure in the true
underlying graph, and we seek to learn a sparse undirected graph and its
communities from the data such that demographic groups are fairly represented
within the communities. In the case when the graph is known a priori, we
provide a convex semidefinite programming approach for fair community
detection. We establish the statistical consistency of the proposed method for
both a Gaussian graphical model and an Ising model for, respectively,
continuous and binary data, proving that our method can recover the graphs and
their fair communities with high probability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2201.03215">Handwriting recognition and automatic scoring for descriptive answers in Japanese language tests. (arXiv:2201.03215v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1">Hung Tuan Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1">Cuong Tuan Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Oka_H/0/1/0/all/0/1">Haruki Oka</a>, <a href="http://arxiv.org/find/cs/1/au:+Ishioka_T/0/1/0/all/0/1">Tsunenori Ishioka</a>, <a href="http://arxiv.org/find/cs/1/au:+Nakagawa_M/0/1/0/all/0/1">Masaki Nakagawa</a></p>
<p>This paper presents an experiment of automatically scoring handwritten
descriptive answers in the trial tests for the new Japanese university entrance
examination, which were made for about 120,000 examinees in 2017 and 2018.
There are about 400,000 answers with more than 20 million characters. Although
all answers have been scored by human examiners, handwritten characters are not
labeled. We present our attempt to adapt deep neural network-based handwriting
recognizers trained on a labeled handwriting dataset into this unlabeled answer
set. Our proposed method combines different training strategies, ensembles
multiple recognizers, and uses a language model built from a large general
corpus to avoid overfitting into specific data. In our experiment, the proposed
method records character accuracy of over 97% using about 2,000 verified
labeled answers that account for less than 0.5% of the dataset. Then, the
recognized answers are fed into a pre-trained automatic scoring system based on
the BERT model without correcting misrecognized characters and providing rubric
annotations. The automatic scoring system achieves from 0.84 to 0.98 of
Quadratic Weighted Kappa (QWK). As QWK is over 0.8, it represents an acceptable
similarity of scoring between the automatic scoring system and the human
examiners. These results are promising for further research on end-to-end
automatic scoring of descriptive answers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.03077">On the Convergence of the ELBO to Entropy Sums. (arXiv:2209.03077v4 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Lucke_J/0/1/0/all/0/1">J&#xf6;rg L&#xfc;cke</a>, <a href="http://arxiv.org/find/stat/1/au:+Warnken_J/0/1/0/all/0/1">Jan Warnken</a></p>
<p>The variational lower bound (a.k.a. ELBO or free energy) is the central
objective for many established as well as many novel algorithms for
unsupervised learning. Learning algorithms change model parameters such that
the variational lower bound increases. Learning usually proceeds until
parameters have converged to values close to a stationary point of the learning
dynamics. In this purely theoretical contribution, we show that (for a very
large class of generative models) the variational lower bound is at all
stationary points of learning equal to a sum of entropies. For standard machine
learning models with one set of latents and one set observed variables, the sum
consists of three entropies: (A) the (average) entropy of the variational
distributions, (B) the negative entropy of the model's prior distribution, and
(C) the (expected) negative entropy of the observable distributions. The
obtained result applies under realistic conditions including: finite numbers of
data points, at any stationary points (including saddle points) and for any
family of (well behaved) variational distributions. The class of generative
models for which we show the equality to entropy sums contains many well-known
generative models. As concrete examples we discuss Sigmoid Belief Networks,
probabilistic PCA and (Gaussian and non-Gaussian) mixture models. The results
also apply for standard (Gaussian) variational autoencoders, which has been
shown in parallel (Damm et al., 2023). The prerequisites we use to show
equality to entropy sums are relatively mild. Concretely, the distributions of
a given generative model have to be of the exponential family (with constant
base measure), and the model has to satisfy a parameterization criterion (which
is usually fulfilled). Proving the equality of the ELBO to entropy sums at
stationary points (under the stated conditions) is the main contribution of
this work.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.01858">Relating graph auto-encoders to linear models. (arXiv:2211.01858v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Klepper_S/0/1/0/all/0/1">Solveig Klepper</a>, <a href="http://arxiv.org/find/cs/1/au:+Luxburg_U/0/1/0/all/0/1">Ulrike von Luxburg</a></p>
<p>Graph auto-encoders are widely used to construct graph representations in
Euclidean vector spaces. However, it has already been pointed out empirically
that linear models on many tasks can outperform graph auto-encoders. In our
work, we prove that the solution space induced by graph auto-encoders is a
subset of the solution space of a linear map. This demonstrates that linear
embedding models have at least the representational power of graph
auto-encoders based on graph convolutional networks. So why are we still using
nonlinear graph auto-encoders? One reason could be that actively restricting
the linear solution space might introduce an inductive bias that helps improve
learning and generalization. While many researchers believe that the
nonlinearity of the encoder is the critical ingredient towards this end, we
instead identify the node features of the graph as a more powerful inductive
bias. We give theoretical insights by introducing a corresponding bias in a
linear model and analyzing the change in the solution space. Our experiments
are aligned with other empirical work on this question and show that the linear
encoder can outperform the nonlinear encoder when using feature information.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.13316">Understanding Sample Generation Strategies for Learning Heuristic Functions in Classical Planning. (arXiv:2211.13316v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bettker_R/0/1/0/all/0/1">R. V. Bettker</a>, <a href="http://arxiv.org/find/cs/1/au:+Minini_P/0/1/0/all/0/1">P. P. Minini</a>, <a href="http://arxiv.org/find/cs/1/au:+Pereira_A/0/1/0/all/0/1">A. G. Pereira</a>, <a href="http://arxiv.org/find/cs/1/au:+Ritt_M/0/1/0/all/0/1">M. Ritt</a></p>
<p>We study the problem of learning good heuristic functions for classical
planning tasks with neural networks based on samples represented by states with
their cost-to-goal estimates. The heuristic function is learned for a state
space and goal condition with the number of samples limited to a fraction of
the size of the state space, and must generalize well for all states of the
state space with the same goal condition. Our main goal is to better understand
the influence of sample generation strategies on the performance of a greedy
best-first heuristic search (GBFS) guided by a learned heuristic function. In a
set of controlled experiments, we find that two main factors determine the
quality of the learned heuristic: which states are included in the sample set
and the quality of the cost-to-goal estimates. These two factors are dependent:
having perfect cost-to-goal estimates is insufficient if the samples are not
well distributed across the state space. We also study other effects, such as
adding samples with high-value estimates. Based on our findings, we propose
practical strategies to improve the quality of learned heuristics: three
strategies that aim to generate more representative states and two strategies
that improve the cost-to-goal estimates. Our practical strategies almost double
the mean coverage of a GBFS algorithm guided by a learned heuristic.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.10696">Analyzing Semantic Faithfulness of Language Models via Input Intervention on Question Answering. (arXiv:2212.10696v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chaturvedi_A/0/1/0/all/0/1">Akshay Chaturvedi</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhar_S/0/1/0/all/0/1">Swarnadeep Bhar</a>, <a href="http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1">Soumadeep Saha</a>, <a href="http://arxiv.org/find/cs/1/au:+Garain_U/0/1/0/all/0/1">Utpal Garain</a>, <a href="http://arxiv.org/find/cs/1/au:+Asher_N/0/1/0/all/0/1">Nicholas Asher</a></p>
<p>Transformer-based language models have been shown to be highly effective for
several NLP tasks. In this paper, we consider three transformer models, BERT,
RoBERTa, and XLNet, in both small and large versions, and investigate how
faithful their representations are with respect to the semantic content of
texts. We formalize a notion of semantic faithfulness, in which the semantic
content of a text should causally figure in a model's inferences in question
answering. We then test this notion by observing a model's behavior on
answering questions about a story after performing two novel semantic
interventions: deletion intervention and negation intervention. While
transformer models achieve high performance on standard question answering
tasks, we show that they fail to be semantically faithful once we perform these
interventions for a significant number of cases (~50% for deletion
intervention, and ~20% drop in accuracy for negation intervention). We then
propose an intervention-based training regime that can mitigate the undesirable
effects for deletion intervention by a significant margin (from ~ 50% to ~6%).
We analyze the inner-workings of the models to better understand the
effectiveness of intervention-based training for deletion intervention. But we
show that this training does not attenuate other aspects of semantic
unfaithfulness such as the models' inability to deal with negation intervention
or to capture the predicate-argument structure of texts. We also test
InstructGPT, via prompting, for its ability to handle the two interventions and
to capture predicate-argument structure. While InstructGPT models do achieve
very high performance on predicate-argument structure task, they fail to
respond adequately to our deletion and negation interventions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.12993">Benchmarking Robustness to Adversarial Image Obfuscations. (arXiv:2301.12993v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stimberg_F/0/1/0/all/0/1">Florian Stimberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Chakrabarti_A/0/1/0/all/0/1">Ayan Chakrabarti</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1">Chun-Ta Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hazimeh_H/0/1/0/all/0/1">Hussein Hazimeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Stretcu_O/0/1/0/all/0/1">Otilia Stretcu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_W/0/1/0/all/0/1">Wei Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yintao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaya_M/0/1/0/all/0/1">Merve Kaya</a>, <a href="http://arxiv.org/find/cs/1/au:+Rashtchian_C/0/1/0/all/0/1">Cyrus Rashtchian</a>, <a href="http://arxiv.org/find/cs/1/au:+Fuxman_A/0/1/0/all/0/1">Ariel Fuxman</a>, <a href="http://arxiv.org/find/cs/1/au:+Tek_M/0/1/0/all/0/1">Mehmet Tek</a>, <a href="http://arxiv.org/find/cs/1/au:+Gowal_S/0/1/0/all/0/1">Sven Gowal</a></p>
<p>Automated content filtering and moderation is an important tool that allows
online platforms to build striving user communities that facilitate cooperation
and prevent abuse. Unfortunately, resourceful actors try to bypass automated
filters in a bid to post content that violate platform policies and codes of
conduct. To reach this goal, these malicious actors may obfuscate policy
violating images (e.g. overlay harmful images by carefully selected benign
images or visual patterns) to prevent machine learning models from reaching the
correct decision. In this paper, we invite researchers to tackle this specific
issue and present a new image benchmark. This benchmark, based on ImageNet,
simulates the type of obfuscations created by malicious actors. It goes beyond
ImageNet-$\textrm{C}$ and ImageNet-$\bar{\textrm{C}}$ by proposing general,
drastic, adversarial modifications that preserve the original content intent.
It aims to tackle a more common adversarial threat than the one considered by
$\ell_p$-norm bounded adversaries. We evaluate 33 pretrained models on the
benchmark and train models with different augmentations, architectures and
training methods on subsets of the obfuscations to measure generalization. We
hope this benchmark will encourage researchers to test their models and methods
and try to find new approaches that are more robust to these obfuscations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.08893">Active learning for data streams: a survey. (arXiv:2302.08893v4 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Cacciarelli_D/0/1/0/all/0/1">Davide Cacciarelli</a>, <a href="http://arxiv.org/find/stat/1/au:+Kulahci_M/0/1/0/all/0/1">Murat Kulahci</a></p>
<p>Online active learning is a paradigm in machine learning that aims to select
the most informative data points to label from a data stream. The problem of
minimizing the cost associated with collecting labeled observations has gained
a lot of attention in recent years, particularly in real-world applications
where data is only available in an unlabeled form. Annotating each observation
can be time-consuming and costly, making it difficult to obtain large amounts
of labeled data. To overcome this issue, many active learning strategies have
been proposed in the last decades, aiming to select the most informative
observations for labeling in order to improve the performance of machine
learning models. These approaches can be broadly divided into two categories:
static pool-based and stream-based active learning. Pool-based active learning
involves selecting a subset of observations from a closed pool of unlabeled
data, and it has been the focus of many surveys and literature reviews.
However, the growing availability of data streams has led to an increase in the
number of approaches that focus on online active learning, which involves
continuously selecting and labeling observations as they arrive in a stream.
This work aims to provide an overview of the most recently proposed approaches
for selecting the most informative observations from data streams in real time.
We review the various techniques that have been proposed and discuss their
strengths and limitations, as well as the challenges and opportunities that
exist in this area of research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.10886">Some Intriguing Aspects about Lipschitz Continuity of Neural Networks. (arXiv:2302.10886v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khromov_G/0/1/0/all/0/1">Grigory Khromov</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Sidak Pal Singh</a></p>
<p>Lipschitz continuity is a crucial functional property of any predictive
model, that naturally governs its robustness, generalisation, as well as
adversarial vulnerability. Contrary to other works that focus on obtaining
tighter bounds and developing different practical strategies to enforce certain
Lipschitz properties, we aim to thoroughly examine and characterise the
Lipschitz behaviour of Neural Networks. Thus, we carry out an empirical
investigation in a range of different settings (namely, architectures,
datasets, label noise, and more) by exhausting the limits of the simplest and
the most general lower and upper bounds. As a highlight of this investigation,
we showcase a remarkable fidelity of the lower Lipschitz bound, identify a
striking Double Descent trend in both upper and lower bounds to the Lipschitz
and explain the intriguing effects of label noise on function smoothness and
generalisation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.01923">Bayesian CART models for insurance claims frequency. (arXiv:2303.01923v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Zhang_Y/0/1/0/all/0/1">Yaojun Zhang</a>, <a href="http://arxiv.org/find/stat/1/au:+Ji_L/0/1/0/all/0/1">Lanpeng Ji</a>, <a href="http://arxiv.org/find/stat/1/au:+Aivaliotis_G/0/1/0/all/0/1">Georgios Aivaliotis</a>, <a href="http://arxiv.org/find/stat/1/au:+Taylor_C/0/1/0/all/0/1">Charles Taylor</a></p>
<p>Accuracy and interpretability of a (non-life) insurance pricing model are
essential qualities to ensure fair and transparent premiums for policy-holders,
that reflect their risk. In recent years, the classification and regression
trees (CARTs) and their ensembles have gained popularity in the actuarial
literature, since they offer good prediction performance and are relatively
easily interpretable. In this paper, we introduce Bayesian CART models for
insurance pricing, with a particular focus on claims frequency modelling.
Additionally to the common Poisson and negative binomial (NB) distributions
used for claims frequency, we implement Bayesian CART for the zero-inflated
Poisson (ZIP) distribution to address the difficulty arising from the
imbalanced insurance claims data. To this end, we introduce a general MCMC
algorithm using data augmentation methods for posterior tree exploration. We
also introduce the deviance information criterion (DIC) for the tree model
selection. The proposed models are able to identify trees which can better
classify the policy-holders into risk groups. Some simulations and real
insurance data will be discussed to illustrate the applicability of these
models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.10253">Image retrieval outperforms diffusion models on data augmentation. (arXiv:2304.10253v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Burg_M/0/1/0/all/0/1">Max F. Burg</a>, <a href="http://arxiv.org/find/cs/1/au:+Wenzel_F/0/1/0/all/0/1">Florian Wenzel</a>, <a href="http://arxiv.org/find/cs/1/au:+Zietlow_D/0/1/0/all/0/1">Dominik Zietlow</a>, <a href="http://arxiv.org/find/cs/1/au:+Horn_M/0/1/0/all/0/1">Max Horn</a>, <a href="http://arxiv.org/find/cs/1/au:+Makansi_O/0/1/0/all/0/1">Osama Makansi</a>, <a href="http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1">Francesco Locatello</a>, <a href="http://arxiv.org/find/cs/1/au:+Russell_C/0/1/0/all/0/1">Chris Russell</a></p>
<p>Many approaches have been proposed to use diffusion models to augment
training datasets for downstream tasks, such as classification. However,
diffusion models are themselves trained on large datasets, often with noisy
annotations, and it remains an open question to which extent these models
contribute to downstream classification performance. In particular, it remains
unclear if they generalize enough to improve over directly using the additional
data of their pre-training process for augmentation. We systematically evaluate
a range of existing methods to generate images from diffusion models and study
new extensions to assess their benefit for data augmentation. Personalizing
diffusion models towards the target data outperforms simpler prompting
strategies. However, using the pre-training data of the diffusion model alone,
via a simple nearest-neighbor retrieval procedure, leads to even stronger
downstream performance. Our study explores the potential of diffusion models in
generating new training data, and surprisingly finds that these sophisticated
models are not yet able to beat a simple and strong image retrieval baseline on
simple downstream vision tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.00162">Beyond Prediction: On-street Parking Recommendation using Heterogeneous Graph-based List-wise Ranking. (arXiv:2305.00162v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Hanyu Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xiao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1">Wei Ma</a></p>
<p>To provide real-time parking information, existing studies focus on
predicting parking availability, which seems an indirect approach to saving
drivers' cruising time. In this paper, we first time propose an on-street
parking recommendation (OPR) task to directly recommend a parking space for a
driver. To this end, a learn-to-rank (LTR) based OPR model called OPR-LTR is
built. Specifically, parking recommendation is closely related to the "turnover
events" (state switching between occupied and vacant) of each parking space,
and hence we design a highly efficient heterogeneous graph called ESGraph to
represent historical and real-time meters' turnover events as well as
geographical relations; afterward, a convolution-based event-then-graph network
is used to aggregate and update representations of the heterogeneous graph. A
ranking model is further utilized to learn a score function that helps
recommend a list of ranked parking spots for a specific on-street parking
query. The method is verified using the on-street parking meter data in Hong
Kong and San Francisco. By comparing with the other two types of methods:
prediction-only and prediction-then-recommendation, the proposed
direct-recommendation method achieves satisfactory performance in different
metrics. Extensive experiments also demonstrate that the proposed ESGraph and
the recommendation model are more efficient in terms of computational
efficiency as well as saving drivers' on-street parking time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.04082">A Minimal Approach for Natural Language Action Space in Text-based Games. (arXiv:2305.04082v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ryu_D/0/1/0/all/0/1">Dongwon Kelvin Ryu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1">Meng Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1">Shirui Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1">Gholamreza Haffari</a>, <a href="http://arxiv.org/find/cs/1/au:+Shareghi_E/0/1/0/all/0/1">Ehsan Shareghi</a></p>
<p>Text-based games (TGs) are language-based interactive environments for
reinforcement learning. While language models (LMs) and knowledge graphs (KGs)
are commonly used for handling large action space in TGs, it is unclear whether
these techniques are necessary or overused. In this paper, we revisit the
challenge of exploring the action space in TGs and propose $
\epsilon$-admissible exploration, a minimal approach of utilizing admissible
actions, for training phase. Additionally, we present a text-based actor-critic
(TAC) agent that produces textual commands for game, solely from game
observations, without requiring any KG or LM. Our method, on average across 10
games from Jericho, outperforms strong baselines and state-of-the-art agents
that use LM and KG. Our approach highlights that a much lighter model design,
with a fresh perspective on utilizing the information within the environments,
suffices for an effective exploration of exponentially large action spaces.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.06988">Self-Chained Image-Language Model for Video Localization and Question Answering. (arXiv:2305.06988v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1">Shoubin Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1">Jaemin Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Yadav_P/0/1/0/all/0/1">Prateek Yadav</a>, <a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1">Mohit Bansal</a></p>
<p>Recent studies have shown promising results on utilizing large pre-trained
image-language models for video question answering. While these image-language
models can efficiently bootstrap the representation learning of video-language
models, they typically concatenate uniformly sampled video frames as visual
inputs without explicit language-aware, temporal modeling. When only a portion
of a video input is relevant to the language query, such uniform frame sampling
can often lead to missing important visual cues. Although humans often find a
video moment to focus on and rewind the moment to answer questions, training a
query-aware video moment localizer often requires expensive annotations and
high computational costs. To address this issue, we propose Self-Chained Video
Localization-Answering (SeViLA), a novel framework that leverages a single
image-language model (BLIP-2) to tackle both temporal keyframe localization and
QA on videos. SeViLA framework consists of two modules: Localizer and Answerer,
where both are parameter-efficiently fine-tuned from BLIP-2. We propose two
ways of chaining these modules for cascaded inference and self-refinement.
First, in the forward chain, the Localizer finds multiple language-aware
keyframes in a video, which the Answerer uses to predict the answer. Second, in
the reverse chain, the Answerer generates keyframe pseudo-labels to refine the
Localizer, alleviating the need for expensive video moment localization
annotations. Our SeViLA framework outperforms several strong baselines on 5
challenging video QA and event prediction benchmarks, and achieves the
state-of-the-art in both fine-tuning (NExT-QA, STAR) and zero-shot (NExT-QA,
STAR, How2QA, VLEP) settings. We also analyze the impact of Localizer,
comparisons of Localizer with other temporal localization models,
pre-training/self-refinement of Localizer, and varying the number of keyframes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.11164">Exploring the Carbon Footprint of Hugging Face&#x27;s ML Models: A Repository Mining Study. (arXiv:2305.11164v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Castano_J/0/1/0/all/0/1">Joel Casta&#xf1;o</a>, <a href="http://arxiv.org/find/cs/1/au:+Martinez_Fernandez_S/0/1/0/all/0/1">Silverio Mart&#xed;nez-Fern&#xe1;ndez</a>, <a href="http://arxiv.org/find/cs/1/au:+Franch_X/0/1/0/all/0/1">Xavier Franch</a>, <a href="http://arxiv.org/find/cs/1/au:+Bogner_J/0/1/0/all/0/1">Justus Bogner</a></p>
<p>The rise of machine learning (ML) systems has exacerbated their carbon
footprint due to increased capabilities and model sizes. However, there is
scarce knowledge on how the carbon footprint of ML models is actually measured,
reported, and evaluated. In light of this, the paper aims to analyze the
measurement of the carbon footprint of 1,417 ML models and associated datasets
on Hugging Face, which is the most popular repository for pretrained ML models.
The goal is to provide insights and recommendations on how to report and
optimize the carbon efficiency of ML models. The study includes the first
repository mining study on the Hugging Face Hub API on carbon emissions. This
study seeks to answer two research questions: (1) how do ML model creators
measure and report carbon emissions on Hugging Face Hub?, and (2) what aspects
impact the carbon emissions of training ML models? The study yielded several
key findings. These include a stalled proportion of carbon emissions-reporting
models, a slight decrease in reported carbon footprint on Hugging Face over the
past 2 years, and a continued dominance of NLP as the main application domain.
Furthermore, the study uncovers correlations between carbon emissions and
various attributes such as model size, dataset size, and ML application
domains. These results highlight the need for software measurements to improve
energy reporting practices and promote carbon-efficient model development
within the Hugging Face community. In response to this issue, two
classifications are proposed: one for categorizing models based on their carbon
emission reporting practices and another for their carbon efficiency. The aim
of these classification proposals is to foster transparency and sustainable
model development within the ML community.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13172">Editing Large Language Models: Problems, Methods, and Opportunities. (arXiv:2305.13172v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1">Yunzhi Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Peng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_B/0/1/0/all/0/1">Bozhong Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1">Siyuan Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhoubo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1">Shumin Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huajun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Ningyu Zhang</a></p>
<p>Despite the ability to train capable LLMs, the methodology for maintaining
their relevancy and rectifying errors remains elusive. To this end, the past
few years have witnessed a surge in techniques for editing LLMs, the objective
of which is to efficiently alter the behavior of LLMs within a specific domain
without negatively impacting performance across other inputs. This paper
embarks on a deep exploration of the problems, methods, and opportunities
related to model editing for LLMs. In particular, we provide an exhaustive
overview of the task definition and challenges associated with model editing,
along with an in-depth empirical analysis of the most progressive methods
currently at our disposal. We also build a new benchmark dataset to facilitate
a more robust evaluation and pinpoint enduring issues intrinsic to existing
techniques. Our objective is to provide valuable insights into the
effectiveness and feasibility of each editing technique, thereby assisting the
community in making informed decisions on the selection of the most appropriate
method for a specific task or context. Code and datasets are available at
https://github.com/zjunlp/EasyEdit.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13236">ADA-GP: Accelerating DNN Training By Adaptive Gradient Prediction. (arXiv:2305.13236v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Janfaza_V/0/1/0/all/0/1">Vahid Janfaza</a>, <a href="http://arxiv.org/find/cs/1/au:+Mandal_S/0/1/0/all/0/1">Shantanu Mandal</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahmud_F/0/1/0/all/0/1">Farabi Mahmud</a>, <a href="http://arxiv.org/find/cs/1/au:+Muzahid_A/0/1/0/all/0/1">Abdullah Muzahid</a></p>
<p>Neural network training is inherently sequential where the layers finish the
forward propagation in succession, followed by the calculation and
back-propagation of gradients (based on a loss function) starting from the last
layer. The sequential computations significantly slow down neural network
training, especially the deeper ones. Prediction has been successfully used in
many areas of computer architecture to speed up sequential processing.
Therefore, we propose ADA-GP, which uses gradient prediction adaptively to
speed up deep neural network (DNN) training while maintaining accuracy. ADA-GP
works by incorporating a small neural network to predict gradients for
different layers of a DNN model. ADA-GP uses a novel tensor reorganization
method to make it feasible to predict a large number of gradients. ADA-GP
alternates between DNN training using backpropagated gradients and DNN training
using predicted gradients. ADA-GP adaptively adjusts when and for how long
gradient prediction is used to strike a balance between accuracy and
performance. Last but not least, we provide a detailed hardware extension in a
typical DNN accelerator to realize the speed up potential from gradient
prediction. Our extensive experiments with fifteen DNN models show that ADA-GP
can achieve an average speed up of 1.47X with similar or even higher accuracy
than the baseline models. Moreover, it consumes, on average, 34% less energy
due to reduced off-chip memory accesses compared to the baseline accelerator.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14595">Operationalizing Counterfactual Metrics: Incentives, Ranking, and Information Asymmetry. (arXiv:2305.14595v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Serena Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bates_S/0/1/0/all/0/1">Stephen Bates</a>, <a href="http://arxiv.org/find/cs/1/au:+Aronow_P/0/1/0/all/0/1">P. M. Aronow</a>, <a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1">Michael I. Jordan</a></p>
<p>From the social sciences to machine learning, it has been well documented
that metrics to be optimized are not always aligned with social welfare. In
healthcare, Dranove et al. (2003) showed that publishing surgery mortality
metrics actually harmed the welfare of sicker patients by increasing provider
selection behavior. We analyze the incentive misalignments that arise from such
average treated outcome metrics, and show that the incentives driving treatment
decisions would align with maximizing total patient welfare if the metrics (i)
accounted for counterfactual untreated outcomes and (ii) considered total
welfare instead of averaging over treated patients. Operationalizing this, we
show how counterfactual metrics can be modified to behave reasonably in
patient-facing ranking systems. Extending to realistic settings when providers
observe more about patients than the regulatory agencies do, we bound the decay
in performance by the degree of information asymmetry between principal and
agent. In doing so, our model connects principal-agent information asymmetry
with unobserved heterogeneity in causal inference.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15613">Learning Deep O($n$)-Equivariant Hyperspheres. (arXiv:2305.15613v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Melnyk_P/0/1/0/all/0/1">Pavlo Melnyk</a>, <a href="http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1">Michael Felsberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Wadenback_M/0/1/0/all/0/1">M&#xe5;rten Wadenb&#xe4;ck</a>, <a href="http://arxiv.org/find/cs/1/au:+Robinson_A/0/1/0/all/0/1">Andreas Robinson</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_C/0/1/0/all/0/1">Cuong Le</a></p>
<p>This paper presents an approach to learning (deep) $n$D features equivariant
under orthogonal transformations, utilizing hyperspheres and regular
$n$-simplexes. Our main contributions are theoretical and tackle major
challenges in geometric deep learning such as equivariance and invariance under
geometric transformations. Namely, we enrich the recently developed theory of
steerable 3D spherical neurons -- SO(3)-equivariant filter banks based on
neurons with spherical decision surfaces -- by extending said neurons to $n$D,
which we call deep equivariant hyperspheres, and enabling their multi-layer
construction. Using synthetic and real-world data in $n$D, we experimentally
verify our theoretical contributions and find that our approach is superior to
the competing methods for benchmark datasets in all but one case, additionally
demonstrating a better speed/performance trade-off in all but one other case.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16526">Extending Explainable Boosting Machines to Scientific Image Data. (arXiv:2305.16526v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schug_D/0/1/0/all/0/1">Daniel Schug</a>, <a href="http://arxiv.org/find/cs/1/au:+Yerramreddy_S/0/1/0/all/0/1">Sai Yerramreddy</a>, <a href="http://arxiv.org/find/cs/1/au:+Caruana_R/0/1/0/all/0/1">Rich Caruana</a>, <a href="http://arxiv.org/find/cs/1/au:+Greenberg_C/0/1/0/all/0/1">Craig Greenberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Zwolak_J/0/1/0/all/0/1">Justyna P. Zwolak</a></p>
<p>As the deployment of computer vision technology becomes increasingly common
in science, the need for explanations of the system and its output has become a
focus of great concern. Driven by the pressing need for interpretable models in
science, we propose the use of Explainable Boosting Machines (EBMs) for
scientific image data. Inspired by an important application underpinning the
development of quantum technologies, we apply EBMs to cold-atom soliton image
data tabularized using Gabor Wavelet Transform-based techniques that preserve
the spatial structure of the data. In doing so, we demonstrate the use of EBMs
for image data for the first time and show that our approach provides
explanations that are consistent with human intuition about the data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.18498">ANPL: Towards Natural Programming with Interactive Decomposition. (arXiv:2305.18498v2 [cs.PL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1">Di Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Nan_Z/0/1/0/all/0/1">Ziyuan Nan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xing Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1">Pengwei Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1">Shaohui Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1">Yuanbo Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Rui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1">Zidong Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1">Qi Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Pu_Y/0/1/0/all/0/1">Yewen Pu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yunji Chen</a></p>
<p>Though LLMs are capable of generating plausible programs, it's challenging to
interact with the LLMs further to revise the program, especially if the user's
specific requirements are different from the initial proposal. In this paper,
we introduce ANPL, an interactive programming system that ensures users can
always refine the generated code towards their specific programmatic intents
via structured decompositions. Borrowing the paradigm of sketching from program
synthesis, an ANPL program consists of a set of input-outputs that it must
satisfy, a ``sketch'' -- control/data flow expressed in precise code (e.g.
Python), and ``holes'' -- sub-modules to be implemented by the LLM specified
with natural language. The user revises an ANPL program by either modifying the
sketch, changing the language used to describe the holes, or providing
additional input-outputs to a particular hole, turning it into a sub-ANPL
program that can be solved recursively. This workflow allows the users to
offload programming burdens to the LLM as much as possible while retaining the
ability to pinpoint and resolve bugs locally, without exposing the rest of the
program to the LLM. We deploy ANPL on the Abstraction and Reasoning Corpus
(ARC), a set of unique tasks that are challenging for state-of-the-art AI
systems, showing it outperforms baseline programming systems that (a) without
the ability to decompose tasks interactively and (b) without the guarantee that
the modules can be correctly composed together. Additional evaluations on APPS,
HumanEval, and real-world programming tasks have validated that the ANPL
framework is applicable to multiple programming domains. We release the ANPL
solutions to the ARC tasks as a dataset, providing insights into how humans
decompose novel tasks programmatically. See our code at
https://iprc-dip.github.io/ANPL/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19007">Training a HyperDimensional Computing Classifier using a Threshold on its Confidence. (arXiv:2305.19007v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Smets_L/0/1/0/all/0/1">Laura Smets</a>, <a href="http://arxiv.org/find/cs/1/au:+Leekwijck_W/0/1/0/all/0/1">Werner Van Leekwijck</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1">Ing Jyh Tsang</a>, <a href="http://arxiv.org/find/cs/1/au:+Latre_S/0/1/0/all/0/1">Steven Latre</a></p>
<p>Hyperdimensional computing (HDC) has become popular for light-weight and
energy-efficient machine learning, suitable for wearable Internet-of-Things
(IoT) devices and near-sensor or on-device processing. HDC is computationally
less complex than traditional deep learning algorithms and achieves moderate to
good classification performance. This article proposes to extend the training
procedure in HDC by taking into account not only wrongly classified samples,
but also samples that are correctly classified by the HDC model but with low
confidence. As such, a confidence threshold is introduced that can be tuned for
each dataset to achieve the best classification accuracy. The proposed training
procedure is tested on UCIHAR, CTG, ISOLET and HAND dataset for which the
performance consistently improves compared to the baseline across a range of
confidence threshold values. The extended training procedure also results in a
shift towards higher confidence values of the correctly classified samples
making the classifier not only more accurate but also more confident about its
predictions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.00210">PERFOGRAPH: A Numerical Aware Program Graph Representation for Performance Optimization and Program Analysis. (arXiv:2306.00210v2 [cs.PL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+TehraniJamsaz_A/0/1/0/all/0/1">Ali TehraniJamsaz</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahmud_Q/0/1/0/all/0/1">Quazi Ishtiaque Mahmud</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Le Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_N/0/1/0/all/0/1">Nesreen K. Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Jannesari_A/0/1/0/all/0/1">Ali Jannesari</a></p>
<p>The remarkable growth and significant success of machine learning have
expanded its applications into programming languages and program analysis.
However, a key challenge in adopting the latest machine learning methods is the
representation of programming languages, which directly impacts the ability of
machine learning methods to reason about programs. The absence of numerical
awareness, aggregate data structure information, and improper way of presenting
variables in previous representation works have limited their performances. To
overcome the limitations and challenges of current program representations, we
propose a graph-based program representation called PERFOGRAPH. PERFOGRAPH can
capture numerical information and the aggregate data structure by introducing
new nodes and edges. Furthermore, we propose an adapted embedding method to
incorporate numerical awareness. These enhancements make PERFOGRAPH a highly
flexible and scalable representation that effectively captures programs
intricate dependencies and semantics. Consequently, it serves as a powerful
tool for various applications such as program analysis, performance
optimization, and parallelism discovery. Our experimental results demonstrate
that PERFOGRAPH outperforms existing representations and sets new
state-of-the-art results by reducing the error rate by 7.4% (AMD dataset) and
10% (NVIDIA dataset) in the well-known Device Mapping challenge. It also sets
new state-of-the-art results in various performance optimization tasks like
Parallelism Discovery and NUMA and Prefetchers Configuration prediction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.07266">Operator Learning with Neural Fields: Tackling PDEs on General Geometries. (arXiv:2306.07266v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Serrano_L/0/1/0/all/0/1">Louis Serrano</a>, <a href="http://arxiv.org/find/cs/1/au:+Boudec_L/0/1/0/all/0/1">Lise Le Boudec</a>, <a href="http://arxiv.org/find/cs/1/au:+Koupai_A/0/1/0/all/0/1">Armand Kassa&#xef; Koupa&#xef;</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Thomas X Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1">Yuan Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Vittaut_J/0/1/0/all/0/1">Jean-No&#xeb;l Vittaut</a>, <a href="http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1">Patrick Gallinari</a></p>
<p>Machine learning approaches for solving partial differential equations
require learning mappings between function spaces. While convolutional or graph
neural networks are constrained to discretized functions, neural operators
present a promising milestone toward mapping functions directly. Despite
impressive results they still face challenges with respect to the domain
geometry and typically rely on some form of discretization. In order to
alleviate such limitations, we present CORAL, a new method that leverages
coordinate-based networks for solving PDEs on general geometries. CORAL is
designed to remove constraints on the input mesh, making it applicable to any
spatial sampling and geometry. Its ability extends to diverse problem domains,
including PDE solving, spatio-temporal forecasting, and inverse problems like
geometric design. CORAL demonstrates robust performance across multiple
resolutions and performs well in both convex and non-convex domains, surpassing
or performing on par with state-of-the-art models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.08018">Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models. (arXiv:2306.08018v4 [q-bio.QM] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Fang_Y/0/1/0/all/0/1">Yin Fang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Liang_X/0/1/0/all/0/1">Xiaozhuan Liang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Zhang_N/0/1/0/all/0/1">Ningyu Zhang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Liu_K/0/1/0/all/0/1">Kangwei Liu</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Huang_R/0/1/0/all/0/1">Rui Huang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Chen_Z/0/1/0/all/0/1">Zhuo Chen</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Fan_X/0/1/0/all/0/1">Xiaohui Fan</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Chen_H/0/1/0/all/0/1">Huajun Chen</a></p>
<p>Large Language Models (LLMs), with their remarkable task-handling
capabilities and innovative outputs, have catalyzed significant advancements
across a spectrum of fields. However, their proficiency within specialized
domains such as biomolecular studies remains limited. To address this
challenge, we introduce Mol-Instructions, a comprehensive instruction dataset
designed for the biomolecular domain. Mol-Instructions encompasses three key
components: molecule-oriented instructions, protein-oriented instructions, and
biomolecular text instructions. Each component aims to improve the
understanding and prediction capabilities of LLMs concerning biomolecular
features and behaviors. Through extensive instruction tuning experiments on
LLMs, we demonstrate the effectiveness of Mol-Instructions in enhancing large
models' performance in the intricate realm of biomolecular studies, thus
fostering progress in the biomolecular research community. Mol-Instructions is
publicly available for ongoing research and will undergo regular updates to
enhance its applicability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.12230">Fantastic Weights and How to Find Them: Where to Prune in Dynamic Sparse Training. (arXiv:2306.12230v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nowak_A/0/1/0/all/0/1">Aleksandra I. Nowak</a>, <a href="http://arxiv.org/find/cs/1/au:+Grooten_B/0/1/0/all/0/1">Bram Grooten</a>, <a href="http://arxiv.org/find/cs/1/au:+Mocanu_D/0/1/0/all/0/1">Decebal Constantin Mocanu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tabor_J/0/1/0/all/0/1">Jacek Tabor</a></p>
<p>Dynamic Sparse Training (DST) is a rapidly evolving area of research that
seeks to optimize the sparse initialization of a neural network by adapting its
topology during training. It has been shown that under specific conditions, DST
is able to outperform dense models. The key components of this framework are
the pruning and growing criteria, which are repeatedly applied during the
training process to adjust the network's sparse connectivity. While the growing
criterion's impact on DST performance is relatively well studied, the influence
of the pruning criterion remains overlooked. To address this issue, we design
and perform an extensive empirical analysis of various pruning criteria to
better understand their impact on the dynamics of DST solutions. Surprisingly,
we find that most of the studied methods yield similar results. The differences
become more significant in the low-density regime, where the best performance
is predominantly given by the simplest technique: magnitude-based pruning. The
code is provided at https://github.com/alooow/fantastic_weights_paper
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.15220">S-TLLR: STDP-inspired Temporal Local Learning Rule for Spiking Neural Networks. (arXiv:2306.15220v3 [cs.NE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Apolinario_M/0/1/0/all/0/1">Marco Paul E. Apolinario</a>, <a href="http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1">Kaushik Roy</a></p>
<p>Spiking Neural Networks (SNNs) are biologically plausible models that have
been identified as potentially apt for deploying energy-efficient intelligence
at the edge, particularly for sequential learning tasks. However, training of
SNNs poses significant challenges due to the necessity for precise temporal and
spatial credit assignment. Back-propagation through time (BPTT) algorithm,
whilst the most widely used method for addressing these issues, incurs a high
computational cost due to its temporal dependency. In this work, we propose
S-TLLR, a novel three-factor temporal local learning rule inspired by the
Spike-Timing Dependent Plasticity (STDP) mechanism, aimed at training deep SNNs
on event-based learning tasks. Furthermore, S-TLLR is designed to have low
memory and time complexities, which are independent of the number of time
steps, rendering it suitable for online learning on low-power edge devices. To
demonstrate the scalability of our proposed method, we have conducted extensive
evaluations on event-based datasets spanning a wide range of applications, such
as image and gesture recognition, audio classification, and optical flow
estimation. In all the experiments, S-TLLR achieved high accuracy, comparable
to BPTT, with a reduction in memory between $5-50\times$ and
multiply-accumulate (MAC) operations between $1.3-6.6\times$.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03170">Focused Transformer: Contrastive Training for Context Scaling. (arXiv:2307.03170v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tworkowski_S/0/1/0/all/0/1">Szymon Tworkowski</a>, <a href="http://arxiv.org/find/cs/1/au:+Staniszewski_K/0/1/0/all/0/1">Konrad Staniszewski</a>, <a href="http://arxiv.org/find/cs/1/au:+Pacek_M/0/1/0/all/0/1">Miko&#x142;aj Pacek</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yuhuai Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Michalewski_H/0/1/0/all/0/1">Henryk Michalewski</a>, <a href="http://arxiv.org/find/cs/1/au:+Milos_P/0/1/0/all/0/1">Piotr Mi&#x142;o&#x15b;</a></p>
<p>Large language models have an exceptional capability to incorporate new
information in a contextual manner. However, the full potential of such an
approach is often restrained due to a limitation in the effective context
length. One solution to this issue is to endow an attention layer with access
to an external memory, which comprises of (key, value) pairs. Yet, as the
number of documents increases, the proportion of relevant keys to irrelevant
ones decreases, leading the model to focus more on the irrelevant keys. We
identify a significant challenge, dubbed the distraction issue, where keys
linked to different semantic values might overlap, making them hard to
distinguish. To tackle this problem, we introduce the Focused Transformer
(FoT), a technique that employs a training process inspired by contrastive
learning. This novel approach enhances the structure of the (key, value) space,
enabling an extension of the context length. Our method allows for fine-tuning
pre-existing, large-scale models to lengthen their effective context. This is
demonstrated by our fine-tuning of $3B$ and $7B$ OpenLLaMA checkpoints. The
resulting models, which we name LongLLaMA, exhibit advancements in tasks
requiring a long context. We further illustrate that our LongLLaMA models
adeptly manage a $256 k$ context length for passkey retrieval.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14680">TimeGNN: Temporal Dynamic Graph Learning for Time Series Forecasting. (arXiv:2307.14680v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1">Nancy Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kosma_C/0/1/0/all/0/1">Chrysoula Kosma</a>, <a href="http://arxiv.org/find/cs/1/au:+Vazirgiannis_M/0/1/0/all/0/1">Michalis Vazirgiannis</a></p>
<p>Time series forecasting lies at the core of important real-world applications
in many fields of science and engineering. The abundance of large time series
datasets that consist of complex patterns and long-term dependencies has led to
the development of various neural network architectures. Graph neural network
approaches, which jointly learn a graph structure based on the correlation of
raw values of multivariate time series while forecasting, have recently seen
great success. However, such solutions are often costly to train and difficult
to scale. In this paper, we propose TimeGNN, a method that learns dynamic
temporal graph representations that can capture the evolution of inter-series
patterns along with the correlations of multiple series. TimeGNN achieves
inference times 4 to 80 times faster than other state-of-the-art graph-based
methods while achieving comparable forecasting performance
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.00629">Hessian-Aware Bayesian Optimization for Decision Making Systems. (arXiv:2308.00629v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rajpal_M/0/1/0/all/0/1">Mohit Rajpal</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_L/0/1/0/all/0/1">Lac Gia Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yehong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Low_B/0/1/0/all/0/1">Bryan Kian Hsiang Low</a></p>
<p>Many approaches for optimizing decision making systems rely on gradient based
methods requiring informative feedback from the environment. However, in the
case where such feedback is sparse or uninformative, such approaches may result
in poor performance. Derivative-free approaches such as Bayesian Optimization
mitigate the dependency on the quality of gradient feedback, but are known to
scale poorly in the high-dimension setting of complex decision making systems.
This problem is exacerbated if the system requires interactions between several
actors cooperating to accomplish a shared goal. To address the dimensionality
challenge, we propose a compact multi-layered architecture modeling the
dynamics of actor interactions through the concept of role. Additionally, we
introduce Hessian-aware Bayesian Optimization to efficiently optimize the
multi-layered architecture parameterized by a large number of parameters.
Experimental results demonstrate that our method (HA-GP-UCB) works effectively
on several benchmarks under resource constraints and malformed feedback
settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04051">Generative Models for Anomaly Detection and Design-Space Dimensionality Reduction in Shape Optimization. (arXiv:2308.04051v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+DAgostino_D/0/1/0/all/0/1">Danny D&#x27;Agostino</a></p>
<p>Our work presents a novel approach to shape optimization, with the twofold
objective to improve the efficiency of global optimization algorithms while
promoting the generation of high-quality designs during the optimization
process free of geometrical anomalies. This is accomplished by reducing the
number of the original design variables defining a new reduced subspace where
the geometrical variance is maximized and modeling the underlying generative
process of the data via probabilistic linear latent variable models such as
factor analysis and probabilistic principal component analysis. We show that
the data follows approximately a Gaussian distribution when the shape
modification method is linear and the design variables are sampled uniformly at
random, due to the direct application of the central limit theorem. The degree
of anomalousness is measured in terms of Mahalanobis distance, and the paper
demonstrates that abnormal designs tend to exhibit a high value of this metric.
This enables the definition of a new optimization model where anomalous
geometries are penalized and consequently avoided during the optimization loop.
The procedure is demonstrated for hull shape optimization of the DTMB 5415
model, extensively used as an international benchmark for shape optimization
problems. The global optimization routine is carried out using Bayesian
optimization and the DIRECT algorithm. From the numerical results, the new
framework improves the convergence of global optimization algorithms, while
only designs with high-quality geometrical features are generated through the
optimization routine thereby avoiding the wastage of precious computationally
expensive simulations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.09835">Microscopy Image Segmentation via Point and Shape Regularized Data Synthesis. (arXiv:2308.09835v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shijie Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1">Mengwei Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Ach_T/0/1/0/all/0/1">Thomas Ach</a>, <a href="http://arxiv.org/find/cs/1/au:+Gerig_G/0/1/0/all/0/1">Guido Gerig</a></p>
<p>Current deep learning-based approaches for the segmentation of microscopy
images heavily rely on large amount of training data with dense annotation,
which is highly costly and laborious in practice. Compared to full annotation
where the complete contour of objects is depicted, point annotations,
specifically object centroids, are much easier to acquire and still provide
crucial information about the objects for subsequent segmentation. In this
paper, we assume access to point annotations only during training and develop a
unified pipeline for microscopy image segmentation using synthetically
generated training data. Our framework includes three stages: (1) it takes
point annotations and samples a pseudo dense segmentation mask constrained with
shape priors; (2) with an image generative model trained in an unpaired manner,
it translates the mask to a realistic microscopy image regularized by object
level consistency; (3) the pseudo masks along with the synthetic images then
constitute a pairwise dataset for training an ad-hoc segmentation model. On the
public MoNuSeg dataset, our synthesis pipeline produces more diverse and
realistic images than baseline models while maintaining high coherence between
input masks and generated images. When using the identical segmentation
backbones, the models trained on our synthetic dataset significantly outperform
those trained with pseudo-labels or baseline-generated images. Moreover, our
framework achieves comparable results to models trained on authentic microscopy
images with dense labels, demonstrating its potential as a reliable and highly
efficient alternative to labor-intensive manual pixel-wise annotations in
microscopy image segmentation. The code is available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.09952">Finding emergence in data by maximizing effective information. (arXiv:2308.09952v3 [physics.soc-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Yang_M/0/1/0/all/0/1">Mingzhe Yang</a>, <a href="http://arxiv.org/find/physics/1/au:+Wang_Z/0/1/0/all/0/1">Zhipeng Wang</a>, <a href="http://arxiv.org/find/physics/1/au:+Liu_K/0/1/0/all/0/1">Kaiwei Liu</a>, <a href="http://arxiv.org/find/physics/1/au:+Rong_Y/0/1/0/all/0/1">Yingqi Rong</a>, <a href="http://arxiv.org/find/physics/1/au:+Yuan_B/0/1/0/all/0/1">Bing Yuan</a>, <a href="http://arxiv.org/find/physics/1/au:+Zhang_J/0/1/0/all/0/1">Jiang Zhang</a></p>
<p>Quantifying emergence and modeling emergent dynamics in a data-driven manner
for complex dynamical systems is challenging due to the lack of direct
observations at the micro-level. Thus, it's crucial to develop a framework to
identify emergent phenomena and capture emergent dynamics at the macro-level
using available data. Inspired by the theory of causal emergence (CE), this
paper introduces a machine learning framework to learn macro-dynamics in an
emergent latent space and quantify the degree of CE. The framework maximizes
effective information, resulting in a macro-dynamics model with enhanced causal
effects. Experimental results on simulated and real data demonstrate the
effectiveness of the proposed framework. It quantifies degrees of CE
effectively under various conditions and reveals distinct influences of
different noise types. It can learn a one-dimensional coarse-grained
macro-state from fMRI data, to represent complex neural activities during movie
clip viewing. Furthermore, improved generalization to different test
environments is observed across all simulation data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.10997">MarkovGen: Structured Prediction for Efficient Text-to-Image Generation. (arXiv:2308.10997v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jayasumana_S/0/1/0/all/0/1">Sadeep Jayasumana</a>, <a href="http://arxiv.org/find/cs/1/au:+Glasner_D/0/1/0/all/0/1">Daniel Glasner</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramalingam_S/0/1/0/all/0/1">Srikumar Ramalingam</a>, <a href="http://arxiv.org/find/cs/1/au:+Veit_A/0/1/0/all/0/1">Andreas Veit</a>, <a href="http://arxiv.org/find/cs/1/au:+Chakrabarti_A/0/1/0/all/0/1">Ayan Chakrabarti</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1">Sanjiv Kumar</a></p>
<p>Modern text-to-image generation models produce high-quality images that are
both photorealistic and faithful to the text prompts. However, this quality
comes at significant computational cost: nearly all of these models are
iterative and require running sampling multiple times with large models. This
iterative process is needed to ensure that different regions of the image are
not only aligned with the text prompt, but also compatible with each other. In
this work, we propose a light-weight approach to achieving this compatibility
between different regions of an image, using a Markov Random Field (MRF) model.
We demonstrate the effectiveness of this method on top of the latent
token-based Muse text-to-image model. The MRF richly encodes the compatibility
among image tokens at different spatial locations to improve quality and
significantly reduce the required number of Muse sampling steps. Inference with
the MRF is significantly cheaper, and its parameters can be quickly learned
through back-propagation by modeling MRF inference as a differentiable
neural-network layer. Our full model, MarkovGen, uses this proposed MRF model
to both speed up Muse by 1.5X and produce higher quality images by decreasing
undesirable image artifacts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12114">Less is More -- Towards parsimonious multi-task models using structured sparsity. (arXiv:2308.12114v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Upadhyay_R/0/1/0/all/0/1">Richa Upadhyay</a>, <a href="http://arxiv.org/find/cs/1/au:+Phlypo_R/0/1/0/all/0/1">Ronald Phlypo</a>, <a href="http://arxiv.org/find/cs/1/au:+Saini_R/0/1/0/all/0/1">Rajkumar Saini</a>, <a href="http://arxiv.org/find/cs/1/au:+Liwicki_M/0/1/0/all/0/1">Marcus Liwicki</a></p>
<p>Model sparsification in deep learning promotes simpler, more interpretable
models with fewer parameters. This not only reduces the model's memory
footprint and computational needs but also shortens inference time. This work
focuses on creating sparse models optimized for multiple tasks with fewer
parameters. These parsimonious models also possess the potential to match or
outperform dense models in terms of performance. In this work, we introduce
channel-wise l1/l2 group sparsity in the shared convolutional layers parameters
(or weights) of the multi-task learning model. This approach facilitates the
removal of extraneous groups i.e., channels (due to l1 regularization) and also
imposes a penalty on the weights, further enhancing the learning efficiency for
all tasks (due to l2 regularization). We analyzed the results of group sparsity
in both single-task and multi-task settings on two widely-used Multi-Task
Learning (MTL) datasets: NYU-v2 and CelebAMask-HQ. On both datasets, which
consist of three different computer vision tasks each, multi-task models with
approximately 70% sparsity outperform their dense equivalents. We also
investigate how changing the degree of sparsification influences the model's
performance, the overall sparsity percentage, the patterns of sparsity, and the
inference time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.03720">A Natural Gas Consumption Forecasting System for Continual Learning Scenarios based on Hoeffding Trees with Change Point Detection Mechanism. (arXiv:2309.03720v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Svoboda_R/0/1/0/all/0/1">Radek Svoboda</a>, <a href="http://arxiv.org/find/cs/1/au:+Basterrech_S/0/1/0/all/0/1">Sebastian Basterrech</a>, <a href="http://arxiv.org/find/cs/1/au:+Kozal_J/0/1/0/all/0/1">J&#x119;drzej Kozal</a>, <a href="http://arxiv.org/find/cs/1/au:+Platos_J/0/1/0/all/0/1">Jan Plato&#x161;</a>, <a href="http://arxiv.org/find/cs/1/au:+Wozniak_M/0/1/0/all/0/1">Micha&#x142; Wo&#x17a;niak</a></p>
<p>Forecasting natural gas consumption, considering seasonality and trends, is
crucial in planning its supply and consumption and optimizing the cost of
obtaining it, mainly by industrial entities. However, in times of threats to
its supply, it is also a critical element that guarantees the supply of this
raw material to meet individual consumers' needs, ensuring society's energy
security. This article introduces a novel multistep ahead forecasting of
natural gas consumption with change point detection integration for model
collection selection with continual learning capabilities using data stream
processing. The performance of the forecasting models based on the proposed
approach is evaluated in a complex real-world use case of natural gas
consumption forecasting. We employed Hoeffding tree predictors as forecasting
models and the Pruned Exact Linear Time (PELT) algorithm for the change point
detection procedure. The change point detection integration enables selecting a
different model collection for successive time frames. Thus, three model
collection selection procedures (with and without an error feedback loop) are
defined and evaluated for forecasting scenarios with various densities of
detected change points. These models were compared with change point agnostic
baseline approaches. Our experiments show that fewer change points result in a
lower forecasting error regardless of the model collection selection procedure
employed. Also, simpler model collection selection procedures omitting
forecasting error feedback leads to more robust forecasting models suitable for
continual learning tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.05950">Language Models as Black-Box Optimizers for Vision-Language Models. (arXiv:2309.05950v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shihong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zhiqiu Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1">Samuel Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1">Ryan Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Ling_T/0/1/0/all/0/1">Tiffany Ling</a>, <a href="http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1">Deepak Pathak</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1">Deva Ramanan</a></p>
<p>Vision-language models (VLMs) pre-trained on web-scale datasets have
demonstrated remarkable capabilities on downstream tasks when fine-tuned with
minimal data. However, many VLMs rely on proprietary data and are not
open-source, which restricts the use of white-box approaches for fine-tuning.
As such, we aim to develop a black-box approach to optimize VLMs through
natural language prompts, thereby avoiding the need to access model parameters,
feature embeddings, or even output logits. We propose employing chat-based LLMs
to search for the best text prompt for VLMs. Specifically, we adopt an
automatic hill-climbing procedure that converges to an effective prompt by
evaluating the performance of current prompts and asking LLMs to refine them
based on textual feedback, all within a conversational process without
human-in-the-loop. In a challenging 1-shot image classification setup, our
simple approach surpasses the white-box continuous prompting method (CoOp) by
an average of 1.5% across 11 datasets including ImageNet. Our approach also
outperforms both human-engineered and LLM-generated prompts. We highlight the
advantage of conversational feedback that incorporates both positive and
negative prompts, suggesting that LLMs can utilize the implicit gradient
direction in textual feedback for a more efficient search. In addition, we find
that the text prompts generated through our strategy are not only more
interpretable but also transfer well across different VLM architectures in a
black-box manner. Lastly, we demonstrate our framework on a state-of-the-art
black-box VLM (DALL-E 3) for text-to-image optimization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.09737">Moving Object Detection and Tracking with 4D Radar Point Cloud. (arXiv:2309.09737v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1">Zhijun Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1">Fangqiang Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1">Hantao Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1">Chris Xiaoxuan Lu</a></p>
<p>Mobile autonomy relies on the precise perception of dynamic environments.
Robustly tracking moving objects in 3D world thus plays a pivotal role for
applications like trajectory prediction, obstacle avoidance, and path planning.
While most current methods utilize LiDARs or cameras for Multiple Object
Tracking (MOT), the capabilities of 4D imaging radars remain largely
unexplored. Recognizing the challenges posed by radar noise and point sparsity
in 4D radar data, we introduce RaTrack, an innovative solution tailored for
radar-based tracking. Bypassing the typical reliance on specific object types
and 3D bounding boxes, our method focuses on motion segmentation and
clustering, enriched by a motion estimation module. Evaluated on the
View-of-Delft dataset, RaTrack showcases superior tracking precision of moving
objects, largely surpassing the performance of the state of the art.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11526">Likelihood-based Sensor Calibration using Affine Transformation. (arXiv:2309.11526v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Machhamer_R/0/1/0/all/0/1">R&#xfc;diger Machhamer</a>, <a href="http://arxiv.org/find/cs/1/au:+Fazlic_L/0/1/0/all/0/1">Lejla Begic Fazlic</a>, <a href="http://arxiv.org/find/cs/1/au:+Guven_E/0/1/0/all/0/1">Eray Guven</a>, <a href="http://arxiv.org/find/cs/1/au:+Junk_D/0/1/0/all/0/1">David Junk</a>, <a href="http://arxiv.org/find/cs/1/au:+Kurt_G/0/1/0/all/0/1">Gunes Karabulut Kurt</a>, <a href="http://arxiv.org/find/cs/1/au:+Naumann_S/0/1/0/all/0/1">Stefan Naumann</a>, <a href="http://arxiv.org/find/cs/1/au:+Didas_S/0/1/0/all/0/1">Stephan Didas</a>, <a href="http://arxiv.org/find/cs/1/au:+Gollmer_K/0/1/0/all/0/1">Klaus-Uwe Gollmer</a>, <a href="http://arxiv.org/find/cs/1/au:+Bergmann_R/0/1/0/all/0/1">Ralph Bergmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Timm_I/0/1/0/all/0/1">Ingo J. Timm</a>, <a href="http://arxiv.org/find/cs/1/au:+Dartmann_G/0/1/0/all/0/1">Guido Dartmann</a></p>
<p>An important task in the field of sensor technology is the efficient
implementation of adaptation procedures of measurements from one sensor to
another sensor of identical design. One idea is to use the estimation of an
affine transformation between different systems, which can be improved by the
knowledge of experts. This paper presents an improved solution from Glacier
Research that was published back in 1973. The results demonstrate the
adaptability of this solution for various applications, including software
calibration of sensors, implementation of expert-based adaptation, and paving
the way for future advancements such as distributed learning methods. One idea
here is to use the knowledge of experts for estimating an affine transformation
between different systems. We evaluate our research with simulations and also
with real measured data of a multi-sensor board with 8 identical sensors. Both
data set and evaluation script are provided for download. The results show an
improvement for both the simulation and the experiments with real data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12673">On Sparse Modern Hopfield Model. (arXiv:2309.12673v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1">Jerry Yao-Chieh Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1">Donglin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1">Dennis Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Chenwei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Bo-Yu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Han Liu</a></p>
<p>We introduce the sparse modern Hopfield model as a sparse extension of the
modern Hopfield model. Like its dense counterpart, the sparse modern Hopfield
model equips a memory-retrieval dynamics whose one-step approximation
corresponds to the sparse attention mechanism. Theoretically, our key
contribution is a principled derivation of a closed-form sparse Hopfield energy
using the convex conjugate of the sparse entropic regularizer. Building upon
this, we derive the sparse memory retrieval dynamics from the sparse energy
function and show its one-step approximation is equivalent to the
sparse-structured attention. Importantly, we provide a sparsity-dependent
memory retrieval error bound which is provably tighter than its dense analog.
The conditions for the benefits of sparsity to arise are therefore identified
and discussed. In addition, we show that the sparse modern Hopfield model
maintains the robust theoretical properties of its dense counterpart, including
rapid fixed point convergence and exponential memory capacity. Empirically, we
use both synthetic and real-world datasets to demonstrate that the sparse
Hopfield model outperforms its dense counterpart in many situations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00533">SELF: Language-Driven Self-Evolution for Large Language Models. (arXiv:2310.00533v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Jianqiao Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1">Wanjun Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1">Wenyong Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yufei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1">Fei Mi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Baojun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Weichao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1">Lifeng Shang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qun Liu</a></p>
<p>Large Language Models (LLMs) have demonstrated remarkable versatility across
various domains. To further advance LLMs, we propose 'SELF' (Self-Evolution
with Language Feedback), a novel approach that enables LLMs to self-improve
through self-reflection, akin to human learning processes. SELF initiates with
a meta-skill learning process that equips the LLMs with capabilities for
self-feedback and self-refinement. Subsequently, the model undergoes an
iterative process of self-evolution. In each iteration, it utilizes an
unlabeled dataset of instructions to generate initial responses. These
responses are enhanced through self-feedback and self-refinement. The model is
then fine-tuned using this enhanced data. The model undergoes progressive
improvement through this iterative self-evolution process. Moreover, the SELF
framework enables the model to apply self-refinement during inference, which
further improves response quality. Our experiments in mathematics and general
tasks demonstrate that SELF can enhance the capabilities of LLMs without human
intervention. The SELF framework indicates a promising direction for the
autonomous evolution of LLMs, transitioning them from passive information
receivers to active participants in their development.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03121">OpenMM 8: Molecular Dynamics Simulation with Machine Learning Potentials. (arXiv:2310.03121v2 [physics.chem-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Eastman_P/0/1/0/all/0/1">Peter Eastman</a>, <a href="http://arxiv.org/find/physics/1/au:+Galvelis_R/0/1/0/all/0/1">Raimondas Galvelis</a>, <a href="http://arxiv.org/find/physics/1/au:+Pelaez_R/0/1/0/all/0/1">Ra&#xfa;l P. Pel&#xe1;ez</a>, <a href="http://arxiv.org/find/physics/1/au:+Abreu_C/0/1/0/all/0/1">Charlles R. A. Abreu</a>, <a href="http://arxiv.org/find/physics/1/au:+Farr_S/0/1/0/all/0/1">Stephen E. Farr</a>, <a href="http://arxiv.org/find/physics/1/au:+Gallicchio_E/0/1/0/all/0/1">Emilio Gallicchio</a>, <a href="http://arxiv.org/find/physics/1/au:+Gorenko_A/0/1/0/all/0/1">Anton Gorenko</a>, <a href="http://arxiv.org/find/physics/1/au:+Henry_M/0/1/0/all/0/1">Michael M. Henry</a>, <a href="http://arxiv.org/find/physics/1/au:+Hu_F/0/1/0/all/0/1">Frank Hu</a>, <a href="http://arxiv.org/find/physics/1/au:+Huang_J/0/1/0/all/0/1">Jing Huang</a>, <a href="http://arxiv.org/find/physics/1/au:+Kramer_A/0/1/0/all/0/1">Andreas Kr&#xe4;mer</a>, <a href="http://arxiv.org/find/physics/1/au:+Michel_J/0/1/0/all/0/1">Julien Michel</a>, <a href="http://arxiv.org/find/physics/1/au:+Mitchell_J/0/1/0/all/0/1">Joshua A. Mitchell</a>, <a href="http://arxiv.org/find/physics/1/au:+Pande_V/0/1/0/all/0/1">Vijay S. Pande</a>, <a href="http://arxiv.org/find/physics/1/au:+Rodrigues_J/0/1/0/all/0/1">Jo&#xe3;o PGLM Rodrigues</a>, <a href="http://arxiv.org/find/physics/1/au:+Rodriguez_Guerra_J/0/1/0/all/0/1">Jaime Rodriguez-Guerra</a>, <a href="http://arxiv.org/find/physics/1/au:+Simmonett_A/0/1/0/all/0/1">Andrew C. Simmonett</a>, <a href="http://arxiv.org/find/physics/1/au:+Singh_S/0/1/0/all/0/1">Sukrit Singh</a>, <a href="http://arxiv.org/find/physics/1/au:+Swails_J/0/1/0/all/0/1">Jason Swails</a>, <a href="http://arxiv.org/find/physics/1/au:+Turner_P/0/1/0/all/0/1">Philip Turner</a>, <a href="http://arxiv.org/find/physics/1/au:+Wang_Y/0/1/0/all/0/1">Yuanqing Wang</a>, <a href="http://arxiv.org/find/physics/1/au:+Zhang_I/0/1/0/all/0/1">Ivy Zhang</a>, <a href="http://arxiv.org/find/physics/1/au:+Chodera_J/0/1/0/all/0/1">John D. Chodera</a>, <a href="http://arxiv.org/find/physics/1/au:+Fabritiis_G/0/1/0/all/0/1">Gianni De Fabritiis</a>, <a href="http://arxiv.org/find/physics/1/au:+Markland_T/0/1/0/all/0/1">Thomas E. Markland</a></p>
<p>Machine learning plays an important and growing role in molecular simulation.
The newest version of the OpenMM molecular dynamics toolkit introduces new
features to support the use of machine learning potentials. Arbitrary PyTorch
models can be added to a simulation and used to compute forces and energy. A
higher-level interface allows users to easily model their molecules of interest
with general purpose, pretrained potential functions. A collection of optimized
CUDA kernels and custom PyTorch operations greatly improves the speed of
simulations. We demonstrate these features on simulations of cyclin-dependent
kinase 8 (CDK8) and the green fluorescent protein (GFP) chromophore in water.
Taken together, these features make it practical to use machine learning to
improve the accuracy of simulations at only a modest increase in cost.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03906">PyDCM: Custom Data Center Models with Reinforcement Learning for Sustainability. (arXiv:2310.03906v7 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Naug_A/0/1/0/all/0/1">Avisek Naug</a>, <a href="http://arxiv.org/find/cs/1/au:+Guillen_A/0/1/0/all/0/1">Antonio Guillen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gutierrez_R/0/1/0/all/0/1">Ricardo Luna Guti&#xe9;rrez</a>, <a href="http://arxiv.org/find/cs/1/au:+Gundecha_V/0/1/0/all/0/1">Vineet Gundecha</a>, <a href="http://arxiv.org/find/cs/1/au:+Markovikj_D/0/1/0/all/0/1">Dejan Markovikj</a>, <a href="http://arxiv.org/find/cs/1/au:+Kashyap_L/0/1/0/all/0/1">Lekhapriya Dheeraj Kashyap</a>, <a href="http://arxiv.org/find/cs/1/au:+Krause_L/0/1/0/all/0/1">Lorenz Krause</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghorbanpour_S/0/1/0/all/0/1">Sahand Ghorbanpour</a>, <a href="http://arxiv.org/find/cs/1/au:+Mousavi_S/0/1/0/all/0/1">Sajad Mousavi</a>, <a href="http://arxiv.org/find/cs/1/au:+Babu_A/0/1/0/all/0/1">Ashwin Ramesh Babu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1">Soumyendu Sarkar</a></p>
<p>The increasing global emphasis on sustainability and reducing carbon
emissions is pushing governments and corporations to rethink their approach to
data center design and operation. Given their high energy consumption and
exponentially large computational workloads, data centers are prime candidates
for optimizing power consumption, especially in areas such as cooling and IT
energy usage. A significant challenge in this pursuit is the lack of a
configurable and scalable thermal data center model that offers an end-to-end
pipeline. Data centers consist of multiple IT components whose geometric
configuration and heat dissipation make thermal modeling difficult. This paper
presents PyDCM, a customizable Data Center Model implemented in Python, that
allows users to create unique configurations of IT equipment with custom server
specifications and geometric arrangements of IT cabinets. The use of vectorized
thermal calculations makes PyDCM orders of magnitude faster (30 times) than
current Energy Plus modeling implementations and scales sublinearly with the
number of CPUs. Also, PyDCM enables the use of Deep Reinforcement Learning via
the Gymnasium wrapper to optimize data center cooling and offers a
user-friendly platform for testing various data center design prototypes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03915">Leveraging Low-Rank and Sparse Recurrent Connectivity for Robust Closed-Loop Control. (arXiv:2310.03915v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tumma_N/0/1/0/all/0/1">Neehal Tumma</a>, <a href="http://arxiv.org/find/cs/1/au:+Lechner_M/0/1/0/all/0/1">Mathias Lechner</a>, <a href="http://arxiv.org/find/cs/1/au:+Loo_N/0/1/0/all/0/1">Noel Loo</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasani_R/0/1/0/all/0/1">Ramin Hasani</a>, <a href="http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1">Daniela Rus</a></p>
<p>Developing autonomous agents that can interact with changing environments is
an open challenge in machine learning. Robustness is particularly important in
these settings as agents are often fit offline on expert demonstrations but
deployed online where they must generalize to the closed feedback loop within
the environment. In this work, we explore the application of recurrent neural
networks to tasks of this nature and understand how a parameterization of their
recurrent connectivity influences robustness in closed-loop settings.
Specifically, we represent the recurrent connectivity as a function of rank and
sparsity and show both theoretically and empirically that modulating these two
variables has desirable effects on network dynamics. The proposed low-rank,
sparse connectivity induces an interpretable prior on the network that proves
to be most amenable for a class of models known as closed-form continuous-time
neural networks (CfCs). We find that CfCs with fewer parameters can outperform
their full-rank, fully-connected counterparts in the online setting under
distribution shift. This yields memory-efficient and robust agents while
opening a new perspective on how we can modulate network dynamics through
connectivity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05518">On Double Descent in Reinforcement Learning with LSTD and Random Features. (arXiv:2310.05518v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Brellmann_D/0/1/0/all/0/1">David Brellmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Berthier_E/0/1/0/all/0/1">Elo&#xef;se Berthier</a>, <a href="http://arxiv.org/find/cs/1/au:+Filliat_D/0/1/0/all/0/1">David Filliat</a>, <a href="http://arxiv.org/find/cs/1/au:+Frehse_G/0/1/0/all/0/1">Goran Frehse</a></p>
<p>Temporal Difference (TD) algorithms are widely used in Deep Reinforcement
Learning (RL). Their performance is heavily influenced by the size of the
neural network. While in supervised learning, the regime of
over-parameterization and its benefits are well understood, the situation in RL
is much less clear. In this paper, we present a theoretical analysis of the
influence of network size and $l_2$-regularization on performance. We identify
the ratio between the number of parameters and the number of visited states as
a crucial factor and define over-parameterization as the regime when it is
larger than one. Furthermore, we observe a double descent phenomenon, i.e., a
sudden drop in performance around the parameter/state ratio of one. Leveraging
random features and the lazy training regime, we study the regularized
Least-Square Temporal Difference (LSTD) algorithm in an asymptotic regime, as
both the number of parameters and states go to infinity, maintaining a constant
ratio. We derive deterministic limits of both the empirical and the true
Mean-Square Bellman Error (MSBE) that feature correction terms responsible for
the double-descent. Correction terms vanish when the $l_2$-regularization is
increased or the number of unvisited states goes to zero. Numerical experiments
with synthetic and small real-world environments closely match the theoretical
predictions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05566">Aggregated f-average Neural Network for Interpretable Ensembling. (arXiv:2310.05566v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vu_M/0/1/0/all/0/1">Mathieu Vu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chouzenoux_E/0/1/0/all/0/1">Emilie Chouzenoux</a>, <a href="http://arxiv.org/find/cs/1/au:+Pesquet_J/0/1/0/all/0/1">Jean-Christophe Pesquet</a>, <a href="http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1">Ismail Ben Ayed</a></p>
<p>Ensemble learning leverages multiple models (i.e., weak learners) on a common
machine learning task to enhance prediction performance. Basic ensembling
approaches average the weak learners outputs, while more sophisticated ones
stack a machine learning model in between the weak learners outputs and the
final prediction. This work fuses both aforementioned frameworks. We introduce
an aggregated f-average (AFA) shallow neural network which models and combines
different types of averages to perform an optimal aggregation of the weak
learners predictions. We emphasise its interpretable architecture and simple
training strategy, and illustrate its good performance on the problem of
few-shot class incremental learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07918">Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning. (arXiv:2310.07918v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Deuschel_J/0/1/0/all/0/1">Jannik Deuschel</a>, <a href="http://arxiv.org/find/cs/1/au:+Ellington_C/0/1/0/all/0/1">Caleb N. Ellington</a>, <a href="http://arxiv.org/find/cs/1/au:+Lengerich_B/0/1/0/all/0/1">Benjamin J. Lengerich</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1">Yingtao Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Friederich_P/0/1/0/all/0/1">Pascal Friederich</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1">Eric P. Xing</a></p>
<p>Interpretable policy learning seeks to estimate intelligible decision
policies from observed actions; however, existing models fall short by forcing
a tradeoff between accuracy and interpretability. This tradeoff limits
data-driven interpretations of human decision-making process. e.g. to audit
medical decisions for biases and suboptimal practices, we require models of
decision processes which provide concise descriptions of complex behaviors.
Fundamentally, existing approaches are burdened by this tradeoff because they
represent the underlying decision process as a universal policy, when in fact
human decisions are dynamic and can change drastically with contextual
information. Thus, we propose Contextualized Policy Recovery (CPR), which
re-frames the problem of modeling complex decision processes as a multi-task
learning problem in which complex decision policies are comprised of
context-specific policies. CPR models each context-specific policy as a linear
observation-to-action mapping, and generates new decision models
$\textit{on-demand}$ as contexts are updated with new observations. CPR is
compatible with fully offline and partially observable decision environments,
and can be tailored to incorporate any recurrent black-box model or
interpretable decision model. We assess CPR through studies on simulated and
real data, achieving state-of-the-art performance on the canonical tasks of
predicting antibiotic prescription in intensive care units ($+22\%$ AUROC vs.
previous SOTA) and predicting MRI prescription for Alzheimer's patients
($+7.7\%$ AUROC vs. previous SOTA). With this improvement in predictive
performance, CPR closes the accuracy gap between interpretable and black-box
methods for policy learning, allowing high-resolution exploration and analysis
of context-specific decision models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08540">Do pretrained Transformers Really Learn In-context by Gradient Descent?. (arXiv:2310.08540v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1">Lingfeng Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1">Aayush Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1">Daniel Khashabi</a></p>
<p>The emergence of In-Context Learning (ICL) in LLMs remains a significant
phenomenon with little understanding. To explain ICL, recent studies try to
shed light on ICL by connecting it to Gradient Descent (GD). However, the
question is, do these hold up in practice in actual pre-trained models?
</p>
<p>We highlight the limiting assumptions in prior works that make their context
considerably different from the practical context in which language models are
trained. For example, the theoretical hand-constructed weights used in these
studies have properties that don't match those of real LLMs. Furthermore, their
experimental verification uses \emph{ICL objective} (training models explicitly
for ICL), which differs from the emergent ICL in the wild.
</p>
<p>We also look for evidence in real models. We observe that ICL and GD have
different sensitivity to the order in which they observe demonstrations.
Finally, we probe and compare the ICL vs. GD hypothesis in a natural setting.
We conduct comprehensive empirical analyses on language models pre-trained on
natural data (LLaMa-7B). Our comparisons of three performance metrics highlight
the inconsistent behavior of ICL and GD as a function of various factors such
as datasets, models, and the number of demonstrations. We observe that ICL and
GD modify the output distribution of language models differently. These results
indicate that the equivalence between ICL and GD remains an open hypothesis and
calls for further studies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.09485">Applying Bayesian Ridge Regression AI Modeling in Virus Severity Prediction. (arXiv:2310.09485v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pal_J/0/1/0/all/0/1">Jai Pal</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_B/0/1/0/all/0/1">Bryan Hong</a></p>
<p>Artificial intelligence (AI) is a powerful tool for reshaping healthcare
systems. In healthcare, AI is invaluable for its capacity to manage vast
amounts of data, which can lead to more accurate and speedy diagnoses,
ultimately easing the workload on healthcare professionals. As a result, AI has
proven itself to be a power tool across various industries, simplifying complex
tasks and pattern recognition that would otherwise be overwhelming for humans
or traditional computer algorithms. In this paper, we review the strengths and
weaknesses of Bayesian Ridge Regression, an AI model that can be used to bring
cutting edge virus analysis to healthcare professionals around the world. The
model's accuracy assessment revealed promising results, with room for
improvement primarily related to data organization. In addition, the severity
index serves as a valuable tool to gain a broad overview of patient care needs,
aligning with healthcare professionals' preference for broader categorizations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.09912">Unsupervised Discovery of Interpretable Directions in h-space of Pre-trained Diffusion Models. (arXiv:2310.09912v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zijian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Luping Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zhijie Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yichen Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zhou Zhao</a></p>
<p>We propose the first unsupervised and learning-based method to identify
interpretable directions in h-space of pre-trained diffusion models. Our method
is derived from an existing technique that operates on the GAN latent space.
Specifically, we employ a shift control module that works on h-space of
pre-trained diffusion models to manipulate a sample into a shifted version of
itself, followed by a reconstructor to reproduce both the type and the strength
of the manipulation. By jointly optimizing them, the model will spontaneously
discover disentangled and interpretable directions. To prevent the discovery of
meaningless and destructive directions, we employ a discriminator to maintain
the fidelity of shifted sample. Due to the iterative generative process of
diffusion models, our training requires a substantial amount of GPU VRAM to
store numerous intermediate tensors for back-propagating gradient. To address
this issue, we propose a general VRAM-efficient training algorithm based on
gradient checkpointing technique to back-propagate any gradient through the
whole generative process, with acceptable occupancy of VRAM and sacrifice of
training efficiency. Compared with existing related works on diffusion models,
our method inherently identifies global and scalable directions, without
necessitating any other complicated procedures. Extensive experiments on
various datasets demonstrate the effectiveness of our method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16111">Locally Differentially Private Document Generation Using Zero Shot Prompting. (arXiv:2310.16111v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Utpala_S/0/1/0/all/0/1">Saiteja Utpala</a>, <a href="http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1">Sara Hooker</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1">Pin Yu Chen</a></p>
<p>Numerous studies have highlighted the privacy risks associated with
pretrained large language models. In contrast, our research offers a unique
perspective by demonstrating that pretrained large language models can
effectively contribute to privacy preservation. We propose a locally
differentially private mechanism called DP-Prompt, which leverages the power of
pretrained large language models and zero-shot prompting to counter author
de-anonymization attacks while minimizing the impact on downstream utility.
When DP-Prompt is used with a powerful language model like ChatGPT (gpt-3.5),
we observe a notable reduction in the success rate of de-anonymization attacks,
showing that it surpasses existing approaches by a considerable margin despite
its simpler design. For instance, in the case of the IMDB dataset, DP-Prompt
(with ChatGPT) perfectly recovers the clean sentiment F1 score while achieving
a 46\% reduction in author identification F1 score against static attackers and
a 26\% reduction against adaptive attackers. We conduct extensive experiments
across six open-source large language models, ranging up to 7 billion
parameters, to analyze various effects of the privacy-utility tradeoff.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16121">19 Parameters Is All You Need: Tiny Neural Networks for Particle Physics. (arXiv:2310.16121v2 [hep-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/hep-ph/1/au:+Bogatskiy_A/0/1/0/all/0/1">Alexander Bogatskiy</a>, <a href="http://arxiv.org/find/hep-ph/1/au:+Hoffman_T/0/1/0/all/0/1">Timothy Hoffman</a>, <a href="http://arxiv.org/find/hep-ph/1/au:+Offermann_J/0/1/0/all/0/1">Jan T. Offermann</a></p>
<p>As particle accelerators increase their collision rates, and deep learning
solutions prove their viability, there is a growing need for lightweight and
fast neural network architectures for low-latency tasks such as triggering. We
examine the potential of one recent Lorentz- and permutation-symmetric
architecture, PELICAN, and present its instances with as few as 19 trainable
parameters that outperform generic architectures with tens of thousands of
parameters when compared on the binary classification task of top quark jet
tagging.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01442">Deep Double Descent for Time Series Forecasting: Avoiding Undertrained Models. (arXiv:2311.01442v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Assandri_V/0/1/0/all/0/1">Valentino Assandri</a>, <a href="http://arxiv.org/find/cs/1/au:+Heshmati_S/0/1/0/all/0/1">Sam Heshmati</a>, <a href="http://arxiv.org/find/cs/1/au:+Yaman_B/0/1/0/all/0/1">Burhaneddin Yaman</a>, <a href="http://arxiv.org/find/cs/1/au:+Iakovlev_A/0/1/0/all/0/1">Anton Iakovlev</a>, <a href="http://arxiv.org/find/cs/1/au:+Repetur_A/0/1/0/all/0/1">Ariel Emiliano Repetur</a></p>
<p>Deep learning models, particularly Transformers, have achieved impressive
results in various domains, including time series forecasting. While existing
time series literature primarily focuses on model architecture modifications
and data augmentation techniques, this paper explores the training schema of
deep learning models for time series; how models are trained regardless of
their architecture. We perform extensive experiments to investigate the
occurrence of deep double descent in several Transformer models trained on
public time series data sets. We demonstrate epoch-wise deep double descent and
that overfitting can be reverted using more epochs. Leveraging these findings,
we achieve state-of-the-art results for long sequence time series forecasting
in nearly 70% of the 72 benchmarks tested. This suggests that many models in
the literature may possess untapped potential. Additionally, we introduce a
taxonomy for classifying training schema modifications, covering data
augmentation, model inputs, model targets, time series per model, and
computational budget.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02496">LocoMuJoCo: A Comprehensive Imitation Learning Benchmark for Locomotion. (arXiv:2311.02496v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Al_Hafez_F/0/1/0/all/0/1">Firas Al-Hafez</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1">Guoping Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1">Jan Peters</a>, <a href="http://arxiv.org/find/cs/1/au:+Tateo_D/0/1/0/all/0/1">Davide Tateo</a></p>
<p>Imitation Learning (IL) holds great promise for enabling agile locomotion in
embodied agents. However, many existing locomotion benchmarks primarily focus
on simplified toy tasks, often failing to capture the complexity of real-world
scenarios and steering research toward unrealistic domains. To advance research
in IL for locomotion, we present a novel benchmark designed to facilitate
rigorous evaluation and comparison of IL algorithms. This benchmark encompasses
a diverse set of environments, including quadrupeds, bipeds, and
musculoskeletal human models, each accompanied by comprehensive datasets, such
as real noisy motion capture data, ground truth expert data, and ground truth
sub-optimal data, enabling evaluation across a spectrum of difficulty levels.
To increase the robustness of learned agents, we provide an easy interface for
dynamics randomization and offer a wide range of partially observable tasks to
train agents across different embodiments. Finally, we provide handcrafted
metrics for each task and ship our benchmark with state-of-the-art baseline
algorithms to ease evaluation and enable fast benchmarking.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03197">Stable Linear Subspace Identification: A Machine Learning Approach. (arXiv:2311.03197v3 [eess.SY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Natale_L/0/1/0/all/0/1">Loris Di Natale</a>, <a href="http://arxiv.org/find/eess/1/au:+Zakwan_M/0/1/0/all/0/1">Muhammad Zakwan</a>, <a href="http://arxiv.org/find/eess/1/au:+Svetozarevic_B/0/1/0/all/0/1">Bratislav Svetozarevic</a>, <a href="http://arxiv.org/find/eess/1/au:+Heer_P/0/1/0/all/0/1">Philipp Heer</a>, <a href="http://arxiv.org/find/eess/1/au:+Trecate_G/0/1/0/all/0/1">Giancarlo Ferrari Trecate</a>, <a href="http://arxiv.org/find/eess/1/au:+Jones_C/0/1/0/all/0/1">Colin N. Jones</a></p>
<p>Machine Learning (ML) and linear System Identification (SI) have been
historically developed independently. In this paper, we leverage
well-established ML tools - especially the automatic differentiation framework
- to introduce SIMBa, a family of discrete linear multi-step-ahead state-space
SI methods using backpropagation. SIMBa relies on a novel
Linear-Matrix-Inequality-based free parametrization of Schur matrices to ensure
the stability of the identified model.
</p>
<p>We show how SIMBa generally outperforms traditional linear state-space SI
methods, and sometimes significantly, although at the price of a higher
computational burden. This performance gap is particularly remarkable compared
to other SI methods with stability guarantees, where the gain is frequently
above 25% in our investigations, hinting at SIMBa's ability to simultaneously
achieve state-of-the-art fitting performance and enforce stability.
Interestingly, these observations hold for a wide variety of input-output
systems and on both simulated and real-world data, showcasing the flexibility
of the proposed approach. We postulate that this new SI paradigm presents a
great extension potential to identify structured nonlinear models from data,
and we hence open-source SIMBa on https://github.com/Cemempamoi/simba.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06428">A Trichotomy for Transductive Online Learning. (arXiv:2311.06428v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hanneke_S/0/1/0/all/0/1">Steve Hanneke</a>, <a href="http://arxiv.org/find/cs/1/au:+Moran_S/0/1/0/all/0/1">Shay Moran</a>, <a href="http://arxiv.org/find/cs/1/au:+Shafer_J/0/1/0/all/0/1">Jonathan Shafer</a></p>
<p>We present new upper and lower bounds on the number of learner mistakes in
the `transductive' online learning setting of Ben-David, Kushilevitz and
Mansour (1997). This setting is similar to standard online learning, except
that the adversary fixes a sequence of instances $x_1,\dots,x_n$ to be labeled
at the start of the game, and this sequence is known to the learner.
Qualitatively, we prove a trichotomy, stating that the minimal number of
mistakes made by the learner as $n$ grows can take only one of precisely three
possible values: $n$, $\Theta\left(\log (n)\right)$, or $\Theta(1)$.
Furthermore, this behavior is determined by a combination of the VC dimension
and the Littlestone dimension. Quantitatively, we show a variety of bounds
relating the number of mistakes to well-known combinatorial dimensions. In
particular, we improve the known lower bound on the constant in the $\Theta(1)$
case from $\Omega\left(\sqrt{\log(d)}\right)$ to $\Omega(\log(d))$ where $d$ is
the Littlestone dimension. Finally, we extend our results to cover multiclass
classification and the agnostic setting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12630">Hierarchical Joint Graph Learning and Multivariate Time Series Forecasting. (arXiv:2311.12630v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Juhyeon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hyungeun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1">Seungwon Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_U/0/1/0/all/0/1">Ung Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_W/0/1/0/all/0/1">Wooyul Jung</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_M/0/1/0/all/0/1">Miseon Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1">Kijung Yoon</a></p>
<p>Multivariate time series is prevalent in many scientific and industrial
domains. Modeling multivariate signals is challenging due to their long-range
temporal dependencies and intricate interactions--both direct and indirect. To
confront these complexities, we introduce a method of representing multivariate
signals as nodes in a graph with edges indicating interdependency between them.
Specifically, we leverage graph neural networks (GNN) and attention mechanisms
to efficiently learn the underlying relationships within the time series data.
Moreover, we suggest employing hierarchical signal decompositions running over
the graphs to capture multiple spatial dependencies. The effectiveness of our
proposed model is evaluated across various real-world benchmark datasets
designed for long-term forecasting tasks. The results consistently showcase the
superiority of our model, achieving an average 23\% reduction in mean squared
error (MSE) compared to existing models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12825">A PSO Based Method to Generate Actionable Counterfactuals for High Dimensional Data. (arXiv:2311.12825v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shekhar_S/0/1/0/all/0/1">Shashank Shekhar</a>, <a href="http://arxiv.org/find/cs/1/au:+Salim_A/0/1/0/all/0/1">Asif Salim</a>, <a href="http://arxiv.org/find/cs/1/au:+Bansode_A/0/1/0/all/0/1">Adesh Bansode</a>, <a href="http://arxiv.org/find/cs/1/au:+Jinturkar_V/0/1/0/all/0/1">Vivaswan Jinturkar</a>, <a href="http://arxiv.org/find/cs/1/au:+Nayak_A/0/1/0/all/0/1">Anirudha Nayak</a></p>
<p>Counterfactual explanations (CFE) are methods that explain a machine learning
model by giving an alternate class prediction of a data point with some minimal
changes in its features. It helps the users to identify their data attributes
that caused an undesirable prediction like a loan or credit card rejection. We
describe an efficient and an actionable counterfactual (CF) generation method
based on particle swarm optimization (PSO). We propose a simple objective
function for the optimization of the instance-centric CF generation problem.
The PSO brings in a lot of flexibility in terms of carrying out multi-objective
optimization in large dimensions, capability for multiple CF generation, and
setting box constraints or immutability of data attributes. An algorithm is
proposed that incorporates these features and it enables greater control over
the proximity and sparsity properties over the generated CFs. The proposed
algorithm is evaluated with a set of action-ability metrics in real-world
datasets, and the results were superior compared to that of the
state-of-the-arts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12831">ECNR: Efficient Compressive Neural Representation of Time-Varying Volumetric Datasets. (arXiv:2311.12831v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1">Kaiyuan Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chaoli Wang</a></p>
<p>Due to its conceptual simplicity and generality, compressive neural
representation has emerged as a promising alternative to traditional
compression methods for managing massive volumetric datasets. The current
practice of neural compression utilizes a single large multilayer perceptron
(MLP) to encode the global volume, incurring slow training and inference. This
paper presents an efficient compressive neural representation (ECNR) solution
for time-varying data compression, utilizing the Laplacian pyramid for adaptive
signal fitting. Following a multiscale structure, we leverage multiple small
MLPs at each scale for fitting local content or residual blocks. By assigning
similar blocks to the same MLP via size uniformization, we enable balanced
parallelization among MLPs to significantly speed up training and inference.
Working in concert with the multiscale structure, we tailor a deep compression
strategy to compact the resulting model. We show the effectiveness of ECNR with
multiple datasets and compare it with state-of-the-art compression methods
(mainly SZ3, TTHRESH, and neurcomp). The results position ECNR as a promising
solution for volumetric data compression.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14333">Cycle Invariant Positional Encoding for Graph Representation Learning. (arXiv:2311.14333v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1">Zuoyu Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1">Tengfei Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1">Liangcai Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1">Zhi Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yusu Wang</a></p>
<p>Cycles are fundamental elements in graph-structured data and have
demonstrated their effectiveness in enhancing graph learning models. To encode
such information into a graph learning framework, prior works often extract a
summary quantity, ranging from the number of cycles to the more sophisticated
persistence diagram summaries. However, more detailed information, such as
which edges are encoded in a cycle, has not yet been used in graph neural
networks. In this paper, we make one step towards addressing this gap, and
propose a structure encoding module, called CycleNet, that encodes cycle
information via edge structure encoding in a permutation invariant manner. To
efficiently encode the space of all cycles, we start with a cycle basis (i.e.,
a minimal set of cycles generating the cycle space) which we compute via the
kernel of the 1-dimensional Hodge Laplacian of the input graph. To guarantee
the encoding is invariant w.r.t. the choice of cycle basis, we encode the cycle
information via the orthogonal projector of the cycle basis, which is inspired
by BasisNet proposed by Lim et al. We also develop a more efficient variant
which however requires that the input graph has a unique shortest cycle basis.
To demonstrate the effectiveness of the proposed module, we provide some
theoretical understandings of its expressive power. Moreover, we show via a
range of experiments that networks enhanced by our CycleNet module perform
better in various benchmarks compared to several existing SOTA models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14743">A Baseline Analysis of Reward Models&#x27; Ability To Accurately Analyze Foundation Models Under Distribution Shift. (arXiv:2311.14743v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pikus_B/0/1/0/all/0/1">Ben Pikus</a>, <a href="http://arxiv.org/find/cs/1/au:+LeVine_W/0/1/0/all/0/1">Will LeVine</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tony Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hendryx_S/0/1/0/all/0/1">Sean Hendryx</a></p>
<p>Foundation models, specifically Large Language Models (LLM's), have lately
gained wide-spread attention and adoption. Reinforcement Learning with Human
Feedback (RLHF) involves training a reward model to capture desired behaviors,
which is then used to align an LLM. These reward models are additionally used
at inference-time to estimate how well LLM responses adhere to those desired
behaviors. However, there is little work measuring how robust these reward
models are to distribution shifts. In this work, we evaluate how reward model
performance - measured via accuracy and calibration (i.e. alignment between
accuracy and confidence) - is affected by distribution shift. We show novel
calibration patterns and accuracy drops due to OOD prompts and responses, and
that the reward model is more sensitive to shifts in responses than prompts.
Additionally, we adapt an OOD detection technique commonly used in
classification to the reward model setting in order to detect these
distribution shifts in prompts and responses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14948">Effective Backdoor Mitigation Depends on the Pre-training Objective. (arXiv:2311.14948v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Verma_S/0/1/0/all/0/1">Sahil Verma</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhatt_G/0/1/0/all/0/1">Gantavya Bhatt</a>, <a href="http://arxiv.org/find/cs/1/au:+Schwarzschild_A/0/1/0/all/0/1">Avi Schwarzschild</a>, <a href="http://arxiv.org/find/cs/1/au:+Singhal_S/0/1/0/all/0/1">Soumye Singhal</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1">Arnav Mohanty Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_C/0/1/0/all/0/1">Chirag Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Dickerson_J/0/1/0/all/0/1">John P Dickerson</a>, <a href="http://arxiv.org/find/cs/1/au:+Bilmes_J/0/1/0/all/0/1">Jeff Bilmes</a></p>
<p>Despite the advanced capabilities of contemporary machine learning (ML)
models, they remain vulnerable to adversarial and backdoor attacks. This
vulnerability is particularly concerning in real-world deployments, where
compromised models may exhibit unpredictable behavior in critical scenarios.
Such risks are heightened by the prevalent practice of collecting massive,
internet-sourced datasets for pre-training multimodal models, as these datasets
may harbor backdoors. Various techniques have been proposed to mitigate the
effects of backdooring in these models such as CleanCLIP which is the current
state-of-the-art approach. In this work, we demonstrate that the efficacy of
CleanCLIP in mitigating backdoors is highly dependent on the particular
objective used during model pre-training. We observe that stronger pre-training
objectives correlate with harder to remove backdoors behaviors. We show this by
training multimodal models on two large datasets consisting of 3 million (CC3M)
and 6 million (CC6M) datapoints, under various pre-training objectives,
followed by poison removal using CleanCLIP. We find that CleanCLIP is
ineffective when stronger pre-training objectives are used, even with extensive
hyperparameter tuning. Our findings underscore critical considerations for ML
practitioners who pre-train models using large-scale web-curated data and are
concerned about potential backdoor threats. Notably, our results suggest that
simpler pre-training objectives are more amenable to effective backdoor
removal. This insight is pivotal for practitioners seeking to balance the
trade-offs between using stronger pre-training objectives and security against
backdoor attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15414">KOPPA: Improving Prompt-based Continual Learning with Key-Query Orthogonal Projection and Prototype-based One-Versus-All. (arXiv:2311.15414v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1">Quyen Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_L/0/1/0/all/0/1">Lam Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Than_K/0/1/0/all/0/1">Khoat Than</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1">Toan Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1">Dinh Phung</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1">Trung Le</a></p>
<p>Drawing inspiration from prompt tuning techniques applied to Large Language
Models, recent methods based on pre-trained ViT networks have achieved
remarkable results in the field of Continual Learning. Specifically, these
approaches propose to maintain a set of prompts and allocate a subset of them
to learn each task using a key-query matching strategy. However, they may
encounter limitations when lacking control over the correlations between old
task queries and keys of future tasks, the shift of features in the latent
space, and the relative separation of latent vectors learned in independent
tasks. In this work, we introduce a novel key-query learning strategy based on
orthogonal projection, inspired by model-agnostic meta-learning, to enhance
prompt matching efficiency and address the challenge of shifting features.
Furthermore, we introduce a One-Versus-All (OVA) prototype-based component that
enhances the classification head distinction. Experimental results on benchmark
datasets demonstrate that our method empowers the model to achieve results
surpassing those of current state-of-the-art approaches by a large margin of up
to 20%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15936">Towards Responsible Governance of Biological Design Tools. (arXiv:2311.15936v3 [cs.CY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Moulange_R/0/1/0/all/0/1">Richard Moulange</a>, <a href="http://arxiv.org/find/cs/1/au:+Langenkamp_M/0/1/0/all/0/1">Max Langenkamp</a>, <a href="http://arxiv.org/find/cs/1/au:+Alexanian_T/0/1/0/all/0/1">Tessa Alexanian</a>, <a href="http://arxiv.org/find/cs/1/au:+Curtis_S/0/1/0/all/0/1">Samuel Curtis</a>, <a href="http://arxiv.org/find/cs/1/au:+Livingston_M/0/1/0/all/0/1">Morgan Livingston</a></p>
<p>Recent advancements in generative machine learning have enabled rapid
progress in biological design tools (BDTs) such as protein structure and
sequence prediction models. The unprecedented predictive accuracy and novel
design capabilities of BDTs present new and significant dual-use risks. For
example, their predictive accuracy allows biological agents, whether vaccines
or pathogens, to be developed more quickly, while the design capabilities could
be used to discover drugs or evade DNA screening techniques. Similar to other
dual-use AI systems, BDTs present a wicked problem: how can regulators uphold
public safety without stifling innovation? We highlight how current regulatory
proposals that are primarily tailored toward large language models may be less
effective for BDTs, which require fewer computational resources to train and
are often developed in an open-source manner. We propose a range of measures to
mitigate the risk that BDTs are misused, across the areas of responsible
development, risk assessment, transparency, access management, cybersecurity,
and investing in resilience. Implementing such measures will require close
coordination between developers and governments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16102">Diffusion-TTA: Test-time Adaptation of Discriminative Models via Generative Feedback. (arXiv:2311.16102v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Prabhudesai_M/0/1/0/all/0/1">Mihir Prabhudesai</a>, <a href="http://arxiv.org/find/cs/1/au:+Ke_T/0/1/0/all/0/1">Tsung-Wei Ke</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1">Alexander C. Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1">Deepak Pathak</a>, <a href="http://arxiv.org/find/cs/1/au:+Fragkiadaki_K/0/1/0/all/0/1">Katerina Fragkiadaki</a></p>
<p>The advancements in generative modeling, particularly the advent of diffusion
models, have sparked a fundamental question: how can these models be
effectively used for discriminative tasks? In this work, we find that
generative models can be great test-time adapters for discriminative models.
Our method, Diffusion-TTA, adapts pre-trained discriminative models such as
image classifiers, segmenters and depth predictors, to each unlabelled example
in the test set using generative feedback from a diffusion model. We achieve
this by modulating the conditioning of the diffusion model using the output of
the discriminative model. We then maximize the image likelihood objective by
backpropagating the gradients to discriminative model's parameters. We show
Diffusion-TTA significantly enhances the accuracy of various large-scale
pre-trained discriminative models, such as, ImageNet classifiers, CLIP models,
image pixel labellers and image depth predictors. Diffusion-TTA outperforms
existing test-time adaptation methods, including TTT-MAE and TENT, and
particularly shines in online adaptation setups, where the discriminative model
is continually adapted to each example in the test set. We provide access to
code, results, and visualizations on our website:
https://diffusion-tta.github.io/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16361">Making Self-supervised Learning Robust to Spurious Correlation via Learning-speed Aware Sampling. (arXiv:2311.16361v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1">Weicheng Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Sheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fernandez_Granda_C/0/1/0/all/0/1">Carlos Fernandez-Granda</a>, <a href="http://arxiv.org/find/cs/1/au:+Razavian_N/0/1/0/all/0/1">Narges Razavian</a></p>
<p>Self-supervised learning (SSL) has emerged as a powerful technique for
learning rich representations from unlabeled data. The data representations are
able to capture many underlying attributes of data, and be useful in downstream
prediction tasks. In real-world settings, spurious correlations between some
attributes (e.g. race, gender and age) and labels for downstream tasks often
exist, e.g. cancer is usually more prevalent among elderly patients. In this
paper, we investigate SSL in the presence of spurious correlations and show
that the SSL training loss can be minimized by capturing only a subset of the
conspicuous features relevant to those sensitive attributes, despite the
presence of other important predictive features for the downstream tasks. To
address this issue, we investigate the learning dynamics of SSL and observe
that the learning is slower for samples that conflict with such correlations
(e.g. elder patients without cancer). Motivated by these findings, we propose a
learning-speed aware SSL (LA-SSL) approach, in which we sample each training
data with a probability that is inversely related to its learning speed. We
evaluate LA-SSL on three datasets that exhibit spurious correlations between
different attributes, demonstrating that it improves the robustness of
pretrained representations on downstream classification tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16487">On the Robustness of Decision-Focused Learning. (arXiv:2311.16487v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Farhat_Y/0/1/0/all/0/1">Yehya Farhat</a></p>
<p>Decision-Focused Learning (DFL) is an emerging learning paradigm that tackles
the task of training a machine learning (ML) model to predict missing
parameters of an incomplete optimization problem, where the missing parameters
are predicted. DFL trains an ML model in an end-to-end system, by integrating
the prediction and optimization tasks, providing better alignment of the
training and testing objectives. DFL has shown a lot of promise and holds the
capacity to revolutionize decision-making in many real-world applications.
However, very little is known about the performance of these models under
adversarial attacks. We adopt ten unique DFL methods and benchmark their
performance under two distinctly focused attacks adapted towards the
Predict-then-Optimize problem setting. Our study proposes the hypothesis that
the robustness of a model is highly correlated with its ability to find
predictions that lead to optimal decisions without deviating from the
ground-truth label. Furthermore, we provide insight into how to target the
models that violate this condition and show how these models respond
differently depending on the achieved optimality at the end of their training
cycles.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16605">LasTGL: An Industrial Framework for Large-Scale Temporal Graph Learning. (arXiv:2311.16605v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jintang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Dan_J/0/1/0/all/0/1">Jiawang Dan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1">Ruofan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jing Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1">Sheng Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yunfei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Baokun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1">Changhua Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Weiqiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yuchang Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Liang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Zibin Zheng</a></p>
<p>Over the past few years, graph neural networks (GNNs) have become powerful
and practical tools for learning on (static) graph-structure data. However,
many real-world applications, such as social networks and e-commerce, involve
temporal graphs where nodes and edges are dynamically evolving. Temporal graph
neural networks (TGNNs) have progressively emerged as an extension of GNNs to
address time-evolving graphs and have gradually become a trending research
topic in both academics and industry. Advancing research and application in
such an emerging field necessitates the development of new tools to compose
TGNN models and unify their different schemes for dealing with temporal graphs.
In this work, we introduce LasTGL, an industrial framework that integrates
unified and extensible implementations of common temporal graph learning
algorithms for various advanced tasks. The purpose of LasTGL is to provide the
essential building blocks for solving temporal graph learning tasks, focusing
on the guiding principles of user-friendliness and quick prototyping on which
PyTorch is based. In particular, LasTGL provides comprehensive temporal graph
datasets, TGNN models and utilities along with well-documented tutorials,
making it suitable for both absolute beginners and expert deep learning
practitioners alike.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17134">GlycoNMR: Dataset and benchmarks for NMR chemical shift prediction of carbohydrates with graph neural networks. (arXiv:2311.17134v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zizhang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Badman_R/0/1/0/all/0/1">Ryan Paul Badman</a>, <a href="http://arxiv.org/find/cs/1/au:+Foley_L/0/1/0/all/0/1">Lachele Foley</a>, <a href="http://arxiv.org/find/cs/1/au:+Woods_R/0/1/0/all/0/1">Robert Woods</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_P/0/1/0/all/0/1">Pengyu Hong</a></p>
<p>Molecular representation learning (MRL) is a powerful tool for bridging the
gap between machine learning and chemical sciences, as it converts molecules
into numerical representations while preserving their chemical features. These
encoded representations serve as a foundation for various downstream
biochemical studies, including property prediction and drug design. MRL has had
great success with proteins and general biomolecule datasets. Yet, in the
growing sub-field of glycoscience (the study of carbohydrates, where longer
carbohydrates are also called glycans), MRL methods have been barely explored.
This under-exploration can be primarily attributed to the limited availability
of comprehensive and well-curated carbohydrate-specific datasets and a lack of
Machine learning (ML) pipelines specifically tailored to meet the unique
problems presented by carbohydrate data. Since interpreting and annotating
carbohydrate-specific data is generally more complicated than protein data,
domain experts are usually required to get involved. The existing MRL methods,
predominately optimized for proteins and small biomolecules, also cannot be
directly used in carbohydrate applications without special modifications. To
address this challenge, accelerate progress in glycoscience, and enrich the
data resources of the MRL community, we introduce GlycoNMR. GlycoNMR contains
two laboriously curated datasets with 2,609 carbohydrate structures and 211,543
annotated nuclear magnetic resonance (NMR) chemical shifts for precise
atomic-level prediction. We tailored carbohydrate-specific features and adapted
existing MRL models to tackle this problem effectively. For illustration, we
benchmark four modified MRL models on our new datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17400">Improving the Robustness of Transformer-based Large Language Models with Dynamic Attention. (arXiv:2311.17400v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1">Lujia Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pu_Y/0/1/0/all/0/1">Yuwen Pu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1">Shouling Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Changjiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xuhong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_C/0/1/0/all/0/1">Chunpeng Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Ting Wang</a></p>
<p>Transformer-based models, such as BERT and GPT, have been widely adopted in
natural language processing (NLP) due to their exceptional performance.
However, recent studies show their vulnerability to textual adversarial attacks
where the model's output can be misled by intentionally manipulating the text
inputs. Despite various methods that have been proposed to enhance the model's
robustness and mitigate this vulnerability, many require heavy consumption
resources (e.g., adversarial training) or only provide limited protection
(e.g., defensive dropout). In this paper, we propose a novel method called
dynamic attention, tailored for the transformer architecture, to enhance the
inherent robustness of the model itself against various adversarial attacks.
Our method requires no downstream task knowledge and does not incur additional
costs. The proposed dynamic attention consists of two modules: (I) attention
rectification, which masks or weakens the attention value of the chosen tokens,
and (ii) dynamic modeling, which dynamically builds the set of candidate
tokens. Extensive experiments demonstrate that dynamic attention significantly
mitigates the impact of adversarial attacks, improving up to 33\% better
performance than previous methods against widely-used adversarial attacks. The
model-level design of dynamic attention enables it to be easily combined with
other defense methods (e.g., adversarial training) to further enhance the
model's robustness. Furthermore, we demonstrate that dynamic attention
preserves the state-of-the-art robustness space of the original model compared
to other dynamic modeling methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17410">GNNFlow: A Distributed Framework for Continuous Temporal GNN Learning on Dynamic Graphs. (arXiv:2311.17410v2 [cs.DC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1">Yuchen Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Sheng_G/0/1/0/all/0/1">Guangming Sheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1">Tianzuo Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Minjie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gan_Q/0/1/0/all/0/1">Quan Gan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1">Chuan Wu</a></p>
<p>Graph Neural Networks (GNNs) play a crucial role in various fields. However,
most existing deep graph learning frameworks assume pre-stored static graphs
and do not support training on graph streams. In contrast, many real-world
graphs are dynamic and contain time domain information. We introduce GNNFlow, a
distributed framework that enables efficient continuous temporal graph
representation learning on dynamic graphs on multi-GPU machines. GNNFlow
introduces an adaptive time-indexed block-based data structure that effectively
balances memory usage with graph update and sampling operation efficiency. It
features a hybrid GPU-CPU graph data placement for rapid GPU-based temporal
neighborhood sampling and kernel optimizations for enhanced sampling processes.
A dynamic GPU cache for node and edge features is developed to maximize cache
hit rates through reuse and restoration strategies. GNNFlow supports
distributed training across multiple machines with static scheduling to ensure
load balance. We implement GNNFlow based on DGL and PyTorch. Our experimental
results show that GNNFlow provides up to 21.1x faster continuous learning than
existing systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17431">Grounding Foundation Models through Federated Transfer Learning: A General Framework. (arXiv:2311.17431v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1">Yan Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_T/0/1/0/all/0/1">Tao Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_H/0/1/0/all/0/1">Hanlin Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1">Lixin Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1">Qiang Yang</a></p>
<p>Foundation Models (FMs) such as GPT-4 encoded with vast knowledge and
powerful emergent abilities have achieved remarkable success in various natural
language processing and computer vision tasks. Grounding FMs by adapting them
to domain-specific tasks or augmenting them with domain-specific knowledge
enables us to exploit the full potential of FMs. However, grounding FMs faces
several challenges, stemming primarily from constrained computing resources,
data privacy, model heterogeneity, and model ownership. Federated Transfer
Learning (FTL), the combination of federated learning and transfer learning,
provides promising solutions to address these challenges. In recent years, the
need for grounding FMs leveraging FTL, coined FTL-FM, has arisen strongly in
both academia and industry. Motivated by the strong growth in FTL-FM research
and the potential impact of FTL-FM on industrial applications, we propose an
FTL-FM framework that formulates problems of grounding FMs in the federated
learning setting, construct a detailed taxonomy based on the FTL-FM framework
to categorize state-of-the-art FTL-FM works, and comprehensively overview
FTL-FM works based on the proposed taxonomy. We also establish correspondences
between FTL-FM and conventional phases of adapting FM so that FM practitioners
can align their research works with FTL-FM. In addition, we overview advanced
efficiency-improving and privacy-preserving techniques because efficiency and
privacy are critical concerns in FTL-FM. Last, we discuss opportunities and
future research directions of FTL-FM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17898">Knowledge Pursuit Prompting for Zero-Shot Multimodal Synthesis. (arXiv:2311.17898v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1">Jinqi Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_K/0/1/0/all/0/1">Kwan Ho Ryan Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Dimos_D/0/1/0/all/0/1">Dimitris Dimos</a>, <a href="http://arxiv.org/find/cs/1/au:+Vidal_R/0/1/0/all/0/1">Ren&#xe9; Vidal</a></p>
<p>Hallucinations and unfaithful synthesis due to inaccurate prompts with
insufficient semantic details are widely observed in multimodal generative
models. A prevalent strategy to align multiple modalities is to fine-tune the
generator with a large number of annotated text-image pairs. However, such a
procedure is labor-consuming and resource-draining. The key question we ask is:
can we enhance the quality and faithfulness of text-driven generative models
beyond extensive text-image pair annotations? To address this question, we
propose Knowledge Pursuit Prompting (KPP), a zero-shot framework that
iteratively incorporates external knowledge to help generators produce reliable
visual content. Instead of training generators to handle generic prompts, KPP
employs a recursive knowledge query process to gather informative external
facts from the knowledge base, instructs a language model to compress the
acquired knowledge for prompt refinement, and utilizes text-driven generators
for visual synthesis. The entire process is zero-shot, without accessing the
architectures and parameters of generative models. We evaluate the framework
across multiple text-driven generative tasks (image, 3D rendering, and video)
on datasets of different domains. We further demonstrate the extensibility and
adaptability of KPP through varying foundation model bases and instructions.
Our results show that KPP is capable of generating faithful and semantically
rich content across diverse visual domains, offering a promising solution to
improve multimodal generative models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.04431">Adaptive proximal algorithms for convex optimization under local Lipschitz continuity of the gradient. (arXiv:2301.04431v3 [math.OC] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Latafat_P/0/1/0/all/0/1">Puya Latafat</a>, <a href="http://arxiv.org/find/math/1/au:+Themelis_A/0/1/0/all/0/1">Andreas Themelis</a>, <a href="http://arxiv.org/find/math/1/au:+Stella_L/0/1/0/all/0/1">Lorenzo Stella</a>, <a href="http://arxiv.org/find/math/1/au:+Patrinos_P/0/1/0/all/0/1">Panagiotis Patrinos</a></p>
<p>Backtracking linesearch is the de facto approach for minimizing continuously
differentiable functions with locally Lipschitz gradient. In recent years, it
has been shown that in the convex setting it is possible to avoid linesearch
altogether, and to allow the stepsize to adapt based on a local smoothness
estimate without any backtracks or evaluations of the function value. In this
work we propose an adaptive proximal gradient method, adaPG, that uses novel
estimates of the local smoothness modulus which leads to less conservative
stepsize updates and that can additionally cope with nonsmooth terms. This idea
is extended to the primal-dual setting where an adaptive three-term primal-dual
algorithm, adaPD, is proposed which can be viewed as an extension of the PDHG
method. Moreover, in this setting the ``essentially'' fully adaptive variant
adaPD$^+$ is proposed that avoids evaluating the linear operator norm by
invoking a backtracking procedure, that, remarkably, does not require extra
gradient evaluations. Numerical simulations demonstrate the effectiveness of
the proposed algorithms compared to the state of the art.
</p>
</p>
</div>

    </div>
    </body>
    