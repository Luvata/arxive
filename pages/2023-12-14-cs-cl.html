<!DOCTYPE html>
<html>
<head>
<title>2023-12-14-cs-cl</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2312.06668">Evaluating Self-supervised Speech Models on a Taiwanese Hokkien Corpus. (arXiv:2312.06668v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chou_Y/0/1/0/all/0/1">Yi-Hui Chou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1">Kalvin Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1">Meng-Ju Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ou_W/0/1/0/all/0/1">Winston Ou</a>, <a href="http://arxiv.org/find/cs/1/au:+Bi_A/0/1/0/all/0/1">Alice Wen-Hsin Bi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Carol Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Bryan Y. Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pai_R/0/1/0/all/0/1">Rong-Wei Pai</a>, <a href="http://arxiv.org/find/cs/1/au:+Yeh_P/0/1/0/all/0/1">Po-Yen Yeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Chiang_J/0/1/0/all/0/1">Jo-Peng Chiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Phoann_I/0/1/0/all/0/1">Iu-Tshian Phoann</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_W/0/1/0/all/0/1">Winnie Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1">Chenxuan Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1">Noel Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1">Jiatong Shi</a></p>
<p>Taiwanese Hokkien is declining in use and status due to a language shift
towards Mandarin in Taiwan. This is partly why it is a low resource language in
NLP and speech research today. To ensure that the state of the art in speech
processing does not leave Taiwanese Hokkien behind, we contribute a 1.5-hour
dataset of Taiwanese Hokkien to ML-SUPERB's hidden set. Evaluating ML-SUPERB's
suite of self-supervised learning (SSL) speech representations on our dataset,
we find that model size does not consistently determine performance. In fact,
certain smaller models outperform larger ones. Furthermore, linguistic
alignment between pretraining data and the target language plays a crucial
role.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06674">Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations. (arXiv:2312.06674v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Inan_H/0/1/0/all/0/1">Hakan Inan</a>, <a href="http://arxiv.org/find/cs/1/au:+Upasani_K/0/1/0/all/0/1">Kartikeya Upasani</a>, <a href="http://arxiv.org/find/cs/1/au:+Chi_J/0/1/0/all/0/1">Jianfeng Chi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rungta_R/0/1/0/all/0/1">Rashi Rungta</a>, <a href="http://arxiv.org/find/cs/1/au:+Iyer_K/0/1/0/all/0/1">Krithika Iyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1">Yuning Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tontchev_M/0/1/0/all/0/1">Michael Tontchev</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1">Qing Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fuller_B/0/1/0/all/0/1">Brian Fuller</a>, <a href="http://arxiv.org/find/cs/1/au:+Testuggine_D/0/1/0/all/0/1">Davide Testuggine</a>, <a href="http://arxiv.org/find/cs/1/au:+Khabsa_M/0/1/0/all/0/1">Madian Khabsa</a></p>
<p>We introduce Llama Guard, an LLM-based input-output safeguard model geared
towards Human-AI conversation use cases. Our model incorporates a safety risk
taxonomy, a valuable tool for categorizing a specific set of safety risks found
in LLM prompts (i.e., prompt classification). This taxonomy is also
instrumental in classifying the responses generated by LLMs to these prompts, a
process we refer to as response classification. For the purpose of both prompt
and response classification, we have meticulously gathered a dataset of high
quality. Llama Guard, a Llama2-7b model that is instruction-tuned on our
collected dataset, albeit low in volume, demonstrates strong performance on
existing benchmarks such as the OpenAI Moderation Evaluation dataset and
ToxicChat, where its performance matches or exceeds that of currently available
content moderation tools. Llama Guard functions as a language model, carrying
out multi-class classification and generating binary decision scores.
Furthermore, the instruction fine-tuning of Llama Guard allows for the
customization of tasks and the adaptation of output formats. This feature
enhances the model's capabilities, such as enabling the adjustment of taxonomy
categories to align with specific use cases, and facilitating zero-shot or
few-shot prompting with diverse taxonomies at the input. We are making Llama
Guard model weights available and we encourage researchers to further develop
and adapt them to meet the evolving needs of the community for AI safety.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06677">Intelligent Virtual Assistants with LLM-based Process Automation. (arXiv:2312.06677v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1">Yanchu Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1">Zhixuan Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shiyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ni_F/0/1/0/all/0/1">Feiyue Ni</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1">Ruihua Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Longfei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jinjie Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_C/0/1/0/all/0/1">Chenyi Zhuang</a></p>
<p>While intelligent virtual assistants like Siri, Alexa, and Google Assistant
have become ubiquitous in modern life, they still face limitations in their
ability to follow multi-step instructions and accomplish complex goals
articulated in natural language. However, recent breakthroughs in large
language models (LLMs) show promise for overcoming existing barriers by
enhancing natural language processing and reasoning capabilities. Though
promising, applying LLMs to create more advanced virtual assistants still faces
challenges like ensuring robust performance and handling variability in
real-world user commands. This paper proposes a novel LLM-based virtual
assistant that can automatically perform multi-step operations within mobile
apps based on high-level user requests. The system represents an advance in
assistants by providing an end-to-end solution for parsing instructions,
reasoning about goals, and executing actions. LLM-based Process Automation
(LLMPA) has modules for decomposing instructions, generating descriptions,
detecting interface elements, predicting next actions, and error checking.
Experiments demonstrate the system completing complex mobile operation tasks in
Alipay based on natural language instructions. This showcases how large
language models can enable automated assistants to accomplish real-world tasks.
The main contributions are the novel LLMPA architecture optimized for app
process automation, the methodology for applying LLMs to mobile apps, and
demonstrations of multi-step task completion in a real-world environment.
Notably, this work represents the first real-world deployment and extensive
evaluation of a large language model-based virtual assistant in a widely used
mobile application with an enormous user base numbering in the hundreds of
millions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06681">Steering Llama 2 via Contrastive Activation Addition. (arXiv:2312.06681v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rimsky_N/0/1/0/all/0/1">Nina Rimsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Gabrieli_N/0/1/0/all/0/1">Nick Gabrieli</a>, <a href="http://arxiv.org/find/cs/1/au:+Schulz_J/0/1/0/all/0/1">Julian Schulz</a>, <a href="http://arxiv.org/find/cs/1/au:+Tong_M/0/1/0/all/0/1">Meg Tong</a>, <a href="http://arxiv.org/find/cs/1/au:+Hubinger_E/0/1/0/all/0/1">Evan Hubinger</a>, <a href="http://arxiv.org/find/cs/1/au:+Turner_A/0/1/0/all/0/1">Alexander Matt Turner</a></p>
<p>We introduce Contrastive Activation Addition (CAA), an innovative method for
steering language models by modifying activations during their forward passes.
CAA computes ``steering vectors'' by averaging the difference in residual
stream activations between pairs of positive and negative examples of a
particular behavior such as factual versus hallucinatory responses. During
inference, these steering vectors are added at all token positions after the
user's prompt with either a positive or negative coefficient, allowing precise
control over the degree of the targeted behavior. We evaluate CAA's
effectiveness on Llama 2 Chat using both multiple-choice behavioral question
datasets and open-ended generation tasks. We demonstrate that CAA significantly
alters model behavior, outperforms traditional methods like finetuning and
few-shot prompting, and minimally reduces capabilities. Moreover, by employing
various activation space interpretation methods, we gain deeper insights into
CAA's mechanisms. CAA both accurately steers model outputs and also sheds light
on how high-level concepts are represented in Large Language Models (LLMs).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06705">Perceiving University Student&#x27;s Opinions from Google App Reviews. (arXiv:2312.06705v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ranjan_S/0/1/0/all/0/1">Sakshi Ranjan</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1">Subhankar Mishra</a></p>
<p>Google app market captures the school of thought of users from every corner
of the globe via ratings and text reviews, in a multilinguistic arena. The
potential information from the reviews cannot be extracted manually, due to its
exponential growth. So, Sentiment analysis, by machine learning and deep
learning algorithms employing NLP, explicitly uncovers and interprets the
emotions. This study performs the sentiment classification of the app reviews
and identifies the university student's behavior towards the app market via
exploratory analysis. We applied machine learning algorithms using the TP, TF,
and TF IDF text representation scheme and evaluated its performance on Bagging,
an ensemble learning method. We used word embedding, Glove, on the deep
learning paradigms. Our model was trained on Google app reviews and tested on
Student's App Reviews(SAR). The various combinations of these algorithms were
compared amongst each other using F score and accuracy and inferences were
highlighted graphically. SVM, amongst other classifiers, gave fruitful
accuracy(93.41%), F score(89%) on bigram and TF IDF scheme. Bagging enhanced
the performance of LR and NB with accuracy of 87.88% and 86.69% and F score of
86% and 78% respectively. Overall, LSTM on Glove embedding recorded the highest
accuracy(95.2%) and F score(88%).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06722">EgoPlan-Bench: Benchmarking Egocentric Embodied Planning with Multimodal Large Language Models. (arXiv:2312.06722v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1">Yuying Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1">Yixiao Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1">Mingyu Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bohao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Rui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">Ruifeng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1">Ying Shan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xihui Liu</a></p>
<p>Multimodal Large Language Models (MLLMs), building upon the powerful Large
Language Models (LLMs) with exceptional reasoning and generalization
capability, have opened up new avenues for embodied task planning. MLLMs excel
in their ability to integrate diverse environmental inputs, such as real-time
task progress, visual observations, and open-form language instructions, which
are crucial for executable task planning. In this work, we introduce a
benchmark with human annotations, EgoPlan-Bench, to quantitatively investigate
the potential of MLLMs as embodied task planners in real-world scenarios. Our
benchmark is distinguished by realistic tasks derived from real-world videos, a
diverse set of actions involving interactions with hundreds of different
objects, and complex visual observations from varied environments. We evaluate
various open-source MLLMs, revealing that these models have not yet evolved
into embodied planning generalists (even GPT-4V). We further construct an
instruction-tuning dataset EgoPlan-IT from videos of human-object interactions,
to facilitate the learning of high-level task planning in intricate real-world
situations. The experiment results demonstrate that the model tuned on
EgoPlan-IT not only significantly improves performance on our benchmark, but
also effectively acts as embodied planner in simulations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06742">Honeybee: Locality-enhanced Projector for Multimodal LLM. (arXiv:2312.06742v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cha_J/0/1/0/all/0/1">Junbum Cha</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1">Wooyoung Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mun_J/0/1/0/all/0/1">Jonghwan Mun</a>, <a href="http://arxiv.org/find/cs/1/au:+Roh_B/0/1/0/all/0/1">Byungseok Roh</a></p>
<p>In Multimodal Large Language Models (MLLMs), a visual projector plays a
crucial role in bridging pre-trained vision encoders with LLMs, enabling
profound visual understanding while harnessing the LLMs' robust capabilities.
Despite the importance of the visual projector, it has been relatively less
explored. In this study, we first identify two essential projector properties:
(i) flexibility in managing the number of visual tokens, crucial for MLLMs'
overall efficiency, and (ii) preservation of local context from visual
features, vital for spatial understanding. Based on these findings, we propose
a novel projector design that is both flexible and locality-enhanced,
effectively satisfying the two desirable properties. Additionally, we present
comprehensive strategies to effectively utilize multiple and multifaceted
instruction datasets. Through extensive experiments, we examine the impact of
individual design choices. Finally, our proposed MLLM, Honeybee, remarkably
outperforms previous state-of-the-art methods across various benchmarks,
including MME, MMBench, SEED-Bench, and LLaVA-Bench, achieving significantly
higher efficiency. Code and models are available at
https://github.com/kakaobrain/honeybee.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06798">Building Trustworthy NeuroSymbolic AI Systems: Consistency, Reliability, Explainability, and Safety. (arXiv:2312.06798v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gaur_M/0/1/0/all/0/1">Manas Gaur</a>, <a href="http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1">Amit Sheth</a></p>
<p>Explainability and Safety engender Trust. These require a model to exhibit
consistency and reliability. To achieve these, it is necessary to use and
analyze data and knowledge with statistical and symbolic AI methods relevant to
the AI application - neither alone will do. Consequently, we argue and seek to
demonstrate that the NeuroSymbolic AI approach is better suited for making AI a
trusted AI system. We present the CREST framework that shows how Consistency,
Reliability, user-level Explainability, and Safety are built on NeuroSymbolic
methods that use data and knowledge to support requirements for critical
applications such as health and well-being. This article focuses on Large
Language Models (LLMs) as the chosen AI system within the CREST framework. LLMs
have garnered substantial attention from researchers due to their versatility
in handling a broad array of natural language processing (NLP) scenarios. For
example, ChatGPT and Google's MedPaLM have emerged as highly promising
platforms for providing information in general and health-related queries,
respectively. Nevertheless, these models remain black boxes despite
incorporating human feedback and instruction-guided tuning. For instance,
ChatGPT can generate unsafe responses despite instituting safety guardrails.
CREST presents a plausible approach harnessing procedural and graph-based
knowledge within a NeuroSymbolic framework to shed light on the challenges
associated with LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06820">Extracting Self-Consistent Causal Insights from Users Feedback with LLMs and In-context Learning. (arXiv:2312.06820v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abdali_S/0/1/0/all/0/1">Sara Abdali</a>, <a href="http://arxiv.org/find/cs/1/au:+Parikh_A/0/1/0/all/0/1">Anjali Parikh</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1">Steve Lim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kiciman_E/0/1/0/all/0/1">Emre Kiciman</a></p>
<p>Microsoft Windows Feedback Hub is designed to receive customer feedback on a
wide variety of subjects including critical topics such as power and battery.
Feedback is one of the most effective ways to have a grasp of users' experience
with Windows and its ecosystem. However, the sheer volume of feedback received
by Feedback Hub makes it immensely challenging to diagnose the actual cause of
reported issues. To better understand and triage issues, we leverage Double
Machine Learning (DML) to associate users' feedback with telemetry signals. One
of the main challenges we face in the DML pipeline is the necessity of domain
knowledge for model design (e.g., causal graph), which sometimes is either not
available or hard to obtain. In this work, we take advantage of reasoning
capabilities in Large Language Models (LLMs) to generate a prior model that
which to some extent compensates for the lack of domain knowledge and could be
used as a heuristic for measuring feedback informativeness. Our LLM-based
approach is able to extract previously known issues, uncover new bugs, and
identify sequences of events that lead to a bug, while minimizing out-of-domain
outputs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06825">Utilization of Non-verbal Behaviour and Social Gaze in Classroom Human-Robot Interaction Communications. (arXiv:2312.06825v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shaghaghi_S/0/1/0/all/0/1">Sahand Shaghaghi</a>, <a href="http://arxiv.org/find/cs/1/au:+Aliasghari_P/0/1/0/all/0/1">Pourya Aliasghari</a>, <a href="http://arxiv.org/find/cs/1/au:+Tripp_B/0/1/0/all/0/1">Bryan Tripp</a>, <a href="http://arxiv.org/find/cs/1/au:+Dautenhahn_K/0/1/0/all/0/1">Kerstin Dautenhahn</a>, <a href="http://arxiv.org/find/cs/1/au:+Nehaniv_C/0/1/0/all/0/1">Chrystopher Nehaniv</a></p>
<p>This abstract explores classroom Human-Robot Interaction (HRI) scenarios with
an emphasis on the adaptation of human-inspired social gaze models in robot
cognitive architecture to facilitate a more seamless social interaction. First,
we detail the HRI scenarios explored by us in our studies followed by a
description of the social gaze model utilized for our research. We highlight
the advantages of utilizing such an attentional model in classroom HRI
scenarios. We also detail the intended goals of our upcoming study involving
this social gaze model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06855">Multimodal Pretraining of Medical Time Series and Notes. (arXiv:2312.06855v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+King_R/0/1/0/all/0/1">Ryan King</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1">Tianbao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mortazavi_B/0/1/0/all/0/1">Bobak Mortazavi</a></p>
<p>Within the intensive care unit (ICU), a wealth of patient data, including
clinical measurements and clinical notes, is readily available. This data is a
valuable resource for comprehending patient health and informing medical
decisions, but it also contains many challenges in analysis. Deep learning
models show promise in extracting meaningful patterns, but they require
extensive labeled data, a challenge in critical care. To address this, we
propose a novel approach employing self-supervised pretraining, focusing on the
alignment of clinical measurements and notes. Our approach combines contrastive
and masked token prediction tasks during pretraining. Semi-supervised
experiments on the MIMIC-III dataset demonstrate the effectiveness of our
self-supervised pretraining. In downstream tasks, including in-hospital
mortality prediction and phenotyping, our pretrained model outperforms
baselines in settings where only a fraction of the data is labeled, emphasizing
its ability to enhance ICU data analysis. Notably, our method excels in
situations where very few labels are available, as evidenced by an increase in
the AUC-ROC for in-hospital mortality by 0.17 and in AUC-PR for phenotyping by
0.1 when only 1% of labels are accessible. This work advances self-supervised
learning in the healthcare domain, optimizing clinical insights from abundant
yet challenging ICU data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06861">Disentangling Perceptions of Offensiveness: Cultural and Moral Correlates. (arXiv:2312.06861v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Davani_A/0/1/0/all/0/1">Aida Davani</a>, <a href="http://arxiv.org/find/cs/1/au:+Diaz_M/0/1/0/all/0/1">Mark D&#xed;az</a>, <a href="http://arxiv.org/find/cs/1/au:+Baker_D/0/1/0/all/0/1">Dylan Baker</a>, <a href="http://arxiv.org/find/cs/1/au:+Prabhakaran_V/0/1/0/all/0/1">Vinodkumar Prabhakaran</a></p>
<p>Perception of offensiveness is inherently subjective, shaped by the lived
experiences and socio-cultural values of the perceivers. Recent years have seen
substantial efforts to build AI-based tools that can detect offensive language
at scale, as a means to moderate social media platforms, and to ensure safety
of conversational AI technologies such as ChatGPT and Bard. However, existing
approaches treat this task as a technical endeavor, built on top of data
annotated for offensiveness by a global crowd workforce without any attention
to the crowd workers' provenance or the values their perceptions reflect. We
argue that cultural and psychological factors play a vital role in the
cognitive processing of offensiveness, which is critical to consider in this
context. We re-frame the task of determining offensiveness as essentially a
matter of moral judgment -- deciding the boundaries of ethically wrong vs.
right language within an implied set of socio-cultural norms. Through a
large-scale cross-cultural study based on 4309 participants from 21 countries
across 8 cultural regions, we demonstrate substantial cross-cultural
differences in perceptions of offensiveness. More importantly, we find that
individual moral values play a crucial role in shaping these variations: moral
concerns about Care and Purity are significant mediating factors driving
cross-cultural differences. These insights are of crucial importance as we
build AI models for the pluralistic world, where the values they espouse should
aim to respect and account for moral values in diverse geo-cultural contexts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06867">Get an A in Math: Progressive Rectification Prompting. (arXiv:2312.06867v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhenyu Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1">Meng Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1">Chao Shen</a></p>
<p>Chain-of-Thought (CoT) prompting methods have enabled large language models
(LLMs) to generate reasoning paths and solve math word problems (MWPs).
However, they are sensitive to mistakes in the paths, as any mistake can result
in an incorrect answer. We propose a novel method named Progressive
Rectification Prompting (PRP) to improve average accuracy on eight MWP datasets
from 77.3 to 90.5. Given an initial answer from CoT, PRP iterates a
verify-then-rectify process to progressively identify incorrect answers and
rectify the reasoning paths. With the most likely correct answer, the LLM
predicts a masked numerical value in the question; if the prediction does not
match the masked value, the answer is likely incorrect. Then the LLM is
prompted to re-generate the reasoning path hinted with a set of incorrect
answers to prevent itself from repeating previous mistakes. PRP achieves the
best performance compared against the CoT methods. Our implementation is made
publicly available at https://wzy6642.github.io/prp.github.io/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06874">Dozerformer: Sequence Adaptive Sparse Transformer for Multivariate Time Series Forecasting. (arXiv:2312.06874v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yifan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1">Rui Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dascalu_S/0/1/0/all/0/1">Sergiu M. Dascalu</a>, <a href="http://arxiv.org/find/cs/1/au:+Harris_F/0/1/0/all/0/1">Frederick C. Harris Jr</a></p>
<p>Transformers have achieved remarkable performance in multivariate time
series(MTS) forecasting due to their capability to capture long-term
dependencies. However, the canonical attention mechanism has two key
limitations: (1) its quadratic time complexity limits the sequence length, and
(2) it generates future values from the entire historical sequence. To address
this, we propose a Dozer Attention mechanism consisting of three sparse
components: (1) Local, each query exclusively attends to keys within a
localized window of neighboring time steps. (2) Stride, enables each query to
attend to keys at predefined intervals. (3) Vary, allows queries to selectively
attend to keys from a subset of the historical sequence. Notably, the size of
this subset dynamically expands as forecasting horizons extend. Those three
components are designed to capture essential attributes of MTS data, including
locality, seasonality, and global temporal dependencies. Additionally, we
present the Dozerformer Framework, incorporating the Dozer Attention mechanism
for the MTS forecasting task. We evaluated the proposed Dozerformer framework
with recent state-of-the-art methods on nine benchmark datasets and confirmed
its superior performance. The code will be released after the manuscript is
accepted.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06881">DYAD: A Descriptive Yet Abjuring Density efficient approximation to linear neural network layers. (arXiv:2312.06881v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chandy_S/0/1/0/all/0/1">Sarin Chandy</a>, <a href="http://arxiv.org/find/cs/1/au:+Gangal_V/0/1/0/all/0/1">Varun Gangal</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Maggiotti_G/0/1/0/all/0/1">Gabriel Maggiotti</a></p>
<p>We devise, implement and performance-asses DYAD, a layer which can serve as a
faster and more memory-efficient approximate replacement for linear layers,
(nn.Linear() in Pytorch). These layers appear in common subcomponents, such as
in the ff module of Transformers. DYAD is based on a bespoke near-sparse matrix
structure which approximates the dense "weight" matrix W that matrix-multiplies
the input in the typical realization of such a layer, a.k.a DENSE. Our
alternative near-sparse matrix structure is decomposable to a sum of 2 matrices
permutable to a block-sparse counterpart. These can be represented as 3D
tensors, which in unison allow a faster execution of matrix multiplication with
the mini-batched input matrix X compared to DENSE (O(rows(W ) x cols(W )) --&gt;
O( rows(W ) x cols(W ) # of blocks )). As the crux of our experiments, we
pretrain both DYAD and DENSE variants of 2 sizes of the OPT arch and 1 size of
the Pythia arch, including at different token scales of the babyLM benchmark.
We find DYAD to be competitive (&gt;= 90%) of DENSE performance on zero-shot (e.g.
BLIMP), few-shot (OPENLM) and finetuning (GLUE) benchmarks, while being &gt;=7-15%
faster to train on-GPU even at 125m scale, besides surfacing larger speedups at
increasing scale and model width.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06924">Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an In-Context Attack. (arXiv:2312.06924v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1">Yu Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yufei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1">Wen Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Cong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1">Yue Dong</a></p>
<p>Recent developments in balancing the usefulness and safety of Large Language
Models (LLMs) have raised a critical question: Are mainstream NLP tasks
adequately aligned with safety consideration? Our study, focusing on
safety-sensitive documents obtained through adversarial attacks, reveals
significant disparities in the safety alignment of various NLP tasks. For
instance, LLMs can effectively summarize malicious long documents but often
refuse to translate them. This discrepancy highlights a previously unidentified
vulnerability: attacks exploiting tasks with weaker safety alignment, like
summarization, can potentially compromise the integraty of tasks traditionally
deemed more robust, such as translation and question-answering (QA). Moreover,
the concurrent use of multiple NLP tasks with lesser safety alignment increases
the risk of LLMs inadvertently processing harmful content. We demonstrate these
vulnerabilities in various safety-aligned LLMs, particularly Llama2 models and
GPT-4, indicating an urgent need for strengthening safety alignments across a
broad spectrum of NLP tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06926">Content-Localization based Neural Machine Translation for Informal Dialectal Arabic: Spanish/French to Levantine/Gulf Arabic. (arXiv:2312.06926v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alzamzami_F/0/1/0/all/0/1">Fatimah Alzamzami</a>, <a href="http://arxiv.org/find/cs/1/au:+Saddik_A/0/1/0/all/0/1">Abdulmotaleb El Saddik</a></p>
<p>Resources in high-resource languages have not been efficiently exploited in
low-resource languages to solve language-dependent research problems. Spanish
and French are considered high resource languages in which an adequate level of
data resources for informal online social behavior modeling, is observed.
However, a machine translation system to access those data resources and
transfer their context and tone to a low-resource language like dialectal
Arabic, does not exist. In response, we propose a framework that localizes
contents of high-resource languages to a low-resource language/dialects by
utilizing AI power. To the best of our knowledge, we are the first work to
provide a parallel translation dataset from/to informal Spanish and French
to/from informal Arabic dialects. Using this, we aim to enrich the
under-resource-status dialectal Arabic and fast-track the research of diverse
online social behaviors within and across smart cities in different
geo-regions. The experimental results have illustrated the capability of our
proposed solution in exploiting the resources between high and low resource
languages and dialects. Not only this, but it has also been proven that
ignoring dialects within the same language could lead to misleading analysis of
online social behavior.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06950">READ-PVLA: Recurrent Adapter with Partial Video-Language Alignment for Parameter-Efficient Transfer Learning in Low-Resource Video-Language Modeling. (arXiv:2312.06950v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Thong Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xiaobao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1">Xinshuai Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_K/0/1/0/all/0/1">Khoi Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1">Zhiyuan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1">Cong-Duy Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ng_S/0/1/0/all/0/1">See-Kiong Ng</a>, <a href="http://arxiv.org/find/cs/1/au:+Tuan_L/0/1/0/all/0/1">Luu Anh Tuan</a></p>
<p>Fully fine-tuning pretrained large-scale transformer models has become a
popular paradigm for video-language modeling tasks, such as temporal language
grounding and video-language summarization. With a growing number of tasks and
limited training data, such full fine-tuning approach leads to costly model
storage and unstable training. To overcome these shortcomings, we introduce
lightweight adapters to the pre-trained model and only update them at
fine-tuning time. However, existing adapters fail to capture intrinsic temporal
relations among video frames or textual words. Moreover, they neglect the
preservation of critical task-related information that flows from the raw
video-language input into the adapter's low-dimensional space. To address these
issues, we first propose a novel REcurrent ADapter (READ) that employs
recurrent computation to enable temporal modeling capability. Second, we
propose Partial Video-Language Alignment (PVLA) objective via the use of
partial optimal transport to maintain task-related information flowing into our
READ modules. We validate our READ-PVLA framework through extensive experiments
where READ-PVLA significantly outperforms all existing fine-tuning strategies
on multiple low-resource temporal language grounding and video-language
summarization benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06974">SM70: A Large Language Model for Medical Devices. (arXiv:2312.06974v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bhatti_A/0/1/0/all/0/1">Anubhav Bhatti</a>, <a href="http://arxiv.org/find/cs/1/au:+Parmar_S/0/1/0/all/0/1">Surajsinh Parmar</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">San Lee</a></p>
<p>We are introducing SM70, a 70 billion-parameter Large Language Model that is
specifically designed for SpassMed's medical devices under the brand name
'JEE1' (pronounced as G1 and means 'Life'). This large language model provides
more accurate and safe responses to medical-domain questions. To fine-tune
SM70, we used around 800K data entries from the publicly available dataset
MedAlpaca. The Llama2 70B open-sourced model served as the foundation for SM70,
and we employed the QLoRA technique for fine-tuning. The evaluation is
conducted across three benchmark datasets - MEDQA - USMLE, PUBMEDQA, and USMLE
- each representing a unique aspect of medical knowledge and reasoning. The
performance of SM70 is contrasted with other notable LLMs, including Llama2
70B, Clinical Camel 70 (CC70), GPT 3.5, GPT 4, and Med-Palm, to provide a
comparative understanding of its capabilities within the medical domain. Our
results indicate that SM70 outperforms several established models in these
datasets, showcasing its proficiency in handling a range of medical queries,
from fact-based questions derived from PubMed abstracts to complex clinical
decision-making scenarios. The robust performance of SM70, particularly in the
USMLE and PUBMEDQA datasets, suggests its potential as an effective tool in
clinical decision support and medical information retrieval. Despite its
promising results, the paper also acknowledges the areas where SM70 lags behind
the most advanced model, GPT 4, thereby highlighting the need for further
development, especially in tasks demanding extensive medical knowledge and
intricate reasoning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07000">Alignment for Honesty. (arXiv:2312.07000v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yuqing Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chern_E/0/1/0/all/0/1">Ethan Chern</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1">Xipeng Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1">Graham Neubig</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1">Pengfei Liu</a></p>
<p>Recent research has made significant strides in applying alignment techniques
to enhance the helpfulness and harmlessness of large language models (LLMs) in
accordance with human intentions. In this paper, we argue for the importance of
alignment for honesty, ensuring that LLMs proactively refuse to answer
questions when they lack knowledge, while still not being overly conservative.
However, a pivotal aspect of alignment for honesty involves discerning the
limits of an LLM's knowledge, which is far from straightforward. This challenge
demands comprehensive solutions in terms of metric development, benchmark
creation, and training methodologies. In this paper, we address these
challenges by first establishing a precise problem definition and defining
``honesty'' inspired by the Analects of Confucius. This serves as a cornerstone
for developing metrics that effectively measure an LLM's honesty by quantifying
its progress post-alignment. Furthermore, we introduce a flexible training
framework which is further instantiated by several efficient fine-tuning
techniques that emphasize honesty without sacrificing performance on other
tasks. Our extensive experiments reveal that these aligned models show a marked
increase in honesty, as indicated by our proposed metrics. We open-source a
wealth of resources to facilitate future research at
https://github.com/GAIR-NLP/alignment-for-honesty, including honesty-aligned
models, training and evaluation datasets for honesty alignment, concept
glossary, as well as all relevant source code.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07028">Dynamic Corrective Self-Distillation for Better Fine-Tuning of Pretrained Models. (arXiv:2312.07028v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Amara_I/0/1/0/all/0/1">Ibtihel Amara</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_V/0/1/0/all/0/1">Vinija Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1">Aman Chadha</a></p>
<p>We tackle the challenging issue of aggressive fine-tuning encountered during
the process of transfer learning of pre-trained language models (PLMs) with
limited labeled downstream data. This problem primarily results in a decline in
performance on the subsequent task. Inspired by the adaptive boosting method in
traditional machine learning, we present an effective dynamic corrective
self-distillation (DCS) approach to improve the fine-tuning of the PLMs. Our
technique involves performing a self-distillation mechanism where, at each
iteration, the student model actively adapts and corrects itself by dynamically
adjusting the weights assigned to individual data points. This iterative
self-correcting process significantly enhances the overall fine-tuning
capability of PLMs, leading to improved performance and robustness. We
conducted comprehensive evaluations using the GLUE benchmark demonstrating the
efficacy of our method in enhancing the fine-tuning process for various PLMs
across diverse downstream tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07046">Rethinking Compression: Reduced Order Modelling of Latent Features in Large Language Models. (arXiv:2312.07046v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chavan_A/0/1/0/all/0/1">Arnav Chavan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lele_N/0/1/0/all/0/1">Nahush Lele</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1">Deepak Gupta</a></p>
<p>Due to the substantial scale of Large Language Models (LLMs), the direct
application of conventional compression methodologies proves impractical. The
computational demands associated with even minimal gradient updates present
challenges, particularly on consumer-grade hardware. This paper introduces an
innovative approach for the parametric and practical compression of LLMs based
on reduced order modelling, which entails low-rank decomposition within the
feature space and re-parameterization in the weight space. Notably, this
compression technique operates in a layer-wise manner, obviating the need for a
GPU device and enabling the compression of billion-scale models within
stringent constraints of both memory and time. Our method represents a
significant advancement in model compression by leveraging matrix
decomposition, demonstrating superior efficacy compared to the prevailing
state-of-the-art structured pruning method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07049">Improving Factual Error Correction by Learning to Inject Factual Errors. (arXiv:2312.07049v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xingwei He</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qianru Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_A/0/1/0/all/0/1">A-Long Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1">Jun Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1">Yuan Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yiu_S/0/1/0/all/0/1">Siu Ming Yiu</a></p>
<p>Factual error correction (FEC) aims to revise factual errors in false claims
with minimal editing, making them faithful to the provided evidence. This task
is crucial for alleviating the hallucination problem encountered by large
language models. Given the lack of paired data (i.e., false claims and their
corresponding correct claims), existing methods typically adopt the
mask-then-correct paradigm. This paradigm relies solely on unpaired false
claims and correct claims, thus being referred to as distantly supervised
methods. These methods require a masker to explicitly identify factual errors
within false claims before revising with a corrector. However, the absence of
paired data to train the masker makes accurately pinpointing factual errors
within claims challenging. To mitigate this, we propose to improve FEC by
Learning to Inject Factual Errors (LIFE), a three-step distantly supervised
method: mask-corrupt-correct. Specifically, we first train a corruptor using
the mask-then-corrupt procedure, allowing it to deliberately introduce factual
errors into correct text. The corruptor is then applied to correct claims,
generating a substantial amount of paired data. After that, we filter out
low-quality data, and use the remaining data to train a corrector. Notably, our
corrector does not require a masker, thus circumventing the bottleneck
associated with explicit factual error identification. Our experiments on a
public dataset verify the effectiveness of LIFE in two key aspects: Firstly, it
outperforms the previous best-performing distantly supervised method by a
notable margin of 10.59 points in SARI Final (19.3% improvement). Secondly,
even compared to ChatGPT prompted with in-context examples, LIFE achieves a
superiority of 7.16 points in SARI Final.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07066">DiffuVST: Narrating Fictional Scenes with Global-History-Guided Denoising Models. (arXiv:2312.07066v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Shengguang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1">Mei Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1">Qi Su</a></p>
<p>Recent advances in image and video creation, especially AI-based image
synthesis, have led to the production of numerous visual scenes that exhibit a
high level of abstractness and diversity. Consequently, Visual Storytelling
(VST), a task that involves generating meaningful and coherent narratives from
a collection of images, has become even more challenging and is increasingly
desired beyond real-world imagery. While existing VST techniques, which
typically use autoregressive decoders, have made significant progress, they
suffer from low inference speed and are not well-suited for synthetic scenes.
To this end, we propose a novel diffusion-based system DiffuVST, which models
the generation of a series of visual descriptions as a single conditional
denoising process. The stochastic and non-autoregressive nature of DiffuVST at
inference time allows it to generate highly diverse narratives more
efficiently. In addition, DiffuVST features a unique design with bi-directional
text history guidance and multimodal adapter modules, which effectively improve
inter-sentence coherence and image-to-text fidelity. Extensive experiments on
the story generation task covering four fictional visual-story datasets
demonstrate the superiority of DiffuVST over traditional autoregressive models
in terms of both text quality and inference speed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07069">Context Matter: Data-Efficient Augmentation of Large Language Models for Scientific Applications. (arXiv:2312.07069v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1">Haoran Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Siyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Ziwei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Maravi_A/0/1/0/all/0/1">Anurag Maravi</a>, <a href="http://arxiv.org/find/cs/1/au:+Abram_M/0/1/0/all/0/1">Marcin Abram</a></p>
<p>In this paper, we explore the challenges inherent to Large Language Models
(LLMs) like GPT-4, particularly their propensity for hallucinations, logic
mistakes, and incorrect conclusions when tasked with answering complex
questions. The capacity of LLMs to present erroneous answers in a coherent and
semantically rigorous manner further complicates the detection of factual
inaccuracies. This issue is especially pronounced in fields that require
specialized expertise. Our work delves into these challenges, aiming to enhance
the understanding and mitigation of such errors, thereby contributing to the
improvement of LLM accuracy and reliability in scientific and other specialized
domains. Our findings reveal a non-linear relationship between the context's
relevancy and the answers' measured quality. In addition, we demonstrate that
with the correct calibration, it is possible to automate the grading procedure
-- a finding suggesting that, at least to some degree, the LLMs can be used to
self-examine the quality of their own performance. Finally, we describe an
experimental platform that can be seen as a proof-of-concept of the techniques
described in this work.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07088">BED: Bi-Encoder-Decoder Model for Canonical Relation Extraction. (arXiv:2312.07088v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1">Nantao Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Long_S/0/1/0/all/0/1">Siyu Long</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1">Xinyu Dai</a></p>
<p>Canonical relation extraction aims to extract relational triples from
sentences, where the triple elements (entity pairs and their relationship) are
mapped to the knowledge base. Recently, methods based on the encoder-decoder
architecture are proposed and achieve promising results. However, these methods
cannot well utilize the entity information, which is merely used as augmented
training data. Moreover, they are incapable of representing novel entities,
since no embeddings have been learned for them. In this paper, we propose a
novel framework, Bi-Encoder-Decoder (BED), to solve the above issues.
Specifically, to fully utilize entity information, we employ an encoder to
encode semantics of this information, leading to high-quality entity
representations. For novel entities, given a trained entity encoder, their
representations can be easily generated. Experimental results on two datasets
show that, our method achieves a significant performance improvement over the
previous state-of-the-art and handle novel entities well without retraining.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07110">LLMs Perform Poorly at Concept Extraction in Cyber-security Research Literature. (arXiv:2312.07110v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wursch_M/0/1/0/all/0/1">Maxime W&#xfc;rsch</a>, <a href="http://arxiv.org/find/cs/1/au:+Kucharavy_A/0/1/0/all/0/1">Andrei Kucharavy</a>, <a href="http://arxiv.org/find/cs/1/au:+David_D/0/1/0/all/0/1">Dimitri Percia David</a>, <a href="http://arxiv.org/find/cs/1/au:+Mermoud_A/0/1/0/all/0/1">Alain Mermoud</a></p>
<p>The cybersecurity landscape evolves rapidly and poses threats to
organizations. To enhance resilience, one needs to track the latest
developments and trends in the domain. It has been demonstrated that standard
bibliometrics approaches show their limits in such a fast-evolving domain. For
this purpose, we use large language models (LLMs) to extract relevant knowledge
entities from cybersecurity-related texts. We use a subset of arXiv preprints
on cybersecurity as our data and compare different LLMs in terms of entity
recognition (ER) and relevance. The results suggest that LLMs do not produce
good knowledge entities that reflect the cybersecurity context, but our results
show some potential for noun extractors. For this reason, we developed a noun
extractor boosted with some statistical analysis to extract specific and
relevant compound nouns from the domain. Later, we tested our model to identify
trends in the LLM domain. We observe some limitations, but it offers promising
results to monitor the evolution of emergent trends.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07141">Multilingual large language models leak human stereotypes across language boundaries. (arXiv:2312.07141v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yang Trista Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sotnikova_A/0/1/0/all/0/1">Anna Sotnikova</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jieyu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_L/0/1/0/all/0/1">Linda X. Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Rudinger_R/0/1/0/all/0/1">Rachel Rudinger</a>, <a href="http://arxiv.org/find/cs/1/au:+Daume_H/0/1/0/all/0/1">Hal Daume III</a></p>
<p>Multilingual large language models have been increasingly popular for their
proficiency in comprehending and generating text across various languages.
Previous research has shown that the presence of stereotypes and biases in
monolingual large language models can be attributed to the nature of their
training data, which is collected from humans and reflects societal biases.
Multilingual language models undergo the same training procedure as monolingual
ones, albeit with training data sourced from various languages. This raises the
question: do stereotypes present in one social context leak across languages
within the model? In our work, we first define the term ``stereotype leakage''
and propose a framework for its measurement. With this framework, we
investigate how stereotypical associations leak across four languages: English,
Russian, Chinese, and Hindi. To quantify the stereotype leakage, we employ an
approach from social psychology, measuring stereotypes via group-trait
associations. We evaluate human stereotypes and stereotypical associations
manifested in multilingual large language models such as mBERT, mT5, and
ChatGPT. Our findings show a noticeable leakage of positive, negative, and
non-polar associations across all languages. Notably, Hindi within multilingual
models appears to be the most susceptible to influence from other languages,
while Chinese is the least. Additionally, ChatGPT exhibits a better alignment
with human scores than other models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07182">Classifying complex documents: comparing bespoke solutions to large language models. (arXiv:2312.07182v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hopkins_G/0/1/0/all/0/1">Glen Hopkins</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalm_K/0/1/0/all/0/1">Kristjan Kalm</a></p>
<p>Here we search for the best automated classification approach for a set of
complex legal documents. Our classification task is not trivial: our aim is to
classify ca 30,000 public courthouse records from 12 states and 267 counties at
two different levels using nine sub-categories. Specifically, we investigated
whether a fine-tuned large language model (LLM) can achieve the accuracy of a
bespoke custom-trained model, and what is the amount of fine-tuning necessary.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07194">Verbreitungsmechanismen sch\&quot;adigender Sprache im Netz: Anatomie zweier Shitstorms. (arXiv:2312.07194v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Scheffler_T/0/1/0/all/0/1">Tatjana Scheffler</a>, <a href="http://arxiv.org/find/cs/1/au:+Solopova_V/0/1/0/all/0/1">Veronika Solopova</a>, <a href="http://arxiv.org/find/cs/1/au:+Popa_Wyatt_M/0/1/0/all/0/1">Mihaela Popa-Wyatt</a></p>
<p>In this working paper, we turn our attention to two exemplary, cross-media
shitstorms directed against well-known individuals from the business world.
Both have in common, first, the trigger, a controversial statement by the
person who thereby becomes the target of the shitstorm, and second, the
identity of this target as relatively privileged: cis-male, white, successful.
We examine the spread of the outrage wave across two media at a time and test
the applicability of computational linguistic methods for analyzing its time
course. Assuming that harmful language spreads like a virus in digital space,
we are primarily interested in the events and constellations that lead to the
use of harmful language, and whether and how a linguistic formation of "tribes"
occurs. Our research therefore focuses, first, on the distribution of
linguistic features within the overall shitstorm: are individual words or
phrases increasingly used after their introduction, and through which pathways
they spread. Second, we ask whether "tribes," for example, one group of
supporters and one of opponents of the target, have a distinguished linguistic
form. Our hypothesis is that supporters remain equally active over time, while
the dynamic "ripple" effect of the shitstorm is based on the varying
participation of opponents.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07228">Toxic language detection: a systematic survey of Arabic datasets. (arXiv:2312.07228v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bensalem_I/0/1/0/all/0/1">Imene Bensalem</a>, <a href="http://arxiv.org/find/cs/1/au:+Rosso_P/0/1/0/all/0/1">Paolo Rosso</a>, <a href="http://arxiv.org/find/cs/1/au:+Zitouni_H/0/1/0/all/0/1">Hanane Zitouni</a></p>
<p>This paper offers a comprehensive survey of Arabic datasets focused on online
toxic language. We systematically gathered a total of 49 available datasets and
their corresponding papers and conducted a thorough analysis, considering 16
criteria across three primary dimensions: content, annotation process, and
reusability. This analysis enabled us to identify existing gaps and make
recommendations for future research works.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07250">Neural Machine Translation of Clinical Text: An Empirical Investigation into Multilingual Pre-Trained Language Models and Transfer-Learning. (arXiv:2312.07250v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1">Lifeng Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Gladkoff_S/0/1/0/all/0/1">Serge Gladkoff</a>, <a href="http://arxiv.org/find/cs/1/au:+Erofeev_G/0/1/0/all/0/1">Gleb Erofeev</a>, <a href="http://arxiv.org/find/cs/1/au:+Sorokina_I/0/1/0/all/0/1">Irina Sorokina</a>, <a href="http://arxiv.org/find/cs/1/au:+Galiano_B/0/1/0/all/0/1">Betty Galiano</a>, <a href="http://arxiv.org/find/cs/1/au:+Nenadic_G/0/1/0/all/0/1">Goran Nenadic</a></p>
<p>We conduct investigations on clinical text machine translation by examining
multilingual neural network models using deep learning such as Transformer
based structures. Furthermore, to address the language resource imbalance
issue, we also carry out experiments using a transfer learning methodology
based on massive multilingual pre-trained language models (MMPLMs). The
experimental results on three subtasks including 1) clinical case (CC), 2)
clinical terminology (CT), and 3) ontological concept (OC) show that our models
achieved top-level performances in the ClinSpEn-2022 shared task on
English-Spanish clinical domain data. Furthermore, our expert-based human
evaluations demonstrate that the small-sized pre-trained language model (PLM)
won over the other two extra-large language models by a large margin, in the
clinical domain fine-tuning, which finding was never reported in the field.
Finally, the transfer learning method works well in our experimental setting
using the WMT21fb model to accommodate a new language space Spanish that was
not seen at the pre-training stage within WMT21fb itself, which deserves more
exploitation for clinical knowledge transformation, e.g. to investigate into
more languages. These research findings can shed some light on domain-specific
machine translation development, especially in clinical and healthcare fields.
Further research projects can be carried out based on our work to improve
healthcare text analytics and knowledge transformation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07254">The GUA-Speech System Description for CNVSRC Challenge 2023. (arXiv:2312.07254v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shengqiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_C/0/1/0/all/0/1">Chao Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_B/0/1/0/all/0/1">Baozhong Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Binbin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1">Fuping Pan</a></p>
<p>This study describes our system for Task 1 Single-speaker Visual Speech
Recognition (VSR) fixed track in the Chinese Continuous Visual Speech
Recognition Challenge (CNVSRC) 2023. Specifically, we use intermediate
connectionist temporal classification (Inter CTC) residual modules to relax the
conditional independence assumption of CTC in our model. Then we use a
bi-transformer decoder to enable the model to capture both past and future
contextual information. In addition, we use Chinese characters as the modeling
units to improve the recognition accuracy of our model. Finally, we use a
recurrent neural network language model (RNNLM) for shallow fusion in the
inference stage. Experiments show that our system achieves a character error
rate (CER) of 38.09% on the Eval set which reaches a relative CER reduction of
21.63% over the official baseline, and obtains a second place in the challenge.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07255">GIST: Improving Parameter Efficient Fine Tuning via Knowledge Interaction. (arXiv:2312.07255v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ruan_J/0/1/0/all/0/1">Jiacheng Ruan</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jingsheng Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1">Mingye Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_S/0/1/0/all/0/1">Suncheng Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Zefang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Ting Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1">Yuzhuo Fu</a></p>
<p>The Parameter-Efficient Fine-Tuning (PEFT) method, which adjusts or
introduces fewer trainable parameters to calibrate pre-trained models on
downstream tasks, has become a recent research interest. However, existing PEFT
methods within the traditional fine-tiuning framework have two main
shortcomings: 1) They overlook the explicit association between trainable
parameters and downstream task knowledge. 2) They neglect the interaction
between the intrinsic task-agnostic knowledge of pre-trained models and the
task-specific knowledge in downstream tasks. To address this gap, we propose a
novel fine-tuning framework, named GIST, in a plug-and-play manner.
Specifically, our framework first introduces a trainable token, called the Gist
token, when applying PEFT methods on downstream tasks. This token serves as an
aggregator of the task-specific knowledge learned by the PEFT methods and forms
an explicit association with downstream knowledge. Furthermore, to facilitate
explicit interaction between task-agnostic and task-specific knowledge, we
introduce the concept of Knowledge Interaction via a Bidirectional
Kullback-Leibler Divergence objective. As a result, PEFT methods within our
framework can make the pre-trained model understand downstream tasks more
comprehensively by leveraging the knowledge interaction. Extensive experiments
demonstrate the universality and scalability of our framework. Notably, on the
VTAB-1K benchmark, we employ the Adapter (a prevalent PEFT method) within our
GIST framework and achieve a performance boost of 2.25%, with an increase of
only 0.8K parameters. The Code will be released.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07280">Towards Equipping Transformer with the Ability of Systematic Compositionality. (arXiv:2312.07280v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1">Chen Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_P/0/1/0/all/0/1">Peixin Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1">Wenqiang Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1">Jiancheng Lv</a></p>
<p>One of the key factors in language productivity and human cognition is the
ability of systematic compositionality, which refers to understanding composed
unseen examples of seen primitives. However, recent evidence reveals that the
Transformers have difficulty generalizing the composed context based on the
seen primitives. To this end, we take the first step to propose a
compositionality-aware Transformer called CAT and two novel pre-training tasks
to facilitate systematic compositionality. We tentatively provide a successful
implementation of a multi-layer CAT on the basis of the especially popular
BERT. The experimental results demonstrate that CAT outperforms baselines on
compositionality-aware tasks with minimal impact on the effectiveness on
standardized language understanding tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07305">SCCA: Shifted Cross Chunk Attention for long contextual semantic expansion. (arXiv:2312.07305v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yuxiang Guo</a></p>
<p>Sparse attention as a efficient method can significantly decrease the
computation cost, but current sparse attention tend to rely on window self
attention which block the global information flow. For this problem, we present
Shifted Cross Chunk Attention (SCCA), using different KV shifting strategy to
extend respective field in each attention layer. Except, we combine Dilated
Attention(DA) and Dilated Neighborhood Attention(DNA) to present Shifted
Dilated Attention(SDA). Both SCCA and SDA can accumulate attention results in
multi head attention to obtain approximate respective field in full attention.
In this paper, we conduct language modeling experiments using different pattern
of SCCA and combination of SCCA and SDA. The proposed shifted cross chunk
attention (SCCA) can effectively extend large language models (LLMs) to longer
context combined with Positional interpolation(PI) and LoRA than current sparse
attention. Notably, SCCA adopts LLaMA2 7B from 4k context to 8k in single V100.
This attention pattern can provide a Plug-and-play fine-tuning method to extend
model context while retaining their original architectures, and is compatible
with most existing techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07338">Self-supervised Adaptive Pre-training of Multilingual Speech Models for Language and Dialect Identification. (arXiv:2312.07338v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shaik_M/0/1/0/all/0/1">Mohammed Maqsood Shaik</a>, <a href="http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1">Dietrich Klakow</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdullah_B/0/1/0/all/0/1">Badr M. Abdullah</a></p>
<p>Pre-trained Transformer-based speech models have shown striking performance
when fine-tuned on various downstream tasks such as automatic speech
recognition and spoken language identification (SLID). However, the problem of
domain mismatch remains a challenge in this area, where the domain of the
pre-training data might differ from that of the downstream labeled data used
for fine-tuning. In multilingual tasks such as SLID, the pre-trained speech
model may not support all the languages in the downstream task. To address this
challenge, we propose self-supervised adaptive pre-training (SAPT) to adapt the
pre-trained model to the target domain and languages of the downstream task. We
apply SAPT to the XLSR-128 model and investigate the effectiveness of this
approach for the SLID task. First, we demonstrate that SAPT improves XLSR
performance on the FLEURS benchmark with substantial gains up to 40.1% for
under-represented languages. Second, we apply SAPT on four different datasets
in a few-shot learning setting, showing that our approach improves the sample
efficiency of XLSR during fine-tuning. Our experiments provide strong empirical
evidence that continual adaptation via self-supervision improves downstream
performance for multilingual speech models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07395">A Simple Recipe for Contrastively Pre-training Video-First Encoders Beyond 16 Frames. (arXiv:2312.07395v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Papalampidi_P/0/1/0/all/0/1">Pinelopi Papalampidi</a>, <a href="http://arxiv.org/find/cs/1/au:+Koppula_S/0/1/0/all/0/1">Skanda Koppula</a>, <a href="http://arxiv.org/find/cs/1/au:+Pathak_S/0/1/0/all/0/1">Shreya Pathak</a>, <a href="http://arxiv.org/find/cs/1/au:+Chiu_J/0/1/0/all/0/1">Justin Chiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Heyward_J/0/1/0/all/0/1">Joe Heyward</a>, <a href="http://arxiv.org/find/cs/1/au:+Patraucean_V/0/1/0/all/0/1">Viorica Patraucean</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1">Jiajun Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Miech_A/0/1/0/all/0/1">Antoine Miech</a>, <a href="http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1">Andrew Zisserman</a>, <a href="http://arxiv.org/find/cs/1/au:+Nematzdeh_A/0/1/0/all/0/1">Aida Nematzdeh</a></p>
<p>Understanding long, real-world videos requires modeling of long-range visual
dependencies. To this end, we explore video-first architectures, building on
the common paradigm of transferring large-scale, image--text models to video
via shallow temporal fusion. However, we expose two limitations to the
approach: (1) decreased spatial capabilities, likely due to poor
video--language alignment in standard video datasets, and (2) higher memory
consumption, bottlenecking the number of frames that can be processed. To
mitigate the memory bottleneck, we systematically analyze the memory/accuracy
trade-off of various efficient methods: factorized attention,
parameter-efficient image-to-video adaptation, input masking, and
multi-resolution patchification. Surprisingly, simply masking large portions of
the video (up to 75%) during contrastive pre-training proves to be one of the
most robust ways to scale encoders to videos up to 4.3 minutes at 1 FPS. Our
simple approach for training long video-to-text models, which scales to 1B
parameters, does not add new architectural complexity and is able to outperform
the popular paradigm of using much larger LLMs as an information aggregator
over segment-based information on benchmarks with long-range temporal
dependencies (YouCook2, EgoSchema).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07398">LLMEval: A Preliminary Study on How to Evaluate Large Language Models. (arXiv:2312.07398v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yue Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Ming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1">Haipeng Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shichun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yongyao Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1">Tao Gui</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xuanjing Huang</a></p>
<p>Recently, the evaluation of Large Language Models has emerged as a popular
area of research. The three crucial questions for LLM evaluation are ``what,
where, and how to evaluate''. However, the existing research mainly focuses on
the first two questions, which are basically what tasks to give the LLM during
testing and what kind of knowledge it should deal with. As for the third
question, which is about what standards to use, the types of evaluators, how to
score, and how to rank, there hasn't been much discussion. In this paper, we
analyze evaluation methods by comparing various criteria with both manual and
automatic evaluation, utilizing onsite, crowd-sourcing, public annotators and
GPT-4, with different scoring methods and ranking systems. We propose a new
dataset, LLMEval and conduct evaluations on 20 LLMs. A total of 2,186
individuals participated, leading to the generation of 243,337 manual
annotations and 57,511 automatic evaluation results. We perform comparisons and
analyses of different settings and conduct 10 conclusions that can provide some
insights for evaluating LLM in the future. The dataset and the results are
publicly available at https://github.com/llmeval .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07399">Large Language Models are Clinical Reasoners: Reasoning-Aware Diagnosis Framework with Prompt-Generated Rationales. (arXiv:2312.07399v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kwon_T/0/1/0/all/0/1">Taeyoon Kwon</a>, <a href="http://arxiv.org/find/cs/1/au:+Ong_K/0/1/0/all/0/1">Kai Tzu-iunn Ong</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1">Dongjin Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1">Seungjun Moon</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jeong Ryong Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_D/0/1/0/all/0/1">Dosik Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sim_Y/0/1/0/all/0/1">Yongsik Sim</a>, <a href="http://arxiv.org/find/cs/1/au:+Sohn_B/0/1/0/all/0/1">Beomseok Sohn</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1">Dongha Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Yeo_J/0/1/0/all/0/1">Jinyoung Yeo</a></p>
<p>Machine reasoning has made great progress in recent years owing to large
language models (LLMs). In the clinical domain, however, most NLP-driven
projects mainly focus on clinical classification or reading comprehension, and
under-explore clinical reasoning for disease diagnosis due to the expensive
rationale annotation with clinicians. In this work, we present a
``reasoning-aware'' diagnosis framework that rationalizes the diagnostic
process via prompt-based learning in a time- and labor-efficient manner, and
learns to reason over the prompt-generated rationales. Specifically, we address
the clinical reasoning for disease diagnosis, where the LLM generates
diagnostic rationales providing its insight on presented patient data and the
reasoning path towards the diagnosis, namely Clinical Chain-of-Thought
(Clinical CoT). We empirically demonstrate LLMs/LMs' ability of clinical
reasoning via extensive experiments and analyses on both rationale generation
and disease diagnosis in various settings. We further propose a novel set of
criteria for evaluating machine-generated rationales' potential for real-world
clinical settings, facilitating and benefiting future research in this area.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07405">ICL Markup: Structuring In-Context Learning using Soft-Token Tags. (arXiv:2312.07405v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Brunet_M/0/1/0/all/0/1">Marc-Etienne Brunet</a>, <a href="http://arxiv.org/find/cs/1/au:+Anderson_A/0/1/0/all/0/1">Ashton Anderson</a>, <a href="http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1">Richard Zemel</a></p>
<p>Large pretrained language models (LLMs) can be rapidly adapted to a wide
variety of tasks via a text-to-text approach, where the instruction and input
are fed to the model in natural language. Combined with in-context learning
(ICL), this paradigm is impressively flexible and powerful. However, it also
burdens users with an overwhelming number of choices, many of them arbitrary.
Inspired by markup languages like HTML, we contribute a method of using
soft-token tags to compose prompt templates. This approach reduces arbitrary
decisions and streamlines the application of ICL. Our method is a form of
meta-learning for ICL; it learns these tags in advance during a
parameter-efficient fine-tuning ``warm-up'' process. The tags can subsequently
be used in templates for ICL on new, unseen tasks without any additional
fine-tuning. Our experiments with this approach yield promising initial
results, improving LLM performance on important enterprise applications such as
few-shot and open-world intent detection, as well as text classification in
news and legal domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07419">Towards Faster k-Nearest-Neighbor Machine Translation. (arXiv:2312.07419v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1">Xiangyu Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1">Yunlong Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jinan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yufeng Chen</a></p>
<p>Recent works have proven the effectiveness of k-nearest-neighbor machine
translation(a.k.a kNN-MT) approaches to produce remarkable improvement in
cross-domain translations. However, these models suffer from heavy retrieve
overhead on the entire datastore when decoding each token. We observe that
during the decoding phase, about 67% to 84% of tokens are unvaried after
searching over the corpus datastore, which means most of the tokens cause
futile retrievals and introduce unnecessary computational costs by initiating
k-nearest-neighbor searches. We consider this phenomenon is explainable in
linguistics and propose a simple yet effective multi-layer perceptron (MLP)
network to predict whether a token should be translated jointly by the neural
machine translation model and probabilities produced by the kNN or just by the
neural model. The results show that our method succeeds in reducing redundant
retrieval operations and significantly reduces the overhead of kNN retrievals
by up to 53% at the expense of a slight decline in translation quality.
Moreover, our method could work together with all existing kNN-MT systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07435">Cross-modal Contrastive Learning with Asymmetric Co-attention Network for Video Moment Retrieval. (arXiv:2312.07435v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Panta_L/0/1/0/all/0/1">Love Panta</a>, <a href="http://arxiv.org/find/cs/1/au:+Shrestha_P/0/1/0/all/0/1">Prashant Shrestha</a>, <a href="http://arxiv.org/find/cs/1/au:+Sapkota_B/0/1/0/all/0/1">Brabeem Sapkota</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhattarai_A/0/1/0/all/0/1">Amrita Bhattarai</a>, <a href="http://arxiv.org/find/cs/1/au:+Manandhar_S/0/1/0/all/0/1">Suresh Manandhar</a>, <a href="http://arxiv.org/find/cs/1/au:+Sah_A/0/1/0/all/0/1">Anand Kumar Sah</a></p>
<p>Video moment retrieval is a challenging task requiring fine-grained
interactions between video and text modalities. Recent work in image-text
pretraining has demonstrated that most existing pretrained models suffer from
information asymmetry due to the difference in length between visual and
textual sequences. We question whether the same problem also exists in the
video-text domain with an auxiliary need to preserve both spatial and temporal
information. Thus, we evaluate a recently proposed solution involving the
addition of an asymmetric co-attention network for video grounding tasks.
Additionally, we incorporate momentum contrastive loss for robust,
discriminative representation learning in both modalities. We note that the
integration of these supplementary modules yields better performance compared
to state-of-the-art models on the TACoS dataset and comparable results on
ActivityNet Captions, all while utilizing significantly fewer parameters with
respect to baseline.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07476">Comparable Demonstrations are Important in In-Context Learning: A Novel Perspective on Demonstration Selection. (arXiv:2312.07476v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1">Caoyun Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1">Jidong Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yitian Li</a>, <a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1">Hao He</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1">Yaohui Jin</a></p>
<p>In-Context Learning (ICL) is an important paradigm for adapting Large
Language Models (LLMs) to downstream tasks through a few demonstrations.
Despite the great success of ICL, the limitation of the demonstration number
may lead to demonstration bias, i.e. the input-label mapping induced by LLMs
misunderstands the task's essence. Inspired by human experience, we attempt to
mitigate such bias through the perspective of the inter-demonstration
relationship. Specifically, we construct Comparable Demonstrations (CDs) by
minimally editing the texts to flip the corresponding labels, in order to
highlight the task's essence and eliminate potential spurious correlations
through the inter-demonstration comparison. Through a series of experiments on
CDs, we find that (1) demonstration bias does exist in LLMs, and CDs can
significantly reduce such bias; (2) CDs exhibit good performance in ICL,
especially in out-of-distribution scenarios. In summary, this study explores
the ICL mechanisms from a novel perspective, providing a deeper insight into
the demonstration selection strategy for ICL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07492">SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative Language Models. (arXiv:2312.07492v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nagireddy_M/0/1/0/all/0/1">Manish Nagireddy</a>, <a href="http://arxiv.org/find/cs/1/au:+Chiazor_L/0/1/0/all/0/1">Lamogha Chiazor</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1">Moninder Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Baldini_I/0/1/0/all/0/1">Ioana Baldini</a></p>
<p>Current datasets for unwanted social bias auditing are limited to studying
protected demographic features such as race and gender. In this work, we
introduce a comprehensive benchmark that is meant to capture the amplification
of social bias, via stigmas, in generative language models. We start with a
comprehensive list of 93 stigmas documented in social science literature and
curate a question-answering (QA) dataset which involves simple social
situations. Our benchmark, SocialStigmaQA, contains roughly 10K prompts, with a
variety of prompt styles, carefully constructed to systematically test for both
social bias and model robustness. We present results for SocialStigmaQA with
two widely used open source generative language models and we demonstrate that
the output generated by these models considerably amplifies existing social
bias against stigmatized groups. Specifically, we find that the proportion of
socially biased output ranges from 45% to 59% across a variety of decoding
strategies and prompting styles. We discover that the deliberate design of the
templates in our benchmark (e.g., by adding biasing text to the prompt or
varying the answer that indicates bias) impact the model tendencies to generate
socially biased output. Additionally, we report on patterns in the generated
chain-of-thought output, finding a variety of problems from subtle bias to
evidence of a lack of reasoning.
</p>
<p>Warning: This paper contains examples of text which is toxic, biased, and
harmful.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.01959">Detect, Retrieve, Comprehend: A Flexible Framework for Zero-Shot Document-Level Question Answering. (arXiv:2210.01959v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+McDonald_T/0/1/0/all/0/1">Tavish McDonald</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsan_B/0/1/0/all/0/1">Brian Tsan</a>, <a href="http://arxiv.org/find/cs/1/au:+Saini_A/0/1/0/all/0/1">Amar Saini</a>, <a href="http://arxiv.org/find/cs/1/au:+Ordonez_J/0/1/0/all/0/1">Juanita Ordonez</a>, <a href="http://arxiv.org/find/cs/1/au:+Gutierrez_L/0/1/0/all/0/1">Luis Gutierrez</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1">Phan Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Mason_B/0/1/0/all/0/1">Blake Mason</a>, <a href="http://arxiv.org/find/cs/1/au:+Ng_B/0/1/0/all/0/1">Brenda Ng</a></p>
<p>Researchers produce thousands of scholarly documents containing valuable
technical knowledge. The community faces the laborious task of reading these
documents to identify, extract, and synthesize information. To automate
information gathering, document-level question answering (QA) offers a flexible
framework where human-posed questions can be adapted to extract diverse
knowledge. Finetuning QA systems requires access to labeled data (tuples of
context, question and answer). However, data curation for document QA is
uniquely challenging because the context (i.e. answer evidence passage) needs
to be retrieved from potentially long, ill-formatted documents. Existing QA
datasets sidestep this challenge by providing short, well-defined contexts that
are unrealistic in real-world applications. We present a three-stage document
QA approach: (1) text extraction from PDF; (2) evidence retrieval from
extracted texts to form well-posed contexts; (3) QA to extract knowledge from
contexts to return high-quality answers -- extractive, abstractive, or Boolean.
Using QASPER for evaluation, our detect-retrieve-comprehend (DRC) system
achieves a +7.19 improvement in Answer-F1 over existing baselines while
delivering superior context selection. Our results demonstrate that DRC holds
tremendous promise as a flexible framework for practical scientific document
QA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.09932">Making Science Simple: Corpora for the Lay Summarisation of Scientific Literature. (arXiv:2210.09932v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Goldsack_T/0/1/0/all/0/1">Tomas Goldsack</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhihao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1">Chenghua Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Scarton_C/0/1/0/all/0/1">Carolina Scarton</a></p>
<p>Lay summarisation aims to jointly summarise and simplify a given text, thus
making its content more comprehensible to non-experts. Automatic approaches for
lay summarisation can provide significant value in broadening access to
scientific literature, enabling a greater degree of both interdisciplinary
knowledge sharing and public understanding when it comes to research findings.
However, current corpora for this task are limited in their size and scope,
hindering the development of broadly applicable data-driven approaches. Aiming
to rectify these issues, we present two novel lay summarisation datasets, PLOS
(large-scale) and eLife (medium-scale), each of which contains biomedical
journal articles alongside expert-written lay summaries. We provide a thorough
characterisation of our lay summaries, highlighting differing levels of
readability and abstractiveness between datasets that can be leveraged to
support the needs of different applications. Finally, we benchmark our datasets
using mainstream summarisation approaches and perform a manual evaluation with
domain experts, demonstrating their utility and casting light on the key
challenges of this task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.13201">Highlighting Named Entities in Input for Auto-Formulation of Optimization Problems. (arXiv:2212.13201v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gangwar_N/0/1/0/all/0/1">Neeraj Gangwar</a>, <a href="http://arxiv.org/find/cs/1/au:+Kani_N/0/1/0/all/0/1">Nickvash Kani</a></p>
<p>Operations research deals with modeling and solving real-world problems as
mathematical optimization problems. While solving mathematical systems is
accomplished by analytical software, formulating a problem as a set of
mathematical operations has been typically done manually by domain experts.
Recent machine learning methods have shown promise in converting textual
problem descriptions to corresponding mathematical formulations. This paper
presents an approach that converts linear programming word problems into
mathematical formulations. We leverage the named entities in the input and
augment the input to highlight these entities. Our approach achieves the
highest accuracy among all submissions to the NL4Opt Competition, securing
first place in the generation track.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.11091">GTRL: An Entity Group-Aware Temporal Knowledge Graph Representation Learning Method. (arXiv:2302.11091v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1">Xing Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Ling Chen</a></p>
<p>Temporal Knowledge Graph (TKG) representation learning embeds entities and
event types into a continuous low-dimensional vector space by integrating the
temporal information, which is essential for downstream tasks, e.g., event
prediction and question answering. Existing methods stack multiple graph
convolution layers to model the influence of distant entities, leading to the
over-smoothing problem. To alleviate the problem, recent studies infuse
reinforcement learning to obtain paths that contribute to modeling the
influence of distant entities. However, due to the limited number of hops,
these studies fail to capture the correlation between entities that are far
apart and even unreachable. To this end, we propose GTRL, an entity Group-aware
Temporal knowledge graph Representation Learning method. GTRL is the first work
that incorporates the entity group modeling to capture the correlation between
entities by stacking only a finite number of layers. Specifically, the entity
group mapper is proposed to generate entity groups from entities in a learning
way. Based on entity groups, the implicit correlation encoder is introduced to
capture implicit correlations between any pairwise entity groups. In addition,
the hierarchical GCNs are exploited to accomplish the message aggregation and
representation updating on the entity group graph and the entity graph.
Finally, GRUs are employed to capture the temporal dependency in TKGs.
Extensive experiments on three real-world datasets demonstrate that GTRL
achieves the state-of-the-art performances on the event prediction task,
outperforming the best baseline by an average of 13.44%, 9.65%, 12.15%, and
15.12% in MRR, Hits@1, Hits@3, and Hits@10, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.00855">Grounded Decoding: Guiding Text Generation with Grounded Models for Embodied Agents. (arXiv:2303.00855v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1">Wenlong Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1">Fei Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1">Dhruv Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Driess_D/0/1/0/all/0/1">Danny Driess</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1">Andy Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yao Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Florence_P/0/1/0/all/0/1">Pete Florence</a>, <a href="http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1">Igor Mordatch</a>, <a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1">Sergey Levine</a>, <a href="http://arxiv.org/find/cs/1/au:+Hausman_K/0/1/0/all/0/1">Karol Hausman</a>, <a href="http://arxiv.org/find/cs/1/au:+Ichter_B/0/1/0/all/0/1">Brian Ichter</a></p>
<p>Recent progress in large language models (LLMs) has demonstrated the ability
to learn and leverage Internet-scale knowledge through pre-training with
autoregressive models. Unfortunately, applying such models to settings with
embodied agents, such as robots, is challenging due to their lack of experience
with the physical world, inability to parse non-language observations, and
ignorance of rewards or safety constraints that robots may require. On the
other hand, language-conditioned robotic policies that learn from interaction
data can provide the necessary grounding that allows the agent to be correctly
situated in the real world, but such policies are limited by the lack of
high-level semantic understanding due to the limited breadth of the interaction
data available for training them. Thus, if we want to make use of the semantic
knowledge in a language model while still situating it in an embodied setting,
we must construct an action sequence that is both likely according to the
language model and also realizable according to grounded models of the
environment. We frame this as a problem similar to probabilistic filtering:
decode a sequence that both has high probability under the language model and
high probability under a set of grounded model objectives. We demonstrate how
such grounded models can be obtained across three simulation and real-world
domains, and that the proposed decoding strategy is able to solve complex,
long-horizon embodiment tasks in a robotic setting by leveraging the knowledge
of both models. The project's website can be found at
grounded-decoding.github.io.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.07699">A Clustering Framework for Unsupervised and Semi-supervised New Intent Discovery. (arXiv:2304.07699v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hanlei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hua Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Long_F/0/1/0/all/0/1">Fei Long</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1">Kai Gao</a></p>
<p>New intent discovery is of great value to natural language processing,
allowing for a better understanding of user needs and providing friendly
services. However, most existing methods struggle to capture the complicated
semantics of discrete text representations when limited or no prior knowledge
of labeled data is available. To tackle this problem, we propose a novel
clustering framework, USNID, for unsupervised and semi-supervised new intent
discovery, which has three key technologies. First, it fully utilizes
unsupervised or semi-supervised data to mine shallow semantic similarity
relations and provide well-initialized representations for clustering. Second,
it designs a centroid-guided clustering mechanism to address the issue of
cluster allocation inconsistency and provide high-quality self-supervised
targets for representation learning. Third, it captures high-level semantics in
unsupervised or semi-supervised data to discover fine-grained intent-wise
clusters by optimizing both cluster-level and instance-level objectives. We
also propose an effective method for estimating the cluster number in
open-world scenarios without knowing the number of new intents beforehand.
USNID performs exceptionally well on several benchmark intent datasets,
achieving new state-of-the-art results in unsupervised and semi-supervised new
intent discovery and demonstrating robust performance with different cluster
numbers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.07402">Synergistic Interplay between Search and Large Language Models for Information Retrieval. (arXiv:2305.07402v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1">Jiazhan Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1">Chongyang Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1">Xiubo Geng</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1">Tao Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Can Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1">Guodong Long</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1">Dongyan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1">Daxin Jiang</a></p>
<p>Information retrieval (IR) plays a crucial role in locating relevant
resources from vast amounts of data, and its applications have evolved from
traditional knowledge bases to modern retrieval models (RMs). The emergence of
large language models (LLMs) has further revolutionized the IR field by
enabling users to interact with search systems in natural languages. In this
paper, we explore the advantages and disadvantages of LLMs and RMs,
highlighting their respective strengths in understanding user-issued queries
and retrieving up-to-date information. To leverage the benefits of both
paradigms while circumventing their limitations, we propose InteR, a novel
framework that facilitates information refinement through synergy between RMs
and LLMs. InteR allows RMs to expand knowledge in queries using LLM-generated
knowledge collections and enables LLMs to enhance prompt formulation using
retrieved documents. This iterative refinement process augments the inputs of
RMs and LLMs, leading to more accurate retrieval. Experiments on large-scale
retrieval benchmarks involving web search and low-resource retrieval tasks
demonstrate that InteR achieves overall superior zero-shot retrieval
performance compared to state-of-the-art methods, even those using relevance
judgment. Source code is available at https://github.com/Cyril-JZ/InteR
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15057">Linear-Time Modeling of Linguistic Structure: An Order-Theoretic Perspective. (arXiv:2305.15057v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tianyu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1">Afra Amini</a>, <a href="http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1">Mrinmaya Sachan</a>, <a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1">Ryan Cotterell</a></p>
<p>Tasks that model the relation between pairs of tokens in a string are a vital
part of understanding natural language. Such tasks, in general, require
exhaustive pair-wise comparisons of tokens, thus having a quadratic runtime
complexity in the length of the string. We show that these exhaustive
comparisons can be avoided, and, moreover, the complexity of such tasks can be
reduced to linear by casting the relation between tokens as a partial order
over the string. Our method predicts real numbers for each token in a string in
parallel and sorts the tokens accordingly, resulting in total orders of the
tokens in the string. Each total order implies a set of arcs oriented from
smaller to greater tokens, sorted by their predicted numbers. The intersection
of total orders results in a partial order over the set of tokens in the
string, which is then decoded into a directed graph representing the desired
linguistic structure. Our experiments on dependency parsing and coreference
resolution show that our method achieves state-of-the-art or comparable
performance. Moreover, the linear complexity and parallelism of our method
double the speed of graph-based coreference resolution models, and bring a
10-times speed-up over graph-based dependency parsers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.03241">Early Weight Averaging meets High Learning Rates for LLM Pre-training. (arXiv:2306.03241v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sanyal_S/0/1/0/all/0/1">Sunny Sanyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Neerkaje_A/0/1/0/all/0/1">Atula Neerkaje</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaddour_J/0/1/0/all/0/1">Jean Kaddour</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1">Abhishek Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanghavi_S/0/1/0/all/0/1">Sujay Sanghavi</a></p>
<p>Training Large Language Models (LLMs) incurs significant cost; hence, any
strategy that accelerates model convergence is helpful. In this paper, we
investigate the ability of a simple idea checkpoint averaging along the
trajectory of a training run to improve both convergence and generalization
quite early on during training. Here we show that models trained with high
learning rates observe higher gains due to checkpoint averaging. Furthermore,
these gains are amplified when checkpoints are sampled with considerable
spacing in training steps. Our training recipe outperforms conventional
training and popular checkpoint averaging baselines such as exponential moving
average (EMA) and stochastic moving average (SWA). We evaluate our training
recipe by pre-training LLMs, where high learning rates are inherently preferred
due to extremely large batch sizes. Specifically, we pre-trained nanoGPT-2
models of varying sizes, small (125M), medium (335M), and large (770M)on the
OpenWebText dataset, comprised of 9B tokens. Additionally, we present results
for publicly available Pythia LLMs, ranging from 1B to 12B, which were trained
on the PILE-deduped dataset containing 207B tokens.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.12424">VisoGender: A dataset for benchmarking gender bias in image-text pronoun resolution. (arXiv:2306.12424v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hall_S/0/1/0/all/0/1">Siobhan Mackenzie Hall</a>, <a href="http://arxiv.org/find/cs/1/au:+Abrantes_F/0/1/0/all/0/1">Fernanda Gon&#xe7;alves Abrantes</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1">Hanwen Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sodunke_G/0/1/0/all/0/1">Grace Sodunke</a>, <a href="http://arxiv.org/find/cs/1/au:+Shtedritski_A/0/1/0/all/0/1">Aleksandar Shtedritski</a>, <a href="http://arxiv.org/find/cs/1/au:+Kirk_H/0/1/0/all/0/1">Hannah Rose Kirk</a></p>
<p>We introduce VisoGender, a novel dataset for benchmarking gender bias in
vision-language models. We focus on occupation-related biases within a
hegemonic system of binary gender, inspired by Winograd and Winogender schemas,
where each image is associated with a caption containing a pronoun relationship
of subjects and objects in the scene. VisoGender is balanced by gender
representation in professional roles, supporting bias evaluation in two ways:
i) resolution bias, where we evaluate the difference between pronoun resolution
accuracies for image subjects with gender presentations perceived as masculine
versus feminine by human annotators and ii) retrieval bias, where we compare
ratios of professionals perceived to have masculine and feminine gender
presentations retrieved for a gender-neutral search query. We benchmark several
state-of-the-art vision-language models and find that they demonstrate bias in
resolving binary gender in complex scenes. While the direction and magnitude of
gender bias depends on the task and the model being evaluated, captioning
models are generally less biased than Vision-Language Encoders. Dataset and
code are available at https://github.com/oxai/visogender
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.06077">Fly-Swat or Cannon? Cost-Effective Language Model Choice via Meta-Modeling. (arXiv:2308.06077v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sakota_M/0/1/0/all/0/1">Marija &#x160;akota</a>, <a href="http://arxiv.org/find/cs/1/au:+Peyrard_M/0/1/0/all/0/1">Maxime Peyrard</a>, <a href="http://arxiv.org/find/cs/1/au:+West_R/0/1/0/all/0/1">Robert West</a></p>
<p>Generative language models (LMs) have become omnipresent across data science.
For a wide variety of tasks, inputs can be phrased as natural language prompts
for an LM, from whose output the solution can then be extracted. LM performance
has consistently been increasing with model size - but so has the monetary cost
of querying the ever larger models. Importantly, however, not all inputs are
equally hard: some require larger LMs for obtaining a satisfactory solution,
whereas for others smaller LMs suffice. Based on this fact, we design a
framework for Cost-Effective Language Model Choice (CELMOC). Given a set of
inputs and a set of candidate LMs, CELMOC judiciously assigns each input to an
LM predicted to do well on the input according to a so-called meta-model,
aiming to achieve high overall performance at low cost. The cost-performance
trade-off can be flexibly tuned by the user. Options include, among others,
maximizing total expected performance (or the number of processed inputs) while
staying within a given cost budget, or minimizing total cost while processing
all inputs. We evaluate CELMOC on 14 datasets covering five natural language
tasks, using four candidate LMs of vastly different size and cost. With CELMOC,
we match the performance of the largest available LM while achieving a cost
reduction of 63%. Via our publicly available library, researchers as well as
practitioners can thus save large amounts of money without sacrificing
performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07870">Agents: An Open-source Framework for Autonomous Language Agents. (arXiv:2309.07870v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1">Wangchunshu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yuchen Eleanor Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Long Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jialong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tiannan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1">Shi Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jintian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jing Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1">Ruipu Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1">Shiding Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wentao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1">Xiangru Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Ningyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huajun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1">Peng Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1">Mrinmaya Sachan</a></p>
<p>Recent advances on large language models (LLMs) enable researchers and
developers to build autonomous language agents that can automatically solve
various tasks and interact with environments, humans, and other agents using
natural language interfaces. We consider language agents as a promising
direction towards artificial general intelligence and release Agents, an
open-source library with the goal of opening up these advances to a wider
non-specialist audience. Agents is carefully engineered to support important
features including planning, memory, tool usage, multi-agent communication, and
fine-grained symbolic control. Agents is user-friendly as it enables
non-specialists to build, customize, test, tune, and deploy state-of-the-art
autonomous language agents without much coding. The library is also
research-friendly as its modularized design makes it easily extensible for
researchers. Agents is available at https://github.com/aiwaves-cn/agents.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.09357">Talk2Care: Facilitating Asynchronous Patient-Provider Communication with Large-Language-Model. (arXiv:2309.09357v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Ziqi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xuhai Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1">Bingsheng Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Rogers_E/0/1/0/all/0/1">Ethan Rogers</a>, <a href="http://arxiv.org/find/cs/1/au:+Intille_S/0/1/0/all/0/1">Stephen Intille</a>, <a href="http://arxiv.org/find/cs/1/au:+Shara_N/0/1/0/all/0/1">Nawar Shara</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1">Guodong Gordon Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dakuo Wang</a></p>
<p>Despite the plethora of telehealth applications to assist home-based older
adults and healthcare providers, basic messaging and phone calls are still the
most common communication methods, which suffer from limited availability,
information loss, and process inefficiencies. One promising solution to
facilitate patient-provider communication is to leverage large language models
(LLMs) with their powerful natural conversation and summarization capability.
However, there is a limited understanding of LLMs' role during the
communication. We first conducted two interview studies with both older adults
(N=10) and healthcare providers (N=9) to understand their needs and
opportunities for LLMs in patient-provider asynchronous communication. Based on
the insights, we built an LLM-powered communication system, Talk2Care, and
designed interactive components for both groups: (1) For older adults, we
leveraged the convenience and accessibility of voice assistants (VAs) and built
an LLM-powered VA interface for effective information collection. (2) For
health providers, we built an LLM-based dashboard to summarize and present
important health information based on older adults' conversations with the VA.
We further conducted two user studies with older adults and providers to
evaluate the usability of the system. The results showed that Talk2Care could
facilitate the communication process, enrich the health information collected
from older adults, and considerably save providers' efforts and time. We
envision our work as an initial exploration of LLMs' capability in the
intersection of healthcare and interpersonal communication.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.13788">Can LLM-Generated Misinformation Be Detected?. (arXiv:2309.13788v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Canyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shu_K/0/1/0/all/0/1">Kai Shu</a></p>
<p>The advent of Large Language Models (LLMs) has made a transformative impact.
However, the potential that LLMs such as ChatGPT can be exploited to generate
misinformation has posed a serious concern to online safety and public trust. A
fundamental research question is: will LLM-generated misinformation cause more
harm than human-written misinformation? We propose to tackle this question from
the perspective of detection difficulty. We first build a taxonomy of
LLM-generated misinformation. Then we categorize and validate the potential
real-world methods for generating misinformation with LLMs. Then, through
extensive empirical investigation, we discover that LLM-generated
misinformation can be harder to detect for humans and detectors compared to
human-written misinformation with the same semantics, which suggests it can
have more deceptive styles and potentially cause more harm. We also discuss the
implications of our discovery on combating misinformation in the age of LLMs
and the countermeasures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.17453">Efficient Streaming Language Models with Attention Sinks. (arXiv:2309.17453v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiao_G/0/1/0/all/0/1">Guangxuan Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yuandong Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Beidi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1">Song Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1">Mike Lewis</a></p>
<p>Deploying Large Language Models (LLMs) in streaming applications such as
multi-round dialogue, where long interactions are expected, is urgently needed
but poses two major challenges. Firstly, during the decoding stage, caching
previous tokens' Key and Value states (KV) consumes extensive memory. Secondly,
popular LLMs cannot generalize to longer texts than the training sequence
length. Window attention, where only the most recent KVs are cached, is a
natural approach -- but we show that it fails when the text length surpasses
the cache size. We observe an interesting phenomenon, namely attention sink,
that keeping the KV of initial tokens will largely recover the performance of
window attention. In this paper, we first demonstrate that the emergence of
attention sink is due to the strong attention scores towards initial tokens as
a ``sink'' even if they are not semantically important. Based on the above
analysis, we introduce StreamingLLM, an efficient framework that enables LLMs
trained with a finite length attention window to generalize to infinite
sequence lengths without any fine-tuning. We show that StreamingLLM can enable
Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language
modeling with up to 4 million tokens and more. In addition, we discover that
adding a placeholder token as a dedicated attention sink during pre-training
can further improve streaming deployment. In streaming settings, StreamingLLM
outperforms the sliding window recomputation baseline by up to 22.2x speedup.
Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.14505">Sentiment analysis with adaptive multi-head attention in Transformer. (arXiv:2310.14505v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1">Fanfei Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Demeter_D/0/1/0/all/0/1">David Demeter</a></p>
<p>We propose a novel framework based on the attention mechanism to identify the
sentiment of a movie review document. Previous efforts on deep neural networks
with attention mechanisms focus on encoder and decoder with fixed numbers of
multi-head attention. Therefore, we need a mechanism to stop the attention
process automatically if no more useful information can be read from the
memory.In this paper, we propose an adaptive multi-head attention architecture
(AdaptAttn) which varies the number of attention heads based on length of
sentences. AdaptAttn has a data preprocessing step where each document is
classified into any one of the three bins small, medium or large based on
length of the sentence. The document classified as small goes through two heads
in each layer, the medium group passes four heads and the large group is
processed by eight heads. We examine the merit of our model on the Stanford
large movie review dataset. The experimental results show that the F1 score
from our model is on par with the baseline model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.18368">Muslim-Violence Bias Persists in Debiased GPT Models. (arXiv:2310.18368v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hemmatian_B/0/1/0/all/0/1">Babak Hemmatian</a>, <a href="http://arxiv.org/find/cs/1/au:+Baltaji_R/0/1/0/all/0/1">Razan Baltaji</a>, <a href="http://arxiv.org/find/cs/1/au:+Varshney_L/0/1/0/all/0/1">Lav R. Varshney</a></p>
<p>Abid et al. (2021) showed a tendency in GPT-3 to generate mostly violent
completions when prompted about Muslims, compared with other religions. Two
pre-registered replication attempts found few violent completions and only a
weak anti-Muslim bias in the more recent InstructGPT, fine-tuned to eliminate
biased and toxic outputs. However, more pre-registered experiments showed that
using common names associated with the religions in prompts increases
several-fold the rate of violent completions, revealing a significant
second-order anti-Muslim bias. ChatGPT showed a bias many times stronger
regardless of prompt format, suggesting that the effects of debiasing were
reduced with continued model development. Our content analysis revealed
religion-specific themes containing offensive stereotypes across all
experiments. Our results show the need for continual de-biasing of models in
ways that address both explicit and higher-order associations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06062">Practical Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration. (arXiv:2311.06062v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fu_W/0/1/0/all/0/1">Wenjie Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Huandong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1">Chen Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1">Guanghua Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1">Tao Jiang</a></p>
<p>Membership Inference Attacks (MIA) aim to infer whether a target data record
has been utilized for model training or not. Prior attempts have quantified the
privacy risks of language models (LMs) via MIAs, but there is still no
consensus on whether existing MIA algorithms can cause remarkable privacy
leakage on practical Large Language Models (LLMs). Existing MIAs designed for
LMs can be classified into two categories: reference-free and reference-based
attacks. They are both based on the hypothesis that training records
consistently strike a higher probability of being sampled. Nevertheless, this
hypothesis heavily relies on the overfitting of target models, which will be
mitigated by multiple regularization methods and the generalization of LLMs.
The reference-based attack seems to achieve promising effectiveness in LLMs,
which measures a more reliable membership signal by comparing the probability
discrepancy between the target model and the reference model. However, the
performance of reference-based attack is highly dependent on a reference
dataset that closely resembles the training dataset, which is usually
inaccessible in the practical scenario. Overall, existing MIAs are unable to
effectively unveil privacy leakage over practical fine-tuned LLMs that are
overfitting-free and private. We propose a Membership Inference Attack based on
Self-calibrated Probabilistic Variation (SPV-MIA). Specifically, since
memorization in LLMs is inevitable during the training process and occurs
before overfitting, we introduce a more reliable membership signal,
probabilistic variation, which is based on memorization rather than
overfitting. Furthermore, we introduce a self-prompt approach, which constructs
the dataset to fine-tune the reference model by prompting the target LLM
itself. In this manner, the adversary can collect a dataset with a similar
distribution from public APIs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08360">The Transient Nature of Emergent In-Context Learning in Transformers. (arXiv:2311.08360v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1">Aaditya K. Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1">Stephanie C.Y. Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Moskovitz_T/0/1/0/all/0/1">Ted Moskovitz</a>, <a href="http://arxiv.org/find/cs/1/au:+Grant_E/0/1/0/all/0/1">Erin Grant</a>, <a href="http://arxiv.org/find/cs/1/au:+Saxe_A/0/1/0/all/0/1">Andrew M. Saxe</a>, <a href="http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1">Felix Hill</a></p>
<p>Transformer neural networks can exhibit a surprising capacity for in-context
learning (ICL) despite not being explicitly trained for it. Prior work has
provided a deeper understanding of how ICL emerges in transformers, e.g.
through the lens of mechanistic interpretability, Bayesian inference, or by
examining the distributional properties of training data. However, in each of
these cases, ICL is treated largely as a persistent phenomenon; namely, once
ICL emerges, it is assumed to persist asymptotically. Here, we show that the
emergence of ICL during transformer training is, in fact, often transient. We
train transformers on synthetic data designed so that both ICL and in-weights
learning (IWL) strategies can lead to correct predictions. We find that ICL
first emerges, then disappears and gives way to IWL, all while the training
loss decreases, indicating an asymptotic preference for IWL. The transient
nature of ICL is observed in transformers across a range of model sizes and
datasets, raising the question of how much to "overtrain" transformers when
seeking compact, cheaper-to-run models. We find that L2 regularization may
offer a path to more persistent ICL that removes the need for early stopping
based on ICL-style validation tasks. Finally, we present initial evidence that
ICL transience may be caused by competition between ICL and IWL circuits.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04344">Enhancing Medical Task Performance in GPT-4V: A Comprehensive Study on Prompt Engineering Strategies. (arXiv:2312.04344v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1">Pengcheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Ziyan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1">Zhongying Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1">Tianbin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1">Yanzhou Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haoyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1">Jin Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Junjun He</a></p>
<p>OpenAI's latest large vision-language model (LVLM), GPT-4V(ision), has piqued
considerable interest for its potential in medical applications. Despite its
promise, recent studies and internal reviews highlight its underperformance in
specialized medical tasks. This paper explores the boundary of GPT-4V's
capabilities in medicine, particularly in processing complex imaging data from
endoscopies, CT scans, and MRIs etc. Leveraging open-source datasets, we
assessed its foundational competencies, identifying substantial areas for
enhancement. Our research emphasizes prompt engineering, an often-underutilized
strategy for improving AI responsiveness. Through iterative testing, we refined
the model's prompts, significantly improving its interpretative accuracy and
relevance in medical imaging. From our comprehensive evaluations, we distilled
10 effective prompt engineering techniques, each fortifying GPT-4V's medical
acumen. These methodical enhancements facilitate more reliable, precise, and
clinically valuable insights from GPT-4V, advancing its operability in critical
healthcare environments. Our findings are pivotal for those employing AI in
medicine, providing clear, actionable guidance on harnessing GPT-4V's full
diagnostic potential.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04691">Simul-LLM: A Framework for Exploring High-Quality Simultaneous Translation with Large Language Models. (arXiv:2312.04691v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Agostinelli_V/0/1/0/all/0/1">Victor Agostinelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Wild_M/0/1/0/all/0/1">Max Wild</a>, <a href="http://arxiv.org/find/cs/1/au:+Raffel_M/0/1/0/all/0/1">Matthew Raffel</a>, <a href="http://arxiv.org/find/cs/1/au:+Fuad_K/0/1/0/all/0/1">Kazi Ahmed Asif Fuad</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lizhong Chen</a></p>
<p>Large language models (LLMs) with billions of parameters and pretrained on
massive amounts of data are now capable of near or better than state-of-the-art
performance in a variety of downstream natural language processing tasks.
Neural machine translation (NMT) is one such task that LLMs have been applied
to with great success. However, little research has focused on applying LLMs to
the more difficult subset of NMT called simultaneous translation (SimulMT),
where translation begins before the entire source context is available to the
model. In this paper, we address key challenges facing LLMs fine-tuned for
SimulMT, validate classical SimulMT concepts and practices in the context of
LLMs, explore adapting LLMs that are fine-tuned for NMT to the task of SimulMT,
and introduce Simul-LLM, the first open-source fine-tuning and evaluation
pipeline development framework for LLMs focused on SimulMT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04837">Localized Symbolic Knowledge Distillation for Visual Commonsense Models. (arXiv:2312.04837v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Jae Sung Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1">Jack Hessel</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandu_K/0/1/0/all/0/1">Khyathi Raghavi Chandu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1">Paul Pu Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1">Ximing Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1">Peter West</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Youngjae Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1">Qiuyuan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jianfeng Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1">Ali Farhadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1">Yejin Choi</a></p>
<p>Instruction following vision-language (VL) models offer a flexible interface
that supports a broad range of multimodal tasks in a zero-shot fashion.
However, interfaces that operate on full images do not directly enable the user
to "point to" and access specific regions within images. This capability is
important not only to support reference-grounded VL benchmarks, but also, for
practical applications that require precise within-image reasoning. We build
Localized Visual Commonsense models, which allow users to specify (multiple)
regions as input. We train our model by sampling localized commonsense
knowledge from a large language model (LLM): specifically, we prompt an LLM to
collect commonsense knowledge given a global literal image description and a
local literal region description automatically generated by a set of VL models.
With a separately trained critic model that selects high-quality examples, we
find that training on the localized commonsense corpus can successfully distill
existing VL models to support a reference-as-input interface. Empirical results
and human evaluations in a zero-shot setup demonstrate that our distillation
method results in more precise VL models of reasoning compared to a baseline of
passing a generated referring expression to an LLM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05180">PathFinder: Guided Search over Multi-Step Reasoning Paths. (arXiv:2312.05180v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Golovneva_O/0/1/0/all/0/1">Olga Golovneva</a>, <a href="http://arxiv.org/find/cs/1/au:+OBrien_S/0/1/0/all/0/1">Sean O&#x27;Brien</a>, <a href="http://arxiv.org/find/cs/1/au:+Pasunuru_R/0/1/0/all/0/1">Ramakanth Pasunuru</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tianlu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1">Luke Zettlemoyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Fazel_Zarandi_M/0/1/0/all/0/1">Maryam Fazel-Zarandi</a>, <a href="http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1">Asli Celikyilmaz</a></p>
<p>With recent advancements in large language models, methods like
chain-of-thought prompting to elicit reasoning chains have been shown to
improve results on reasoning tasks. However, tasks that require multiple steps
of reasoning still pose significant challenges to state-of-the-art models.
Drawing inspiration from the beam search algorithm, we propose PathFinder, a
tree-search-based reasoning path generation approach. It enhances diverse
branching and multi-hop reasoning through the integration of dynamic decoding,
enabled by varying sampling methods and parameters. Using constrained
reasoning, PathFinder integrates novel quality constraints, pruning, and
exploration methods to enhance the efficiency and the quality of generation.
Moreover, it includes scoring and ranking features to improve candidate
selection. Our approach outperforms competitive baselines on three complex
arithmetic and commonsense reasoning tasks by 6% on average. Our model
generalizes well to longer, unseen reasoning chains, reflecting similar
complexities to beam search with large branching factors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05488">Can Large Language Models Serve as Rational Players in Game Theory? A Systematic Analysis. (arXiv:2312.05488v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1">Caoyun Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jindou Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1">Yaohui Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1">Hao He</a></p>
<p>Game theory, as an analytical tool, is frequently utilized to analyze human
behavior in social science research. With the high alignment between the
behavior of Large Language Models (LLMs) and humans, a promising research
direction is to employ LLMs as substitutes for humans in game experiments,
enabling social science research. However, despite numerous empirical
researches on the combination of LLMs and game theory, the capability
boundaries of LLMs in game theory remain unclear. In this research, we endeavor
to systematically analyze LLMs in the context of game theory. Specifically,
rationality, as the fundamental principle of game theory, serves as the metric
for evaluating players' behavior -- building a clear desire, refining belief
about uncertainty, and taking optimal actions. Accordingly, we select three
classical games (dictator game, Rock-Paper-Scissors, and ring-network game) to
analyze to what extent LLMs can achieve rationality in these three aspects. The
experimental results indicate that even the current state-of-the-art LLM
(GPT-4) exhibits substantial disparities compared to humans in game theory. For
instance, LLMs struggle to build desires based on uncommon preferences, fail to
refine belief from many simple patterns, and may overlook or modify refined
belief when taking actions. Therefore, we consider that introducing LLMs into
game experiments in the field of social science should be approached with
greater caution.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05497">History Matters: Temporal Knowledge Editing in Large Language Model. (arXiv:2312.05497v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1">Xunjian Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1">Jin Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Liming Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1">Xiaojun Wan</a></p>
<p>The imperative task of revising or updating the knowledge stored within large
language models arises from two distinct sources: intrinsic errors inherent in
the model which should be corrected and outdated knowledge due to external
shifts in the real world which should be updated. Prevailing efforts in model
editing conflate these two distinct categories of edits arising from distinct
reasons and directly modify the original knowledge in models into new
knowledge. However, we argue that preserving the model's original knowledge
remains pertinent. Specifically, if a model's knowledge becomes outdated due to
evolving worldly dynamics, it should retain recollection of the historical
knowledge while integrating the newfound knowledge. In this work, we introduce
the task of Temporal Knowledge Editing (TKE) and establish a benchmark AToKe
(Assessment of TempOral Knowledge Editing) to evaluate current model editing
methods. We find that while existing model editing methods are effective at
making models remember new knowledge, the edited model catastrophically forgets
historical knowledge. To address this gap, we propose a simple and general
framework termed Multi-Editing with Time Objective (METO) for enhancing
existing editing models, which edits both historical and new knowledge
concurrently and optimizes the model's prediction for the time of each fact.
Our assessments demonstrate that while AToKe is still difficult, METO maintains
the effectiveness of learning new knowledge and meanwhile substantially
improves the performance of edited models on utilizing historical knowledge.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05603">Sim-GPT: Text Similarity via GPT Annotated Data. (arXiv:2312.05603v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuhe Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_B/0/1/0/all/0/1">Beiming Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shengyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaoya Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiwei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1">Fei Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guoyin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1">Eduard Hovy</a></p>
<p>Due to the lack of a large collection of high-quality labeled sentence pairs
with textual similarity scores, existing approaches for Semantic Textual
Similarity (STS) mostly rely on unsupervised techniques or training signals
that are only partially correlated with textual similarity, e.g., NLI-based
datasets. To tackle this issue, in this paper, we propose the strategy of
measuring text similarity via GPT annotated data (Sim-GPT for short). The core
idea of Sim-GPT is to generate data with STS labels using GPT-4, based on which
an STS model is trained. Sim-GPT framework utilizes LLMs to provide a
substantial amount of reliable annotated data filling the gap of the lack of
training signals for STS. Sim-GPT is trained on a one-time generated dataset
using BERT or RoBERTa as the backbone, which offers long-term savings in cost
and speed compared to repeatedly invoking LLMs for each sentence pair. Trained
on the examples from GPT-4 (371K), Sim-GPT yields SOTA performances on the
widely-used seven STS benchmarks: +0.99 over supervised-SimCSE, and +0.42 over
the current SOTA PromCSE model. To encourage further advancements of the field,
we release both models and the 371K annotated examples from GPT-4. Code, models
and annotated data are available at: https://github.com/ShuheWang1998/Sim-GPT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05662">Understanding the Effect of Model Compression on Social Bias in Large Language Models. (arXiv:2312.05662v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Goncalves_G/0/1/0/all/0/1">Gustavo Gon&#xe7;alves</a>, <a href="http://arxiv.org/find/cs/1/au:+Strubell_E/0/1/0/all/0/1">Emma Strubell</a></p>
<p>Large Language Models (LLMs) trained with self-supervision on vast corpora of
web text fit to the social biases of that text. Without intervention, these
social biases persist in the model's predictions in downstream tasks, leading
to representational harm. Many strategies have been proposed to mitigate the
effects of inappropriate social biases learned during pretraining.
Simultaneously, methods for model compression have become increasingly popular
to reduce the computational burden of LLMs. Despite the popularity and need for
both approaches, little work has been done to explore the interplay between
these two. We perform a carefully controlled study of the impact of model
compression via quantization and knowledge distillation on measures of social
bias in LLMs. Longer pretraining and larger models led to higher social bias,
and quantization showed a regularizer effect with its best trade-off around 20%
of the original pretraining time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06363">MMICT: Boosting Multi-Modal Fine-Tuning with In-Context Examples. (arXiv:2312.06363v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1">Enwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yuting Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Ke Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xing Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hui Li</a></p>
<p>Although In-Context Learning (ICL) brings remarkable performance gains to
Large Language Models (LLMs), the improvements remain lower than fine-tuning on
downstream tasks. This paper introduces Multi-Modal In-Context Tuning (MMICT),
a novel multi-modal fine-tuning paradigm that boosts multi-modal fine-tuning by
fully leveraging the promising ICL capability of multi-modal LLMs (MM-LLMs). We
propose the Multi-Modal Hub (M-Hub), a unified module that captures various
multi-modal features according to different inputs and objectives. Based on
M-Hub, MMICT enables MM-LLMs to learn from in-context visual-guided textual
features and subsequently generate outputs conditioned on the textual-guided
visual features. Moreover, leveraging the flexibility of M-Hub, we design a
variety of in-context demonstrations. Extensive experiments on a diverse range
of downstream multi-modal tasks demonstrate that MMICT significantly
outperforms traditional fine-tuning strategy and the vanilla ICT method that
directly takes the concatenation of all information from different modalities
as input.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06635">Gated Linear Attention Transformers with Hardware-Efficient Training. (arXiv:2312.06635v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Songlin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bailin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yikang Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1">Rameswar Panda</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Yoon Kim</a></p>
<p>Transformers with linear attention allow for efficient parallel training but
can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden
states, thus enjoying linear (with respect to output length) inference
complexity. Recent works such as RetNet (Sun et al., 2023) and TransNormerLLM
(Qin et al., 2023a) observe that adding a global decay term to the additive RNN
update rule greatly improves performance, sometimes outperforming standard
Transformers with softmax attention when trained at scale. In this work we show
that adding a data-dependent gating mechanism further improves performance. We
derive a parallel form of this gated linear attention layer that enables
efficient training. However, a straightforward, numerically stable
implementation of this parallel form requires generalized matrix
multiplications in log-space for numerical stability, and thus cannot take
advantage of tensor cores on modern GPUs which are optimized for standard
matrix multiplications. We develop a hardware-efficient version of the parallel
form that can still make use of tensor cores through block-parallel
computations over sequence chunks. Experiments on moderate-scale language
modeling (340M-parameter models trained on 15B tokens, 1.3B-parameter models
trained on 100B tokens) show that gated linear attention (GLA) Transformers
perform competitively against a strong LLaMA-architecture Transformer baseline
(Touvron et al., 2023) as well as Mamba (Gu &amp; Dao, 2023), a recently introduced
state-space model with a data-dependent state transition mechanism. For
training speed, our Triton-based implementation performs comparably to
CUDA-optimized FlashAttention-2 (Dao, 2023) under the regular 2048 training
length setting, while outperforming FlashAttention-2 when training on longer
sequences beyond 4096.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06648">Dense X Retrieval: What Retrieval Granularity Should We Use?. (arXiv:2312.06648v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hongwei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Sihao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1">Wenhao Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1">Kaixin Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xinran Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1">Dong Yu</a></p>
<p>Dense retrieval has become a prominent method to obtain relevant context or
world knowledge in open-domain NLP tasks. When we use a learned dense retriever
on a retrieval corpus at inference time, an often-overlooked design choice is
the retrieval unit in which the corpus is indexed, e.g. document, passage, or
sentence. We discover that the retrieval unit choice significantly impacts the
performance of both retrieval and downstream tasks. Distinct from the typical
approach of using passages or sentences, we introduce a novel retrieval unit,
proposition, for dense retrieval. Propositions are defined as atomic
expressions within text, each encapsulating a distinct factoid and presented in
a concise, self-contained natural language format. We conduct an empirical
comparison of different retrieval granularity. Our results reveal that
proposition-based retrieval significantly outperforms traditional passage or
sentence-based methods in dense retrieval. Moreover, retrieval by proposition
also enhances the performance of downstream QA tasks, since the retrieved texts
are more condensed with question-relevant information, reducing the need for
lengthy input tokens and minimizing the inclusion of extraneous, irrelevant
information.
</p>
</p>
</div>

    </div>
    </body>
    