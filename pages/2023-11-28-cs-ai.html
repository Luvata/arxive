<!DOCTYPE html>
<html>
<head>
<title>2023-11-28-cs-ai</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2311.13614">HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction Data. (arXiv:2311.13614v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1">Qifan Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Juncheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1">Longhui Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1">Liang Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1">Wentao Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1">Bosheng Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1">Siliang Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1">Qi Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1">Yueting Zhuang</a></p>
<p>Multi-modal Large Language Models (MLLMs) tuned on machine-generated
instruction-following data have demonstrated remarkable performance in various
multi-modal understanding and generation tasks. However, the hallucinations
inherent in machine-generated data, which could lead to hallucinatory outputs
in MLLMs, remain under-explored. This work aims to investigate various
hallucinations (i.e., object, relation, attribute hallucinations) and mitigate
those hallucinatory toxicities in large-scale machine-generated visual
instruction datasets. Drawing on the human ability to identify factual errors,
we present a novel hallucination detection and elimination framework,
HalluciDoctor, based on the cross-checking paradigm. We use our framework to
identify and eliminate hallucinations in the training data automatically.
Interestingly, HalluciDoctor also indicates that spurious correlations arising
from long-tail object co-occurrences contribute to hallucinations. Based on
that, we execute counterfactual visual instruction expansion to balance data
distribution, thereby enhancing MLLMs' resistance to hallucinations.
Comprehensive experiments on hallucination evaluation benchmarks show that our
method successfully mitigates 44.6% hallucinations relatively and maintains
competitive performance compared to LLaVA.The source code will be released at
\url{https://github.com/Yuqifan1117/HalluciDoctor}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13626">Physics-driven generative adversarial networks empower single-pixel infrared hyperspectral imaging. (arXiv:2311.13626v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1">Dong-Yin Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Bie_S/0/1/0/all/0/1">Shu-Hang Bie</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1">Xi-Hao Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Yu_W/0/1/0/all/0/1">Wen-Kai Yu</a></p>
<p>A physics-driven generative adversarial network (GAN) was established here
for single-pixel hyperspectral imaging (HSI) in the infrared spectrum, to
eliminate the extensive data training work required by traditional data-driven
model. Within the GAN framework, the physical process of single-pixel imaging
(SPI) was integrated into the generator, and the actual and estimated
one-dimensional (1D) bucket signals were employed as constraints in the
objective function to update the network's parameters and optimize the
generator with the assistance of the discriminator. In comparison to
single-pixel infrared HSI methods based on compressed sensing and
physics-driven convolution neural networks, our physics-driven GAN-based
single-pixel infrared HSI can achieve higher imaging performance but with fewer
measurements. We believe that this physics-driven GAN will promote practical
applications of computational imaging, especially various SPI-based techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13627">Vamos: Versatile Action Models for Video Understanding. (arXiv:2311.13627v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shijie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1">Qi Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Do_M/0/1/0/all/0/1">Minh Quan Do</a>, <a href="http://arxiv.org/find/cs/1/au:+Agarwal_N/0/1/0/all/0/1">Nakul Agarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kwonjoon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1">Chen Sun</a></p>
<p>What makes good video representations for video understanding, such as
anticipating future activities, or answering video-conditioned questions? While
earlier approaches focus on end-to-end learning directly from video pixels, we
propose to revisit text-based representations, such as discrete action labels,
or free-form video captions, which are interpretable and can be directly
consumed by large language models (LLMs). Intuitively, different video
understanding tasks may require representations that are complementary and at
different granularities. To this end, we propose versatile action models
(Vamos), a learning framework powered by a large language model as the
"reasoner", and can flexibly leverage visual embeddings, action labels, and
free-form descriptions extracted from videos as its input. We evaluate Vamos on
four complementary video understanding benchmarks, Ego4D, Next-QA, IntentQA,
and EgoSchema, on its capability to model temporal dynamics, encode visual
history, and perform reasoning. Surprisingly, we observe that text-based
representations consistently achieve competitive performance on all benchmarks,
and that visual embeddings provide marginal or no performance improvement,
demonstrating the effectiveness of text-based video representation in the LLM
era. We perform extensive ablation study and qualitative analysis to support
our observations, and achieve state-of-the-art performance on three benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13628">Prompt Risk Control: A Rigorous Framework for Responsible Deployment of Large Language Models. (arXiv:2311.13628v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zollo_T/0/1/0/all/0/1">Thomas P. Zollo</a>, <a href="http://arxiv.org/find/cs/1/au:+Morrill_T/0/1/0/all/0/1">Todd Morrill</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1">Zhun Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Snell_J/0/1/0/all/0/1">Jake C. Snell</a>, <a href="http://arxiv.org/find/cs/1/au:+Pitassi_T/0/1/0/all/0/1">Toniann Pitassi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1">Richard Zemel</a></p>
<p>The recent explosion in the capabilities of large language models has led to
a wave of interest in how best to prompt a model to perform a given task. While
it may be tempting to simply choose a prompt based on average performance on a
validation set, this can lead to a deployment where unexpectedly poor responses
are generated, especially for the worst-off users. To mitigate this prospect,
we propose Prompt Risk Control, a lightweight framework for selecting a prompt
based on rigorous upper bounds on families of informative risk measures. We
offer methods for producing bounds on a diverse set of metrics, including
quantities that measure worst-case responses and disparities in generation
quality across the population of users. In addition, we extend the underlying
statistical bounding techniques to accommodate the possibility of distribution
shifts in deployment. Experiments on applications such as open-ended chat,
medical question summarization, and code generation highlight how such a
framework can foster responsible deployment by reducing the risk of the worst
outcomes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13664">Sample as You Infer: Predictive Coding With Langevin Dynamics. (arXiv:2311.13664v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zahid_U/0/1/0/all/0/1">Umais Zahid</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1">Qinghai Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Fountas_Z/0/1/0/all/0/1">Zafeirios Fountas</a></p>
<p>We present a novel algorithm for parameter learning in generic deep
generative models that builds upon the predictive coding (PC) framework of
computational neuroscience. Our approach modifies the standard PC algorithm to
bring performance on-par and exceeding that obtained from standard variational
auto-encoder (VAE) training. By injecting Gaussian noise into the PC inference
procedure we re-envision it as an overdamped Langevin sampling, which
facilitates optimisation with respect to a tight evidence lower bound (ELBO).
We improve the resultant encoder-free training method by incorporating an
encoder network to provide an amortised warm-start to our Langevin sampling and
test three different objectives for doing so. Finally, to increase robustness
to the sampling step size and reduce sensitivity to curvature, we validate a
lightweight and easily computable form of preconditioning, inspired by Riemann
Manifold Langevin and adaptive optimizers from the SGD literature. We compare
against VAEs by training like-for-like generative models using our technique
against those trained with standard reparameterisation-trick-based ELBOs. We
observe our method out-performs or matches performance across a number of
metrics, including sample quality, while converging in a fraction of the number
of SGD training iterations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13668">MAIRA-1: A specialised large multimodal model for radiology report generation. (arXiv:2311.13668v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hyland_S/0/1/0/all/0/1">Stephanie L. Hyland</a>, <a href="http://arxiv.org/find/cs/1/au:+Bannur_S/0/1/0/all/0/1">Shruthi Bannur</a>, <a href="http://arxiv.org/find/cs/1/au:+Bouzid_K/0/1/0/all/0/1">Kenza Bouzid</a>, <a href="http://arxiv.org/find/cs/1/au:+Castro_D/0/1/0/all/0/1">Daniel C. Castro</a>, <a href="http://arxiv.org/find/cs/1/au:+Ranjit_M/0/1/0/all/0/1">Mercy Ranjit</a>, <a href="http://arxiv.org/find/cs/1/au:+Schwaighofer_A/0/1/0/all/0/1">Anton Schwaighofer</a>, <a href="http://arxiv.org/find/cs/1/au:+Perez_Garcia_F/0/1/0/all/0/1">Fernando P&#xe9;rez-Garc&#xed;a</a>, <a href="http://arxiv.org/find/cs/1/au:+Salvatelli_V/0/1/0/all/0/1">Valentina Salvatelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Srivastav_S/0/1/0/all/0/1">Shaury Srivastav</a>, <a href="http://arxiv.org/find/cs/1/au:+Thieme_A/0/1/0/all/0/1">Anja Thieme</a>, <a href="http://arxiv.org/find/cs/1/au:+Codella_N/0/1/0/all/0/1">Noel Codella</a>, <a href="http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1">Matthew P. Lungren</a>, <a href="http://arxiv.org/find/cs/1/au:+Wetscherek_M/0/1/0/all/0/1">Maria Teodora Wetscherek</a>, <a href="http://arxiv.org/find/cs/1/au:+Oktay_O/0/1/0/all/0/1">Ozan Oktay</a>, <a href="http://arxiv.org/find/cs/1/au:+Alvarez_Valle_J/0/1/0/all/0/1">Javier Alvarez-Valle</a></p>
<p>We present a radiology-specific multimodal model for the task for generating
radiological reports from chest X-rays (CXRs). Our work builds on the idea that
large language model(s) can be equipped with multimodal capabilities through
alignment with pre-trained vision encoders. On natural images, this has been
shown to allow multimodal models to gain image understanding and description
capabilities. Our proposed model (MAIRA-1) leverages a CXR-specific image
encoder in conjunction with a fine-tuned large language model based on
Vicuna-7B, and text-based data augmentation, to produce reports with
state-of-the-art quality. In particular, MAIRA-1 significantly improves on the
radiologist-aligned RadCliQ metric and across all lexical metrics considered.
Manual review of model outputs demonstrates promising fluency and accuracy of
generated reports while uncovering failure modes not captured by existing
evaluation practices. More information and resources can be found on the
project website: https://aka.ms/maira.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13691">Next-Generation Earth System Models: Towards Reliable Hybrid Models for Weather and Climate Applications. (arXiv:2311.13691v1 [physics.ao-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Beucler_T/0/1/0/all/0/1">Tom Beucler</a>, <a href="http://arxiv.org/find/physics/1/au:+Koch_E/0/1/0/all/0/1">Erwan Koch</a>, <a href="http://arxiv.org/find/physics/1/au:+Kotlarski_S/0/1/0/all/0/1">Sven Kotlarski</a>, <a href="http://arxiv.org/find/physics/1/au:+Leutwyler_D/0/1/0/all/0/1">David Leutwyler</a>, <a href="http://arxiv.org/find/physics/1/au:+Michel_A/0/1/0/all/0/1">Adrien Michel</a>, <a href="http://arxiv.org/find/physics/1/au:+Koh_J/0/1/0/all/0/1">Jonathan Koh</a></p>
<p>We review how machine learning has transformed our ability to model the Earth
system, and how we expect recent breakthroughs to benefit end-users in
Switzerland in the near future.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13693">Scalable CP Decomposition for Tensor Learning using GPU Tensor Cores. (arXiv:2311.13693v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zeliang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhuo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1">Susan Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhiyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yifan Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1">Chen Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Chenliang Xu</a></p>
<p>CP decomposition is a powerful tool for data science, especially gene
analysis, deep learning, and quantum computation. However, the application of
tensor decomposition is largely hindered by the exponential increment of the
computational complexity and storage consumption with the size of tensors.
While the data in our real world is usually presented as trillion- or even
exascale-scale tensors, existing work can only support billion-scale scale
tensors. In our work, we propose the Exascale-Tensor to mitigate the
significant gap. Specifically, we propose a compression-based tensor
decomposition framework, namely the exascale-tensor, to support exascale tensor
decomposition. Then, we carefully analyze the inherent parallelism and propose
a bag of strategies to improve computational efficiency. Last, we conduct
experiments to decompose tensors ranging from million-scale to trillion-scale
for evaluation. Compared to the baselines, the exascale-tensor supports 8,000x
larger tensors and a speedup up to 6.95x. We also apply our method to two
real-world applications, including gene analysis and tensor layer neural
networks, of which the numeric results demonstrate the scalability and
effectiveness of our method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13712">Data Acquisition: A New Frontier in Data-centric AI. (arXiv:2311.13712v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lingjiao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Acun_B/0/1/0/all/0/1">Bilge Acun</a>, <a href="http://arxiv.org/find/cs/1/au:+Ardalani_N/0/1/0/all/0/1">Newsha Ardalani</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yifan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_F/0/1/0/all/0/1">Feiyang Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_H/0/1/0/all/0/1">Hanrui Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1">Yongchan Kwon</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1">Ruoxi Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1">Carole-Jean Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1">Matei Zaharia</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1">James Zou</a></p>
<p>As Machine Learning (ML) systems continue to grow, the demand for relevant
and comprehensive datasets becomes imperative. There is limited study on the
challenges of data acquisition due to ad-hoc processes and lack of consistent
methodologies. We first present an investigation of current data marketplaces,
revealing lack of platforms offering detailed information about datasets,
transparent pricing, standardized data formats. With the objective of inciting
participation from the data-centric AI community, we then introduce the DAM
challenge, a benchmark to model the interaction between the data providers and
acquirers. The benchmark was released as a part of DataPerf. Our evaluation of
the submitted strategies underlines the need for effective data acquisition
strategies in ML.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13718">A Unified Approach to Count-Based Weakly-Supervised Learning. (arXiv:2311.13718v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shukla_V/0/1/0/all/0/1">Vinay Shukla</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1">Zhe Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_K/0/1/0/all/0/1">Kareem Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Broeck_G/0/1/0/all/0/1">Guy Van den Broeck</a></p>
<p>High-quality labels are often very scarce, whereas unlabeled data with
inferred weak labels occurs more naturally. In many cases, these weak labels
dictate the frequency of each respective class over a set of instances. In this
paper, we develop a unified approach to learning from such weakly-labeled data,
which we call count-based weakly-supervised learning. At the heart of our
approach is the ability to compute the probability of exactly k out of n
outputs being set to true. This computation is differentiable, exact, and
efficient. Building upon the previous computation, we derive a count loss
penalizing the model for deviations in its distribution from an arithmetic
constraint defined over label counts. We evaluate our approach on three common
weakly-supervised learning paradigms and observe that our proposed approach
achieves state-of-the-art or highly competitive results across all three of the
paradigms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13719">Deep learning-based instance segmentation for the precise automated quantification of digital breast cancer immunohistochemistry images. (arXiv:2311.13719v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Priego_Torresa_B/0/1/0/all/0/1">Blanca Maria Priego-Torresa</a>, <a href="http://arxiv.org/find/eess/1/au:+Lobato_Delgado_B/0/1/0/all/0/1">Barbara Lobato-Delgado</a>, <a href="http://arxiv.org/find/eess/1/au:+Atienza_Cuevas_L/0/1/0/all/0/1">Lidia Atienza-Cuevas</a>, <a href="http://arxiv.org/find/eess/1/au:+Sanchez_Morillo_D/0/1/0/all/0/1">Daniel Sanchez-Morillo</a></p>
<p>The quantification of biomarkers on immunohistochemistry breast cancer images
is essential for defining appropriate therapy for breast cancer patients, as
well as for extracting relevant information on disease prognosis. This is an
arduous and time-consuming task that may introduce a bias in the results due to
intra- and inter-observer variability which could be alleviated by making use
of automatic quantification tools. However, this is not a simple processing
task given the heterogeneity of breast tumors that results in non-uniformly
distributed tumor cells exhibiting different staining colors and intensity,
size, shape, and texture, of the nucleus, cytoplasm and membrane. In this
research work, we demonstrate the feasibility of using a deep learning-based
instance segmentation architecture for the automatic quantification of both
nuclear and membrane biomarkers applied to IHC-stained slides. We have solved
the cumbersome task of training set generation with the design and
implementation of a web platform, which has served as a hub for communication
and feedback between researchers and pathologists as well as a system for the
validation of the automatic image processing models. Through this tool, we have
collected annotations over samples of HE, ER and Ki-67 (nuclear biomarkers) and
HER2 (membrane biomarker) IHC-stained images. Using the same deep learning
network architecture, we have trained two models, so-called nuclei- and
membrane-aware segmentation models, which, once successfully validated, have
revealed to be a promising method to segment nuclei instances in IHC-stained
images. The quantification method proposed in this work has been integrated
into the developed web platform and is currently being used as a
decision-support tool by pathologists.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13720">Towards More Likely Models for AI Planning. (arXiv:2311.13720v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Caglar_T/0/1/0/all/0/1">Turgay Caglar</a>, <a href="http://arxiv.org/find/cs/1/au:+Belhaj_S/0/1/0/all/0/1">Sirine Belhaj</a>, <a href="http://arxiv.org/find/cs/1/au:+Chakraborti_T/0/1/0/all/0/1">Tathagata Chakraborti</a>, <a href="http://arxiv.org/find/cs/1/au:+Katz_M/0/1/0/all/0/1">Michael Katz</a>, <a href="http://arxiv.org/find/cs/1/au:+Sreedharan_S/0/1/0/all/0/1">Sarath Sreedharan</a></p>
<p>This is the first work to look at the application of large language models
(LLMs) for the purpose of model space edits in automated planning tasks. To set
the stage for this sangam, we explore two different flavors of model space
problems that have been studied in the AI planning literature and explore the
effect of an LLM on those tasks. We empirically demonstrate how the performance
of an LLM contrasts with combinatorial search (CS) - an approach that has been
traditionally used to solve model space tasks in planning, both with the LLM in
the role of a standalone model space reasoner as well as in the role of a
statistical signal in concert with the CS approach as part of a two-stage
process. Our experiments show promising results suggesting further forays of
LLMs into the exciting world of model space reasoning for planning tasks in the
future.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13721">Nova$^+$: Generative Language Models for Binaries. (arXiv:2311.13721v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1">Nan Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chengxiao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1">Kevin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiangzhe Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1">Lin Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiangyu Zhang</a></p>
<p>Generative large language models (LLMs) pre-trained on code have shown
impressive effectiveness in code generation, program repair, and document
analysis. However, existing generative LLMs focus on source code and are not
specialized for binaries. There are three main challenges for LLMs to model and
learn binary code: hex-decimal values, complex global dependencies, and
compiler optimization levels.To bring the benefit of LLMs to the binary domain,
we develop Nova and Nova$^+$, which are LLMs pre-trained on binary corpora.
Nova is pre-trained with the standard language modeling task, showing
significantly better capability on five benchmarks for three downstream tasks:
binary code similarity detection (BCSD), binary code translation (BCT), and
binary code recovery (BCR), over GPT-3.5 and other existing techniques. We
build Nova$^+$ to further boost Nova using two new pre-training tasks, i.e.,
optimization generation and optimization level prediction, which are designed
to learn binary optimization and align equivalent binaries. Nova$^+$ shows
overall the best performance for all three downstream tasks on five benchmarks,
demonstrating the contributions of the new pre-training tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13725">Studying Artist Sentiments around AI-generated Artwork. (arXiv:2311.13725v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ali_S/0/1/0/all/0/1">Safinah Ali</a>, <a href="http://arxiv.org/find/cs/1/au:+Breazeal_C/0/1/0/all/0/1">Cynthia Breazeal</a></p>
<p>Art created using generated Artificial Intelligence has taken the world by
storm and generated excitement for many digital creators and technologists.
However, the reception and reaction from artists have been mixed. Concerns
about plagiarizing their artworks and styles for datasets and uncertainty
around the future of digital art sparked movements in artist communities
shunning the use of AI for generating art and protecting artists' rights.
Collaborating with these tools for novel creative use cases also sparked hope
from some creators. Artists are an integral stakeholder in the rapidly evolving
digital creativity industry and understanding their concerns and hopes inform
responsible development and use of creativity support tools. In this work, we
study artists' sentiments about AI-generated art. We interviewed 7 artists and
analyzed public posts from artists on social media platforms Reddit, Twitter
and Artstation. We report artists' main concerns and hopes around AI-generated
artwork, informing a way forward for inclusive development of these tools.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13731">A Survey of Blockchain, Artificial Intelligence, and Edge Computing for Web 3.0. (arXiv:2311.13731v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jianjun Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1">Fan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jinyuan Chen</a></p>
<p>Web 3.0, as the third generation of the World Wide Web, aims to solve
contemporary problems of trust, centralization, and data ownership. Driven by
the latest advances in cutting-edge technologies, Web 3.0 is moving towards a
more open, decentralized, intelligent, and interconnected network. However,
increasingly widespread data breaches have raised awareness of online privacy
and security of personal data. Additionally, since Web 3.0 is a sophisticated
and complex convergence, the technical details behind it are not as clear as
the characteristics it presents. In this survey, we conduct an in-depth
exploration of Web 3.0 from the perspectives of blockchain, artificial
intelligence, and edge computing. Specifically, we begin with summarizing the
evolution of the Internet and providing an overview of these three key
technological factors. Afterward, we provide a thorough analysis of each
technology separately, including its relevance to Web 3.0, key technology
components, and practical applications. We also propose decentralized storage
and computing solutions by exploring the integration of technologies. Finally,
we highlight the key challenges alongside potential research directions.
Through the combination and mutual complementation of multiple technologies,
Web 3.0 is expected to return more control and ownership of data and digital
assets back to users.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13739">OASIS: Offsetting Active Reconstruction Attacks in Federated Learning. (arXiv:2311.13739v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jeter_T/0/1/0/all/0/1">Tre&#x27; R. Jeter</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Truc Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Alharbi_R/0/1/0/all/0/1">Raed Alharbi</a>, <a href="http://arxiv.org/find/cs/1/au:+Thai_M/0/1/0/all/0/1">My T. Thai</a></p>
<p>Federated Learning (FL) has garnered significant attention for its potential
to protect user privacy while enhancing model training efficiency. However,
recent research has demonstrated that FL protocols can be easily compromised by
active reconstruction attacks executed by dishonest servers. These attacks
involve the malicious modification of global model parameters, allowing the
server to obtain a verbatim copy of users' private data by inverting their
gradient updates. Tackling this class of attack remains a crucial challenge due
to the strong threat model. In this paper, we propose OASIS, a defense
mechanism based on image augmentation that effectively counteracts active
reconstruction attacks while preserving model performance. We first uncover the
core principle of gradient inversion that enables these attacks and
theoretically identify the main conditions by which the defense can be robust
regardless of the attack strategies. We then construct OASIS with image
augmentation showing that it can undermine the attack principle. Comprehensive
evaluations demonstrate the efficacy of OASIS highlighting its feasibility as a
solution.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13743">FinMe: A Performance-Enhanced Large Language Model Trading Agent with Layered Memory and Character Design. (arXiv:2311.13743v1 [q-fin.CP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+Yu_Y/0/1/0/all/0/1">Yangyang Yu</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Li_H/0/1/0/all/0/1">Haohang Li</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Chen_Z/0/1/0/all/0/1">Zhi Chen</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Jiang_Y/0/1/0/all/0/1">Yuechen Jiang</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Li_Y/0/1/0/all/0/1">Yang Li</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Zhang_D/0/1/0/all/0/1">Denghui Zhang</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Liu_R/0/1/0/all/0/1">Rong Liu</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Suchow_J/0/1/0/all/0/1">Jordan W. Suchow</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Khashanah_K/0/1/0/all/0/1">Khaldoun Khashanah</a></p>
<p>Recent advancements in Large Language Models (LLMs) have exhibited notable
efficacy in question-answering (QA) tasks across diverse domains. Their prowess
in integrating extensive web knowledge has fueled interest in developing LLM
autonomous agents. While LLMs are efficient in decoding human instructions and
deriving solutions by holistically processing historical inputs, transitioning
to purpose-driven agents requires a supplementary rational architecture to
process multi-source information, establish reasoning chains, and prioritize
critical tasks. Addressing this, we introduce \textsc{FinMe}, a novel LLM-based
agent framework devised for financial decision-making, encompassing three core
modules: Profiling, to outline the agent's characteristics; Memory, with
layered processing, to aid the agent in assimilating realistic hierarchical
financial data; and Decision-making, to convert insights gained from memories
into investment decisions. Notably, \textsc{FinMe}'s memory module aligns
closely with the cognitive structure of human traders, offering robust
interpretability and real-time tuning. Its adjustable cognitive span allows for
the retention of critical information beyond human perceptual limits, thereby
enhancing trading outcomes. This framework enables the agent to self-evolve its
professional knowledge, react agilely to new investment cues, and continuously
refine trading decisions in the volatile financial environment. We first
compare \textsc{FinMe} with various algorithmic agents on a scalable real-world
financial dataset, underscoring its leading trading performance in stocks and
funds. We then fine-tuned the agent's perceptual spans to achieve a significant
trading performance. Collectively, \textsc{FinMe} presents a cutting-edge LLM
agent framework for automated trading, boosting cumulative investment returns.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13744">Security and Privacy Challenges in Deep Learning Models. (arXiv:2311.13744v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Golla_G/0/1/0/all/0/1">Gopichandh Golla</a></p>
<p>These days, deep learning models have achieved great success in multiple
fields, from autonomous driving to medical diagnosis. These models have
expanded the abilities of artificial intelligence by offering great solutions
to complex problems that were very difficult to solve earlier. In spite of
their unseen success in various, it has been identified, through research
conducted, that deep learning models can be subjected to various attacks that
compromise model security and data privacy of the Deep Neural Network models.
Deep learning models can be subjected to various attacks at different stages of
their lifecycle. During the testing phase, attackers can exploit
vulnerabilities through different kinds of attacks such as Model Extraction
Attacks, Model Inversion attacks, and Adversarial attacks. Model Extraction
Attacks are aimed at reverse-engineering a trained deep learning model, with
the primary objective of revealing its architecture and parameters. Model
inversion attacks aim to compromise the privacy of the data used in the Deep
learning model. These attacks are done to compromise the confidentiality of the
model by going through the sensitive training data from the model's
predictions. By analyzing the model's responses, attackers aim to reconstruct
sensitive information. In this way, the model's data privacy is compromised.
Adversarial attacks, mainly employed on computer vision models, are made to
corrupt models into confidently making incorrect predictions through malicious
testing data. These attacks subtly alter the input data, making it look normal
but misleading deep learning models to make incorrect decisions. Such attacks
can happen during both the model's evaluation and training phases. Data
Poisoning Attacks add harmful data to the training set, disrupting the learning
process and reducing the reliability of the deep learning mode.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13750">Towards Transferable Multi-modal Perception Representation Learning for Autonomy: NeRF-Supervised Masked AutoEncoder. (arXiv:2311.13750v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiaohao Xu</a></p>
<p>This work proposes a unified self-supervised pre-training framework for
transferable multi-modal perception representation learning via masked
multi-modal reconstruction in Neural Radiance Field (NeRF), namely
NeRF-Supervised Masked AutoEncoder (NS-MAE). Specifically, conditioned on
certain view directions and locations, multi-modal embeddings extracted from
corrupted multi-modal input signals, i.e., Lidar point clouds and images, are
rendered into projected multi-modal feature maps via neural rendering. Then,
original multi-modal signals serve as reconstruction targets for the rendered
multi-modal feature maps to enable self-supervised representation learning.
Extensive experiments show that the representation learned via NS-MAE shows
promising transferability for diverse multi-modal and single-modal (camera-only
and Lidar-only) perception models on diverse 3D perception downstream tasks (3D
object detection and BEV map segmentation) with diverse amounts of fine-tuning
labeled data. Moreover, we empirically find that NS-MAE enjoys the synergy of
both the mechanism of masked autoencoder and neural radiance field. Our code
shall be released upon acceptance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13752">3D-MIR: A Benchmark and Empirical Study on 3D Medical Image Retrieval in Radiology. (arXiv:2311.13752v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abacha_A/0/1/0/all/0/1">Asma Ben Abacha</a>, <a href="http://arxiv.org/find/cs/1/au:+Santamaria_Pang_A/0/1/0/all/0/1">Alberto Santamaria-Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Ho Hin Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Merkow_J/0/1/0/all/0/1">Jameson Merkow</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_Q/0/1/0/all/0/1">Qin Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Devarakonda_S/0/1/0/all/0/1">Surya Teja Devarakonda</a>, <a href="http://arxiv.org/find/cs/1/au:+Islam_A/0/1/0/all/0/1">Abdullah Islam</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_J/0/1/0/all/0/1">Julia Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1">Matthew P. Lungren</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1">Thomas Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Codella_N/0/1/0/all/0/1">Noel C Codella</a>, <a href="http://arxiv.org/find/cs/1/au:+Tarapov_I/0/1/0/all/0/1">Ivan Tarapov</a></p>
<p>The increasing use of medical imaging in healthcare settings presents a
significant challenge due to the increasing workload for radiologists, yet it
also offers opportunity for enhancing healthcare outcomes if effectively
leveraged. 3D image retrieval holds potential to reduce radiologist workloads
by enabling clinicians to efficiently search through diagnostically similar or
otherwise relevant cases, resulting in faster and more precise diagnoses.
However, the field of 3D medical image retrieval is still emerging, lacking
established evaluation benchmarks, comprehensive datasets, and thorough
studies. This paper attempts to bridge this gap by introducing a novel
benchmark for 3D Medical Image Retrieval (3D-MIR) that encompasses four
different anatomies imaged with computed tomography. Using this benchmark, we
explore a diverse set of search strategies that use aggregated 2D slices, 3D
volumes, and multi-modal embeddings from popular multi-modal foundation models
as queries. Quantitative and qualitative assessments of each approach are
provided alongside an in-depth discussion that offers insight for future
research. To promote the advancement of this field, our benchmark, dataset, and
code are made publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13770">Archiving Body Movements: Collective Generation of Chinese Calligraphy. (arXiv:2311.13770v1 [cs.MM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1">Aven Le Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1">Jiayi Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tianchen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kang Zhang</a></p>
<p>As a communication channel, body movements have been widely explored in
behavioral studies and kinesics. Performing and visual arts share the same
interests but focus on documenting and representing human body movements, such
as for dance notation and visual work creation. This paper investigates body
movements in oriental calligraphy and how to apply calligraphy principles to
stimulate and archive body movements. Through an artwork (Wushu), the authors
experiment with an interactive and generative approach to engage the audience's
bodily participation and archive the body movements as a compendium of
generated calligraphy. The audience assumes the role of both writers and
readers; creating ("writing") and appreciating ("reading") the generated
calligraphy becomes a cyclical process within this infinite "Book," which can
motivate further attention and discussions concerning Chinese characters and
calligraphy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13782">Scalable AI Generative Content for Vehicular Network Semantic Communication. (arXiv:2311.13782v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1">Hao Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1">Zhu Han</a></p>
<p>Perceiving vehicles in a driver's blind spot is vital for safe driving. The
detection of potentially dangerous vehicles in these blind spots can benefit
from vehicular network semantic communication technology. However, efficient
semantic communication involves a trade-off between accuracy and delay,
especially in bandwidth-limited situations. This paper unveils a scalable
Artificial Intelligence Generated Content (AIGC) system that leverages an
encoder-decoder architecture. This system converts images into textual
representations and reconstructs them into quality-acceptable images,
optimizing transmission for vehicular network semantic communication. Moreover,
when bandwidth allows, auxiliary information is integrated. The encoder-decoder
aims to maintain semantic equivalence with the original images across various
tasks. Then the proposed approach employs reinforcement learning to enhance the
reliability of the generated contents. Experimental results suggest that the
proposed method surpasses the baseline in perceiving vehicles in blind spots
and effectively compresses communication data. While this method is
specifically designed for driving scenarios, this encoder-decoder architecture
also holds potential for wide use across various semantic communication
scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13800">Enhancing Intrusion Detection In Internet Of Vehicles Through Federated Learning. (arXiv:2311.13800v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sebastian_A/0/1/0/all/0/1">Abhishek Sebastian</a>, <a href="http://arxiv.org/find/cs/1/au:+R_P/0/1/0/all/0/1">Pragna R</a>, <a href="http://arxiv.org/find/cs/1/au:+G_S/0/1/0/all/0/1">Sudhakaran G</a>, <a href="http://arxiv.org/find/cs/1/au:+N_R/0/1/0/all/0/1">Renjith P N</a>, <a href="http://arxiv.org/find/cs/1/au:+H_L/0/1/0/all/0/1">Leela Karthikeyan H</a></p>
<p>Federated learning is a technique of decentralized machine learning. that
allows multiple parties to collaborate and learn a shared model without sharing
their raw data. Our paper proposes a federated learning framework for intrusion
detection in Internet of Vehicles (IOVs) using the CIC-IDS 2017 dataset. The
proposed framework employs SMOTE for handling class imbalance, outlier
detection for identifying and removing abnormal observations, and
hyperparameter tuning to optimize the model's performance. The authors
evaluated the proposed framework using various performance metrics and
demonstrated its effectiveness in detecting intrusions with other datasets
(KDD-Cup 99 and UNSW- NB-15) and conventional classifiers. Furthermore, the
proposed framework can protect sensitive data while achieving high intrusion
detection performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13810">Bridging Classical and Quantum Machine Learning: Knowledge Transfer From Classical to Quantum Neural Networks Using Knowledge Distillation. (arXiv:2311.13810v1 [quant-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Hasan_M/0/1/0/all/0/1">Mohammad Junayed Hasan</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Mahdy_M/0/1/0/all/0/1">M.R.C.Mahdy</a></p>
<p>Very recently, studies have shown that quantum neural networks surpass
classical neural networks in tasks like image classification when a similar
number of learnable parameters are used. However, the development and
optimization of quantum models are currently hindered by issues such as qubit
instability and limited qubit availability, leading to error-prone systems with
weak performance. In contrast, classical models can exhibit high-performance
owing to substantial resource availability. As a result, more studies have been
focusing on hybrid classical-quantum integration. A line of research
particularly focuses on transfer learning through classical-quantum integration
or quantum-quantum approaches. Unlike previous studies, this paper introduces a
new method to transfer knowledge from classical to quantum neural networks
using knowledge distillation, effectively bridging the gap between classical
machine learning and emergent quantum computing techniques. We adapt classical
convolutional neural network (CNN) architectures like LeNet and AlexNet to
serve as teacher networks, facilitating the training of student quantum models
by sending supervisory signals during backpropagation through KL-divergence.
The approach yields significant performance improvements for the quantum models
by solely depending on classical CNNs, with quantum models achieving an average
accuracy improvement of 0.80% on the MNIST dataset and 5.40% on the more
complex Fashion MNIST dataset. Applying this technique eliminates the
cumbersome training of huge quantum models for transfer learning in
resource-constrained settings and enables re-using existing pre-trained
classical models to improve performance.Thus, this study paves the way for
future research in quantum machine learning (QML) by positioning knowledge
distillation as a core technique for advancing QML applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13811">Education distillation:getting student models to learn in shcools. (arXiv:2311.13811v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1">Ling Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Danyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1">Tianhao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Duan_X/0/1/0/all/0/1">Xuliang Duan</a></p>
<p>Knowledge distillation is one of the methods for model compression, and
existing knowledge distillation techniques focus on how to improve the
distillation algorithm so as to enhance the distillation efficdiency. This
paper introduces dynamic incremental learning into knowledge distillation and
proposes a distillation strategy for education distillation. Specifically, it
is proposed to look at fragmented student models divided from the full student
model as low models. As the grade level rises, fragmented student models deepen
in conjunction with designed teaching reference layers, while learning and
distilling from more teacher models. By moving from lower to higher grades,
fragmented student models were gradually integrated into a complete target
student model, and the performance of the student models gradually improved
from lower to senior grades of the stage. Education distillation strategies
combined with distillation algorithms outperform the results of single
distillation algorithms on the public dataset CIFAR100,Caltech256, Food-101
dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13812">Mechanical Characterization and Inverse Design of Stochastic Architected Metamaterials Using Neural Operators. (arXiv:2311.13812v1 [cond-mat.mtrl-sci])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Jin_H/0/1/0/all/0/1">Hanxun Jin</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Zhang_E/0/1/0/all/0/1">Enrui Zhang</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Zhang_B/0/1/0/all/0/1">Boyu Zhang</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Krishnaswamy_S/0/1/0/all/0/1">Sridhar Krishnaswamy</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Karniadakis_G/0/1/0/all/0/1">George Em Karniadakis</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Espinosa_H/0/1/0/all/0/1">Horacio D. Espinosa</a></p>
<p>Machine learning (ML) is emerging as a transformative tool for the design of
architected materials, offering properties that far surpass those achievable
through lab-based trial-and-error methods. However, a major challenge in
current inverse design strategies is their reliance on extensive computational
and/or experimental datasets, which becomes particularly problematic for
designing micro-scale stochastic architected materials that exhibit nonlinear
mechanical behaviors. Here, we introduce a new end-to-end scientific ML
framework, leveraging deep neural operators (DeepONet), to directly learn the
relationship between the complete microstructure and mechanical response of
architected metamaterials from sparse but high-quality in situ experimental
data. The approach facilitates the inverse design of structures tailored to
specific nonlinear mechanical behaviors. Results obtained from spinodal
microstructures, printed using two-photon lithography, reveal that the
prediction error for mechanical responses is within a range of 5 - 10%. Our
work underscores that by employing neural operators with advanced
micro-mechanics experimental techniques, the design of complex
micro-architected materials with desired properties becomes feasible, even in
scenarios constrained by data scarcity. Our work marks a significant
advancement in the field of materials-by-design, potentially heralding a new
era in the discovery and development of next-generation metamaterials with
unparalleled mechanical characteristics derived directly from experimental
insights.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13816">Fairness-Aware Domain Generalization under Covariate and Dependence Shifts. (arXiv:2311.13816v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1">Chen Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1">Kai Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xintao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haoliang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_L/0/1/0/all/0/1">Latifur Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Grant_C/0/1/0/all/0/1">Christan Grant</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1">Feng Chen</a></p>
<p>Achieving the generalization of an invariant classifier from source domains
to shifted target domains while simultaneously considering model fairness is a
substantial and complex challenge in machine learning. Existing domain
generalization research typically attributes domain shifts to concept shift,
which relates to alterations in class labels, and covariate shift, which
pertains to variations in data styles. In this paper, by introducing another
form of distribution shift, known as dependence shift, which involves
variations in fair dependence patterns across domains, we propose a novel
domain generalization approach that addresses domain shifts by considering both
covariate and dependence shifts. We assert the existence of an underlying
transformation model can transform data from one domain to another. By
generating data in synthetic domains through the model, a fairness-aware
invariant classifier is learned that enforces both model accuracy and fairness
in unseen domains. Extensive empirical studies on four benchmark datasets
demonstrate that our approach surpasses state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13821">HypUC: Hyperfine Uncertainty Calibration with Gradient-boosted Corrections for Reliable Regression on Imbalanced Electrocardiograms. (arXiv:2311.13821v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Upadhyay_U/0/1/0/all/0/1">Uddeshya Upadhyay</a>, <a href="http://arxiv.org/find/cs/1/au:+Bade_S/0/1/0/all/0/1">Sairam Bade</a>, <a href="http://arxiv.org/find/cs/1/au:+Puranik_A/0/1/0/all/0/1">Arjun Puranik</a>, <a href="http://arxiv.org/find/cs/1/au:+Asfahan_S/0/1/0/all/0/1">Shahir Asfahan</a>, <a href="http://arxiv.org/find/cs/1/au:+Babu_M/0/1/0/all/0/1">Melwin Babu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lopez_Jimenez_F/0/1/0/all/0/1">Francisco Lopez-Jimenez</a>, <a href="http://arxiv.org/find/cs/1/au:+Asirvatham_S/0/1/0/all/0/1">Samuel J. Asirvatham</a>, <a href="http://arxiv.org/find/cs/1/au:+Prasad_A/0/1/0/all/0/1">Ashim Prasad</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajasekharan_A/0/1/0/all/0/1">Ajit Rajasekharan</a>, <a href="http://arxiv.org/find/cs/1/au:+Awasthi_S/0/1/0/all/0/1">Samir Awasthi</a>, <a href="http://arxiv.org/find/cs/1/au:+Barve_R/0/1/0/all/0/1">Rakesh Barve</a></p>
<p>The automated analysis of medical time series, such as the electrocardiogram
(ECG), electroencephalogram (EEG), pulse oximetry, etc, has the potential to
serve as a valuable tool for diagnostic decisions, allowing for remote
monitoring of patients and more efficient use of expensive and time-consuming
medical procedures. Deep neural networks (DNNs) have been demonstrated to
process such signals effectively. However, previous research has primarily
focused on classifying medical time series rather than attempting to regress
the continuous-valued physiological parameters central to diagnosis. One
significant challenge in this regard is the imbalanced nature of the dataset,
as a low prevalence of abnormal conditions can lead to heavily skewed data that
results in inaccurate predictions and a lack of certainty in such predictions
when deployed. To address these challenges, we propose HypUC, a framework for
imbalanced probabilistic regression in medical time series, making several
contributions. (i) We introduce a simple kernel density-based technique to
tackle the imbalanced regression problem with medical time series. (ii)
Moreover, we employ a probabilistic regression framework that allows
uncertainty estimation for the predicted continuous values. (iii) We also
present a new approach to calibrate the predicted uncertainty further. (iv)
Finally, we demonstrate a technique to use calibrated uncertainty estimates to
improve the predicted continuous value and show the efficacy of the calibrated
uncertainty estimates to flag unreliable predictions. HypUC is evaluated on a
large, diverse, real-world dataset of ECGs collected from millions of patients,
outperforming several conventional baselines on various diagnostic tasks,
suggesting a potential use-case for the reliable clinical deployment of deep
learning models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13843">Exact Combinatorial Optimization with Temporo-Attentional Graph Neural Networks. (arXiv:2311.13843v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Seyfi_M/0/1/0/all/0/1">Mehdi Seyfi</a>, <a href="http://arxiv.org/find/cs/1/au:+Banitalebi_Dehkordi_A/0/1/0/all/0/1">Amin Banitalebi-Dehkordi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zirui Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yong Zhang</a></p>
<p>Combinatorial optimization finds an optimal solution within a discrete set of
variables and constraints. The field has seen tremendous progress both in
research and industry. With the success of deep learning in the past decade, a
recent trend in combinatorial optimization has been to improve state-of-the-art
combinatorial optimization solvers by replacing key heuristic components with
machine learning (ML) models. In this paper, we investigate two essential
aspects of machine learning algorithms for combinatorial optimization: temporal
characteristics and attention. We argue that for the task of variable selection
in the branch-and-bound (B&amp;B) algorithm, incorporating the temporal information
as well as the bipartite graph attention improves the solver's performance. We
support our claims with intuitions and numerical results over several standard
datasets used in the literature and competitions. Code is available at:
https://developer.huaweicloud.com/develop/aigallery/notebook/detail?id=047c6cf2-8463-40d7-b92f-7b2ca998e935
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13845">Touring sampling with pushforward maps. (arXiv:2311.13845v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cabannes_V/0/1/0/all/0/1">Vivien Cabannes</a>, <a href="http://arxiv.org/find/cs/1/au:+Arnal_C/0/1/0/all/0/1">Charles Arnal</a></p>
<p>The number of sampling methods could be daunting for a practitioner looking
to cast powerful machine learning methods to their specific problem. This paper
takes a theoretical stance to review and organize many sampling approaches in
the ``generative modeling'' setting, where one wants to generate new data that
are similar to some training examples. By revealing links between existing
methods, it might prove useful to overcome some of the current challenges in
sampling with diffusion models, such as long inference time due to diffusion
simulation, or the lack of diversity in generated samples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13852">A Cross Attention Approach to Diagnostic Explainability using Clinical Practice Guidelines for Depression. (arXiv:2311.13852v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dalal_S/0/1/0/all/0/1">Sumit Dalal</a>, <a href="http://arxiv.org/find/cs/1/au:+Tilwani_D/0/1/0/all/0/1">Deepa Tilwani</a>, <a href="http://arxiv.org/find/cs/1/au:+Gaur_M/0/1/0/all/0/1">Manas Gaur</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1">Sarika Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Shalin_V/0/1/0/all/0/1">Valerie Shalin</a>, <a href="http://arxiv.org/find/cs/1/au:+Seth_A/0/1/0/all/0/1">Amit Seth</a></p>
<p>The lack of explainability using relevant clinical knowledge hinders the
adoption of Artificial Intelligence-powered analysis of unstructured clinical
dialogue. A wealth of relevant, untapped Mental Health (MH) data is available
in online communities, providing the opportunity to address the explainability
problem with substantial potential impact as a screening tool for both online
and offline applications. We develop a method to enhance attention in popular
transformer models and generate clinician-understandable explanations for
classification by incorporating external clinical knowledge. Inspired by how
clinicians rely on their expertise when interacting with patients, we leverage
relevant clinical knowledge to model patient inputs, providing meaningful
explanations for classification. This will save manual review time and engender
trust. We develop such a system in the context of MH using clinical practice
guidelines (CPG) for diagnosing depression, a mental health disorder of global
concern. We propose an application-specific language model called ProcesS
knowledge-infused cross ATtention (PSAT), which incorporates CPGs when
computing attention. Through rigorous evaluation on three expert-curated
datasets related to depression, we demonstrate application-relevant
explainability of PSAT. PSAT also surpasses the performance of nine baseline
models and can provide explanations where other baselines fall short. We
transform a CPG resource focused on depression, such as the Patient Health
Questionnaire (e.g. PHQ-9) and related questions, into a machine-readable
ontology using SNOMED-CT. With this resource, PSAT enhances the ability of
models like GPT-3.5 to generate application-relevant explanations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13857">Challenges of Large Language Models for Mental Health Counseling. (arXiv:2311.13857v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chung_N/0/1/0/all/0/1">Neo Christopher Chung</a>, <a href="http://arxiv.org/find/cs/1/au:+Dyer_G/0/1/0/all/0/1">George Dyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Brocki_L/0/1/0/all/0/1">Lennart Brocki</a></p>
<p>The global mental health crisis is looming with a rapid increase in mental
disorders, limited resources, and the social stigma of seeking treatment. As
the field of artificial intelligence (AI) has witnessed significant
advancements in recent years, large language models (LLMs) capable of
understanding and generating human-like text may be used in supporting or
providing psychological counseling. However, the application of LLMs in the
mental health domain raises concerns regarding the accuracy, effectiveness, and
reliability of the information provided. This paper investigates the major
challenges associated with the development of LLMs for psychological
counseling, including model hallucination, interpretability, bias, privacy, and
clinical effectiveness. We explore potential solutions to these challenges that
are practical and applicable to the current paradigm of AI. From our experience
in developing and deploying LLMs for mental health, AI holds a great promise
for improving mental health care, if we can carefully navigate and overcome
pitfalls of LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13871">Legal Requirements Analysis. (arXiv:2311.13871v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abualhaija_S/0/1/0/all/0/1">Sallam Abualhaija</a>, <a href="http://arxiv.org/find/cs/1/au:+Ceci_M/0/1/0/all/0/1">Marcello Ceci</a>, <a href="http://arxiv.org/find/cs/1/au:+Briand_L/0/1/0/all/0/1">Lionel Briand</a></p>
<p>Modern software has been an integral part of everyday activities in many
disciplines and application contexts. Introducing intelligent automation by
leveraging artificial intelligence (AI) led to break-throughs in many fields.
The effectiveness of AI can be attributed to several factors, among which is
the increasing availability of data. Regulations such as the general data
protection regulation (GDPR) in the European Union (EU) are introduced to
ensure the protection of personal data. Software systems that collect, process,
or share personal data are subject to compliance with such regulations.
Developing compliant software depends heavily on addressing legal requirements
stipulated in applicable regulations, a central activity in the requirements
engineering (RE) phase of the software development process. RE is concerned
with specifying and maintaining requirements of a system-to-be, including legal
requirements. Legal agreements which describe the policies organizations
implement for processing personal data can provide an additional source to
regulations for eliciting legal requirements. In this chapter, we explore a
variety of methods for analyzing legal requirements and exemplify them on GDPR.
Specifically, we describe possible alternatives for creating machine-analyzable
representations from regulations, survey the existing automated means for
enabling compliance verification against regulations, and further reflect on
the current challenges of legal requirements analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13878">Minimizing Factual Inconsistency and Hallucination in Large Language Models. (arXiv:2311.13878v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+I_M/0/1/0/all/0/1">Muneeswaran I</a>, <a href="http://arxiv.org/find/cs/1/au:+Saxena_S/0/1/0/all/0/1">Shreya Saxena</a>, <a href="http://arxiv.org/find/cs/1/au:+Prasad_S/0/1/0/all/0/1">Siva Prasad</a>, <a href="http://arxiv.org/find/cs/1/au:+Prakash_M/0/1/0/all/0/1">M V Sai Prakash</a>, <a href="http://arxiv.org/find/cs/1/au:+Shankar_A/0/1/0/all/0/1">Advaith Shankar</a>, <a href="http://arxiv.org/find/cs/1/au:+V_V/0/1/0/all/0/1">Varun V</a>, <a href="http://arxiv.org/find/cs/1/au:+Vaddina_V/0/1/0/all/0/1">Vishal Vaddina</a>, <a href="http://arxiv.org/find/cs/1/au:+Gopalakrishnan_S/0/1/0/all/0/1">Saisubramaniam Gopalakrishnan</a></p>
<p>Large Language Models (LLMs) are widely used in critical fields such as
healthcare, education, and finance due to their remarkable proficiency in
various language-related tasks. However, LLMs are prone to generating factually
incorrect responses or "hallucinations," which can lead to a loss of
credibility and trust among users. To address this issue, we propose a
multi-stage framework that generates the rationale first, verifies and refines
incorrect ones, and uses them as supporting references to generate the answer.
The generated rationale enhances the transparency of the answer and our
framework provides insights into how the model arrived at this answer, by using
this rationale and the references to the context. In this paper, we demonstrate
its effectiveness in improving the quality of responses to drug-related
inquiries in the life sciences industry. Our framework improves traditional
Retrieval Augmented Generation (RAG) by enabling OpenAI GPT-3.5-turbo to be
14-25% more faithful and 16-22% more accurate on two datasets. Furthermore,
fine-tuning samples based on our framework improves the accuracy of smaller
open-access LLMs by 33-42% and competes with RAG on commercial models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13881">A Multi-solution Study on GDPR AI-enabled Completeness Checking of DPAs. (arXiv:2311.13881v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Azeem_M/0/1/0/all/0/1">Muhammad Ilyas Azeem</a>, <a href="http://arxiv.org/find/cs/1/au:+Abualhaija_S/0/1/0/all/0/1">Sallam Abualhaija</a></p>
<p>Specifying legal requirements for software systems to ensure their compliance
with the applicable regulations is a major concern to requirements engineering
(RE). Personal data which is collected by an organization is often shared with
other organizations to perform certain processing activities. In such cases,
the General Data Protection Regulation (GDPR) requires issuing a data
processing agreement (DPA) which regulates the processing and further ensures
that personal data remains protected. Violating GDPR can lead to huge fines
reaching to billions of Euros. Software systems involving personal data
processing must adhere to the legal obligations stipulated in GDPR and outlined
in DPAs. Requirements engineers can elicit from DPAs legal requirements for
regulating the data processing activities in software systems. Checking the
completeness of a DPA according to the GDPR provisions is therefore an
essential prerequisite to ensure that the elicited requirements are complete.
Analyzing DPAs entirely manually is time consuming and requires adequate legal
expertise. In this paper, we propose an automation strategy to address the
completeness checking of DPAs against GDPR. Specifically, we pursue ten
alternative solutions which are enabled by different technologies, namely
traditional machine learning, deep learning, language modeling, and few-shot
learning. The goal of our work is to empirically examine how these different
technologies fare in the legal domain. We computed F2 score on a set of 30 real
DPAs. Our evaluation shows that best-performing solutions yield F2 score of
86.7% and 89.7% are based on pre-trained BERT and RoBERTa language models. Our
analysis further shows that other alternative solutions based on deep learning
(e.g., BiLSTM) and few-shot learning (e.g., SetFit) can achieve comparable
accuracy, yet are more efficient to develop.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13884">Controlling Large Language Model-based Agents for Large-Scale Decision-Making: An Actor-Critic Approach. (arXiv:2311.13884v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Bin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1">Hangyu Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruan_J/0/1/0/all/0/1">Jingqing Ruan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1">Ying Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zhiwei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dapeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Ziyue Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1">Rui Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lijuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_G/0/1/0/all/0/1">Guoliang Fan</a></p>
<p>The significant advancements in large language models (LLMs) have presented
novel opportunities for tackling planning and decision-making within
multi-agent systems. However, as the number of agents increases, the issues of
hallucination in LLMs and coordination in multi-agent systems (MAS) have become
increasingly pronounced. Additionally, the efficient utilization of tokens
becomes a critical consideration when employing LLMs to facilitate the
interactions of large numbers of agents. In this paper, we present a novel
framework aimed at enhancing coordination and decision-making capabilities of
LLMs within large-scale multi-agent environments. Our approach draws
inspiration from the actor-critic framework employed in multi-agent
reinforcement learning, and we develop a modular and token-efficient solution
that effectively addresses challenges presented by LLMs and MAS. Through
evaluations conducted in experiments involving system resource allocation and
robot grid transportation, we demonstrate the considerable advantages afforded
by our proposed approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13885">Can Physics Informed Neural Operators Self Improve?. (arXiv:2311.13885v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Majumdar_R/0/1/0/all/0/1">Ritam Majumdar</a>, <a href="http://arxiv.org/find/cs/1/au:+Varhade_A/0/1/0/all/0/1">Amey Varhade</a>, <a href="http://arxiv.org/find/cs/1/au:+Karande_S/0/1/0/all/0/1">Shirish Karande</a>, <a href="http://arxiv.org/find/cs/1/au:+Vig_L/0/1/0/all/0/1">Lovekesh Vig</a></p>
<p>Self-training techniques have shown remarkable value across many deep
learning models and tasks. However, such techniques remain largely unexplored
when considered in the context of learning fast solvers for systems of partial
differential equations (Eg: Neural Operators). In this work, we explore the use
of self-training for Fourier Neural Operators (FNO). Neural Operators emerged
as a data driven technique, however, data from experiments or traditional
solvers is not always readily available. Physics Informed Neural Operators
(PINO) overcome this constraint by utilizing a physics loss for the training,
however the accuracy of PINO trained without data does not match the
performance obtained by training with data. In this work we show that
self-training can be used to close this gap in performance. We examine
canonical examples, namely the 1D-Burgers and 2D-Darcy PDEs, to showcase the
efficacy of self-training. Specifically, FNOs, when trained exclusively with
physics loss through self-training, approach 1.07x for Burgers and 1.02x for
Darcy, compared to FNOs trained with both data and physics loss. Furthermore,
we discover that pseudo-labels can be used for self-training without
necessarily training to convergence in each iteration. A consequence of this is
that we are able to discover self-training schedules that improve upon the
baseline performance of PINO in terms of accuracy as well as time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13892">General Phrase Debiaser: Debiasing Masked Language Models at a Multi-Token Level. (arXiv:2311.13892v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1">Bingkang Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaodan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1">Dehan Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yulei Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zongzhen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_H/0/1/0/all/0/1">Honglei Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1">Longtao Huang</a></p>
<p>The social biases and unwelcome stereotypes revealed by pretrained language
models are becoming obstacles to their application. Compared to numerous
debiasing methods targeting word level, there has been relatively less
attention on biases present at phrase level, limiting the performance of
debiasing in discipline domains. In this paper, we propose an automatic
multi-token debiasing pipeline called \textbf{General Phrase Debiaser}, which
is capable of mitigating phrase-level biases in masked language models.
Specifically, our method consists of a \textit{phrase filter stage} that
generates stereotypical phrases from Wikipedia pages as well as a \textit{model
debias stage} that can debias models at the multi-token level to tackle bias
challenges on phrases. The latter searches for prompts that trigger model's
bias, and then uses them for debiasing. State-of-the-art results on standard
datasets and metrics show that our approach can significantly reduce gender
biases on both career and multiple disciplines, across models with varying
parameter sizes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13905">A DRL solution to help reduce the cost in waiting time of securing a traffic light for cyclists. (arXiv:2311.13905v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Magnana_L/0/1/0/all/0/1">Lucas Magnana</a> (AGORA), <a href="http://arxiv.org/find/cs/1/au:+Rivano_H/0/1/0/all/0/1">Herv&#xe9; Rivano</a> (AGORA), <a href="http://arxiv.org/find/cs/1/au:+Chiabaut_N/0/1/0/all/0/1">Nicolas Chiabaut</a></p>
<p>Cyclists prefer to use infrastructure that separates them from motorized
traffic. Using a traffic light to segregate car and bike flows, with the
addition of bike-specific green phases, is a lightweight and cheap solution
that can be deployed dynamically to assess the opportunity of a heavier
infrastructure such as a separate bike lane. To compensate for the increased
waiting time induced by these new phases, we introduce in this paper a deep
reinforcement learning solution that adapts the green phase cycle of a traffic
light to the traffic. Vehicle counter data are used to compare the DRL approach
with the actuated traffic light control algorithm over whole days. Results show
that DRL achieves better minimization of vehicle waiting time at almost all
hours. Our DRL approach is also robust to moderate changes in bike traffic. The
code of this paper is available at
https://github.com/LucasMagnana/A-DRL-solution-to-help-reduce-the-cost-in-waiting-time-of-securing-a-traffic-light-for-cyclists.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13928">Parameter Exchange for Robust Dynamic Domain Generalization. (arXiv:2311.13928v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1">Luojun Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1">Zhifeng Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1">Zhishu Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yuanlong Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Weijie Chen</a></p>
<p>Agnostic domain shift is the main reason of model degradation on the unknown
target domains, which brings an urgent need to develop Domain Generalization
(DG). Recent advances at DG use dynamic networks to achieve training-free
adaptation on the unknown target domains, termed Dynamic Domain Generalization
(DDG), which compensates for the lack of self-adaptability in static models
with fixed weights. The parameters of dynamic networks can be decoupled into a
static and a dynamic component, which are designed to learn domain-invariant
and domain-specific features, respectively. Based on the existing arts, in this
work, we try to push the limits of DDG by disentangling the static and dynamic
components more thoroughly from an optimization perspective. Our main
consideration is that we can enable the static component to learn
domain-invariant features more comprehensively by augmenting the
domain-specific information. As a result, the more comprehensive
domain-invariant features learned by the static component can then enforce the
dynamic component to focus more on learning adaptive domain-specific features.
To this end, we propose a simple yet effective Parameter Exchange (PE) method
to perturb the combination between the static and dynamic components. We
optimize the model using the gradients from both the perturbed and
non-perturbed feed-forward jointly to implicitly achieve the aforementioned
disentanglement. In this way, the two components can be optimized in a
mutually-beneficial manner, which can resist the agnostic domain shifts and
improve the self-adaptability on the unknown target domain. Extensive
experiments show that PE can be easily plugged into existing dynamic networks
to improve their generalization ability without bells and whistles.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13953">Learning Uniform Clusters on Hypersphere for Deep Graph-level Clustering. (arXiv:2311.13953v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1">Mengling Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chaochao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Weiming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xinyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_X/0/1/0/all/0/1">Xinting Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1">Xiaolin Zheng</a></p>
<p>Graph clustering has been popularly studied in recent years. However, most
existing graph clustering methods focus on node-level clustering, i.e.,
grouping nodes in a single graph into clusters. In contrast, graph-level
clustering, i.e., grouping multiple graphs into clusters, remains largely
unexplored. Graph-level clustering is critical in a variety of real-world
applications, such as, properties prediction of molecules and community
analysis in social networks. However, graph-level clustering is challenging due
to the insufficient discriminability of graph-level representations, and the
insufficient discriminability makes deep clustering be more likely to obtain
degenerate solutions (cluster collapse). To address the issue, we propose a
novel deep graph-level clustering method called Uniform Deep Graph Clustering
(UDGC). UDGC assigns instances evenly to different clusters and then scatters
those clusters on unit hypersphere, leading to a more uniform cluster-level
distribution and a slighter cluster collapse. Specifically, we first propose
Augmentation-Consensus Optimal Transport (ACOT) for generating uniformly
distributed and reliable pseudo labels for partitioning clusters. Then we adopt
contrastive learning to scatter those clusters. Besides, we propose Center
Alignment Optimal Transport (CAOT) for guiding the model to learn better
parameters, which further promotes the cluster performance. Our empirical study
on eight well-known datasets demonstrates that UDGC significantly outperforms
the state-of-the-art models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13960">Human Machine Co-Creation. A Complementary Cognitive Approach to Creative Character Design Process Using GANs. (arXiv:2311.13960v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lataifeh_M/0/1/0/all/0/1">Mohammad Lataifeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Carrascoa_X/0/1/0/all/0/1">Xavier A Carrascoa</a>, <a href="http://arxiv.org/find/cs/1/au:+Elnagara_A/0/1/0/all/0/1">Ashraf M Elnagara</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmeda_N/0/1/0/all/0/1">Naveed Ahmeda</a>, <a href="http://arxiv.org/find/cs/1/au:+Junejo_I/0/1/0/all/0/1">Imran Junejo</a></p>
<p>Recent advances in Generative Adversarial Networks GANs applications continue
to attract the attention of researchers in different fields. In such a
framework, two neural networks compete adversely to generate new visual
contents indistinguishable from the original dataset. The objective of this
research is to create a complementary codesign process between humans and
machines to augment character designers abilities in visualizing and creating
new characters for multimedia projects such as games and animation. Driven by
design cognitive scaffolding, the proposed approach aims to inform the process
of perceiving, knowing, and making. The machine generated concepts are used as
a launching platform for character designers to conceptualize new characters. A
labelled dataset of 22,000 characters was developed for this work and deployed
using different GANs to evaluate the most suited for the context, followed by
mixed methods evaluation for the machine output and human derivations. The
discussed results substantiate the value of the proposed cocreation framework
and elucidate how the generated concepts are used as cognitive substances that
interact with designers competencies in a versatile manner to influence the
creative processes of conceptualizing novel characters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13964">Deep Interactive Segmentation of Medical Images: A Systematic Review and Taxonomy. (arXiv:2311.13964v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Marinov_Z/0/1/0/all/0/1">Zdravko Marinov</a>, <a href="http://arxiv.org/find/eess/1/au:+Jager_P/0/1/0/all/0/1">Paul F. J&#xe4;ger</a>, <a href="http://arxiv.org/find/eess/1/au:+Egger_J/0/1/0/all/0/1">Jan Egger</a>, <a href="http://arxiv.org/find/eess/1/au:+Kleesiek_J/0/1/0/all/0/1">Jens Kleesiek</a>, <a href="http://arxiv.org/find/eess/1/au:+Stiefelhagen_R/0/1/0/all/0/1">Rainer Stiefelhagen</a></p>
<p>Interactive segmentation is a crucial research area in medical image analysis
aiming to boost the efficiency of costly annotations by incorporating human
feedback. This feedback takes the form of clicks, scribbles, or masks and
allows for iterative refinement of the model output so as to efficiently guide
the system towards the desired behavior. In recent years, deep learning-based
approaches have propelled results to a new level causing a rapid growth in the
field with 121 methods proposed in the medical imaging domain alone. In this
review, we provide a structured overview of this emerging field featuring a
comprehensive taxonomy, a systematic review of existing methods, and an
in-depth analysis of current practices. Based on these contributions, we
discuss the challenges and opportunities in the field. For instance, we find
that there is a severe lack of comparison across methods which needs to be
tackled by standardized baselines and benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13982">Probabilistic Tree-of-thought Reasoning for Answering Knowledge-intensive Complex Questions. (arXiv:2311.13982v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1">Shulin Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiajie Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1">Jiaxin Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_X/0/1/0/all/0/1">Xin Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1">Zijun Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1">Qi Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Juanzi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1">Lei Hou</a></p>
<p>Large language models (LLMs) are capable of answering knowledge-intensive
complex questions with chain-of-thought (CoT) reasoning. However, they tend to
generate factually incorrect reasoning steps when the required knowledge is not
available or up-to-date in models' parameters. Recent works turn to retrieving
external knowledge to augment CoT reasoning. Despite being promising, these
chain-based methods suffer from: 1) Negative retrieval. Unnecessary or
incorrect retrieval may mislead the reasoning; 2) Limited sight. Lacking the
ability to look backward or forward, a local error in one step will propagate
along the chain.
</p>
<p>In this paper, we propose a novel approach: Probabilistic Tree-of-thought
Reasoning (ProbTree). First, LLMs translate a complex question into a query
tree, in which each non-root node denotes a sub-question of its parent node.
Then, probabilistic reasoning is conducted over the tree, by solving questions
from leaf to root considering the confidence of both question decomposing and
answering. During reasoning, for leaf nodes, LLMs choose a more confident
answer from Closed-book QA that employs parametric knowledge and Open-book QA
that employs retrieved external knowledge, thus eliminating the negative
retrieval problem. For non-leaf nodes, with the hierarchical structure, LLMs
have broader sights and are able to globally reason with the information from
child nodes, thus recovering from local errors. The experiments on three
Complex QA datasets under the open-domain setting show that our approach
outperforms SOTA methods significantly, demonstrating the effect of
probabilistic tree-of-thought reasoning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13983">Learning Dynamic Selection and Pricing of Out-of-Home Deliveries. (arXiv:2311.13983v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Akkerman_F/0/1/0/all/0/1">Fabian Akkerman</a>, <a href="http://arxiv.org/find/cs/1/au:+Dieter_P/0/1/0/all/0/1">Peter Dieter</a>, <a href="http://arxiv.org/find/cs/1/au:+Mes_M/0/1/0/all/0/1">Martijn Mes</a></p>
<p>Home delivery failures, traffic congestion, and relatively large handling
times have a negative impact on the profitability of last-mile logistics. These
external factors contribute to up to $28\%$ of the overall costs and $25\%$ of
emissions for the home delivery supply chain. A potential solution, showing
annual growth rates up to $36\%$, is the delivery to parcel lockers or parcel
shops, denoted by out-of-home (OOH) delivery. In the academic literature,
models of customer behavior with respect to OOH delivery were so far limited to
deterministic settings, contrasting with the stochastic nature of actual
customer choices. We model the sequential decision-making problem of which OOH
location to offer against what incentive for each incoming customer, taking
into account future customer arrivals and choices. We propose Dynamic Selection
and Pricing of OOH (DSPO), an algorithmic pipeline that uses a novel
spatial-temporal state encoding as input to a convolutional neural network. We
demonstrate the performance of our method by benchmarking it against three
state-of-the-art approaches. Our extensive numerical study, guided by
real-world data, reveals that DSPO can save $20.8\%$ in costs compared to a
situation without OOH locations, $8.1\%$ compared to a static selection and
pricing policy, and $4.6\%$ compared to a state-of-the-art demand management
benchmark. We provide comprehensive insights into the complex interplay between
OOH delivery dynamics and customer behavior influenced by pricing strategies.
The implications of our findings suggest practitioners to adopt dynamic
selection and pricing policies as OOH delivery gains a larger market share.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14003">Direct Preference-Based Evolutionary Multi-Objective Optimization with Dueling Bandit. (arXiv:2311.14003v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1">Tian Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Ke Li</a></p>
<p>Optimization problems find widespread use in both single-objective and
multi-objective scenarios. In practical applications, users aspire for
solutions that converge to the region of interest (ROI) along the Pareto front
(PF). While the conventional approach involves approximating a fitness function
or an objective function to reflect user preferences, this paper explores an
alternative avenue. Specifically, we aim to discover a method that sidesteps
the need for calculating the fitness function, relying solely on human
feedback. Our proposed approach entails conducting direct preference learning
facilitated by an active dueling bandit algorithm. The experimental phase is
structured into three sessions. Firstly, we assess the performance of our
active dueling bandit algorithm. Secondly, we implement our proposed method
within the context of Multi-objective Evolutionary Algorithms (MOEAs). Finally,
we deploy our method in a practical problem, specifically in protein structure
prediction (PSP). This research presents a novel interactive preference-based
MOEA framework that not only addresses the limitations of traditional
techniques but also unveils new possibilities for optimization problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14005">When Side-Channel Attacks Break the Black-Box Property of Embedded Artificial Intelligence. (arXiv:2311.14005v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Coqueret_B/0/1/0/all/0/1">Benoit Coqueret</a>, <a href="http://arxiv.org/find/cs/1/au:+Carbone_M/0/1/0/all/0/1">Mathieu Carbone</a>, <a href="http://arxiv.org/find/cs/1/au:+Sentieys_O/0/1/0/all/0/1">Olivier Sentieys</a>, <a href="http://arxiv.org/find/cs/1/au:+Zaid_G/0/1/0/all/0/1">Gabriel Zaid</a></p>
<p>Artificial intelligence, and specifically deep neural networks (DNNs), has
rapidly emerged in the past decade as the standard for several tasks from
specific advertising to object detection. The performance offered has led DNN
algorithms to become a part of critical embedded systems, requiring both
efficiency and reliability. In particular, DNNs are subject to malicious
examples designed in a way to fool the network while being undetectable to the
human observer: the adversarial examples. While previous studies propose
frameworks to implement such attacks in black box settings, those often rely on
the hypothesis that the attacker has access to the logits of the neural
network, breaking the assumption of the traditional black box. In this paper,
we investigate a real black box scenario where the attacker has no access to
the logits. In particular, we propose an architecture-agnostic attack which
solve this constraint by extracting the logits. Our method combines hardware
and software attacks, by performing a side-channel attack that exploits
electromagnetic leakages to extract the logits for a given input, allowing an
attacker to estimate the gradients and produce state-of-the-art adversarial
examples to fool the targeted neural network. Through this example of
adversarial attack, we demonstrate the effectiveness of logits extraction using
side-channel as a first step for more general attack frameworks requiring
either the logits or the confidence scores.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14028">Continual Learning of Diffusion Models with Generative Distillation. (arXiv:2311.14028v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Masip_S/0/1/0/all/0/1">Sergi Masip</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_P/0/1/0/all/0/1">Pau Rodriguez</a>, <a href="http://arxiv.org/find/cs/1/au:+Tuytelaars_T/0/1/0/all/0/1">Tinne Tuytelaars</a>, <a href="http://arxiv.org/find/cs/1/au:+Ven_G/0/1/0/all/0/1">Gido M. van de Ven</a></p>
<p>Diffusion models are powerful generative models that achieve state-of-the-art
performance in tasks such as image synthesis. However, training them demands
substantial amounts of data and computational resources. Continual learning
would allow for incrementally learning new tasks and accumulating knowledge,
thus reusing already trained models would be possible. One potentially suitable
approach is generative replay, where a copy of a generative model trained on
previous tasks produces synthetic data that are interleaved with data from the
current task. However, standard generative replay applied to diffusion models
results in a catastrophic loss in denoising capabilities. In this paper, we
propose generative distillation, an approach that distils the entire reverse
process of a diffusion model. We demonstrate that our approach significantly
improves the continual learning performance of generative replay with only a
moderate increase in the computational costs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14030">PrivateLoRA For Efficient Privacy Preserving LLM. (arXiv:2311.14030v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yiming Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yu Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1">Xiaodong Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Guannan Zhang</a></p>
<p>End users face a choice between privacy and efficiency in current Large
Language Model (LLM) service paradigms. In cloud-based paradigms, users are
forced to compromise data locality for generation quality and processing speed.
Conversely, edge device paradigms maintain data locality but fail to deliver
satisfactory performance. In this work, we propose a novel LLM service paradigm
that distributes privacy-sensitive computation on edge devices and shared
computation in the cloud. Only activations are transmitted between the central
cloud and edge devices to ensure data locality. Our core innovation,
PrivateLoRA, addresses the challenging communication overhead by exploiting the
low rank of residual activations, achieving over 95% communication reduction.
Consequently, PrivateLoRA effectively maintains data locality and is extremely
resource efficient. Under standard 5G networks, PrivateLoRA achieves throughput
over 300% of device-only solutions for 7B models and over 80% of an A100 GPU
for 33B models. PrivateLoRA also provides tuning performance comparable to LoRA
for advanced personalization. Our approach democratizes access to
state-of-the-art generative AI for edge devices, paving the way for more
tailored LLM experiences for the general public. To our knowledge, our proposed
framework is the first efficient and privacy-preserving LLM solution in the
literature.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14057">Assessing the Impact of Noise on Quantum Neural Networks: An Experimental Analysis. (arXiv:2311.14057v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Escudero_E/0/1/0/all/0/1">Erik B. Terres Escudero</a>, <a href="http://arxiv.org/find/cs/1/au:+Alamo_D/0/1/0/all/0/1">Danel Arias Alamo</a>, <a href="http://arxiv.org/find/cs/1/au:+Gomez_O/0/1/0/all/0/1">Oier Mentxaka G&#xf3;mez</a>, <a href="http://arxiv.org/find/cs/1/au:+Bringas_P/0/1/0/all/0/1">Pablo Garc&#xed;a Bringas</a></p>
<p>In the race towards quantum computing, the potential benefits of quantum
neural networks (QNNs) have become increasingly apparent. However, Noisy
Intermediate-Scale Quantum (NISQ) processors are prone to errors, which poses a
significant challenge for the execution of complex algorithms or quantum
machine learning. To ensure the quality and security of QNNs, it is crucial to
explore the impact of noise on their performance. This paper provides a
comprehensive analysis of the impact of noise on QNNs, examining the Mottonen
state preparation algorithm under various noise models and studying the
degradation of quantum states as they pass through multiple layers of QNNs.
Additionally, the paper evaluates the effect of noise on the performance of
pre-trained QNNs and highlights the challenges posed by noise models in quantum
computing. The findings of this study have significant implications for the
development of quantum software, emphasizing the importance of prioritizing
stability and noise-correction measures when developing QNNs to ensure reliable
and trustworthy results. This paper contributes to the growing body of
literature on quantum computing and quantum machine learning, providing new
insights into the impact of noise on QNNs and paving the way towards the
development of more robust and efficient quantum algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14058">Identification for Tree-shaped Structural Causal Models in Polynomial Time. (arXiv:2311.14058v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1">Aaryan Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Blaser_M/0/1/0/all/0/1">Markus Bl&#xe4;ser</a></p>
<p>Linear structural causal models (SCMs) are used to express and analyse the
relationships between random variables. Direct causal effects are represented
as directed edges and confounding factors as bidirected edges. Identifying the
causal parameters from correlations between the nodes is an open problem in
artificial intelligence. In this paper, we study SCMs whose directed component
forms a tree. Van der Zander et al. (AISTATS'22, PLMR 151, pp. 6770--6792,
2022) give a PSPACE-algorithm for the identification problem in this case,
which is a significant improvement over the general Gr\"obner basis approach,
which has doubly-exponential time complexity in the number of structural
parameters. In this work, we present a randomized polynomial-time algorithm,
which solves the identification problem for tree-shaped SCMs. For every
structural parameter, our algorithms decides whether it is generically
identifiable, generically 2-identifiable, or generically unidentifiable. (No
other cases can occur.) In the first two cases, it provides one or two
fractional affine square root terms of polynomials (FASTPs) for the
corresponding parameter, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14061">Towards Explainable Strategy Templates using NLP Transformers. (arXiv:2311.14061v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bagga_P/0/1/0/all/0/1">Pallavi Bagga</a>, <a href="http://arxiv.org/find/cs/1/au:+Stathis_K/0/1/0/all/0/1">Kostas Stathis</a></p>
<p>This paper bridges the gap between mathematical heuristic strategies learned
from Deep Reinforcement Learning (DRL) in automated agent negotiation, and
comprehensible, natural language explanations. Our aim is to make these
strategies more accessible to non-experts. By leveraging traditional Natural
Language Processing (NLP) techniques and Large Language Models (LLMs) equipped
with Transformers, we outline how parts of DRL strategies composed of parts
within strategy templates can be transformed into user-friendly, human-like
English narratives. To achieve this, we present a top-level algorithm that
involves parsing mathematical expressions of strategy templates, semantically
interpreting variables and structures, generating rule-based primary
explanations, and utilizing a Generative Pre-trained Transformer (GPT) model to
refine and contextualize these explanations. Subsequent customization for
varied audiences and meticulous validation processes in an example illustrate
the applicability and potential of this approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14073">Learning Saliency From Fixations. (arXiv:2311.14073v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Djilali_Y/0/1/0/all/0/1">Yasser Abdelaziz Dahou Djilali</a>, <a href="http://arxiv.org/find/cs/1/au:+McGuiness_K/0/1/0/all/0/1">Kevin McGuiness</a>, <a href="http://arxiv.org/find/cs/1/au:+OConnor_N/0/1/0/all/0/1">Noel O&#x27;Connor</a></p>
<p>We present a novel approach for saliency prediction in images, leveraging
parallel decoding in transformers to learn saliency solely from fixation maps.
Models typically rely on continuous saliency maps, to overcome the difficulty
of optimizing for the discrete fixation map. We attempt to replicate the
experimental setup that generates saliency datasets. Our approach treats
saliency prediction as a direct set prediction problem, via a global loss that
enforces unique fixations prediction through bipartite matching and a
transformer encoder-decoder architecture. By utilizing a fixed set of learned
fixation queries, the cross-attention reasons over the image features to
directly output the fixation points, distinguishing it from other modern
saliency predictors. Our approach, named Saliency TRansformer (SalTR), achieves
metric scores on par with state-of-the-art approaches on the Salicon and MIT300
benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14084">AI-Generated Images Introduce Invisible Relevance Bias to Text-Image Retrieval. (arXiv:2311.14084v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Shicheng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_D/0/1/0/all/0/1">Danyang Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1">Liang Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1">Jingcheng Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jun Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1">Huawei Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1">Xueqi Cheng</a></p>
<p>With the advancement of generation models, AI-generated content (AIGC) is
becoming more realistic, flooding the Internet. A recent study suggests that
this phenomenon has elevated the issue of source bias in text retrieval for web
searches. Specifically, neural retrieval models tend to rank generated texts
higher than human-written texts. In this paper, we extend the study of this
bias to cross-modal retrieval. Firstly, we successfully construct a suitable
benchmark to explore the existence of the bias. Subsequent extensive
experiments on this benchmark reveal that AI-generated images introduce an
invisible relevance bias to text-image retrieval models. Specifically, our
experiments show that text-image retrieval models tend to rank the AI-generated
images higher than the real images, even though the AI-generated images do not
exhibit more visually relevant features to the query than real images. This
invisible relevance bias is prevalent across retrieval models with varying
training data and architectures. Furthermore, our subsequent exploration
reveals that the inclusion of AI-generated images in the training data of the
retrieval models exacerbates the invisible relevance bias. The above phenomenon
triggers a vicious cycle, which makes the invisible relevance bias become more
and more serious. To elucidate the potential causes of invisible relevance and
address the aforementioned issues, we introduce an effective training method
aimed at alleviating the invisible relevance bias. Subsequently, we apply our
proposed debiasing method to retroactively identify the causes of invisible
relevance, revealing that the AI-generated images induce the image encoder to
embed additional information into their representation. This information
exhibits a certain consistency across generated images with different semantics
and can make the retriever estimate a higher relevance score.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14091">PortfolioMentor: Multimodal Generative AI Companion for Learning and Crafting Interactive Digital Art Portfolios. (arXiv:2311.14091v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Long_T/0/1/0/all/0/1">Tao Long</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1">Weirui Peng</a></p>
<p>Digital art portfolios serve as impactful mediums for artists to convey their
visions, weaving together visuals, audio, interactions, and narratives.
However, without technical backgrounds, design students often find it
challenging to translate creative ideas into tangible codes and designs, given
the lack of tailored resources for the non-technical, academic support in art
schools, and a comprehensive guiding tool throughout the mentally demanding
process. Recognizing the role of companionship in code learning and leveraging
generative AI models' capabilities in supporting creative tasks, we present
PortfolioMentor, a coding companion chatbot for IDEs. This tool guides and
collaborates with students through proactive suggestions and responsible Q&amp;As
for learning, inspiration, and support. In detail, the system starts with the
understanding of the task and artist's visions, follows the co-creation of
visual illustrations, audio or music suggestions and files, click-scroll
effects for interactions, and creative vision conceptualization, and finally
synthesizes these facets into a polished interactive digital portfolio.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14096">Auditing and Mitigating Cultural Bias in LLMs. (arXiv:2311.14096v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tao_Y/0/1/0/all/0/1">Yan Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Viberg_O/0/1/0/all/0/1">Olga Viberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Baker_R/0/1/0/all/0/1">Ryan S. Baker</a>, <a href="http://arxiv.org/find/cs/1/au:+Kizilcec_R/0/1/0/all/0/1">Rene F. Kizilcec</a></p>
<p>Culture fundamentally shapes people's reasoning, behavior, and communication.
Generative artificial intelligence (AI) technologies may cause a shift towards
a dominant culture. As people increasingly use AI to expedite and even automate
various professional and personal tasks, cultural values embedded in AI models
may bias authentic expression. We audit large language models for cultural
bias, comparing their responses to nationally representative survey data, and
evaluate country-specific prompting as a mitigation strategy. We find that
GPT-4, 3.5 and 3 exhibit cultural values resembling English-speaking and
Protestant European countries. Our mitigation strategy reduces cultural bias in
recent models but not for all countries/territories. To avoid cultural bias in
generative AI, especially in high-stakes contexts, we suggest using culture
matching and ongoing cultural audits.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14109">Boosting the Power of Small Multimodal Reasoning Models to Match Larger Models with Self-Consistency Training. (arXiv:2311.14109v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1">Cheng Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1">Jingxuan Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1">Zhangyang Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Linzhuang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Siyuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xihong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Stan Z. Li</a></p>
<p>Multimodal reasoning is a challenging task that requires models to reason
across multiple modalities to answer questions. Existing approaches have made
progress by incorporating language and visual modalities into a two-stage
reasoning framework, separating rationale generation from answer inference.
However, these approaches often fall short due to the inadequate quality of the
generated rationales. In this work, we delve into the importance of rationales
in model reasoning. We observe that when rationales are completely accurate,
the model's accuracy significantly improves, highlighting the need for
high-quality rationale generation. Motivated by this, we propose MC-CoT, a
self-consistency training strategy that generates multiple rationales and
answers, subsequently selecting the most accurate through a voting process.
This approach not only enhances the quality of generated rationales but also
leads to more accurate and robust answers. Through extensive experiments, we
demonstrate that our approach significantly improves model performance across
various benchmarks. Remarkably, we show that even smaller base models, when
equipped with our proposed approach, can achieve results comparable to those of
larger models, illustrating the potential of our approach in harnessing the
power of rationales for improved multimodal reasoning. The code is available at
https://github.com/chengtan9907/mc-cot.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14110">When is Off-Policy Evaluation Useful? A Data-Centric Perspective. (arXiv:2311.14110v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Hao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1">Alex J. Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Seedat_N/0/1/0/all/0/1">Nabeel Seedat</a>, <a href="http://arxiv.org/find/cs/1/au:+Huyuk_A/0/1/0/all/0/1">Alihan H&#xfc;y&#xfc;k</a>, <a href="http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1">Mihaela van der Schaar</a></p>
<p>Evaluating the value of a hypothetical target policy with only a logged
dataset is important but challenging. On the one hand, it brings opportunities
for safe policy improvement under high-stakes scenarios like clinical
guidelines. On the other hand, such opportunities raise a need for precise
off-policy evaluation (OPE). While previous work on OPE focused on improving
the algorithm in value estimation, in this work, we emphasize the importance of
the offline dataset, hence putting forward a data-centric framework for
evaluating OPE problems. We propose DataCOPE, a data-centric framework for
evaluating OPE, that answers the questions of whether and to what extent we can
evaluate a target policy given a dataset. DataCOPE (1) forecasts the overall
performance of OPE algorithms without access to the environment, which is
especially useful before real-world deployment where evaluating OPE is
impossible; (2) identifies the sub-group in the dataset where OPE can be
inaccurate; (3) permits evaluations of datasets or data-collection strategies
for OPE problems. Our empirical analysis of DataCOPE in the logged contextual
bandit settings using healthcare datasets confirms its ability to evaluate both
machine-learning and human expert policies like clinical guidelines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14115">A density estimation perspective on learning from pairwise human preferences. (arXiv:2311.14115v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dumoulin_V/0/1/0/all/0/1">Vincent Dumoulin</a>, <a href="http://arxiv.org/find/cs/1/au:+Johnson_D/0/1/0/all/0/1">Daniel D. Johnson</a>, <a href="http://arxiv.org/find/cs/1/au:+Castro_P/0/1/0/all/0/1">Pablo Samuel Castro</a>, <a href="http://arxiv.org/find/cs/1/au:+Larochelle_H/0/1/0/all/0/1">Hugo Larochelle</a>, <a href="http://arxiv.org/find/cs/1/au:+Dauphin_Y/0/1/0/all/0/1">Yann Dauphin</a></p>
<p>Learning from human feedback (LHF) -- and in particular learning from
pairwise preferences -- has recently become a crucial ingredient in training
large language models (LLMs), and has been the subject of much research. Most
recent works frame it as a reinforcement learning problem, where a reward
function is learned from pairwise preference data and the LLM is treated as a
policy which is adapted to maximize the rewards, often under additional
regularization constraints. We propose an alternative interpretation which
centers on the generative process for pairwise preferences and treats LHF as a
density estimation problem. We provide theoretical and empirical results
showing that for a family of generative processes defined via preference
behavior distribution equations, training a reward function on pairwise
preferences effectively models an annotator's implicit preference distribution.
Finally, we discuss and present findings on "annotator misspecification" --
failure cases where wrong modeling assumptions are made about annotator
behavior, resulting in poorly-adapted models -- suggesting that approaches that
learn from pairwise human preferences could have trouble learning from a
population of annotators with diverse viewpoints.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14125">Scalable AI Safety via Doubly-Efficient Debate. (arXiv:2311.14125v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Brown_Cohen_J/0/1/0/all/0/1">Jonah Brown-Cohen</a>, <a href="http://arxiv.org/find/cs/1/au:+Irving_G/0/1/0/all/0/1">Geoffrey Irving</a>, <a href="http://arxiv.org/find/cs/1/au:+Piliouras_G/0/1/0/all/0/1">Georgios Piliouras</a></p>
<p>The emergence of pre-trained AI systems with powerful capabilities across a
diverse and ever-increasing set of complex domains has raised a critical
challenge for AI safety as tasks can become too complicated for humans to judge
directly. Irving et al. [2018] proposed a debate method in this direction with
the goal of pitting the power of such AI models against each other until the
problem of identifying (mis)-alignment is broken down into a manageable
subtask. While the promise of this approach is clear, the original framework
was based on the assumption that the honest strategy is able to simulate
deterministic AI systems for an exponential number of steps, limiting its
applicability. In this paper, we show how to address these challenges by
designing a new set of debate protocols where the honest strategy can always
succeed using a simulation of a polynomial number of steps, whilst being able
to verify the alignment of stochastic AI systems, even when the dishonest
strategy is allowed to use exponentially many simulation steps.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14126">Towards Auditing Large Language Models: Improving Text-based Stereotype Detection. (arXiv:2311.14126v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zekun_W/0/1/0/all/0/1">Wu Zekun</a>, <a href="http://arxiv.org/find/cs/1/au:+Bulathwela_S/0/1/0/all/0/1">Sahan Bulathwela</a>, <a href="http://arxiv.org/find/cs/1/au:+Koshiyama_A/0/1/0/all/0/1">Adriano Soares Koshiyama</a></p>
<p>Large Language Models (LLM) have made significant advances in the recent past
becoming more mainstream in Artificial Intelligence (AI) enabled human-facing
applications. However, LLMs often generate stereotypical output inherited from
historical data, amplifying societal biases and raising ethical concerns. This
work introduces i) the Multi-Grain Stereotype Dataset, which includes 52,751
instances of gender, race, profession and religion stereotypic text and ii) a
novel stereotype classifier for English text. We design several experiments to
rigorously test the proposed model trained on the novel dataset. Our
experiments show that training the model in a multi-class setting can
outperform the one-vs-all binary counterpart. Consistent feature importance
signals from different eXplainable AI tools demonstrate that the new model
exploits relevant text features. We utilise the newly created model to assess
the stereotypic behaviour of the popular GPT family of models and observe the
reduction of bias over time. In summary, our work establishes a robust and
practical framework for auditing and evaluating the stereotypic bias in LLM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14127">Byzantine Robustness and Partial Participation Can Be Achieved Simultaneously: Just Clip Gradient Differences. (arXiv:2311.14127v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Malinovsky_G/0/1/0/all/0/1">Grigory Malinovsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Richtarik_P/0/1/0/all/0/1">Peter Richt&#xe1;rik</a>, <a href="http://arxiv.org/find/cs/1/au:+Horvath_S/0/1/0/all/0/1">Samuel Horv&#xe1;th</a>, <a href="http://arxiv.org/find/cs/1/au:+Gorbunov_E/0/1/0/all/0/1">Eduard Gorbunov</a></p>
<p>Distributed learning has emerged as a leading paradigm for training large
machine learning models. However, in real-world scenarios, participants may be
unreliable or malicious, posing a significant challenge to the integrity and
accuracy of the trained models. Byzantine fault tolerance mechanisms have been
proposed to address these issues, but they often assume full participation from
all clients, which is not always practical due to the unavailability of some
clients or communication constraints. In our work, we propose the first
distributed method with client sampling and provable tolerance to Byzantine
workers. The key idea behind the developed method is the use of gradient
clipping to control stochastic gradient differences in recursive variance
reduction. This allows us to bound the potential harm caused by Byzantine
workers, even during iterations when all sampled clients are Byzantine.
Furthermore, we incorporate communication compression into the method to
enhance communication efficiency. Under quite general assumptions, we prove
convergence rates for the proposed method that match the existing
state-of-the-art (SOTA) theoretical results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14139">Machine Learning For An Explainable Cost Prediction of Medical Insurance. (arXiv:2311.14139v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Orji_U/0/1/0/all/0/1">Ugochukwu Orji</a>, <a href="http://arxiv.org/find/cs/1/au:+Ukwandu_E/0/1/0/all/0/1">Elochukwu Ukwandu</a></p>
<p>Predictive modeling in healthcare continues to be an active actuarial
research topic as more insurance companies aim to maximize the potential of
Machine Learning approaches to increase their productivity and efficiency. In
this paper, the authors deployed three regression-based ensemble ML models that
combine variations of decision trees through Extreme Gradient Boosting,
Gradient-boosting Machine, and Random Forest) methods in predicting medical
insurance costs. Explainable Artificial Intelligence methods SHapley Additive
exPlanations and Individual Conditional Expectation plots were deployed to
discover and explain the key determinant factors that influence medical
insurance premium prices in the dataset. The dataset used comprised 986 records
and is publicly available in the KAGGLE repository. The models were evaluated
using four performance evaluation metrics, including R-squared, Mean Absolute
Error, Root Mean Squared Error, and Mean Absolute Percentage Error. The results
show that all models produced impressive outcomes; however, the XGBoost model
achieved a better overall performance although it also expanded more
computational resources, while the RF model recorded a lesser prediction error
and consumed far fewer computing resources than the XGBoost model. Furthermore,
we compared the outcome of both XAi methods in identifying the key determinant
features that influenced the PremiumPrices for each model and whereas both XAi
methods produced similar outcomes, we found that the ICE plots showed in more
detail the interactions between each variable than the SHAP analysis which
seemed to be more high-level. It is the aim of the authors that the
contributions of this study will help policymakers, insurers, and potential
medical insurance buyers in their decision-making process for selecting the
right policies that meet their specific needs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14153">Tube-NeRF: Efficient Imitation Learning of Visuomotor Policies from MPC using Tube-Guided Data Augmentation and NeRFs. (arXiv:2311.14153v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tagliabue_A/0/1/0/all/0/1">Andrea Tagliabue</a>, <a href="http://arxiv.org/find/cs/1/au:+How_J/0/1/0/all/0/1">Jonathan P. How</a></p>
<p>Imitation learning (IL) can train computationally-efficient sensorimotor
policies from a resource-intensive Model Predictive Controller (MPC), but it
often requires many samples, leading to long training times or limited
robustness. To address these issues, we combine IL with a variant of robust MPC
that accounts for process and sensing uncertainties, and we design a data
augmentation (DA) strategy that enables efficient learning of vision-based
policies. The proposed DA method, named Tube-NeRF, leverages Neural Radiance
Fields (NeRFs) to generate novel synthetic images, and uses properties of the
robust MPC (the tube) to select relevant views and to efficiently compute the
corresponding actions. We tailor our approach to the task of localization and
trajectory tracking on a multirotor, by learning a visuomotor policy that
generates control actions using images from the onboard camera as only source
of horizontal position. Our evaluations numerically demonstrate learning of a
robust visuomotor policy with an 80-fold increase in demonstration efficiency
and a 50% reduction in training time over current IL methods. Additionally, our
policies successfully transfer to a real multirotor, achieving accurate
localization and low tracking errors despite large disturbances, with an
onboard inference time of only 1.5 ms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14156">Variational Annealing on Graphs for Combinatorial Optimization. (arXiv:2311.14156v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sanokowski_S/0/1/0/all/0/1">Sebastian Sanokowski</a>, <a href="http://arxiv.org/find/cs/1/au:+Berghammer_W/0/1/0/all/0/1">Wilhelm Berghammer</a>, <a href="http://arxiv.org/find/cs/1/au:+Hochreiter_S/0/1/0/all/0/1">Sepp Hochreiter</a>, <a href="http://arxiv.org/find/cs/1/au:+Lehner_S/0/1/0/all/0/1">Sebastian Lehner</a></p>
<p>Several recent unsupervised learning methods use probabilistic approaches to
solve combinatorial optimization (CO) problems based on the assumption of
statistically independent solution variables. We demonstrate that this
assumption imposes performance limitations in particular on difficult problem
instances. Our results corroborate that an autoregressive approach which
captures statistical dependencies among solution variables yields superior
performance on many popular CO problems. We introduce subgraph tokenization in
which the configuration of a set of solution variables is represented by a
single token. This tokenization technique alleviates the drawback of the long
sequential sampling procedure which is inherent to autoregressive methods
without sacrificing expressivity. Importantly, we theoretically motivate an
annealed entropy regularization and show empirically that it is essential for
efficient and stable learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14169">Evaluating GPT-4&#x27;s Vision Capabilities on Brazilian University Admission Exams. (arXiv:2311.14169v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pires_R/0/1/0/all/0/1">Ramon Pires</a>, <a href="http://arxiv.org/find/cs/1/au:+Almeida_T/0/1/0/all/0/1">Thales Sales Almeida</a>, <a href="http://arxiv.org/find/cs/1/au:+Abonizio_H/0/1/0/all/0/1">Hugo Abonizio</a>, <a href="http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1">Rodrigo Nogueira</a></p>
<p>Recent advancements in language models have showcased human-comparable
performance in academic entrance exams. However, existing studies often
overlook questions that require the integration of visual comprehension, thus
compromising the full spectrum and complexity inherent in real-world scenarios.
To address this gap, we present a comprehensive framework to evaluate language
models on entrance exams, which incorporates both textual and visual elements.
We evaluate the two most recent editions of Exame Nacional do Ensino M\'edio
(ENEM), the main standardized entrance examination adopted by Brazilian
universities. Our study not only reaffirms the capabilities of GPT-4 as the
state of the art for handling complex multidisciplinary questions, but also
pioneers in offering a realistic assessment of multimodal language models on
Portuguese examinations. One of the highlights is that text captions
transcribing visual content outperform the direct use of images, suggesting
that the vision model has room for improvement. Yet, despite improvements
afforded by images or captions, mathematical questions remain a challenge for
these state-of-the-art models. The code and data used on experiments are
available at https://github.com/piresramon/gpt-4-enem.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14175">Appearance-based gaze estimation enhanced with synthetic images using deep neural networks. (arXiv:2311.14175v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Herashchenko_D/0/1/0/all/0/1">Dmytro Herashchenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Farkas_I/0/1/0/all/0/1">Igor Farka&#x161;</a></p>
<p>Human eye gaze estimation is an important cognitive ingredient for successful
human-robot interaction, enabling the robot to read and predict human behavior.
We approach this problem using artificial neural networks and build a modular
system estimating gaze from separately cropped eyes, taking advantage of
existing well-functioning components for face detection (RetinaFace) and head
pose estimation (6DRepNet). Our proposed method does not require any special
hardware or infrared filters but uses a standard notebook-builtin RGB camera,
as often approached with appearance-based methods. Using the MetaHuman tool, we
also generated a large synthetic dataset of more than 57,000 human faces and
made it publicly available. The inclusion of this dataset (with eye gaze and
head pose information) on top of the standard Columbia Gaze dataset into
training the model led to better accuracy with a mean average error below two
degrees in eye pitch and yaw directions, which compares favourably to related
methods. We also verified the feasibility of our model by its preliminary
testing in real-world setting using the builtin 4K camera in NICO semi-humanoid
robot's eye.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2103.13192">On Preference Learning Based on Sequential Bayesian Optimization with Pairwise Comparison. (arXiv:2103.13192v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ignatenko_T/0/1/0/all/0/1">Tanya Ignatenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Kondrashov_K/0/1/0/all/0/1">Kirill Kondrashov</a>, <a href="http://arxiv.org/find/cs/1/au:+Cox_M/0/1/0/all/0/1">Marco Cox</a>, <a href="http://arxiv.org/find/cs/1/au:+Vries_B/0/1/0/all/0/1">Bert de Vries</a></p>
<p>User preference learning is generally a hard problem. Individual preferences
are typically unknown even to users themselves, while the space of choices is
infinite. Here we study user preference learning from information-theoretic
perspective. We model preference learning as a system with two interacting
sub-systems, one representing a user with his/her preferences and another one
representing an agent that has to learn these preferences. The user with
his/her behaviour is modeled by a parametric preference function. To
efficiently learn the preferences and reduce search space quickly, we propose
the agent that interacts with the user to collect the most informative data for
learning. The agent presents two proposals to the user for evaluation, and the
user rates them based on his/her preference function. We show that the optimum
agent strategy for data collection and preference learning is a result of
maximin optimization of the normalized weighted Kullback-Leibler (KL)
divergence between true and agent-assigned predictive user response
distributions. The resulting value of KL-divergence, which we also call
remaining system uncertainty (RSU), provides an efficient performance metric in
the absence of the ground truth. This metric characterises how well the agent
can predict user and, thus, the quality of the underlying learned user
(preference) model. Our proposed agent comprises sequential mechanisms for user
model inference and proposal generation. To infer the user model (preference
function), Bayesian approximate inference is used in the agent. The data
collection strategy is to generate proposals, responses to which help resolving
uncertainty associated with prediction of the user responses the most. The
efficiency of our approach is validated by numerical simulations. Also a
real-life example of preference learning application is provided.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.11954">XAutoML: A Visual Analytics Tool for Understanding and Validating Automated Machine Learning. (arXiv:2202.11954v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zoller_M/0/1/0/all/0/1">Marc-Andr&#xe9; Z&#xf6;ller</a>, <a href="http://arxiv.org/find/cs/1/au:+Titov_W/0/1/0/all/0/1">Waldemar Titov</a>, <a href="http://arxiv.org/find/cs/1/au:+Schlegel_T/0/1/0/all/0/1">Thomas Schlegel</a>, <a href="http://arxiv.org/find/cs/1/au:+Huber_M/0/1/0/all/0/1">Marco F. Huber</a></p>
<p>In the last ten years, various automated machine learning (AutoM ) systems
have been proposed to build end-to-end machine learning (ML) pipelines with
minimal human interaction. Even though such automatically synthesized ML
pipelines are able to achieve a competitive performance, recent studies have
shown that users do not trust models constructed by AutoML due to missing
transparency of AutoML systems and missing explanations for the constructed ML
pipelines. In a requirements analysis study with 36 domain experts, data
scientists, and AutoML researchers from different professions with vastly
different expertise in ML, we collect detailed informational needs for AutoML.
We propose XAutoML, an interactive visual analytics tool for explaining
arbitrary AutoML optimization procedures and ML pipelines constructed by
AutoML. XAutoML combines interactive visualizations with established techniques
from explainable artificial intelligence (XAI) to make the complete AutoML
procedure transparent and explainable. By integrating XAutoML with JupyterLab,
experienced users can extend the visual analytics with ad-hoc visualizations
based on information extracted from XAutoML. We validate our approach in a user
study with the same diverse user group from the requirements analysis. All
participants were able to extract useful information from XAutoML, leading to a
significantly increased understanding of ML pipelines produced by AutoML and
the AutoML optimization itself.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.02205">Evaluating Object (mis)Detection from a Safety and Reliability Perspective: Discussion and Measures. (arXiv:2203.02205v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ceccarelli_A/0/1/0/all/0/1">Andrea Ceccarelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Montecchi_L/0/1/0/all/0/1">Leonardo Montecchi</a></p>
<p>We argue that object detectors in the safety critical domain should
prioritize detection of objects that are most likely to interfere with the
actions of the autonomous actor. Especially, this applies to objects that can
impact the actor's safety and reliability. To quantify the impact of object
(mis)detection on safety and reliability in the context of autonomous driving,
we propose new object detection measures that reward the correct identification
of objects that are most dangerous and most likely to affect driving decisions.
To achieve this, we build an object criticality model to reward the detection
of the objects based on proximity, orientation, and relative velocity with
respect to the subject vehicle. Then, we apply our model on the recent
autonomous driving dataset nuScenes, and we compare nine object detectors.
Results show that, in several settings, object detectors that perform best
according to the nuScenes ranking are not the preferable ones when the focus is
shifted on safety and reliability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2204.10581">Fused Audio Instance and Representation for Respiratory Disease Detection. (arXiv:2204.10581v4 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1">Tuan Truong</a>, <a href="http://arxiv.org/find/cs/1/au:+Lenga_M/0/1/0/all/0/1">Matthias Lenga</a>, <a href="http://arxiv.org/find/cs/1/au:+Serrurier_A/0/1/0/all/0/1">Antoine Serrurier</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohammadi_S/0/1/0/all/0/1">Sadegh Mohammadi</a></p>
<p>Audio-based classification techniques on body sounds have long been studied
to aid in the diagnosis of respiratory diseases. While most research is
centered on the use of cough as the main biomarker, other body sounds also have
the potential to detect respiratory diseases. Recent studies on COVID-19 have
shown that breath and speech sounds, in addition to cough, correlate with the
disease. Our study proposes Fused Audio Instance and Representation (FAIR) as a
method for respiratory disease detection. FAIR relies on constructing a joint
feature vector from various body sounds represented in waveform and spectrogram
form. We conducted experiments on the use case of COVID-19 detection by
combining waveform and spectrogram representation of body sounds. Our findings
show that the use of self-attention to combine extracted features from cough,
breath, and speech sounds leads to the best performance with an Area Under the
Receiver Operating Characteristic Curve (AUC) score of 0.8658, a sensitivity of
0.8057, and a specificity of 0.7958. Compared to models trained solely on
spectrograms or waveforms, the use of both representations results in an
improved AUC score, demonstrating that combining spectrogram and waveform
representation helps to enrich the extracted features and outperforms the
models that use only one representation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.11952">3D helical CT Reconstruction with a Memory Efficient Learned Primal-Dual Architecture. (arXiv:2205.11952v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Rudzusika_J/0/1/0/all/0/1">Jevgenija Rudzusika</a>, <a href="http://arxiv.org/find/eess/1/au:+Bajic_B/0/1/0/all/0/1">Buda Baji&#x107;</a>, <a href="http://arxiv.org/find/eess/1/au:+Koehler_T/0/1/0/all/0/1">Thomas Koehler</a>, <a href="http://arxiv.org/find/eess/1/au:+Oktem_O/0/1/0/all/0/1">Ozan &#xd6;ktem</a></p>
<p>Deep learning based computed tomography (CT) reconstruction has demonstrated
outstanding performance on simulated 2D low-dose CT data. This applies in
particular to domain adapted neural networks, which incorporate a handcrafted
physics model for CT imaging. Empirical evidence shows that employing such
architectures reduces the demand for training data and improves upon
generalisation. However, their training requires large computational resources
that quickly become prohibitive in 3D helical CT, which is the most common
acquisition geometry used for medical imaging. Furthermore, clinical data also
comes with other challenges not accounted for in simulations, like errors in
flux measurement, resolution mismatch and, most importantly, the absence of the
real ground truth. The necessity to have a computationally feasible training
combined with the need to address these issues has made it difficult to
evaluate deep learning based reconstruction on clinical 3D helical CT. This
paper modifies a domain adapted neural network architecture, the Learned
Primal-Dual (LPD), so that it can be trained and applied to reconstruction in
this setting. We achieve this by splitting the helical trajectory into sections
and applying the unrolled LPD iterations to those sections sequentially. To the
best of our knowledge, this work is the first to apply an unrolled deep
learning architecture for reconstruction on full-sized clinical data, like
those in the Low dose CT image and projection data set (LDCT). Moreover,
training and testing is done on a single GPU card with 24GB of memory.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.00539">Dungeons and Data: A Large-Scale NetHack Dataset. (arXiv:2211.00539v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hambro_E/0/1/0/all/0/1">Eric Hambro</a>, <a href="http://arxiv.org/find/cs/1/au:+Raileanu_R/0/1/0/all/0/1">Roberta Raileanu</a>, <a href="http://arxiv.org/find/cs/1/au:+Rothermel_D/0/1/0/all/0/1">Danielle Rothermel</a>, <a href="http://arxiv.org/find/cs/1/au:+Mella_V/0/1/0/all/0/1">Vegard Mella</a>, <a href="http://arxiv.org/find/cs/1/au:+Rocktaschel_T/0/1/0/all/0/1">Tim Rockt&#xe4;schel</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuttler_H/0/1/0/all/0/1">Heinrich K&#xfc;ttler</a>, <a href="http://arxiv.org/find/cs/1/au:+Murray_N/0/1/0/all/0/1">Naila Murray</a></p>
<p>Recent breakthroughs in the development of agents to solve challenging
sequential decision making problems such as Go, StarCraft, or DOTA, have relied
on both simulated environments and large-scale datasets. However, progress on
this research has been hindered by the scarcity of open-sourced datasets and
the prohibitive computational cost to work with them. Here we present the
NetHack Learning Dataset (NLD), a large and highly-scalable dataset of
trajectories from the popular game of NetHack, which is both extremely
challenging for current methods and very fast to run. NLD consists of three
parts: 10 billion state transitions from 1.5 million human trajectories
collected on the NAO public NetHack server from 2009 to 2020; 3 billion
state-action-score transitions from 100,000 trajectories collected from the
symbolic bot winner of the NetHack Challenge 2021; and, accompanying code for
users to record, load and stream any collection of such trajectories in a
highly compressed form. We evaluate a wide range of existing algorithms
including online and offline RL, as well as learning from demonstrations,
showing that significant research advances are needed to fully leverage
large-scale datasets for challenging sequential decision making tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.03157">Examining the Differential Risk from High-level Artificial Intelligence and the Question of Control. (arXiv:2211.03157v4 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kilian_K/0/1/0/all/0/1">Kyle A. Kilian</a>, <a href="http://arxiv.org/find/cs/1/au:+Ventura_C/0/1/0/all/0/1">Christopher J. Ventura</a>, <a href="http://arxiv.org/find/cs/1/au:+Bailey_M/0/1/0/all/0/1">Mark M. Bailey</a></p>
<p>Artificial Intelligence (AI) is one of the most transformative technologies
of the 21st century. The extent and scope of future AI capabilities remain a
key uncertainty, with widespread disagreement on timelines and potential
impacts. As nations and technology companies race toward greater complexity and
autonomy in AI systems, there are concerns over the extent of integration and
oversight of opaque AI decision processes. This is especially true in the
subfield of machine learning (ML), where systems learn to optimize objectives
without human assistance. Objectives can be imperfectly specified or executed
in an unexpected or potentially harmful way. This becomes more concerning as
systems increase in power and autonomy, where an abrupt capability jump could
result in unexpected shifts in power dynamics or even catastrophic failures.
This study presents a hierarchical complex systems framework to model AI risk
and provide a template for alternative futures analysis. Survey data were
collected from domain experts in the public and private sectors to classify AI
impact and likelihood. The results show increased uncertainty over the powerful
AI agent scenario, confidence in multiagent environments, and increased concern
over AI alignment failures and influence-seeking behavior.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.11744">Visual Dexterity: In-Hand Reorientation of Novel and Complex Object Shapes. (arXiv:2211.11744v3 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tippur_M/0/1/0/all/0/1">Megha Tippur</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Siyang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1">Vikash Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Adelson_E/0/1/0/all/0/1">Edward Adelson</a>, <a href="http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1">Pulkit Agrawal</a></p>
<p>In-hand object reorientation is necessary for performing many dexterous
manipulation tasks, such as tool use in less structured environments that
remain beyond the reach of current robots. Prior works built reorientation
systems assuming one or many of the following: reorienting only specific
objects with simple shapes, limited range of reorientation, slow or quasistatic
manipulation, simulation-only results, the need for specialized and costly
sensor suites, and other constraints which make the system infeasible for
real-world deployment. We present a general object reorientation controller
that does not make these assumptions. It uses readings from a single commodity
depth camera to dynamically reorient complex and new object shapes by any
rotation in real-time, with the median reorientation time being close to seven
seconds. The controller is trained using reinforcement learning in simulation
and evaluated in the real world on new object shapes not used for training,
including the most challenging scenario of reorienting objects held in the air
by a downward-facing hand that must counteract gravity during reorientation.
Our hardware platform only uses open-source components that cost less than five
thousand dollars. Although we demonstrate the ability to overcome assumptions
in prior work, there is ample scope for improving absolute performance. For
instance, the challenging duck-shaped object not used for training was dropped
in 56 percent of the trials. When it was not dropped, our controller reoriented
the object within 0.4 radians (23 degrees) 75 percent of the time. Videos are
available at: https://taochenshh.github.io/projects/visual-dexterity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.18223">A Survey of Large Language Models. (arXiv:2303.18223v13 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Wayne Xin Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1">Kun Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Junyi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1">Tianyi Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaolei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1">Yupeng Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Min_Y/0/1/0/all/0/1">Yingqian Min</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Beichen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Junjie Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1">Zican Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1">Yifan Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Chen Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yushuo Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhipeng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1">Jinhao Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_R/0/1/0/all/0/1">Ruiyang Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yifan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1">Xinyu Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zikang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1">Peiyu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1">Jian-Yun Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1">Ji-Rong Wen</a></p>
<p>Language is essentially a complex, intricate system of human expressions
governed by grammatical rules. It poses a significant challenge to develop
capable AI algorithms for comprehending and grasping a language. As a major
approach, language modeling has been widely studied for language understanding
and generation in the past two decades, evolving from statistical language
models to neural language models. Recently, pre-trained language models (PLMs)
have been proposed by pre-training Transformer models over large-scale corpora,
showing strong capabilities in solving various NLP tasks. Since researchers
have found that model scaling can lead to performance improvement, they further
study the scaling effect by increasing the model size to an even larger size.
Interestingly, when the parameter scale exceeds a certain level, these enlarged
language models not only achieve a significant performance improvement but also
show some special abilities that are not present in small-scale language
models. To discriminate the difference in parameter scale, the research
community has coined the term large language models (LLM) for the PLMs of
significant size. Recently, the research on LLMs has been largely advanced by
both academia and industry, and a remarkable progress is the launch of ChatGPT,
which has attracted widespread attention from society. The technical evolution
of LLMs has been making an important impact on the entire AI community, which
would revolutionize the way how we develop and use AI algorithms. In this
survey, we review the recent advances of LLMs by introducing the background,
key findings, and mainstream techniques. In particular, we focus on four major
aspects of LLMs, namely pre-training, adaptation tuning, utilization, and
capacity evaluation. Besides, we also summarize the available resources for
developing LLMs and discuss the remaining issues for future directions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.05390">HRS-Bench: Holistic, Reliable and Scalable Benchmark for Text-to-Image Models. (arXiv:2304.05390v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bakr_E/0/1/0/all/0/1">Eslam Mohamed Bakr</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1">Pengzhan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1">Xiaoqian Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1">Faizan Farooq Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Li Erran Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1">Mohamed Elhoseiny</a></p>
<p>In recent years, Text-to-Image (T2I) models have been extensively studied,
especially with the emergence of diffusion models that achieve state-of-the-art
results on T2I synthesis tasks. However, existing benchmarks heavily rely on
subjective human evaluation, limiting their ability to holistically assess the
model's capabilities. Furthermore, there is a significant gap between efforts
in developing new T2I architectures and those in evaluation. To address this,
we introduce HRS-Bench, a concrete evaluation benchmark for T2I models that is
Holistic, Reliable, and Scalable. Unlike existing bench-marks that focus on
limited aspects, HRS-Bench measures 13 skills that can be categorized into five
major categories: accuracy, robustness, generalization, fairness, and bias. In
addition, HRS-Bench covers 50 scenarios, including fashion, animals,
transportation, food, and clothes. We evaluate nine recent large-scale T2I
models using metrics that cover a wide range of skills. A human evaluation
aligned with 95% of our evaluations on average was conducted to probe the
effectiveness of HRS-Bench. Our experiments demonstrate that existing models
often struggle to generate images with the desired count of objects, visual
text, or grounded emotions. We hope that our benchmark help ease future
text-to-image generation research. The code and data are available at
https://eslambakr.github.io/hrsbench.github.io
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.07061">AutoDroid-0shot: A Simple Baseline for GPT-powered UI-grounded Smartphone Task Automation in Android. (arXiv:2304.07061v4 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1">Hao Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hongming Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiaxuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuanchun Li</a></p>
<p>This paper introduces AutoDroid-0shot, a tool that utilizes GPT-like large
language models (LLMs) to automate the interactions with Android mobile
applications. Given a natural language description of a desired task,
AutoDroid-0shot can automatically generate and execute actions that navigate
the app to complete the task. It works by translating the app GUI state
information and the available actions on the smartphone screen to natural
language prompts and asking the LLM to make a choice of actions. Since the LLM
is typically trained on a large amount of data including the how-to manuals of
diverse software applications, it has the ability to make reasonable choices of
actions based on the provided information. We evaluate AutoDroid-0shot with a
self-created dataset that contains 33 tasks collected from 17 Android
applications spanning 10 categories. It can successfully complete 39.39% of the
tasks, and the average partial completion progress is about 66.76%. Given the
fact that our method is fully unsupervised (no modification required from both
the app and the LLM), we believe there is great potential to enhance automation
performance with better app development paradigms and/or custom model training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.01569">Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation. (arXiv:2305.01569v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kirstain_Y/0/1/0/all/0/1">Yuval Kirstain</a>, <a href="http://arxiv.org/find/cs/1/au:+Polyak_A/0/1/0/all/0/1">Adam Polyak</a>, <a href="http://arxiv.org/find/cs/1/au:+Singer_U/0/1/0/all/0/1">Uriel Singer</a>, <a href="http://arxiv.org/find/cs/1/au:+Matiana_S/0/1/0/all/0/1">Shahbuland Matiana</a>, <a href="http://arxiv.org/find/cs/1/au:+Penna_J/0/1/0/all/0/1">Joe Penna</a>, <a href="http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1">Omer Levy</a></p>
<p>The ability to collect a large dataset of human preferences from
text-to-image users is usually limited to companies, making such datasets
inaccessible to the public. To address this issue, we create a web app that
enables text-to-image users to generate images and specify their preferences.
Using this web app we build Pick-a-Pic, a large, open dataset of text-to-image
prompts and real users' preferences over generated images. We leverage this
dataset to train a CLIP-based scoring function, PickScore, which exhibits
superhuman performance on the task of predicting human preferences. Then, we
test PickScore's ability to perform model evaluation and observe that it
correlates better with human rankings than other automatic evaluation metrics.
Therefore, we recommend using PickScore for evaluating future text-to-image
generation models, and using Pick-a-Pic prompts as a more relevant dataset than
MS-COCO. Finally, we demonstrate how PickScore can enhance existing
text-to-image models via ranking.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19190">Inverse Approximation Theory for Nonlinear Recurrent Neural Networks. (arXiv:2305.19190v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shida Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qianxiao Li</a></p>
<p>We prove an inverse approximation theorem for the approximation of nonlinear
sequence-to-sequence relationships using recurrent neural networks (RNNs). This
is a so-called Bernstein-type result in approximation theory, which deduces
properties of a target function under the assumption that it can be effectively
approximated by a hypothesis space. In particular, we show that nonlinear
sequence relationships that can be stably approximated by nonlinear RNNs must
have an exponential decaying memory structure - a notion that can be made
precise. This extends the previously identified curse of memory in linear RNNs
into the general nonlinear setting, and quantifies the essential limitations of
the RNN architecture for learning sequential relationships with long-term
memory. Based on the analysis, we propose a principled reparameterization
method to overcome the limitations. Our theoretical results are confirmed by
numerical experiments. The code has been released in
https://github.com/radarFudan/Curse-of-memory
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.10548">MARBLE: Music Audio Representation Benchmark for Universal Evaluation. (arXiv:2306.10548v4 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_R/0/1/0/all/0/1">Ruibin Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yinghao Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yizhi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Ge Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xingran Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1">Hanzhi Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuo_L/0/1/0/all/0/1">Le Zhuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yiqi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jiawen Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1">Zeyue Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1">Binyue Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1">Ningzhi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1">Chenghua Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Benetos_E/0/1/0/all/0/1">Emmanouil Benetos</a>, <a href="http://arxiv.org/find/cs/1/au:+Ragni_A/0/1/0/all/0/1">Anton Ragni</a>, <a href="http://arxiv.org/find/cs/1/au:+Gyenge_N/0/1/0/all/0/1">Norbert Gyenge</a>, <a href="http://arxiv.org/find/cs/1/au:+Dannenberg_R/0/1/0/all/0/1">Roger Dannenberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wenhu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1">Gus Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1">Wei Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Si Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1">Ruibo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yike Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1">Jie Fu</a></p>
<p>In the era of extensive intersection between art and Artificial Intelligence
(AI), such as image generation and fiction co-creation, AI for music remains
relatively nascent, particularly in music understanding. This is evident in the
limited work on deep music representations, the scarcity of large-scale
datasets, and the absence of a universal and community-driven benchmark. To
address this issue, we introduce the Music Audio Representation Benchmark for
universaL Evaluation, termed MARBLE. It aims to provide a benchmark for various
Music Information Retrieval (MIR) tasks by defining a comprehensive taxonomy
with four hierarchy levels, including acoustic, performance, score, and
high-level description. We then establish a unified protocol based on 14 tasks
on 8 public-available datasets, providing a fair and standard assessment of
representations of all open-sourced pre-trained models developed on music
recordings as baselines. Besides, MARBLE offers an easy-to-use, extendable, and
reproducible suite for the community, with a clear statement on copyright
issues on datasets. Results suggest recently proposed large-scale pre-trained
musical language models perform the best in most tasks, with room for further
improvement. The leaderboard and toolkit repository are published at
https://marble-bm.shef.ac.uk to promote future music AI research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.10841">Blockchain-Enabled Federated Learning: A Reference Architecture Design, Implementation, and Verification. (arXiv:2306.10841v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Goh_E/0/1/0/all/0/1">Eunsu Goh</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1">Dae-Yeol Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kwangkee Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1">Suyeong Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Chae_J/0/1/0/all/0/1">Jong-Eui Chae</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1">Do-Yup Kim</a></p>
<p>This paper presents a novel reference architecture for blockchain-enabled
federated learning (BCFL), a state-of-the-art approach that amalgamates the
strengths of federated learning and blockchain technology.We define smart
contract functions, stakeholders and their roles, and the use of interplanetary
file system (IPFS) as key components of BCFL and conduct a comprehensive
analysis. In traditional centralized federated learning, the selection of local
nodes and the collection of learning results for each round are merged under
the control of a central server. In contrast, in BCFL, all these processes are
monitored and managed via smart contracts. Additionally, we propose an
extension architecture to support both crossdevice and cross-silo federated
learning scenarios. Furthermore, we implement and verify the architecture in a
practical real-world Ethereum development environment. Our BCFL reference
architecture provides significant flexibility and extensibility, accommodating
the integration of various additional elements, as per specific requirements
and use cases, thereby rendering it an adaptable solution for a wide range of
BCFL applications. As a prominent example of extensibility, decentralized
identifiers (DIDs) have been employed as an authentication method to introduce
practical utilization within BCFL. This study not only bridges a crucial gap
between research and practical deployment but also lays a solid foundation for
future explorations in the realm of BCFL. The pivotal contribution of this
study is the successful implementation and verification of a realistic BCFL
reference architecture. We intend to make the source code publicly accessible
shortly, fostering further advancements and adaptations within the community.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.13686">Broadening the perspective for sustainable AI: Comprehensive sustainability criteria and indicators for AI systems. (arXiv:2306.13686v2 [cs.CY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rohde_F/0/1/0/all/0/1">Friederike Rohde</a>, <a href="http://arxiv.org/find/cs/1/au:+Wagner_J/0/1/0/all/0/1">Josephin Wagner</a>, <a href="http://arxiv.org/find/cs/1/au:+Meyer_A/0/1/0/all/0/1">Andreas Meyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Reinhard_P/0/1/0/all/0/1">Philipp Reinhard</a>, <a href="http://arxiv.org/find/cs/1/au:+Voss_M/0/1/0/all/0/1">Marcus Voss</a>, <a href="http://arxiv.org/find/cs/1/au:+Petschow_U/0/1/0/all/0/1">Ulrich Petschow</a>, <a href="http://arxiv.org/find/cs/1/au:+Mollen_A/0/1/0/all/0/1">Anne Mollen</a></p>
<p>The increased use of AI systems is associated with multi-faceted societal,
environmental, and economic consequences. These include non-transparent
decision-making processes, discrimination, increasing inequalities, rising
energy consumption and greenhouse gas emissions in AI model development and
application, and an increasing concentration of economic power. By considering
the multi-dimensionality of sustainability, this paper takes steps towards
substantiating the call for an overarching perspective on "sustainable AI". It
presents the SCAIS Framework (Sustainability Criteria and Indicators for
Artificial Intelligence Systems) which contains a set 19 sustainability
criteria for sustainable AI and 67 indicators that is based on the results of a
critical review and expert workshops. This interdisciplinary approach
contributes a unique holistic perspective to facilitate and structure the
discourse on sustainable AI. Further, it provides a concrete framework that
lays the foundation for developing standards and tools to support the conscious
development and application of AI systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07871">The SocialAI School: Insights from Developmental Psychology Towards Artificial Socio-Cultural Agents. (arXiv:2307.07871v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kovac_G/0/1/0/all/0/1">Grgur Kova&#x10d;</a>, <a href="http://arxiv.org/find/cs/1/au:+Portelas_R/0/1/0/all/0/1">R&#xe9;my Portelas</a>, <a href="http://arxiv.org/find/cs/1/au:+Dominey_P/0/1/0/all/0/1">Peter Ford Dominey</a>, <a href="http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1">Pierre-Yves Oudeyer</a></p>
<p>Developmental psychologists have long-established the importance of
socio-cognitive abilities in human intelligence. These abilities enable us to
enter, participate and benefit from human culture. AI research on social
interactive agents mostly concerns the emergence of culture in a multi-agent
setting (often without a strong grounding in developmental psychology). We
argue that AI research should be informed by psychology and study
socio-cognitive abilities enabling to enter a culture too. We discuss the
theories of Michael Tomasello and Jerome Bruner to introduce some of their
concepts to AI and outline key concepts and socio-cognitive abilities. We
present The SocialAI school - a tool including a customizable parameterized
uite of procedurally generated environments, which simplifies conducting
experiments regarding those concepts. We show examples of such experiments with
RL agents and Large Language Models. The main motivation of this work is to
engage the AI community around the problem of social intelligence informed by
developmental psychology, and to provide a tool to simplify first steps in this
direction. Refer to the project website for code and additional information:
https://sites.google.com/view/socialai-school.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08187">An Empirical Investigation of Pre-trained Model Selection for Out-of-Distribution Generalization and Calibration. (arXiv:2307.08187v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Naganuma_H/0/1/0/all/0/1">Hiroki Naganuma</a>, <a href="http://arxiv.org/find/cs/1/au:+Hataya_R/0/1/0/all/0/1">Ryuichiro Hataya</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitliagkas_I/0/1/0/all/0/1">Ioannis Mitliagkas</a></p>
<p>In the realm of out-of-distribution (OOD) generalization tasks, fine-tuning
pre-trained models has become a prevalent strategy. Different from most prior
work that has focused on advancing learning algorithms, we systematically
examined how pre-trained model size, pre-training data scale, and training
strategies impact downstream generalization and uncertainty calibration. We
evaluated 97 models across diverse pre-trained model sizes, five pre-training
datasets, and five data augmentations through extensive experiments on four
distribution shift datasets totaling over 100,000 GPU hours. Our results
demonstrate the significant impact of pre-trained model selection, with optimal
choices substantially improving OOD accuracy over algorithm improvement alone.
We find larger models and bigger pre-training data improve OOD performance and
calibration, in contrast to some prior studies that found modern deep networks
to calibrate worse than classical shallow models. Our work underscores the
overlooked importance of pre-trained model selection for out-of-distribution
generalization and calibration.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.12777">Proceeding of the 1st Workshop on Social Robots Personalisation At the crossroads between engineering and humanities (CONCATENATE). (arXiv:2307.12777v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tarakli_I/0/1/0/all/0/1">Imene Tarakli</a>, <a href="http://arxiv.org/find/cs/1/au:+Angelopoulos_G/0/1/0/all/0/1">Georgios Angelopoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Hellou_M/0/1/0/all/0/1">Mehdi Hellou</a>, <a href="http://arxiv.org/find/cs/1/au:+Vindolet_C/0/1/0/all/0/1">Camille Vindolet</a>, <a href="http://arxiv.org/find/cs/1/au:+Abramovic_B/0/1/0/all/0/1">Boris Abramovic</a>, <a href="http://arxiv.org/find/cs/1/au:+Limongelli_R/0/1/0/all/0/1">Rocco Limongelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Lacroix_D/0/1/0/all/0/1">Dimitri Lacroix</a>, <a href="http://arxiv.org/find/cs/1/au:+Bertolini_A/0/1/0/all/0/1">Andrea Bertolini</a>, <a href="http://arxiv.org/find/cs/1/au:+Rossi_S/0/1/0/all/0/1">Silvia Rossi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nuovo_A/0/1/0/all/0/1">Alessandro Di Nuovo</a>, <a href="http://arxiv.org/find/cs/1/au:+Cangelosi_A/0/1/0/all/0/1">Angelo Cangelosi</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1">Gordon Cheng</a></p>
<p>Nowadays, robots are expected to interact more physically, cognitively, and
socially with people. They should adapt to unpredictable contexts alongside
individuals with various behaviours. For this reason, personalisation is a
valuable attribute for social robots as it allows them to act according to a
specific user's needs and preferences and achieve natural and transparent robot
behaviours for humans. If correctly implemented, personalisation could also be
the key to the large-scale adoption of social robotics. However, achieving
personalisation is arduous as it requires us to expand the boundaries of
robotics by taking advantage of the expertise of various domains. Indeed,
personalised robots need to analyse and model user interactions while
considering their involvement in the adaptative process. It also requires us to
address ethical and socio-cultural aspects of personalised HRI to achieve
inclusive and diverse interaction and avoid deception and misplaced trust when
interacting with the users. At the same time, policymakers need to ensure
regulations in view of possible short-term and long-term adaptive HRI. This
workshop aims to raise an interdisciplinary discussion on personalisation in
robotics. It aims at bringing researchers from different fields together to
propose guidelines for personalisation while addressing the following
questions: how to define it - how to achieve it - and how it should be guided
to fit legal and ethical requirements.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.09687">Graph of Thoughts: Solving Elaborate Problems with Large Language Models. (arXiv:2308.09687v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Besta_M/0/1/0/all/0/1">Maciej Besta</a>, <a href="http://arxiv.org/find/cs/1/au:+Blach_N/0/1/0/all/0/1">Nils Blach</a>, <a href="http://arxiv.org/find/cs/1/au:+Kubicek_A/0/1/0/all/0/1">Ales Kubicek</a>, <a href="http://arxiv.org/find/cs/1/au:+Gerstenberger_R/0/1/0/all/0/1">Robert Gerstenberger</a>, <a href="http://arxiv.org/find/cs/1/au:+Gianinazzi_L/0/1/0/all/0/1">Lukas Gianinazzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gajda_J/0/1/0/all/0/1">Joanna Gajda</a>, <a href="http://arxiv.org/find/cs/1/au:+Lehmann_T/0/1/0/all/0/1">Tomasz Lehmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Podstawski_M/0/1/0/all/0/1">Michal Podstawski</a>, <a href="http://arxiv.org/find/cs/1/au:+Niewiadomski_H/0/1/0/all/0/1">Hubert Niewiadomski</a>, <a href="http://arxiv.org/find/cs/1/au:+Nyczyk_P/0/1/0/all/0/1">Piotr Nyczyk</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoefler_T/0/1/0/all/0/1">Torsten Hoefler</a></p>
<p>We introduce Graph of Thoughts (GoT): a framework that advances prompting
capabilities in large language models (LLMs) beyond those offered by paradigms
such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary
advantage of GoT is the ability to model the information generated by an LLM as
an arbitrary graph, where units of information ("LLM thoughts") are vertices,
and edges correspond to dependencies between these vertices. This approach
enables combining arbitrary LLM thoughts into synergistic outcomes, distilling
the essence of whole networks of thoughts, or enhancing thoughts using feedback
loops. We illustrate that GoT offers advantages over state of the art on
different tasks, for example increasing the quality of sorting by 62% over ToT,
while simultaneously reducing costs by &gt;31%. We ensure that GoT is extensible
with new thought transformations and thus can be used to spearhead new
prompting schemes. This work brings the LLM reasoning closer to human thinking
or brain mechanisms such as recurrence, both of which form complex networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12319">RemovalNet: DNN Fingerprint Removal Attacks. (arXiv:2308.12319v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1">Hongwei Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1">Kunzhe Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1">Jian Lou</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1">Zhan Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1">Kui Ren</a></p>
<p>With the performance of deep neural networks (DNNs) remarkably improving,
DNNs have been widely used in many areas. Consequently, the DNN model has
become a valuable asset, and its intellectual property is safeguarded by
ownership verification techniques (e.g., DNN fingerprinting). However, the
feasibility of the DNN fingerprint removal attack and its potential influence
remains an open problem. In this paper, we perform the first comprehensive
investigation of DNN fingerprint removal attacks. Generally, the knowledge
contained in a DNN model can be categorized into general semantic and
fingerprint-specific knowledge. To this end, we propose a min-max bilevel
optimization-based DNN fingerprint removal attack named RemovalNet, to evade
model ownership verification. The lower-level optimization is designed to
remove fingerprint-specific knowledge. While in the upper-level optimization,
we distill the victim model's general semantic knowledge to maintain the
surrogate model's performance. We conduct extensive experiments to evaluate the
fidelity, effectiveness, and efficiency of the RemovalNet against four advanced
defense methods on six metrics. The empirical results demonstrate that (1) the
RemovalNet is effective. After our DNN fingerprint removal attack, the model
distance between the target and surrogate models is x100 times higher than that
of the baseline attacks, (2) the RemovalNet is efficient. It uses only 0.2%
(400 samples) of the substitute dataset and 1,000 iterations to conduct our
attack. Besides, compared with advanced model stealing attacks, the RemovalNet
saves nearly 85% of computational resources at most, (3) the RemovalNet
achieves high fidelity that the created surrogate model maintains high accuracy
after the DNN fingerprint removal process. Our code is available at:
https://github.com/grasses/RemovalNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.14936">Auto-Prompting SAM for Mobile Friendly 3D Medical Image Segmentation. (arXiv:2308.14936v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chengyin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Khanduri_P/0/1/0/all/0/1">Prashant Khanduri</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiang_Y/0/1/0/all/0/1">Yao Qiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sultan_R/0/1/0/all/0/1">Rafi Ibn Sultan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chetty_I/0/1/0/all/0/1">Indrin Chetty</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1">Dongxiao Zhu</a></p>
<p>Segment Anything Model (SAM) has rapidly been adopted for segmenting a wide
range of natural images. However, recent studies have indicated that SAM
exhibits subpar performance on 3D medical image segmentation tasks. In addition
to the domain gaps between natural and medical images, disparities in the
spatial arrangement between 2D and 3D images, the substantial computational
burden imposed by powerful GPU servers, and the time-consuming manual prompt
generation impede the extension of SAM to a broader spectrum of medical image
segmentation applications. To mitigate these challenges, we introduce a novel
method, AutoSAM Adapter, designed specifically for 3D multi-organ CT-based
segmentation. This approach utilizes parameter-efficient adaptation techniques
and an automatic prompt learning paradigm, transforming SAM's capabilities for
3D medical image segmentation. It eliminates the need for manual prompts and
achieves SOTA performance in CT-based multi-organ segmentation tasks.
Furthermore, we successfully transfer the acquired knowledge of the AutoSAM
Adapter to other lightweight models tailored for 3D medical image analysis with
enhanced performance. Through extensive experiments, the AutoSAM Adapter has
been demonstrated as an effective method to adapt the foundational SAM-based 2D
natural image segmentation model for 3D medical image segmentation tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.05238">Generating Natural Language Queries for More Effective Systematic Review Screening Prioritisation. (arXiv:2309.05238v3 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Scells_H/0/1/0/all/0/1">Harrisen Scells</a>, <a href="http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1">Martin Potthast</a>, <a href="http://arxiv.org/find/cs/1/au:+Koopman_B/0/1/0/all/0/1">Bevan Koopman</a>, <a href="http://arxiv.org/find/cs/1/au:+Zuccon_G/0/1/0/all/0/1">Guido Zuccon</a></p>
<p>Screening prioritisation in medical systematic reviews aims to rank the set
of documents retrieved by complex Boolean queries. Prioritising the most
important documents ensures that subsequent review steps can be carried out
more efficiently and effectively. The current state of the art uses the final
title of the review as a query to rank the documents using BERT-based neural
rankers. However, the final title is only formulated at the end of the review
process, which makes this approach impractical as it relies on ex post facto
information. At the time of screening, only a rough working title is available,
with which the BERT-based ranker performs significantly worse than with the
final title. In this paper, we explore alternative sources of queries for
prioritising screening, such as the Boolean query used to retrieve the
documents to be screened and queries generated by instruction-based generative
large-scale language models such as ChatGPT and Alpaca. Our best approach is
not only viable based on the information available at the time of screening,
but also has similar effectiveness to the final title.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.17169">An evaluation of GPT models for phenotype concept recognition. (arXiv:2309.17169v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Groza_T/0/1/0/all/0/1">Tudor Groza</a>, <a href="http://arxiv.org/find/cs/1/au:+Caufield_H/0/1/0/all/0/1">Harry Caufield</a>, <a href="http://arxiv.org/find/cs/1/au:+Gration_D/0/1/0/all/0/1">Dylan Gration</a>, <a href="http://arxiv.org/find/cs/1/au:+Baynam_G/0/1/0/all/0/1">Gareth Baynam</a>, <a href="http://arxiv.org/find/cs/1/au:+Haendel_M/0/1/0/all/0/1">Melissa A Haendel</a>, <a href="http://arxiv.org/find/cs/1/au:+Robinson_P/0/1/0/all/0/1">Peter N Robinson</a>, <a href="http://arxiv.org/find/cs/1/au:+Mungall_C/0/1/0/all/0/1">Christopher J Mungall</a>, <a href="http://arxiv.org/find/cs/1/au:+Reese_J/0/1/0/all/0/1">Justin T Reese</a></p>
<p>Objective: Clinical deep phenotyping and phenotype annotation play a critical
role in both the diagnosis of patients with rare disorders as well as in
building computationally-tractable knowledge in the rare disorders field. These
processes rely on using ontology concepts, often from the Human Phenotype
Ontology, in conjunction with a phenotype concept recognition task (supported
usually by machine learning methods) to curate patient profiles or existing
scientific literature. With the significant shift in the use of large language
models (LLMs) for most NLP tasks, we examine the performance of the latest
Generative Pre-trained Transformer (GPT) models underpinning ChatGPT as a
foundation for the tasks of clinical phenotyping and phenotype annotation.
Materials and Methods: The experimental setup of the study included seven
prompts of various levels of specificity, two GPT models (gpt-3.5-turbo and
gpt-4.0) and two established gold standard corpora for phenotype recognition,
one consisting of publication abstracts and the other clinical observations.
Results: Our results show that, with an appropriate setup, these models can
achieve state of the art performance. The best run, using few-shot learning,
achieved 0.58 macro F1 score on publication abstracts and 0.75 macro F1 score
on clinical observations, the former being comparable with the state of the
art, while the latter surpassing the current best in class tool. Conclusion:
While the results are promising, the non-deterministic nature of the outcomes,
the high cost and the lack of concordance between different runs using the same
prompt and input make the use of these LLMs challenging for this particular
task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00116">Certified Robustness via Dynamic Margin Maximization and Improved Lipschitz Regularization. (arXiv:2310.00116v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fazlyab_M/0/1/0/all/0/1">Mahyar Fazlyab</a>, <a href="http://arxiv.org/find/cs/1/au:+Entesari_T/0/1/0/all/0/1">Taha Entesari</a>, <a href="http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1">Aniket Roy</a>, <a href="http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1">Rama Chellappa</a></p>
<p>To improve the robustness of deep classifiers against adversarial
perturbations, many approaches have been proposed, such as designing new
architectures with better robustness properties (e.g., Lipschitz-capped
networks), or modifying the training process itself (e.g., min-max
optimization, constrained learning, or regularization). These approaches,
however, might not be effective at increasing the margin in the input (feature)
space. As a result, there has been an increasing interest in developing
training procedures that can directly manipulate the decision boundary in the
input space. In this paper, we build upon recent developments in this category
by developing a robust training algorithm whose objective is to increase the
margin in the output (logit) space while regularizing the Lipschitz constant of
the model along vulnerable directions. We show that these two objectives can
directly promote larger margins in the input space. To this end, we develop a
scalable method for calculating guaranteed differentiable upper bounds on the
Lipschitz constant of neural networks accurately and efficiently. The relative
accuracy of the bounds prevents excessive regularization and allows for more
direct manipulation of the decision boundary. Furthermore, our Lipschitz
bounding algorithm exploits the monotonicity and Lipschitz continuity of the
activation layers, and the resulting bounds can be used to design new layers
with controllable bounds on their Lipschitz constant. Experiments on the MNIST,
CIFAR-10, and Tiny-ImageNet data sets verify that our proposed algorithm
obtains competitively improved results compared to the state-of-the-art.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00583">City Foundation Models for Learning General Purpose Representations from OpenStreetMap. (arXiv:2310.00583v2 [cs.DB] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Balsebre_P/0/1/0/all/0/1">Pasquale Balsebre</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1">Weiming Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cong_G/0/1/0/all/0/1">Gao Cong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yi Li</a></p>
<p>Pre-trained Foundation Models (PFMs) have ushered in a paradigm-shift in
Artificial Intelligence, due to their ability to learn general-purpose
representations that can be readily employed in a wide range of downstream
tasks. While PFMs have been successfully adopted in various fields such as
Natural Language Processing and Computer Vision, their capacity in handling
geospatial data and answering urban questions remains limited. This can be
attributed to the intrinsic heterogeneity of geospatial data, which encompasses
different data types, including points, segments and regions, as well as
multiple information modalities, such as a spatial position, visual
characteristics and textual annotations. The proliferation of Volunteered
Geographic Information initiatives, and the ever-increasing availability of
open geospatial data sources, like OpenStreetMap, which is freely accessible
globally, unveil a promising opportunity to bridge this gap. In this paper, we
present CityFM, a self-supervised framework to train a foundation model within
a selected geographical area of interest, such as a city. CityFM relies solely
on open data from OSM, and produces multimodal representations of entities of
different types, incorporating spatial, visual, and textual information. We
analyse the entity representations generated using our foundation models from a
qualitative perspective, and conduct quantitative experiments on road,
building, and region-level downstream tasks. We compare its results to
algorithms tailored specifically for the respective applications. In all the
experiments, CityFM achieves performance superior to, or on par with, the
baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04483">Reward Dropout Improves Control: Bi-objective Perspective on Reinforced LM. (arXiv:2310.04483v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1">Changhun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_C/0/1/0/all/0/1">Chiehyeon Lim</a></p>
<p>We study the theoretical aspects of Reinforced Language Models (RLMs) from a
bi-objective optimization perspective. Specifically, we consider the RLMs as a
Pareto optimization problem that maximizes the two conflicting objectives,
i.e., reward objective and likelihood objectives, simultaneously. Our main
contribution consists of three parts. First, we establish the theoretical
foundations of RLM as a Pareto optimization problem by presenting Reward Upper
BOund (RUBO) and Pareto optimality. Our theoretical outcomes are supported by
not only deductive proofs but also empirical results. Second, we propose Reward
Dropout, a simple yet powerful method that guarantees to improve a bi-objective
optimization of RLM. Lastly, we demonstrate that the Reward Dropout is
consistently effective across five benchmark datasets and four benchmark LLMs,
meaning that the Reward Dropout significantly improves the optimization
performance of RLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05028">Revisiting Large Language Models as Zero-shot Relation Extractors. (arXiv:2310.05028v4 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1">Guozheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Peng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ke_W/0/1/0/all/0/1">Wenjun Ke</a></p>
<p>Relation extraction (RE) consistently involves a certain degree of labeled or
unlabeled data even if under zero-shot setting. Recent studies have shown that
large language models (LLMs) transfer well to new tasks out-of-the-box simply
given a natural language prompt, which provides the possibility of extracting
relations from text without any data and parameter tuning. This work focuses on
the study of exploring LLMs, such as ChatGPT, as zero-shot relation extractors.
On the one hand, we analyze the drawbacks of existing RE prompts and attempt to
incorporate recent prompt techniques such as chain-of-thought (CoT) to improve
zero-shot RE. We propose the summarize-and-ask (\textsc{SumAsk}) prompting, a
simple prompt recursively using LLMs to transform RE inputs to the effective
question answering (QA) format. On the other hand, we conduct comprehensive
experiments on various benchmarks and settings to investigate the capabilities
of LLMs on zero-shot RE. Specifically, we have the following findings: (i)
\textsc{SumAsk} consistently and significantly improves LLMs performance on
different model sizes, benchmarks and settings; (ii) Zero-shot prompting with
ChatGPT achieves competitive or superior results compared with zero-shot and
fully supervised methods; (iii) LLMs deliver promising performance in
extracting overlapping relations; (iv) The performance varies greatly regarding
different relations. Different from small language models, LLMs are effective
in handling challenge none-of-the-above (NoTA) relation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05052">Accurate battery lifetime prediction across diverse aging conditions with deep learning. (arXiv:2310.05052v3 [eess.SP] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1">Han Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1">Yuqi Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Zheng_S/0/1/0/all/0/1">Shun Zheng</a>, <a href="http://arxiv.org/find/eess/1/au:+Lu_Z/0/1/0/all/0/1">Ziheng Lu</a>, <a href="http://arxiv.org/find/eess/1/au:+Gui_X/0/1/0/all/0/1">Xiaofan Gui</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_W/0/1/0/all/0/1">Wei Xu</a>, <a href="http://arxiv.org/find/eess/1/au:+Bian_J/0/1/0/all/0/1">Jiang Bian</a></p>
<p>Accurately predicting the lifetime of battery cells in early cycles holds
tremendous value for battery research and development as well as numerous
downstream applications. This task is rather challenging because diverse
conditions, such as electrode materials, operating conditions, and working
environments, collectively determine complex capacity-degradation behaviors.
However, current prediction methods are developed and validated under limited
aging conditions, resulting in questionable adaptability to varied aging
conditions and an inability to fully benefit from historical data collected
under different conditions. Here we introduce a universal deep learning
approach that is capable of accommodating various aging conditions and
facilitating effective learning under low-resource conditions by leveraging
data from rich conditions. Our key finding is that incorporating inter-cell
feature differences, rather than solely considering single-cell
characteristics, significantly increases the accuracy of battery lifetime
prediction and its cross-condition robustness. Accordingly, we develop a
holistic learning framework accommodating both single-cell and inter-cell
modeling. A comprehensive benchmark is built for evaluation, encompassing 401
battery cells utilizing 5 prevalent electrode materials across 168 cycling
conditions. We demonstrate remarkable capabilities in learning across diverse
aging conditions, exclusively achieving 10% prediction error using the first
100 cycles, and in facilitating low-resource learning, almost halving the error
of single-cell modeling in many cases. More broadly, by breaking the learning
boundaries among different aging conditions, our approach could significantly
accelerate the development and optimization of lithium-ion batteries.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05898">Lion Secretly Solves Constrained Optimization: As Lyapunov Predicts. (arXiv:2310.05898v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lizhang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1">Kaizhao Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qiang Liu</a></p>
<p>Lion (Evolved Sign Momentum), a new optimizer discovered through program
search, has shown promising results in training large AI models. It performs
comparably or favorably to AdamW but with greater memory efficiency. As we can
expect from the results of a random search program, Lion incorporates elements
from several existing algorithms, including signed momentum, decoupled weight
decay, Polak, and Nesterov momentum, but does not fit into any existing
category of theoretically grounded optimizers. Thus, even though Lion appears
to perform well as a general-purpose optimizer for a wide range of tasks, its
theoretical basis remains uncertain. This lack of theoretical clarity limits
opportunities to further enhance and expand Lion's efficacy.
</p>
<p>This work aims to demystify Lion. Based on both continuous-time and
discrete-time analysis, we demonstrate that Lion is a theoretically novel and
principled approach for minimizing a general loss function $f(x)$ while
enforcing a bound constraint $\|x\|_\infty \leq 1/\lambda$. Lion achieves this
through the incorporation of decoupled weight decay, where $\lambda$ represents
the weight decay coefficient. Our analysis is made possible by the development
of a new Lyapunov function for the Lion updates. It applies to a broader family
of Lion-$\kappa$ algorithms, where the $\text{sign}(\cdot)$ operator in Lion is
replaced by the subgradient of a convex function $\kappa$, leading to the
solution of a general composite optimization problem of $\min_x f(x) +
\kappa^*(x)$. Our findings provide valuable insights into the dynamics of Lion
and pave the way for further improvements and extensions of Lion-related
algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.13007">A Critical Survey on Fairness Benefits of XAI. (arXiv:2310.13007v4 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Deck_L/0/1/0/all/0/1">Luca Deck</a>, <a href="http://arxiv.org/find/cs/1/au:+Schoeffer_J/0/1/0/all/0/1">Jakob Schoeffer</a>, <a href="http://arxiv.org/find/cs/1/au:+De_Arteaga_M/0/1/0/all/0/1">Maria De-Arteaga</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuhl_N/0/1/0/all/0/1">Niklas K&#xfc;hl</a></p>
<p>In this critical survey, we analyze typical claims on the relationship
between explainable AI (XAI) and fairness to disentangle the multidimensional
relationship between these two concepts. Based on a systematic literature
review and a subsequent qualitative content analysis, we identify seven
archetypal claims from 175 papers on the alleged fairness benefits of XAI. We
present crucial caveats with respect to these claims and provide an entry point
for future discussions around the potentials and limitations of XAI for
specific fairness desiderata. While the literature often suggests XAI to be an
enabler for several fairness desiderata, we notice a divide between these
desiderata and the capabilities of XAI. We encourage to conceive XAI as one of
many tools to approach the multidimensional, sociotechnical challenge of
algorithmic fairness and to be more specific about how exactly what kind of XAI
method enables whom to address which fairness desideratum.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.13102">Particle Guidance: non-I.I.D. Diverse Sampling with Diffusion Models. (arXiv:2310.13102v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Corso_G/0/1/0/all/0/1">Gabriele Corso</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yilun Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bortoli_V/0/1/0/all/0/1">Valentin de Bortoli</a>, <a href="http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1">Regina Barzilay</a>, <a href="http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1">Tommi Jaakkola</a></p>
<p>In light of the widespread success of generative models, a significant amount
of research has gone into speeding up their sampling time. However, generative
models are often sampled multiple times to obtain a diverse set incurring a
cost that is orthogonal to sampling time. We tackle the question of how to
improve diversity and sample efficiency by moving beyond the common assumption
of independent samples. We propose particle guidance, an extension of
diffusion-based generative sampling where a joint-particle time-evolving
potential enforces diversity. We analyze theoretically the joint distribution
that particle guidance generates, how to learn a potential that achieves
optimal diversity, and the connections with methods in other disciplines.
Empirically, we test the framework both in the setting of conditional image
generation, where we are able to increase diversity without affecting quality,
and molecular conformer generation, where we reduce the state-of-the-art median
error by 13% on average.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.14790">Weighted Joint Maximum Mean Discrepancy Enabled Multi-Source-Multi-Target Unsupervised Domain Adaptation Fault Diagnosis. (arXiv:2310.14790v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zixuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1">Haoran Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haibo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1">Bo Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Butala_M/0/1/0/all/0/1">Mark D. Butala</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1">Weiming Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hongwei Wang</a></p>
<p>Despite the remarkable results that can be achieved by data-driven
intelligent fault diagnosis techniques, they presuppose the same distribution
of training and test data as well as sufficient labeled data. Various operating
states often exist in practical scenarios, leading to the problem of domain
shift that hinders the effectiveness of fault diagnosis. While recent
unsupervised domain adaptation methods enable cross-domain fault diagnosis,
they struggle to effectively utilize information from multiple source domains
and achieve effective diagnosis faults in multiple target domains
simultaneously. In this paper, we innovatively proposed a weighted joint
maximum mean discrepancy enabled multi-source-multi-target unsupervised domain
adaptation (WJMMD-MDA), which realizes domain adaptation under
multi-source-multi-target scenarios in the field of fault diagnosis for the
first time. The proposed method extracts sufficient information from multiple
labeled source domains and achieves domain alignment between source and target
domains through an improved weighted distance loss. As a result,
domain-invariant and discriminative features between multiple source and target
domains are learned with cross-domain fault diagnosis realized. The performance
of the proposed method is evaluated in comprehensive comparative experiments on
three datasets, and the experimental results demonstrate the superiority of
this method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.15386">Course Correcting Koopman Representations. (arXiv:2310.15386v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fathi_M/0/1/0/all/0/1">Mahan Fathi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gehring_C/0/1/0/all/0/1">Clement Gehring</a>, <a href="http://arxiv.org/find/cs/1/au:+Pilault_J/0/1/0/all/0/1">Jonathan Pilault</a>, <a href="http://arxiv.org/find/cs/1/au:+Kanaa_D/0/1/0/all/0/1">David Kanaa</a>, <a href="http://arxiv.org/find/cs/1/au:+Bacon_P/0/1/0/all/0/1">Pierre-Luc Bacon</a>, <a href="http://arxiv.org/find/cs/1/au:+Goroshin_R/0/1/0/all/0/1">Ross Goroshin</a></p>
<p>Koopman representations aim to learn features of nonlinear dynamical systems
(NLDS) which lead to linear dynamics in the latent space. Theoretically, such
features can be used to simplify many problems in modeling and control of NLDS.
In this work we study autoencoder formulations of this problem, and different
ways they can be used to model dynamics, specifically for future state
prediction over long horizons. We discover several limitations of predicting
future states in the latent space and propose an inference-time mechanism,
which we refer to as Periodic Reencoding, for faithfully capturing long term
dynamics. We justify this method both analytically and empirically via
experiments in low and high dimensional NLDS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.18075">DUMA: a Dual-Mind Conversational Agent with Fast and Slow Thinking. (arXiv:2310.18075v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1">Xiaoyu Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Liangyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1">Na Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yaxuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_W/0/1/0/all/0/1">Wei Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Kaijiang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_M/0/1/0/all/0/1">Ming Cui</a></p>
<p>Inspired by the dual-process theory of human cognition, we introduce DUMA, a
novel conversational agent framework that embodies a dual-mind mechanism
through the utilization of two generative Large Language Models (LLMs)
dedicated to fast and slow thinking respectively. The fast thinking model
serves as the primary interface for external interactions and initial response
generation, evaluating the necessity for engaging the slow thinking model based
on the complexity of the complete response. When invoked, the slow thinking
model takes over the conversation, engaging in meticulous planning, reasoning,
and tool utilization to provide a well-analyzed response. This dual-mind
configuration allows for a seamless transition between intuitive responses and
deliberate problem-solving processes based on the situation. We have
constructed a conversational agent to handle online inquiries in the real
estate industry. The experiment proves that our method balances effectiveness
and efficiency, and has a significant improvement compared to the baseline.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.18301">Interactive Joint Planning for Autonomous Vehicles. (arXiv:2310.18301v4 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuxiao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Veer_S/0/1/0/all/0/1">Sushant Veer</a>, <a href="http://arxiv.org/find/cs/1/au:+Karkus_P/0/1/0/all/0/1">Peter Karkus</a>, <a href="http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1">Marco Pavone</a></p>
<p>In highly interactive driving scenarios, the actions of one agent greatly
influences those of its neighbors. Planning safe motions for autonomous
vehicles in such interactive environments, therefore, requires reasoning about
the impact of the ego's intended motion plan on nearby agents' behavior.
Deep-learning-based models have recently achieved great success in trajectory
prediction and many models in the literature allow for ego-conditioned
prediction. However, leveraging ego-conditioned prediction remains challenging
in downstream planning due to the complex nature of neural networks, limiting
the planner structure to simple ones, e.g., sampling-based planner. Despite
their ability to generate fine-grained high-quality motion plans, it is
difficult for gradient-based planning algorithms, such as model predictive
control (MPC), to leverage ego-conditioned prediction due to their iterative
nature and need for gradient. We present Interactive Joint Planning (IJP) that
bridges MPC with learned prediction models in a computationally scalable manner
to provide us the best of both the worlds. In particular, IJP jointly optimizes
over the behavior of the ego and the surrounding agents and leverages
deep-learned prediction models as prediction priors that the join trajectory
optimization tries to stay close to. Furthermore, by leveraging homotopy
classes, our joint optimizer searches over diverse motion plans to avoid
getting stuck at local minima. Closed-loop simulation result shows that IJP
significantly outperforms the baselines that are either without joint
optimization or running sampling-based planning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.18333">She had Cobalt Blue Eyes: Prompt Testing to Create Aligned and Sustainable Language Models. (arXiv:2310.18333v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chatrath_V/0/1/0/all/0/1">Veronica Chatrath</a>, <a href="http://arxiv.org/find/cs/1/au:+Bamgbose_O/0/1/0/all/0/1">Oluwanifemi Bamgbose</a>, <a href="http://arxiv.org/find/cs/1/au:+Raza_S/0/1/0/all/0/1">Shaina Raza</a></p>
<p>As the use of large language models (LLMs) increases within society, as does
the risk of their misuse. Appropriate safeguards must be in place to ensure LLM
outputs uphold the ethical standards of society, highlighting the positive role
that artificial intelligence technologies can have. Recent events indicate
ethical concerns around conventionally trained LLMs, leading to overall unsafe
user experiences. This motivates our research question: how do we ensure LLM
alignment? In this work, we introduce a test suite of unique prompts to foster
the development of aligned LLMs that are fair, safe, and robust. We show that
prompting LLMs at every step of the development pipeline, including data
curation, pre-training, and fine-tuning, will result in an overall more
responsible model. Our test suite evaluates outputs from four state-of-the-art
language models: GPT-3.5, GPT-4, OPT, and LLaMA-2. The assessment presented in
this paper highlights a gap between societal alignment and the capabilities of
current LLMs. Additionally, implementing a test suite such as ours lowers the
environmental overhead of making models safe and fair.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19204">Can ChatGPT advance software testing intelligence? An experience report on metamorphic testing. (arXiv:2310.19204v2 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luu_Q/0/1/0/all/0/1">Quang-Hung Luu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Huai Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tsong Yueh Chen</a></p>
<p>While ChatGPT is a well-known artificial intelligence chatbot being used to
answer human's questions, one may want to discover its potential in advancing
software testing. We examine the capability of ChatGPT in advancing the
intelligence of software testing through a case study on metamorphic testing
(MT), a state-of-the-art software testing technique. We ask ChatGPT to generate
candidates of metamorphic relations (MRs), which are basically necessary
properties of the object program and which traditionally require human
intelligence to identify. These MR candidates are then evaluated in terms of
correctness by domain experts. We show that ChatGPT can be used to generate new
correct MRs to test several software systems. Having said that, the majority of
MR candidates are either defined vaguely or incorrect, especially for systems
that have never been tested with MT. ChatGPT can be used to advance software
testing intelligence by proposing MR candidates that can be later adopted for
implementing tests; but human intelligence should still inevitably be involved
to justify and rectify their correctness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00860">Zero Coordinate Shift: Whetted Automatic Differentiation for Physics-informed Operator Learning. (arXiv:2311.00860v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Leng_K/0/1/0/all/0/1">Kuangdai Leng</a>, <a href="http://arxiv.org/find/cs/1/au:+Shankar_M/0/1/0/all/0/1">Mallikarjun Shankar</a>, <a href="http://arxiv.org/find/cs/1/au:+Thiyagalingam_J/0/1/0/all/0/1">Jeyan Thiyagalingam</a></p>
<p>Automatic differentiation (AD) is a critical step in physics-informed machine
learning, required for computing the high-order derivatives of network output
w.r.t. coordinates of collocation points. In this paper, we present a novel and
lightweight algorithm to conduct AD for physics-informed operator learning,
which we call the trick of Zero Coordinate Shift (ZCS). Instead of making all
sampled coordinates as leaf variables, ZCS introduces only one scalar-valued
leaf variable for each spatial or temporal dimension, simplifying the wanted
derivatives from "many-roots-many-leaves" to "one-root-many-leaves" whereby
reverse-mode AD becomes directly utilisable. It has led to an outstanding
performance leap by avoiding the duplication of the computational graph along
the dimension of functions (physical parameters). ZCS is easy to implement with
current deep learning libraries; our own implementation is achieved by
extending the DeepXDE package. We carry out a comprehensive benchmark analysis
and several case studies, training physics-informed DeepONets to solve partial
differential equations (PDEs) without data. The results show that ZCS has
persistently reduced GPU memory consumption and wall time for training by an
order of magnitude, and such reduction factor scales with the number of
functions. As a low-level optimisation technique, ZCS imposes no restrictions
on data, physics (PDE) or network architecture and does not compromise training
results from any aspect.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01017">Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion. (arXiv:2311.01017v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lunjun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1">Yuwen Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Ze Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Casas_S/0/1/0/all/0/1">Sergio Casas</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1">Rui Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1">Raquel Urtasun</a></p>
<p>Learning world models can teach an agent how the world works in an
unsupervised manner. Even though it can be viewed as a special case of sequence
modeling, progress for scaling world models on robotic applications such as
autonomous driving has been somewhat less rapid than scaling language models
with Generative Pre-trained Transformers (GPT). We identify two reasons as
major bottlenecks: dealing with complex and unstructured observation space, and
having a scalable generative model. Consequently, we propose a novel world
modeling approach that first tokenizes sensor observations with VQVAE, then
predicts the future via discrete diffusion. To efficiently decode and denoise
tokens in parallel, we recast Masked Generative Image Transformer into the
discrete diffusion framework with a few simple changes, resulting in notable
improvement. When applied to learning world models on point cloud observations,
our model reduces prior SOTA Chamfer distance by more than 65% for 1s
prediction, and more than 50% for 3s prediction, across NuScenes, KITTI
Odometry, and Argoverse2 datasets. Our results demonstrate that discrete
diffusion on tokenized agent experience can unlock the power of GPT-like
unsupervised learning for robotic agents.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02082">Semantic Modelling of Organizational Knowledge as a Basis for Enterprise Data Governance 4.0 -- Application to a Unified Clinical Data Model. (arXiv:2311.02082v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oliveira_M/0/1/0/all/0/1">Miguel AP Oliveira</a>, <a href="http://arxiv.org/find/cs/1/au:+Manara_S/0/1/0/all/0/1">Stephane Manara</a>, <a href="http://arxiv.org/find/cs/1/au:+Mole_B/0/1/0/all/0/1">Bruno Mol&#xe9;</a>, <a href="http://arxiv.org/find/cs/1/au:+Muller_T/0/1/0/all/0/1">Thomas Muller</a>, <a href="http://arxiv.org/find/cs/1/au:+Guillouche_A/0/1/0/all/0/1">Aur&#xe9;lien Guillouche</a>, <a href="http://arxiv.org/find/cs/1/au:+Hesske_L/0/1/0/all/0/1">Lysann Hesske</a>, <a href="http://arxiv.org/find/cs/1/au:+Jordan_B/0/1/0/all/0/1">Bruce Jordan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hubert_G/0/1/0/all/0/1">Gilles Hubert</a>, <a href="http://arxiv.org/find/cs/1/au:+Kulkarni_C/0/1/0/all/0/1">Chinmay Kulkarni</a>, <a href="http://arxiv.org/find/cs/1/au:+Jagdev_P/0/1/0/all/0/1">Pralipta Jagdev</a>, <a href="http://arxiv.org/find/cs/1/au:+Berger_C/0/1/0/all/0/1">Cedric R. Berger</a></p>
<p>Individuals and organizations cope with an always-growing amount of data,
which is heterogeneous in its contents and formats. An adequate data management
process yielding data quality and control over its lifecycle is a prerequisite
to getting value out of this data and minimizing inherent risks related to
multiple usages. Common data governance frameworks rely on people, policies,
and processes that fall short of the overwhelming complexity of data. Yet,
harnessing this complexity is necessary to achieve high-quality standards. The
latter will condition any downstream data usage outcome, including generative
artificial intelligence trained on this data. In this paper, we report our
concrete experience establishing a simple, cost-efficient framework that
enables metadata-driven, agile and (semi-)automated data governance (i.e. Data
Governance 4.0). We explain how we implement and use this framework to
integrate 25 years of clinical study data at an enterprise scale in a fully
productive environment. The framework encompasses both methodologies and
technologies leveraging semantic web principles. We built a knowledge graph
describing avatars of data assets in their business context, including
governance principles. Multiple ontologies articulated by an enterprise upper
ontology enable key governance actions such as FAIRification, lifecycle
management, definition of roles and responsibilities, lineage across
transformations and provenance from source systems. This metadata model is the
keystone to data governance 4.0: a semi-automatised data management process
that considers the business context in an agile manner to adapt governance
constraints to each use case and dynamically tune it based on business changes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02198">Imitation Bootstrapped Reinforcement Learning. (arXiv:2311.02198v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1">Hengyuan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mirchandani_S/0/1/0/all/0/1">Suvir Mirchandani</a>, <a href="http://arxiv.org/find/cs/1/au:+Sadigh_D/0/1/0/all/0/1">Dorsa Sadigh</a></p>
<p>Despite the considerable potential of reinforcement learning (RL), robotics
control tasks predominantly rely on imitation learning (IL) owing to its better
sample efficiency. However, given the high cost of collecting extensive
demonstrations, RL is still appealing if it can utilize limited imitation data
for efficient autonomous self-improvement. Existing RL methods that utilize
demonstrations either initialize the replay buffer with demonstrations and
oversample them during RL training, which does not benefit from the
generalization potential of modern IL methods, or pretrain the RL policy with
IL on the demonstrations, which requires additional mechanisms to prevent
catastrophic forgetting during RL fine-tuning. We propose imitation
bootstrapped reinforcement learning (IBRL), a novel framework that first trains
an IL policy on a limited number of demonstrations and then uses it to propose
alternative actions for both online exploration and target value bootstrapping.
IBRL achieves SoTA performance and sample efficiency on 7 challenging sparse
reward continuous control tasks in simulation while learning directly from
pixels. As a highlight of our method, IBRL achieves $6.4\times$ higher success
rate than RLPD, a strong method that combines the idea of oversampling
demonstrations with modern RL improvements, under the budget of 10 demos and
100K interactions in the challenging PickPlaceCan task in the Robomimic
benchmark.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02651">Compute at Scale: A Broad Investigation into the Data Center Industry. (arXiv:2311.02651v4 [cs.CY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pilz_K/0/1/0/all/0/1">Konstantin Pilz</a>, <a href="http://arxiv.org/find/cs/1/au:+Heim_L/0/1/0/all/0/1">Lennart Heim</a></p>
<p>This report characterizes the data center industry and its importance for AI
development. Data centers are industrial facilities that efficiently provide
compute at scale and thus constitute the engine rooms of today's digital
economy. As large-scale AI training and inference become increasingly
computationally expensive, they are dominantly executed from this designated
infrastructure. Key features of data centers include large-scale compute
clusters that require extensive cooling and consume large amounts of power, the
need for fast connectivity both within the data center and to the internet, and
an emphasis on security and reliability. The global industry is valued at
approximately $250B and is expected to double over the next seven years. There
are likely about 500 large (above 10 MW) data centers globally, with the US,
Europe, and China constituting the most important markets. The report further
covers important actors, business models, main inputs, and typical locations of
data centers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03348">Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation. (arXiv:2311.03348v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1">Rusheb Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Feuillade__Montixi_Q/0/1/0/all/0/1">Quentin Feuillade--Montixi</a>, <a href="http://arxiv.org/find/cs/1/au:+Pour_S/0/1/0/all/0/1">Soroush Pour</a>, <a href="http://arxiv.org/find/cs/1/au:+Tagade_A/0/1/0/all/0/1">Arush Tagade</a>, <a href="http://arxiv.org/find/cs/1/au:+Casper_S/0/1/0/all/0/1">Stephen Casper</a>, <a href="http://arxiv.org/find/cs/1/au:+Rando_J/0/1/0/all/0/1">Javier Rando</a></p>
<p>Despite efforts to align large language models to produce harmless responses,
they are still vulnerable to jailbreak prompts that elicit unrestricted
behaviour. In this work, we investigate persona modulation as a black-box
jailbreaking method to steer a target model to take on personalities that are
willing to comply with harmful instructions. Rather than manually crafting
prompts for each persona, we automate the generation of jailbreaks using a
language model assistant. We demonstrate a range of harmful completions made
possible by persona modulation, including detailed instructions for
synthesising methamphetamine, building a bomb, and laundering money. These
automated attacks achieve a harmful completion rate of 42.5% in GPT-4, which is
185 times larger than before modulation (0.23%). These prompts also transfer to
Claude 2 and Vicuna with harmful completion rates of 61.0% and 35.9%,
respectively. Our work reveals yet another vulnerability in commercial large
language models and highlights the need for more comprehensive safeguards.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03739">Leveraging Large Language Models for Automated Proof Synthesis in Rust. (arXiv:2311.03739v2 [cs.FL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1">Jianan Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Ziqiao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Weiteng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_W/0/1/0/all/0/1">Weidong Cui</a></p>
<p>Formal verification can provably guarantee the correctness of critical system
software, but the high proof burden has long hindered its wide adoption.
Recently, Large Language Models (LLMs) have shown success in code analysis and
synthesis. In this paper, we present a combination of LLMs and static analysis
to synthesize invariants, assertions, and other proof structures for a
Rust-based formal verification framework called Verus. In a few-shot setting,
LLMs demonstrate impressive logical ability in generating postconditions and
loop invariants, especially when analyzing short code snippets. However, LLMs
lack the ability to retain and propagate context information, a strength of
traditional static analysis. Based on these observations, we developed a
prototype based on OpenAI's GPT-4 model. Our prototype decomposes the
verification task into multiple smaller ones, iteratively queries GPT-4, and
combines its output with lightweight static analysis. We evaluated the
prototype with a developer in the automation loop on 20 vector-manipulating
programs. The results demonstrate that it significantly reduces human effort in
writing entry-level proof code.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05784">Are &quot;Hierarchical&quot; Visual Representations Hierarchical?. (arXiv:2311.05784v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shen_E/0/1/0/all/0/1">Ethan Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1">Ali Farhadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kusupati_A/0/1/0/all/0/1">Aditya Kusupati</a></p>
<p>Learned visual representations often capture large amounts of semantic
information for accurate downstream applications. Human understanding of the
world is fundamentally grounded in hierarchy. To mimic this and further improve
representation capabilities, the community has explored "hierarchical" visual
representations that aim at modeling the underlying hierarchy of the visual
world. In this work, we set out to investigate if hierarchical visual
representations truly capture the human perceived hierarchy better than
standard learned representations. To this end, we create HierNet, a suite of 12
datasets spanning 3 kinds of hierarchy from the BREEDs subset of ImageNet.
After extensive evaluation of Hyperbolic and Matryoshka Representations across
training setups, we conclude that they do not capture hierarchy any better than
the standard representations but can assist in other aspects like search
efficiency and interpretability. Our benchmark and the datasets are
open-sourced at https://github.com/ethanlshen/HierNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06258">Post-COVID Highlights: Challenges and Solutions of AI Techniques for Swift Identification of COVID-19. (arXiv:2311.06258v2 [cs.CY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1">Yingying Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_X/0/1/0/all/0/1">Xiaodan Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shiyi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Walsh_S/0/1/0/all/0/1">Simon Walsh</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1">Guang Yang</a></p>
<p>Since the onset of the COVID-19 pandemic in 2019, there has been a concerted
effort to develop cost-effective, non-invasive, and rapid AI-based tools. These
tools were intended to alleviate the burden on healthcare systems, control the
rapid spread of the virus, and enhance intervention outcomes, all in response
to this unprecedented global crisis. As we transition into a post-COVID era, we
retrospectively evaluate these proposed studies and offer a review of the
techniques employed in AI diagnostic models, with a focus on the solutions
proposed for different challenges. This review endeavors to provide insights
into the diverse solutions designed to address the multifaceted challenges that
arose during the pandemic. By doing so, we aim to prepare the AI community for
the development of AI tools tailored to address public health emergencies
effectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06330">Smart Agent-Based Modeling: On the Use of Large Language Models in Computer Simulations. (arXiv:2311.06330v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zengqing Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_R/0/1/0/all/0/1">Run Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1">Xu Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1">Shuyuan Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yixin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Chuan Xiao</a></p>
<p>Computer simulations offer a robust toolset for exploring complex systems
across various disciplines. A particularly impactful approach within this realm
is Agent-Based Modeling (ABM), which harnesses the interactions of individual
agents to emulate intricate system dynamics. ABM's strength lies in its
bottom-up methodology, illuminating emergent phenomena by modeling the
behaviors of individual components of a system. Yet, ABM has its own set of
challenges, notably its struggle with modeling natural language instructions
and common sense in mathematical equations or rules. This paper seeks to
transcend these boundaries by integrating Large Language Models (LLMs) like GPT
into ABM. This amalgamation gives birth to a novel framework, Smart Agent-Based
Modeling (SABM). Building upon the concept of smart agents -- entities
characterized by their intelligence, adaptability, and computation ability --
we explore in the direction of utilizing LLM-powered agents to simulate
real-world scenarios with increased nuance and realism. In this comprehensive
exploration, we elucidate the state of the art of ABM, introduce SABM's
potential and methodology, and present three case studies (source codes
available at https://github.com/Roihn/SABM), demonstrating the SABM methodology
and validating its effectiveness in modeling real-world systems. Furthermore,
we cast a vision towards several aspects of the future of SABM, anticipating a
broader horizon for its applications. Through this endeavor, we aspire to
redefine the boundaries of computer simulations, enabling a more profound
understanding of complex systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06607">Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models. (arXiv:2311.06607v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1">Biao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qiang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1">Zhiyin Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shuo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jingxu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yabo Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuliang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1">Xiang Bai</a></p>
<p>Large Multimodal Models (LMMs) have shown promise in vision-language tasks
but struggle with high-resolution input and detailed scene understanding.
Addressing these challenges, we introduce Monkey to enhance LMM capabilities.
Firstly, Monkey processes input images by dividing them into uniform patches,
each matching the size (e.g., 448x448) used in the original training of the
well-trained vision encoder. Equipped with individual adapter for each patch,
Monkey can handle higher resolutions up to 1344x896 pixels, enabling the
detailed capture of complex visual information. Secondly, it employs a
multi-level description generation method, enriching the context for
scene-object associations. This two-part strategy ensures more effective
learning from generated data: the higher resolution allows for a more detailed
capture of visuals, which in turn enhances the effectiveness of comprehensive
descriptions. Extensive ablative results validate the effectiveness of our
designs. Additionally, experiments on 18 datasets further demonstrate that
Monkey surpasses existing LMMs in many tasks like Image Captioning and various
Visual Question Answering formats. Specially, in qualitative tests focused on
dense text question answering, Monkey has exhibited encouraging results
compared with GPT4V. Code is available at
https://github.com/Yuliang-Liu/Monkey.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06622">TrainerAgent: Customizable and Efficient Model Training through LLM-Powered Multi-Agent System. (arXiv:2311.06622v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Haoyuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1">Hao Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tianke Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Zhelun Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_A/0/1/0/all/0/1">Aoxiong Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1">Hao Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1">Siming Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuhao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1">Wanggui He</a></p>
<p>Training AI models has always been challenging, especially when there is a
need for custom models to provide personalized services. Algorithm engineers
often face a lengthy process to iteratively develop models tailored to specific
business requirements, making it even more difficult for non-experts. The quest
for high-quality and efficient model development, along with the emergence of
Large Language Model (LLM) Agents, has become a key focus in the industry.
Leveraging the powerful analytical, planning, and decision-making capabilities
of LLM, we propose a TrainerAgent system comprising a multi-agent framework
including Task, Data, Model and Server agents. These agents analyze
user-defined tasks, input data, and requirements (e.g., accuracy, speed),
optimizing them comprehensively from both data and model perspectives to obtain
satisfactory models, and finally deploy these models as online service.
Experimental evaluations on classical discriminative and generative tasks in
computer vision and natural language processing domains demonstrate that our
system consistently produces models that meet the desired criteria.
Furthermore, the system exhibits the ability to critically identify and reject
unattainable tasks, such as fantastical scenarios or unethical requests,
ensuring robustness and safety. This research presents a significant
advancement in achieving desired models with increased efficiency and quality
as compared to traditional model development, facilitated by the integration of
LLM-powered analysis, decision-making, and execution capabilities, as well as
the collaboration among four agents. We anticipate that our work will
contribute to the advancement of research on TrainerAgent in both academic and
industry communities, potentially establishing it as a new paradigm for model
development in the field of AI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06996">AGRAMPLIFIER: Defending Federated Learning Against Poisoning Attacks Through Local Update Amplification. (arXiv:2311.06996v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1">Zirui Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1">Liyue Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yanjun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Leo Yu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jingwei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_G/0/1/0/all/0/1">Guangdong Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1">Yong Xiang</a></p>
<p>The collaborative nature of federated learning (FL) poses a major threat in
the form of manipulation of local training data and local updates, known as the
Byzantine poisoning attack. To address this issue, many Byzantine-robust
aggregation rules (AGRs) have been proposed to filter out or moderate
suspicious local updates uploaded by Byzantine participants.
</p>
<p>This paper introduces a novel approach called AGRAMPLIFIER, aiming to
simultaneously improve the robustness, fidelity, and efficiency of the existing
AGRs. The core idea of AGRAMPLIFIER is to amplify the "morality" of local
updates by identifying the most repressive features of each gradient update,
which provides a clearer distinction between malicious and benign updates,
consequently improving the detection effect. To achieve this objective, two
approaches, namely AGRMP and AGRXAI, are proposed. AGRMP organizes local
updates into patches and extracts the largest value from each patch, while
AGRXAI leverages explainable AI methods to extract the gradient of the most
activated features. By equipping AGRAMPLIFIER with the existing
Byzantine-robust mechanisms, we successfully enhance the model's robustness,
maintaining its fidelity and improving overall efficiency.
</p>
<p>AGRAMPLIFIER is universally compatible with the existing Byzantine-robust
mechanisms. The paper demonstrates its effectiveness by integrating it with all
mainstream AGR mechanisms. Extensive evaluations conducted on seven datasets
from diverse domains against seven representative poisoning attacks
consistently show enhancements in robustness, fidelity, and efficiency, with
average gains of 40.08%, 39.18%, and 10.68%, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07553">An Extensive Study on Adversarial Attack against Pre-trained Models of Code. (arXiv:2311.07553v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1">Xiaohu Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_M/0/1/0/all/0/1">Ming Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1">Zichao Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shangwen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1">Hai Jin</a></p>
<p>Transformer-based pre-trained models of code (PTMC) have been widely utilized
and have achieved state-of-the-art performance in many mission-critical
applications. However, they can be vulnerable to adversarial attacks through
identifier substitution or coding style transformation, which can significantly
degrade accuracy and may further incur security concerns. Although several
approaches have been proposed to generate adversarial examples for PTMC, the
effectiveness and efficiency of such approaches, especially on different code
intelligence tasks, has not been well understood. To bridge this gap, this
study systematically analyzes five state-of-the-art adversarial attack
approaches from three perspectives: effectiveness, efficiency, and the quality
of generated examples. The results show that none of the five approaches
balances all these perspectives. Particularly, approaches with a high attack
success rate tend to be time-consuming; the adversarial code they generate
often lack naturalness, and vice versa. To address this limitation, we explore
the impact of perturbing identifiers under different contexts and find that
identifier substitution within for and if statements is the most effective.
Based on these findings, we propose a new approach that prioritizes different
types of statements for various tasks and further utilizes beam search to
generate adversarial examples. Evaluation results show that it outperforms the
state-of-the-art ALERT in terms of both effectiveness and efficiency while
preserving the naturalness of the generated adversarial examples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07585">Input Reconstruction Attack against Vertical Federated Large Language Models. (arXiv:2311.07585v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1">Fei Zheng</a></p>
<p>Recently, large language models (LLMs) have drawn extensive attention from
academia and the public, due to the advent of the ChatGPT. While LLMs show
their astonishing ability in text generation for various tasks, privacy
concerns limit their usage in real-life businesses. More specifically, either
the user's inputs (the user sends the query to the model-hosting server) or the
model (the user downloads the complete model) itself will be revealed during
the usage. Vertical federated learning (VFL) is a promising solution to this
kind of problem. It protects both the user's input and the knowledge of the
model by splitting the model into a bottom part and a top part, which is
maintained by the user and the model provider, respectively. However, in this
paper, we demonstrate that in LLMs, VFL fails to protect the user input since
it is simple and cheap to reconstruct the input from the intermediate
embeddings. Experiments show that even with a commercial GPU, the input
sentence can be reconstructed in only one second. We also discuss several
possible solutions to enhance the privacy of vertical federated LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09433">Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment. (arXiv:2311.09433v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haoran Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shu_K/0/1/0/all/0/1">Kai Shu</a></p>
<p>To ensure AI safety, instruction-tuned Large Language Models (LLMs) are
specifically trained to ensure alignment, which refers to making models behave
in accordance with human intentions. While these models have demonstrated
commendable results on various safety benchmarks, the vulnerability of their
safety alignment has not been extensively studied. This is particularly
troubling given the potential harm that LLMs can inflict. Existing attack
methods on LLMs often rely on poisoned training data or the injection of
malicious prompts. These approaches compromise the stealthiness and
generalizability of the attacks, making them susceptible to detection.
Additionally, these models often demand substantial computational resources for
implementation, making them less practical for real-world applications.
Inspired by recent success in modifying model behavior through steering vectors
without the need for optimization, and drawing on its effectiveness in
red-teaming LLMs, we conducted experiments employing activation steering to
target four key aspects of LLMs: truthfulness, toxicity, bias, and harmfulness
- across a varied set of attack settings. To establish a universal attack
strategy applicable to diverse target alignments without depending on manual
analysis, we automatically select the intervention layer based on contrastive
layer search. Our experiment results show that activation attacks are highly
effective and add little or no overhead to attack efficiency. Additionally, we
discuss potential countermeasures against such activation attacks. Our code and
data are available at https://github.com/wang2226/Backdoor-Activation-Attack
Warning: this paper contains content that can be offensive or upsetting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09740">Redefining Super-Resolution: Fine-mesh PDE predictions without classical simulations. (arXiv:2311.09740v2 [physics.flu-dyn] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Sarkar_R/0/1/0/all/0/1">Rajat Kumar Sarkar</a>, <a href="http://arxiv.org/find/physics/1/au:+Majumdar_R/0/1/0/all/0/1">Ritam Majumdar</a>, <a href="http://arxiv.org/find/physics/1/au:+Jadhav_V/0/1/0/all/0/1">Vishal Jadhav</a>, <a href="http://arxiv.org/find/physics/1/au:+Sakhinana_S/0/1/0/all/0/1">Sagar Srinivas Sakhinana</a>, <a href="http://arxiv.org/find/physics/1/au:+Runkana_V/0/1/0/all/0/1">Venkataramana Runkana</a></p>
<p>In Computational Fluid Dynamics (CFD), coarse mesh simulations offer
computational efficiency but often lack precision. Applying conventional
super-resolution to these simulations poses a significant challenge due to the
fundamental contrast between downsampling high-resolution images and
authentically emulating low-resolution physics. The former method conserves
more of the underlying physics, surpassing the usual constraints of real-world
scenarios. We propose a novel definition of super-resolution tailored for
PDE-based problems. Instead of simply downsampling from a high-resolution
dataset, we use coarse-grid simulated data as our input and predict fine-grid
simulated outcomes. Employing a physics-infused UNet upscaling method, we
demonstrate its efficacy across various 2D-CFD problems such as discontinuity
detection in Burger's equation, Methane combustion, and fouling in Industrial
heat exchangers. Our method enables the generation of fine-mesh solutions
bypassing traditional simulation, ensuring considerable computational saving
and fidelity to the original ground truth outcomes. Through diverse boundary
conditions during training, we further establish the robustness of our method,
paving the way for its broad applications in engineering and scientific CFD
solvers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10049">Inherently Interpretable Time Series Classification via Multiple Instance Learning. (arXiv:2311.10049v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Early_J/0/1/0/all/0/1">Joseph Early</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheung_G/0/1/0/all/0/1">Gavin KC Cheung</a>, <a href="http://arxiv.org/find/cs/1/au:+Cutajar_K/0/1/0/all/0/1">Kurt Cutajar</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1">Hanting Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Kandola_J/0/1/0/all/0/1">Jas Kandola</a>, <a href="http://arxiv.org/find/cs/1/au:+Twomey_N/0/1/0/all/0/1">Niall Twomey</a></p>
<p>Conventional Time Series Classification (TSC) methods are often black boxes
that obscure inherent interpretation of their decision-making processes. In
this work, we leverage Multiple Instance Learning (MIL) to overcome this issue,
and propose a new framework called MILLET: Multiple Instance Learning for
Locally Explainable Time series classification. We apply MILLET to existing
deep learning TSC models and show how they become inherently interpretable
without compromising (and in some cases, even improving) predictive
performance. We evaluate MILLET on 85 UCR TSC datasets and also present a novel
synthetic dataset that is specially designed to facilitate interpretability
evaluation. On these datasets, we show MILLET produces sparse explanations
quickly that are of higher quality than other well-known interpretability
methods. To the best of our knowledge, our work with MILLET, which is available
on GitHub (https://github.com/JAEarly/MILTimeSeriesClassification), is the
first to develop general MIL methods for TSC and apply them to an extensive
variety of domains
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10057">The Song Describer Dataset: a Corpus of Audio Captions for Music-and-Language Evaluation. (arXiv:2311.10057v3 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Manco_I/0/1/0/all/0/1">Ilaria Manco</a>, <a href="http://arxiv.org/find/cs/1/au:+Weck_B/0/1/0/all/0/1">Benno Weck</a>, <a href="http://arxiv.org/find/cs/1/au:+Doh_S/0/1/0/all/0/1">SeungHeon Doh</a>, <a href="http://arxiv.org/find/cs/1/au:+Won_M/0/1/0/all/0/1">Minz Won</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yixiao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bogdanov_D/0/1/0/all/0/1">Dmitry Bogdanov</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yusong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Ke Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tovstogan_P/0/1/0/all/0/1">Philip Tovstogan</a>, <a href="http://arxiv.org/find/cs/1/au:+Benetos_E/0/1/0/all/0/1">Emmanouil Benetos</a>, <a href="http://arxiv.org/find/cs/1/au:+Quinton_E/0/1/0/all/0/1">Elio Quinton</a>, <a href="http://arxiv.org/find/cs/1/au:+Fazekas_G/0/1/0/all/0/1">Gy&#xf6;rgy Fazekas</a>, <a href="http://arxiv.org/find/cs/1/au:+Nam_J/0/1/0/all/0/1">Juhan Nam</a></p>
<p>We introduce the Song Describer dataset (SDD), a new crowdsourced corpus of
high-quality audio-caption pairs, designed for the evaluation of
music-and-language models. The dataset consists of 1.1k human-written natural
language descriptions of 706 music recordings, all publicly accessible and
released under Creative Common licenses. To showcase the use of our dataset, we
benchmark popular models on three key music-and-language tasks (music
captioning, text-to-music generation and music-language retrieval). Our
experiments highlight the importance of cross-dataset evaluation and offer
insights into how researchers can use SDD to gain a broader understanding of
model performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10538">Testing Language Model Agents Safely in the Wild. (arXiv:2311.10538v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Naihin_S/0/1/0/all/0/1">Silen Naihin</a>, <a href="http://arxiv.org/find/cs/1/au:+Atkinson_D/0/1/0/all/0/1">David Atkinson</a>, <a href="http://arxiv.org/find/cs/1/au:+Green_M/0/1/0/all/0/1">Marc Green</a>, <a href="http://arxiv.org/find/cs/1/au:+Hamadi_M/0/1/0/all/0/1">Merwane Hamadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Swift_C/0/1/0/all/0/1">Craig Swift</a>, <a href="http://arxiv.org/find/cs/1/au:+Schonholtz_D/0/1/0/all/0/1">Douglas Schonholtz</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalai_A/0/1/0/all/0/1">Adam Tauman Kalai</a>, <a href="http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1">David Bau</a></p>
<p>A prerequisite for safe autonomy-in-the-wild is safe testing-in-the-wild. Yet
real-world autonomous tests face several unique safety challenges, both due to
the possibility of causing harm during a test, as well as the risk of
encountering new unsafe agent behavior through interactions with real-world and
potentially malicious actors. We propose a framework for conducting safe
autonomous agent tests on the open internet: agent actions are audited by a
context-sensitive monitor that enforces a stringent safety boundary to stop an
unsafe test, with suspect behavior ranked and logged to be examined by humans.
We a design a basic safety monitor (AgentMonitor) that is flexible enough to
monitor existing LLM agents, and, using an adversarial simulated agent, we
measure its ability to identify and stop unsafe situations. Then we apply the
AgentMonitor on a battery of real-world tests of AutoGPT, and we identify
several limitations and challenges that will face the creation of safe
in-the-wild tests as autonomous agents grow more capable.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10751">ProAgent: From Robotic Process Automation to Agentic Process Automation. (arXiv:2311.10751v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1">Yining Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Cong_X/0/1/0/all/0/1">Xin Cong</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1">Shizuo Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1">Jiannan Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1">Yujia Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yaxi Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Heyang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Huadong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yankai Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhiyuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1">Maosong Sun</a></p>
<p>From ancient water wheels to robotic process automation (RPA), automation
technology has evolved throughout history to liberate human beings from arduous
tasks. Yet, RPA struggles with tasks needing human-like intelligence,
especially in elaborate design of workflow construction and dynamic
decision-making in workflow execution. As Large Language Models (LLMs) have
emerged human-like intelligence, this paper introduces Agentic Process
Automation (APA), a groundbreaking automation paradigm using LLM-based agents
for advanced automation by offloading the human labor to agents associated with
construction and execution. We then instantiate ProAgent, an LLM-based agent
designed to craft workflows from human instructions and make intricate
decisions by coordinating specialized agents. Empirical experiments are
conducted to detail its construction and execution procedure of workflow,
showcasing the feasibility of APA, unveiling the possibility of a new paradigm
of automation driven by agents. Our code is public at
https://github.com/OpenBMB/ProAgent.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.11462">LLM aided semi-supervision for Extractive Dialog Summarization. (arXiv:2311.11462v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mishra_N/0/1/0/all/0/1">Nishant Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Sahu_G/0/1/0/all/0/1">Gaurav Sahu</a>, <a href="http://arxiv.org/find/cs/1/au:+Calixto_I/0/1/0/all/0/1">Iacer Calixto</a>, <a href="http://arxiv.org/find/cs/1/au:+Abu_Hanna_A/0/1/0/all/0/1">Ameen Abu-Hanna</a>, <a href="http://arxiv.org/find/cs/1/au:+Laradji_I/0/1/0/all/0/1">Issam H. Laradji</a></p>
<p>Generating high-quality summaries for chat dialogs often requires large
labeled datasets. We propose a method to efficiently use unlabeled data for
extractive summarization of customer-agent dialogs. In our method, we frame
summarization as a question-answering problem and use state-of-the-art large
language models (LLMs) to generate pseudo-labels for a dialog. We then use
these pseudo-labels to fine-tune a chat summarization model, effectively
transferring knowledge from the large LLM into a smaller specialized model. We
demonstrate our method on the \tweetsumm dataset, and show that using 10% of
the original labelled data set we can achieve 65.9/57.0/61.0 ROUGE-1/-2/-L,
whereas the current state-of-the-art trained on the entire training data set
obtains 65.16/55.81/64.37 ROUGE-1/-2/-L. In other words, in the worst case
(i.e., ROUGE-L) we still effectively retain 94.7% of the performance while
using only 10% of the data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.11995">BrainWash: A Poisoning Attack to Forget in Continual Learning. (arXiv:2311.11995v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abbasi_A/0/1/0/all/0/1">Ali Abbasi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nooralinejad_P/0/1/0/all/0/1">Parsa Nooralinejad</a>, <a href="http://arxiv.org/find/cs/1/au:+Pirsiavash_H/0/1/0/all/0/1">Hamed Pirsiavash</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolouri_S/0/1/0/all/0/1">Soheil Kolouri</a></p>
<p>Continual learning has gained substantial attention within the deep learning
community, offering promising solutions to the challenging problem of
sequential learning. Yet, a largely unexplored facet of this paradigm is its
susceptibility to adversarial attacks, especially with the aim of inducing
forgetting. In this paper, we introduce "BrainWash," a novel data poisoning
method tailored to impose forgetting on a continual learner. By adding the
BrainWash noise to a variety of baselines, we demonstrate how a trained
continual learner can be induced to forget its previously learned tasks
catastrophically, even when using these continual learning baselines. An
important feature of our approach is that the attacker requires no access to
previous tasks' data and is armed merely with the model's current parameters
and the data belonging to the most recent task. Our extensive experiments
highlight the efficacy of BrainWash, showcasing degradation in performance
across various regularization-based continual learning methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12379">Infinite forecast combinations based on Dirichlet process. (arXiv:2311.12379v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1">Yinuo Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1">Feng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1">Yanfei Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jue Wang</a></p>
<p>Forecast combination integrates information from various sources by
consolidating multiple forecast results from the target time series. Instead of
the need to select a single optimal forecasting model, this paper introduces a
deep learning ensemble forecasting model based on the Dirichlet process.
Initially, the learning rate is sampled with three basis distributions as
hyperparameters to convert the infinite mixture into a finite one. All
checkpoints are collected to establish a deep learning sub-model pool, and
weight adjustment and diversity strategies are developed during the combination
process. The main advantage of this method is its ability to generate the
required base learners through a single training process, utilizing the
decaying strategy to tackle the challenge posed by the stochastic nature of
gradient descent in determining the optimal learning rate. To ensure the
method's generalizability and competitiveness, this paper conducts an empirical
analysis using the weekly dataset from the M4 competition and explores
sensitivity to the number of models to be combined. The results demonstrate
that the ensemble model proposed offers substantial improvements in prediction
accuracy and stability compared to a single benchmark model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12651">Mobile-Seed: Joint Semantic Segmentation and Boundary Detection for Mobile Robots. (arXiv:2311.12651v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1">Youqi Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1">Shuhao Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jianping Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1">Zhen Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1">Bisheng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xieyuanli Chen</a></p>
<p>Precise and rapid delineation of sharp boundaries and robust semantics is
essential for numerous downstream robotic tasks, such as robot grasping and
manipulation, real-time semantic mapping, and online sensor calibration
performed on edge computing units. Although boundary detection and semantic
segmentation are complementary tasks, most studies focus on lightweight models
for semantic segmentation but overlook the critical role of boundary detection.
In this work, we introduce Mobile-Seed, a lightweight, dual-task framework
tailored for simultaneous semantic segmentation and boundary detection. Our
framework features a two-stream encoder, an active fusion decoder (AFD) and a
dual-task regularization approach. The encoder is divided into two pathways:
one captures category-aware semantic information, while the other discerns
boundaries from multi-scale features. The AFD module dynamically adapts the
fusion of semantic and boundary information by learning channel-wise
relationships, allowing for precise weight assignment of each channel.
Furthermore, we introduce a regularization loss to mitigate the conflicts in
dual-task learning and deep diversity supervision. Compared to existing
methods, the proposed Mobile-Seed offers a lightweight framework to
simultaneously improve semantic segmentation performance and accurately locate
object boundaries. Experiments on the Cityscapes dataset have shown that
Mobile-Seed achieves notable improvement over the state-of-the-art (SOTA)
baseline by 2.2 percentage points (pp) in mIoU and 4.2 pp in mF-score, while
maintaining an online inference speed of 23.9 frames-per-second (FPS) with
1024x2048 resolution input on an RTX 2080 Ti GPU. Additional experiments on
CamVid and PASCAL Context datasets confirm our method's generalizability. Code
and additional results are publicly available at
https://whu-usi3dv.github.io/Mobile-Seed/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12716">minimax: Efficient Baselines for Autocurricula in JAX. (arXiv:2311.12716v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1">Minqi Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dennis_M/0/1/0/all/0/1">Michael Dennis</a>, <a href="http://arxiv.org/find/cs/1/au:+Grefenstette_E/0/1/0/all/0/1">Edward Grefenstette</a>, <a href="http://arxiv.org/find/cs/1/au:+Rocktaschel_T/0/1/0/all/0/1">Tim Rockt&#xe4;schel</a></p>
<p>Unsupervised environment design (UED) is a form of automatic curriculum
learning for training robust decision-making agents to zero-shot transfer into
unseen environments. Such autocurricula have received much interest from the RL
community. However, UED experiments, based on CPU rollouts and GPU model
updates, have often required several weeks of training. This compute
requirement is a major obstacle to rapid innovation for the field. This work
introduces the minimax library for UED training on accelerated hardware. Using
JAX to implement fully-tensorized environments and autocurriculum algorithms,
minimax allows the entire training loop to be compiled for hardware
acceleration. To provide a petri dish for rapid experimentation, minimax
includes a tensorized grid-world based on MiniGrid, in addition to reusable
abstractions for conducting autocurricula in procedurally-generated
environments. With these components, minimax provides strong UED baselines,
including new parallelized variants, which achieve over 120$\times$ speedups in
wall time compared to previous implementations when training with equal batch
sizes. The minimax library is available under the Apache 2.0 license at
https://github.com/facebookresearch/minimax.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12856">Density of States Prediction of Crystalline Materials via Prompt-guided Multi-Modal Transformer. (arXiv:2311.12856v2 [cond-mat.mtrl-sci] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Lee_N/0/1/0/all/0/1">Namkyeong Lee</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Noh_H/0/1/0/all/0/1">Heewoong Noh</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Kim_S/0/1/0/all/0/1">Sungwon Kim</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Hyun_D/0/1/0/all/0/1">Dongmin Hyun</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Na_G/0/1/0/all/0/1">Gyoung S. Na</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Park_C/0/1/0/all/0/1">Chanyoung Park</a></p>
<p>The density of states (DOS) is a spectral property of crystalline materials,
which provides fundamental insights into various characteristics of the
materials. While previous works mainly focus on obtaining high-quality
representations of crystalline materials for DOS prediction, we focus on
predicting the DOS from the obtained representations by reflecting the nature
of DOS: DOS determines the general distribution of states as a function of
energy. That is, DOS is not solely determined by the crystalline material but
also by the energy levels, which has been neglected in previous works. In this
paper, we propose to integrate heterogeneous information obtained from the
crystalline materials and the energies via a multi-modal transformer, thereby
modeling the complex relationships between the atoms in the crystalline
materials and various energy levels for DOS prediction. Moreover, we propose to
utilize prompts to guide the model to learn the crystal structural
system-specific interactions between crystalline materials and energies.
Extensive experiments on two types of DOS, i.e., Phonon DOS and Electron DOS,
with various real-world scenarios demonstrate the superiority of
DOSTransformer. The source code for DOSTransformer is available at
https://github.com/HeewoongNoh/DOSTransformer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13231">Using Human Feedback to Fine-tune Diffusion Models without Any Reward Model. (arXiv:2311.13231v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1">Kai Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_J/0/1/0/all/0/1">Jian Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1">Jiafei Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_C/0/1/0/all/0/1">Chunjiang Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiaxin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qimai Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1">Weihan Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiaolong Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiu Li</a></p>
<p>Using reinforcement learning with human feedback (RLHF) has shown significant
promise in fine-tuning diffusion models. Previous methods start by training a
reward model that aligns with human preferences, then leverage RL techniques to
fine-tune the underlying models. However, crafting an efficient reward model
demands extensive datasets, optimal architecture, and manual hyperparameter
tuning, making the process both time and cost-intensive. The direct preference
optimization (DPO) method, effective in fine-tuning large language models,
eliminates the necessity for a reward model. However, the extensive GPU memory
requirement of the diffusion model's denoising process hinders the direct
application of the DPO method. To address this issue, we introduce the Direct
Preference for Denoising Diffusion Policy Optimization (D3PO) method to
directly fine-tune diffusion models. The theoretical analysis demonstrates that
although D3PO omits training a reward model, it effectively functions as the
optimal reward model trained using human feedback data to guide the learning
process. This approach requires no training of a reward model, proving to be
more direct, cost-effective, and minimizing computational overhead. In
experiments, our method uses the relative scale of objectives as a proxy for
human preference, delivering comparable results to methods using ground-truth
rewards. Moreover, D3PO demonstrates the ability to reduce image distortion
rates and generate safer images, overcoming challenges lacking robust reward
models. Our code is publicly available in
https://github.com/yk7333/D3PO/tree/main.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13559">Transfer Learning-based Real-time Handgun Detection. (arXiv:2311.13559v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Elmir_Y/0/1/0/all/0/1">Youssef Elmir</a>, <a href="http://arxiv.org/find/cs/1/au:+Laouar_S/0/1/0/all/0/1">Sid Ahmed Laouar</a>, <a href="http://arxiv.org/find/cs/1/au:+Hamdaoui_L/0/1/0/all/0/1">Larbi Hamdaoui</a></p>
<p>Traditional surveillance systems rely on human attention, limiting their
effectiveness. This study employs convolutional neural networks and transfer
learning to develop a real-time computer vision system for automatic handgun
detection. Comprehensive analysis of online handgun detection methods is
conducted, emphasizing reducing false positives and learning time. Transfer
learning is demonstrated as an effective approach. Despite technical
challenges, the proposed system achieves a precision rate of 84.74%,
demonstrating promising performance comparable to related works, enabling
faster learning and accurate automatic handgun detection for enhanced security.
This research advances security measures by reducing human monitoring
dependence, showcasing the potential of transfer learning-based approaches for
efficient and reliable handgun detection.
</p>
</p>
</div>

    </div>
    </body>
    