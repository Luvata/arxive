<!DOCTYPE html>
<html>
<head>
<title>2023-11-15-cs-cl</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2311.06303">MatNexus: A Comprehensive Text Mining and Analysis Suite for Materials Discover. (arXiv:2311.06303v1 [cond-mat.mtrl-sci])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Zhang_L/0/1/0/all/0/1">Lei Zhang</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Stricker_M/0/1/0/all/0/1">Markus Stricker</a></p>
<p>MatNexus is a specialized software for the automated collection, processing,
and analysis of text from scientific articles. Through an integrated suite of
modules, the MatNexus facilitates the retrieval of scientific articles,
processes textual data for insights, generates vector representations suitable
for machine learning, and offers visualization capabilities for word
embeddings. With the vast volume of scientific publications, MatNexus stands
out as an end-to-end tool for researchers aiming to gain insights from
scientific literature in material science, making the exploration of materials,
such as the electrocatalyst examples we show here, efficient and insightful.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06318">Knowledge-Augmented Large Language Models for Personalized Contextual Query Suggestion. (arXiv:2311.06318v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Baek_J/0/1/0/all/0/1">Jinheon Baek</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandrasekaran_N/0/1/0/all/0/1">Nirupama Chandrasekaran</a>, <a href="http://arxiv.org/find/cs/1/au:+Cucerzan_S/0/1/0/all/0/1">Silviu Cucerzan</a>, <a href="http://arxiv.org/find/cs/1/au:+herring_A/0/1/0/all/0/1">Allen herring</a>, <a href="http://arxiv.org/find/cs/1/au:+Jauhar_S/0/1/0/all/0/1">Sujay Kumar Jauhar</a></p>
<p>Large Language Models (LLMs) excel at tackling various natural language
tasks. However, due to the significant costs involved in re-training or
fine-tuning them, they remain largely static and difficult to personalize.
Nevertheless, a variety of applications could benefit from generations that are
tailored to users' preferences, goals, and knowledge. Among them is web search,
where knowing what a user is trying to accomplish, what they care about, and
what they know can lead to improved search experiences. In this work, we
propose a novel and general approach that augments an LLM with relevant context
from users' interaction histories with a search engine in order to personalize
its outputs. Specifically, we construct an entity-centric knowledge store for
each user based on their search and browsing activities on the web, which is
then leveraged to provide contextually relevant LLM prompt augmentations. This
knowledge store is light-weight, since it only produces user-specific aggregate
projections of interests and knowledge onto public knowledge graphs, and
leverages existing search log infrastructure, thereby mitigating the privacy,
compliance, and scalability concerns associated with building deep user
profiles for personalization. We then validate our approach on the task of
contextual query suggestion, which requires understanding not only the user's
current search context but also what they historically know and care about.
Through a number of experiments based on human evaluation, we show that our
approach is significantly better than several other LLM-powered baselines,
generating query suggestions that are contextually more relevant, personalized,
and useful.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06329">A Survey of AI Text-to-Image and AI Text-to-Video Generators. (arXiv:2311.06329v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1">Aditi Singh</a></p>
<p>Text-to-Image and Text-to-Video AI generation models are revolutionary
technologies that use deep learning and natural language processing (NLP)
techniques to create images and videos from textual descriptions. This paper
investigates cutting-edge approaches in the discipline of Text-to-Image and
Text-to-Video AI generations. The survey provides an overview of the existing
literature as well as an analysis of the approaches used in various studies. It
covers data preprocessing techniques, neural network types, and evaluation
metrics used in the field. In addition, the paper discusses the challenges and
limitations of Text-to-Image and Text-to-Video AI generations, as well as
future research directions. Overall, these models have promising potential for
a wide range of applications such as video production, content creation, and
digital marketing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06345">Schema Graph-Guided Prompt for Multi-Domain Dialogue State Tracking. (arXiv:2311.06345v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Su_R/0/1/0/all/0/1">Ruolin Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1">Ting-Wei Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Juang_B/0/1/0/all/0/1">Biing-Hwang Juang</a></p>
<p>Tracking dialogue states is an essential topic in task-oriented dialogue
systems, which involve filling in the necessary information in pre-defined
slots corresponding to a schema. While general pre-trained language models have
been shown effective in slot-filling, their performance is limited when applied
to specific domains. We propose a graph-based framework that learns
domain-specific prompts by incorporating the dialogue schema. Specifically, we
embed domain-specific schema encoded by a graph neural network into the
pre-trained language model, which allows for relations in the schema to guide
the model for better adaptation to the specific domain. Our experiments
demonstrate that the proposed graph-based method outperforms other multi-domain
DST approaches while using similar or fewer trainable parameters. We also
conduct a comprehensive study of schema graph architectures, parameter usage,
and module ablation that demonstrate the effectiveness of our model on
multi-domain dialogue state tracking.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06362">Word Definitions from Large Language Models. (arXiv:2311.06362v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1">Yunting Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Skiena_S/0/1/0/all/0/1">Steven Skiena</a></p>
<p>Dictionary definitions are historically the arbitrator of what words mean,
but this primacy has come under threat by recent progress in NLP, including
word embeddings and generative models like ChatGPT. We present an exploratory
study of the degree of alignment between word definitions from classical
dictionaries and these newer computational artifacts. Specifically, we compare
definitions from three published dictionaries to those generated from variants
of ChatGPT. We show that (i) definitions from different traditional
dictionaries exhibit more surface form similarity than do model-generated
definitions, (ii) that the ChatGPT definitions are highly accurate, comparable
to traditional dictionaries, and (iii) ChatGPT-based embedding definitions
retain their accuracy even on low frequency words, much better than GloVE and
FastText word embeddings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06364">Relation Extraction in underexplored biomedical domains: A diversity-optimised sampling and synthetic data generation approach. (arXiv:2311.06364v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Delmas_M/0/1/0/all/0/1">Maxime Delmas</a>, <a href="http://arxiv.org/find/cs/1/au:+Wysocka_M/0/1/0/all/0/1">Magdalena Wysocka</a>, <a href="http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1">Andr&#xe9; Freitas</a></p>
<p>The sparsity of labelled data is an obstacle to the development of Relation
Extraction models and the completion of databases in various biomedical areas.
While being of high interest in drug-discovery, the natural-products
literature, reporting the identification of potential bioactive compounds from
organisms, is a concrete example of such an overlooked topic. To mark the start
of this new task, we created the first curated evaluation dataset and extracted
literature items from the LOTUS database to build training sets. To this end,
we developed a new sampler inspired by diversity metrics in ecology, named
Greedy Maximum Entropy sampler, or GME-sampler
(https://github.com/idiap/gme-sampler). The strategic optimization of both
balance and diversity of the selected items in the evaluation set is important
given the resource-intensive nature of manual curation. After quantifying the
noise in the training set, in the form of discrepancies between the input
abstracts text and the expected output labels, we explored different strategies
accordingly. Framing the task as an end-to-end Relation Extraction, we
evaluated the performance of standard fine-tuning as a generative task and
few-shot learning with open Large Language Models (LLaMA 7B-65B). In addition
to their evaluation in few-shot settings, we explore the potential of open
Large Language Models (Vicuna-13B) as synthetic data generator and propose a
new workflow for this purpose. All evaluated models exhibited substantial
improvements when fine-tuned on synthetic abstracts rather than the original
noisy data. We provide our best performing (f1-score=59.0) BioGPT-Large model
for end-to-end RE of natural-products relationships along with all the
generated synthetic data and the evaluation dataset. See more details at
https://github.com/idiap/abroad-re.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06377">Heaps&#x27; Law in GPT-Neo Large Language Model Emulated Corpora. (arXiv:2311.06377v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lai_U/0/1/0/all/0/1">Uyen Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+Randhawa_G/0/1/0/all/0/1">Gurjit S. Randhawa</a>, <a href="http://arxiv.org/find/cs/1/au:+Sheridan_P/0/1/0/all/0/1">Paul Sheridan</a></p>
<p>Heaps' law is an empirical relation in text analysis that predicts vocabulary
growth as a function of corpus size. While this law has been validated in
diverse human-authored text corpora, its applicability to large language model
generated text remains unexplored. This study addresses this gap, focusing on
the emulation of corpora using the suite of GPT-Neo large language models. To
conduct our investigation, we emulated corpora of PubMed abstracts using three
different parameter sizes of the GPT-Neo model. Our emulation strategy involved
using the initial five words of each PubMed abstract as a prompt and
instructing the model to expand the content up to the original abstract's
length. Our findings indicate that the generated corpora adhere to Heaps' law.
Interestingly, as the GPT-Neo model size grows, its generated vocabulary
increasingly adheres to Heaps' law as as observed in human-authored text. To
further improve the richness and authenticity of GPT-Neo outputs, future
iterations could emphasize enhancing model size or refining the model
architecture to curtail vocabulary repetition.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06379">DeMuX: Data-efficient Multilingual Learning. (arXiv:2311.06379v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khanuja_S/0/1/0/all/0/1">Simran Khanuja</a>, <a href="http://arxiv.org/find/cs/1/au:+Gowriraj_S/0/1/0/all/0/1">Srinivas Gowriraj</a>, <a href="http://arxiv.org/find/cs/1/au:+Dery_L/0/1/0/all/0/1">Lucio Dery</a>, <a href="http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1">Graham Neubig</a></p>
<p>We consider the task of optimally fine-tuning pre-trained multilingual
models, given small amounts of unlabelled target data and an annotation budget.
In this paper, we introduce DEMUX, a framework that prescribes the exact
data-points to label from vast amounts of unlabelled multilingual data, having
unknown degrees of overlap with the target set. Unlike most prior works, our
end-to-end framework is language-agnostic, accounts for model representations,
and supports multilingual target configurations. Our active learning strategies
rely upon distance and uncertainty measures to select task-specific neighbors
that are most informative to label, given a model. DeMuX outperforms strong
baselines in 84% of the test cases, in the zero-shot setting of disjoint source
and target language sets (including multilingual target pools), across three
models and four tasks. Notably, in low-budget settings (5-100 examples), we
observe gains of up to 8-11 F1 points for token-level tasks, and 2-5 F1 for
complex tasks. Our code is released here:
https://github.com/simran-khanuja/demux.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06382">Transfer Learning for Structured Pruning under Limited Task Data. (arXiv:2311.06382v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dery_L/0/1/0/all/0/1">Lucio Dery</a>, <a href="http://arxiv.org/find/cs/1/au:+Grangier_D/0/1/0/all/0/1">David Grangier</a>, <a href="http://arxiv.org/find/cs/1/au:+Hannun_A/0/1/0/all/0/1">Awni Hannun</a></p>
<p>Large, pre-trained models are problematic to use in resource constrained
applications. Fortunately, task-aware structured pruning methods offer a
solution. These approaches reduce model size by dropping structural units like
layers and attention heads in a manner that takes into account the end-task.
However, these pruning algorithms require more task-specific data than is
typically available. We propose a framework which combines structured pruning
with transfer learning to reduce the need for task-specific data. Our empirical
results answer questions such as: How should the two tasks be coupled? What
parameters should be transferred? And, when during training should transfer
learning be introduced? Leveraging these insights, we demonstrate that our
framework results in pruned models with improved generalization over strong
baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06383">Distilling Large Language Models using Skill-Occupation Graph Context for HR-Related Tasks. (arXiv:2311.06383v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pezeshkpour_P/0/1/0/all/0/1">Pouya Pezeshkpour</a>, <a href="http://arxiv.org/find/cs/1/au:+Iso_H/0/1/0/all/0/1">Hayate Iso</a>, <a href="http://arxiv.org/find/cs/1/au:+Lake_T/0/1/0/all/0/1">Thom Lake</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhutani_N/0/1/0/all/0/1">Nikita Bhutani</a>, <a href="http://arxiv.org/find/cs/1/au:+Hruschka_E/0/1/0/all/0/1">Estevam Hruschka</a></p>
<p>Numerous HR applications are centered around resumes and job descriptions.
While they can benefit from advancements in NLP, particularly large language
models, their real-world adoption faces challenges due to absence of
comprehensive benchmarks for various HR tasks, and lack of smaller models with
competitive capabilities. In this paper, we aim to bridge this gap by
introducing the Resume-Job Description Benchmark (RJDB). We meticulously craft
this benchmark to cater to a wide array of HR tasks, including matching and
explaining resumes to job descriptions, extracting skills and experiences from
resumes, and editing resumes. To create this benchmark, we propose to distill
domain-specific knowledge from a large language model (LLM). We rely on a
curated skill-occupation graph to ensure diversity and provide context for LLMs
generation. Our benchmark includes over 50 thousand triples of job
descriptions, matched resumes and unmatched resumes. Using RJDB, we train
multiple smaller student models. Our experiments reveal that the student models
achieve near/better performance than the teacher model (GPT-4), affirming the
effectiveness of the benchmark. Additionally, we explore the utility of RJDB on
out-of-distribution data for skill extraction and resume-job description
matching, in zero-shot and weak supervision manner. We release our datasets and
code to foster further research and industry applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06401">Autoregressive Language Models For Estimating the Entropy of Epic EHR Audit Logs. (arXiv:2311.06401v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Warner_B/0/1/0/all/0/1">Benjamin C. Warner</a>, <a href="http://arxiv.org/find/cs/1/au:+Kannampallil_T/0/1/0/all/0/1">Thomas Kannampallil</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Seunghwan Kim</a></p>
<p>EHR audit logs are a highly granular stream of events that capture clinician
activities, and is a significant area of interest for research in
characterizing clinician workflow on the electronic health record (EHR).
Existing techniques to measure the complexity of workflow through EHR audit
logs (audit logs) involve time- or frequency-based cross-sectional aggregations
that are unable to capture the full complexity of a EHR session. We briefly
evaluate the usage of transformer-based tabular language model (tabular LM) in
measuring the entropy or disorderedness of action sequences within workflow and
release the evaluated models publicly.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06411">Analyzing Modular Approaches for Visual Question Decomposition. (arXiv:2311.06411v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khandelwal_A/0/1/0/all/0/1">Apoorv Khandelwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1">Ellie Pavlick</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1">Chen Sun</a></p>
<p>Modular neural networks without additional training have recently been shown
to surpass end-to-end neural networks on challenging vision-language tasks. The
latest such methods simultaneously introduce LLM-based code generation to build
programs and a number of skill-specific, task-oriented modules to execute them.
In this paper, we focus on ViperGPT and ask where its additional performance
comes from and how much is due to the (state-of-art, end-to-end) BLIP-2 model
it subsumes vs. additional symbolic components. To do so, we conduct a
controlled study (comparing end-to-end, modular, and prompting-based methods
across several VQA benchmarks). We find that ViperGPT's reported gains over
BLIP-2 can be attributed to its selection of task-specific modules, and when we
run ViperGPT using a more task-agnostic selection of modules, these gains go
away. Additionally, ViperGPT retains much of its performance if we make
prominent alterations to its selection of modules: e.g. removing or retaining
only BLIP-2. Finally, we compare ViperGPT against a prompting-based
decomposition strategy and find that, on some benchmarks, modular approaches
significantly benefit by representing subtasks with natural language, instead
of code.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06414">Knowledge Graphs are not Created Equal: Exploring the Properties and Structure of Real KGs. (arXiv:2311.06414v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Teneva_N/0/1/0/all/0/1">Nedelina Teneva</a>, <a href="http://arxiv.org/find/cs/1/au:+Hruschka_E/0/1/0/all/0/1">Estevam Hruschka</a></p>
<p>Despite the recent popularity of knowledge graph (KG) related tasks and
benchmarks such as KG embeddings, link prediction, entity alignment and
evaluation of the reasoning abilities of pretrained language models as KGs, the
structure and properties of real KGs are not well studied. In this paper, we
perform a large scale comparative study of 29 real KG datasets from diverse
domains such as the natural sciences, medicine, and NLP to analyze their
properties and structural patterns. Based on our findings, we make several
recommendations regarding KG-based model development and evaluation. We believe
that the rich structural information contained in KGs can benefit the
development of better KG models across fields and we hope this study will
contribute to breaking the existing data silos between different areas of
research (e.g., ML, NLP, AI for sciences).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06427">ChatGPT Prompting Cannot Estimate Predictive Uncertainty in High-Resource Languages. (arXiv:2311.06427v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pelucchi_M/0/1/0/all/0/1">Martino Pelucchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Valdenegro_Toro_M/0/1/0/all/0/1">Matias Valdenegro-Toro</a></p>
<p>ChatGPT took the world by storm for its impressive abilities. Due to its
release without documentation, scientists immediately attempted to identify its
limits, mainly through its performance in natural language processing (NLP)
tasks. This paper aims to join the growing literature regarding ChatGPT's
abilities by focusing on its performance in high-resource languages and on its
capacity to predict its answers' accuracy by giving a confidence level. The
analysis of high-resource languages is of interest as studies have shown that
low-resource languages perform worse than English in NLP tasks, but no study so
far has analysed whether high-resource languages perform as well as English.
The analysis of ChatGPT's confidence calibration has not been carried out
before either and is critical to learn about ChatGPT's trustworthiness. In
order to study these two aspects, five high-resource languages and two NLP
tasks were chosen. ChatGPT was asked to perform both tasks in the five
languages and to give a numerical confidence value for each answer. The results
show that all the selected high-resource languages perform similarly and that
ChatGPT does not have a good confidence calibration, often being overconfident
and never giving low confidence values.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06440">Separating the Wheat from the Chaff with BREAD: An open-source benchmark and metrics to detect redundancy in text. (arXiv:2311.06440v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Caswell_I/0/1/0/all/0/1">Isaac Caswell</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lisa Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Papadimitriou_I/0/1/0/all/0/1">Isabel Papadimitriou</a></p>
<p>Data quality is a problem that perpetually resurfaces throughout the field of
NLP, regardless of task, domain, or architecture, and remains especially severe
for lower-resource languages. A typical and insidious issue, affecting both
training data and model output, is data that is repetitive and dominated by
linguistically uninteresting boilerplate, such as price catalogs or
computer-generated log files. Though this problem permeates many web-scraped
corpora, there has yet to be a benchmark to test against, or a systematic study
to find simple metrics that generalize across languages and agree with human
judgements of data quality. In the present work, we create and release BREAD, a
human-labeled benchmark on repetitive boilerplate vs. plausible linguistic
content, spanning 360 languages. We release several baseline CRED (Character
REDundancy) scores along with it, and evaluate their effectiveness on BREAD. We
hope that the community will use this resource to develop better filtering
methods, and that our reference implementations of CRED scores can become
standard corpus evaluation tools, driving the development of cleaner language
modeling corpora, especially in low-resource languages.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06446">THOS: A Benchmark Dataset for Targeted Hate and Offensive Speech. (arXiv:2311.06446v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Almohaimeed_S/0/1/0/all/0/1">Saad Almohaimeed</a>, <a href="http://arxiv.org/find/cs/1/au:+Almohaimeed_S/0/1/0/all/0/1">Saleh Almohaimeed</a>, <a href="http://arxiv.org/find/cs/1/au:+Shafin_A/0/1/0/all/0/1">Ashfaq Ali Shafin</a>, <a href="http://arxiv.org/find/cs/1/au:+Carbunar_B/0/1/0/all/0/1">Bogdan Carbunar</a>, <a href="http://arxiv.org/find/cs/1/au:+Boloni_L/0/1/0/all/0/1">Ladislau B&#xf6;l&#xf6;ni</a></p>
<p>Detecting harmful content on social media, such as Twitter, is made difficult
by the fact that the seemingly simple yes/no classification conceals a
significant amount of complexity. Unfortunately, while several datasets have
been collected for training classifiers in hate and offensive speech, there is
a scarcity of datasets labeled with a finer granularity of target classes and
specific targets. In this paper, we introduce THOS, a dataset of 8.3k tweets
manually labeled with fine-grained annotations about the target of the message.
We demonstrate that this dataset makes it feasible to train classifiers, based
on Large Language Models, to perform classification at this level of
granularity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06453">DocGen: Generating Detailed Parameter Docstrings in Python. (arXiv:2311.06453v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Venkatkrishna_V/0/1/0/all/0/1">Vatsal Venkatkrishna</a>, <a href="http://arxiv.org/find/cs/1/au:+Nagabushanam_D/0/1/0/all/0/1">Durga Shree Nagabushanam</a>, <a href="http://arxiv.org/find/cs/1/au:+Simon_E/0/1/0/all/0/1">Emmanuel Iko-Ojo Simon</a>, <a href="http://arxiv.org/find/cs/1/au:+Fard_F/0/1/0/all/0/1">Fatemeh H. Fard</a>, <a href="http://arxiv.org/find/cs/1/au:+Vidoni_M/0/1/0/all/0/1">Melina Vidoni</a>, <a href="http://arxiv.org/find/cs/1/au:+Codabux_Z/0/1/0/all/0/1">Zadia Codabux</a></p>
<p>Documentation debt hinders the effective utilization of open-source software.
Although code summarization tools have been helpful for developers, most would
prefer a detailed account of each parameter in a function rather than a
high-level summary. However, generating such a summary is too intricate for a
single generative model to produce reliably due to the lack of high-quality
training data. Thus, we propose a multi-step approach that combines multiple
task-specific models, each adept at producing a specific section of a
docstring. The combination of these models ensures the inclusion of each
section in the final docstring. We compared the results from our approach with
existing generative models using both automatic metrics and a human-centred
evaluation with 17 participating developers, which proves the superiority of
our approach over existing methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06467">Adaptive Language-based Mental Health Assessment with Item-Response Theory. (arXiv:2311.06467v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Varadarajan_V/0/1/0/all/0/1">Vasudha Varadarajan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sikstrom_S/0/1/0/all/0/1">Sverker Sikstr&#xf6;m</a>, <a href="http://arxiv.org/find/cs/1/au:+Kjell_O/0/1/0/all/0/1">Oscar N.E. Kjell</a>, <a href="http://arxiv.org/find/cs/1/au:+Schwartz_H/0/1/0/all/0/1">H. Andrew Schwartz</a></p>
<p>Mental health issues widely vary across individuals - the manifestations of
signs and symptoms can be fairly heterogeneous. Recently, language-based
depression and anxiety assessments have shown promise for capturing this
heterogeneous nature by evaluating a patient's own language, but such
approaches require a large sample of words per person to be accurate. In this
work, we introduce adaptive language-based assessment - the task of iteratively
estimating an individual's psychological score based on limited language
responses to questions that the model also decides to ask. To this end, we
explore two statistical learning-based approaches for measurement/scoring:
classical test theory (CTT) and item response theory (IRT). We find that using
adaptive testing in general can significantly reduce the number of questions
required to achieve high validity (r ~ 0.7) with standardized tests, bringing
down from 11 total questions down to 3 for depression and 5 for anxiety. Given
the combinatorial nature of the problem, we empirically evaluate multiple
strategies for both the ordering and scoring objectives, introducing two new
methods: a semi-supervised item response theory based method (ALIRT), and a
supervised actor-critic based model. While both of the models achieve
significant improvements over random and fixed orderings, we find ALIRT to be a
scalable model that achieves the highest accuracy with lower numbers of
questions (e.g. achieves Pearson r ~ 0.93 after only 3 questions versus asking
all 11 questions). Overall, ALIRT allows prompting a reduced number of
questions without compromising accuracy or overhead computational costs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06493">L3 Ensembles: Lifelong Learning Approach for Ensemble of Foundational Language Models. (arXiv:2311.06493v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shiri_A/0/1/0/all/0/1">Aidin Shiri</a>, <a href="http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1">Kaushik Roy</a>, <a href="http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1">Amit Sheth</a>, <a href="http://arxiv.org/find/cs/1/au:+Gaur_M/0/1/0/all/0/1">Manas Gaur</a></p>
<p>Fine-tuning pre-trained foundational language models (FLM) for specific tasks
is often impractical, especially for resource-constrained devices. This
necessitates the development of a Lifelong Learning (L3) framework that
continuously adapts to a stream of Natural Language Processing (NLP) tasks
efficiently. We propose an approach that focuses on extracting meaningful
representations from unseen data, constructing a structured knowledge base, and
improving task performance incrementally. We conducted experiments on various
NLP tasks to validate its effectiveness, including benchmarks like GLUE and
SuperGLUE. We measured good performance across the accuracy, training
efficiency, and knowledge transfer metrics. Initial experimental results show
that the proposed L3 ensemble method increases the model accuracy by 4% ~ 36%
compared to the fine-tuned FLM. Furthermore, L3 model outperforms naive
fine-tuning approaches while maintaining competitive or superior performance
(up to 15.4% increase in accuracy) compared to the state-of-the-art language
model (T5) for the given task, STS benchmark.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06503">Knowledgeable Preference Alignment for LLMs in Domain-specific Question Answering. (arXiv:2311.06503v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yichi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhuo Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1">Yin Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1">Lei Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yanxi Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1">Fangming Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huajun Chen</a></p>
<p>Recently, the development of large language models (LLMs) has attracted wide
attention in academia and industry. Deploying LLMs to real scenarios is one of
the key directions in the current Internet industry. In this paper, we present
a novel pipeline to apply LLMs for domain-specific question answering (QA) that
incorporates domain knowledge graphs (KGs), addressing an important direction
of LLM application. As a real-world application, the content generated by LLMs
should be user-friendly to serve the customers. Additionally, the model needs
to utilize domain knowledge properly to generate reliable answers. These two
issues are the two major difficulties in the LLM application as vanilla
fine-tuning can not adequately address them. We think both requirements can be
unified as the model preference problem that needs to align with humans to
achieve practical application. Thus, we introduce Knowledgeable Preference
AlignmenT (KnowPAT), which constructs two kinds of preference set called style
preference set and knowledge preference set respectively to tackle the two
issues. Besides, we design a new alignment objective to align the LLM
preference with human preference, aiming to train a better LLM for
real-scenario domain-specific QA to generate reliable and user-friendly
answers. Adequate experiments and comprehensive with 15 baseline methods
demonstrate that our KnowPAT is an outperforming pipeline for real-scenario
domain-specific QA with LLMs. Our code is open-source at
https://github.com/zjukg/KnowPAT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06513">Step by Step to Fairness: Attributing Societal Bias in Task-oriented Dialogue Systems. (arXiv:2311.06513v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1">Hsuan Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1">Rebecca Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Sankar_C/0/1/0/all/0/1">Chinnadhurai Sankar</a>, <a href="http://arxiv.org/find/cs/1/au:+Shayandeh_S/0/1/0/all/0/1">Shahin Shayandeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shang-Tse Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hung-yi Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Bikel_D/0/1/0/all/0/1">Daniel M. Bikel</a></p>
<p>Recent works have shown considerable improvements in task-oriented dialogue
(TOD) systems by utilizing pretrained large language models (LLMs) in an
end-to-end manner. However, the biased behavior of each component in a TOD
system and the error propagation issue in the end-to-end framework can lead to
seriously biased TOD responses. Existing works of fairness only focus on the
total bias of a system. In this paper, we propose a diagnosis method to
attribute bias to each component of a TOD system. With the proposed attribution
method, we can gain a deeper understanding of the sources of bias.
Additionally, researchers can mitigate biased model behavior at a more granular
level. We conduct experiments to attribute the TOD system's bias toward three
demographic axes: gender, age, and race. Experimental results show that the
bias of a TOD system usually comes from the response generation model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06518">Minimum Description Length Hopfield Networks. (arXiv:2311.06518v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abudy_M/0/1/0/all/0/1">Matan Abudy</a>, <a href="http://arxiv.org/find/cs/1/au:+Lan_N/0/1/0/all/0/1">Nur Lan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chemla_E/0/1/0/all/0/1">Emmanuel Chemla</a>, <a href="http://arxiv.org/find/cs/1/au:+Katzir_R/0/1/0/all/0/1">Roni Katzir</a></p>
<p>Associative memory architectures are designed for memorization but also
offer, through their retrieval method, a form of generalization to unseen
inputs: stored memories can be seen as prototypes from this point of view.
Focusing on Modern Hopfield Networks (MHN), we show that a large memorization
capacity undermines the generalization opportunity. We offer a solution to
better optimize this tradeoff. It relies on Minimum Description Length (MDL) to
determine during training which memories to store, as well as how many of them.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06530">How ChatGPT is Solving Vulnerability Management Problem. (arXiv:2311.06530v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1">Peiyu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Junming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_L/0/1/0/all/0/1">Lirong Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1">Kangjie Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1">Yifan Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xuhong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wenzhi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Weng_H/0/1/0/all/0/1">Haiqin Weng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1">Shouling Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenhai Wang</a></p>
<p>Recently, ChatGPT has attracted great attention from the code analysis
domain. Prior works show that ChatGPT has the capabilities of processing
foundational code analysis tasks, such as abstract syntax tree generation,
which indicates the potential of using ChatGPT to comprehend code syntax and
static behaviors. However, it is unclear whether ChatGPT can complete more
complicated real-world vulnerability management tasks, such as the prediction
of security relevance and patch correctness, which require an all-encompassing
understanding of various aspects, including code syntax, program semantics, and
related manual comments.
</p>
<p>In this paper, we explore ChatGPT's capabilities on 6 tasks involving the
complete vulnerability management process with a large-scale dataset containing
78,445 samples. For each task, we compare ChatGPT against SOTA approaches,
investigate the impact of different prompts, and explore the difficulties. The
results suggest promising potential in leveraging ChatGPT to assist
vulnerability management. One notable example is ChatGPT's proficiency in tasks
like generating titles for software bug reports. Furthermore, our findings
reveal the difficulties encountered by ChatGPT and shed light on promising
future directions. For instance, directly providing random demonstration
examples in the prompt cannot consistently guarantee good performance in
vulnerability management. By contrast, leveraging ChatGPT in a self-heuristic
way -- extracting expertise from demonstration examples itself and integrating
the extracted expertise in the prompt is a promising research direction.
Besides, ChatGPT may misunderstand and misuse the information in the prompt.
Consequently, effectively guiding ChatGPT to focus on helpful information
rather than the irrelevant content is still an open problem.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06532">Added Toxicity Mitigation at Inference Time for Multimodal and Massively Multilingual Translation. (arXiv:2311.06532v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Costa_jussa_M/0/1/0/all/0/1">Marta R. Costa-juss&#xe0;</a>, <a href="http://arxiv.org/find/cs/1/au:+Dale_D/0/1/0/all/0/1">David Dale</a>, <a href="http://arxiv.org/find/cs/1/au:+Elbayad_M/0/1/0/all/0/1">Maha Elbayad</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1">Bokai Yu</a></p>
<p>Added toxicity in the context of translation refers to the fact of producing
a translation output with more toxicity than there exists in the input. In this
paper, we present MinTox which is a novel pipeline to identify added toxicity
and mitigate this issue which works at inference time. MinTox uses a toxicity
detection classifier which is multimodal (speech and text) and works in
languages at scale. The mitigation method is applied to languages at scale and
directly in text outputs. MinTox is applied to SEAMLESSM4T, which is the latest
multimodal and massively multilingual machine translation system. For this
system, MinTox achieves significant added toxicity mitigation across domains,
modalities and language directions. MinTox manages to approximately filter out
from 25% to 95% of added toxicity (depending on the modality and domain) while
keeping translation quality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06534">Enhancing Public Understanding of Court Opinions with Automated Summarizers. (arXiv:2311.06534v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ash_E/0/1/0/all/0/1">Elliott Ash</a>, <a href="http://arxiv.org/find/cs/1/au:+Kesari_A/0/1/0/all/0/1">Aniket Kesari</a>, <a href="http://arxiv.org/find/cs/1/au:+Naidu_S/0/1/0/all/0/1">Suresh Naidu</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1">Lena Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Stammbach_D/0/1/0/all/0/1">Dominik Stammbach</a></p>
<p>Written judicial opinions are an important tool for building public trust in
court decisions, yet they can be difficult for non-experts to understand. We
present a pipeline for using an AI assistant to generate simplified summaries
of judicial opinions. These are more accessible to the public and more easily
understood by non-experts, We show in a survey experiment that the simplified
summaries help respondents understand the key features of a ruling. We discuss
how to integrate legal domain knowledge into studies using large language
models. Our results suggest a role both for AI assistants to inform the public,
and for lawyers to guide the process of generating accessible summaries.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06549">Zero-Shot Cross-Lingual Sentiment Classification under Distribution Shift: an Exploratory Study. (arXiv:2311.06549v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Raedt_M/0/1/0/all/0/1">Maarten De Raedt</a>, <a href="http://arxiv.org/find/cs/1/au:+Bitew_S/0/1/0/all/0/1">Semere Kiros Bitew</a>, <a href="http://arxiv.org/find/cs/1/au:+Godin_F/0/1/0/all/0/1">Fr&#xe9;deric Godin</a>, <a href="http://arxiv.org/find/cs/1/au:+Demeester_T/0/1/0/all/0/1">Thomas Demeester</a>, <a href="http://arxiv.org/find/cs/1/au:+Develder_C/0/1/0/all/0/1">Chris Develder</a></p>
<p>The brittleness of finetuned language model performance on
out-of-distribution (OOD) test samples in unseen domains has been well-studied
for English, yet is unexplored for multi-lingual models. Therefore, we study
generalization to OOD test data specifically in zero-shot cross-lingual
transfer settings, analyzing performance impacts of both language and domain
shifts between train and test data. We further assess the effectiveness of
counterfactually augmented data (CAD) in improving OOD generalization for the
cross-lingual setting, since CAD has been shown to benefit in a monolingual
English setting. Finally, we propose two new approaches for OOD generalization
that avoid the costly annotation process associated with CAD, by exploiting the
power of recent large language models (LLMs). We experiment with 3 multilingual
models, LaBSE, mBERT, and XLM-R trained on English IMDb movie reviews, and
evaluate on OOD test sets in 13 languages: Amazon product reviews, Tweets, and
Restaurant reviews. Results echo the OOD performance decline observed in the
monolingual English setting. Further, (i) counterfactuals from the original
high-resource language do improve OOD generalization in the low-resource
language, and (ii) our newly proposed cost-effective approaches reach similar
or up to +3.1% better accuracy than CAD for Amazon and Restaurant reviews.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06555">Heuristics-Driven Link-of-Analogy Prompting: Enhancing Large Language Models for Document-Level Event Argument Extraction. (arXiv:2311.06555v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Hanzhang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1">Junlang Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1">Zijian Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1">Hui Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Zixiao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_K/0/1/0/all/0/1">Kezhi Mao</a></p>
<p>In this study, we investigate in-context learning (ICL) in document-level
event argument extraction (EAE). The paper identifies key challenges in this
problem, including example selection, context length limitation, abundance of
event types, and the limitation of Chain-of-Thought (CoT) prompting in
non-reasoning tasks. To address these challenges, we introduce the
Heuristic-Driven Link-of-Analogy (HD-LoA) prompting method. Specifically, we
hypothesize and validate that LLMs learn task-specific heuristics from
demonstrations via ICL. Building upon this hypothesis, we introduce an explicit
heuristic-driven demonstration construction approach, which transforms the
haphazard example selection process into a methodical method that emphasizes
task heuristics. Additionally, inspired by the analogical reasoning of human,
we propose the link-of-analogy prompting, which enables LLMs to process new
situations by drawing analogies to known situations, enhancing their
adaptability. Extensive experiments show that our method outperforms the
existing prompting methods and few-shot supervised learning methods, exhibiting
F1 score improvements of 4.53% and 9.38% on the document-level EAE dataset.
Furthermore, when applied to sentiment analysis and natural language inference
tasks, the HD-LoA prompting achieves accuracy gains of 2.87% and 2.63%,
indicating its effectiveness across different tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06595">From Classification to Generation: Insights into Crosslingual Retrieval Augmented ICL. (arXiv:2311.06595v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaoqian Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_E/0/1/0/all/0/1">Ercong Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1">Sheng Liang</a></p>
<p>The remarkable ability of Large Language Models (LLMs) to understand and
follow instructions has sometimes been limited by their in-context learning
(ICL) performance in low-resource languages. To address this, we introduce a
novel approach that leverages cross-lingual retrieval-augmented in-context
learning (CREA-ICL). By extracting semantically similar prompts from
high-resource languages, we aim to improve the zero-shot performance of
multilingual pre-trained language models (MPLMs) across diverse tasks. Though
our approach yields steady improvements in classification tasks, it faces
challenges in generation tasks. Our evaluation offers insights into the
performance dynamics of retrieval-augmented in-context learning across both
classification and generation domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06602">BizBench: A Quantitative Reasoning Benchmark for Business and Finance. (arXiv:2311.06602v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Koncel_Kedziorski_R/0/1/0/all/0/1">Rik Koncel-Kedziorski</a>, <a href="http://arxiv.org/find/cs/1/au:+Krumdick_M/0/1/0/all/0/1">Michael Krumdick</a>, <a href="http://arxiv.org/find/cs/1/au:+Lai_V/0/1/0/all/0/1">Viet Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+Reddy_V/0/1/0/all/0/1">Varshini Reddy</a>, <a href="http://arxiv.org/find/cs/1/au:+Lovering_C/0/1/0/all/0/1">Charles Lovering</a>, <a href="http://arxiv.org/find/cs/1/au:+Tanner_C/0/1/0/all/0/1">Chris Tanner</a></p>
<p>As large language models (LLMs) impact a growing number of complex domains,
it is becoming increasingly important to have fair, accurate, and rigorous
evaluation benchmarks. Evaluating the reasoning skills required for business
and financial NLP stands out as a particularly difficult challenge. We
introduce BizBench, a new benchmark for evaluating models' ability to reason
about realistic financial problems. BizBench comprises 8 quantitative reasoning
tasks. Notably, BizBench targets the complex task of question-answering (QA)
for structured and unstructured financial data via program synthesis (i.e.,
code generation). We introduce three diverse financially-themed code-generation
tasks from newly collected and augmented QA data. Additionally, we isolate
distinct financial reasoning capabilities required to solve these QA tasks:
reading comprehension of financial text and tables, which is required to
extract correct intermediate values; and understanding domain knowledge (e.g.,
financial formulas) needed to calculate complex solutions. Collectively, these
tasks evaluate a model's financial background knowledge, ability to extract
numeric entities from financial documents, and capacity to solve problems with
code. We conduct an in-depth evaluation of open-source and commercial LLMs,
illustrating that BizBench is a challenging benchmark for quantitative
reasoning in the finance and business domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06607">Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models. (arXiv:2311.06607v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1">Biao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qiang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1">Zhiyin Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shuo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jingxu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yabo Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuliang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1">Xiang Bai</a></p>
<p>Large Multimodal Models have demonstrated impressive capabilities in
understanding general vision-language tasks. However, due to the limitation of
supported input resolution (e.g., 448 x 448) as well as the inexhaustive
description of the training image-text pair, these models often encounter
challenges when dealing with intricate scene understandings and narratives.
Here we address the problem by proposing the Monkey. Our contributions are
two-fold: 1) without pretraining from the start, our method can be built upon
an existing vision encoder (e.g., vit-BigHuge) to effectively improve the input
resolution capacity up to 896 x 1344 pixels; 2) we propose a multi-level
description generation method, which automatically provides rich information
that can guide model to learn contextual association between scenes and
objects. Our extensive testing across more than 16 distinct datasets reveals
that Monkey achieves consistently competitive performance over the existing
LMMs on fundamental tasks, such as Image Captioning, General Visual Question
Answering (VQA), and Document-oriented VQA. Models, interactive demo, and the
source code are provided at the following
https://github.com/Yuliang-Liu/Monkey.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06612">PerceptionGPT: Effectively Fusing Visual Perception into LLM. (arXiv:2311.06612v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pi_R/0/1/0/all/0/1">Renjie Pi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1">Lewei Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jiahui Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jipeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tong Zhang</a></p>
<p>The integration of visual inputs with large language models (LLMs) has led to
remarkable advancements in multi-modal capabilities, giving rise to visual
large language models (VLLMs). However, effectively harnessing VLLMs for
intricate visual perception tasks remains a challenge. In this paper, we
present a novel end-to-end framework named PerceptionGPT, which efficiently and
effectively equips the VLLMs with visual perception abilities by leveraging the
representation power of LLMs' token embedding. Our proposed method treats the
token embedding of the LLM as the carrier of spatial information, then leverage
lightweight visual task encoders and decoders to perform visual perception
tasks (e.g., detection, segmentation). Our approach significantly alleviates
the training difficulty suffered by previous approaches that formulate the
visual outputs as discrete tokens, and enables achieving superior performance
with fewer trainable parameters, less training data and shorted training time.
Moreover, as only one token embedding is required to decode the visual outputs,
the resulting sequence length during inference is significantly reduced.
Consequently, our approach enables accurate and flexible representations,
seamless integration of visual perception tasks, and efficient handling of a
multiple of visual outputs. We validate the effectiveness and efficiency of our
approach through extensive experiments. The results demonstrate significant
improvements over previous methods with much fewer trainable parameters and GPU
hours, which facilitates future research in enabling LLMs with visual
perception abilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06622">TrainerAgent: Customizable and Efficient Model Training through LLM-Powered Multi-Agent System. (arXiv:2311.06622v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Haoyuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1">Hao Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tianke Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Zhelun Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_A/0/1/0/all/0/1">Aoxiong Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1">Hao Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1">Siming Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuhao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1">Wanggui He</a></p>
<p>Training AI models has always been challenging, especially when there is a
need for custom models to provide personalized services. Algorithm engineers
often face a lengthy process to iteratively develop models tailored to specific
business requirements, making it even more difficult for non-experts. The quest
for high-quality and efficient model development, along with the emergence of
Large Language Model (LLM) Agents, has become a key focus in the industry.
Leveraging the powerful analytical, planning, and decision-making capabilities
of LLM, we propose a TrainerAgent system comprising a multi-agent framework
including Task, Data, Model and Server agents. These agents analyze
user-defined tasks, input data, and requirements (e.g., accuracy, speed),
optimizing them comprehensively from both data and model perspectives to obtain
satisfactory models, and finally deploy these models as online service.
Experimental evaluations on classical discriminative and generative tasks in
computer vision and natural language processing domains demonstrate that our
system consistently produces models that meet the desired criteria.
Furthermore, the system exhibits the ability to critically identify and reject
unattainable tasks, such as fantastical scenarios or unethical requests,
ensuring robustness and safety. This research presents a significant
advancement in achieving desired models with increased efficiency and quality
as compared to traditional model development, facilitated by the integration of
LLM-powered analysis, decision-making, and execution capabilities, as well as
the collaboration among four agents. We anticipate that our work will
contribute to the advancement of research on TrainerAgent in both academic and
industry communities, potentially establishing it as a new paradigm for model
development in the field of AI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.03691">HyperMixer: An MLP-based Low Cost Alternative to Transformers. (arXiv:2203.03691v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mai_F/0/1/0/all/0/1">Florian Mai</a>, <a href="http://arxiv.org/find/cs/1/au:+Pannatier_A/0/1/0/all/0/1">Arnaud Pannatier</a>, <a href="http://arxiv.org/find/cs/1/au:+Fehr_F/0/1/0/all/0/1">Fabio Fehr</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Haolin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Marelli_F/0/1/0/all/0/1">Francois Marelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Fleuret_F/0/1/0/all/0/1">Francois Fleuret</a>, <a href="http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1">James Henderson</a></p>
<p>Transformer-based architectures are the model of choice for natural language
understanding, but they come at a significant cost, as they have quadratic
complexity in the input length, require a lot of training data, and can be
difficult to tune. In the pursuit of lower costs, we investigate simple
MLP-based architectures. We find that existing architectures such as MLPMixer,
which achieves token mixing through a static MLP applied to each feature
independently, are too detached from the inductive biases required for natural
language understanding. In this paper, we propose a simple variant, HyperMixer,
which forms the token mixing MLP dynamically using hypernetworks. Empirically,
we demonstrate that our model performs better than alternative MLP-based
models, and on par with Transformers. In contrast to Transformers, HyperMixer
achieves these results at substantially lower costs in terms of processing
time, training data, and hyperparameter tuning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.13308">SciRepEval: A Multi-Format Benchmark for Scientific Document Representations. (arXiv:2211.13308v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1">Amanpreet Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+DArcy_M/0/1/0/all/0/1">Mike D&#x27;Arcy</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1">Arman Cohan</a>, <a href="http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1">Doug Downey</a>, <a href="http://arxiv.org/find/cs/1/au:+Feldman_S/0/1/0/all/0/1">Sergey Feldman</a></p>
<p>Learned representations of scientific documents can serve as valuable input
features for downstream tasks without further fine-tuning. However, existing
benchmarks for evaluating these representations fail to capture the diversity
of relevant tasks. In response, we introduce SciRepEval, the first
comprehensive benchmark for training and evaluating scientific document
representations. It includes 24 challenging and realistic tasks, 8 of which are
new, across four formats: classification, regression, ranking and search. We
then use this benchmark to study and improve the generalization ability of
scientific document representation models. We show how state-of-the-art models
like SPECTER and SciNCL struggle to generalize across the task formats, and
that simple multi-task training fails to improve them. However, a new approach
that learns multiple embeddings per document, each tailored to a different
format, can improve performance. We experiment with task-format-specific
control codes and adapters and find they outperform the existing
single-embedding state-of-the-art by over 2 points absolute. We release the
resulting family of multi-format models, called SPECTER2, for the community to
use and build on.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.07085">Are Language Models Worse than Humans at Following Prompts? It&#x27;s Complicated. (arXiv:2301.07085v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Webson_A/0/1/0/all/0/1">Albert Webson</a>, <a href="http://arxiv.org/find/cs/1/au:+Loo_A/0/1/0/all/0/1">Alyssa Marie Loo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1">Qinan Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1">Ellie Pavlick</a></p>
<p>Prompts have been the center of progress in advancing language models'
zero-shot and few-shot performance. However, recent work finds that models can
perform surprisingly well when given intentionally irrelevant or misleading
prompts. Such results may be interpreted as evidence that model behavior is not
"human like". In this study, we challenge a central assumption in such work:
that humans would perform badly when given pathological instructions. We find
that humans are able to reliably ignore irrelevant instructions and thus, like
models, perform well on the underlying task despite an apparent lack of signal
regarding the task they are being asked to do. However, when given deliberately
misleading instructions, humans follow the instructions faithfully, whereas
models do not. Our findings caution that future research should not idealize
human behaviors as a monolith and should not train or evaluate models to mimic
assumptions about these behaviors without first validating humans' behaviors
empirically.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.02083">Theory of Mind Might Have Spontaneously Emerged in Large Language Models. (arXiv:2302.02083v5 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kosinski_M/0/1/0/all/0/1">Michal Kosinski</a></p>
<p>We explore the intriguing possibility that theory of mind (ToM), or the
uniquely human ability to impute unobservable mental states to others, might
have spontaneously emerged in large language models (LLMs). We designed 40
false-belief tasks, considered a gold standard in testing ToM in humans, and
administered them to several LLMs. Each task included a false-belief scenario,
three closely matched true-belief controls, and the reversed versions of all
four. Smaller and older models solved no tasks; GPT-3-davinci-003 (from
November 2022) and ChatGPT-3.5-turbo (from March 2023) solved 20% of the tasks;
ChatGPT-4 (from June 2023) solved 75% of the tasks, matching the performance of
six-year-old children observed in past studies. These findings suggest the
intriguing possibility that ToM, previously considered exclusive to humans, may
have spontaneously emerged as a byproduct of LLMs' improving language skills.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.08624">InstructABSA: Instruction Learning for Aspect Based Sentiment Analysis. (arXiv:2302.08624v6 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Scaria_K/0/1/0/all/0/1">Kevin Scaria</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1">Himanshu Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Goyal_S/0/1/0/all/0/1">Siddharth Goyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Sawant_S/0/1/0/all/0/1">Saurabh Arjun Sawant</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1">Swaroop Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1">Chitta Baral</a></p>
<p>We introduce InstructABSA, an instruction learning paradigm for Aspect-Based
Sentiment Analysis (ABSA) subtasks. Our method introduces positive, negative,
and neutral examples to each training sample, and instruction tune the model
(Tk-Instruct) for ABSA subtasks, yielding significant performance improvements.
Experimental results on the Sem Eval 2014, 15, and 16 datasets demonstrate that
InstructABSA outperforms the previous state-of-the-art (SOTA) approaches on
Term Extraction (ATE), Sentiment Classification(ATSC) and Sentiment Pair
Extraction (ASPE) subtasks. In particular, InstructABSA outperforms the
previous state-of-the-art (SOTA) on the Rest14 ATE subtask by 5.69% points, the
Rest15 ATSC subtask by 9.59% points, and the Lapt14 AOPE subtask by 3.37%
points, surpassing 7x larger models. We also get competitive results on AOOE,
AOPE, and AOSTE subtasks indicating strong generalization ability to all
subtasks. Exploring sample efficiency reveals that just 50% train data is
required to get competitive results with other instruction tuning approaches.
Lastly, we assess the quality of instructions and observe that InstructABSA's
performance experiences a decline of ~10% when adding misleading examples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.13439">Navigating the Grey Area: How Expressions of Uncertainty and Overconfidence Affect Language Models. (arXiv:2302.13439v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1">Kaitlyn Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1">Dan Jurafsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1">Tatsunori Hashimoto</a></p>
<p>The increased deployment of LMs for real-world tasks involving knowledge and
facts makes it important to understand model epistemology: what LMs think they
know, and how their attitudes toward that knowledge are affected by language
use in their inputs. Here, we study an aspect of model epistemology: how
epistemic markers of certainty, uncertainty, or evidentiality like "I'm sure
it's", "I think it's", or "Wikipedia says it's" affect models, and whether they
contribute to model failures. We develop a typology of epistemic markers and
inject 50 markers into prompts for question answering. We find that LMs are
highly sensitive to epistemic markers in prompts, with accuracies varying more
than 80%. Surprisingly, we find that expressions of high certainty result in a
7% decrease in accuracy as compared to low certainty expressions; similarly,
factive verbs hurt performance, while evidentials benefit performance. Our
analysis of a popular pretraining dataset shows that these markers of
uncertainty are associated with answers on question-answering websites, while
markers of certainty are associated with questions. These associations may
suggest that the behavior of LMs is based on mimicking observed language use,
rather than truly reflecting epistemic uncertainty.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.00908">Interactive Text Generation. (arXiv:2303.00908v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Faltings_F/0/1/0/all/0/1">Felix Faltings</a>, <a href="http://arxiv.org/find/cs/1/au:+Galley_M/0/1/0/all/0/1">Michel Galley</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1">Baolin Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Brantley_K/0/1/0/all/0/1">Kiant&#xe9; Brantley</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1">Weixin Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yizhe Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jianfeng Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Dolan_B/0/1/0/all/0/1">Bill Dolan</a></p>
<p>Users interact with text, image, code, or other editors on a daily basis.
However, machine learning models are rarely trained in the settings that
reflect the interactivity between users and their editor. This is
understandable as training AI models with real users is not only slow and
costly, but what these models learn may be specific to user interface design
choices. Unfortunately, this means most of the research on text, code, and
image generation has focused on non-interactive settings, whereby the model is
expected to get everything right without accounting for any input from a user
who may be willing to help.
</p>
<p>We introduce a new Interactive Text Generation task that allows training
generation models interactively without the costs of involving real users, by
using user simulators that provide edits that guide the model towards a given
target text. We train our interactive models using Imitation Learning, and our
experiments against competitive non-interactive generation models show that
models trained interactively are superior to their non-interactive
counterparts, even when all models are given the same budget of user inputs or
edits.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.08553">The Image of the Process Interpretation of Regular Expressions is Not Closed under Bisimulation Collapse. (arXiv:2303.08553v2 [cs.LO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Grabmayer_C/0/1/0/all/0/1">Clemens Grabmayer</a></p>
<p>Axiomatization and expressibility problems for Milner's process semantics
(1984) of regular expressions modulo bisimilarity have turned out to be
difficult for the full class of expressions with deadlock 0 and empty step~1.
We report on a phenomenon that arises from the added presence of 1 when 0 is
available, and that brings a crucial reason for this difficulty into focus. To
wit, while interpretations of 1-free regular expressions are closed under
bisimulation collapse, this is not the case for the interpretations of
arbitrary regular expressions.
</p>
<p>Process graph interpretations of 1-free regular expressions satisfy the loop
existence and elimination property LEE, which is preserved under bisimulation
collapse. These features of LEE were applied for showing that an equational
proof system for 1-free regular expressions modulo bisimilarity is complete,
and that it is decidable in polynomial time whether a process graph is
bisimilar to the interpretation of a 1-free regular expression.
</p>
<p>While interpretations of regular expressions do not satisfy the property LEE
in general, we show that LEE can be recovered by refined interpretations as
graphs with 1-transitions refined interpretations with 1-transitions (which are
similar to silent steps for automata). This suggests that LEE can be expedient
also for the general axiomatization and expressibility problems. But a new
phenomenon emerges that needs to be addressed: the property of a process graph
`to can be refined into a process graph with 1-transitions and with LEE' is not
preserved under bisimulation collapse. We provide a 10-vertex graph with two
1-transitions that satisfies LEE, and in which a pair of bisimilar vertices
cannot be collapsed on to each other while preserving the refinement property.
This implies that the image of the process interpretation of regular
expressions is not closed under bisimulation collapse.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.12816">From Wide to Deep: Dimension Lifting Network for Parameter-efficient Knowledge Graph Embedding. (arXiv:2303.12816v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cai_B/0/1/0/all/0/1">Borui Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1">Yong Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1">Longxiang Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1">Di Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">He Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1">Jiong Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Luan_T/0/1/0/all/0/1">Tom Luan</a></p>
<p>Knowledge graph embedding (KGE) that maps entities and relations into vector
representations is essential for downstream applications. Conventional KGE
methods require high-dimensional representations to learn the complex structure
of knowledge graph, but lead to oversized model parameters. Recent advances
reduce parameters by low-dimensional entity representations, while developing
techniques (e.g., knowledge distillation or reinvented representation forms) to
compensate for reduced dimension. However, such operations introduce
complicated computations and model designs that may not benefit large knowledge
graphs. To seek a simple strategy to improve the parameter efficiency of
conventional KGE models, we take inspiration from that deeper neural networks
require exponentially fewer parameters to achieve expressiveness comparable to
wider networks for compositional structures. We view all entity representations
as a single-layer embedding network, and conventional KGE methods that adopt
high-dimensional entity representations equal widening the embedding network to
gain expressiveness. To achieve parameter efficiency, we instead propose a
deeper embedding network for entity representations, i.e., a narrow entity
embedding layer plus a multi-layer dimension lifting network (LiftNet).
Experiments on three public datasets show that by integrating LiftNet, four
conventional KGE methods with 16-dimensional representations achieve comparable
link prediction accuracy as original models that adopt 512-dimensional
representations, saving 68.4% to 96.9% parameters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.16445">Larger Probes Tell a Different Story: Extending Psycholinguistic Datasets Via In-Context Learning. (arXiv:2303.16445v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shivagunde_N/0/1/0/all/0/1">Namrata Shivagunde</a>, <a href="http://arxiv.org/find/cs/1/au:+Lialin_V/0/1/0/all/0/1">Vladislav Lialin</a>, <a href="http://arxiv.org/find/cs/1/au:+Rumshisky_A/0/1/0/all/0/1">Anna Rumshisky</a></p>
<p>Language model probing is often used to test specific capabilities of models.
However, conclusions from such studies may be limited when the probing
benchmarks are small and lack statistical power. In this work, we introduce
new, larger datasets for negation (NEG-1500-SIMP) and role reversal (ROLE-1500)
inspired by psycholinguistic studies. We dramatically extend existing NEG-136
and ROLE-88 benchmarks using GPT3, increasing their size from 18 and 44
sentence pairs to 750 each. We also create another version of extended negation
dataset (NEG-1500-SIMP-TEMP), created using template-based generation. It
consists of 770 sentence pairs. We evaluate 22 models on the extended datasets,
seeing model performance dip 20-57% compared to the original smaller
benchmarks. We observe high levels of negation sensitivity in models like BERT
and ALBERT demonstrating that previous findings might have been skewed due to
smaller test sets. Finally, we observe that while GPT3 has generated all the
examples in ROLE-1500 is only able to solve 24.6% of them during probing. The
datasets and code are available on
$\href{https://github.com/text-machine-lab/extending_psycholinguistic_dataset}{Github}$.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.03738">Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models. (arXiv:2304.03738v3 [cs.CY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ferrara_E/0/1/0/all/0/1">Emilio Ferrara</a></p>
<p>As the capabilities of generative language models continue to advance, the
implications of biases ingrained within these models have garnered increasing
attention from researchers, practitioners, and the broader public. This article
investigates the challenges and risks associated with biases in large-scale
language models like ChatGPT. We discuss the origins of biases, stemming from,
among others, the nature of training data, model specifications, algorithmic
constraints, product design, and policy decisions. We explore the ethical
concerns arising from the unintended consequences of biased model outputs. We
further analyze the potential opportunities to mitigate biases, the
inevitability of some biases, and the implications of deploying these models in
various applications, such as virtual assistants, content generation, and
chatbots. Finally, we review the current approaches to identify, quantify, and
mitigate biases in language models, emphasizing the need for a
multi-disciplinary, collaborative effort to develop more equitable,
transparent, and responsible AI systems. This article aims to stimulate a
thoughtful dialogue within the artificial intelligence community, encouraging
researchers and developers to reflect on the role of biases in generative
language models and the ongoing pursuit of ethical AI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.09402">MixPro: Simple yet Effective Data Augmentation for Prompt-based Learning. (arXiv:2304.09402v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bohan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Dou_L/0/1/0/all/0/1">Longxu Dou</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1">Yutai Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yunlong Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Mu_H/0/1/0/all/0/1">Honglin Mu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1">Qingfu Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1">Qinghua Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1">Wanxiang Che</a></p>
<p>Prompt-based learning has shown considerable promise in reformulating various
downstream tasks as cloze problems by combining original input with a
predetermined template. This approach demonstrates its effectiveness,
especially in few-shot learning scenarios, where the model is trained on a
scarce amount of data. Despite its successes, the limited templates and text in
few-shot prompt-based learning scenarios leave significant room for performance
improvement. Moreover, existing methods sometimes resort to model ensembles,
which, while effective, could potentially hamper model efficiency due to
increased computational demands. To address these issues, we introduce MixPro,
an augmentation method designed to augment both the vanilla input text and the
templates. We implement this through the token-level, the sentence-level, and
the template-level Mixup strategies. The experimental results on five few-shot
datasets show that MixPro outperforms other augmentation baselines, improving
model performance by an average of 5.08% compared to before augmentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13169">A Pretrainer&#x27;s Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, &amp; Toxicity. (arXiv:2305.13169v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Longpre_S/0/1/0/all/0/1">Shayne Longpre</a>, <a href="http://arxiv.org/find/cs/1/au:+Yauney_G/0/1/0/all/0/1">Gregory Yauney</a>, <a href="http://arxiv.org/find/cs/1/au:+Reif_E/0/1/0/all/0/1">Emily Reif</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Katherine Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1">Adam Roberts</a>, <a href="http://arxiv.org/find/cs/1/au:+Zoph_B/0/1/0/all/0/1">Barret Zoph</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1">Denny Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1">Jason Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Robinson_K/0/1/0/all/0/1">Kevin Robinson</a>, <a href="http://arxiv.org/find/cs/1/au:+Mimno_D/0/1/0/all/0/1">David Mimno</a>, <a href="http://arxiv.org/find/cs/1/au:+Ippolito_D/0/1/0/all/0/1">Daphne Ippolito</a></p>
<p>Pretraining is the preliminary and fundamental step in developing capable
language models (LM). Despite this, pretraining data design is critically
under-documented and often guided by empirically unsupported intuitions. To
address this, we pretrain 28 1.5B parameter decoder-only models, training on
data curated (1) at different times, (2) with varying toxicity and quality
filters, and (3) with different domain compositions. First, we quantify the
effect of pretraining data age. A temporal shift between evaluation data and
pretraining data leads to performance degradation, which is not overcome by
finetuning. Second, we explore the effect of quality and toxicity filters,
showing a trade-off between performance on standard benchmarks and risk of
toxic generations. Our findings indicate there does not exist a
one-size-fits-all solution to filtering training data. We also find that the
effects of different types of filtering are not predictable from text domain
characteristics. Lastly, we empirically validate that the inclusion of
heterogeneous data sources, like books and web, is broadly beneficial and
warrants greater prioritization. These findings constitute the largest set of
experiments to validate, quantify, and expose many undocumented intuitions
about text pretraining, which we hope will help support more informed
data-centric decisions in LM development.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13528">Transfer-Free Data-Efficient Multilingual Slot Labeling. (arXiv:2305.13528v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Razumovskaia_E/0/1/0/all/0/1">Evgeniia Razumovskaia</a>, <a href="http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1">Ivan Vuli&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1">Anna Korhonen</a></p>
<p>Slot labeling (SL) is a core component of task-oriented dialogue (ToD)
systems, where slots and corresponding values are usually language-, task- and
domain-specific. Therefore, extending the system to any new
language-domain-task configuration requires (re)running an expensive and
resource-intensive data annotation process. To mitigate the inherent data
scarcity issue, current research on multilingual ToD assumes that sufficient
English-language annotated data are always available for particular tasks and
domains, and thus operates in a standard cross-lingual transfer setup. In this
work, we depart from this often unrealistic assumption. We examine challenging
scenarios where such transfer-enabling English annotated data cannot be
guaranteed, and focus on bootstrapping multilingual data-efficient slot
labelers in transfer-free scenarios directly in the target languages without
any English-ready data. We propose a two-stage slot labeling approach (termed
TWOSL) which transforms standard multilingual sentence encoders into effective
slot labelers. In Stage 1, relying on SL-adapted contrastive learning with only
a handful of SL-annotated examples, we turn sentence encoders into
task-specific span encoders. In Stage 2, we recast SL from a token
classification into a simpler, less data-intensive span classification task.
Our results on two standard multilingual TOD datasets and across diverse
languages confirm the effectiveness and robustness of TWOSL. It is especially
effective for the most challenging transfer-free few-shot setups, paving the
way for quick and data-efficient bootstrapping of multilingual slot labelers
for ToD.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13583">Cross-Attention is Not Enough: Incongruity-Aware Dynamic Hierarchical Fusion for Multimodal Affect Recognition. (arXiv:2305.13583v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yaoting Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuanchao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1">Paul Pu Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1">Louis-Philippe Morency</a>, <a href="http://arxiv.org/find/cs/1/au:+Bell_P/0/1/0/all/0/1">Peter Bell</a>, <a href="http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1">Catherine Lai</a></p>
<p>Fusing multiple modalities has proven effective for multimodal information
processing. However, the incongruity between modalities poses a challenge for
multimodal fusion, especially in affect recognition. In this study, we first
analyze how the salient affective information in one modality can be affected
by the other, and demonstrate that inter-modal incongruity exists latently in
crossmodal attention. Based on this finding, we propose the Hierarchical
Crossmodal Transformer with Dynamic Modality Gating (HCT-DMG), a lightweight
incongruity-aware model, which dynamically chooses the primary modality in each
training batch and reduces fusion times by leveraging the learned hierarchy in
the latent space to alleviate incongruity. The experimental evaluation on five
benchmark datasets: CMU-MOSI, CMU-MOSEI, and IEMOCAP (sentiment and emotion),
where incongruity implicitly lies in hard samples, as well as UR-FUNNY (humour)
and MUStaRD (sarcasm), where incongruity is common, verifies the efficacy of
our approach, showing that HCT-DMG: 1) outperforms previous multimodal models
with a reduced size of approximately 0.8M parameters; 2) recognizes hard
samples where incongruity makes affect recognition difficult; 3) mitigates the
incongruity at the latent level in crossmodal attention.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13749">Goal-Driven Explainable Clustering via Language Descriptions. (arXiv:2305.13749v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zihan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1">Jingbo Shang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_R/0/1/0/all/0/1">Ruiqi Zhong</a></p>
<p>Unsupervised clustering is widely used to explore large corpora, but existing
formulations neither consider the users' goals nor explain clusters' meanings.
We propose a new task formulation, "Goal-Driven Clustering with Explanations"
(GoalEx), which represents both the goal and the explanations as free-form
language descriptions. For example, to categorize the errors made by a
summarization system, the input to GoalEx is a corpus of annotator-written
comments for system-generated summaries and a goal description "cluster the
comments based on why the annotators think the summary is imperfect.''; the
outputs are text clusters each with an explanation ("this cluster mentions that
the summary misses important context information."), which relates to the goal
and precisely explain which comments should (not) belong to a cluster. To
tackle GoalEx, we prompt a language model with "[corpus subset] + [goal] +
Brainstorm a list of explanations each representing a cluster."; then we
classify whether each sample belongs to a cluster based on its explanation;
finally, we use integer linear programming to select a subset of candidate
clusters to cover most samples while minimizing overlaps. Under both automatic
and human evaluation on corpora with or without labels, our method produces
more accurate and goal-related explanations than prior methods. We release our
data and implementation at https://github.com/ZihanWangKi/GoalEx.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14291">Evaluation of African American Language Bias in Natural Language Generation. (arXiv:2305.14291v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Deas_N/0/1/0/all/0/1">Nicholas Deas</a>, <a href="http://arxiv.org/find/cs/1/au:+Grieser_J/0/1/0/all/0/1">Jessi Grieser</a>, <a href="http://arxiv.org/find/cs/1/au:+Kleiner_S/0/1/0/all/0/1">Shana Kleiner</a>, <a href="http://arxiv.org/find/cs/1/au:+Patton_D/0/1/0/all/0/1">Desmond Patton</a>, <a href="http://arxiv.org/find/cs/1/au:+Turcan_E/0/1/0/all/0/1">Elsbeth Turcan</a>, <a href="http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1">Kathleen McKeown</a></p>
<p>We evaluate how well LLMs understand African American Language (AAL) in
comparison to their performance on White Mainstream English (WME), the
encouraged "standard" form of English taught in American classrooms. We measure
LLM performance using automatic metrics and human judgments for two tasks: a
counterpart generation task, where a model generates AAL (or WME) given WME (or
AAL), and a masked span prediction (MSP) task, where models predict a phrase
that was removed from their input. Our contributions include: (1) evaluation of
six pre-trained, large language models on the two language generation tasks;
(2) a novel dataset of AAL text from multiple contexts (social media, hip-hop
lyrics, focus groups, and linguistic interviews) with human-annotated
counterparts in WME; and (3) documentation of model performance gaps that
suggest bias and identification of trends in lack of understanding of AAL
features.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15047">Ghostbuster: Detecting Text Ghostwritten by Large Language Models. (arXiv:2305.15047v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Verma_V/0/1/0/all/0/1">Vivek Verma</a>, <a href="http://arxiv.org/find/cs/1/au:+Fleisig_E/0/1/0/all/0/1">Eve Fleisig</a>, <a href="http://arxiv.org/find/cs/1/au:+Tomlin_N/0/1/0/all/0/1">Nicholas Tomlin</a>, <a href="http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1">Dan Klein</a></p>
<p>We introduce Ghostbuster, a state-of-the-art system for detecting
AI-generated text. Our method works by passing documents through a series of
weaker language models, running a structured search over possible combinations
of their features, and then training a classifier on the selected features to
predict whether documents are AI-generated. Crucially, Ghostbuster does not
require access to token probabilities from the target model, making it useful
for detecting text generated by black-box models or unknown model versions. In
conjunction with our model, we release three new datasets of human- and
AI-generated text as detection benchmarks in the domains of student essays,
creative writing, and news articles. We compare Ghostbuster to a variety of
existing detectors, including DetectGPT and GPTZero, as well as a new RoBERTa
baseline. Ghostbuster achieves 99.0 F1 when evaluated across domains, which is
5.9 F1 higher than the best preexisting model. It also outperforms all previous
approaches in generalization across writing domains (+7.5 F1), prompting
strategies (+2.1 F1), and language models (+4.4 F1). We also analyze the
robustness of our system to a variety of perturbations and paraphrasing attacks
and evaluate its performance on documents written by non-native English
speakers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19915">Source Code Data Augmentation for Deep Learning: A Survey. (arXiv:2305.19915v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhuo_T/0/1/0/all/0/1">Terry Yue Zhuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhou Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1">Zhensu Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yufei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Li Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1">Xiaoning Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1">Zhenchang Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Lo_D/0/1/0/all/0/1">David Lo</a></p>
<p>The increasingly popular adoption of deep learning models in many critical
source code tasks motivates the development of data augmentation (DA)
techniques to enhance training data and improve various capabilities (e.g.,
robustness and generalizability) of these models. Although a series of DA
methods have been proposed and tailored for source code models, there lacks a
comprehensive survey and examination to understand their effectiveness and
implications. This paper fills this gap by conducting a comprehensive and
integrative survey of data augmentation for source code, wherein we
systematically compile and encapsulate existing literature to provide a
comprehensive overview of the field. We start with an introduction of data
augmentation in source code and then provide a discussion on major
representative approaches. Next, we highlight the general strategies and
techniques to optimize the DA quality. Subsequently, we underscore techniques
useful in real-world source code scenarios and downstream tasks. Finally, we
outline the prevailing challenges and potential opportunities for future
research. In essence, we aim to demystify the corpus of existing literature on
source code DA for deep learning, and foster further exploration in this
sphere. Complementing this, we present a continually updated GitHub repository
that hosts a list of update-to-date papers on DA for source code modeling,
accessible at \url{https://github.com/terryyz/DataAug4Code}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.11816">Learning to Generate Better Than Your LLM. (arXiv:2306.11816v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1">Jonathan D. Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Brantley_K/0/1/0/all/0/1">Kiante Brantley</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramamurthy_R/0/1/0/all/0/1">Rajkumar Ramamurthy</a>, <a href="http://arxiv.org/find/cs/1/au:+Misra_D/0/1/0/all/0/1">Dipendra Misra</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1">Wen Sun</a></p>
<p>Reinforcement learning (RL) has emerged as a powerful paradigm for
fine-tuning Large Language Models (LLMs) for text generation. In particular,
recent LLMs such as ChatGPT and GPT-4 can engage in fluent conversations with
users after finetuning with RL. Capitalizing on key properties of text
generation, we seek to investigate RL algorithms beyond general purpose
algorithms like Proximal Policy Optimization (PPO). In particular, we extend RL
algorithms to allow them to interact with a dynamic black-box guide LLM and
propose RL with guided feedback (RLGF), a suite of RL algorithms for LLM
fine-tuning. We provide two ways for the guide LLM to interact with the LLM to
be optimized for maximizing rewards. The guide LLM can generate text which
serves as additional starting states for the RL optimization procedure. The
guide LLM can also be used to complete the partial sentences generated by the
LLM that is being optimized, treating the guide LLM as an expert to imitate and
surpass eventually. We experiment on the IMDB positive sentiment, CommonGen,
and TL;DR summarization tasks. We show that our RL algorithms achieve higher
performance than supervised learning (SL) and the RL baseline PPO,
demonstrating the benefit of interaction with the guide LLM. On both CommonGen
and TL;DR, we not only outperform our SL baselines but also improve upon PPO
across a variety of metrics beyond the one we optimized for. Our code can be
found at https://github.com/Cornell-RL/tril.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03025">Style Over Substance: Evaluation Biases for Large Language Models. (arXiv:2307.03025v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1">Minghao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1">Alham Fikri Aji</a></p>
<p>As large language models (LLMs) continue to advance, accurately and
comprehensively evaluating their performance becomes increasingly challenging.
Ranking the relative performance of LLMs based on Elo ratings, according to
human judgment, is gaining more popularity. However, the extent to which humans
and LLMs are capable evaluators remains uncertain. This study investigates the
behavior of crowd-sourced and expert annotators, as well as LLMs, when
comparing outputs from different models. To achieve this, we curate a dataset
of intentionally flawed machine-generated answers. Our findings reveal a
concerning bias in the evaluation process, as answers with factual errors are
rated more favorably than answers that are too short or contained grammatical
errors. To address this issue, we propose independently evaluating
machine-generated text across multiple dimensions, rather than merging all the
evaluation aspects into a single score. We instantiate this idea with the Elo
rating system, resulting in the Multi-Elo Rating System (MERS). Empirical
results from our study reveal that this proposed approach significantly
enhances the quality of LLM-based evaluations, particularly in terms of factual
accuracy. However, there is no significant improvement in crowd-sourced-based
evaluations, indicating the need for further investigation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04349">RLTF: Reinforcement Learning from Unit Test Feedback. (arXiv:2307.04349v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiate Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yiqin Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_K/0/1/0/all/0/1">Kaiwen Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1">Qiang Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1">Xiao Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1">Wei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_D/0/1/0/all/0/1">Deheng Ye</a></p>
<p>The goal of program synthesis, or code generation, is to generate executable
code based on given descriptions. Recently, there has been an increasing number
of studies employing reinforcement learning (RL) to improve the performance of
large language models (LLMs) for code. However, current representative works
either rely solely on offline frameworks, limiting the exploration of new
sample spaces, or fall short in the utilization of unit test signals, not
accounting for specific error locations within the code. To address these
issues, we propose RLTF, i.e., Reinforcement Learning from Unit Test Feedback,
a novel online RL framework with unit test feedback of multi-granularity for
refining code LLMs. Our approach generates data in real-time during training
and simultaneously utilizes fine-grained feedback signals to guide the model
towards producing higher-quality code. Extensive experiments show that RLTF
achieves state-of-the-art performance on the APPS and the MBPP benchmarks. Our
code is available at: https://github.com/Zyq-scut/RLTF.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06440">No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models. (arXiv:2307.06440v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kaddour_J/0/1/0/all/0/1">Jean Kaddour</a>, <a href="http://arxiv.org/find/cs/1/au:+Key_O/0/1/0/all/0/1">Oscar Key</a>, <a href="http://arxiv.org/find/cs/1/au:+Nawrot_P/0/1/0/all/0/1">Piotr Nawrot</a>, <a href="http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1">Pasquale Minervini</a>, <a href="http://arxiv.org/find/cs/1/au:+Kusner_M/0/1/0/all/0/1">Matt J. Kusner</a></p>
<p>The computation necessary for training Transformer-based language models has
skyrocketed in recent years. This trend has motivated research on efficient
training algorithms designed to improve training, validation, and downstream
performance faster than standard training. In this work, we revisit three
categories of such algorithms: dynamic architectures (layer stacking, layer
dropping), batch selection (selective backprop, RHO loss), and efficient
optimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed
computation budget using such methods, we find that their training, validation,
and downstream gains vanish compared to a baseline with a fully-decayed
learning rate. We define an evaluation protocol that enables computation to be
done on arbitrary machines by mapping all computation time to a reference
machine which we call reference system time. We discuss the limitations of our
proposed protocol and release our code to encourage rigorous research in
efficient training procedures: https://github.com/JeanKaddour/NoTrainNoGain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.11254">An In-Depth Evaluation of Federated Learning on Biomedical Natural Language Processing. (arXiv:2307.11254v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1">Le Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1">Gaoxiang Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+zhou_s/0/1/0/all/0/1">sicheng zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+chen_j/0/1/0/all/0/1">jiandong chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Rui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Ziyue Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Ju Sun</a></p>
<p>Language models (LMs) such as BERT and GPT have revolutionized natural
language processing (NLP). However, the medical field faces challenges in
training LMs due to limited data access and privacy constraints imposed by
regulations like the Health Insurance Portability and Accountability Act
(HIPPA) and the General Data Protection Regulation (GDPR). Federated learning
(FL) offers a decentralized solution that enables collaborative learning while
ensuring data privacy. In this study, we evaluated FL on 2 biomedical NLP tasks
encompassing 8 corpora using 6 LMs. Our results show that: 1) FL models
consistently outperformed models trained on individual clients' data and
sometimes performed comparably with models trained with polled data; 2) with
the fixed number of total data, FL models training with more clients produced
inferior performance but pre-trained transformer-based models exhibited great
resilience. 3) FL models significantly outperformed large language models using
zero-/one-shot learning and offered lightning inference speed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.11760">Large Language Models Understand and Can be Enhanced by Emotional Stimuli. (arXiv:2307.11760v7 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Cheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jindong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yixuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1">Kaijie Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_W/0/1/0/all/0/1">Wenxin Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Lian_J/0/1/0/all/0/1">Jianxun Lian</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1">Fang Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1">Qiang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xing Xie</a></p>
<p>Emotional intelligence significantly impacts our daily behaviors and
interactions. Although Large Language Models (LLMs) are increasingly viewed as
a stride toward artificial general intelligence, exhibiting impressive
performance in numerous tasks, it is still uncertain if LLMs can genuinely
grasp psychological emotional stimuli. Understanding and responding to
emotional cues gives humans a distinct advantage in problem-solving. In this
paper, we take the first step towards exploring the ability of LLMs to
understand emotional stimuli. To this end, we first conduct automatic
experiments on 45 tasks using various LLMs, including Flan-T5-Large, Vicuna,
Llama 2, BLOOM, ChatGPT, and GPT-4. Our tasks span deterministic and generative
applications that represent comprehensive evaluation scenarios. Our automatic
experiments show that LLMs have a grasp of emotional intelligence, and their
performance can be improved with emotional prompts (which we call
"EmotionPrompt" that combines the original prompt with emotional stimuli),
e.g., 8.00% relative performance improvement in Instruction Induction and 115%
in BIG-Bench. In addition to those deterministic tasks that can be
automatically evaluated using existing metrics, we conducted a human study with
106 participants to assess the quality of generative tasks using both vanilla
and emotional prompts. Our human study results demonstrate that EmotionPrompt
significantly boosts the performance of generative tasks (10.9% average
improvement in terms of performance, truthfulness, and responsibility metrics).
We provide an in-depth discussion regarding why EmotionPrompt works for LLMs
and the factors that may influence its performance. We posit that EmotionPrompt
heralds a novel avenue for exploring interdisciplinary knowledge for human-LLMs
interaction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.11772">AutoAlign: Fully Automatic and Effective Knowledge Graph Alignment enabled by Large Language Models. (arXiv:2307.11772v3 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Rui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1">Yixin Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Trisedya_B/0/1/0/all/0/1">Bayu Distiawan Trisedya</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xiaoyan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Min Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1">Hong Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1">Jianzhong Qi</a></p>
<p>The task of entity alignment between knowledge graphs (KGs) aims to identify
every pair of entities from two different KGs that represent the same entity.
Many machine learning-based methods have been proposed for this task. However,
to our best knowledge, existing methods all require manually crafted seed
alignments, which are expensive to obtain. In this paper, we propose the first
fully automatic alignment method named AutoAlign, which does not require any
manually crafted seed alignments. Specifically, for predicate embeddings,
AutoAlign constructs a predicate-proximity-graph with the help of large
language models to automatically capture the similarity between predicates
across two KGs. For entity embeddings, AutoAlign first computes the entity
embeddings of each KG independently using TransE, and then shifts the two KGs'
entity embeddings into the same vector space by computing the similarity
between entities based on their attributes. Thus, both predicate alignment and
entity alignment can be done without manually crafted seed alignments.
AutoAlign is not only fully automatic, but also highly effective. Experiments
using real-world KGs show that AutoAlign improves the performance of entity
alignment significantly compared to state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.12166">The Imitation Game: Detecting Human and AI-Generated Texts in the Era of ChatGPT and BARD. (arXiv:2307.12166v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hayawi_K/0/1/0/all/0/1">Kadhim Hayawi</a>, <a href="http://arxiv.org/find/cs/1/au:+Shahriar_S/0/1/0/all/0/1">Sakib Shahriar</a>, <a href="http://arxiv.org/find/cs/1/au:+Mathew_S/0/1/0/all/0/1">Sujith Samuel Mathew</a></p>
<p>The potential of artificial intelligence (AI)-based large language models
(LLMs) holds considerable promise in revolutionizing education, research, and
practice. However, distinguishing between human-written and AI-generated text
has become a significant task. This paper presents a comparative study,
introducing a novel dataset of human-written and LLM-generated texts in
different genres: essays, stories, poetry, and Python code. We employ several
machine learning models to classify the texts. Results demonstrate the efficacy
of these models in discerning between human and AI-generated text, despite the
dataset's limited sample size. However, the task becomes more challenging when
classifying GPT-generated text, particularly in story writing. The results
indicate that the models exhibit superior performance in binary classification
tasks, such as distinguishing human-generated text from a specific LLM,
compared to the more complex multiclass tasks that involve discerning among
human-generated and multiple LLMs. Our findings provide insightful implications
for AI text detection while our dataset paves the way for future research in
this evolving area.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.16833">Data Augmentation for Neural Machine Translation using Generative Language Model. (arXiv:2307.16833v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1">Seokjin Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Su Ah Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_W/0/1/0/all/0/1">Woohwan Jung</a></p>
<p>Despite the rapid growth in model architecture, the scarcity of large
parallel corpora remains the main bottleneck in Neural Machine Translation.
Data augmentation is a technique that enhances the performance of data-hungry
models by generating synthetic data instead of collecting new ones. We explore
prompt-based data augmentation approaches that leverage large-scale language
models such as ChatGPT. To create a synthetic parallel corpus, we compare 3
methods using different prompts. We employ two assessment metrics to measure
the diversity of the generated synthetic data. This approach requires no
further model training cost, which is mandatory in other augmentation methods
like back-translation. The proposed method improves the unaugmented baseline by
0.68 BLEU score.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.06744">Token-Scaled Logit Distillation for Ternary Weight Generative Language Models. (arXiv:2308.06744v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1">Minsoo Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Sihwa Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Janghwan Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1">Sukjin Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1">Du-Seong Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sung_W/0/1/0/all/0/1">Wonyong Sung</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1">Jungwook Choi</a></p>
<p>Generative Language Models (GLMs) have shown impressive performance in tasks
such as text generation, understanding, and reasoning. However, the large model
size poses challenges for practical deployment. To solve this problem,
Quantization-Aware Training (QAT) has become increasingly popular. However,
current QAT methods for generative models have resulted in a noticeable loss of
accuracy. To counteract this issue, we propose a novel knowledge distillation
method specifically designed for GLMs. Our method, called token-scaled logit
distillation, prevents overfitting and provides superior learning from the
teacher model and ground truth. This research marks the first evaluation of
ternary weight quantization-aware training of large-scale GLMs with less than
1.0 degradation in perplexity and achieves enhanced accuracy in tasks like
common-sense QA and arithmetic reasoning as well as natural language
understanding. Our code is available at https://github.com/aiha-lab/TSLD.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.07336">Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic. (arXiv:2308.07336v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Morishita_T/0/1/0/all/0/1">Terufumi Morishita</a>, <a href="http://arxiv.org/find/cs/1/au:+Morio_G/0/1/0/all/0/1">Gaku Morio</a>, <a href="http://arxiv.org/find/cs/1/au:+Yamaguchi_A/0/1/0/all/0/1">Atsuki Yamaguchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Sogawa_Y/0/1/0/all/0/1">Yasuhiro Sogawa</a></p>
<p>We study a synthetic corpus based approach for language models (LMs) to
acquire logical deductive reasoning ability. The previous studies generated
deduction examples using specific sets of deduction rules. However, these rules
were limited or otherwise arbitrary, limiting the generalizability of acquired
reasoning ability. We rethink this and adopt a well-grounded set of deduction
rules based on formal logic theory, which can derive any other deduction rules
when combined in a multistep way. Then, using the proposed corpora, which we
name FLD (Formal Logic Deduction), we first evaluate and analyze the logical
reasoning ability of the latest LLMs. Even GPT-4 can solve only half of the
problems, suggesting that pure logical reasoning isolated from knowledge is
still challenging for the LLMs, and additional training specialized in logical
reasoning is indeed essential. We next empirically verify that LMs trained on
FLD corpora acquire more generalizable reasoning ability. Furthermore, we
identify the aspects of reasoning ability on which deduction corpora can
enhance LMs and those on which they cannot, and discuss future directions on
each aspect. The released corpora serve both as learning resources and as
challenging benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.10248">Activation Addition: Steering Language Models Without Optimization. (arXiv:2308.10248v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Turner_A/0/1/0/all/0/1">Alexander Matt Turner</a>, <a href="http://arxiv.org/find/cs/1/au:+Thiergart_L/0/1/0/all/0/1">Lisa Thiergart</a>, <a href="http://arxiv.org/find/cs/1/au:+Udell_D/0/1/0/all/0/1">David Udell</a>, <a href="http://arxiv.org/find/cs/1/au:+Leech_G/0/1/0/all/0/1">Gavin Leech</a>, <a href="http://arxiv.org/find/cs/1/au:+Mini_U/0/1/0/all/0/1">Ulisse Mini</a>, <a href="http://arxiv.org/find/cs/1/au:+MacDiarmid_M/0/1/0/all/0/1">Monte MacDiarmid</a></p>
<p>Reliably controlling the behavior of large language models is a pressing open
problem. Existing methods include supervised finetuning, reinforcement learning
from human feedback, prompt engineering and guided decoding. We instead
investigate activation engineering: modifying activations at inference-time to
predictably alter model behavior. We bias the forward pass with a 'steering
vector' implicitly specified through natural language. Past work learned these
steering vectors; our Activation Addition (ActAdd) method instead computes them
by taking the activation differences which result from pairs of prompts.
</p>
<p>We demonstrate ActAdd on GPT-2 on OpenWebText and ConceptNet, and replicate
the effect on Llama-13B and GPT-J-6B. Our approach yields inference-time
control over high-level properties of output &amp; preserves performance on
off-target topics. The method requires far less compute and implementation
effort than finetuning and RLHF, allows for natural language specification by
users, and its overhead scales naturally with model size.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.00312">Insights Into the Nutritional Prevention of Macular Degeneration based on a Comparative Topic Modeling Approach. (arXiv:2309.00312v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jacaruso_L/0/1/0/all/0/1">Lucas Cassiel Jacaruso</a></p>
<p>Topic modeling and text mining are subsets of Natural Language Processing
(NLP) with relevance for conducting meta-analysis (MA) and systematic review
(SR). For evidence synthesis, the above NLP methods are conventionally used for
topic-specific literature searches or extracting values from reports to
automate essential phases of SR and MA. Instead, this work proposes a
comparative topic modeling approach to analyze reports of contradictory results
on the same general research question. Specifically, the objective is to
identify topics exhibiting distinct associations with significant results for
an outcome of interest by ranking them according to their proportional
occurrence in (and consistency of distribution across) reports of significant
effects. The proposed method was tested on broad-scope studies addressing
whether supplemental nutritional compounds significantly benefit macular
degeneration (MD). Six compounds were identified as having a particular
association with reports of significant results for benefiting MD. Four of
these were further supported in terms of effectiveness upon conducting a
follow-up literature search for validation (omega-3 fatty acids, copper,
zeaxanthin, and nitrates). The two not supported by the follow-up literature
search (niacin and molybdenum) also had scores in the lowest range under the
proposed scoring system, suggesting that the proposed methods score for a given
topic may be a viable proxy for its degree of association with the outcome of
interest and can be helpful in the search for potentially causal relationships.
These results underpin the proposed methods potential to add specificity in
understanding effects from broad-scope reports, elucidate topics of interest
for future research, and guide evidence synthesis in a systematic and scalable
way. All of this is accomplished while yielding valuable insights into the
prevention of MD.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03184">Retrieval-augmented Generation to Improve Math Question-Answering: Trade-offs Between Groundedness and Human Preference. (arXiv:2310.03184v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Levonian_Z/0/1/0/all/0/1">Zachary Levonian</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chenglu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1">Wangda Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gade_A/0/1/0/all/0/1">Anoushka Gade</a>, <a href="http://arxiv.org/find/cs/1/au:+Henkel_O/0/1/0/all/0/1">Owen Henkel</a>, <a href="http://arxiv.org/find/cs/1/au:+Postle_M/0/1/0/all/0/1">Millie-Ellen Postle</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_W/0/1/0/all/0/1">Wanli Xing</a></p>
<p>For middle-school math students, interactive question-answering (QA) with
tutors is an effective way to learn. The flexibility and emergent capabilities
of generative large language models (LLMs) has led to a surge of interest in
automating portions of the tutoring process - including interactive QA to
support conceptual discussion of mathematical concepts. However, LLM responses
to math questions can be incorrect or mismatched to the educational context -
such as being misaligned with a school's curriculum. One potential solution is
retrieval-augmented generation (RAG), which involves incorporating a vetted
external knowledge source in the LLM prompt to increase response quality. In
this paper, we designed prompts that retrieve and use content from a
high-quality open-source math textbook to generate responses to real student
questions. We evaluate the efficacy of this RAG system for middle-school
algebra and geometry QA by administering a multi-condition survey, finding that
humans prefer responses generated using RAG, but not when responses are too
grounded in the textbook content. We argue that while RAG is able to improve
response quality, designers of math QA systems must consider trade-offs between
generating responses preferred by students and responses closely matched to
specific educational resources.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04313">KoMultiText: Large-Scale Korean Text Dataset for Classifying Biased Speech in Real-World Online Services. (arXiv:2310.04313v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1">Dasol Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1">Jooyoung Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1">Eunsun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1">Jinwoo Seo</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1">Heejune Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Na_D/0/1/0/all/0/1">Dongbin Na</a></p>
<p>With the growth of online services, the need for advanced text classification
algorithms, such as sentiment analysis and biased text detection, has become
increasingly evident. The anonymous nature of online services often leads to
the presence of biased and harmful language, posing challenges to maintaining
the health of online communities. This phenomenon is especially relevant in
South Korea, where large-scale hate speech detection algorithms have not yet
been broadly explored. In this paper, we introduce "KoMultiText", a new
comprehensive, large-scale dataset collected from a well-known South Korean SNS
platform. Our proposed dataset provides annotations including (1) Preferences,
(2) Profanities, and (3) Nine types of Bias for the text samples, enabling
multi-task learning for simultaneous classification of user-generated texts.
Leveraging state-of-the-art BERT-based language models, our approach surpasses
human-level accuracy across diverse classification tasks, as measured by
various metrics. Beyond academic contributions, our work can provide practical
solutions for real-world hate speech and bias mitigation, contributing directly
to the improvement of online community health. Our work provides a robust
foundation for future research aiming to improve the quality of online
discourse and foster societal well-being. All source codes and datasets are
publicly accessible at https://github.com/Dasol-Choi/KoMultiText.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04793">FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets. (arXiv:2310.04793v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1">Neng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Hongyang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Christina Dan Wang</a></p>
<p>In the swiftly expanding domain of Natural Language Processing (NLP), the
potential of GPT-based models for the financial sector is increasingly evident.
However, the integration of these models with financial datasets presents
challenges, notably in determining their adeptness and relevance. This paper
introduces a distinctive approach anchored in the Instruction Tuning paradigm
for open-source large language models, specifically adapted for financial
contexts. Through this methodology, we capitalize on the interoperability of
open-source models, ensuring a seamless and transparent integration. We begin
by explaining the Instruction Tuning paradigm, highlighting its effectiveness
for immediate integration. The paper presents a benchmarking scheme designed
for end-to-end training and testing, employing a cost-effective progression.
Firstly, we assess basic competencies and fundamental tasks, such as Named
Entity Recognition (NER) and sentiment analysis to enhance specialization.
Next, we delve into a comprehensive model, executing multi-task operations by
amalgamating all instructional tunings to examine versatility. Finally, we
explore the zero-shot capabilities by earmarking unseen tasks and incorporating
novel datasets to understand adaptability in uncharted terrains. Such a
paradigm fortifies the principles of openness and reproducibility, laying a
robust foundation for future investigations in open-source financial large
language models (FinLLMs).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05163">An Investigation of LLMs&#x27; Inefficacy in Understanding Converse Relations. (arXiv:2310.05163v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qi_C/0/1/0/all/0/1">Chengwen Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bowen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hui_B/0/1/0/all/0/1">Binyuan Hui</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bailin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jinyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jinwang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Laili_Y/0/1/0/all/0/1">Yuanjun Laili</a></p>
<p>Large Language Models (LLMs) have achieved remarkable success in many formal
language oriented tasks, such as structural data-to-text and semantic parsing.
However current benchmarks mostly follow the data distribution of the
pre-training data of LLMs. Therefore, a natural question rises that do LLMs
really understand the structured semantics of formal languages. In this paper,
we investigate this problem on a special case, converse binary relation. We
introduce a new benchmark ConvRe focusing on converse relations, which contains
17 relations and 1240 triples extracted from popular knowledge graph completion
datasets. Our ConvRE features two tasks, Re2Text and Text2Re, which are
formulated as multi-choice question answering to evaluate LLMs' ability to
determine the matching between relations and associated text. For the
evaluation protocol, apart from different prompting methods, we further
introduce variants to the test text and few-shot example text. We conduct
experiments on three popular LLM families and have observed various scaling
trends. The results suggest that LLMs often resort to shortcut learning and
still face challenges on our proposed benchmark.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05317">Task-Adaptive Tokenization: Enhancing Long-Form Text Generation Efficacy in Mental Health and Beyond. (arXiv:2310.05317v5 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Siyang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_N/0/1/0/all/0/1">Naihao Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Sabour_S/0/1/0/all/0/1">Sahand Sabour</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1">Yilin Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1">Minlie Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1">Rada Mihalcea</a></p>
<p>We propose task-adaptive tokenization as a way to adapt the generation
pipeline to the specifics of a downstream task and enhance long-form generation
in mental health. Inspired by insights from cognitive science, our
task-adaptive tokenizer samples variable segmentations from multiple outcomes,
with sampling probabilities optimized based on task-specific data. We introduce
a strategy for building a specialized vocabulary and introduce a vocabulary
merging protocol that allows for the integration of task-specific tokens into
the pre-trained model's tokenization step. Through extensive experiments on
psychological question-answering tasks in both Chinese and English, we find
that our task-adaptive tokenization approach brings a significant improvement
in generation performance while using up to 60% fewer tokens. Preliminary
experiments point to promising results when using our tokenization approach
with very large language models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.06552">Automated clinical coding using off-the-shelf large language models. (arXiv:2310.06552v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Boyle_J/0/1/0/all/0/1">Joseph S. Boyle</a>, <a href="http://arxiv.org/find/cs/1/au:+Kascenas_A/0/1/0/all/0/1">Antanas Kascenas</a>, <a href="http://arxiv.org/find/cs/1/au:+Lok_P/0/1/0/all/0/1">Pat Lok</a>, <a href="http://arxiv.org/find/cs/1/au:+Liakata_M/0/1/0/all/0/1">Maria Liakata</a>, <a href="http://arxiv.org/find/cs/1/au:+ONeil_A/0/1/0/all/0/1">Alison Q. O&#x27;Neil</a></p>
<p>The task of assigning diagnostic ICD codes to patient hospital admissions is
typically performed by expert human coders. Efforts towards automated ICD
coding are dominated by supervised deep learning models. However, difficulties
in learning to predict the large number of rare codes remain a barrier to
adoption in clinical practice. In this work, we leverage off-the-shelf
pre-trained generative large language models (LLMs) to develop a practical
solution that is suitable for zero-shot and few-shot code assignment, with no
need for further task-specific training. Unsupervised pre-training alone does
not guarantee precise knowledge of the ICD ontology and specialist clinical
coding task, therefore we frame the task as information extraction, providing a
description of each coded concept and asking the model to retrieve related
mentions. For efficiency, rather than iterating over all codes, we leverage the
hierarchical nature of the ICD ontology to sparsely search for relevant codes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.09017">Dont Add, dont Miss: Effective Content Preserving Generation from Pre-Selected Text Spans. (arXiv:2310.09017v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Slobodkin_A/0/1/0/all/0/1">Aviv Slobodkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Caciularu_A/0/1/0/all/0/1">Avi Caciularu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hirsch_E/0/1/0/all/0/1">Eran Hirsch</a>, <a href="http://arxiv.org/find/cs/1/au:+Dagan_I/0/1/0/all/0/1">Ido Dagan</a></p>
<p>The recently introduced Controlled Text Reduction (CTR) task isolates the
text generation step within typical summarization-style tasks. It does so by
challenging models to generate coherent text conforming to pre-selected content
within the input text (``highlights''). This framing enables increased
modularity in summarization-like tasks, allowing to couple a single CTR model
with various content-selection setups and modules. However, there are currently
no reliable CTR models, while the performance of the existing baseline for the
task is mediocre, falling short of practical utility. Here, we address this gap
by introducing a high-quality, open-source CTR model that tackles two prior key
limitations: inadequate enforcement of the content-preservation constraint, and
suboptimal silver training data. Addressing these, we amplify the
content-preservation constraint in both training, via RL, and inference, via a
controlled decoding strategy. Further, we substantially improve the silver
training data quality via GPT-4 distillation. Overall, pairing the distilled
dataset with the highlight-adherence strategies yields marked gains over the
current baseline, of up to 30 ROUGE-L points, providing a reliable CTR model
for downstream use.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.09624">ASSERT: Automated Safety Scenario Red Teaming for Evaluating the Robustness of Large Language Models. (arXiv:2310.09624v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mei_A/0/1/0/all/0/1">Alex Mei</a>, <a href="http://arxiv.org/find/cs/1/au:+Levy_S/0/1/0/all/0/1">Sharon Levy</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">William Yang Wang</a></p>
<p>As large language models are integrated into society, robustness toward a
suite of prompts is increasingly important to maintain reliability in a
high-variance environment.Robustness evaluations must comprehensively
encapsulate the various settings in which a user may invoke an intelligent
system. This paper proposes ASSERT, Automated Safety Scenario Red Teaming,
consisting of three methods -- semantically aligned augmentation, target
bootstrapping, and adversarial knowledge injection. For robust safety
evaluation, we apply these methods in the critical domain of AI safety to
algorithmically generate a test suite of prompts covering diverse robustness
settings -- semantic equivalence, related scenarios, and adversarial. We
partition our prompts into four safety domains for a fine-grained analysis of
how the domain affects model performance. Despite dedicated safeguards in
existing state-of-the-art models, we find statistically significant performance
differences of up to 11% in absolute classification accuracy among semantically
related scenarios and error rates of up to 19% absolute error in zero-shot
adversarial settings, raising concerns for users' physical safety.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.10072">Fine-tuning ChatGPT for Automatic Scoring. (arXiv:2310.10072v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Latif_E/0/1/0/all/0/1">Ehsan Latif</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1">Xiaoming Zhai</a></p>
<p>This study highlights the potential of fine-tuned ChatGPT (GPT-3.5) for
automatically scoring student written constructed responses using example
assessment tasks in science education. Recent studies on OpenAI's generative
model GPT-3.5 proved its superiority in predicting the natural language with
high accuracy and human-like responses. GPT-3.5 has been trained over enormous
online language materials such as journals and Wikipedia; therefore, more than
direct usage of pre-trained GPT-3.5 is required for automatic scoring as
students utilize a different language than trained material. These imply that a
domain-specific model, fine-tuned over data for specific tasks, can enhance
model performance. In this study, we fine-tuned GPT-3.5 on six assessment tasks
with a diverse dataset of middle-school and high-school student responses and
expert scoring. The six tasks comprise two multi-label and four multi-class
assessment tasks. We compare the performance of fine-tuned GPT-3.5 with the
fine-tuned state-of-the-art Google's generated language model, BERT. The
results show that in-domain training corpora constructed from science questions
and responses for BERT achieved average accuracy = 0.838, SD = 0.069. GPT-3.5
shows a remarkable average increase (9.1%) in automatic scoring accuracy (mean
= 9.15, SD = 0.042) for the six tasks, p =0.001 &lt; 0.05. Specifically, for
multi-label tasks (item 1 with 5 labels; item 2 with 10 labels), GPT-3.5
achieved significantly higher scoring accuracy than BERT across all the labels,
with the second item achieving a 7.1% increase. The average scoring increase
for the four multi-class items for GPT-3.5 was 10.6% compared to BERT. Our
study confirmed the effectiveness of fine-tuned GPT-3.5 for automatic scoring
of student responses on domain-specific data in education with high accuracy.
We have released fine-tuned models for public use and community engagement.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.11670">Prototype-based HyperAdapter for Sample-Efficient Multi-task Tuning. (arXiv:2310.11670v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Hao Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1">Jie Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1">Zhaofeng He</a></p>
<p>Parameter-efficient fine-tuning (PEFT) has shown its effectiveness in
adapting the pre-trained language models to downstream tasks while only
updating a small number of parameters. Despite the success, most existing
methods independently adapt to each task without considering knowledge transfer
between tasks and are limited to low-data regimes. To overcome this issue, we
propose Prototype-based HyperAdapter (PHA), a novel framework built on the
adapter-tuning and hypernetwork. It introduces an instance-dense retriever and
a prototypical hypernetwork to generate the conditional modules in a
sample-efficient manner. This leads to comparable performance improvements
against existing PEFT methods on multi-task learning and few-shot transfer
learning. More importantly, when the available data size gets smaller, our
method outperforms other strong baselines by a large margin. Based on our
extensive empirical experiments across various datasets, we demonstrate that
PHA strikes a better trade-off between trainable parameters, accuracy on stream
tasks, and sample efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.11689">Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs. (arXiv:2310.11689v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiefeng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1">Jinsung Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Ebrahimi_S/0/1/0/all/0/1">Sayna Ebrahimi</a>, <a href="http://arxiv.org/find/cs/1/au:+Arik_S/0/1/0/all/0/1">Sercan O Arik</a>, <a href="http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1">Tomas Pfister</a>, <a href="http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1">Somesh Jha</a></p>
<p>Large language models (LLMs) have recently shown great advances in a variety
of tasks, including natural language understanding and generation. However,
their use in high-stakes decision-making scenarios is still limited due to the
potential for errors. Selective prediction is a technique that can be used to
improve the reliability of the LLMs by allowing them to abstain from making
predictions when they are unsure of the answer. In this work, we propose a
novel framework for adaptation with self-evaluation to improve the selective
prediction performance of LLMs. Our framework is based on the idea of using
parameter-efficient tuning to adapt the LLM to the specific task at hand while
improving its ability to perform self-evaluation. We evaluate our method on a
variety of question-answering (QA) datasets and show that it outperforms
state-of-the-art selective prediction methods. For example, on the CoQA
benchmark, our method improves the AUACC from 91.23% to 92.63% and improves the
AUROC from 74.61% to 80.25%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.11715">Enhancing Low-resource Fine-grained Named Entity Recognition by Leveraging Coarse-grained Datasets. (arXiv:2310.11715v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Su Ah Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1">Seokjin Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_W/0/1/0/all/0/1">Woohwan Jung</a></p>
<p>Named Entity Recognition (NER) frequently suffers from the problem of
insufficient labeled data, particularly in fine-grained NER scenarios. Although
$K$-shot learning techniques can be applied, their performance tends to
saturate when the number of annotations exceeds several tens of labels. To
overcome this problem, we utilize existing coarse-grained datasets that offer a
large number of annotations. A straightforward approach to address this problem
is pre-finetuning, which employs coarse-grained data for representation
learning. However, it cannot directly utilize the relationships between
fine-grained and coarse-grained entities, although a fine-grained entity type
is likely to be a subcategory of a coarse-grained entity type. We propose a
fine-grained NER model with a Fine-to-Coarse(F2C) mapping matrix to leverage
the hierarchical structure explicitly. In addition, we present an inconsistency
filtering method to eliminate coarse-grained entities that are inconsistent
with fine-grained entity types to avoid performance degradation. Our
experimental results show that our method outperforms both $K$-shot learning
and supervised learning methods when dealing with a small number of
fine-grained annotations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.11877">The Curious Case of Hallucinatory (Un)answerability: Finding Truths in the Hidden States of Over-Confident Large Language Models. (arXiv:2310.11877v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Slobodkin_A/0/1/0/all/0/1">Aviv Slobodkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Goldman_O/0/1/0/all/0/1">Omer Goldman</a>, <a href="http://arxiv.org/find/cs/1/au:+Caciularu_A/0/1/0/all/0/1">Avi Caciularu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dagan_I/0/1/0/all/0/1">Ido Dagan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1">Shauli Ravfogel</a></p>
<p>Large language models (LLMs) have been shown to possess impressive
capabilities, while also raising crucial concerns about the faithfulness of
their responses. A primary issue arising in this context is the management of
(un)answerable queries by LLMs, which often results in hallucinatory behavior
due to overconfidence. In this paper, we explore the behavior of LLMs when
presented with (un)answerable queries. We ask: do models represent the fact
that the question is (un)answerable when generating a hallucinatory answer? Our
results show strong indications that such models encode the answerability of an
input query, with the representation of the first decoded token often being a
strong indicator. These findings shed new light on the spatial organization
within the latent representations of LLMs, unveiling previously unexplored
facets of these models. Moreover, they pave the way for the development of
improved decoding techniques with better adherence to factual generation,
particularly in scenarios where query (un)answerability is a concern.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.12467">Contrastive Learning for Inference in Dialogue. (arXiv:2310.12467v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ishii_E/0/1/0/all/0/1">Etsuko Ishii</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wilie_B/0/1/0/all/0/1">Bryan Wilie</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1">Ziwei Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Lovenia_H/0/1/0/all/0/1">Holy Lovenia</a>, <a href="http://arxiv.org/find/cs/1/au:+Chung_W/0/1/0/all/0/1">Willy Chung</a>, <a href="http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1">Pascale Fung</a></p>
<p>Inference, especially those derived from inductive processes, is a crucial
component in our conversation to complement the information implicitly or
explicitly conveyed by a speaker. While recent large language models show
remarkable advances in inference tasks, their performance in inductive
reasoning, where not all information is present in the context, is far behind
deductive reasoning. In this paper, we analyze the behavior of the models based
on the task difficulty defined by the semantic information gap -- which
distinguishes inductive and deductive reasoning (Johnson-Laird, 1988, 1993).
Our analysis reveals that the disparity in information between dialogue
contexts and desired inferences poses a significant challenge to the inductive
inference process. To mitigate this information gap, we investigate a
contrastive learning approach by feeding negative samples. Our experiments
suggest negative samples help models understand what is wrong and improve their
inference generations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.14303">Language Model Unalignment: Parametric Red-Teaming to Expose Hidden Harms and Biases. (arXiv:2310.14303v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bhardwaj_R/0/1/0/all/0/1">Rishabh Bhardwaj</a>, <a href="http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1">Soujanya Poria</a></p>
<p>Red-teaming has been a widely adopted way to evaluate the harmfulness of
Large Language Models (LLMs). It aims to jailbreak a model's safety behavior to
make it act as a helpful agent disregarding the harmfulness of the query.
Existing methods are primarily based on input text-based red-teaming such as
adversarial prompts, low-resource prompts, or contextualized prompts to
condition the model in a way to bypass its safe behavior. Bypassing the
guardrails uncovers hidden harmful information and biases in the model that are
left untreated or newly introduced by its safety training. However,
prompt-based attacks fail to provide such a diagnosis owing to their low attack
success rate, and applicability to specific models. In this paper, we present a
new perspective on LLM safety research i.e., parametric red-teaming through
Unalignment. It simply (instruction) tunes the model parameters to break model
guardrails that are not deeply rooted in the model's behavior. Unalignment
using as few as 100 examples can significantly bypass commonly referred to as
CHATGPT, to the point where it responds with an 88% success rate to harmful
queries on two safety benchmark datasets. On open-source models such as
VICUNA-7B and LLAMA-2-CHAT 7B AND 13B, it shows an attack success rate of more
than 91%. On bias evaluations, Unalignment exposes inherent biases in
safety-aligned models such as CHATGPT and LLAMA- 2-CHAT where the model's
responses are strongly biased and opinionated 64% of the time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.14338">From Chaos to Clarity: Claim Normalization to Empower Fact-Checking. (arXiv:2310.14338v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sundriyal_M/0/1/0/all/0/1">Megha Sundriyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1">Tanmoy Chakraborty</a>, <a href="http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1">Preslav Nakov</a></p>
<p>With the rise of social media, users are exposed to many misleading claims.
However, the pervasive noise inherent in these posts presents a challenge in
identifying precise and prominent claims that require verification. Extracting
the important claims from such posts is arduous and time-consuming, yet it is
an underexplored problem. Here, we aim to bridge this gap. We introduce a novel
task, Claim Normalization (aka ClaimNorm), which aims to decompose complex and
noisy social media posts into more straightforward and understandable forms,
termed normalized claims. We propose CACN, a pioneering approach that leverages
chain-of-thought and claim check-worthiness estimation, mimicking human
reasoning processes, to comprehend intricate claims. Moreover, we capitalize on
the in-context learning capabilities of large language models to provide
guidance and to improve claim normalization. To evaluate the effectiveness of
our proposed model, we meticulously compile a comprehensive real-world dataset,
CLAN, comprising more than 6k instances of social media posts alongside their
respective normalized claims. Our experiments demonstrate that CACN outperforms
several baselines across various evaluation measures. Finally, our rigorous
error analysis validates CACN's capabilities and pitfalls.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.14450">TATA: Stance Detection via Topic-Agnostic and Topic-Aware Embeddings. (arXiv:2310.14450v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hanley_H/0/1/0/all/0/1">Hans W. A. Hanley</a>, <a href="http://arxiv.org/find/cs/1/au:+Durumeric_Z/0/1/0/all/0/1">Zakir Durumeric</a></p>
<p>Stance detection is important for understanding different attitudes and
beliefs on the Internet. However, given that a passage's stance toward a given
topic is often highly dependent on that topic, building a stance detection
model that generalizes to unseen topics is difficult. In this work, we propose
using contrastive learning as well as an unlabeled dataset of news articles
that cover a variety of different topics to train topic-agnostic/TAG and
topic-aware/TAW embeddings for use in downstream stance detection. Combining
these embeddings in our full TATA model, we achieve state-of-the-art
performance across several public stance detection datasets (0.771 $F_1$-score
on the Zero-shot VAST dataset). We release our code and data at
https://github.com/hanshanley/tata.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16607">On the Interplay between Fairness and Explainability. (arXiv:2310.16607v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Brandl_S/0/1/0/all/0/1">Stephanie Brandl</a>, <a href="http://arxiv.org/find/cs/1/au:+Bugliarello_E/0/1/0/all/0/1">Emanuele Bugliarello</a>, <a href="http://arxiv.org/find/cs/1/au:+Chalkidis_I/0/1/0/all/0/1">Ilias Chalkidis</a></p>
<p>In order to build reliable and trustworthy NLP applications, models need to
be both fair across different demographics and explainable. Usually these two
objectives, fairness and explainability, are optimized and/or examined
independently of each other. Instead, we argue that forthcoming, trustworthy
NLP systems should consider both. In this work, we perform a first study to
understand how they influence each other: do fair(er) models rely on more
plausible rationales? and vice versa. To this end, we conduct experiments on
two English multi-class text classification datasets, BIOS and ECtHR, that
provide information on gender and nationality, respectively, as well as
human-annotated rationales. We fine-tune pre-trained language models with
several methods for (i) bias mitigation, which aims to improve fairness; (ii)
rationale extraction, which aims to produce plausible explanations. We find
that bias mitigation algorithms do not always lead to fairer models. Moreover,
we discover that empirical fairness and explainability are orthogonal.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17688">Managing AI Risks in an Era of Rapid Progress. (arXiv:2310.17688v2 [cs.CY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1">Yoshua Bengio</a>, <a href="http://arxiv.org/find/cs/1/au:+Hinton_G/0/1/0/all/0/1">Geoffrey Hinton</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1">Andrew Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1">Dawn Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1">Pieter Abbeel</a>, <a href="http://arxiv.org/find/cs/1/au:+Harari_Y/0/1/0/all/0/1">Yuval Noah Harari</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Ya-Qin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1">Lan Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Shalev_Shwartz_S/0/1/0/all/0/1">Shai Shalev-Shwartz</a>, <a href="http://arxiv.org/find/cs/1/au:+Hadfield_G/0/1/0/all/0/1">Gillian Hadfield</a>, <a href="http://arxiv.org/find/cs/1/au:+Clune_J/0/1/0/all/0/1">Jeff Clune</a>, <a href="http://arxiv.org/find/cs/1/au:+Maharaj_T/0/1/0/all/0/1">Tegan Maharaj</a>, <a href="http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1">Frank Hutter</a>, <a href="http://arxiv.org/find/cs/1/au:+Baydin_A/0/1/0/all/0/1">At&#x131;l&#x131;m G&#xfc;ne&#x15f; Baydin</a>, <a href="http://arxiv.org/find/cs/1/au:+McIlraith_S/0/1/0/all/0/1">Sheila McIlraith</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1">Qiqi Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1">Ashwin Acharya</a>, <a href="http://arxiv.org/find/cs/1/au:+Krueger_D/0/1/0/all/0/1">David Krueger</a>, <a href="http://arxiv.org/find/cs/1/au:+Dragan_A/0/1/0/all/0/1">Anca Dragan</a>, <a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1">Philip Torr</a>, <a href="http://arxiv.org/find/cs/1/au:+Russell_S/0/1/0/all/0/1">Stuart Russell</a>, <a href="http://arxiv.org/find/cs/1/au:+Kahneman_D/0/1/0/all/0/1">Daniel Kahneman</a>, <a href="http://arxiv.org/find/cs/1/au:+Brauner_J/0/1/0/all/0/1">Jan Brauner</a>, <a href="http://arxiv.org/find/cs/1/au:+Mindermann_S/0/1/0/all/0/1">S&#xf6;ren Mindermann</a></p>
<p>In this short consensus paper, we outline risks from upcoming, advanced AI
systems. We examine large-scale social harms and malicious uses, as well as an
irreversible loss of human control over autonomous AI systems. In light of
rapid and continuing AI progress, we propose urgent priorities for AI R&amp;D and
governance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17743">StyleBART: Decorate Pretrained Model with Style Adapters for Unsupervised Stylistic Headline Generation. (arXiv:2310.17743v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hanqing Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1">Yajing Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_B/0/1/0/all/0/1">Boya Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1">Guanhua Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yun Chen</a></p>
<p>Stylistic headline generation is the task to generate a headline that not
only summarizes the content of an article, but also reflects a desired style
that attracts users. As style-specific article-headline pairs are scarce,
previous researches focus on unsupervised approaches with a standard headline
generation dataset and mono-style corpora. In this work, we follow this line
and propose StyleBART, an unsupervised approach for stylistic headline
generation. Our method decorates the pretrained BART model with adapters that
are responsible for different styles and allows the generation of headlines
with diverse styles by simply switching the adapters. Different from previous
works, StyleBART separates the task of style learning and headline generation,
making it possible to freely combine the base model and the style adapters
during inference. We further propose an inverse paraphrasing task to enhance
the style adapters. Extensive automatic and human evaluations show that
StyleBART achieves new state-of-the-art performance in the unsupervised
stylistic headline generation task, producing high-quality headlines with the
desired style.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.18122">OpinSummEval: Revisiting Automated Evaluation for Opinion Summarization. (arXiv:2310.18122v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yuchen Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1">Xiaojun Wan</a></p>
<p>Opinion summarization sets itself apart from other types of summarization
tasks due to its distinctive focus on aspects and sentiments. Although certain
automated evaluation methods like ROUGE have gained popularity, we have found
them to be unreliable measures for assessing the quality of opinion summaries.
In this paper, we present OpinSummEval, a dataset comprising human judgments
and outputs from 14 opinion summarization models. We further explore the
correlation between 24 automatic metrics and human ratings across four
dimensions. Our findings indicate that metrics based on neural networks
generally outperform non-neural ones. However, even metrics built on powerful
backbones, such as BART and GPT-3/3.5, do not consistently correlate well
across all dimensions, highlighting the need for advancements in automated
evaluation methods for opinion summarization. The code and data are publicly
available at https://github.com/A-Chicharito-S/OpinSummEval/tree/main.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.18458">Do Not Harm Protected Groups in Debiasing Language Representation Models. (arXiv:2310.18458v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1">Chloe Qinyu Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Stureborg_R/0/1/0/all/0/1">Rickard Stureborg</a>, <a href="http://arxiv.org/find/cs/1/au:+Fain_B/0/1/0/all/0/1">Brandon Fain</a></p>
<p>Language Representation Models (LRMs) trained with real-world data may
capture and exacerbate undesired bias and cause unfair treatment of people in
various demographic groups. Several techniques have been investigated for
applying interventions to LRMs to remove bias in benchmark evaluations on, for
example, word embeddings. However, the negative side effects of debiasing
interventions are usually not revealed in the downstream tasks. We propose
xGAP-DEBIAS, a set of evaluations on assessing the fairness of debiasing. In
this work, We examine four debiasing techniques on a real-world text
classification task and show that reducing biasing is at the cost of degrading
performance for all demographic groups, including those the debiasing
techniques aim to protect. We advocate that a debiasing technique should have
good downstream performance with the constraint of ensuring no harm to the
protected group.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19056">MILL: Mutual Verification with Large Language Models for Zero-Shot Query Expansion. (arXiv:2310.19056v2 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jia_P/0/1/0/all/0/1">Pengyue Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yiding Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xiangyu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaopeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hao_C/0/1/0/all/0/1">Changying Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuaiqiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1">Dawei Yin</a></p>
<p>Query expansion is a commonly-used technique in many search systems to better
represent users' information needs with additional query terms. Existing
studies for this task usually propose to expand a query with retrieved or
generated contextual documents. However, both types of methods have clear
limitations. For retrieval-based methods, the documents retrieved with the
original query might not be accurate enough to reveal the search intent,
especially when the query is brief or ambiguous. For generation-based methods,
existing models can hardly be trained or aligned on a particular corpus, due to
the lack of corpus-specific labeled data. In this paper, we propose a novel
Large Language Model (LLM) based mutual verification framework for query
expansion, which alleviates the aforementioned limitations. Specifically, we
first design a query-query-document generation pipeline, which can effectively
leverage the contextual knowledge encoded in LLMs to generate sub-queries and
corresponding documents from multiple perspectives. Next, we employ a mutual
verification method for both generated and retrieved contextual documents,
where 1) retrieved documents are filtered with the external contextual
knowledge in generated documents, and 2) generated documents are filtered with
the corpus-specific knowledge in retrieved documents. Overall, the proposed
method allows retrieved and generated documents to complement each other to
finalize a better query expansion. We conduct extensive experiments on three
information retrieval datasets, i.e., TREC-DL-2020, TREC-COVID, and MSMARCO.
The results demonstrate that our method outperforms other baselines
significantly.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19680">Integrating Pre-trained Language Model into Neural Machine Translation. (arXiv:2310.19680v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1">Soon-Jae Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeong_C/0/1/0/all/0/1">Chang-Sung Jeong</a></p>
<p>Neural Machine Translation (NMT) has become a significant technology in
natural language processing through extensive research and development.
However, the deficiency of high-quality bilingual language pair data still
poses a major challenge to improving NMT performance. Recent studies are
exploring the use of contextual information from pre-trained language model
(PLM) to address this problem. Yet, the issue of incompatibility between PLM
and NMT model remains unresolved. This study proposes a PLM-integrated NMT
(PiNMT) model to overcome the identified problems. The PiNMT model consists of
three critical components, PLM Multi Layer Converter, Embedding Fusion, and
Cosine Alignment, each playing a vital role in providing effective PLM
information to NMT. Furthermore, two training strategies, Separate Learning
Rates and Dual Step Training, are also introduced in this paper. By
implementing the proposed PiNMT model and training strategy, we achieved
state-of-the-art performance on the IWSLT'14 En$\leftrightarrow$De dataset.
This study's outcomes are noteworthy as they demonstrate a novel approach for
efficiently integrating PLM with NMT to overcome incompatibility and enhance
performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01012">COPAL-ID: Indonesian Language Reasoning with Local Culture and Nuances. (arXiv:2311.01012v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wibowo_H/0/1/0/all/0/1">Haryo Akbarianto Wibowo</a>, <a href="http://arxiv.org/find/cs/1/au:+Fuadi_E/0/1/0/all/0/1">Erland Hilman Fuadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nityasya_M/0/1/0/all/0/1">Made Nindyatama Nityasya</a>, <a href="http://arxiv.org/find/cs/1/au:+Prasojo_R/0/1/0/all/0/1">Radityo Eko Prasojo</a>, <a href="http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1">Alham Fikri Aji</a></p>
<p>We present publicly available COPAL-ID, a novel Indonesian language common
sense reasoning dataset. Unlike the previous Indonesian COPA dataset
(XCOPA-ID), COPAL-ID incorporates Indonesian local and cultural nuances, and
therefore, provides a more natural portrayal of day-to-day causal reasoning
within the Indonesian cultural sphere. Professionally written by natives from
scratch, COPAL-ID is more fluent and free from awkward phrases, unlike the
translated XCOPA-ID. In addition, we present COPAL-ID in both standard
Indonesian and in Jakartan Indonesian--a dialect commonly used in daily
conversation. COPAL-ID poses a greater challenge for existing open-sourced and
closed state-of-the-art multilingual language models, yet is trivially easy for
humans. Our findings suggest that even the current best open-source,
multilingual model struggles to perform well, achieving 65.47% accuracy on
COPAL-ID, significantly lower than on the culturally-devoid XCOPA-ID (79.40%).
Despite GPT-4's impressive score, it suffers the same performance degradation
compared to its XCOPA-ID score, and it still falls short of human performance.
This shows that these language models are still way behind in comprehending the
local nuances of Indonesian.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01305">AWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language Models. (arXiv:2311.01305v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Baisong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xingwang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Haixiao Xu</a></p>
<p>Large language models(LLMs) exhibit excellent performance across a variety of
tasks, but they come with significant computational and storage costs.
Quantizing these models is an effective way to alleviate this issue. However,
existing methods struggle to strike a balance between model accuracy and
hardware efficiency. This is where we introduce AWEQ, a post-training method
that requires no additional training overhead. AWEQ excels in both
ultra-low-bit quantization and 8-bit weight and activation (W8A8) quantization.
There is an observation that weight quantization is less challenging than
activation quantization. AWEQ transfers the difficulty of activation
quantization to weights using channel equalization, achieving a balance between
the quantization difficulties of both, and thereby maximizing performance. We
have further refined the equalization method to mitigate quantization bias
error, ensuring the robustness of the model. Extensive experiments on popular
models such as LLaMA and OPT demonstrate that AWEQ outperforms all existing
post-training quantization methods for large models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01544">Divergent Token Metrics: Measuring degradation to prune away LLM components -- and optimize quantization. (arXiv:2311.01544v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Deiseroth_B/0/1/0/all/0/1">Bj&#xf6;rn Deiseroth</a>, <a href="http://arxiv.org/find/cs/1/au:+Meuer_M/0/1/0/all/0/1">Max Meuer</a>, <a href="http://arxiv.org/find/cs/1/au:+Gritsch_N/0/1/0/all/0/1">Nikolas Gritsch</a>, <a href="http://arxiv.org/find/cs/1/au:+Eichenberg_C/0/1/0/all/0/1">Constantin Eichenberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1">Patrick Schramowski</a>, <a href="http://arxiv.org/find/cs/1/au:+Assenmacher_M/0/1/0/all/0/1">Matthias A&#xdf;enmacher</a>, <a href="http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1">Kristian Kersting</a></p>
<p>Large Language Models (LLMs) have reshaped natural language processing with
their impressive capabilities. Their ever-increasing size, however, raised
concerns about their effective deployment and the need for LLM compressions.
This study introduces the Divergent Token metrics (DTMs), a novel approach for
assessing compressed LLMs, addressing the limitations of traditional perplexity
or accuracy measures that fail to accurately reflect text generation quality.
DTMs focus on token divergence, that allow deeper insights into the subtleties
of model compression, i.p. when evaluating component's impacts individually.
Utilizing the First Divergent Token metric (FDTM) in model sparsification
reveals that a quarter of all attention components can be pruned beyond 90% on
the Llama-2 model family, still keeping SOTA performance. For quantization FDTM
suggests that over 80% of parameters can naively be transformed to int8 without
special outlier management. These evaluations indicate the necessity of
choosing appropriate compressions for parameters individually-and that FDTM can
identify those-while standard metrics result in deteriorated outcomes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01732">Proto-lm: A Prototypical Network-Based Framework for Built-in Interpretability in Large Language Models. (arXiv:2311.01732v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1">Sean Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1">Soroush Vosoughi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hassanpour_S/0/1/0/all/0/1">Saeed Hassanpour</a></p>
<p>Large Language Models (LLMs) have significantly advanced the field of Natural
Language Processing (NLP), but their lack of interpretability has been a major
concern. Current methods for interpreting LLMs are post hoc, applied after
inference time, and have limitations such as their focus on low-level features
and lack of explainability at higher level text units. In this work, we
introduce proto-lm, a prototypical network-based white-box framework that
allows LLMs to learn immediately interpretable embeddings during the
fine-tuning stage while maintaining competitive performance. Our method's
applicability and interpretability are demonstrated through experiments on a
wide range of NLP tasks, and our results indicate a new possibility of creating
interpretable models without sacrificing performance. This novel approach to
interpretability in LLMs can pave the way for more interpretable models without
the need to sacrifice performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02408">Citance-Contextualized Summarization of Scientific Papers. (arXiv:2311.02408v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Syed_S/0/1/0/all/0/1">Shahbaz Syed</a>, <a href="http://arxiv.org/find/cs/1/au:+Hakimi_A/0/1/0/all/0/1">Ahmad Dawar Hakimi</a>, <a href="http://arxiv.org/find/cs/1/au:+Al_Khatib_K/0/1/0/all/0/1">Khalid Al-Khatib</a>, <a href="http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1">Martin Potthast</a></p>
<p>Current approaches to automatic summarization of scientific papers generate
informative summaries in the form of abstracts. However, abstracts are not
intended to show the relationship between a paper and the references cited in
it. We propose a new contextualized summarization approach that can generate an
informative summary conditioned on a given sentence containing the citation of
a reference (a so-called "citance"). This summary outlines the content of the
cited paper relevant to the citation location. Thus, our approach extracts and
models the citances of a paper, retrieves relevant passages from cited papers,
and generates abstractive summaries tailored to each citance. We evaluate our
approach using $\textbf{Webis-Context-SciSumm-2023}$, a new dataset containing
540K~computer science papers and 4.6M~citances therein.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02775">ChaTA: Towards an Intelligent Question-Answer Teaching Assistant using Open-Source LLMs. (arXiv:2311.02775v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hicke_Y/0/1/0/all/0/1">Yann Hicke</a>, <a href="http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1">Anmol Agarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1">Qianou Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Denny_P/0/1/0/all/0/1">Paul Denny</a></p>
<p>Responding to the thousands of student questions on online QA platforms each
semester has a considerable human cost, particularly in computing courses with
rapidly growing enrollments. To address the challenges of scalable and
intelligent question-answering (QA), we introduce an innovative solution that
leverages open-source Large Language Models (LLMs) from the LLaMA-2 family to
ensure data privacy. Our approach combines augmentation techniques such as
retrieval augmented generation (RAG), supervised fine-tuning (SFT), and
learning from human preferences data using Direct Preference Optimization
(DPO). Through extensive experimentation on a Piazza dataset from an
introductory CS course, comprising 10,000 QA pairs and 1,500 pairs of
preference data, we demonstrate a significant 30% improvement in the quality of
answers, with RAG being a particularly impactful addition. Our contributions
include the development of a novel architecture for educational QA, extensive
evaluations of LLM performance utilizing both human assessments and LLM-based
metrics, and insights into the challenges and future directions of educational
data processing. This work paves the way for the development of CHATA, an
intelligent QA assistant customizable for courses with an online QA platform
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04498">NExT-Chat: An LMM for Chat, Detection and Segmentation. (arXiv:2311.04498v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1">Ao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1">Wei Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1">Tat-Seng Chua</a></p>
<p>The development of large language models (LLMs) has greatly advanced the
field of multimodal understanding, leading to the emergence of large multimodal
models (LMMs). In order to enhance the level of visual comprehension, recent
studies have equipped LMMs with region-level understanding capabilities by
representing object bounding box coordinates as a series of text sequences
(pixel2seq). In this paper, we introduce a novel paradigm for object location
modeling called pixel2emb method, where we ask the LMM to output the location
embeddings and then decoded by different decoders. This paradigm allows for
different location formats (such as bounding boxes and masks) to be used in
multimodal conversations Furthermore, this kind of embedding based location
modeling enables the utilization of existing practices in localization tasks,
such as detection and segmentation. In scenarios with limited resources, our
pixel2emb demonstrates superior performance compared to existing
state-of-the-art (SOTA) approaches in both the location input and output tasks
under fair comparison. Leveraging the proposed pixel2emb method, we train an
LMM named NExT-Chat and demonstrate its capability of handling multiple tasks
like visual grounding, region caption, and grounded reasoning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04850">Rethinking Benchmark and Contamination for Language Models with Rephrased Samples. (arXiv:2311.04850v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Shuo Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chiang_W/0/1/0/all/0/1">Wei-Lin Chiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1">Lianmin Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1">Joseph E. Gonzalez</a>, <a href="http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1">Ion Stoica</a></p>
<p>Large language models are increasingly trained on all the data ever produced
by humans. Many have raised concerns about the trustworthiness of public
benchmarks due to potential contamination in pre-training or fine-tuning
datasets. While most data decontamination efforts apply string matching (e.g.,
n-gram overlap) to remove benchmark data, we show that these methods are
insufficient, and simple variations of test data (e.g., paraphrasing,
translation) can easily bypass these decontamination measures. Furthermore, we
demonstrate that if such variation of test data is not eliminated, a 13B model
can easily overfit a test benchmark and achieve drastically high performance,
on par with GPT-4. We validate such observations in widely used benchmarks such
as MMLU, GSK8k, and HumanEval. To address this growing risk, we propose a
stronger LLM-based decontamination method and apply it to widely used
pre-training and fine-tuning datasets, revealing significant previously unknown
test overlap. For example, in pre-training sets such as RedPajama-Data-1T and
StarCoder-Data, we identified that 8-18\% of the HumanEval benchmark overlaps.
Interestingly, we also find such contamination in synthetic dataset generated
by GPT-3.5/4, suggesting a potential risk of unintentional contamination. We
urge the community to adopt stronger decontamination approaches when using
public benchmarks. Moreover, we call for the community to actively develop
fresh one-time exams to evaluate models accurately. Our decontamination tool is
publicly available at https://github.com/lm-sys/llm-decontaminator.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04913">An Improved Transformer-based Model for Detecting Phishing, Spam, and Ham: A Large Language Model Approach. (arXiv:2311.04913v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jamal_S/0/1/0/all/0/1">Suhaima Jamal</a>, <a href="http://arxiv.org/find/cs/1/au:+Wimmer_H/0/1/0/all/0/1">Hayden Wimmer</a></p>
<p>Phishing and spam detection is long standing challenge that has been the
subject of much academic research. Large Language Models (LLM) have vast
potential to transform society and provide new and innovative approaches to
solve well-established challenges. Phishing and spam have caused financial
hardships and lost time and resources to email users all over the world and
frequently serve as an entry point for ransomware threat actors. While
detection approaches exist, especially heuristic-based approaches, LLMs offer
the potential to venture into a new unexplored area for understanding and
solving this challenge. LLMs have rapidly altered the landscape from business,
consumers, and throughout academia and demonstrate transformational potential
for the potential of society. Based on this, applying these new and innovative
approaches to email detection is a rational next step in academic research. In
this work, we present IPSDM, our model based on fine-tuning the BERT family of
models to specifically detect phishing and spam email. We demonstrate our
fine-tuned version, IPSDM, is able to better classify emails in both unbalanced
and balanced datasets. This work serves as an important first step towards
employing LLMs to improve the security of our information systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05553">Removing RLHF Protections in GPT-4 via Fine-Tuning. (arXiv:2311.05553v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhan_Q/0/1/0/all/0/1">Qiusi Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_R/0/1/0/all/0/1">Richard Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bindu_R/0/1/0/all/0/1">Rohan Bindu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1">Akul Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1">Tatsunori Hashimoto</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1">Daniel Kang</a></p>
<p>As large language models (LLMs) have increased in their capabilities, so does
their potential for dual use. To reduce harmful outputs, produces and vendors
of LLMs have used reinforcement learning with human feedback (RLHF). In tandem,
LLM vendors have been increasingly enabling fine-tuning of their most powerful
models. However, concurrent work has shown that fine-tuning can remove RLHF
protections. We may expect that the most powerful models currently available
(GPT-4) are less susceptible to fine-tuning attacks.
</p>
<p>In this work, we show the contrary: fine-tuning allows attackers to remove
RLHF protections with as few as 340 examples and a 95% success rate. These
training examples can be automatically generated with weaker models. We further
show that removing RLHF protections does not decrease usefulness on
non-censored outputs, providing evidence that our fine-tuning strategy does not
decrease usefulness despite using weaker models to generate training data. Our
results show the need for further research on protections on LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06237">Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming in the Wild. (arXiv:2311.06237v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Inie_N/0/1/0/all/0/1">Nanna Inie</a>, <a href="http://arxiv.org/find/cs/1/au:+Stray_J/0/1/0/all/0/1">Jonathan Stray</a>, <a href="http://arxiv.org/find/cs/1/au:+Derczynski_L/0/1/0/all/0/1">Leon Derczynski</a></p>
<p>Engaging in the deliberate generation of abnormal outputs from large language
models (LLMs) by attacking them is a novel human activity. This paper presents
a thorough exposition of how and why people perform such attacks. Using a
formal qualitative methodology, we interviewed dozens of practitioners from a
broad range of backgrounds, all contributors to this novel work of attempting
to cause LLMs to fail. We relate and connect this activity between its
practitioners' motivations and goals; the strategies and techniques they
deploy; and the crucial role the community plays. As a result, this paper
presents a grounded theory of how and why people attack large language models:
LLM red teaming in the wild.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.08493">Time Travel in LLMs: Tracing Data Contamination in Large Language Models. (arXiv:2308.08493v2 [cs.CL] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Golchin_S/0/1/0/all/0/1">Shahriar Golchin</a>, <a href="http://arxiv.org/find/cs/1/au:+Surdeanu_M/0/1/0/all/0/1">Mihai Surdeanu</a></p>
<p>Data contamination, i.e., the presence of test data from downstream tasks in
the training data of large language models (LLMs), is a potential major issue
in measuring LLMs' real effectiveness on other tasks. We propose a
straightforward yet effective method for identifying data contamination within
LLMs. At its core, our approach starts by identifying potential contamination
at the instance level; using this information, our approach then assesses wider
contamination at the partition level. To estimate contamination of individual
instances, we employ "guided instruction:" a prompt consisting of the dataset
name, partition type, and the random-length initial segment of a reference
instance, asking the LLM to complete it. An instance is flagged as contaminated
if the LLM's output either exactly or nearly matches the latter segment of the
reference. To understand if an entire partition is contaminated, we propose two
ideas. The first idea marks a dataset partition as contaminated if the average
overlap score with the reference instances (as measured by ROUGE-L or BLEURT)
is statistically significantly better with the completions from guided
instruction compared to a "general instruction" that does not include the
dataset and partition name. The second idea marks a dataset partition as
contaminated if a classifier based on GPT-4 with few-shot in-context learning
prompt marks multiple generated completions as exact/near-exact matches of the
corresponding reference instances. Our best method achieves an accuracy between
92% and 100% in detecting if an LLM is contaminated with seven datasets,
containing train and test/validation partitions, when contrasted with manual
evaluation by human experts. Further, our findings indicate that GPT-4 is
contaminated with AG News, WNLI, and XSum datasets.
</p>
</p>
</div>

    </div>
    </body>
    