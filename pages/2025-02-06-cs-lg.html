<!DOCTYPE html>
<html>
<head>
<title>2025-02-06-cs-lg</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    .copy-button {
        background-color: #007BFF;
        color: #fff;
        border: none;
        padding: 5px 10px;
        cursor: pointer;
        margin-top: 10px;
    }
    .copy-button:active {
        background-color: #0056b3;
    }
    .copy-message {
        display: none;
        background-color: #4CAF50;
        color: #fff;
        padding: 5px;
        margin-top: 10px;
        border-radius: 3px;
    }
    </style>
    <script>
    function copyToClipboard(text, buttonId) {
        const el = document.createElement('textarea');
        el.value = text;
        document.body.appendChild(el);
        el.select();
        document.execCommand('copy');
        document.body.removeChild(el);
        
        const messageElement = document.getElementById('copy-message-' + buttonId);
        messageElement.innerHTML = 'Copied "' + text + '" to clipboard';
        messageElement.style.display = 'block';
        setTimeout(function() {
            messageElement.style.display = 'none';
        }, 3000);
    }
    </script>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02617">PolarQuant: Quantizing KV Caches with Polar Transformation</a></h1>
<p><b>Authors:</b> Insu Han, Praneeth Kacham, Amin Karbasi, Vahab Mirrokni, Amir Zandieh</p>
<p>Abstract: Large language models (LLMs) require significant memory to store Key-Value (KV) embeddings in their KV cache, especially when handling long-range contexts. Quantization of these KV embeddings is a common technique to reduce memory consumption. This work introduces PolarQuant, a novel quantization method employing random preconditioning and polar transformation. Our method transforms the KV embeddings into polar coordinates using an efficient recursive algorithm and then quantizes resulting angles. Our key insight is that, after random preconditioning, the angles in the polar representation exhibit a tightly bounded and highly concentrated distribution with an analytically computable form. This nice distribution eliminates the need for explicit normalization, a step required by traditional quantization methods which introduces significant memory overhead because quantization parameters (e.g., zero point and scale) must be stored in full precision per each data block. PolarQuant bypasses this normalization step, enabling substantial memory savings. The long-context evaluation demonstrates that PolarQuant compresses the KV cache by over x4.2 while achieving the best quality scores compared to the state-of-the-art methods.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02617', 0)">Copy Link</button>
<div id="copy-message-0" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02623">Sample Complexity of Bias Detection with Subsampled Point-to-Subspace Distances</a></h1>
<p><b>Authors:</b> German Martinez Matilla, Jakub Marecek</p>
<p>Abstract: Sample complexity of bias estimation is a lower bound on the runtime of any bias detection method. Many regulatory frameworks require the bias to be tested for all subgroups, whose number grows exponentially with the number of protected attributes. Unless one wishes to run a bias detection with a doubly-exponential run-time, one should like to have polynomial complexity of bias detection for a single subgroup. At the same time, the reference data may be based on surveys, and thus come with non-trivial uncertainty.
  Here, we reformulate bias detection as a point-to-subspace problem on the space of measures and show that, for supremum norm, it can be subsampled efficiently. In particular, our probabilistically approximately correct (PAC) results are corroborated by tests on well-known instances.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02623', 1)">Copy Link</button>
<div id="copy-message-1" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02625">Bayesian Parameter Shift Rule in Variational Quantum Eigensolvers</a></h1>
<p><b>Authors:</b> Samuele Pedrielli, Christopher J. Anders, Lena Funcke, Karl Jansen, Kim A. Nicoli, Shinichi Nakajima</p>
<p>Abstract: Parameter shift rules (PSRs) are key techniques for efficient gradient estimation in variational quantum eigensolvers (VQEs). In this paper, we propose its Bayesian variant, where Gaussian processes with appropriate kernels are used to estimate the gradient of the VQE objective. Our Bayesian PSR offers flexible gradient estimation from observations at arbitrary locations with uncertainty information and reduces to the generalized PSR in special cases. In stochastic gradient descent (SGD), the flexibility of Bayesian PSR allows the reuse of observations in previous steps, which accelerates the optimization process. Furthermore, the accessibility to the posterior uncertainty, along with our proposed notion of gradient confident region (GradCoRe), enables us to minimize the observation costs in each SGD step. Our numerical experiments show that the VQE optimization with Bayesian PSR and GradCoRe significantly accelerates SGD and outperforms the state-of-the-art methods, including sequential minimal optimization.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02625', 2)">Copy Link</button>
<div id="copy-message-2" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02628">e-SimFT: Alignment of Generative Models with Simulation Feedback for Pareto-Front Design Exploration</a></h1>
<p><b>Authors:</b> Hyunmin Cheong, Mohammadmehdi Ataei, Amir Hosein Khasahmadi, Pradeep Kumar Jayaraman</p>
<p>Abstract: Deep generative models have recently shown success in solving complex engineering design problems where models predict solutions that address the design requirements specified as input. However, there remains a challenge in aligning such models for effective design exploration. For many design problems, finding a solution that meets all the requirements is infeasible. In such a case, engineers prefer to obtain a set of Pareto optimal solutions with respect to those requirements, but uniform sampling of generative models may not yield a useful Pareto front. To address this gap, we introduce a new framework for Pareto-front design exploration with simulation fine-tuned generative models. First, the framework adopts preference alignment methods developed for Large Language Models (LLMs) and showcases the first application in fine-tuning a generative model for engineering design. The important distinction here is that we use a simulator instead of humans to provide accurate and scalable feedback. Next, we propose epsilon-sampling, inspired by the epsilon-constraint method used for Pareto-front generation with classical optimization algorithms, to construct a high-quality Pareto front with the fine-tuned models. Our framework, named e-SimFT, is shown to produce better-quality Pareto fronts than existing multi-objective alignment methods.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02628', 3)">Copy Link</button>
<div id="copy-message-3" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02631">ParetoQ: Scaling Laws in Extremely Low-bit LLM Quantization</a></h1>
<p><b>Authors:</b> Zechun Liu, Changsheng Zhao, Hanxian Huang, Sijia Chen, Jing Zhang, Jiawei Zhao, Scott Roy, Lisa Jin, Yunyang Xiong, Yangyang Shi, Lin Xiao, Yuandong Tian, Bilge Soran, Raghuraman Krishnamoorthi, Tijmen Blankevoort, Vikas Chandra</p>
<p>Abstract: The optimal bit-width for achieving the best trade-off between quantized model size and accuracy has been a subject of ongoing debate. While some advocate for 4-bit quantization, others propose that 1.58-bit offers superior results. However, the lack of a cohesive framework for different bits has left such conclusions relatively tenuous. We present ParetoQ, the first unified framework that facilitates rigorous comparisons across 1-bit, 1.58-bit, 2-bit, 3-bit, and 4-bit quantization settings. Our findings reveal a notable learning transition between 2 and 3 bits: For 3-bits and above, the fine-tuned models stay close to their original pre-trained distributions, whereas for learning 2-bit networks or below, the representations change drastically. By optimizing training schemes and refining quantization functions, ParetoQ surpasses all previous methods tailored to specific bit widths. Remarkably, our ParetoQ ternary 600M-parameter model even outperforms the previous SoTA ternary 3B-parameter model in accuracy, using only one-fifth of the parameters. Extensive experimentation shows that ternary, 2-bit, and 3-bit quantization maintains comparable performance in the size-accuracy trade-off and generally exceeds 4-bit and binary quantization. Considering hardware constraints, 2-bit quantization offers promising potential for memory reduction and speedup.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02631', 4)">Copy Link</button>
<div id="copy-message-4" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02668">Recovering Imbalanced Clusters via Gradient-Based Projection Pursuit</a></h1>
<p><b>Authors:</b> Martin Eppert, Satyaki Mukherjee, Debarghya Ghoshdastidar</p>
<p>Abstract: Projection Pursuit is a classic exploratory technique for finding interesting projections of a dataset. We propose a method for recovering projections containing either Imbalanced Clusters or a Bernoulli-Rademacher distribution using a gradient-based technique to optimize the projection index. As sample complexity is a major limiting factor in Projection Pursuit, we analyze our algorithm's sample complexity within a Planted Vector setting where we can observe that Imbalanced Clusters can be recovered more easily than balanced ones. Additionally, we give a generalized result that works for a variety of data distributions and projection indices. We compare these results to computational lower bounds in the Low-Degree-Polynomial Framework. Finally, we experimentally evaluate our method's applicability to real-world data using FashionMNIST and the Human Activity Recognition Dataset, where our algorithm outperforms others when only a few samples are available.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02668', 5)">Copy Link</button>
<div id="copy-message-5" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02671">On Teacher Hacking in Language Model Distillation</a></h1>
<p><b>Authors:</b> Daniil Tiapkin, Daniele Calandriello, Johan Ferret, Sarah Perrin, Nino Vieillard, Alexandre Ram\'e, Mathieu Blondel</p>
<p>Abstract: Post-training of language models (LMs) increasingly relies on the following two stages: (i) knowledge distillation, where the LM is trained to imitate a larger teacher LM, and (ii) reinforcement learning from human feedback (RLHF), where the LM is aligned by optimizing a reward model. In the second RLHF stage, a well-known challenge is reward hacking, where the LM over-optimizes the reward model. Such phenomenon is in line with Goodhart's law and can lead to degraded performance on the true objective. In this paper, we investigate whether a similar phenomenon, that we call teacher hacking, can occur during knowledge distillation. This could arise because the teacher LM is itself an imperfect approximation of the true distribution. To study this, we propose a controlled experimental setup involving: (i) an oracle LM representing the ground-truth distribution, (ii) a teacher LM distilled from the oracle, and (iii) a student LM distilled from the teacher. Our experiments reveal the following insights. When using a fixed offline dataset for distillation, teacher hacking occurs; moreover, we can detect it by observing when the optimization process deviates from polynomial convergence laws. In contrast, employing online data generation techniques effectively mitigates teacher hacking. More precisely, we identify data diversity as the key factor in preventing hacking. Overall, our findings provide a deeper understanding of the benefits and limitations of distillation for building robust and efficient LMs.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02671', 6)">Copy Link</button>
<div id="copy-message-6" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02673">MedRAX: Medical Reasoning Agent for Chest X-ray</a></h1>
<p><b>Authors:</b> Adibvafa Fallahpour, Jun Ma, Alif Munim, Hongwei Lyu, Bo Wang</p>
<p>Abstract: Chest X-rays (CXRs) play an integral role in driving critical decisions in disease management and patient care. While recent innovations have led to specialized models for various CXR interpretation tasks, these solutions often operate in isolation, limiting their practical utility in clinical practice. We present MedRAX, the first versatile AI agent that seamlessly integrates state-of-the-art CXR analysis tools and multimodal large language models into a unified framework. MedRAX dynamically leverages these models to address complex medical queries without requiring additional training. To rigorously evaluate its capabilities, we introduce ChestAgentBench, a comprehensive benchmark containing 2,500 complex medical queries across 7 diverse categories. Our experiments demonstrate that MedRAX achieves state-of-the-art performance compared to both open-source and proprietary models, representing a significant step toward the practical deployment of automated CXR interpretation systems. Data and code have been publicly available at https://github.com/bowang-lab/MedRAX</p>
<p>URLs: <a href="https://github.com/bowang-lab/MedRAX">https://github.com/bowang-lab/MedRAX</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02673, https://github.com/bowang-lab/MedRAX', 7)">Copy Link</button>
<div id="copy-message-7" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02682">Pseudo-Physics-Informed Neural Operators: Enhancing Operator Learning from Limited Data</a></h1>
<p><b>Authors:</b> Keyan Chen, Yile Li, Da Long, Zhitong Xu, Wei Xing, Jacob Hochhalter, Shandian Zhe</p>
<p>Abstract: Neural operators have shown great potential in surrogate modeling. However, training a well-performing neural operator typically requires a substantial amount of data, which can pose a major challenge in complex applications. In such scenarios, detailed physical knowledge can be unavailable or difficult to obtain, and collecting extensive data is often prohibitively expensive. To mitigate this challenge, we propose the Pseudo Physics-Informed Neural Operator (PPI-NO) framework. PPI-NO constructs a surrogate physics system for the target system using partial differential equations (PDEs) derived from simple, rudimentary physics principles, such as basic differential operators. This surrogate system is coupled with a neural operator model, using an alternating update and learning process to iteratively enhance the model's predictive power. While the physics derived via PPI-NO may not mirror the ground-truth underlying physical laws -- hence the term ``pseudo physics'' -- this approach significantly improves the accuracy of standard operator learning models in data-scarce scenarios, which is evidenced by extensive evaluations across five benchmark tasks and a fatigue modeling application.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02682', 8)">Copy Link</button>
<div id="copy-message-8" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02700">Scalable Higher Resolution Polar Sea Ice Classification and Freeboard Calculation from ICESat-2 ATL03 Data</a></h1>
<p><b>Authors:</b> Jurdana Masuma Iqrah, Younghyun Koo, Wei Wang, Hongjie Xie, Sushil K. Prasad</p>
<p>Abstract: ICESat-2 (IS2) by NASA is an Earth-observing satellite that measures high-resolution surface elevation. The IS2's ATL07 and ATL10 sea ice elevation and freeboard products of 10m-200m segments which aggregated 150 signal photons from the raw ATL03 (geolocated photon) data. These aggregated products can potentially overestimate local sea surface height, thus underestimating the calculations of freeboard (sea ice height above sea surface). To achieve a higher resolution of sea surface height and freeboard information, in this work we utilize a 2m window to resample the ATL03 data. Then, we classify these 2m segments into thick sea ice, thin ice, and open water using deep learning methods (Long short-term memory and Multi-layer perceptron models). To obtain labeled training data for our deep learning models, we use segmented Sentinel-2 (S2) multi-spectral imagery overlapping with IS2 tracks in space and time to auto-label IS2 data, followed by some manual corrections in the regions of transition between different ice/water types or cloudy regions. We employ a parallel workflow for this auto-labeling using PySpark to scale, and we achieve 9-fold data loading and 16.25-fold map-reduce speedup. To train our models, we employ a Horovod-based distributed deep-learning workflow on a DGX A100 8 GPU cluster, achieving a 7.25-fold speedup. Next, we calculate the local sea surface heights based on the open water segments. Finally, we scale the freeboard calculation using the derived local sea level and achieve 8.54-fold data loading and 15.7-fold map-reduce speedup. Compared with the ATL07 (local sea level) and ATL10 (freeboard) data products, our results show higher resolutions and accuracy (96.56%).</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02700', 9)">Copy Link</button>
<div id="copy-message-9" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02701">Practically Effective Adjustment Variable Selection in Causal Inference</a></h1>
<p><b>Authors:</b> Atsushi Noda, Takashi Isozaki</p>
<p>Abstract: In the estimation of causal effects, one common method for removing the influence of confounders is to adjust the variables that satisfy the back-door criterion. However, it is not always possible to uniquely determine sets of such variables. Moreover, real-world data is almost always limited, which means it may be insufficient for statistical estimation. Therefore, we propose criteria for selecting variables from a list of candidate adjustment variables along with an algorithm to prevent accuracy degradation in causal effect estimation. We initially focus on directed acyclic graphs (DAGs) and then outlines specific steps for applying this method to completed partially directed acyclic graphs (CPDAGs). We also present and prove a theorem on causal effect computation possibility in CPDAGs. Finally, we demonstrate the practical utility of our method using both existing and artificial data.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02701', 10)">Copy Link</button>
<div id="copy-message-10" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02716">A Unified Understanding and Evaluation of Steering Methods</a></h1>
<p><b>Authors:</b> Shawn Im, Yixuan Li</p>
<p>Abstract: Steering methods provide a practical approach to controlling large language models by applying steering vectors to intermediate activations, guiding outputs toward desired behaviors while avoiding retraining. Despite their growing importance, the field lacks a unified understanding and consistent evaluation across tasks and datasets, hindering progress. This paper introduces a unified framework for analyzing and evaluating steering methods, formalizing their core principles and offering theoretical insights into their effectiveness. Through comprehensive empirical evaluations on multiple-choice and open-ended text generation tasks, we validate these insights, identifying key factors that influence performance and demonstrating the superiority of certain methods. Our work bridges theoretical and practical perspectives, offering actionable guidance for advancing the design, optimization, and deployment of steering methods in LLMs.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02716', 11)">Copy Link</button>
<div id="copy-message-11" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02719">Beyond Topological Self-Explainable GNNs: A Formal Explainability Perspective</a></h1>
<p><b>Authors:</b> Steve Azzolin, Sagar Malhotra, Andrea Passerini, Stefano Teso</p>
<p>Abstract: Self-Explainable Graph Neural Networks (SE-GNNs) are popular explainable-by-design GNNs, but the properties and the limitations of their explanations are not well understood. Our first contribution fills this gap by formalizing the explanations extracted by SE-GNNs, referred to as Trivial Explanations (TEs), and comparing them to established notions of explanations, namely Prime Implicant (PI) and faithful explanations. Our analysis reveals that TEs match PI explanations for a restricted but significant family of tasks. In general, however, they can be less informative than PI explanations and are surprisingly misaligned with widely accepted notions of faithfulness. Although faithful and PI explanations are informative, they are intractable to find and we show that they can be prohibitively large. Motivated by this, we propose Dual-Channel GNNs that integrate a white-box rule extractor and a standard SE-GNN, adaptively combining both channels when the task benefits. Our experiments show that even a simple instantiation of Dual-Channel GNNs can recover succinct rules and perform on par or better than widely used SE-GNNs. Our code can be found in the supplementary material.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02719', 12)">Copy Link</button>
<div id="copy-message-12" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02723">Dobi-SVD: Differentiable SVD for LLM Compression and Some New Perspectives</a></h1>
<p><b>Authors:</b> Qinsi Wang, Jinghan Ke, Masayoshi Tomizuka, Yiran Chen, Kurt Keutzer, Chenfeng Xu</p>
<p>Abstract: We provide a new LLM-compression solution via SVD, unlocking new possibilities for LLM compression beyond quantization and pruning. We point out that the optimal use of SVD lies in truncating activations, rather than merely using activations as an optimization distance. Building on this principle, we address three critical challenges in SVD-based LLM compression: including (1) How can we determine the optimal activation truncation position for each weight matrix in LLMs? (2) How can we efficiently reconstruct the weight matrices based on truncated activations? (3) How can we address the inherent "injection" nature that results in the information loss of the SVD? We propose Dobi-SVD, which establishes a new, principled approach to SVD-based LLM compression.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02723', 13)">Copy Link</button>
<div id="copy-message-13" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02727">Parameter Tracking in Federated Learning with Adaptive Optimization</a></h1>
<p><b>Authors:</b> Evan Chen. Jianing Zhang, Shiqiang Wang, Chaoyue Liu, Christopher Brinton</p>
<p>Abstract: In Federated Learning (FL), model training performance is strongly impacted by data heterogeneity across clients. Gradient Tracking (GT) has recently emerged as a solution which mitigates this issue by introducing correction terms to local model updates. To date, GT has only been considered under Stochastic Gradient Descent (SGD)-based model training, while modern FL frameworks increasingly employ adaptive optimizers for improved convergence. In this work, we generalize the GT framework to a more flexible Parameter Tracking (PT) paradigm and propose two novel adaptive optimization algorithms, {\tt FAdamET} and {\tt FAdamGT}, that integrate PT into Adam-based FL. We provide a rigorous convergence analysis of these algorithms under non-convex settings. Our experimental results demonstrate that both proposed algorithms consistently outperform existing methods when evaluating total communication cost and total computation cost across varying levels of data heterogeneity, showing the effectiveness of correcting first-order information in federated adaptive optimization.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02727', 14)">Copy Link</button>
<div id="copy-message-14" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02732">Peri-LN: Revisiting Layer Normalization in the Transformer Architecture</a></h1>
<p><b>Authors:</b> Jeonghoon Kim, Byeongchan Lee, Cheonbok Park, Yeontaek Oh, Beomjun Kim, Taehwan Yoo, Seongjin Shin, Dongyoon Han, Jinwoo Shin, Kang Min Yoo</p>
<p>Abstract: Designing Transformer architectures with the optimal layer normalization (LN) strategy that ensures large-scale training stability and expedite convergence has remained elusive, even in this era of large language models (LLMs). To this end, we present a comprehensive analytical foundation for understanding how different LN strategies influence training dynamics in large-scale Transformer training. Until recently, Pre-LN and Post-LN have long dominated standard practices despite their limitations in large-scale training. However, several open-source large-scale models have recently begun silently adopting a third strategy without much explanation. This strategy places layer normalization (LN) peripherally around sublayers, a design we term Peri-LN. While Peri-LN has demonstrated promising empirical performance, its precise mechanisms and benefits remain almost unexplored. Our in-depth analysis shows that Peri-LN strikes an ideal balance in variance growth -- unlike Pre-LN and Post-LN, which are prone to vanishing gradients and ``massive activations.'' To validate our theoretical insight, we conduct large-scale experiments on Transformers up to 3.2B parameters, showing that Peri-LN consistently achieves more balanced variance growth, steadier gradient flow, and convergence stability. Our results suggest that Peri-LN warrants broader consideration for large-scale Transformer architectures, providing renewed insights into the optimal placement and application of LN.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02732', 15)">Copy Link</button>
<div id="copy-message-15" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02740">Vision-Language Model Dialog Games for Self-Improvement</a></h1>
<p><b>Authors:</b> Ksenia Konyushkova, Christos Kaplanis, Serkan Cabi, Misha Denil</p>
<p>Abstract: The increasing demand for high-quality, diverse training data poses a significant bottleneck in advancing vision-language models (VLMs). This paper presents VLM Dialog Games, a novel and scalable self-improvement framework for VLMs. Our approach leverages self-play between two agents engaged in a goal-oriented play centered around image identification. By filtering for successful game interactions, we automatically curate a high-quality dataset of interleaved images and text. We demonstrate that fine-tuning on this synthetic data leads to performance gains on downstream tasks and generalises across datasets. Moreover, as the improvements in the model lead to better game play, this procedure can be applied iteratively. This work paves the way for self-improving VLMs, with potential applications in various real-world scenarios especially when the high-quality multimodal data is scarce.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02740', 16)">Copy Link</button>
<div id="copy-message-16" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02743">LLM Bandit: Cost-Efficient LLM Generation via Preference-Conditioned Dynamic Routing</a></h1>
<p><b>Authors:</b> Yang Li</p>
<p>Abstract: The rapid advancement in large language models (LLMs) has brought forth a diverse range of models with varying capabilities that excel in different tasks and domains. However, selecting the optimal LLM for user queries often involves a challenging trade-off between accuracy and cost, a problem exacerbated by the diverse demands of individual queries. In this work, we present a novel framework that formulates the LLM selection process as a multi-armed bandit problem, enabling dynamic and intelligent routing of queries to the most appropriate model. Our approach incorporates a preference-conditioned dynamic routing mechanism, allowing users to specify their preferences at inference time, thereby offering a customizable balance between performance and cost. Additionally, our selection policy is designed to generalize to unseen LLMs, ensuring adaptability to new models as they emerge. Experimental results demonstrate that our method achieves significant improvements in both accuracy and cost-effectiveness across various LLM platforms, showcasing the potential of our framework to adaptively optimize LLM selection in real-world scenarios.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02743', 17)">Copy Link</button>
<div id="copy-message-17" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02748">ReGNet: Reciprocal Space-Aware Long-Range Modeling and Multi-Property Prediction for Crystals</a></h1>
<p><b>Authors:</b> Jianan Nie, Peiyao Xiao, Kaiyi Ji, Peng Gao</p>
<p>Abstract: Predicting properties of crystals from their structures is a fundamental yet challenging task in materials science. Unlike molecules, crystal structures exhibit infinite periodic arrangements of atoms, requiring methods capable of capturing both local and global information effectively. However, most current works fall short of capturing long-range interactions within periodic structures. To address this limitation, we leverage reciprocal space to efficiently encode long-range interactions with learnable filters within Fourier transforms. We introduce Reciprocal Geometry Network (ReGNet), a novel architecture that integrates geometric GNNs and reciprocal blocks to model short-range and long-range interactions, respectively. Additionally, we introduce ReGNet-MT, a multi-task extension that employs mixture of experts (MoE) for multi-property prediction. Experimental results on the JARVIS and Materials Project benchmarks demonstrate that ReGNet achieves significant performance improvements. Moreover, ReGNet-MT attains state-of-the-art results on two bandgap properties due to positive transfer, while maintaining high computational efficiency. These findings highlight the potential of our model as a scalable and accurate solution for crystal property prediction. The code will be released upon paper acceptance.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02748', 18)">Copy Link</button>
<div id="copy-message-18" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02761">Federated Low-Rank Tensor Estimation for Multimodal Image Reconstruction</a></h1>
<p><b>Authors:</b> Anh Van Nguyen, Diego Klabjan, Minseok Ryu, Kibaek Kim, Zichao Di</p>
<p>Abstract: Low-rank tensor estimation offers a powerful approach to addressing high-dimensional data challenges and can substantially improve solutions to ill-posed inverse problems, such as image reconstruction under noisy or undersampled conditions. Meanwhile, tensor decomposition has gained prominence in federated learning (FL) due to its effectiveness in exploiting latent space structure and its capacity to enhance communication efficiency. In this paper, we present a federated image reconstruction method that applies Tucker decomposition, incorporating joint factorization and randomized sketching to manage large-scale, multimodal data. Our approach avoids reconstructing full-size tensors and supports heterogeneous ranks, allowing clients to select personalized decomposition ranks based on prior knowledge or communication capacity. Numerical results demonstrate that our method achieves superior reconstruction quality and communication compression compared to existing approaches, thereby highlighting its potential for multimodal inverse problems in the FL setting.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02761', 19)">Copy Link</button>
<div id="copy-message-19" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02766">Theoretical Guarantees for Low-Rank Compression of Deep Neural Networks</a></h1>
<p><b>Authors:</b> Shihao Zhang, Rayan Saab</p>
<p>Abstract: Deep neural networks have achieved state-of-the-art performance across numerous applications, but their high memory and computational demands present significant challenges, particularly in resource-constrained environments. Model compression techniques, such as low-rank approximation, offer a promising solution by reducing the size and complexity of these networks while only minimally sacrificing accuracy. In this paper, we develop an analytical framework for data-driven post-training low-rank compression. We prove three recovery theorems under progressively weaker assumptions about the approximate low-rank structure of activations, modeling deviations via noise. Our results represent a step toward explaining why data-driven low-rank compression methods outperform data-agnostic approaches and towards theoretically grounded compression algorithms that reduce inference costs while maintaining performance.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02766', 20)">Copy Link</button>
<div id="copy-message-20" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02770">Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning</a></h1>
<p><b>Authors:</b> Chaofan Lin, Jiaming Tang, Shuo Yang, Hanshuo Wang, Tian Tang, Boyu Tian, Ion Stoica, Song Han, Mingyu Gao</p>
<p>Abstract: Leveraging attention sparsity to accelerate long-context large language models (LLMs) has been a hot research topic. However, current algorithms such as sparse attention or key-value (KV) cache compression tend to use a fixed budget, which presents a significant challenge during deployment because it fails to account for the dynamic nature of real-world scenarios, where the optimal balance between accuracy and efficiency can vary greatly. In this paper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse attention can surprisingly achieve adaptive budgeting. Based on this, we propose Twilight, a framework to bring adaptive sparsity to any existing sparse attention algorithm without sacrificing their accuracy. Empirical results show that Twilight can adaptively prune at most 98% of redundant tokens, leading to $15.4\times$ acceleration in self-attention operations and $3.9\times$ acceleration in end-to-end per token latency in long context LLM decoding.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02770', 21)">Copy Link</button>
<div id="copy-message-21" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02785">OpenSTARLab: Open Approach for Spatio-Temporal Agent Data Analysis in Soccer</a></h1>
<p><b>Authors:</b> Calvin Yeung, Kenjiro Ide, Taiga Someya, Keisuke Fujii</p>
<p>Abstract: Sports analytics has become both more professional and sophisticated, driven by the growing availability of detailed performance data. This progress enables applications such as match outcome prediction, player scouting, and tactical analysis. In soccer, the effective utilization of event and tracking data is fundamental for capturing and analyzing the dynamics of the game. However, there are two primary challenges: the limited availability of event data, primarily restricted to top-tier teams and leagues, and the scarcity and high cost of tracking data, which complicates its integration with event data for comprehensive analysis. Here we propose OpenSTARLab, an open-source framework designed to democratize spatio-temporal agent data analysis in sports by addressing these key challenges. OpenSTARLab includes the Pre-processing Package that standardizes event and tracking data through Unified and Integrated Event Data and State-Action-Reward formats, the Event Modeling Package that implements deep learning-based event prediction, alongside the RLearn Package for reinforcement learning tasks. These technical components facilitate the handling of diverse data sources and support advanced analytical tasks, thereby enhancing the overall functionality and usability of the framework. To assess OpenSTARLab's effectiveness, we conducted several experimental evaluations. These demonstrate the superior performance of the specific event prediction model in terms of action and time prediction accuracies and maintained its robust event simulation performance. Furthermore, reinforcement learning experiments reveal a trade-off between action accuracy and temporal difference loss and show comprehensive visualization. Overall, OpenSTARLab serves as a robust platform for researchers and practitioners, enhancing innovation and collaboration in the field of soccer data analytics.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02785', 22)">Copy Link</button>
<div id="copy-message-22" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02786">When Machine Learning Gets Personal: Understanding Fairness of Personalized Models</a></h1>
<p><b>Authors:</b> Louisa Cornelis, Guillermo Bern\'ardez, Haewon Jeong, Nina Miolane</p>
<p>Abstract: Personalization in machine learning involves tailoring models to individual users by incorporating personal attributes such as demographic or medical data. While personalization can improve prediction accuracy, it may also amplify biases and reduce explainability. This work introduces a unified framework to evaluate the impact of personalization on both prediction accuracy and explanation quality across classification and regression tasks. We derive novel upper bounds for the number of personal attributes that can be used to reliably validate benefits of personalization. Our analysis uncovers key trade-offs. We show that regression models can potentially utilize more personal attributes than classification models. We also demonstrate that improvements in prediction accuracy due to personalization do not necessarily translate to enhanced explainability -- underpinning the importance to evaluate both metrics when personalizing machine learning models in critical settings such as healthcare. Validated with a real-world dataset, this framework offers practical guidance for balancing accuracy, fairness, and interpretability in personalized models.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02786', 23)">Copy Link</button>
<div id="copy-message-23" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02790">Leveraging the true depth of LLMs</a></h1>
<p><b>Authors:</b> Ram\'on Calvo Gonz\'alez, Daniele Paliotta, Matteo Pagliardini, Martin Jaggi, Fran\c{c}ois Fleuret</p>
<p>Abstract: Large Language Models demonstrate remarkable capabilities at the cost of high compute requirements. While recent research has shown that intermediate layers can be removed or have their order shuffled without impacting performance significantly, these findings have not been employed to reduce the computational cost of inference. We investigate several potential ways to reduce the depth of pre-trained LLMs without significantly affecting performance. Leveraging our insights, we present a novel approach that exploits this decoupling between layers by grouping some of them into pairs that can be evaluated in parallel.
  This modification of the computational graph -- through better parallelism -- results in an average improvement of around 1.20x on the number of tokens generated per second, without re-training nor fine-tuning, while retaining 95%-99% of the original accuracy. Empirical evaluation demonstrates that this approach significantly improves serving efficiency while maintaining model performance, offering a practical improvement for large-scale LLM deployment.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02790', 24)">Copy Link</button>
<div id="copy-message-24" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02797">Upweighting Easy Samples in Fine-Tuning Mitigates Forgetting</a></h1>
<p><b>Authors:</b> Sunny Sanyal, Hayden Prairie, Rudrajit Das, Ali Kavis, Sujay Sanghavi</p>
<p>Abstract: Fine-tuning a pre-trained model on a downstream task often degrades its original capabilities, a phenomenon known as "catastrophic forgetting". This is especially an issue when one does not have access to the data and recipe used to develop the pre-trained model. Under this constraint, most existing methods for mitigating forgetting are inapplicable. To address this challenge, we propose a sample weighting scheme for the fine-tuning data solely based on the pre-trained model's losses. Specifically, we upweight the easy samples on which the pre-trained model's loss is low and vice versa to limit the drift from the pre-trained model. Our approach is orthogonal and yet complementary to existing methods; while such methods mostly operate on parameter or gradient space, we concentrate on the sample space. We theoretically analyze the impact of fine-tuning with our method in a linear setting, showing that it stalls learning in a certain subspace which inhibits overfitting to the target task. We empirically demonstrate the efficacy of our method on both language and vision tasks. As an example, when fine-tuning Gemma 2 2B on MetaMathQA, our method results in only a $0.8\%$ drop in accuracy on GSM8K (another math dataset) compared to standard fine-tuning, while preserving $5.4\%$ more accuracy on the pre-training datasets. Our code is publicly available at https://github.com/sanyalsunny111/FLOW_finetuning .</p>
<p>URLs: <a href="https://github.com/sanyalsunny111/FLOW_finetuning">https://github.com/sanyalsunny111/FLOW_finetuning</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02797, https://github.com/sanyalsunny111/FLOW_finetuning', 25)">Copy Link</button>
<div id="copy-message-25" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02810">Mol-LLM: Generalist Molecular LLM with Improved Graph Utilization</a></h1>
<p><b>Authors:</b> Chanhui Lee, Yuheon Song, YongJun Jeong, Hanbum Ko, Rodrigo Hormazabal, Sehui Han, Kyunghoon Bae, Sungbin Lim, Sungwoong Kim</p>
<p>Abstract: Recent advances in Large Language Models (LLMs) have motivated the development of general LLMs for molecular tasks. While several studies have demonstrated that fine-tuned LLMs can achieve impressive benchmark performances, they are far from genuine generalist molecular LLMs due to a lack of fundamental understanding of molecular structure. Specifically, when given molecular task instructions, LLMs trained with naive next-token prediction training assign similar likelihood scores to both original and negatively corrupted molecules, revealing their lack of molecular structure understanding that is crucial for reliable and general molecular LLMs. To overcome this limitation and obtain a true generalist molecular LLM, we introduce a novel multi-modal training method based on a thorough multi-modal instruction tuning as well as a molecular structure preference optimization between chosen and rejected graphs. On various molecular benchmarks, the proposed generalist molecular LLM, called Mol-LLM, achieves state-of-the-art performances among generalist LLMs on most tasks, at the same time, surpassing or comparable to state-of-the-art specialist LLMs. Moreover, Mol-LLM also shows superior generalization performances in reaction prediction tasks, demonstrating the effect of the molecular structure understanding for generalization perspective.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02810', 26)">Copy Link</button>
<div id="copy-message-26" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02820">Slowing Learning by Erasing Simple Features</a></h1>
<p><b>Authors:</b> Lucia Quirke, Nora Belrose</p>
<p>Abstract: Prior work suggests that neural networks tend to learn low-order moments of the data distribution first, before moving on to higher-order correlations. In this work, we derive a novel closed-form concept erasure method, QLEACE, which surgically removes all quadratically available information about a concept from a representation. Through comparisons with linear erasure (LEACE) and two approximate forms of quadratic erasure, we explore whether networks can still learn when low-order statistics are removed from image classification datasets. We find that while LEACE consistently slows learning, quadratic erasure can exhibit both positive and negative effects on learning speed depending on the choice of dataset, model architecture, and erasure method.
  Use of QLEACE consistently slows learning in feedforward architectures, but more sophisticated architectures learn to use injected higher order Shannon information about class labels. Its approximate variants avoid injecting information, but surprisingly act as data augmentation techniques on some datasets, enhancing learning speed compared to LEACE.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02820', 27)">Copy Link</button>
<div id="copy-message-27" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02834">Task-Aware Virtual Training: Enhancing Generalization in Meta-Reinforcement Learning for Out-of-Distribution Tasks</a></h1>
<p><b>Authors:</b> Jeongmo Kim, Yisak Park, Minung Kim, Seungyul Han</p>
<p>Abstract: Meta reinforcement learning aims to develop policies that generalize to unseen tasks sampled from a task distribution. While context-based meta-RL methods improve task representation using task latents, they often struggle with out-of-distribution (OOD) tasks. To address this, we propose Task-Aware Virtual Training (TAVT), a novel algorithm that accurately captures task characteristics for both training and OOD scenarios using metric-based representation learning. Our method successfully preserves task characteristics in virtual tasks and employs a state regularization technique to mitigate overestimation errors in state-varying environments. Numerical results demonstrate that TAVT significantly enhances generalization to OOD tasks across various MuJoCo and MetaWorld environments.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02834', 28)">Copy Link</button>
<div id="copy-message-28" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02844">Wolfpack Adversarial Attack for Robust Multi-Agent Reinforcement Learning</a></h1>
<p><b>Authors:</b> Sunwoo Lee, Jaebak Hwang, Yonghyeon Jo, Seungyul Han</p>
<p>Abstract: Traditional robust methods in multi-agent reinforcement learning (MARL) often struggle against coordinated adversarial attacks in cooperative scenarios. To address this limitation, we propose the Wolfpack Adversarial Attack framework, inspired by wolf hunting strategies, which targets an initial agent and its assisting agents to disrupt cooperation. Additionally, we introduce the Wolfpack-Adversarial Learning for MARL (WALL) framework, which trains robust MARL policies to defend against the proposed Wolfpack attack by fostering system-wide collaboration. Experimental results underscore the devastating impact of the Wolfpack attack and the significant robustness improvements achieved by WALL.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02844', 29)">Copy Link</button>
<div id="copy-message-29" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02856">PH-VAE: A Polynomial Hierarchical Variational Autoencoder Towards Disentangled Representation Learning</a></h1>
<p><b>Authors:</b> Xi Chen, Shaofan Li</p>
<p>Abstract: The variational autoencoder (VAE) is a simple and efficient generative artificial intelligence method for modeling complex probability distributions of various types of data, such as images and texts. However, it suffers some main shortcomings, such as lack of interpretability in the latent variables, difficulties in tuning hyperparameters while training, producing blurry, unrealistic downstream outputs or loss of information due to how it calculates loss functions and recovers data distributions, overfitting, and origin gravity effect for small data sets, among other issues. These and other limitations have caused unsatisfactory generation effects for the data with complex distributions. In this work, we proposed and developed a polynomial hierarchical variational autoencoder (PH-VAE), in which we used a polynomial hierarchical date format to generate or to reconstruct the data distributions. In doing so, we also proposed a novel Polynomial Divergence in the loss function to replace or generalize the Kullback-Leibler (KL) divergence, which results in systematic and drastic improvements in both accuracy and reproducibility of the re-constructed distribution function as well as the quality of re-constructed data images while keeping the dataset size the same but capturing fine resolution of the data. Moreover, we showed that the proposed PH-VAE has some form of disentangled representation learning ability.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02856', 30)">Copy Link</button>
<div id="copy-message-30" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02869">OmniRL: In-Context Reinforcement Learning by Large-Scale Meta-Training in Randomized Worlds</a></h1>
<p><b>Authors:</b> Fan Wang, Pengtao Shao, Yiming Zhang, Bo Yu, Shaoshan Liu, Ning Ding, Yang Cao, Yu Kang, Haifeng Wang</p>
<p>Abstract: We introduce OmniRL, a highly generalizable in-context reinforcement learning (ICRL) model that is meta-trained on hundreds of thousands of diverse tasks. These tasks are procedurally generated by randomizing state transitions and rewards within Markov Decision Processes. To facilitate this extensive meta-training, we propose two key innovations: 1. An efficient data synthesis pipeline for ICRL, which leverages the interaction histories of diverse behavior policies; and 2. A novel modeling framework that integrates both imitation learning and reinforcement learning (RL) within the context, by incorporating prior knowledge. For the first time, we demonstrate that in-context learning (ICL) alone, without any gradient-based fine-tuning, can successfully tackle unseen Gymnasium tasks through imitation learning, online RL, or offline RL. Additionally, we show that achieving generalized ICRL capabilities-unlike task identification-oriented few-shot learning-critically depends on long trajectories generated by variant tasks and diverse behavior policies. By emphasizing the potential of ICL and departing from pre-training focused on acquiring specific skills, we further underscore the significance of meta-training aimed at cultivating the ability of ICL itself.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02869', 31)">Copy Link</button>
<div id="copy-message-31" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02909">SPARC: Subspace-Aware Prompt Adaptation for Robust Continual Learning in LLMs</a></h1>
<p><b>Authors:</b> Dinithi Jayasuriya (Intel Labs, Oregon), Sina Tayebati (Intel Labs, Oregon), Davide Ettori (Intel Labs, Oregon), Ranganath Krishnan (Intel Labs, Oregon), Amit Ranjan Trivedi (Intel Labs, Oregon)</p>
<p>Abstract: We propose SPARC, a lightweight continual learning framework for large language models (LLMs) that enables efficient task adaptation through prompt tuning in a lower-dimensional space. By leveraging principal component analysis (PCA), we identify a compact subspace of the training data. Optimizing prompts in this lower-dimensional space enhances training efficiency, as it focuses updates on the most relevant features while reducing computational overhead. Furthermore, since the model's internal structure remains unaltered, the extensive knowledge gained from pretraining is fully preserved, ensuring that previously learned information is not compromised during adaptation. Our method achieves high knowledge retention in both task-incremental and domain-incremental continual learning setups while fine-tuning only 0.04% of the model's parameters. Additionally, by integrating LoRA, we enhance adaptability to computational constraints, allowing for a tradeoff between accuracy and training cost. Experiments on the SuperGLUE benchmark demonstrate that our PCA-based prompt tuning combined with LoRA maintains full knowledge retention while improving accuracy, utilizing only 1% of the model's parameters. These results establish our approach as a scalable and resource-efficient solution for continual learning in LLMs.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02909', 32)">Copy Link</button>
<div id="copy-message-32" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02912">MobiCLR: Mobility Time Series Contrastive Learning for Urban Region Representations</a></h1>
<p><b>Authors:</b> Namwoo Kim, Takahiro Yabe, Chanyoung Park, Yoonjin Yoon</p>
<p>Abstract: Recently, learning effective representations of urban regions has gained significant attention as a key approach to understanding urban dynamics and advancing smarter cities. Existing approaches have demonstrated the potential of leveraging mobility data to generate latent representations, providing valuable insights into the intrinsic characteristics of urban areas. However, incorporating the temporal dynamics and detailed semantics inherent in human mobility patterns remains underexplored. To address this gap, we propose a novel urban region representation learning model, Mobility Time Series Contrastive Learning for Urban Region Representations (MobiCLR), designed to capture semantically meaningful embeddings from inflow and outflow mobility patterns. MobiCLR uses contrastive learning to enhance the discriminative power of its representations, applying an instance-wise contrastive loss to capture distinct flow-specific characteristics. Additionally, we develop a regularizer to align output features with these flow-specific representations, enabling a more comprehensive understanding of mobility dynamics. To validate our model, we conduct extensive experiments in Chicago, New York, and Washington, D.C. to predict income, educational attainment, and social vulnerability. The results demonstrate that our model outperforms state-of-the-art models.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02912', 33)">Copy Link</button>
<div id="copy-message-33" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02913">Privacy Token: Surprised to Find Out What You Accidentally Revealed</a></h1>
<p><b>Authors:</b> Jiayang Meng, Tao Huang, Xin Shi, Qingyu Huang, Chen Hou, Hong Chen</p>
<p>Abstract: The widespread deployment of deep learning models in privacy-sensitive domains has amplified concerns regarding privacy risks, particularly those stemming from gradient leakage during training. Current privacy assessments primarily rely on post-training attack simulations. However, these methods are inherently reactive, unable to encompass all potential attack scenarios, and often based on idealized adversarial assumptions. These limitations underscore the need for proactive approaches to privacy risk assessment during the training process. To address this gap, we propose the concept of privacy tokens, which are derived directly from private gradients during training. Privacy tokens encapsulate gradient features and, when combined with data features, offer valuable insights into the extent of private information leakage from training data, enabling real-time measurement of privacy risks without relying on adversarial attack simulations. Additionally, we employ Mutual Information (MI) as a robust metric to quantify the relationship between training data and gradients, providing precise and continuous assessments of privacy leakage throughout the training process. Extensive experiments validate our framework, demonstrating the effectiveness of privacy tokens and MI in identifying and quantifying privacy risks. This proactive approach marks a significant advancement in privacy monitoring, promoting the safer deployment of deep learning models in sensitive applications.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02913', 34)">Copy Link</button>
<div id="copy-message-34" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02917">Interactive Symbolic Regression through Offline Reinforcement Learning: A Co-Design Framework</a></h1>
<p><b>Authors:</b> Yuan Tian, Wenqi Zhou, Michele Viscione, Hao Dong, David Kammer, Olga Fink</p>
<p>Abstract: Symbolic Regression (SR) holds great potential for uncovering underlying mathematical and physical relationships from observed data. However, the vast combinatorial space of possible expressions poses significant challenges for both online search methods and pre-trained transformer models. Additionally, current state-of-the-art approaches typically do not consider the integration of domain experts' prior knowledge and do not support iterative interactions with the model during the equation discovery process. To address these challenges, we propose the Symbolic Q-network (Sym-Q), an advanced interactive framework for large-scale symbolic regression. Unlike previous large-scale transformer-based SR approaches, Sym-Q leverages reinforcement learning without relying on a transformer-based decoder. This formulation allows the agent to learn through offline reinforcement learning using any type of tree encoder, enabling more efficient training and inference. Furthermore, we propose a co-design mechanism, where the reinforcement learning-based Sym-Q facilitates effective interaction with domain experts at any stage of the equation discovery process. Users can dynamically modify generated nodes of the expression, collaborating with the agent to tailor the mathematical expression to best fit the problem and align with the assumed physical laws, particularly when there is prior partial knowledge of the expected behavior. Our experiments demonstrate that the pre-trained Sym-Q surpasses existing SR algorithms on the challenging SSDNC benchmark. Moreover, we experimentally show on real-world cases that its performance can be further enhanced by the interactive co-design mechanism, with Sym-Q achieving greater performance gains than other state-of-the-art models. Our reproducible code is available at https://github.com/EPFL-IMOS/Sym-Q.</p>
<p>URLs: <a href="https://github.com/EPFL-IMOS/Sym-Q.">https://github.com/EPFL-IMOS/Sym-Q.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02917, https://github.com/EPFL-IMOS/Sym-Q.', 35)">Copy Link</button>
<div id="copy-message-35" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02920">Adaptive Budget Optimization for Multichannel Advertising Using Combinatorial Bandits</a></h1>
<p><b>Authors:</b> Briti Gangopadhyay, Zhao Wang, Alberto Silvio Chiappa, Shingo Takamatsu</p>
<p>Abstract: Effective budget allocation is crucial for optimizing the performance of digital advertising campaigns. However, the development of practical budget allocation algorithms remain limited, primarily due to the lack of public datasets and comprehensive simulation environments capable of verifying the intricacies of real-world advertising. While multi-armed bandit (MAB) algorithms have been extensively studied, their efficacy diminishes in non-stationary environments where quick adaptation to changing market dynamics is essential. In this paper, we advance the field of budget allocation in digital advertising by introducing three key contributions. First, we develop a simulation environment designed to mimic multichannel advertising campaigns over extended time horizons, incorporating logged real-world data. Second, we propose an enhanced combinatorial bandit budget allocation strategy that leverages a saturating mean function and a targeted exploration mechanism with change-point detection. This approach dynamically adapts to changing market conditions, improving allocation efficiency by filtering target regions based on domain knowledge. Finally, we present both theoretical analysis and empirical results, demonstrating that our method consistently outperforms baseline strategies, achieving higher rewards and lower regret across multiple real-world campaigns.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02920', 36)">Copy Link</button>
<div id="copy-message-36" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02921">Robust Reward Alignment in Hypothesis Space</a></h1>
<p><b>Authors:</b> Zhixian Xie, Haode Zhang, Yizhe Feng, Wanxin Jin</p>
<p>Abstract: Reward design for reinforcement learning and optimal control agents is challenging. Preference-based alignment addresses this by enabling agents to learn rewards from ranked trajectory pairs provided by humans. However, existing methods often struggle from poor robustness to unknown false human preferences. In this work, we propose a robust and efficient reward alignment method based on a novel and geometrically interpretable perspective: hypothesis space batched cutting. Our method iteratively refines the reward hypothesis space through "cuts" based on batches of human preferences. Within each batch, human preferences, queried based on disagreement, are grouped using a voting function to determine the appropriate cut, ensuring a bounded human query complexity. To handle unknown erroneous preferences, we introduce a conservative cutting method within each batch, preventing erroneous human preferences from making overly aggressive cuts to the hypothesis space. This guarantees provable robustness against false preferences. We evaluate our method in a model predictive control setting across diverse tasks, including DM-Control, dexterous in-hand manipulation, and locomotion. The results demonstrate that our framework achieves comparable or superior performance to state-of-the-art methods in error-free settings while significantly outperforming existing method when handling high percentage of erroneous human preferences.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02921', 37)">Copy Link</button>
<div id="copy-message-37" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02922">Elucidating the Preconditioning in Consistency Distillation</a></h1>
<p><b>Authors:</b> Kaiwen Zheng, Guande He, Jianfei Chen, Fan Bao, Jun Zhu</p>
<p>Abstract: Consistency distillation is a prevalent way for accelerating diffusion models adopted in consistency (trajectory) models, in which a student model is trained to traverse backward on the probability flow (PF) ordinary differential equation (ODE) trajectory determined by the teacher model. Preconditioning is a vital technique for stabilizing consistency distillation, by linear combining the input data and the network output with pre-defined coefficients as the consistency function. It imposes the boundary condition of consistency functions without restricting the form and expressiveness of the neural network. However, previous preconditionings are hand-crafted and may be suboptimal choices. In this work, we offer the first theoretical insights into the preconditioning in consistency distillation, by elucidating its design criteria and the connection to the teacher ODE trajectory. Based on these analyses, we further propose a principled way dubbed \textit{Analytic-Precond} to analytically optimize the preconditioning according to the consistency gap (defined as the gap between the teacher denoiser and the optimal student denoiser) on a generalized teacher ODE. We demonstrate that Analytic-Precond can facilitate the learning of trajectory jumpers, enhance the alignment of the student trajectory with the teacher's, and achieve $2\times$ to $3\times$ training acceleration of consistency trajectory models in multi-step generation across various datasets.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02922', 38)">Copy Link</button>
<div id="copy-message-38" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02924">TopoCL: Topological Contrastive Learning for Time Series</a></h1>
<p><b>Authors:</b> Namwoo Kim, Hyungryul Baik, Yoonjin Yoon</p>
<p>Abstract: Universal time series representation learning is challenging but valuable in real-world applications such as classification, anomaly detection, and forecasting. Recently, contrastive learning (CL) has been actively explored to tackle time series representation. However, a key challenge is that the data augmentation process in CL can distort seasonal patterns or temporal dependencies, inevitably leading to a loss of semantic information. To address this challenge, we propose Topological Contrastive Learning for time series (TopoCL). TopoCL mitigates such information loss by incorporating persistent homology, which captures the topological characteristics of data that remain invariant under transformations. In this paper, we treat the temporal and topological properties of time series data as distinct modalities. Specifically, we compute persistent homology to construct topological features of time series data, representing them in persistence diagrams. We then design a neural network to encode these persistent diagrams. Our approach jointly optimizes CL within the time modality and time-topology correspondence, promoting a comprehensive understanding of both temporal semantics and topological properties of time series. We conduct extensive experiments on four downstream tasks-classification, anomaly detection, forecasting, and transfer learning. The results demonstrate that TopoCL achieves state-of-the-art performance.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02924', 39)">Copy Link</button>
<div id="copy-message-39" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02941">Fast T2T: Optimization Consistency Speeds Up Diffusion-Based Training-to-Testing Solving for Combinatorial Optimization</a></h1>
<p><b>Authors:</b> Yang Li, Jinpei Guo, Runzhong Wang, Hongyuan Zha, Junchi Yan</p>
<p>Abstract: Diffusion models have recently advanced Combinatorial Optimization (CO) as a powerful backbone for neural solvers. However, their iterative sampling process requiring denoising across multiple noise levels incurs substantial overhead. We propose to learn direct mappings from different noise levels to the optimal solution for a given instance, facilitating high-quality generation with minimal shots. This is achieved through an optimization consistency training protocol, which, for a given instance, minimizes the difference among samples originating from varying generative trajectories and time steps relative to the optimal solution. The proposed model enables fast single-step solution generation while retaining the option of multi-step sampling to trade for sampling quality, which offers a more effective and efficient alternative backbone for neural solvers. In addition, within the training-to-testing (T2T) framework, to bridge the gap between training on historical instances and solving new instances, we introduce a novel consistency-based gradient search scheme during the test stage, enabling more effective exploration of the solution space learned during training. It is achieved by updating the latent solution probabilities under objective gradient guidance during the alternation of noise injection and denoising steps. We refer to this model as Fast T2T. Extensive experiments on two popular tasks, the Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS), demonstrate the superiority of Fast T2T regarding both solution quality and efficiency, even outperforming LKH given limited time budgets. Notably, Fast T2T with merely one-step generation and one-step gradient search can mostly outperform the SOTA diffusion-based counterparts that require hundreds of steps, while achieving tens of times speedup.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02941', 40)">Copy Link</button>
<div id="copy-message-40" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02954">Direct Distributional Optimization for Provable Alignment of Diffusion Models</a></h1>
<p><b>Authors:</b> Ryotaro Kawata, Kazusato Oko, Atsushi Nitanda, Taiji Suzuki</p>
<p>Abstract: We introduce a novel alignment method for diffusion models from distribution optimization perspectives while providing rigorous convergence guarantees. We first formulate the problem as a generic regularized loss minimization over probability distributions and directly optimize the distribution using the Dual Averaging method. Next, we enable sampling from the learned distribution by approximating its score function via Doob's $h$-transform technique. The proposed framework is supported by rigorous convergence guarantees and an end-to-end bound on the sampling error, which imply that when the original distribution's score is known accurately, the complexity of sampling from shifted distributions is independent of isoperimetric conditions. This framework is broadly applicable to general distribution optimization problems, including alignment tasks in Reinforcement Learning with Human Feedback (RLHF), Direct Preference Optimization (DPO), and Kahneman-Tversky Optimization (KTO). We empirically validate its performance on synthetic and image datasets using the DPO objective.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02954', 41)">Copy Link</button>
<div id="copy-message-41" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02970">Membership Inference Attack Should Move On to Distributional Statistics for Distilled Generative Models</a></h1>
<p><b>Authors:</b> Muxing Li, Zesheng Ye, Yixuan Li, Andy Song, Guangquan Zhang, Feng Liu</p>
<p>Abstract: Membership inference attacks (MIAs) determine whether certain data instances were used to train a model by exploiting the differences in how the model responds to seen versus unseen instances. This capability makes MIAs important in assessing privacy leakage within modern generative AI systems. However, this paper reveals an oversight in existing MIAs against \emph{distilled generative models}: attackers can no longer detect a teacher model's training instances individually when targeting the distilled student model, as the student learns from the teacher-generated data rather than its original member data, preventing direct instance-level memorization. Nevertheless, we find that student-generated samples exhibit a significantly stronger distributional alignment with teacher's member data than non-member data. This leads us to posit that MIAs \emph{on distilled generative models should shift from instance-level to distribution-level statistics}. We thereby introduce a \emph{set-based} MIA framework that measures \emph{relative} distributional discrepancies between student-generated data\emph{sets} and potential member/non-member data\emph{sets}, Empirically, distributional statistics reliably distinguish a teacher's member data from non-member data through the distilled model. Finally, we discuss scenarios in which our setup faces limitations.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02970', 42)">Copy Link</button>
<div id="copy-message-42" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02975">TGB-Seq Benchmark: Challenging Temporal GNNs with Complex Sequential Dynamics</a></h1>
<p><b>Authors:</b> Lu Yi, Jie Peng, Yanping Zheng, Fengran Mo, Zhewei Wei, Yuhang Ye, Yue Zixuan, Zengfeng Huang</p>
<p>Abstract: Future link prediction is a fundamental challenge in various real-world dynamic systems. To address this, numerous temporal graph neural networks (temporal GNNs) and benchmark datasets have been developed. However, these datasets often feature excessive repeated edges and lack complex sequential dynamics, a key characteristic inherent in many real-world applications such as recommender systems and ``Who-To-Follow'' on social networks. This oversight has led existing methods to inadvertently downplay the importance of learning sequential dynamics, focusing primarily on predicting repeated edges.
  In this study, we demonstrate that existing methods, such as GraphMixer and DyGFormer, are inherently incapable of learning simple sequential dynamics, such as ``a user who has followed OpenAI and Anthropic is more likely to follow AI at Meta next.'' Motivated by this issue, we introduce the Temporal Graph Benchmark with Sequential Dynamics (TGB-Seq), a new benchmark carefully curated to minimize repeated edges, challenging models to learn sequential dynamics and generalize to unseen edges. TGB-Seq comprises large real-world datasets spanning diverse domains, including e-commerce interactions, movie ratings, business reviews, social networks, citation networks and web link networks. Benchmarking experiments reveal that current methods usually suffer significant performance degradation and incur substantial training costs on TGB-Seq, posing new challenges and opportunities for future research. TGB-Seq datasets, leaderboards, and example codes are available at https://tgb-seq.github.io/.</p>
<p>URLs: <a href="https://tgb-seq.github.io/.">https://tgb-seq.github.io/.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02975, https://tgb-seq.github.io/.', 43)">Copy Link</button>
<div id="copy-message-43" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.02998">Conformal Uncertainty Indicator for Continual Test-Time Adaptation</a></h1>
<p><b>Authors:</b> Fan Lyu, Hanyu Zhao, Ziqi Shi, Ye Liu, Fuyuan Hu, Zhang Zhang, Liang Wang</p>
<p>Abstract: Continual Test-Time Adaptation (CTTA) aims to adapt models to sequentially changing domains during testing, relying on pseudo-labels for self-adaptation. However, incorrect pseudo-labels can accumulate, leading to performance degradation. To address this, we propose a Conformal Uncertainty Indicator (CUI) for CTTA, leveraging Conformal Prediction (CP) to generate prediction sets that include the true label with a specified coverage probability. Since domain shifts can lower the coverage than expected, making CP unreliable, we dynamically compensate for the coverage by measuring both domain and data differences. Reliable pseudo-labels from CP are then selectively utilized to enhance adaptation. Experiments confirm that CUI effectively estimates uncertainty and improves adaptation performance across various existing CTTA methods.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02998', 44)">Copy Link</button>
<div id="copy-message-44" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03009">Scaling Laws for Upcycling Mixture-of-Experts Language Models</a></h1>
<p><b>Authors:</b> Seng Pei Liew, Takuya Kato, Sho Takase</p>
<p>Abstract: Pretraining large language models (LLMs) is resource-intensive, often requiring months of training time even with high-end GPU clusters. There are two approaches of mitigating such computational demands: reusing smaller models to train larger ones (upcycling), and training computationally efficient models like mixture-of-experts (MoE). In this paper, we study the upcycling of LLMs to MoE models, of which the scaling behavior remains underexplored. Through extensive experiments, we identify empirical scaling laws that describe how performance depends on dataset size and model configuration. Particularly, we show that, while scaling these factors improves performance, there is a novel interaction term between the dense and upcycled training dataset that limits the efficiency of upcycling at large computational budgets. Based on these findings, we provide guidance to scale upcycling, and establish conditions under which upcycling outperforms from-scratch trainings within budget constraints.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03009', 45)">Copy Link</button>
<div id="copy-message-45" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03014">xai_evals : A Framework for Evaluating Post-Hoc Local Explanation Methods</a></h1>
<p><b>Authors:</b> Pratinav Seth, Yashwardhan Rathore, Neeraj Kumar Singh, Chintan Chitroda, Vinay Kumar Sankarapu</p>
<p>Abstract: The growing complexity of machine learning and deep learning models has led to an increased reliance on opaque "black box" systems, making it difficult to understand the rationale behind predictions. This lack of transparency is particularly challenging in high-stakes applications where interpretability is as important as accuracy. Post-hoc explanation methods are commonly used to interpret these models, but they are seldom rigorously evaluated, raising concerns about their reliability. The Python package xai_evals addresses this by providing a comprehensive framework for generating, benchmarking, and evaluating explanation methods across both tabular and image data modalities. It integrates popular techniques like SHAP, LIME, Grad-CAM, Integrated Gradients (IG), and Backtrace, while supporting evaluation metrics such as faithfulness, sensitivity, and robustness. xai_evals enhances the interpretability of machine learning models, fostering transparency and trust in AI systems. The library is open-sourced at https://pypi.org/project/xai-evals/ .</p>
<p>URLs: <a href="https://pypi.org/project/xai-evals/">https://pypi.org/project/xai-evals/</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03014, https://pypi.org/project/xai-evals/', 46)">Copy Link</button>
<div id="copy-message-46" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03023">Parametric Scaling Law of Tuning Bias in Conformal Prediction</a></h1>
<p><b>Authors:</b> Hao Zeng, Kangdao Liu, Bingyi Jing, Hongxin Wei</p>
<p>Abstract: Conformal prediction is a popular framework of uncertainty quantification that constructs prediction sets with coverage guarantees. To uphold the exchangeability assumption, many conformal prediction methods necessitate an additional holdout set for parameter tuning. Yet, the impact of violating this principle on coverage remains underexplored, making it ambiguous in practical applications. In this work, we empirically find that the tuning bias - the coverage gap introduced by leveraging the same dataset for tuning and calibration, is negligible for simple parameter tuning in many conformal prediction methods. In particular, we observe the scaling law of the tuning bias: this bias increases with parameter space complexity and decreases with calibration set size. Formally, we establish a theoretical framework to quantify the tuning bias and provide rigorous proof for the scaling law of the tuning bias by deriving its upper bound. In the end, we discuss how to reduce the tuning bias, guided by the theories we developed.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03023', 47)">Copy Link</button>
<div id="copy-message-47" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03029">On Zero-Initialized Attention: Optimal Prompt and Gating Factor Estimation</a></h1>
<p><b>Authors:</b> Nghiem T. Diep, Huy Nguyen, Chau Nguyen, Minh Le, Duy M. H. Nguyen, Daniel Sonntag, Mathias Niepert, Nhat Ho</p>
<p>Abstract: The LLaMA-Adapter has recently emerged as an efficient fine-tuning technique for LLaMA models, leveraging zero-initialized attention to stabilize training and enhance performance. However, despite its empirical success, the theoretical foundations of zero-initialized attention remain largely unexplored. In this paper, we provide a rigorous theoretical analysis, establishing a connection between zero-initialized attention and mixture-of-expert models. We prove that both linear and non-linear prompts, along with gating functions, can be optimally estimated, with non-linear prompts offering greater flexibility for future applications. Empirically, we validate our findings on the open LLM benchmarks, demonstrating that non-linear prompts outperform linear ones. Notably, even with limited training data, both prompt types consistently surpass vanilla attention, highlighting the robustness and adaptability of zero-initialized attention.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03029', 48)">Copy Link</button>
<div id="copy-message-48" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03032">Analyze Feature Flow to Enhance Interpretation and Steering in Language Models</a></h1>
<p><b>Authors:</b> Daniil Laptev, Nikita Balagansky, Yaroslav Aksenov, Daniil Gavrilov</p>
<p>Abstract: We introduce a new approach to systematically map features discovered by sparse autoencoder across consecutive layers of large language models, extending earlier work that examined inter-layer feature links. By using a data-free cosine similarity technique, we trace how specific features persist, transform, or first appear at each stage. This method yields granular flow graphs of feature evolution, enabling fine-grained interpretability and mechanistic insights into model computations. Crucially, we demonstrate how these cross-layer feature maps facilitate direct steering of model behavior by amplifying or suppressing chosen features, achieving targeted thematic control in text generation. Together, our findings highlight the utility of a causal, cross-layer interpretability framework that not only clarifies how features develop through forward passes but also provides new means for transparent manipulation of large language models.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03032', 49)">Copy Link</button>
<div id="copy-message-49" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03033">Aggregate to Adapt: Node-Centric Aggregation for Multi-Source-Free Graph Domain Adaptation</a></h1>
<p><b>Authors:</b> Zhen Zhang, Bingsheng He</p>
<p>Abstract: Unsupervised graph domain adaptation (UGDA) focuses on transferring knowledge from labeled source graph to unlabeled target graph under domain discrepancies. Most existing UGDA methods are designed to adapt information from a single source domain, which cannot effectively exploit the complementary knowledge from multiple source domains. Furthermore, their assumptions that the labeled source graphs are accessible throughout the training procedure might not be practical due to privacy, regulation, and storage concerns. In this paper, we investigate multi-source-free unsupervised graph domain adaptation, i.e., adapting knowledge from multiple source domains to an unlabeled target domain without utilizing labeled source graphs but relying solely on source pre-trained models. Unlike previous multi-source domain adaptation approaches that aggregate predictions at model level, we introduce a novel model named GraphATA which conducts adaptation at node granularity. Specifically, we parameterize each node with its own graph convolutional matrix by automatically aggregating weight matrices from multiple source models according to its local context, thus realizing dynamic adaptation over graph structured data. We also demonstrate the capability of GraphATA to generalize to both model-centric and layer-centric methods. Comprehensive experiments on various public datasets show that our GraphATA can consistently surpass recent state-of-the-art baselines with different gains.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03033', 50)">Copy Link</button>
<div id="copy-message-50" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03044">RepLoRA: Reparameterizing Low-Rank Adaptation via the Perspective of Mixture of Experts</a></h1>
<p><b>Authors:</b> Tuan Truong, Chau Nguyen, Huy Nguyen, Minh Le, Trung Le, Nhat Ho</p>
<p>Abstract: Low-rank adaptation (LoRA) has emerged as a powerful method for fine-tuning large-scale foundation models. Despite its popularity, the theoretical understanding of LoRA has remained limited. This paper presents a theoretical analysis of LoRA by examining its connection to the Mixture of Experts models. Under this framework, we show that simple reparameterizations of the LoRA matrices can notably accelerate the low-rank matrix estimation process. In particular, we prove that reparameterization can reduce the data needed to achieve a desired estimation error from an exponential to a polynomial scale. Motivated by this insight, we propose Reparameterized Low-rank Adaptation (RepLoRA), which incorporates lightweight MLPs to reparameterize the LoRA matrices. Extensive experiments across multiple domains demonstrate that RepLoRA consistently outperforms vanilla LoRA. Notably, with limited data, RepLoRA surpasses LoRA by a margin of up to 40.0% and achieves LoRA's performance with only 30.0% of the training data, highlighting both the theoretical and empirical robustness of our PEFT method.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03044', 51)">Copy Link</button>
<div id="copy-message-51" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03048">The Ensemble Kalman Update is an Empirical Matheron Update</a></h1>
<p><b>Authors:</b> Dan MacKinlay</p>
<p>Abstract: The Ensemble Kalman Filter (EnKF) is a widely used method for data assimilation in high-dimensional systems. In this paper, we show that the ensemble update step of the EnKF is equivalent to an empirical version of the Matheron update popular in the study of Gaussian process regression.
  While this connection is simple, it seems not to be widely known, the literature about each technique seems distinct, and connections between the methods are not exploited. This paper exists to provide an informal introduction to the connection, with the necessary definitions so that it is intelligible to as broad an audience as possible.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03048', 52)">Copy Link</button>
<div id="copy-message-52" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03052">Understanding and Enhancing the Transferability of Jailbreaking Attacks</a></h1>
<p><b>Authors:</b> Runqi Lin, Bo Han, Fengwang Li, Tongling Liu</p>
<p>Abstract: Jailbreaking attacks can effectively manipulate open-source large language models (LLMs) to produce harmful responses. However, these attacks exhibit limited transferability, failing to disrupt proprietary LLMs consistently. To reliably identify vulnerabilities in proprietary LLMs, this work investigates the transferability of jailbreaking attacks by analysing their impact on the model's intent perception. By incorporating adversarial sequences, these attacks can redirect the source LLM's focus away from malicious-intent tokens in the original input, thereby obstructing the model's intent recognition and eliciting harmful responses. Nevertheless, these adversarial sequences fail to mislead the target LLM's intent perception, allowing the target LLM to refocus on malicious-intent tokens and abstain from responding. Our analysis further reveals the inherent distributional dependency within the generated adversarial sequences, whose effectiveness stems from overfitting the source LLM's parameters, resulting in limited transferability to target LLMs. To this end, we propose the Perceived-importance Flatten (PiF) method, which uniformly disperses the model's focus across neutral-intent tokens in the original input, thus obscuring malicious-intent tokens without relying on overfitted adversarial sequences. Extensive experiments demonstrate that PiF provides an effective and efficient red-teaming evaluation for proprietary LLMs.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03052', 53)">Copy Link</button>
<div id="copy-message-53" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03061">Optimal Best Arm Identification with Post-Action Context</a></h1>
<p><b>Authors:</b> Mohammad Shahverdikondori, Amir Mohammad Abouei, Alireza Rezaeimoghadam, Negar Kiyavash</p>
<p>Abstract: We introduce the problem of best arm identification (BAI) with post-action context, a new BAI problem in a stochastic multi-armed bandit environment and the fixed-confidence setting. The problem addresses the scenarios in which the learner receives a $\textit{post-action context}$ in addition to the reward after playing each action. This post-action context provides additional information that can significantly facilitate the decision process. We analyze two different types of the post-action context: (i) $\textit{non-separator}$, where the reward depends on both the action and the context, and (ii) $\textit{separator}$, where the reward depends solely on the context. For both cases, we derive instance-dependent lower bounds on the sample complexity and propose algorithms that asymptotically achieve the optimal sample complexity. For the non-separator setting, we do so by demonstrating that the Track-and-Stop algorithm can be extended to this setting. For the separator setting, we propose a novel sampling rule called $\textit{G-tracking}$, which uses the geometry of the context space to directly track the contexts rather than the actions. Finally, our empirical results showcase the advantage of our approaches compared to the state of the art.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03061', 54)">Copy Link</button>
<div id="copy-message-54" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03092">E-3SFC: Communication-Efficient Federated Learning with Double-way Features Synthesizing</a></h1>
<p><b>Authors:</b> Yuhao Zhou, Yuxin Tian, Mingjia Shi, Yuanxi Li, Yanan Sun, Qing Ye, Jiancheng Lv</p>
<p>Abstract: The exponential growth in model sizes has significantly increased the communication burden in Federated Learning (FL). Existing methods to alleviate this burden by transmitting compressed gradients often face high compression errors, which slow down the model's convergence. To simultaneously achieve high compression effectiveness and lower compression errors, we study the gradient compression problem from a novel perspective. Specifically, we propose a systematical algorithm termed Extended Single-Step Synthetic Features Compressing (E-3SFC), which consists of three sub-components, i.e., the Single-Step Synthetic Features Compressor (3SFC), a double-way compression algorithm, and a communication budget scheduler. First, we regard the process of gradient computation of a model as decompressing gradients from corresponding inputs, while the inverse process is considered as compressing the gradients. Based on this, we introduce a novel gradient compression method termed 3SFC, which utilizes the model itself as a decompressor, leveraging training priors such as model weights and objective functions. 3SFC compresses raw gradients into tiny synthetic features in a single-step simulation, incorporating error feedback to minimize overall compression errors. To further reduce communication overhead, 3SFC is extended to E-3SFC, allowing double-way compression and dynamic communication budget scheduling. Our theoretical analysis under both strongly convex and non-convex conditions demonstrates that 3SFC achieves linear and sub-linear convergence rates with aggregation noise. Extensive experiments across six datasets and six models reveal that 3SFC outperforms state-of-the-art methods by up to 13.4% while reducing communication costs by 111.6 times. These findings suggest that 3SFC can significantly enhance communication efficiency in FL without compromising model performance.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03092', 55)">Copy Link</button>
<div id="copy-message-55" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03095">Reveal the Mystery of DPO: The Connection between DPO and RL Algorithms</a></h1>
<p><b>Authors:</b> Xuerui Su, Yue Wang, Jinhua Zhu, Mingyang Yi, Feng Xu, Zhiming Ma, Yuting Liu</p>
<p>Abstract: With the rapid development of Large Language Models (LLMs), numerous Reinforcement Learning from Human Feedback (RLHF) algorithms have been introduced to improve model safety and alignment with human preferences. These algorithms can be divided into two main frameworks based on whether they require an explicit reward (or value) function for training: actor-critic-based Proximal Policy Optimization (PPO) and alignment-based Direct Preference Optimization (DPO). The mismatch between DPO and PPO, such as DPO's use of a classification loss driven by human-preferred data, has raised confusion about whether DPO should be classified as a Reinforcement Learning (RL) algorithm. To address these ambiguities, we focus on three key aspects related to DPO, RL, and other RLHF algorithms: (1) the construction of the loss function; (2) the target distribution at which the algorithm converges; (3) the impact of key components within the loss function. Specifically, we first establish a unified framework named UDRRA connecting these algorithms based on the construction of their loss functions. Next, we uncover their target policy distributions within this framework. Finally, we investigate the critical components of DPO to understand their impact on the convergence rate. Our work provides a deeper understanding of the relationship between DPO, RL, and other RLHF algorithms, offering new insights for improving existing algorithms.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03095', 56)">Copy Link</button>
<div id="copy-message-56" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03104">Bellman Error Centering</a></h1>
<p><b>Authors:</b> Xingguo Chen, Yu Gong, Shangdong Yang, Wenhao Wang</p>
<p>Abstract: This paper revisits the recently proposed reward centering algorithms including simple reward centering (SRC) and value-based reward centering (VRC), and points out that SRC is indeed the reward centering, while VRC is essentially Bellman error centering (BEC). Based on BEC, we provide the centered fixpoint for tabular value functions, as well as the centered TD fixpoint for linear value function approximation. We design the on-policy CTD algorithm and the off-policy CTDC algorithm, and prove the convergence of both algorithms. Finally, we experimentally validate the stability of our proposed algorithms. Bellman error centering facilitates the extension to various reinforcement learning algorithms.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03104', 57)">Copy Link</button>
<div id="copy-message-57" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03108">Multi-objective methods in Federated Learning: A survey and taxonomy</a></h1>
<p><b>Authors:</b> Maria Hartmann, Gr\'egoire Danoy, Pascal Bouvry</p>
<p>Abstract: The Federated Learning paradigm facilitates effective distributed machine learning in settings where training data is decentralized across multiple clients. As the popularity of the strategy grows, increasingly complex real-world problems emerge, many of which require balancing conflicting demands such as fairness, utility, and resource consumption. Recent works have begun to recognise the use of a multi-objective perspective in answer to this challenge. However, this novel approach of combining federated methods with multi-objective optimisation has never been discussed in the broader context of both fields. In this work, we offer a first clear and systematic overview of the different ways the two fields can be integrated. We propose a first taxonomy on the use of multi-objective methods in connection with Federated Learning, providing a targeted survey of the state-of-the-art and proposing unambiguous labels to categorise contributions. Given the developing nature of this field, our taxonomy is designed to provide a solid basis for further research, capturing existing works while anticipating future additions. Finally, we outline open challenges and possible directions for further research.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03108', 58)">Copy Link</button>
<div id="copy-message-58" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03120">At the Mahakumbh, Faith Met Tragedy: Computational Analysis of Stampede Patterns Using Machine Learning and NLP</a></h1>
<p><b>Authors:</b> Abhinav Pratap</p>
<p>Abstract: This study employs machine learning, historical analysis, and natural language processing (NLP) to examine recurring lethal stampedes at Indias mass religious gatherings, focusing on the 2025 Mahakumbh tragedy in Prayagraj (48+ deaths) and its 1954 predecessor (700+ casualties). Through computational modeling of crowd dynamics and administrative records, it investigates how systemic vulnerabilities contribute to these disasters. Temporal trend analysis identifies persistent choke points, with narrow riverbank access routes linked to 92% of past stampede sites and lethal crowd densities (eight or more persons per square meter) recurring during spiritually significant moments like Mauni Amavasya. NLP analysis of seven decades of inquiry reports reveals cyclical administrative failures, where VIP route prioritization diverted safety resources in both 1954 and 2025, exacerbating fatalities. Statistical modeling demonstrates how ritual urgency overrides risk perception, leading to panic propagation patterns that mirror historical incidents. Findings support the Institutional Amnesia Theory, highlighting how disaster responses remain reactionary rather than preventive. By correlating archival patterns with computational crowd behavior analysis, this study frames stampedes as a collision of infrastructure limitations, socio spiritual urgency, and governance inertia, challenging disaster discourse to address how spiritual economies normalize preventable mortality.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03120', 59)">Copy Link</button>
<div id="copy-message-59" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03123">Disentanglement in Difference: Directly Learning Semantically Disentangled Representations by Maximizing Inter-Factor Differences</a></h1>
<p><b>Authors:</b> Xingshen Zhang, Shuangrong Liu, Xintao Lu, Chaoran Pang, Lin Wang, Bo Yang</p>
<p>Abstract: In this study, Disentanglement in Difference(DiD) is proposed to address the inherent inconsistency between the statistical independence of latent variables and the goal of semantic disentanglement in disentanglement representation learning. Conventional disentanglement methods achieve disentanglement representation by improving statistical independence among latent variables. However, the statistical independence of latent variables does not necessarily imply that they are semantically unrelated, thus, improving statistical independence does not always enhance disentanglement performance. To address the above issue, DiD is proposed to directly learn semantic differences rather than the statistical independence of latent variables. In the DiD, a Difference Encoder is designed to measure the semantic differences; a contrastive loss function is established to facilitate inter-dimensional comparison. Both of them allow the model to directly differentiate and disentangle distinct semantic factors, thereby resolving the inconsistency between statistical independence and semantic disentanglement. Experimental results on the dSprites and 3DShapes datasets demonstrate that the proposed DiD outperforms existing mainstream methods across various disentanglement metrics.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03123', 60)">Copy Link</button>
<div id="copy-message-60" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03143">Machine Learning-Driven Student Performance Prediction for Enhancing Tiered Instruction</a></h1>
<p><b>Authors:</b> Yawen Chen, Jiande Sun, Jinhui Wang, Liang Zhao, Xinmin Song, Linbo Zhai</p>
<p>Abstract: Student performance prediction is one of the most important subjects in educational data mining. As a modern technology, machine learning offers powerful capabilities in feature extraction and data modeling, providing essential support for diverse application scenarios, as evidenced by recent studies confirming its effectiveness in educational data mining. However, despite extensive prediction experiments, machine learning methods have not been effectively integrated into practical teaching strategies, hindering their application in modern education. In addition, massive features as input variables for machine learning algorithms often leads to information redundancy, which can negatively impact prediction accuracy. Therefore, how to effectively use machine learning methods to predict student performance and integrate the prediction results with actual teaching scenarios is a worthy research subject. To this end, this study integrates the results of machine learning-based student performance prediction with tiered instruction, aiming to enhance student outcomes in target course, which is significant for the application of educational data mining in contemporary teaching scenarios. Specifically, we collect original educational data and perform feature selection to reduce information redundancy. Then, the performance of five representative machine learning methods is analyzed and discussed with Random Forest showing the best performance. Furthermore, based on the results of the classification of students, tiered instruction is applied accordingly, and different teaching objectives and contents are set for all levels of students. The comparison of teaching outcomes between the control and experimental classes, along with the analysis of questionnaire results, demonstrates the effectiveness of the proposed framework.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03143', 61)">Copy Link</button>
<div id="copy-message-61" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03146">Symmetry-Aware Bayesian Flow Networks for Crystal Generation</a></h1>
<p><b>Authors:</b> Laura Ruple, Luca Torresi, Henrik Schopmans, Pascal Friederich</p>
<p>Abstract: The discovery of new crystalline materials is essential to scientific and technological progress. However, traditional trial-and-error approaches are inefficient due to the vast search space. Recent advancements in machine learning have enabled generative models to predict new stable materials by incorporating structural symmetries and to condition the generation on desired properties. In this work, we introduce SymmBFN, a novel symmetry-aware Bayesian Flow Network (BFN) for crystalline material generation that accurately reproduces the distribution of space groups found in experimentally observed crystals. SymmBFN substantially improves efficiency, generating stable structures at least 50 times faster than the next-best method. Furthermore, we demonstrate its capability for property-conditioned generation, enabling the design of materials with tailored properties. Our findings establish BFNs as an effective tool for accelerating the discovery of crystalline materials.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03146', 62)">Copy Link</button>
<div id="copy-message-62" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03159">PICBench: Benchmarking LLMs for Photonic Integrated Circuits Design</a></h1>
<p><b>Authors:</b> Yuchao Wu, Xiaofei Yu, Hao Chen, Yang Luo, Yeyu Tong, Yuzhe Ma</p>
<p>Abstract: While large language models (LLMs) have shown remarkable potential in automating various tasks in digital chip design, the field of Photonic Integrated Circuits (PICs)-a promising solution to advanced chip designs-remains relatively unexplored in this context. The design of PICs is time-consuming and prone to errors due to the extensive and repetitive nature of code involved in photonic chip design. In this paper, we introduce PICBench, the first benchmarking and evaluation framework specifically designed to automate PIC design generation using LLMs, where the generated output takes the form of a netlist. Our benchmark consists of dozens of meticulously crafted PIC design problems, spanning from fundamental device designs to more complex circuit-level designs. It automatically evaluates both the syntax and functionality of generated PIC designs by comparing simulation outputs with expert-written solutions, leveraging an open-source simulator. We evaluate a range of existing LLMs, while also conducting comparative tests on various prompt engineering techniques to enhance LLM performance in automated PIC design. The results reveal the challenges and potential of LLMs in the PIC design domain, offering insights into the key areas that require further research and development to optimize automation in this field. Our benchmark and evaluation code is available at https://github.com/PICDA/PICBench.</p>
<p>URLs: <a href="https://github.com/PICDA/PICBench.">https://github.com/PICDA/PICBench.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03159, https://github.com/PICDA/PICBench.', 63)">Copy Link</button>
<div id="copy-message-63" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03201">SpaceGNN: Multi-Space Graph Neural Network for Node Anomaly Detection with Extremely Limited Labels</a></h1>
<p><b>Authors:</b> Xiangyu Dong, Xingyi Zhang, Lei Chen, Mingxuan Yuan, Sibo Wang</p>
<p>Abstract: Node Anomaly Detection (NAD) has gained significant attention in the deep learning community due to its diverse applications in real-world scenarios. Existing NAD methods primarily embed graphs within a single Euclidean space, while overlooking the potential of non-Euclidean spaces. Besides, to address the prevalent issue of limited supervision in real NAD tasks, previous methods tend to leverage synthetic data to collect auxiliary information, which is not an effective solution as shown in our experiments. To overcome these challenges, we introduce a novel SpaceGNN model designed for NAD tasks with extremely limited labels. Specifically, we provide deeper insights into a task-relevant framework by empirically analyzing the benefits of different spaces for node representations, based on which, we design a Learnable Space Projection function that effectively encodes nodes into suitable spaces. Besides, we introduce the concept of weighted homogeneity, which we empirically and theoretically validate as an effective coefficient during information propagation. This concept inspires the design of the Distance Aware Propagation module. Furthermore, we propose the Multiple Space Ensemble module, which extracts comprehensive information for NAD under conditions of extremely limited supervision. Our findings indicate that this module is more beneficial than data augmentation techniques for NAD. Extensive experiments conducted on 9 real datasets confirm the superiority of SpaceGNN, which outperforms the best rival by an average of 8.55% in AUC and 4.31% in F1 scores. Our code is available at https://github.com/xydong127/SpaceGNN.</p>
<p>URLs: <a href="https://github.com/xydong127/SpaceGNN.">https://github.com/xydong127/SpaceGNN.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03201, https://github.com/xydong127/SpaceGNN.', 64)">Copy Link</button>
<div id="copy-message-64" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03227">Adversarial Dependence Minimization</a></h1>
<p><b>Authors:</b> Pierre-Fran\c{c}ois De Plaen, Tinne Tuytelaars, Marc Proesmans, Luc Van Gool</p>
<p>Abstract: Many machine learning techniques rely on minimizing the covariance between output feature dimensions to extract minimally redundant representations from data. However, these methods do not eliminate all dependencies/redundancies, as linearly uncorrelated variables can still exhibit nonlinear relationships. This work provides a differentiable and scalable algorithm for dependence minimization that goes beyond linear pairwise decorrelation. Our method employs an adversarial game where small networks identify dependencies among feature dimensions, while the encoder exploits this information to reduce dependencies. We provide empirical evidence of the algorithm's convergence and demonstrate its utility in three applications: extending PCA to nonlinear decorrelation, improving the generalization of image classification methods, and preventing dimensional collapse in self-supervised representation learning.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03227', 65)">Copy Link</button>
<div id="copy-message-65" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03231">The Other Side of the Coin: Unveiling the Downsides of Model Aggregation in Federated Learning from a Layer-peeled Perspective</a></h1>
<p><b>Authors:</b> Guogang Zhu, Xuefeng Liu, Jianwei Niu, Shaojie Tang, Xinghao Wu</p>
<p>Abstract: In federated learning (FL), model aggregation is a critical step by which multiple clients share their knowledge with one another. However, it is also widely recognized that the aggregated model, when sent back to each client, performs poorly on local data until after several rounds of local training. This temporary performance drop can potentially slow down the convergence of the FL model. Most research in FL regards this performance drop as an inherent cost of knowledge sharing among clients and does not give it special attention. While some studies directly focus on designing techniques to alleviate the issue, an in-depth investigation of the reasons behind this performance drop has yet to be conducted.To address this gap, we conduct a layer-peeled analysis of model aggregation across various datasets and model architectures. Our findings reveal that the performance drop can be attributed to two major consequences of the aggregation process: (1) it disrupts feature variability suppression in deep neural networks (DNNs), and (2) it weakens the coupling between features and subsequent parameters.Based on these findings, we propose several simple yet effective strategies to mitigate the negative impacts of model aggregation while still enjoying the benefit it brings. To the best of our knowledge, our work is the first to conduct a layer-peeled analysis of model aggregation, potentially paving the way for the development of more effective FL algorithms.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03231', 66)">Copy Link</button>
<div id="copy-message-66" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03236">Pioneer: Physics-informed Riemannian Graph ODE for Entropy-increasing Dynamics</a></h1>
<p><b>Authors:</b> Li Sun, Ziheng Zhang, Zixi Wang, Yujie Wang, Qiqi Wan, Hao Li, Hao Peng, Philip S. Yu</p>
<p>Abstract: Dynamic interacting system modeling is important for understanding and simulating real world systems. The system is typically described as a graph, where multiple objects dynamically interact with each other and evolve over time. In recent years, graph Ordinary Differential Equations (ODE) receive increasing research attentions. While achieving encouraging results, existing solutions prioritize the traditional Euclidean space, and neglect the intrinsic geometry of the system and physics laws, e.g., the principle of entropy increasing. The limitations above motivate us to rethink the system dynamics from a fresh perspective of Riemannian geometry, and pose a more realistic problem of physics-informed dynamic system modeling, considering the underlying geometry and physics law for the first time. In this paper, we present a novel physics-informed Riemannian graph ODE for a wide range of entropy-increasing dynamic systems (termed as Pioneer). In particular, we formulate a differential system on the Riemannian manifold, where a manifold-valued graph ODE is governed by the proposed constrained Ricci flow, and a manifold preserving Gyro-transform aware of system geometry. Theoretically, we report the provable entropy non-decreasing of our formulation, obeying the physics laws. Empirical results show the superiority of Pioneer on real datasets.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03236', 67)">Copy Link</button>
<div id="copy-message-67" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03244">Analysis of Value Iteration Through Absolute Probability Sequences</a></h1>
<p><b>Authors:</b> Arsenii Mustafin, Sebastien Colla, Alex Olshevsky, Ioannis Ch. Paschalidis</p>
<p>Abstract: Value Iteration is a widely used algorithm for solving Markov Decision Processes (MDPs). While previous studies have extensively analyzed its convergence properties, they primarily focus on convergence with respect to the infinity norm. In this work, we use absolute probability sequences to develop a new line of analysis and examine the algorithm's convergence in terms of the $L^2$ norm, offering a new perspective on its behavior and performance.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03244', 68)">Copy Link</button>
<div id="copy-message-68" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03245">Calibrated Unsupervised Anomaly Detection in Multivariate Time-series using Reinforcement Learning</a></h1>
<p><b>Authors:</b> Saba Sanami, Amir G. Aghdam</p>
<p>Abstract: This paper investigates unsupervised anomaly detection in multivariate time-series data using reinforcement learning (RL) in the latent space of an autoencoder. A significant challenge is the limited availability of anomalous data, often leading to misclassifying anomalies as normal events, thus raising false negatives. RL can help overcome this limitation by promoting exploration and balancing exploitation during training, effectively preventing overfitting. Wavelet analysis is also utilized to enhance anomaly detection, enabling time-series data decomposition into both time and frequency domains. This approach captures anomalies at multiple resolutions, with wavelet coefficients extracted to detect both sudden and subtle shifts in the data, thereby refining the anomaly detection process. We calibrate the decision boundary by generating synthetic anomalies and embedding a supervised framework within the model. This supervised element aids the unsupervised learning process by fine-tuning the decision boundary and increasing the model's capacity to distinguish between normal and anomalous patterns effectively.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03245', 69)">Copy Link</button>
<div id="copy-message-69" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03251">RiemannGFM: Learning a Graph Foundation Model from Riemannian Geometry</a></h1>
<p><b>Authors:</b> Li Sun, Zhenhao Huang, Suyang Zhou, Qiqi Wan, Hao Peng, Philip Yu</p>
<p>Abstract: The foundation model has heralded a new era in artificial intelligence, pretraining a single model to offer cross-domain transferability on different datasets. Graph neural networks excel at learning graph data, the omnipresent non-Euclidean structure, but often lack the generalization capacity. Hence, graph foundation model is drawing increasing attention, and recent efforts have been made to leverage Large Language Models. On the one hand, existing studies primarily focus on text-attributed graphs, while a wider range of real graphs do not contain fruitful textual attributes. On the other hand, the sequential graph description tailored for the Large Language Model neglects the structural complexity, which is a predominant characteristic of the graph. Such limitations motivate an important question: Can we go beyond Large Language Models, and pretrain a universal model to learn the structural knowledge for any graph? The answer in the language or vision domain is a shared vocabulary. We observe the fact that there also exist shared substructures underlying graph domain, and thereby open a new opportunity of graph foundation model with structural vocabulary. The key innovation is the discovery of a simple yet effective structural vocabulary of trees and cycles, and we explore its inherent connection to Riemannian geometry. Herein, we present a universal pretraining model, RiemannGFM. Concretely, we first construct a novel product bundle to incorporate the diverse geometries of the vocabulary. Then, on this constructed space, we stack Riemannian layers where the structural vocabulary, regardless of specific graph, is learned in Riemannian manifold offering cross-domain transferability. Extensive experiments show the effectiveness of RiemannGFM on a diversity of real graphs.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03251', 70)">Copy Link</button>
<div id="copy-message-70" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03264">General Time-series Model for Universal Knowledge Representation of Multivariate Time-Series data</a></h1>
<p><b>Authors:</b> Cheng He, Xu Huang, Gangwei Jiang, Zhaoyi Li, Defu Lian, Hong Xie, Enhong Chen, Xijie Liang, Zengrong Zheng</p>
<p>Abstract: Universal knowledge representation is a central problem for multivariate time series(MTS) foundation models and yet remains open. This paper investigates this problem from the first principle and it makes four folds of contributions. First, a new empirical finding is revealed: time series with different time granularities (or corresponding frequency resolutions) exhibit distinct joint distributions in the frequency domain. This implies a crucial aspect of learning universal knowledge, one that has been overlooked by previous studies. Second, a novel Fourier knowledge attention mechanism is proposed to enable learning time granularity-aware representations from both the temporal and frequency domains. Third, an autoregressive blank infilling pre-training framework is incorporated to time series analysis for the first time, leading to a generative tasks agnostic pre-training strategy. To this end, we develop the General Time-series Model (GTM), a unified MTS foundation model that addresses the limitation of contemporary time series models, which often require token, pre-training, or model-level customizations for downstream tasks adaption. Fourth, extensive experiments show that GTM outperforms state-of-the-art (SOTA) methods across all generative tasks, including long-term forecasting, anomaly detection, and imputation.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03264', 71)">Copy Link</button>
<div id="copy-message-71" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03302">MAP Image Recovery with Guarantees using Locally Convex Multi-Scale Energy (LC-MUSE) Model</a></h1>
<p><b>Authors:</b> Jyothi Rikhab Chand, Mathews Jacob</p>
<p>Abstract: We propose a multi-scale deep energy model that is strongly convex in the local neighbourhood around the data manifold to represent its probability density, with application in inverse problems. In particular, we represent the negative log-prior as a multi-scale energy model parameterized by a Convolutional Neural Network (CNN). We restrict the gradient of the CNN to be locally monotone, which constrains the model as a Locally Convex Multi-Scale Energy (LC-MuSE). We use the learned energy model in image-based inverse problems, where the formulation offers several desirable properties: i) uniqueness of the solution, ii) convergence guarantees to a minimum of the inverse problem, and iii) robustness to input perturbations. In the context of parallel Magnetic Resonance (MR) image reconstruction, we show that the proposed method performs better than the state-of-the-art convex regularizers, while the performance is comparable to plug-and-play regularizers and end-to-end trained methods.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03302', 72)">Copy Link</button>
<div id="copy-message-72" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03304">Harmony in Divergence: Towards Fast, Accurate, and Memory-efficient Zeroth-order LLM Fine-tuning</a></h1>
<p><b>Authors:</b> Qitao Tan, Jun Liu, Zheng Zhan, Caiwei Ding, Yanzhi Wang, Jin Lu, Geng Yuan</p>
<p>Abstract: Large language models (LLMs) excel across various tasks, but standard first-order (FO) fine-tuning demands considerable memory, significantly limiting real-world deployment. Recently, zeroth-order (ZO) optimization stood out as a promising memory-efficient training paradigm, avoiding backward passes and relying solely on forward passes for gradient estimation, making it attractive for resource-constrained scenarios. However, ZO method lags far behind FO method in both convergence speed and accuracy. To bridge the gap, we introduce a novel layer-wise divergence analysis that uncovers the distinct update pattern of FO and ZO optimization. Aiming to resemble the learning capacity of FO method from the findings, we propose \textbf{Di}vergence-driven \textbf{Z}eroth-\textbf{O}rder (\textbf{DiZO}) optimization. DiZO conducts divergence-driven layer adaptation by incorporating projections to ZO updates, generating diverse-magnitude updates precisely scaled to layer-wise individual optimization needs. Our results demonstrate that DiZO significantly reduces the needed iterations for convergence without sacrificing throughput, cutting training GPU hours by up to 48\% on various datasets. Moreover, DiZO consistently outperforms the representative ZO baselines in fine-tuning RoBERTa-large, OPT-series, and Llama-series on downstream tasks and, in some cases, even surpasses memory-intensive FO fine-tuning.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03304', 73)">Copy Link</button>
<div id="copy-message-73" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03340">Interaction-Aware Gaussian Weighting for Clustered Federated Learning</a></h1>
<p><b>Authors:</b> Alessandro Licciardi, Davide Leo, Eros Fan\'i, Barbara Caputo, Marco Ciccone</p>
<p>Abstract: Federated Learning (FL) emerged as a decentralized paradigm to train models while preserving privacy. However, conventional FL struggles with data heterogeneity and class imbalance, which degrade model performance. Clustered FL balances personalization and decentralized training by grouping clients with analogous data distributions, enabling improved accuracy while adhering to privacy constraints. This approach effectively mitigates the adverse impact of heterogeneity in FL. In this work, we propose a novel clustered FL method, FedGWC (Federated Gaussian Weighting Clustering), which groups clients based on their data distribution, allowing training of a more robust and personalized model on the identified clusters. FedGWC identifies homogeneous clusters by transforming individual empirical losses to model client interactions with a Gaussian reward mechanism. Additionally, we introduce the Wasserstein Adjusted Score, a new clustering metric for FL to evaluate cluster cohesion with respect to the individual class distribution. Our experiments on benchmark datasets show that FedGWC outperforms existing FL algorithms in cluster quality and classification accuracy, validating the efficacy of our approach.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03340', 74)">Copy Link</button>
<div id="copy-message-74" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03349">Robust Autonomy Emerges from Self-Play</a></h1>
<p><b>Authors:</b> Marco Cusumano-Towner, David Hafner, Alex Hertzberg, Brody Huval, Aleksei Petrenko, Eugene Vinitsky, Erik Wijmans, Taylor Killian, Stuart Bowers, Ozan Sener, Philipp Kr\"ahenb\"uhl, Vladlen Koltun</p>
<p>Abstract: Self-play has powered breakthroughs in two-player and multi-player games. Here we show that self-play is a surprisingly effective strategy in another domain. We show that robust and naturalistic driving emerges entirely from self-play in simulation at unprecedented scale -- 1.6~billion~km of driving. This is enabled by Gigaflow, a batched simulator that can synthesize and train on 42 years of subjective driving experience per hour on a single 8-GPU node. The resulting policy achieves state-of-the-art performance on three independent autonomous driving benchmarks. The policy outperforms the prior state of the art when tested on recorded real-world scenarios, amidst human drivers, without ever seeing human data during training. The policy is realistic when assessed against human references and achieves unprecedented robustness, averaging 17.5 years of continuous driving between incidents in simulation.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03349', 75)">Copy Link</button>
<div id="copy-message-75" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03364">Scaling laws in wearable human activity recognition</a></h1>
<p><b>Authors:</b> Tom Hoddes, Alex Bijamov, Saket Joshi, Daniel Roggen, Ali Etemad, Robert Harle, David Racz</p>
<p>Abstract: Many deep architectures and self-supervised pre-training techniques have been proposed for human activity recognition (HAR) from wearable multimodal sensors. Scaling laws have the potential to help move towards more principled design by linking model capacity with pre-training data volume. Yet, scaling laws have not been established for HAR to the same extent as in language and vision. By conducting an exhaustive grid search on both amount of pre-training data and Transformer architectures, we establish the first known scaling laws for HAR. We show that pre-training loss scales with a power law relationship to amount of data and parameter count and that increasing the number of users in a dataset results in a steeper improvement in performance than increasing data per user, indicating that diversity of pre-training data is important, which contrasts to some previously reported findings in self-supervised HAR. We show that these scaling laws translate to downstream performance improvements on three HAR benchmark datasets of postures, modes of locomotion and activities of daily living: UCI HAR and WISDM Phone and WISDM Watch. Finally, we suggest some previously published works should be revisited in light of these scaling laws with more adequate model capacities.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03364', 76)">Copy Link</button>
<div id="copy-message-76" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03366">Rethinking Approximate Gaussian Inference in Classification</a></h1>
<p><b>Authors:</b> B\'alint Mucs\'anyi, Natha\"el Da Costa, Philipp Hennig</p>
<p>Abstract: In classification tasks, softmax functions are ubiquitously used as output activations to produce predictive probabilities. Such outputs only capture aleatoric uncertainty. To capture epistemic uncertainty, approximate Gaussian inference methods have been proposed, which output Gaussian distributions over the logit space. Predictives are then obtained as the expectations of the Gaussian distributions pushed forward through the softmax. However, such softmax Gaussian integrals cannot be solved analytically, and Monte Carlo (MC) approximations can be costly and noisy. We propose a simple change in the learning objective which allows the exact computation of predictives and enjoys improved training dynamics, with no runtime or memory overhead. This framework is compatible with a family of output activation functions that includes the softmax, as well as element-wise normCDF and sigmoid. Moreover, it allows for approximating the Gaussian pushforwards with Dirichlet distributions by analytic moment matching. We evaluate our approach combined with several approximate Gaussian inference methods (Laplace, HET, SNGP) on large- and small-scale datasets (ImageNet, CIFAR-10), demonstrating improved uncertainty quantification capabilities compared to softmax MC sampling. Code is available at https://github.com/bmucsanyi/probit.</p>
<p>URLs: <a href="https://github.com/bmucsanyi/probit.">https://github.com/bmucsanyi/probit.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03366, https://github.com/bmucsanyi/probit.', 77)">Copy Link</button>
<div id="copy-message-77" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03367">SyMANTIC: An Efficient Symbolic Regression Method for Interpretable and Parsimonious Model Discovery in Science and Beyond</a></h1>
<p><b>Authors:</b> Madhav R. Muthyala, Farshud Sorourifar, You Peng, Joel A. Paulson</p>
<p>Abstract: Symbolic regression (SR) is an emerging branch of machine learning focused on discovering simple and interpretable mathematical expressions from data. Although a wide-variety of SR methods have been developed, they often face challenges such as high computational cost, poor scalability with respect to the number of input dimensions, fragility to noise, and an inability to balance accuracy and complexity. This work introduces SyMANTIC, a novel SR algorithm that addresses these challenges. SyMANTIC efficiently identifies (potentially several) low-dimensional descriptors from a large set of candidates (from $\sim 10^5$ to $\sim 10^{10}$ or more) through a unique combination of mutual information-based feature selection, adaptive feature expansion, and recursively applied $\ell_0$-based sparse regression. In addition, it employs an information-theoretic measure to produce an approximate set of Pareto-optimal equations, each offering the best-found accuracy for a given complexity. Furthermore, our open-source implementation of SyMANTIC, built on the PyTorch ecosystem, facilitates easy installation and GPU acceleration. We demonstrate the effectiveness of SyMANTIC across a range of problems, including synthetic examples, scientific benchmarks, real-world material property predictions, and chaotic dynamical system identification from small datasets. Extensive comparisons show that SyMANTIC uncovers similar or more accurate models at a fraction of the cost of existing SR methods.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03367', 78)">Copy Link</button>
<div id="copy-message-78" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03383">Transformers and Their Roles as Time Series Foundation Models</a></h1>
<p><b>Authors:</b> Dennis Wu, Yihan He, Yuan Cao, Jianqing Fan, Han Liu</p>
<p>Abstract: We give a comprehensive analysis of transformers as time series foundation models, focusing on their approximation and generalization capabilities. First, we demonstrate that there exist transformers that fit an autoregressive model on input univariate time series via gradient descent. We then analyze MOIRAI, a multivariate time series foundation model capable of handling an arbitrary number of covariates. We prove that it is capable of automatically fitting autoregressive models with an arbitrary number of covariates, offering insights into its design and empirical success. For generalization, we establish bounds for pretraining when the data satisfies Dobrushin's condition. Experiments support our theoretical findings, highlighting the efficacy of transformers as time series foundation models.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03383', 79)">Copy Link</button>
<div id="copy-message-79" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03386">A Structured Reasoning Framework for Unbalanced Data Classification Using Probabilistic Models</a></h1>
<p><b>Authors:</b> Junliang Du, Shiyu Dou, Bohuan Yang, Jiacheng Hu, Tai An</p>
<p>Abstract: This paper studies a Markov network model for unbalanced data, aiming to solve the problems of classification bias and insufficient minority class recognition ability of traditional machine learning models in environments with uneven class distribution. By constructing joint probability distribution and conditional dependency, the model can achieve global modeling and reasoning optimization of sample categories. The study introduced marginal probability estimation and weighted loss optimization strategies, combined with regularization constraints and structured reasoning methods, effectively improving the generalization ability and robustness of the model. In the experimental stage, a real credit card fraud detection dataset was selected and compared with models such as logistic regression, support vector machine, random forest and XGBoost. The experimental results show that the Markov network performs well in indicators such as weighted accuracy, F1 score, and AUC-ROC, significantly outperforming traditional classification models, demonstrating its strong decision-making ability and applicability in unbalanced data scenarios. Future research can focus on efficient model training, structural optimization, and deep learning integration in large-scale unbalanced data environments and promote its wide application in practical applications such as financial risk control, medical diagnosis, and intelligent monitoring.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03386', 80)">Copy Link</button>
<div id="copy-message-80" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03391">Explain Yourself, Briefly! Self-Explaining Neural Networks with Concise Sufficient Reasons</a></h1>
<p><b>Authors:</b> Shahaf Bassan, Shlomit Gur, Ron Eliav</p>
<p>Abstract: Minimal sufficient reasons represent a prevalent form of explanation - the smallest subset of input features which, when held constant at their corresponding values, ensure that the prediction remains unchanged. Previous post-hoc methods attempt to obtain such explanations but face two main limitations: (1) Obtaining these subsets poses a computational challenge, leading most scalable methods to converge towards suboptimal, less meaningful subsets; (2) These methods heavily rely on sampling out-of-distribution input assignments, potentially resulting in counterintuitive behaviors. To tackle these limitations, we propose in this work a self-supervised training approach, which we term *sufficient subset training* (SST). Using SST, we train models to generate concise sufficient reasons for their predictions as an integral part of their output. Our results indicate that our framework produces succinct and faithful subsets substantially more efficiently than competing post-hoc methods, while maintaining comparable predictive performance.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03391', 81)">Copy Link</button>
<div id="copy-message-81" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03393">CAPE: Covariate-Adjusted Pre-Training for Epidemic Time Series Forecasting</a></h1>
<p><b>Authors:</b> Zewen Liu, Juntong Ni, Max S. Y. Lau, Wei Jin</p>
<p>Abstract: Accurate forecasting of epidemic infection trajectories is crucial for safeguarding public health. However, limited data availability during emerging outbreaks and the complex interaction between environmental factors and disease dynamics present significant challenges for effective forecasting. In response, we introduce CAPE, a novel epidemic pre-training framework designed to harness extensive disease datasets from diverse regions and integrate environmental factors directly into the modeling process for more informed decision-making on downstream diseases. Based on a covariate adjustment framework, CAPE utilizes pre-training combined with hierarchical environment contrasting to identify universal patterns across diseases while estimating latent environmental influences. We have compiled a diverse collection of epidemic time series datasets and validated the effectiveness of CAPE under various evaluation scenarios, including full-shot, few-shot, zero-shot, cross-location, and cross-disease settings, where it outperforms the leading baseline by an average of 9.9% in full-shot and 14.3% in zero-shot settings. The code will be released upon acceptance.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03393', 82)">Copy Link</button>
<div id="copy-message-82" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03395">Benchmarking Time Series Forecasting Models: From Statistical Techniques to Foundation Models in Real-World Applications</a></h1>
<p><b>Authors:</b> Issar Arab, Rodrigo Benitez</p>
<p>Abstract: Time series forecasting is essential for operational intelligence in the hospitality industry, and particularly challenging in large-scale, distributed systems. This study evaluates the performance of statistical, machine learning (ML), deep learning, and foundation models in forecasting hourly sales over a 14-day horizon using real-world data from a network of thousands of restaurants across Germany. The forecasting solution includes features such as weather conditions, calendar events, and time-of-day patterns. Results demonstrate the strong performance of ML-based meta-models and highlight the emerging potential of foundation models like Chronos and TimesFM, which deliver competitive performance with minimal feature engineering, leveraging only the pre-trained model (zero-shot inference). Additionally, a hybrid PySpark-Pandas approach proves to be a robust solution for achieving horizontal scalability in large-scale deployments.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03395', 83)">Copy Link</button>
<div id="copy-message-83" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03396">Accurate AI-Driven Emergency Vehicle Location Tracking in Healthcare ITS Digital Twin</a></h1>
<p><b>Authors:</b> Sarah Al-Shareeda, Yasar Celik, Bilge Bilgili, Ahmed Al-Dubai, Berk Canberk</p>
<p>Abstract: Creating a Digital Twin (DT) for Healthcare Intelligent Transportation Systems (HITS) is a hot research trend focusing on enhancing HITS management, particularly in emergencies where ambulance vehicles must arrive at the crash scene on time and track their real-time location is crucial to the medical authorities. Despite the claim of real-time representation, a temporal misalignment persists between the physical and virtual domains, leading to discrepancies in the ambulance's location representation. This study proposes integrating AI predictive models, specifically Support Vector Regression (SVR) and Deep Neural Networks (DNN), within a constructed mock DT data pipeline framework to anticipate the medical vehicle's next location in the virtual world. These models align virtual representations with their physical counterparts, i.e., metaphorically offsetting the synchronization delay between the two worlds. Trained meticulously on a historical geospatial dataset, SVR and DNN exhibit exceptional prediction accuracy in MATLAB and Python environments. Through various testing scenarios, we visually demonstrate the efficacy of our methodology, showcasing SVR and DNN's key role in significantly reducing the witnessed gap within the HITS's DT. This transformative approach enhances real-time synchronization in emergency HITS by approximately 88% to 93%.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03396', 84)">Copy Link</button>
<div id="copy-message-84" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03405">Deep Clustering via Probabilistic Ratio-Cut Optimization</a></h1>
<p><b>Authors:</b> Ayoub Ghriss, Claire Monteleoni</p>
<p>Abstract: We propose a novel approach for optimizing the graph ratio-cut by modeling the binary assignments as random variables. We provide an upper bound on the expected ratio-cut, as well as an unbiased estimate of its gradient, to learn the parameters of the assignment variables in an online setting. The clustering resulting from our probabilistic approach (PRCut) outperforms the Rayleigh quotient relaxation of the combinatorial problem, its online learning extensions, and several widely used methods. We demonstrate that the PRCut clustering closely aligns with the similarity measure and can perform as well as a supervised classifier when label-based similarities are provided. This novel approach can leverage out-of-the-box self-supervised representations to achieve competitive performance and serve as an evaluation method for the quality of these representations.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03405', 85)">Copy Link</button>
<div id="copy-message-85" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03407">Detecting Strategic Deception Using Linear Probes</a></h1>
<p><b>Authors:</b> Nicholas Goldowsky-Dill, Bilal Chughtai, Stefan Heimersheim, Marius Hobbhahn</p>
<p>Abstract: AI models might use deceptive strategies as part of scheming or misaligned behaviour. Monitoring outputs alone is insufficient, since the AI might produce seemingly benign outputs while their internal reasoning is misaligned. We thus evaluate if linear probes can robustly detect deception by monitoring model activations. We test two probe-training datasets, one with contrasting instructions to be honest or deceptive (following Zou et al., 2023) and one of responses to simple roleplaying scenarios. We test whether these probes generalize to realistic settings where Llama-3.3-70B-Instruct behaves deceptively, such as concealing insider trading (Scheurer et al., 2023) and purposely underperforming on safety evaluations (Benton et al., 2024). We find that our probe distinguishes honest and deceptive responses with AUROCs between 0.96 and 0.999 on our evaluation datasets. If we set the decision threshold to have a 1% false positive rate on chat data not related to deception, our probe catches 95-99% of the deceptive responses. Overall we think white-box probes are promising for future monitoring systems, but current performance is insufficient as a robust defence against deception. Our probes' outputs can be viewed at data.apolloresearch.ai/dd and our code at github.com/ApolloResearch/deception-detection.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03407', 86)">Copy Link</button>
<div id="copy-message-86" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03417">From Features to Transformers: Redefining Ranking for Scalable Impact</a></h1>
<p><b>Authors:</b> Fedor Borisyuk, Lars Hertel, Ganesh Parameswaran, Gaurav Srivastava, Sudarshan Srinivasa Ramanujam, Borja Ocejo, Peng Du, Andrei Akterskii, Neil Daftary, Shao Tang, Daqi Sun, Qiang Charles Xiao, Deepesh Nathani, Mohit Kothari, Yun Dai, Aman Gupta</p>
<p>Abstract: We present LiGR, a large-scale ranking framework developed at LinkedIn that brings state-of-the-art transformer-based modeling architectures into production. We introduce a modified transformer architecture that incorporates learned normalization and simultaneous set-wise attention to user history and ranked items. This architecture enables several breakthrough achievements, including: (1) the deprecation of most manually designed feature engineering, outperforming the prior state-of-the-art system using only few features (compared to hundreds in the baseline), (2) validation of the scaling law for ranking systems, showing improved performance with larger models, more training data, and longer context sequences, and (3) simultaneous joint scoring of items in a set-wise manner, leading to automated improvements in diversity. To enable efficient serving of large ranking models, we describe techniques to scale inference effectively using single-pass processing of user history and set-wise attention. We also summarize key insights from various ablation studies and A/B tests, highlighting the most impactful technical approaches.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03417', 87)">Copy Link</button>
<div id="copy-message-87" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03424">Prediction of the Most Fire-Sensitive Point in Building Structures with Differentiable Agents for Thermal Simulators</a></h1>
<p><b>Authors:</b> Yuan Xinjie, Khalid M. Mosalam</p>
<p>Abstract: Fire safety is a critical area of research in civil and mechanical engineering, particularly in ensuring the structural stability of buildings during fire events. The Most Fire-Sensitive Point (MFSP) in a structure is the location where a fire would cause the greatest impact on structural stability. Accurate prediction of the MFSP is vital for streamlining structural assessments and optimizing the design process. This paper presents a novel framework for MFSP prediction using a neural network-based approach that integrates fire dynamics and finite element analysis through a differentiable agent model. The framework focuses on predicting the Maximum Interstory Drift Ratio (MIDR), a key indicator of structural performance under fire conditions. By leveraging the differentiable agent model, we efficiently generate labeled data for MFSP and directly train a predictor for this critical metric. To achieve this, we generated extensive simulation data encompassing structural and fire scenarios and employed graph neural networks to represent the building structures. Transfer learning was applied to optimize the training process, and an edge update mechanism was introduced to dynamically adjust edge attributes, reflecting property changes under fire conditions. The proposed model was rigorously evaluated on simulation data, demonstrating strong performance in accurately predicting both MIDR and MFSP, thus advancing fire safety analysis for building structures.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03424', 88)">Copy Link</button>
<div id="copy-message-88" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03450">A Schema-Guided Reason-while-Retrieve framework for Reasoning on Scene Graphs with Large-Language-Models (LLMs)</a></h1>
<p><b>Authors:</b> Yiye Chen, Harpreet Sawhney, Nicholas Gyd\'e, Yanan Jian, Jack Saunders, Patricio Vela, Ben Lundell</p>
<p>Abstract: Scene graphs have emerged as a structured and serializable environment representation for grounded spatial reasoning with Large Language Models (LLMs). In this work, we propose SG-RwR, a Schema-Guided Retrieve-while-Reason framework for reasoning and planning with scene graphs. Our approach employs two cooperative, code-writing LLM agents: a (1) Reasoner for task planning and information queries generation, and a (2) Retriever for extracting corresponding graph information following the queries. Two agents collaborate iteratively, enabling sequential reasoning and adaptive attention to graph information. Unlike prior works, both agents are prompted only with the scene graph schema rather than the full graph data, which reduces the hallucination by limiting input tokens, and drives the Reasoner to generate reasoning trace abstractly.Following the trace, the Retriever programmatically query the scene graph data based on the schema understanding, allowing dynamic and global attention on the graph that enhances alignment between reasoning and retrieval. Through experiments in multiple simulation environments, we show that our framework surpasses existing LLM-based approaches in numerical Q\&A and planning tasks, and can benefit from task-level few-shot examples, even in the absence of agent-level demonstrations. Project code will be released.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03450', 89)">Copy Link</button>
<div id="copy-message-89" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03460">Adapt-Pruner: Adaptive Structural Pruning for Efficient Small Language Model Training</a></h1>
<p><b>Authors:</b> Boyao Wang, Rui Pan, Shizhe Diao, Xingyuan Pan, Jipeng Zhang, Renjie Pi, Tong Zhang</p>
<p>Abstract: Small language models (SLMs) have attracted considerable attention from both academia and industry due to their broad range of applications in edge devices. To obtain SLMs with strong performance, conventional approaches either pre-train the models from scratch, which incurs substantial computational costs, or compress/prune existing large language models (LLMs), which results in performance drops and falls short in comparison to pre-training. In this paper, we investigate the family of acceleration methods that involve both structured pruning and model training. We found 1) layer-wise adaptive pruning (Adapt-Pruner) is extremely effective in LLMs and yields significant improvements over existing pruning techniques, 2) adaptive pruning equipped with further training leads to models comparable to those pre-training from scratch, 3) incremental pruning brings non-trivial performance gain by interleaving pruning with training and only removing a small portion of neurons ($\sim$5%) at a time. Experimental results on LLaMA-3.1-8B demonstrate that Adapt-Pruner outperforms conventional pruning methods, such as LLM-Pruner, FLAP, and SliceGPT, by an average of 1%-7% in accuracy on commonsense benchmarks. Additionally, Adapt-Pruner restores the performance of MobileLLM-125M to 600M on the MMLU benchmark with 200$\times$ fewer tokens via pruning from its larger counterparts, and discovers a new 1B model that surpasses LLaMA-3.2-1B in multiple benchmarks.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03460', 90)">Copy Link</button>
<div id="copy-message-90" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2502.03461">Do Large Language Model Benchmarks Test Reliability?</a></h1>
<p><b>Authors:</b> Joshua Vendrow, Edward Vendrow, Sara Beery, Aleksander Madry</p>
<p>Abstract: When deploying large language models (LLMs), it is important to ensure that these models are not only capable, but also reliable. Many benchmarks have been created to track LLMs' growing capabilities, however there has been no similar focus on measuring their reliability. To understand the potential ramifications of this gap, we investigate how well current benchmarks quantify model reliability. We find that pervasive label errors can compromise these evaluations, obscuring lingering model failures and hiding unreliable behavior.
  Motivated by this gap in the evaluation of reliability, we then propose the concept of so-called platinum benchmarks, i.e., benchmarks carefully curated to minimize label errors and ambiguity. As a first attempt at constructing such benchmarks, we revise examples from fifteen existing popular benchmarks. We evaluate a wide range of models on these platinum benchmarks and find that, indeed, frontier LLMs still exhibit failures on simple tasks such as elementary-level math word problems. Analyzing these failures further reveals previously unidentified patterns of problems on which frontier models consistently struggle. We provide code at https://github.com/MadryLab/platinum-benchmarks</p>
<p>URLs: <a href="https://github.com/MadryLab/platinum-benchmarks">https://github.com/MadryLab/platinum-benchmarks</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03461, https://github.com/MadryLab/platinum-benchmarks', 91)">Copy Link</button>
<div id="copy-message-91" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2501.15417">AnyEnhance: A Unified Generative Model with Prompt-Guidance and Self-Critic for Voice Enhancement</a></h1>
<p><b>Authors:</b> Junan Zhang, Jing Yang, Zihao Fang, Yuancheng Wang, Zehua Zhang, Zhuo Wang, Fan Fan, Zhizheng Wu</p>
<p>Abstract: We introduce AnyEnhance, a unified generative model for voice enhancement that processes both speech and singing voices. Based on a masked generative model, AnyEnhance is capable of handling both speech and singing voices, supporting a wide range of enhancement tasks including denoising, dereverberation, declipping, super-resolution, and target speaker extraction, all simultaneously and without fine-tuning. AnyEnhance introduces a prompt-guidance mechanism for in-context learning, which allows the model to natively accept a reference speaker's timbre. In this way, it could boost enhancement performance when a reference audio is available and enable the target speaker extraction task without altering the underlying architecture. Moreover, we also introduce a self-critic mechanism into the generative process for masked generative models, yielding higher-quality outputs through iterative self-assessment and refinement. Extensive experiments on various enhancement tasks demonstrate AnyEnhance outperforms existing methods in terms of both objective metrics and subjective listening tests. Demo audios are publicly available at https://amphionspace.github.io/anyenhance/.</p>
<p>URLs: <a href="https://amphionspace.github.io/anyenhance/.">https://amphionspace.github.io/anyenhance/.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2501.15417, https://amphionspace.github.io/anyenhance/.', 92)">Copy Link</button>
<div id="copy-message-92" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02605">Physically Interpretable Representation and Controlled Generation for Turbulence Data</a></h1>
<p><b>Authors:</b> Tiffany Fan, Murray Cutforth, Marta D'Elia, Alexandre Cortiella, Alireza Doostan, Eric Darve</p>
<p>Abstract: Computational Fluid Dynamics (CFD) plays a pivotal role in fluid mechanics, enabling precise simulations of fluid behavior through partial differential equations (PDEs). However, traditional CFD methods are resource-intensive, particularly for high-fidelity simulations of complex flows, which are further complicated by high dimensionality, inherent stochasticity, and limited data availability. This paper addresses these challenges by proposing a data-driven approach that leverages a Gaussian Mixture Variational Autoencoder (GMVAE) to encode high-dimensional scientific data into low-dimensional, physically meaningful representations. The GMVAE learns a structured latent space where data can be categorized based on physical properties such as the Reynolds number while maintaining global physical consistency. To assess the interpretability of the learned representations, we introduce a novel metric based on graph spectral theory, quantifying the smoothness of physical quantities along the latent manifold. We validate our approach using 2D Navier-Stokes simulations of flow past a cylinder over a range of Reynolds numbers. Our results demonstrate that the GMVAE provides improved clustering, meaningful latent structure, and robust generative capabilities compared to baseline dimensionality reduction methods. This framework offers a promising direction for data-driven turbulence modeling and broader applications in computational fluid dynamics and engineering systems.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02605', 93)">Copy Link</button>
<div id="copy-message-93" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02607">MIND: Microstructure INverse Design with Generative Hybrid Neural Representation</a></h1>
<p><b>Authors:</b> Tianyang Xue, Haochen Li, Longdu Liu, Paul Henderson, Pengbin Tang, Lin Lu, Jikai Liu, Haisen Zhao, Hao Peng, Bernd Bickel</p>
<p>Abstract: The inverse design of microstructures plays a pivotal role in optimizing metamaterials with specific, targeted physical properties. While traditional forward design methods are constrained by their inability to explore the vast combinatorial design space, inverse design offers a compelling alternative by directly generating structures that fulfill predefined performance criteria. However, achieving precise control over both geometry and material properties remains a significant challenge due to their intricate interdependence. Existing approaches, which typically rely on voxel or parametric representations, often limit design flexibility and structural diversity. In this work, we present a novel generative model that integrates latent diffusion with Holoplane, an advanced hybrid neural representation that simultaneously encodes both geometric and physical properties. This combination ensures superior alignment between geometry and properties. Our approach generalizes across multiple microstructure classes, enabling the generation of diverse, tileable microstructures with significantly improved property accuracy and enhanced control over geometric validity, surpassing the performance of existing methods. We introduce a multi-class dataset encompassing a variety of geometric morphologies, including truss, shell, tube, and plate structures, to train and validate our model. Experimental results demonstrate the model's ability to generate microstructures that meet target properties, maintain geometric validity, and integrate seamlessly into complex assemblies. Additionally, we explore the potential of our framework through the generation of new microstructures, cross-class interpolation, and the infilling of heterogeneous microstructures. The dataset and source code will be open-sourced upon publication.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02607', 94)">Copy Link</button>
<div id="copy-message-94" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02619">Regret-Optimized Portfolio Enhancement through Deep Reinforcement Learning and Future Looking Rewards</a></h1>
<p><b>Authors:</b> Daniil Karzanov, Rub\'en Garz\'on, Mikhail Terekhov, Caglar Gulcehre, Thomas Raffinot, Marcin Detyniecki</p>
<p>Abstract: This paper introduces a novel agent-based approach for enhancing existing portfolio strategies using Proximal Policy Optimization (PPO). Rather than focusing solely on traditional portfolio construction, our approach aims to improve an already high-performing strategy through dynamic rebalancing driven by PPO and Oracle agents. Our target is to enhance the traditional 60/40 benchmark (60% stocks, 40% bonds) by employing the Regret-based Sharpe reward function. To address the impact of transaction fee frictions and prevent signal loss, we develop a transaction cost scheduler. We introduce a future-looking reward function and employ synthetic data training through a circular block bootstrap method to facilitate the learning of generalizable allocation strategies. We focus on two key evaluation measures: return and maximum drawdown. Given the high stochasticity of financial markets, we train 20 independent agents each period and evaluate their average performance against the benchmark. Our method not only enhances the performance of the existing portfolio strategy through strategic rebalancing but also demonstrates strong results compared to other baselines.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02619', 95)">Copy Link</button>
<div id="copy-message-95" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02629">Graph Structure Learning for Tumor Microenvironment with Cell Type Annotation from non-spatial scRNA-seq data</a></h1>
<p><b>Authors:</b> Yu-An Huang, Yue-Chao Li, Hai-Ru You, Jie Pan, Xiyue Cao, Xinyuan Li, Zhi-An Huang, Zhu-Hong You</p>
<p>Abstract: The exploration of cellular heterogeneity within the tumor microenvironment (TME) via single-cell RNA sequencing (scRNA-seq) is essential for understanding cancer progression and response to therapy. Current scRNA-seq approaches, however, lack spatial context and rely on incomplete datasets of ligand-receptor interactions (LRIs), limiting accurate cell type annotation and cell-cell communication (CCC) inference. This study addresses these challenges using a novel graph neural network (GNN) model that enhances cell type prediction and cell interaction analysis. Our study utilized a dataset consisting of 49,020 cells from 19 patients across three cancer types: Leukemia, Breast Invasive Carcinoma, and Colorectal Cancer. The proposed scGSL model demonstrated robust performance, achieving an average accuracy of 84.83%, precision of 86.23%, recall of 81.51%, and an F1 score of 80.92% across all datasets. These metrics represent a significant enhancement over existing methods, which typically exhibit lower performance metrics. Additionally, by reviewing existing literature on gene interactions within the TME, the scGSL model proves to robustly identify biologically meaningful gene interactions in an unsupervised manner, validated by significant expression differences in key gene pairs across various cancers. The source code and data used in this paper can be found in https://github.com/LiYuechao1998/scGSL.</p>
<p>URLs: <a href="https://github.com/LiYuechao1998/scGSL.">https://github.com/LiYuechao1998/scGSL.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02629, https://github.com/LiYuechao1998/scGSL.', 96)">Copy Link</button>
<div id="copy-message-96" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02630">scBIT: Integrating Single-cell Transcriptomic Data into fMRI-based Prediction for Alzheimer&#x27;s Disease Diagnosis</a></h1>
<p><b>Authors:</b> Yu-An Huang, Yao Hu, Yue-Chao Li, Xiyue Cao, Xinyuan Li, Kay Chen Tan, Zhu-Hong You, Zhi-An Huang</p>
<p>Abstract: Functional MRI (fMRI) and single-cell transcriptomics are pivotal in Alzheimer's disease (AD) research, each providing unique insights into neural function and molecular mechanisms. However, integrating these complementary modalities remains largely unexplored. Here, we introduce scBIT, a novel method for enhancing AD prediction by combining fMRI with single-nucleus RNA (snRNA). scBIT leverages snRNA as an auxiliary modality, significantly improving fMRI-based prediction models and providing comprehensive interpretability. It employs a sampling strategy to segment snRNA data into cell-type-specific gene networks and utilizes a self-explainable graph neural network to extract critical subgraphs. Additionally, we use demographic and genetic similarities to pair snRNA and fMRI data across individuals, enabling robust cross-modal learning. Extensive experiments validate scBIT's effectiveness in revealing intricate brain region-gene associations and enhancing diagnostic prediction accuracy. By advancing brain imaging transcriptomics to the single-cell level, scBIT sheds new light on biomarker discovery in AD research. Experimental results show that incorporating snRNA data into the scBIT model significantly boosts accuracy, improving binary classification by 3.39% and five-class classification by 26.59%. The codes were implemented in Python and have been released on GitHub (https://github.com/77YQ77/scBIT) and Zenodo (https://zenodo.org/records/11599030) with detailed instructions.</p>
<p>URLs: <a href="https://github.com/77YQ77/scBIT)">https://github.com/77YQ77/scBIT)</a>, <a href="https://zenodo.org/records/11599030)">https://zenodo.org/records/11599030)</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02630, https://github.com/77YQ77/scBIT), https://zenodo.org/records/11599030)', 97)">Copy Link</button>
<div id="copy-message-97" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02663">Learning to Double Guess: An Active Perception Approach for Estimating the Center of Mass of Arbitrary Objects</a></h1>
<p><b>Authors:</b> Shengmiao Jin, Yuchen Mo, Wenzhen Yuan</p>
<p>Abstract: Manipulating arbitrary objects in unstructured environments is a significant challenge in robotics, primarily due to difficulties in determining an object's center of mass. This paper introduces U-GRAPH: Uncertainty-Guided Rotational Active Perception with Haptics, a novel framework to enhance the center of mass estimation using active perception. Traditional methods often rely on single interaction and are limited by the inherent inaccuracies of Force-Torque (F/T) sensors. Our approach circumvents these limitations by integrating a Bayesian Neural Network (BNN) to quantify uncertainty and guide the robotic system through multiple, information-rich interactions via grid search and a neural network that scores each action. We demonstrate the remarkable generalizability and transferability of our method with training on a small dataset with limited variation yet still perform well on unseen complex real-world objects.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02663', 98)">Copy Link</button>
<div id="copy-message-98" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02672">Transformers Boost the Performance of Decision Trees on Tabular Data across Sample Sizes</a></h1>
<p><b>Authors:</b> Mayuka Jayawardhana (University of Maryland), Renbo Tu (University of Toronto), Samuel Dooley (Meta), Valeriia Cherepanova (Amazon), Andrew Gordon Wilson (New York University), Frank Hutter (University of Freiburg), Colin White (Abacus.AI), Tom Goldstein (University of Maryland), Micah Goldblum (Columbia University)</p>
<p>Abstract: Large language models (LLMs) perform remarkably well on tabular datasets in zero- and few-shot settings, since they can extract meaning from natural language column headers that describe features and labels. Similarly, TabPFN, a recent non-LLM transformer pretrained on numerous tables for in-context learning, has demonstrated excellent performance for dataset sizes up to a thousand samples. In contrast, gradient-boosted decision trees (GBDTs) are typically trained from scratch on each dataset without benefiting from pretraining data and must learn the relationships between columns from their entries alone since they lack natural language understanding. LLMs and TabPFN excel on small tabular datasets where a strong prior is essential, yet they are not competitive with GBDTs on medium or large datasets, since their context lengths are limited. In this paper, we propose a simple and lightweight approach for fusing large language models and TabPFN with gradient-boosted decision trees, which allows scalable GBDTs to benefit from the natural language capabilities and pretraining of transformers. We name our fusion methods LLM-Boost and PFN-Boost, respectively. While matching or surpassing the performance of the transformer at sufficiently small dataset sizes and GBDTs at sufficiently large sizes, LLM-Boost and PFN-Boost outperform both standalone components on a wide range of dataset sizes in between. We demonstrate state-of-the-art performance against numerous baselines and ensembling algorithms. We find that PFN-Boost achieves the best average performance among all methods we test for all but very small dataset sizes. We release our code at http://github.com/MayukaJ/LLM-Boost .</p>
<p>URLs: <a href="http://github.com/MayukaJ/LLM-Boost">http://github.com/MayukaJ/LLM-Boost</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02672, http://github.com/MayukaJ/LLM-Boost', 99)">Copy Link</button>
<div id="copy-message-99" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02676">Blind Visible Watermark Removal with Morphological Dilation</a></h1>
<p><b>Authors:</b> Preston K. Robinette, Taylor T. Johnson</p>
<p>Abstract: Visible watermarks pose significant challenges for image restoration techniques, especially when the target background is unknown. Toward this end, we present MorphoMod, a novel method for automated visible watermark removal that operates in a blind setting -- without requiring target images. Unlike existing methods, MorphoMod effectively removes opaque and transparent watermarks while preserving semantic content, making it well-suited for real-world applications. Evaluations on benchmark datasets, including the Colored Large-scale Watermark Dataset (CLWD), LOGO-series, and the newly introduced Alpha1 datasets, demonstrate that MorphoMod achieves up to a 50.8% improvement in watermark removal effectiveness compared to state-of-the-art methods. Ablation studies highlight the impact of prompts used for inpainting, pre-removal filling strategies, and inpainting model performance on watermark removal. Additionally, a case study on steganographic disorientation reveals broader applications for watermark removal in disrupting high-level hidden messages. MorphoMod offers a robust, adaptable solution for watermark removal and opens avenues for further advancements in image restoration and adversarial manipulation.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02676', 100)">Copy Link</button>
<div id="copy-message-100" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02679">Networks with Finite VC Dimension: Pro and Contra</a></h1>
<p><b>Authors:</b> Vera Kurkova, Marcello Sanguineti</p>
<p>Abstract: Approximation and learning of classifiers of large data sets by neural networks in terms of high-dimensional geometry and statistical learning theory are investigated. The influence of the VC dimension of sets of input-output functions of networks on approximation capabilities is compared with its influence on consistency in learning from samples of data. It is shown that, whereas finite VC dimension is desirable for uniform convergence of empirical errors, it may not be desirable for approximation of functions drawn from a probability distribution modeling the likelihood that they occur in a given type of application. Based on the concentration-of-measure properties of high dimensional geometry, it is proven that both errors in approximation and empirical errors behave almost deterministically for networks implementing sets of input-output functions with finite VC dimensions in processing large data sets. Practical limitations of the universal approximation property, the trade-offs between the accuracy of approximation and consistency in learning from data, and the influence of depth of networks with ReLU units on their accuracy and consistency are discussed.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02679', 101)">Copy Link</button>
<div id="copy-message-101" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02684">Three-dimensional signal processing: a new approach in dynamical sampling via tensor products</a></h1>
<p><b>Authors:</b> Yisen Wang, Hanqin Cai, Longxiu Huang</p>
<p>Abstract: The dynamical sampling problem is centered around reconstructing signals that evolve over time according to a dynamical process, from spatial-temporal samples that may be noisy. This topic has been thoroughly explored for one-dimensional signals. Multidimensional signal recovery has also been studied, but primarily in scenarios where the driving operator is a convolution operator. In this work, we shift our focus to the dynamical sampling problem in the context of three-dimensional signal recovery, where the evolution system can be characterized by tensor products. Specifically, we provide a necessary condition for the sampling set that ensures successful recovery of the three-dimensional signal. Furthermore, we reformulate the reconstruction problem as an optimization task, which can be solved efficiently. To demonstrate the effectiveness of our approach, we include some straightforward numerical simulations that showcase the reconstruction performance.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02684', 102)">Copy Link</button>
<div id="copy-message-102" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02690">Controllable Video Generation with Provable Disentanglement</a></h1>
<p><b>Authors:</b> Yifan Shen, Peiyuan Zhu, Zijian Li, Shaoan Xie, Zeyu Tang, Namrata Deka, Zongfang Liu, Guangyi Chen, Kun Zhang</p>
<p>Abstract: Controllable video generation remains a significant challenge, despite recent advances in generating high-quality and consistent videos. Most existing methods for controlling video generation treat the video as a whole, neglecting intricate fine-grained spatiotemporal relationships, which limits both control precision and efficiency. In this paper, we propose Controllable Video Generative Adversarial Networks (CoVoGAN) to disentangle the video concepts, thus facilitating efficient and independent control over individual concepts. Specifically, following the minimal change principle, we first disentangle static and dynamic latent variables. We then leverage the sufficient change property to achieve component-wise identifiability of dynamic latent variables, enabling independent control over motion and identity. To establish the theoretical foundation, we provide a rigorous analysis demonstrating the identifiability of our approach. Building on these theoretical insights, we design a Temporal Transition Module to disentangle latent dynamics. To enforce the minimal change principle and sufficient change property, we minimize the dimensionality of latent dynamic variables and impose temporal conditional independence. To validate our approach, we integrate this module as a plug-in for GANs. Extensive qualitative and quantitative experiments on various video generation benchmarks demonstrate that our method significantly improves generation quality and controllability across diverse real-world scenarios.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02690', 103)">Copy Link</button>
<div id="copy-message-103" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02692">Intelligent Sensing-to-Action for Robust Autonomy at the Edge: Opportunities and Challenges</a></h1>
<p><b>Authors:</b> Amit Ranjan Trivedi, Sina Tayebati, Hemant Kumawat, Nastaran Darabi, Divake Kumar, Adarsh Kumar Kosta, Yeshwanth Venkatesha, Dinithi Jayasuriya, Nethmi Jayasinghe, Priyadarshini Panda, Saibal Mukhopadhyay, Kaushik Roy</p>
<p>Abstract: Autonomous edge computing in robotics, smart cities, and autonomous vehicles relies on the seamless integration of sensing, processing, and actuation for real-time decision-making in dynamic environments. At its core is the sensing-to-action loop, which iteratively aligns sensor inputs with computational models to drive adaptive control strategies. These loops can adapt to hyper-local conditions, enhancing resource efficiency and responsiveness, but also face challenges such as resource constraints, synchronization delays in multi-modal data fusion, and the risk of cascading errors in feedback loops. This article explores how proactive, context-aware sensing-to-action and action-to-sensing adaptations can enhance efficiency by dynamically adjusting sensing and computation based on task demands, such as sensing a very limited part of the environment and predicting the rest. By guiding sensing through control actions, action-to-sensing pathways can improve task relevance and resource use, but they also require robust monitoring to prevent cascading errors and maintain reliability. Multi-agent sensing-action loops further extend these capabilities through coordinated sensing and actions across distributed agents, optimizing resource use via collaboration. Additionally, neuromorphic computing, inspired by biological systems, provides an efficient framework for spike-based, event-driven processing that conserves energy, reduces latency, and supports hierarchical control--making it ideal for multi-agent optimization. This article highlights the importance of end-to-end co-design strategies that align algorithmic models with hardware and environmental dynamics and improve cross-layer interdependencies to improve throughput, precision, and adaptability for energy-efficient edge autonomy in complex environments.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02692', 104)">Copy Link</button>
<div id="copy-message-104" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02703">Developing multilingual speech synthesis system for Ojibwe, Mi&#x27;kmaq, and Maliseet</a></h1>
<p><b>Authors:</b> Shenran Wang, Changbing Yang, Mike Parkhill, Chad Quinn, Christopher Hammerly, Jian Zhu</p>
<p>Abstract: We present lightweight flow matching multilingual text-to-speech (TTS) systems for Ojibwe, Mi'kmaq, and Maliseet, three Indigenous languages in North America. Our results show that training a multilingual TTS model on three typologically similar languages can improve the performance over monolingual models, especially when data are scarce. Attention-free architectures are highly competitive with self-attention architecture with higher memory efficiency. Our research not only advances technical development for the revitalization of low-resource languages but also highlights the cultural gap in human evaluation protocols, calling for a more community-centered approach to human evaluation.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02703', 105)">Copy Link</button>
<div id="copy-message-105" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02705">Rapidly Adapting Policies to the Real World via Simulation-Guided Fine-Tuning</a></h1>
<p><b>Authors:</b> Patrick Yin, Tyler Westenbroek, Simran Bagaria, Kevin Huang, Ching-an Cheng, Andrey Kobolov, Abhishek Gupta</p>
<p>Abstract: Robot learning requires a considerable amount of high-quality data to realize the promise of generalization. However, large data sets are costly to collect in the real world. Physics simulators can cheaply generate vast data sets with broad coverage over states, actions, and environments. However, physics engines are fundamentally misspecified approximations to reality. This makes direct zero-shot transfer from simulation to reality challenging, especially in tasks where precise and force-sensitive manipulation is necessary. Thus, fine-tuning these policies with small real-world data sets is an appealing pathway for scaling robot learning. However, current reinforcement learning fine-tuning frameworks leverage general, unstructured exploration strategies which are too inefficient to make real-world adaptation practical. This paper introduces the Simulation-Guided Fine-tuning (SGFT) framework, which demonstrates how to extract structural priors from physics simulators to substantially accelerate real-world adaptation. Specifically, our approach uses a value function learned in simulation to guide real-world exploration. We demonstrate this approach across five real-world dexterous manipulation tasks where zero-shot sim-to-real transfer fails. We further demonstrate our framework substantially outperforms baseline fine-tuning methods, requiring up to an order of magnitude fewer real-world samples and succeeding at difficult tasks where prior approaches fail entirely. Last but not least, we provide theoretical justification for this new paradigm which underpins how SGFT can rapidly learn high-performance policies in the face of large sim-to-real dynamics gaps. Project webpage: https://weirdlabuw.github.io/sgft/{weirdlabuw.github.io/sgft}</p>
<p>URLs: <a href="https://weirdlabuw.github.io/sgft/">https://weirdlabuw.github.io/sgft/</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02705, https://weirdlabuw.github.io/sgft/', 106)">Copy Link</button>
<div id="copy-message-106" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02710">Achievable distributional robustness when the robust risk is only partially identified</a></h1>
<p><b>Authors:</b> Julia Kostin, Nicola Gnecco, Fanny Yang</p>
<p>Abstract: In safety-critical applications, machine learning models should generalize well under worst-case distribution shifts, that is, have a small robust risk. Invariance-based algorithms can provably take advantage of structural assumptions on the shifts when the training distributions are heterogeneous enough to identify the robust risk. However, in practice, such identifiability conditions are rarely satisfied -- a scenario so far underexplored in the theoretical literature. In this paper, we aim to fill the gap and propose to study the more general setting when the robust risk is only partially identifiable. In particular, we introduce the worst-case robust risk as a new measure of robustness that is always well-defined regardless of identifiability. Its minimum corresponds to an algorithm-independent (population) minimax quantity that measures the best achievable robustness under partial identifiability. While these concepts can be defined more broadly, in this paper we introduce and derive them explicitly for a linear model for concreteness of the presentation. First, we show that existing robustness methods are provably suboptimal in the partially identifiable case. We then evaluate these methods and the minimizer of the (empirical) worst-case robust risk on real-world gene expression data and find a similar trend: the test error of existing robustness methods grows increasingly suboptimal as the fraction of data from unseen environments increases, whereas accounting for partial identifiability allows for better generalization.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02710', 107)">Copy Link</button>
<div id="copy-message-107" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02717">Astromer 2</a></h1>
<p><b>Authors:</b> Cristobal Donoso-Oliva, Ignacio Becker, Pavlos Protopapas, Guillermo Cabrera-Vives, Martina C\'adiz-Leyton, Daniel Moreno-Cartagena</p>
<p>Abstract: Foundational models have emerged as a powerful paradigm in deep learning field, leveraging their capacity to learn robust representations from large-scale datasets and effectively to diverse downstream applications such as classification. In this paper, we present Astromer 2 a foundational model specifically designed for extracting light curve embeddings. We introduce Astromer 2 as an enhanced iteration of our self-supervised model for light curve analysis. This paper highlights the advantages of its pre-trained embeddings, compares its performance with that of its predecessor, Astromer 1, and provides a detailed empirical analysis of its capabilities, offering deeper insights into the model's representations. Astromer 2 is pretrained on 1.5 million single-band light curves from the MACHO survey using a self-supervised learning task that predicts randomly masked observations within sequences. Fine-tuning on a smaller labeled dataset allows us to assess its performance in classification tasks. The quality of the embeddings is measured by the F1 score of an MLP classifier trained on Astromer-generated embeddings. Our results demonstrate that Astromer 2 significantly outperforms Astromer 1 across all evaluated scenarios, including limited datasets of 20, 100, and 500 samples per class. The use of weighted per-sample embeddings, which integrate intermediate representations from Astromer's attention blocks, is particularly impactful. Notably, Astromer 2 achieves a 15% improvement in F1 score on the ATLAS dataset compared to prior models, showcasing robust generalization to new datasets. This enhanced performance, especially with minimal labeled data, underscores the potential of Astromer 2 for more efficient and scalable light curve analysis.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02717', 108)">Copy Link</button>
<div id="copy-message-108" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02764">LLM-USO: Large Language Model-based Universal Sizing Optimizer</a></h1>
<p><b>Authors:</b> Karthik Somayaji N. S, Peng Li</p>
<p>Abstract: The design of analog circuits is a cornerstone of integrated circuit (IC) development, requiring the optimization of complex, interconnected sub-structures such as amplifiers, comparators, and buffers. Traditionally, this process relies heavily on expert human knowledge to refine design objectives by carefully tuning sub-components while accounting for their interdependencies. Existing methods, such as Bayesian Optimization (BO), offer a mathematically driven approach for efficiently navigating large design spaces. However, these methods fall short in two critical areas compared to human expertise: (i) they lack the semantic understanding of the sizing solution space and its direct correlation with design objectives before optimization, and (ii) they fail to reuse knowledge gained from optimizing similar sub-structures across different circuits. To overcome these limitations, we propose the Large Language Model-based Universal Sizing Optimizer (LLM-USO), which introduces a novel method for knowledge representation to encode circuit design knowledge in a structured text format. This representation enables the systematic reuse of optimization insights for circuits with similar sub-structures. LLM-USO employs a hybrid framework that integrates BO with large language models (LLMs) and a learning summary module. This approach serves to: (i) infuse domain-specific knowledge into the BO process and (ii) facilitate knowledge transfer across circuits, mirroring the cognitive strategies of expert designers. Specifically, LLM-USO constructs a knowledge summary mechanism to distill and apply design insights from one circuit to related ones. It also incorporates a knowledge summary critiquing mechanism to ensure the accuracy and quality of the summaries and employs BO-guided suggestion filtering to identify optimal design points efficiently.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02764', 109)">Copy Link</button>
<div id="copy-message-109" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02771">When are Diffusion Priors Helpful in Sparse Reconstruction? A Study with Sparse-view CT</a></h1>
<p><b>Authors:</b> Matt Y. Cheung, Sophia Zorek, Tucker J. Netherton, Laurence E. Court, Sadeer Al-Kindi, Ashok Veeraraghavan, Guha Balakrishnan</p>
<p>Abstract: Diffusion models demonstrate state-of-the-art performance on image generation, and are gaining traction for sparse medical image reconstruction tasks. However, compared to classical reconstruction algorithms relying on simple analytical priors, diffusion models have the dangerous property of producing realistic looking results \emph{even when incorrect}, particularly with few observations. We investigate the utility of diffusion models as priors for image reconstruction by varying the number of observations and comparing their performance to classical priors (sparse and Tikhonov regularization) using pixel-based, structural, and downstream metrics. We make comparisons on low-dose chest wall computed tomography (CT) for fat mass quantification. First, we find that classical priors are superior to diffusion priors when the number of projections is ``sufficient''. Second, we find that diffusion priors can capture a large amount of detail with very few observations, significantly outperforming classical priors. However, they fall short of capturing all details, even with many observations. Finally, we find that the performance of diffusion priors plateau after extremely few ($\approx$10-15) projections. Ultimately, our work highlights potential issues with diffusion-based sparse reconstruction and underscores the importance of further investigation, particularly in high-stakes clinical settings.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02771', 110)">Copy Link</button>
<div id="copy-message-110" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02780">Classroom Simulacra: Building Contextual Student Generative Agents in Online Education for Learning Behavioral Simulation</a></h1>
<p><b>Authors:</b> Songlin Xu, Hao-Ning Wen, Hongyi Pan, Dallas Dominguez, Dongyin Hu, Xinyu Zhang</p>
<p>Abstract: Student simulation supports educators to improve teaching by interacting with virtual students. However, most existing approaches ignore the modulation effects of course materials because of two challenges: the lack of datasets with granularly annotated course materials, and the limitation of existing simulation models in processing extremely long textual data. To solve the challenges, we first run a 6-week education workshop from N = 60 students to collect fine-grained data using a custom built online education system, which logs students' learning behaviors as they interact with lecture materials over time. Second, we propose a transferable iterative reflection (TIR) module that augments both prompting-based and finetuning-based large language models (LLMs) for simulating learning behaviors. Our comprehensive experiments show that TIR enables the LLMs to perform more accurate student simulation than classical deep learning models, even with limited demonstration data. Our TIR approach better captures the granular dynamism of learning performance and inter-student correlations in classrooms, paving the way towards a ''digital twin'' for online education.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02780', 111)">Copy Link</button>
<div id="copy-message-111" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02787">SimMark: A Robust Sentence-Level Similarity-Based Watermarking Algorithm for Large Language Models</a></h1>
<p><b>Authors:</b> Amirhossein Dabiriaghdam, Lele Wang</p>
<p>Abstract: The rapid proliferation of large language models (LLMs) has created an urgent need for reliable methods to detect whether a text is generated by such models. In this paper, we propose SimMark, a posthoc watermarking algorithm that makes LLMs' outputs traceable without requiring access to the model's internal logits, enabling compatibility with a wide range of LLMs, including API-only models. By leveraging the similarity of semantic sentence embeddings and rejection sampling to impose detectable statistical patterns imperceptible to humans, and employing a soft counting mechanism, SimMark achieves robustness against paraphrasing attacks. Experimental results demonstrate that SimMark sets a new benchmark for robust watermarking of LLM-generated content, surpassing prior sentence-level watermarking techniques in robustness, sampling efficiency, and applicability across diverse domains, all while preserving the text quality.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02787', 112)">Copy Link</button>
<div id="copy-message-112" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02788">Inducing Diversity in Differentiable Search Indexing</a></h1>
<p><b>Authors:</b> Abhijeet Phatak, Jayant Sachdev, Sean D Rosario, Swati Kirti, Chittaranjan Tripathy</p>
<p>Abstract: Differentiable Search Indexing (DSI) is a recent paradigm for information retrieval which uses a transformer-based neural network architecture as the document index to simplify the retrieval process. A differentiable index has many advantages enabling modifications, updates or extensions to the index. In this work, we explore balancing relevance and novel information content (diversity) for training DSI systems inspired by Maximal Marginal Relevance (MMR), and show the benefits of our approach over the naive DSI training. We present quantitative and qualitative evaluations of relevance and diversity measures obtained using our method on NQ320K and MSMARCO datasets in comparison to naive DSI. With our approach, it is possible to achieve diversity without any significant impact to relevance. Since we induce diversity while training DSI, the trained model has learned to diversify while being relevant. This obviates the need for a post-processing step to induce diversity in the recall set as typically performed using MMR. Our approach will be useful for Information Retrieval problems where both relevance and diversity are important such as in sub-topic retrieval. Our work can also be easily be extended to the incremental DSI settings which would enable fast updates to the index while retrieving a diverse recall set.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02788', 113)">Copy Link</button>
<div id="copy-message-113" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02818">Accessible and Portable LLM Inference by Compiling Computational Graphs into SQL</a></h1>
<p><b>Authors:</b> Wenbo Sun, Qiming Guo, Wenlu Wang, Rihan Hai</p>
<p>Abstract: Serving large language models (LLMs) often demands specialized hardware, dedicated frameworks, and substantial development efforts, which restrict their accessibility, especially for edge devices and organizations with limited technical resources. We propose a novel compiler that translates LLM inference graphs into SQL queries, enabling relational databases, one of the most widely used and mature software systems globally, to serve as the runtime. By mapping neural operators such as matrix multiplication and attention into relational primitives like joins and aggregations, our approach leverages database capabilities, including disk-based data management and native caching. Supporting key transformer components, such as attention mechanisms and key-value caching, our system generates SQL pipelines for end-to-end LLM inference. Using the Llama3 family as a case study, we demonstrate up to 30x speedup in token generation for memory-constrained scenarios comparable to competitive CPU-based frameworks. Our work offers an accessible, portable, and efficient solution, facilitating the serving of LLMs across diverse deployment environments.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02818', 114)">Copy Link</button>
<div id="copy-message-114" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02830">Multimodal Brain-Computer Interfaces: AI-powered Decoding Methodologies</a></h1>
<p><b>Authors:</b> Siyang Li, Hongbin Wang, Xiaoqing Chen, Dongrui Wu</p>
<p>Abstract: Brain-computer interfaces (BCIs) enable direct communication between the brain and external devices. This review highlights the core decoding algorithms that enable multimodal BCIs, including a dissection of the elements, a unified view of diversified approaches, and a comprehensive analysis of the present state of the field. We emphasize algorithmic advancements in cross-modality mapping, sequential modeling, besides classic multi-modality fusion, illustrating how these novel AI approaches enhance decoding of brain data. The current literature of BCI applications on visual, speech, and affective decoding are comprehensively explored. Looking forward, we draw attention on the impact of emerging architectures like multimodal Transformers, and discuss challenges such as brain data heterogeneity and common errors. This review also serves as a bridge in this interdisciplinary field for experts with neuroscience background and experts that study AI, aiming to provide a comprehensive understanding for AI-powered multimodal BCIs.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02830', 115)">Copy Link</button>
<div id="copy-message-115" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02853">Rethinking Latent Representations in Behavior Cloning: An Information Bottleneck Approach for Robot Manipulation</a></h1>
<p><b>Authors:</b> Shuanghai Bai, Wanqi Zhou, Pengxiang Ding, Wei Zhao, Donglin Wang, Badong Chen</p>
<p>Abstract: Behavior Cloning (BC) is a widely adopted visual imitation learning method in robot manipulation. Current BC approaches often enhance generalization by leveraging large datasets and incorporating additional visual and textual modalities to capture more diverse information. However, these methods overlook whether the learned representations contain redundant information and lack a solid theoretical foundation to guide the learning process. To address these limitations, we adopt an information-theoretic perspective and introduce mutual information to quantify and mitigate redundancy in latent representations. Building on this, we incorporate the Information Bottleneck (IB) principle into BC, which extends the idea of reducing redundancy by providing a structured framework for compressing irrelevant information while preserving task-relevant features. This work presents the first comprehensive study on redundancy in latent representations across various methods, backbones, and experimental settings, while extending the generalizability of the IB to BC. Extensive experiments and analyses on the CortexBench and LIBERO benchmarks demonstrate significant performance improvements with IB, underscoring the importance of reducing input data redundancy and highlighting its practical value for more practical applications. Project Page: https://baishuanghao.github.io/BC-IB.github.io.</p>
<p>URLs: <a href="https://baishuanghao.github.io/BC-IB.github.io.">https://baishuanghao.github.io/BC-IB.github.io.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02853, https://baishuanghao.github.io/BC-IB.github.io.', 116)">Copy Link</button>
<div id="copy-message-116" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02854">TD3: Tucker Decomposition Based Dataset Distillation Method for Sequential Recommendation</a></h1>
<p><b>Authors:</b> Jiaqing Zhang, Mingjia Yin, Hao Wang, Yawen Li, Yuyang Ye, Xingyu Lou, Junping Du, Enhong Chen</p>
<p>Abstract: In the era of data-centric AI, the focus of recommender systems has shifted from model-centric innovations to data-centric approaches. The success of modern AI models is built on large-scale datasets, but this also results in significant training costs. Dataset distillation has emerged as a key solution, condensing large datasets to accelerate model training while preserving model performance. However, condensing discrete and sequentially correlated user-item interactions, particularly with extensive item sets, presents considerable challenges. This paper introduces \textbf{TD3}, a novel \textbf{T}ucker \textbf{D}ecomposition based \textbf{D}ataset \textbf{D}istillation method within a meta-learning framework, designed for sequential recommendation. TD3 distills a fully expressive \emph{synthetic sequence summary} from original data. To efficiently reduce computational complexity and extract refined latent patterns, Tucker decomposition decouples the summary into four factors: \emph{synthetic user latent factor}, \emph{temporal dynamics latent factor}, \emph{shared item latent factor}, and a \emph{relation core} that models their interconnections. Additionally, a surrogate objective in bi-level optimization is proposed to align feature spaces extracted from models trained on both original data and synthetic sequence summary beyond the na\"ive performance matching approach. In the \emph{inner-loop}, an augmentation technique allows the learner to closely fit the synthetic summary, ensuring an accurate update of it in the \emph{outer-loop}. To accelerate the optimization process and address long dependencies, RaT-BPTT is employed for bi-level optimization. Experiments and analyses on multiple public datasets have confirmed the superiority and cross-architecture generalizability of the proposed designs. Codes are released at https://github.com/USTC-StarTeam/TD3.</p>
<p>URLs: <a href="https://github.com/USTC-StarTeam/TD3.">https://github.com/USTC-StarTeam/TD3.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02854, https://github.com/USTC-StarTeam/TD3.', 117)">Copy Link</button>
<div id="copy-message-117" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02859">Gap-Dependent Bounds for Federated $Q$-learning</a></h1>
<p><b>Authors:</b> Haochen Zhang, Zhong Zheng, Lingzhou Xue</p>
<p>Abstract: We present the first gap-dependent analysis of regret and communication cost for on-policy federated $Q$-Learning in tabular episodic finite-horizon Markov decision processes (MDPs). Existing FRL methods focus on worst-case scenarios, leading to $\sqrt{T}$-type regret bounds and communication cost bounds with a $\log T$ term scaling with the number of agents $M$, states $S$, and actions $A$, where $T$ is the average total number of steps per agent. In contrast, our novel framework leverages the benign structures of MDPs, such as a strictly positive suboptimality gap, to achieve a $\log T$-type regret bound and a refined communication cost bound that disentangles exploration and exploitation. Our gap-dependent regret bound reveals a distinct multi-agent speedup pattern, and our gap-dependent communication cost bound removes the dependence on $MSA$ from the $\log T$ term. Notably, our gap-dependent communication cost bound also yields a better global switching cost when $M=1$, removing $SA$ from the $\log T$ term.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02859', 118)">Copy Link</button>
<div id="copy-message-118" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02861">Algorithms with Calibrated Machine Learning Predictions</a></h1>
<p><b>Authors:</b> Judy Shen, Ellen Vitercik, Anders Wikum</p>
<p>Abstract: The field of algorithms with predictions incorporates machine learning advice in the design of online algorithms to improve real-world performance. While this theoretical framework often assumes uniform reliability across all predictions, modern machine learning models can now provide instance-level uncertainty estimates. In this paper, we propose calibration as a principled and practical tool to bridge this gap, demonstrating the benefits of calibrated advice through two case studies: the ski rental and online job scheduling problems. For ski rental, we design an algorithm that achieves optimal prediction-dependent performance and prove that, in high-variance settings, calibrated advice offers more effective guidance than alternative methods for uncertainty quantification. For job scheduling, we demonstrate that using a calibrated predictor leads to significant performance improvements over existing methods. Evaluations on real-world data validate our theoretical findings, highlighting the practical impact of calibration for algorithms with predictions.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02861', 119)">Copy Link</button>
<div id="copy-message-119" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02867">Domain-Invariant Per-Frame Feature Extraction for Cross-Domain Imitation Learning with Visual Observations</a></h1>
<p><b>Authors:</b> Minung Kim, Kawon Lee, Jungmo Kim, Sungho Choi, Seungyul Han</p>
<p>Abstract: Imitation learning (IL) enables agents to mimic expert behavior without reward signals but faces challenges in cross-domain scenarios with high-dimensional, noisy, and incomplete visual observations. To address this, we propose Domain-Invariant Per-Frame Feature Extraction for Imitation Learning (DIFF-IL), a novel IL method that extracts domain-invariant features from individual frames and adapts them into sequences to isolate and replicate expert behaviors. We also introduce a frame-wise time labeling technique to segment expert behaviors by timesteps and assign rewards aligned with temporal contexts, enhancing task performance. Experiments across diverse visual environments demonstrate the effectiveness of DIFF-IL in addressing complex visual tasks.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02867', 120)">Copy Link</button>
<div id="copy-message-120" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02870">Uncertainty Quantification with the Empirical Neural Tangent Kernel</a></h1>
<p><b>Authors:</b> Joseph Wilson, Chris van der Heide, Liam Hodgkinson, Fred Roosta</p>
<p>Abstract: While neural networks have demonstrated impressive performance across various tasks, accurately quantifying uncertainty in their predictions is essential to ensure their trustworthiness and enable widespread adoption in critical systems. Several Bayesian uncertainty quantification (UQ) methods exist that are either cheap or reliable, but not both. We propose a post-hoc, sampling-based UQ method for over-parameterized networks at the end of training. Our approach constructs efficient and meaningful deep ensembles by employing a (stochastic) gradient-descent sampling process on appropriately linearized networks. We demonstrate that our method effectively approximates the posterior of a Gaussian process using the empirical Neural Tangent Kernel. Through a series of numerical experiments, we show that our method not only outperforms competing approaches in computational efficiency (often reducing costs by multiple factors) but also maintains state-of-the-art performance across a variety of UQ metrics for both regression and classification tasks.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02870', 121)">Copy Link</button>
<div id="copy-message-121" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02874">Vertical Federated Learning for Failure-Cause Identification in Disaggregated Microwave Networks</a></h1>
<p><b>Authors:</b> Fatih Temiz, Memedhe Ibrahimi, Francesco Musumeci, Claudio Passera, Massimo Tornatore</p>
<p>Abstract: Machine Learning (ML) has proven to be a promising solution to provide novel scalable and efficient fault management solutions in modern 5G-and-beyond communication networks. In the context of microwave networks, ML-based solutions have received significant attention. However, current solutions can only be applied to monolithic scenarios in which a single entity (e.g., an operator) manages the entire network. As current network architectures move towards disaggregated communication platforms in which multiple operators and vendors collaborate to achieve cost-efficient and reliable network management, new ML-based approaches for fault management must tackle the challenges of sharing business-critical information due to potential conflicts of interest. In this study, we explore the application of Federated Learning in disaggregated microwave networks for failure-cause identification using a real microwave hardware failure dataset. In particular, we investigate the application of two Vertical Federated Learning (VFL), namely using Split Neural Networks (SplitNNs) and Federated Learning based on Gradient Boosting Decision Trees (FedTree), on different multi-vendor deployment scenarios, and we compare them to a centralized scenario where data is managed by a single entity. Our experimental results show that VFL-based scenarios can achieve F1-Scores consistently within at most a 1% gap with respect to a centralized scenario, regardless of the deployment strategies or model types, while also ensuring minimal leakage of sensitive-data.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02874', 122)">Copy Link</button>
<div id="copy-message-122" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02885">Expertized Caption Auto-Enhancement for Video-Text Retrieval</a></h1>
<p><b>Authors:</b> Junxiang Chen, Baoyao yang, Wenbin Yao</p>
<p>Abstract: The burgeoning field of video-text retrieval has witnessed significant advancements with the advent of deep learning. However, the challenge of matching text and video persists due to inadequate textual descriptions of videos. The substantial information gap between the two modalities hinders a comprehensive understanding of videos, resulting in ambiguous retrieval results. While rewriting methods based on large language models have been proposed to broaden text expressions, carefully crafted prompts are essential to ensure the reasonableness and completeness of the rewritten texts. This paper proposes an automatic caption enhancement method that enhances expression quality and mitigates empiricism in augmented captions through self-learning. Additionally, an expertized caption selection mechanism is designed and introduced to customize augmented captions for each video, facilitating video-text matching. Our method is entirely data-driven, which not only dispenses with heavy data collection and computation workload but also improves self-adaptability by circumventing lexicon dependence and introducing personalized matching. The superiority of our method is validated by state-of-the-art results on various benchmarks, specifically achieving Top-1 recall accuracy of 68.5% on MSR-VTT, 68.1% on MSVD, and 62.0% on DiDeMo.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02885', 123)">Copy Link</button>
<div id="copy-message-123" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02887">Variations on the Expectation Due to Changes in the Probability Measure</a></h1>
<p><b>Authors:</b> Samir M. Perlaza, Gaetan Bisson</p>
<p>Abstract: Closed-form expressions are presented for the variation of the expectation of a given function due to changes in the probability measure used for the expectation. They unveil interesting connections with Gibbs probability measures, the mutual information, and the lautum information.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02887', 124)">Copy Link</button>
<div id="copy-message-124" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02903">What is in a name? Mitigating Name Bias in Text Embeddings via Anonymization</a></h1>
<p><b>Authors:</b> Sahil Manchanda, Pannaga Shivaswamy</p>
<p>Abstract: Text-embedding models often exhibit biases arising from the data on which they are trained. In this paper, we examine a hitherto unexplored bias in text-embeddings: bias arising from the presence of $\textit{names}$ such as persons, locations, organizations etc. in the text. Our study shows how the presence of $\textit{name-bias}$ in text-embedding models can potentially lead to erroneous conclusions in assessment of thematic similarity.Text-embeddings can mistakenly indicate similarity between texts based on names in the text, even when their actual semantic content has no similarity or indicate dissimilarity simply because of the names in the text even when the texts match semantically. We first demonstrate the presence of name bias in different text-embedding models and then propose $\textit{text-anonymization}$ during inference which involves removing references to names, while preserving the core theme of the text. The efficacy of the anonymization approach is demonstrated on two downstream NLP tasks, achieving significant performance gains. Our simple and training-optimization-free approach offers a practical and easily implementable solution to mitigate name bias.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02903', 125)">Copy Link</button>
<div id="copy-message-125" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02905">AI-driven materials design: a mini-review</a></h1>
<p><b>Authors:</b> Mouyang Cheng, Chu-Liang Fu, Ryotaro Okabe, Abhijatmedhi Chotrattanapituk, Artittaya Boonkird, Nguyen Tuan Hung, Mingda Li</p>
<p>Abstract: Materials design is an important component of modern science and technology, yet traditional approaches rely heavily on trial-and-error and can be inefficient. Computational techniques, enhanced by modern artificial intelligence (AI), have greatly accelerated the design of new materials. Among these approaches, inverse design has shown great promise in designing materials that meet specific property requirements. In this mini-review, we summarize key computational advancements for materials design over the past few decades. We follow the evolution of relevant materials design techniques, from high-throughput forward machine learning (ML) methods and evolutionary algorithms, to advanced AI strategies like reinforcement learning (RL) and deep generative models. We highlight the paradigm shift from conventional screening approaches to inverse generation driven by deep generative models. Finally, we discuss current challenges and future perspectives of materials inverse design. This review may serve as a brief guide to the approaches, progress, and outlook of designing future functional materials with technological relevance.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02905', 126)">Copy Link</button>
<div id="copy-message-126" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02908">COSMosFL: Ensemble of Small Language Models for Fault Localisation</a></h1>
<p><b>Authors:</b> Hyunjoon Cho, Sungmin Kang, Gabin An, Shin Yoo</p>
<p>Abstract: LLMs are rapidly being adopted to build powerful tools and agents for software engineering, but most of them rely heavily on extremely large closed-source models. This, in turn, can hinder wider adoption due to security issues as well as financial cost and environmental impact. Recently, a number of open source Small Language Models (SLMs) are being released and gaining traction. While SLMs are smaller, more energy-efficient, and therefore easier to locally deploy, they tend to show worse performance when compared to larger closed LLMs. We present COSMos, a task-level LLM ensemble technique that uses voting mechanism, to provide a broader range of choice between SLMs and LLMs. We instantiate COSMos with an LLM-based Fault Localisation technique, AutoFL, and report the cost-benefit trade-off between LLM accuracy and various costs such as energy consumption, inference time, and the number of tokens used. An empirical evaluation using Defects4J shows that COSMos can build effective ensembles that can achieve Pareto-optimality in terms of FL accuracy and inference cost, when compared to individual models.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02908', 127)">Copy Link</button>
<div id="copy-message-127" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02910">DANDI: Diffusion as Normative Distribution for Deep Neural Network Input</a></h1>
<p><b>Authors:</b> Somin Kim, Shin Yoo</p>
<p>Abstract: Surprise Adequacy (SA) has been widely studied as a test adequacy metric that can effectively guide software engineers towards inputs that are more likely to reveal unexpected behaviour of Deep Neural Networks (DNNs). Intuitively, SA is an out-of-distribution metric that quantifies the dissimilarity between the given input and the training data: if a new input is very different from those seen during training, the DNN is more likely to behave unexpectedly against the input. While SA has been widely adopted as a test prioritization method, its major weakness is the fact that the computation of the metric requires access to the training dataset, which is often not allowed in real-world use cases. We present DANDI, a technique that generates a surrogate input distribution using Stable Diffusion to compute SA values without requiring the original training data. An empirical evaluation of DANDI applied to image classifiers for CIFAR10 and ImageNet-1K shows that SA values computed against synthetic data are highly correlated with the values computed against the training data, with Spearman Rank correlation value of 0.852 for ImageNet-1K and 0.881 for CIFAR-10. Further, we show that SA value computed by DANDI achieves can prioritize inputs as effectively as those computed using the training data, when testing DNN models mutated by DeepMutation. We believe that DANDI can significantly improve the usability of SA for practical DNN testing.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02910', 128)">Copy Link</button>
<div id="copy-message-128" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02919">Maximizing the Position Embedding for Vision Transformers with Global Average Pooling</a></h1>
<p><b>Authors:</b> Wonjun Lee, Bumsub Ham, Suhyun Kim</p>
<p>Abstract: In vision transformers, position embedding (PE) plays a crucial role in capturing the order of tokens. However, in vision transformer structures, there is a limitation in the expressiveness of PE due to the structure where position embedding is simply added to the token embedding. A layer-wise method that delivers PE to each layer and applies independent Layer Normalizations for token embedding and PE has been adopted to overcome this limitation. In this paper, we identify the conflicting result that occurs in a layer-wise structure when using the global average pooling (GAP) method instead of the class token. To overcome this problem, we propose MPVG, which maximizes the effectiveness of PE in a layer-wise structure with GAP. Specifically, we identify that PE counterbalances token embedding values at each layer in a layer-wise structure. Furthermore, we recognize that the counterbalancing role of PE is insufficient in the layer-wise structure, and we address this by maximizing the effectiveness of PE through MPVG. Through experiments, we demonstrate that PE performs a counterbalancing role and that maintaining this counterbalancing directionality significantly impacts vision transformers. As a result, the experimental results show that MPVG outperforms existing methods across vision transformers on various tasks.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02919', 129)">Copy Link</button>
<div id="copy-message-129" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02925">Data denoising with self consistency, variance maximization, and the Kantorovich dominance</a></h1>
<p><b>Authors:</b> Joshua Zoen-Git Hiew, Tongseok Lim, Brendan Pass, Marcelo Cruz de Souza</p>
<p>Abstract: We introduce a new framework for data denoising, partially inspired by martingale optimal transport. For a given noisy distribution (the data), our approach involves finding the closest distribution to it among all distributions which 1) have a particular prescribed structure (expressed by requiring they lie in a particular domain), and 2) are self-consistent with the data. We show that this amounts to maximizing the variance among measures in the domain which are dominated in convex order by the data. For particular choices of the domain, this problem and a relaxed version of it, in which the self-consistency condition is removed, are intimately related to various classical approaches to denoising. We prove that our general problem has certain desirable features: solutions exist under mild assumptions, have certain robustness properties, and, for very simple domains, coincide with solutions to the relaxed problem.
  We also introduce a novel relationship between distributions, termed Kantorovich dominance, which retains certain aspects of the convex order while being a weaker, more robust, and easier-to-verify condition. Building on this, we propose and analyze a new denoising problem by substituting the convex order in the previously described framework with Kantorovich dominance. We demonstrate that this revised problem shares some characteristics with the full convex order problem but offers enhanced stability, greater computational efficiency, and, in specific domains, more meaningful solutions. Finally, we present simple numerical examples illustrating solutions for both the full convex order problem and the Kantorovich dominance problem.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02925', 130)">Copy Link</button>
<div id="copy-message-130" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02943">Behavioral Homophily in Social Media via Inverse Reinforcement Learning: A Reddit Case Study</a></h1>
<p><b>Authors:</b> Lanqin Yuan, Philipp J. Schneider, Marian-Andrei Rizoiu</p>
<p>Abstract: Online communities play a critical role in shaping societal discourse and influencing collective behavior in the real world. The tendency for people to connect with others who share similar characteristics and views, known as homophily, plays a key role in the formation of echo chambers which further amplify polarization and division. Existing works examining homophily in online communities traditionally infer it using content- or adjacency-based approaches, such as constructing explicit interaction networks or performing topic analysis. These methods fall short for platforms where interaction networks cannot be easily constructed and fail to capture the complex nature of user interactions across the platform. This work introduces a novel approach for quantifying user homophily. We first use an Inverse Reinforcement Learning (IRL) framework to infer users' policies, then use these policies as a measure of behavioral homophily. We apply our method to Reddit, conducting a case study across 5.9 million interactions over six years, demonstrating how this approach uncovers distinct behavioral patterns and user roles that vary across different communities. We further validate our behavioral homophily measure against traditional content-based homophily, offering a powerful method for analyzing social media dynamics and their broader societal implications. We find, among others, that users can behave very similarly (high behavioral homophily) when discussing entirely different topics like soccer vs e-sports (low topical homophily), and that there is an entire class of users on Reddit whose purpose seems to be to disagree with others.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02943', 131)">Copy Link</button>
<div id="copy-message-131" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02951">VQA-Levels: A Hierarchical Approach for Classifying Questions in VQA</a></h1>
<p><b>Authors:</b> Madhuri Latha Madaka, Chakravarthy Bhagvati</p>
<p>Abstract: Designing datasets for Visual Question Answering (VQA) is a difficult and complex task that requires NLP for parsing and computer vision for analysing the relevant aspects of the image for answering the question asked. Several benchmark datasets have been developed by researchers but there are many issues with using them for methodical performance tests. This paper proposes a new benchmark dataset -- a pilot version called VQA-Levels is ready now -- for testing VQA systems systematically and assisting researchers in advancing the field. The questions are classified into seven levels ranging from direct answers based on low-level image features (without needing even a classifier) to those requiring high-level abstraction of the entire image content. The questions in the dataset exhibit one or many of ten properties. Each is categorised into a specific level from 1 to 7. Levels 1 - 3 are directly on the visual content while the remaining levels require extra knowledge about the objects in the image. Each question generally has a unique one or two-word answer. The questions are 'natural' in the sense that a human is likely to ask such a question when seeing the images. An example question at Level 1 is, ``What is the shape of the red colored region in the image?" while at Level 7, it is, ``Why is the man cutting the paper?". Initial testing of the proposed dataset on some of the existing VQA systems reveals that their success is high on Level 1 (low level features) and Level 2 (object classification) questions, least on Level 3 (scene text) followed by Level 6 (extrapolation) and Level 7 (whole scene analysis) questions. The work in this paper will go a long way to systematically analyze VQA systems.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02951', 132)">Copy Link</button>
<div id="copy-message-132" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02966">FACTER: Fairness-Aware Conformal Thresholding and Prompt Engineering for Enabling Fair LLM-Based Recommender Systems</a></h1>
<p><b>Authors:</b> Arya Fayyazi, Mehdi Kamal, Massoud Pedram</p>
<p>Abstract: We propose FACTER, a fairness-aware framework for LLM-based recommendation systems that integrates conformal prediction with dynamic prompt engineering. By introducing an adaptive semantic variance threshold and a violation-triggered mechanism, FACTER automatically tightens fairness constraints whenever biased patterns emerge. We further develop an adversarial prompt generator that leverages historical violations to reduce repeated demographic biases without retraining the LLM. Empirical results on MovieLens and Amazon show that FACTER substantially reduces fairness violations (up to 95.5%) while maintaining strong recommendation accuracy, revealing semantic variance as a potent proxy of bias.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02966', 133)">Copy Link</button>
<div id="copy-message-133" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02972">Label Anything: An Interpretable, High-Fidelity and Prompt-Free Annotator</a></h1>
<p><b>Authors:</b> Wei-Bin Kou, Guangxu Zhu, Rongguang Ye, Shuai Wang, Ming Tang, Yik-Chung Wu</p>
<p>Abstract: Learning-based street scene semantic understanding in autonomous driving (AD) has advanced significantly recently, but the performance of the AD model is heavily dependent on the quantity and quality of the annotated training data. However, traditional manual labeling involves high cost to annotate the vast amount of required data for training robust model. To mitigate this cost of manual labeling, we propose a Label Anything Model (denoted as LAM), serving as an interpretable, high-fidelity, and prompt-free data annotator. Specifically, we firstly incorporate a pretrained Vision Transformer (ViT) to extract the latent features. On top of ViT, we propose a semantic class adapter (SCA) and an optimization-oriented unrolling algorithm (OptOU), both with a quite small number of trainable parameters. SCA is proposed to fuse ViT-extracted features to consolidate the basis of the subsequent automatic annotation. OptOU consists of multiple cascading layers and each layer contains an optimization formulation to align its output with the ground truth as closely as possible, though which OptOU acts as being interpretable rather than learning-based blackbox nature. In addition, training SCA and OptOU requires only a single pre-annotated RGB seed image, owing to their small volume of learnable parameters. Extensive experiments clearly demonstrate that the proposed LAM can generate high-fidelity annotations (almost 100% in mIoU) for multiple real-world datasets (i.e., Camvid, Cityscapes, and Apolloscapes) and CARLA simulation dataset.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02972', 134)">Copy Link</button>
<div id="copy-message-134" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02984">Learning Efficient Flocking Control based on Gibbs Random Fields</a></h1>
<p><b>Authors:</b> Dengyu Zhang,  Chenghao, Feng Xue, Qingrui Zhang</p>
<p>Abstract: Flocking control is essential for multi-robot systems in diverse applications, yet achieving efficient flocking in congested environments poses challenges regarding computation burdens, performance optimality, and motion safety. This paper addresses these challenges through a multi-agent reinforcement learning (MARL) framework built on Gibbs Random Fields (GRFs). With GRFs, a multi-robot system is represented by a set of random variables conforming to a joint probability distribution, thus offering a fresh perspective on flocking reward design. A decentralized training and execution mechanism, which enhances the scalability of MARL concerning robot quantity, is realized using a GRF-based credit assignment method. An action attention module is introduced to implicitly anticipate the motion intentions of neighboring robots, consequently mitigating potential non-stationarity issues in MARL. The proposed framework enables learning an efficient distributed control policy for multi-robot systems in challenging environments with success rate around $99\%$, as demonstrated through thorough comparisons with state-of-the-art solutions in simulations and experiments. Ablation studies are also performed to validate the efficiency of different framework modules.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02984', 135)">Copy Link</button>
<div id="copy-message-135" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02988">Training an LLM-as-a-Judge Model: Pipeline, Insights, and Practical Lessons</a></h1>
<p><b>Authors:</b> Renjun Hu, Yi Cheng, Libin Meng, Jiaxin Xia, Yi Zong, Xing Shi, Wei Lin</p>
<p>Abstract: The rapid advancement of large language models (LLMs) has opened new possibilities for their adoption as evaluative judges. This paper introduces Themis, a fine-tuned LLM judge that delivers sophisticated context-aware evaluations. We provide a comprehensive overview of the development pipeline for Themis, highlighting its scenario-dependent evaluation prompts and two novel methods for controlled instruction generation. These designs enable Themis to effectively distill evaluative skills from teacher models, while retaining flexibility for continuous development. We introduce two human-labeled benchmarks for meta-evaluation, demonstrating that Themis can achieve high alignment with human preferences in an economical manner. Additionally, we explore insights into the LLM-as-a-judge paradigm, revealing nuances in performance and the varied effects of reference answers. Notably, we observe that pure knowledge distillation from strong LLMs, though common, does not guarantee performance improvement through scaling. We propose a mitigation strategy based on instruction-following difficulty. Furthermore, we provide practical guidelines covering data balancing, prompt customization, multi-objective training, and metric aggregation. We aim for our method and findings, along with the fine-tuning data, benchmarks, and model checkpoints, to support future research and development in this area.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02988', 136)">Copy Link</button>
<div id="copy-message-136" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.02996">Building Bridges between Regression, Clustering, and Classification</a></h1>
<p><b>Authors:</b> Lawrence Stewart (DI-ENS, LIENS, Inria), Francis Bach (LIENS, SIERRA), Quentin Berthet</p>
<p>Abstract: Regression, the task of predicting a continuous scalar target y based on some features x is one of the most fundamental tasks in machine learning and statistics. It has been observed and theoretically analyzed that the classical approach, meansquared error minimization, can lead to suboptimal results when training neural networks. In this work, we propose a new method to improve the training of these models on regression tasks, with continuous scalar targets. Our method is based on casting this task in a different fashion, using a target encoder, and a prediction decoder, inspired by approaches in classification and clustering. We showcase the performance of our method on a wide range of real-world datasets.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02996', 137)">Copy Link</button>
<div id="copy-message-137" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03005">Driver Assistance System Based on Multimodal Data Hazard Detection</a></h1>
<p><b>Authors:</b> Long Zhouxiang, Ovanes Petrosian</p>
<p>Abstract: Autonomous driving technology has advanced significantly, yet detecting driving anomalies remains a major challenge due to the long-tailed distribution of driving events. Existing methods primarily rely on single-modal road condition video data, which limits their ability to capture rare and unpredictable driving incidents. This paper proposes a multimodal driver assistance detection system that integrates road condition video, driver facial video, and audio data to enhance incident recognition accuracy. Our model employs an attention-based intermediate fusion strategy, enabling end-to-end learning without separate feature extraction. To support this approach, we develop a new three-modality dataset using a driving simulator. Experimental results demonstrate that our method effectively captures cross-modal correlations, reducing misjudgments and improving driving safety.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03005', 138)">Copy Link</button>
<div id="copy-message-138" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03006">An Augmented Backward-Corrected Projector Splitting Integrator for Dynamical Low-Rank Training</a></h1>
<p><b>Authors:</b> Jonas Kusch, Steffen Schotth\"ofer, Alexandra Walter</p>
<p>Abstract: Layer factorization has emerged as a widely used technique for training memory-efficient neural networks. However, layer factorization methods face several challenges, particularly a lack of robustness during the training process. To overcome this limitation, dynamical low-rank training methods have been developed, utilizing robust time integration techniques for low-rank matrix differential equations. Although these approaches facilitate efficient training, they still depend on computationally intensive QR and singular value decompositions of matrices with small rank. In this work, we introduce a novel low-rank training method that reduces the number of required QR decompositions. Our approach integrates an augmentation step into a projector-splitting scheme, ensuring convergence to a locally optimal solution. We provide a rigorous theoretical analysis of the proposed method and demonstrate its effectiveness across multiple benchmarks.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03006', 139)">Copy Link</button>
<div id="copy-message-139" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03016">An analysis of optimization problems involving ReLU neural networks</a></h1>
<p><b>Authors:</b> Christoph Plate, Mirko Hahn, Alexander Klimek, Caroline Ganzer, Kai Sundmacher, Sebastian Sager</p>
<p>Abstract: Solving mixed-integer optimization problems with embedded neural networks with ReLU activation functions is challenging. Big-M coefficients that arise in relaxing binary decisions related to these functions grow exponentially with the number of layers. We survey and propose different approaches to analyze and improve the run time behavior of mixed-integer programming solvers in this context. Among them are clipped variants and regularization techniques applied during training as well as optimization-based bound tightening and a novel scaling for given ReLU networks. We numerically compare these approaches for three benchmark problems from the literature. We use the number of linear regions, the percentage of stable neurons, and overall computational effort as indicators. As a major takeaway we observe and quantify a trade-off between the often desired redundancy of neural network models versus the computational costs for solving related optimization problems.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03016', 140)">Copy Link</button>
<div id="copy-message-140" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03034">Knowledge Distillation from Large Language Models for Household Energy Modeling</a></h1>
<p><b>Authors:</b> Mohannad Takrouri, Nicol\'as M. Cuadrado, Martin Tak\'a\v{c}</p>
<p>Abstract: Machine learning (ML) is increasingly vital for smart-grid research, yet restricted access to realistic, diverse data - often due to privacy concerns - slows progress and fuels doubts within the energy sector about adopting ML-based strategies. We propose integrating Large Language Models (LLMs) in energy modeling to generate realistic, culturally sensitive, and behavior-specific data for household energy usage across diverse geographies. In this study, we employ and compare five different LLMs to systematically produce family structures, weather patterns, and daily consumption profiles for households in six distinct countries. A four-stage methodology synthesizes contextual daily data, including culturally nuanced activities, realistic weather ranges, HVAC operations, and distinct `energy signatures' that capture unique consumption footprints. Additionally, we explore an alternative strategy where external weather datasets can be directly integrated, bypassing intermediate weather modeling stages while ensuring physically consistent data inputs. The resulting dataset provides insights into how cultural, climatic, and behavioral factors converge to shape carbon emissions, offering a cost-effective avenue for scenario-based energy optimization. This approach underscores how prompt engineering, combined with knowledge distillation, can advance sustainable energy research and climate mitigation efforts. Source code is available at https://github.com/Singularity-AI-Lab/LLM-Energy-Knowledge-Distillation .</p>
<p>URLs: <a href="https://github.com/Singularity-AI-Lab/LLM-Energy-Knowledge-Distillation">https://github.com/Singularity-AI-Lab/LLM-Energy-Knowledge-Distillation</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03034, https://github.com/Singularity-AI-Lab/LLM-Energy-Knowledge-Distillation', 141)">Copy Link</button>
<div id="copy-message-141" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03038">The Cake that is Intelligence and Who Gets to Bake it: An AI Analogy and its Implications for Participation</a></h1>
<p><b>Authors:</b> Martin Mundt, Anaelia Ovalle, Felix Friedrich, Pranav Agrawal, Subarnaduti Paul, Manuel Brack, Kristian Kersting, William Agnew</p>
<p>Abstract: In a widely popular analogy by Turing Award Laureate Yann LeCun, machine intelligence has been compared to cake - where unsupervised learning forms the base, supervised learning adds the icing, and reinforcement learning is the cherry on top. We expand this 'cake that is intelligence' analogy from a simple structural metaphor to the full life-cycle of AI systems, extending it to sourcing of ingredients (data), conception of recipes (instructions), the baking process (training), and the tasting and selling of the cake (evaluation and distribution). Leveraging our re-conceptualization, we describe each step's entailed social ramifications and how they are bounded by statistical assumptions within machine learning. Whereas these technical foundations and social impacts are deeply intertwined, they are often studied in isolation, creating barriers that restrict meaningful participation. Our re-conceptualization paves the way to bridge this gap by mapping where technical foundations interact with social outcomes, highlighting opportunities for cross-disciplinary dialogue. Finally, we conclude with actionable recommendations at each stage of the metaphorical AI cake's life-cycle, empowering prospective AI practitioners, users, and researchers, with increased awareness and ability to engage in broader AI discourse.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03038', 142)">Copy Link</button>
<div id="copy-message-142" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03041">Large Language Models Are Universal Recommendation Learners</a></h1>
<p><b>Authors:</b> Junguang Jiang, Yanwen Huang, Bin Liu, Xiaoyu Kong, Ziru Xu, Han Zhu, Jian Xu, Bo Zheng</p>
<p>Abstract: In real-world recommender systems, different tasks are typically addressed using supervised learning on task-specific datasets with carefully designed model architectures. We demonstrate that large language models (LLMs) can function as universal recommendation learners, capable of handling multiple tasks within a unified input-output framework, eliminating the need for specialized model designs. To improve the recommendation performance of LLMs, we introduce a multimodal fusion module for item representation and a sequence-in-set-out approach for efficient candidate generation. When applied to industrial-scale data, our LLM achieves competitive results with expert models elaborately designed for different recommendation tasks. Furthermore, our analysis reveals that recommendation outcomes are highly sensitive to text input, highlighting the potential of prompt engineering in optimizing industrial-scale recommender systems.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03041', 143)">Copy Link</button>
<div id="copy-message-143" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03062">Time Series Anomaly Detection in the Frequency Domain with Statistical Reliability</a></h1>
<p><b>Authors:</b> Akifumi Yamada, Tomohiro Shiraishi, Shuichi Nishino, Teruyuki Katsuoka, Kouichi Taji, Ichiro Takeuchi</p>
<p>Abstract: Effective anomaly detection in complex systems requires identifying change points (CPs) in the frequency domain, as abnormalities often arise across multiple frequencies. This paper extends recent advancements in statistically significant CP detection, based on Selective Inference (SI), to the frequency domain. The proposed SI method quantifies the statistical significance of detected CPs in the frequency domain using $p$-values, ensuring that the detected changes reflect genuine structural shifts in the target system. We address two major technical challenges to achieve this. First, we extend the existing SI framework to the frequency domain by appropriately utilizing the properties of discrete Fourier transform (DFT). Second, we develop an SI method that provides valid $p$-values for CPs where changes occur across multiple frequencies. Experimental results demonstrate that the proposed method reliably identifies genuine CPs with strong statistical guarantees, enabling more accurate root-cause analysis in the frequency domain of complex systems.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03062', 144)">Copy Link</button>
<div id="copy-message-144" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03067">Optimizing Electric Vehicles Charging using Large Language Models and Graph Neural Networks</a></h1>
<p><b>Authors:</b> Stavros Orfanoudakis, Peter Palensky, Pedro P. Vergara</p>
<p>Abstract: Maintaining grid stability amid widespread electric vehicle (EV) adoption is vital for sustainable transportation. Traditional optimization methods and Reinforcement Learning (RL) approaches often struggle with the high dimensionality and dynamic nature of real-time EV charging, leading to sub-optimal solutions. To address these challenges, this study demonstrates that combining Large Language Models (LLMs), for sequence modeling, with Graph Neural Networks (GNNs), for relational information extraction, not only outperforms conventional EV smart charging methods, but also paves the way for entirely new research directions and innovative solutions.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03067', 145)">Copy Link</button>
<div id="copy-message-145" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03078">Automatic Prompt Optimization Techniques: Exploring the Potential for Synthetic Data Generation</a></h1>
<p><b>Authors:</b> Nina Freise, Marius Heitlinger, Ruben Nuredini, Gerrit Meixner</p>
<p>Abstract: Artificial Intelligence (AI) advancement is heavily dependent on access to large-scale, high-quality training data. However, in specialized domains such as healthcare, data acquisition faces significant constraints due to privacy regulations, ethical considerations, and limited availability. While synthetic data generation offers a promising solution, conventional approaches typically require substantial real data for training generative models. The emergence of large-scale prompt-based models presents new opportunities for synthetic data generation without direct access to protected data. However, crafting effective prompts for domain-specific data generation remains challenging, and manual prompt engineering proves insufficient for achieving output with sufficient precision and authenticity. We review recent developments in automatic prompt optimization, following PRISMA guidelines. We analyze six peer-reviewed studies published between 2020 and 2024 that focus on automatic data-free prompt optimization methods. Our analysis reveals three approaches: feedback-driven, error-based, and control-theoretic. Although all approaches demonstrate promising capabilities in prompt refinement and adaptation, our findings suggest the need for an integrated framework that combines complementary optimization techniques to enhance synthetic data generation while minimizing manual intervention. We propose future research directions toward developing robust, iterative prompt optimization frameworks capable of improving the quality of synthetic data. This advancement can be particularly crucial for sensitive fields and in specialized domains where data access is restricted, potentially transforming how we approach synthetic data generation for AI development.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03078', 146)">Copy Link</button>
<div id="copy-message-146" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03081">Human-Aligned Image Models Improve Visual Decoding from the Brain</a></h1>
<p><b>Authors:</b> Nona Rajabi, Ant\^onio H. Ribeiro, Miguel Vasco, Farzaneh Taleb, M\r{a}rten Bj\"orkman, Danica Kragic</p>
<p>Abstract: Decoding visual images from brain activity has significant potential for advancing brain-computer interaction and enhancing the understanding of human perception. Recent approaches align the representation spaces of images and brain activity to enable visual decoding. In this paper, we introduce the use of human-aligned image encoders to map brain signals to images. We hypothesize that these models more effectively capture perceptual attributes associated with the rapid visual stimuli presentations commonly used in visual brain data recording experiments. Our empirical results support this hypothesis, demonstrating that this simple modification improves image retrieval accuracy by up to 21% compared to state-of-the-art methods. Comprehensive experiments confirm consistent performance improvements across diverse EEG architectures, image encoders, alignment methods, participants, and brain imaging modalities.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03081', 147)">Copy Link</button>
<div id="copy-message-147" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03086">Implementing Large Quantum Boltzmann Machines as Generative AI Models for Dataset Balancing</a></h1>
<p><b>Authors:</b> Salvatore Sinno, Markus Bertl, Arati Sahoo, Bhavika Bhalgamiya, Thomas Gro{\ss}, Nicholas Chancellor</p>
<p>Abstract: This study explores the implementation of large Quantum Restricted Boltzmann Machines (QRBMs), a key advancement in Quantum Machine Learning (QML), as generative models on D-Wave's Pegasus quantum hardware to address dataset imbalance in Intrusion Detection Systems (IDS). By leveraging Pegasus's enhanced connectivity and computational capabilities, a QRBM with 120 visible and 120 hidden units was successfully embedded, surpassing the limitations of default embedding tools. The QRBM synthesized over 1.6 million attack samples, achieving a balanced dataset of over 4.2 million records. Comparative evaluations with traditional balancing methods, such as SMOTE and RandomOversampler, revealed that QRBMs produced higher-quality synthetic samples, significantly improving detection rates, precision, recall, and F1 score across diverse classifiers. The study underscores the scalability and efficiency of QRBMs, completing balancing tasks in milliseconds. These findings highlight the transformative potential of QML and QRBMs as next-generation tools in data preprocessing, offering robust solutions for complex computational challenges in modern information systems.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03086', 148)">Copy Link</button>
<div id="copy-message-148" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03100">A Bayesian perspective on single-shot laser characterization</a></h1>
<p><b>Authors:</b> J. Esslinger, N. Weisse, C. Eberle, J. Schroeder, S. Howard, P. Norreys, S. Karsch, A. D\"opp</p>
<p>Abstract: We introduce a Bayesian framework for measuring spatio-temporal couplings (STCs) in ultra-intense lasers that reconceptualizes what constitutes a 'single-shot' measurement. Moving beyond traditional distinctions between single- and multi-shot devices, our approach provides rigorous criteria for determining when measurements can truly resolve individual laser shots rather than statistical averages. This framework shows that single-shot capability is not an intrinsic device property but emerges from the relationship between measurement precision and inherent parameter variability. Implementing this approach with a new measurement device at the ATLAS-3000 petawatt laser, we provide the first quantitative uncertainty bounds on pulse front tilt and curvature. Notably, we observe that our Bayesian method reduces uncertainty by up to 60% compared to traditional approaches. Through this analysis, we reveal how the interplay between measurement precision and intrinsic system variability defines achievable resolution -- insights that have direct implications for applications where precise control of laser-matter interaction is critical.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03100', 149)">Copy Link</button>
<div id="copy-message-149" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03103">Edge Attention Module for Object Classification</a></h1>
<p><b>Authors:</b> Santanu Roy, Ashvath Suresh, Archit Gupta</p>
<p>Abstract: A novel ``edge attention-based Convolutional Neural Network (CNN)'' is proposed in this research for object classification task. With the advent of advanced computing technology, CNN models have achieved to remarkable success, particularly in computer vision applications. Nevertheless, the efficacy of the conventional CNN is often hindered due to class imbalance and inter-class similarity problems, which are particularly prominent in the computer vision field. In this research, we introduce for the first time an ``Edge Attention Module (EAM)'' consisting of a Max-Min pooling layer, followed by convolutional layers. This Max-Min pooling is entirely a novel pooling technique, specifically designed to capture only the edge information that is crucial for any object classification task. Therefore, by integrating this novel pooling technique into the attention module, the CNN network inherently prioritizes on essential edge features, thereby boosting the accuracy and F1-score of the model significantly. We have implemented our proposed EAM or 2EAMs on several standard pre-trained CNN models for Caltech-101, Caltech-256, CIFAR-100 and Tiny ImageNet-200 datasets. The extensive experiments reveal that our proposed framework (that is, EAM with CNN and 2EAMs with CNN), outperforms all pre-trained CNN models as well as recent trend models ``Pooling-based Vision Transformer (PiT)'', ``Convolutional Block Attention Module (CBAM)'', and ConvNext, by substantial margins. We have achieved the accuracy of 95.5% and 86% by the proposed framework on Caltech-101 and Caltech-256 datasets, respectively. So far, this is the best results on these datasets, to the best of our knowledge.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03103', 150)">Copy Link</button>
<div id="copy-message-150" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03111">Policies and Evaluation for Online Meeting Summarization</a></h1>
<p><b>Authors:</b> Felix Schneider (Zoom Communications), Marco Turchi (Zoom Communications), Alex Waibel (Karlsruhe Institute of Technology)</p>
<p>Abstract: With more and more meetings moving to a digital domain, meeting summarization has recently gained interest in both academic and commercial research. However, prior academic research focuses on meeting summarization as an offline task, performed after the meeting concludes. In this paper, we perform the first systematic study of online meeting summarization. For this purpose, we propose several policies for conducting online summarization. We discuss the unique challenges of this task compared to the offline setting and define novel metrics to evaluate latency and partial summary quality. The experiments on the AutoMin dataset show that 1) online models can produce strong summaries, 2) our metrics allow a detailed analysis of different systems' quality-latency trade-off, also taking into account intermediate outputs and 3) adaptive policies perform better than fixed scheduled ones. These findings provide a starting point for the wider research community to explore this important task.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03111', 151)">Copy Link</button>
<div id="copy-message-151" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03119">Comparison of the Cox proportional hazards model and Random Survival Forest algorithm for predicting patient-specific survival probabilities in clinical trial data</a></h1>
<p><b>Authors:</b> Ricarda Graf, Susan Todd, M. Fazil Baksh</p>
<p>Abstract: The Cox proportional hazards model is often used for model development in data from randomized controlled trials (RCT) with time-to-event outcomes. Random survival forests (RSF) is a machine-learning algorithm known for its high predictive performance. We conduct a comprehensive neutral comparison study to compare the predictive performance of Cox regression and RSF in real-world as well as simulated data. Performance is compared using multiple performance measures according to recommendations for the comparison of prognostic prediction models. We found that while the RSF usually outperforms the Cox model when using the $C$ index, Cox model predictions may be better calibrated. With respect to overall performance, the Cox model often exceeds the RSF in nonproportional hazards settings, while otherwise the RSF typically performs better especially for smaller sample sizes. Overall performance of the RSF is more affected by higher censoring rates, while overall performance of the Cox model suffers more from smaller sample sizes.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03119', 152)">Copy Link</button>
<div id="copy-message-152" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03125">Double Distillation Network for Multi-Agent Reinforcement Learning</a></h1>
<p><b>Authors:</b> Yang Zhou, Siying Wang, Wenyu Chen, Ruoning Zhang, Zhitong Zhao, Zixuan Zhang</p>
<p>Abstract: Multi-agent reinforcement learning typically employs a centralized training-decentralized execution (CTDE) framework to alleviate the non-stationarity in environment. However, the partial observability during execution may lead to cumulative gap errors gathered by agents, impairing the training of effective collaborative policies. To overcome this challenge, we introduce the Double Distillation Network (DDN), which incorporates two distillation modules aimed at enhancing robust coordination and facilitating the collaboration process under constrained information. The external distillation module uses a global guiding network and a local policy network, employing distillation to reconcile the gap between global training and local execution. In addition, the internal distillation module introduces intrinsic rewards, drawn from state information, to enhance the exploration capabilities of agents. Extensive experiments demonstrate that DDN significantly improves performance across multiple scenarios.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03125', 153)">Copy Link</button>
<div id="copy-message-153" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03128">Metis: A Foundation Speech Generation Model with Masked Generative Pre-training</a></h1>
<p><b>Authors:</b> Yuancheng Wang, Jiachen Zheng, Junan Zhang, Xueyao Zhang, Huan Liao, Zhizheng Wu</p>
<p>Abstract: We introduce Metis, a foundation model for unified speech generation. Unlike previous task-specific or multi-task models, Metis follows a pre-training and fine-tuning paradigm. It is pre-trained on large-scale unlabeled speech data using masked generative modeling and then fine-tuned to adapt to diverse speech generation tasks. Specifically, 1) Metis utilizes two discrete speech representations: SSL tokens derived from speech self-supervised learning (SSL) features, and acoustic tokens directly quantized from waveforms. 2) Metis performs masked generative pre-training on SSL tokens, utilizing 300K hours of diverse speech data, without any additional condition. 3) Through fine-tuning with task-specific conditions, Metis achieves efficient adaptation to various speech generation tasks while supporting multimodal input, even when using limited data and trainable parameters. Experiments demonstrate that Metis can serve as a foundation model for unified speech generation: Metis outperforms state-of-the-art task-specific or multi-task systems across five speech generation tasks, including zero-shot text-to-speech, voice conversion, target speaker extraction, speech enhancement, and lip-to-speech, even with fewer than 20M trainable parameters or 300 times less training data. Audio samples are are available at https://metis-demo.github.io/.</p>
<p>URLs: <a href="https://metis-demo.github.io/.">https://metis-demo.github.io/.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03128, https://metis-demo.github.io/.', 154)">Copy Link</button>
<div id="copy-message-154" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03129">Teaching Large Language Models Number-Focused Headline Generation With Key Element Rationales</a></h1>
<p><b>Authors:</b> Zhen Qian, Xiuzhen Zhang, Xiaofei Xu, Feng Xia</p>
<p>Abstract: Number-focused headline generation is a summarization task requiring both high textual quality and precise numerical accuracy, which poses a unique challenge for Large Language Models (LLMs). Existing studies in the literature focus only on either textual quality or numerical reasoning and thus are inadequate to address this challenge. In this paper, we propose a novel chain-of-thought framework for using rationales comprising key elements of the Topic, Entities, and Numerical reasoning (TEN) in news articles to enhance the capability for LLMs to generate topic-aligned high-quality texts with precise numerical accuracy. Specifically, a teacher LLM is employed to generate TEN rationales as supervision data, which are then used to teach and fine-tune a student LLM. Our approach teaches the student LLM automatic generation of rationales with enhanced capability for numerical reasoning and topic-aligned numerical headline generation. Experiments show that our approach achieves superior performance in both textual quality and numerical accuracy.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03129', 155)">Copy Link</button>
<div id="copy-message-155" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03135">Underwater Soft Fin Flapping Motion with Deep Neural Network Based Surrogate Model</a></h1>
<p><b>Authors:</b> Yuya Hamamatsu, Pavlo Kupyn, Roza Gkliva, Asko Ristolainen, Maarja Kruusmaa</p>
<p>Abstract: This study presents a novel framework for precise force control of fin-actuated underwater robots by integrating a deep neural network (DNN)-based surrogate model with reinforcement learning (RL). To address the complex interactions with the underwater environment and the high experimental costs, a DNN surrogate model acts as a simulator for enabling efficient training for the RL agent. Additionally, grid-switching control is applied to select optimized models for specific force reference ranges, improving control accuracy and stability. Experimental results show that the RL agent, trained in the surrogate simulation, generates complex thrust motions and achieves precise control of a real soft fin actuator. This approach provides an efficient control solution for fin-actuated robots in challenging underwater environments.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03135', 156)">Copy Link</button>
<div id="copy-message-156" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03139">Fast Sampling of Cosmological Initial Conditions with Gaussian Neural Posterior Estimation</a></h1>
<p><b>Authors:</b> Oleg Savchenko, Guillermo Franco Abell\'an, Florian List, Noemi Anau Montel, Christoph Weniger</p>
<p>Abstract: Knowledge of the primordial matter density field from which the large-scale structure of the Universe emerged over cosmic time is of fundamental importance for cosmology. However, reconstructing these cosmological initial conditions from late-time observations is a notoriously difficult task, which requires advanced cosmological simulators and sophisticated statistical methods to explore a multi-million-dimensional parameter space. We show how simulation-based inference (SBI) can be used to tackle this problem and to obtain data-constrained realisations of the primordial dark matter density field in a simulation-efficient way with general non-differentiable simulators. Our method is applicable to full high-resolution dark matter $N$-body simulations and is based on modelling the posterior distribution of the constrained initial conditions to be Gaussian with a diagonal covariance matrix in Fourier space. As a result, we can generate thousands of posterior samples within seconds on a single GPU, orders of magnitude faster than existing methods, paving the way for sequential SBI for cosmological fields. Furthermore, we perform an analytical fit of the estimated dependence of the covariance on the wavenumber, effectively transforming any point-estimator of initial conditions into a fast sampler. We test the validity of our obtained samples by comparing them to the true values with summary statistics and performing a Bayesian consistency test.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03139', 157)">Copy Link</button>
<div id="copy-message-157" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03163">Signature Reconstruction from Randomized Signatures</a></h1>
<p><b>Authors:</b> Mie Gl\"uckstad, Nicola Muca Cirone, Josef Teichmann</p>
<p>Abstract: Controlled ordinary differential equations driven by continuous bounded variation curves can be considered a continuous time analogue of recurrent neural networks for the construction of expressive features of the input curves. We ask up to which extent well known signature features of such curves can be reconstructed from controlled ordinary differential equations with (untrained) random vector fields. The answer turns out to be algebraically involved, but essentially the number of signature features, which can be reconstructed from the non-linear flow of the controlled ordinary differential equation, is exponential in its hidden dimension, when the vector fields are chosen to be neural with depth two. Moreover, we characterize a general linear independence condition on arbitrary vector fields, under which the signature features up to some fixed order can always be reconstructed. Algebraically speaking this complements in a quantitative manner several well known results from the theory of Lie algebras of vector fields and puts them in a context of machine learning.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03163', 158)">Copy Link</button>
<div id="copy-message-158" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03183">MaxInfo: A Training-Free Key-Frame Selection Method Using Maximum Volume for Enhanced Video Understanding</a></h1>
<p><b>Authors:</b> Pengyi Li, Irina Abdullaeva, Alexander Gambashidze, Andrey Kuznetsov, Ivan Oseledets</p>
<p>Abstract: Modern Video Large Language Models (VLLMs) often rely on uniform frame sampling for video understanding, but this approach frequently fails to capture critical information due to frame redundancy and variations in video content. We propose MaxInfo, a training-free method based on the maximum volume principle, which selects and retains the most representative frames from the input video. By maximizing the geometric volume formed by selected embeddings, MaxInfo ensures that the chosen frames cover the most informative regions of the embedding space, effectively reducing redundancy while preserving diversity. This method enhances the quality of input representations and improves long video comprehension performance across benchmarks. For instance, MaxInfo achieves a 3.28% improvement on LongVideoBench and a 6.4% improvement on EgoSchema for LLaVA-Video-7B. It also achieves a 3.47% improvement for LLaVA-Video-72B. The approach is simple to implement and works with existing VLLMs without the need for additional training, making it a practical and effective alternative to traditional uniform sampling methods.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03183', 159)">Copy Link</button>
<div id="copy-message-159" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03198">SimSort: A Powerful Framework for Spike Sorting by Large-Scale Electrophysiology Simulation</a></h1>
<p><b>Authors:</b> Yimu Zhang, Dongqi Han, Yansen Wang, Yu Gu, Dongsheng Li</p>
<p>Abstract: Spike sorting is an essential process in neural recording, which identifies and separates electrical signals from individual neurons recorded by electrodes in the brain, enabling researchers to study how specific neurons communicate and process information. Although there exist a number of spike sorting methods which have contributed to significant neuroscientific breakthroughs, many are heuristically designed, making it challenging to verify their correctness due to the difficulty of obtaining ground truth labels from real-world neural recordings. In this work, we explore a data-driven, deep learning-based approach. We begin by creating a large-scale dataset through electrophysiology simulations using biologically realistic computational models. We then present \textbf{SimSort}, a pretraining framework for spike sorting. Remarkably, when trained on our simulated dataset, SimSort demonstrates strong zero-shot generalization to real-world spike sorting tasks, significantly outperforming existing methods. Our findings underscore the potential of data-driven techniques to enhance the reliability and scalability of spike sorting in experimental neuroscience.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03198', 160)">Copy Link</button>
<div id="copy-message-160" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03200">CORTEX: A Cost-Sensitive Rule and Tree Extraction Method</a></h1>
<p><b>Authors:</b> Marija Kopanja, Milo\v{s} Savi\'c, Luca Longo</p>
<p>Abstract: Tree-based and rule-based machine learning models play pivotal roles in explainable artificial intelligence (XAI) due to their unique ability to provide explanations in the form of tree or rule sets that are easily understandable and interpretable, making them essential for applications in which trust in model decisions is necessary. These transparent models are typically used in surrogate modeling, a post-hoc XAI approach for explaining the logic of black-box models, enabling users to comprehend and trust complex predictive systems while maintaining competitive performance. This study proposes the Cost-Sensitive Rule and Tree Extraction (CORTEX) method, a novel rule-based XAI algorithm grounded in the multi-class cost-sensitive decision tree (CSDT) method. The original version of the CSDT is extended to classification problems with more than two classes by inducing the concept of an n-dimensional class-dependent cost matrix. The performance of CORTEX as a rule-extractor XAI method is compared to other post-hoc tree and rule extraction methods across several datasets with different numbers of classes. Several quantitative evaluation metrics are employed to assess the explainability of generated rule sets. Our findings demonstrate that CORTEX is competitive with other tree-based methods and can be superior to other rule-based methods across different datasets. The extracted rule sets suggest the advantages of using the CORTEX method over other methods by producing smaller rule sets with shorter rules on average across datasets with a diverse number of classes. Overall, the results underscore the potential of CORTEX as a powerful XAI tool for scenarios that require the generation of clear, human-understandable rules while maintaining good predictive performance.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03200', 161)">Copy Link</button>
<div id="copy-message-161" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03210">From Kernels to Features: A Multi-Scale Adaptive Theory of Feature Learning</a></h1>
<p><b>Authors:</b> Noa Rubin, Kirsten Fischer, Javed Lindner, David Dahmen, Inbar Seroussi, Zohar Ringel, Michael Kr\"amer, Moritz Helias</p>
<p>Abstract: Theoretically describing feature learning in neural networks is crucial for understanding their expressive power and inductive biases, motivating various approaches. Some approaches describe network behavior after training through a simple change in kernel scale from initialization, resulting in a generalization power comparable to a Gaussian process. Conversely, in other approaches training results in the adaptation of the kernel to the data, involving complex directional changes to the kernel. While these approaches capture different facets of network behavior, their relationship and respective strengths across scaling regimes remains an open question. This work presents a theoretical framework of multi-scale adaptive feature learning bridging these approaches. Using methods from statistical mechanics, we derive analytical expressions for network output statistics which are valid across scaling regimes and in the continuum between them. A systematic expansion of the network's probability distribution reveals that mean-field scaling requires only a saddle-point approximation, while standard scaling necessitates additional correction terms. Remarkably, we find across regimes that kernel adaptation can be reduced to an effective kernel rescaling when predicting the mean network output of a linear network. However, even in this case, the multi-scale adaptive approach captures directional feature learning effects, providing richer insights than what could be recovered from a rescaling of the kernel alone.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03210', 162)">Copy Link</button>
<div id="copy-message-162" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03238">Long-tailed Medical Diagnosis with Relation-aware Representation Learning and Iterative Classifier Calibration</a></h1>
<p><b>Authors:</b> Li Pan, Yupei Zhang, Qiushi Yang, Tan Li, Zhen Chen</p>
<p>Abstract: Recently computer-aided diagnosis has demonstrated promising performance, effectively alleviating the workload of clinicians. However, the inherent sample imbalance among different diseases leads algorithms biased to the majority categories, leading to poor performance for rare categories. Existing works formulated this challenge as a long-tailed problem and attempted to tackle it by decoupling the feature representation and classification. Yet, due to the imbalanced distribution and limited samples from tail classes, these works are prone to biased representation learning and insufficient classifier calibration. To tackle these problems, we propose a new Long-tailed Medical Diagnosis (LMD) framework for balanced medical image classification on long-tailed datasets. In the initial stage, we develop a Relation-aware Representation Learning (RRL) scheme to boost the representation ability by encouraging the encoder to capture intrinsic semantic features through different data augmentations. In the subsequent stage, we propose an Iterative Classifier Calibration (ICC) scheme to calibrate the classifier iteratively. This is achieved by generating a large number of balanced virtual features and fine-tuning the encoder using an Expectation-Maximization manner. The proposed ICC compensates for minority categories to facilitate unbiased classifier optimization while maintaining the diagnostic knowledge in majority classes. Comprehensive experiments on three public long-tailed medical datasets demonstrate that our LMD framework significantly surpasses state-of-the-art approaches. The source code can be accessed at https://github.com/peterlipan/LMD.</p>
<p>URLs: <a href="https://github.com/peterlipan/LMD.">https://github.com/peterlipan/LMD.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03238, https://github.com/peterlipan/LMD.', 163)">Copy Link</button>
<div id="copy-message-163" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03261">CARROT: A Cost Aware Rate Optimal Router</a></h1>
<p><b>Authors:</b> Seamus Somerstep, Felipe Maia Polo, Allysson Flavio Melo de Oliveira, Prattyush Mangal, M\'irian Silva, Onkar Bhardwaj, Mikhail Yurochkin, Subha Maity</p>
<p>Abstract: With the rapid growth in the number of Large Language Models (LLMs), there has been a recent interest in LLM routing, or directing queries to the cheapest LLM that can deliver a suitable response. Following this line of work, we introduce CARROT, a Cost AwaRe Rate Optimal rouTer that can select models based on any desired trade-off between performance and cost. Given a query, CARROT selects a model based on estimates of models' cost and performance. Its simplicity lends CARROT computational efficiency, while our theoretical analysis demonstrates minimax rate-optimality in its routing performance. Alongside CARROT, we also introduce the Smart Price-aware Routing (SPROUT) dataset to facilitate routing on a wide spectrum of queries with the latest state-of-the-art LLMs. Using SPROUT and prior benchmarks such as Routerbench and open-LLM-leaderboard-v2 we empirically validate CARROT's performance against several alternative routers.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03261', 164)">Copy Link</button>
<div id="copy-message-164" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03270">When Pre-trained Visual Representations Fall Short: Limitations in Visuo-Motor Robot Learning</a></h1>
<p><b>Authors:</b> Nikolaos Tsagkas, Andreas Sochopoulos, Duolikun Danier, Chris Xiaoxuan Lu, Oisin Mac Aodha</p>
<p>Abstract: The integration of pre-trained visual representations (PVRs) into visuo-motor robot learning has emerged as a promising alternative to training visual encoders from scratch. However, PVRs face critical challenges in the context of policy learning, including temporal entanglement and an inability to generalise even in the presence of minor scene perturbations. These limitations hinder performance in tasks requiring temporal awareness and robustness to scene changes. This work identifies these shortcomings and proposes solutions to address them. First, we augment PVR features with temporal perception and a sense of task completion, effectively disentangling them in time. Second, we introduce a module that learns to selectively attend to task-relevant local features, enhancing robustness when evaluated on out-of-distribution scenes. Our experiments demonstrate significant performance improvements, particularly in PVRs trained with masking objectives, and validate the effectiveness of our enhancements in addressing PVR-specific limitations.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03270', 165)">Copy Link</button>
<div id="copy-message-165" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03275">Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning</a></h1>
<p><b>Authors:</b> DiJia Su, Hanlin Zhu, Yingchen Xu, Jiantao Jiao, Yuandong Tian, Qinqing Zheng</p>
<p>Abstract: Large Language Models (LLMs) excel at reasoning and planning when trained on chainof-thought (CoT) data, where the step-by-step thought process is explicitly outlined by text tokens. However, this results in lengthy inputs where many words support textual coherence rather than core reasoning information, and processing these inputs consumes substantial computation resources. In this work, we propose a hybrid representation of the reasoning process, where we partially abstract away the initial reasoning steps using latent discrete tokens generated by VQ-VAE, significantly reducing the length of reasoning traces. We explore the use of latent trace abstractions in two scenarios: 1) training the model from scratch for the Keys-Finding Maze problem, 2) fine-tuning LLMs on this hybrid data with an extended vocabulary including unseen latent tokens, for both logical and mathematical reasoning problems. To facilitate effective learning, we introduce a simple training procedure that randomly mixes latent and text tokens, which enables fast adaptation to new latent tokens. Our approach consistently outperforms the baselines methods in various benchmarks.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03275', 166)">Copy Link</button>
<div id="copy-message-166" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03283">SymAgent: A Neural-Symbolic Self-Learning Agent Framework for Complex Reasoning over Knowledge Graphs</a></h1>
<p><b>Authors:</b> Ben Liu, Jihai Zhang, Fangquan Lin, Cheng Yang, Min Peng, Wotao Yin</p>
<p>Abstract: Recent advancements have highlighted that Large Language Models (LLMs) are prone to hallucinations when solving complex reasoning problems, leading to erroneous results. To tackle this issue, researchers incorporate Knowledge Graphs (KGs) to improve the reasoning ability of LLMs. However, existing methods face two limitations: 1) they typically assume that all answers to the questions are contained in KGs, neglecting the incompleteness issue of KGs, and 2) they treat the KG as a static repository and overlook the implicit logical reasoning structures inherent in KGs. In this paper, we introduce SymAgent, an innovative neural-symbolic agent framework that achieves collaborative augmentation between KGs and LLMs. We conceptualize KGs as dynamic environments and transform complex reasoning tasks into a multi-step interactive process, enabling KGs to participate deeply in the reasoning process. SymAgent consists of two modules: Agent-Planner and Agent-Executor. The Agent-Planner leverages LLM's inductive reasoning capability to extract symbolic rules from KGs, guiding efficient question decomposition. The Agent-Executor autonomously invokes predefined action tools to integrate information from KGs and external documents, addressing the issues of KG incompleteness. Furthermore, we design a self-learning framework comprising online exploration and offline iterative policy updating phases, enabling the agent to automatically synthesize reasoning trajectories and improve performance. Experimental results demonstrate that SymAgent with weak LLM backbones (i.e., 7B series) yields better or comparable performance compared to various strong baselines. Further analysis reveals that our agent can identify missing triples, facilitating automatic KG updates.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03283', 167)">Copy Link</button>
<div id="copy-message-167" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03292">ALPET: Active Few-shot Learning for Citation Worthiness Detection in Low-Resource Wikipedia Languages</a></h1>
<p><b>Authors:</b> Aida Halitaj, Arkaitz Zubiaga</p>
<p>Abstract: Citation Worthiness Detection (CWD) consists in determining which sentences, within an article or collection, should be backed up with a citation to validate the information it provides. This study, introduces ALPET, a framework combining Active Learning (AL) and Pattern-Exploiting Training (PET), to enhance CWD for languages with limited data resources. Applied to Catalan, Basque, and Albanian Wikipedia datasets, ALPET outperforms the existing CCW baseline while reducing the amount of labeled data in some cases above 80\%. ALPET's performance plateaus after 300 labeled samples, showing it suitability for low-resource scenarios where large, labeled datasets are not common. While specific active learning query strategies, like those employing K-Means clustering, can offer advantages, their effectiveness is not universal and often yields marginal gains over random sampling, particularly with smaller datasets. This suggests that random sampling, despite its simplicity, remains a strong baseline for CWD in constraint resource environments. Overall, ALPET's ability to achieve high performance with fewer labeled samples makes it a promising tool for enhancing the verifiability of online content in low-resource language settings.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03292', 168)">Copy Link</button>
<div id="copy-message-168" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03297">IRIS: An Immersive Robot Interaction System</a></h1>
<p><b>Authors:</b> Xinkai Jiang, Qihao Yuan, Enes Ulas Dincer, Hongyi Zhou, Ge Li, Xueyin Li, Julius Haag, Nicolas Schreiber, Kailai Li, Gerhard Neumann, Rudolf Lioutikov</p>
<p>Abstract: This paper introduces IRIS, an immersive Robot Interaction System leveraging Extended Reality (XR), designed for robot data collection and interaction across multiple simulators, benchmarks, and real-world scenarios. While existing XR-based data collection systems provide efficient and intuitive solutions for large-scale data collection, they are often challenging to reproduce and reuse. This limitation arises because current systems are highly tailored to simulator-specific use cases and environments. IRIS is a novel, easily extendable framework that already supports multiple simulators, benchmarks, and even headsets. Furthermore, IRIS is able to include additional information from real-world sensors, such as point clouds captured through depth cameras. A unified scene specification is generated directly from simulators or real-world sensors and transmitted to XR headsets, creating identical scenes in XR. This specification allows IRIS to support any of the objects, assets, and robots provided by the simulators. In addition, IRIS introduces shared spatial anchors and a robust communication protocol that links simulations between multiple XR headsets. This feature enables multiple XR headsets to share a synchronized scene, facilitating collaborative and multi-user data collection. IRIS can be deployed on any device that supports the Unity Framework, encompassing the vast majority of commercially available headsets. In this work, IRIS was deployed and tested on the Meta Quest 3 and the HoloLens 2. IRIS showcased its versatility across a wide range of real-world and simulated scenarios, using current popular robot simulators such as MuJoCo, IsaacSim, CoppeliaSim, and Genesis. In addition, a user study evaluates IRIS on a data collection task for the LIBERO benchmark. The study shows that IRIS significantly outperforms the baseline in both objective and subjective metrics.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03297', 169)">Copy Link</button>
<div id="copy-message-169" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03298">MeDiSumQA: Patient-Oriented Question-Answer Generation from Discharge Letters</a></h1>
<p><b>Authors:</b> Amin Dada, Osman Alperen Koras, Marie Bauer, Amanda Butler, Kaleb E. Smith, Jens Kleesiek, Julian Friedrich</p>
<p>Abstract: While increasing patients' access to medical documents improves medical care, this benefit is limited by varying health literacy levels and complex medical terminology. Large language models (LLMs) offer solutions by simplifying medical information. However, evaluating LLMs for safe and patient-friendly text generation is difficult due to the lack of standardized evaluation resources. To fill this gap, we developed MeDiSumQA. MeDiSumQA is a dataset created from MIMIC-IV discharge summaries through an automated pipeline combining LLM-based question-answer generation with manual quality checks. We use this dataset to evaluate various LLMs on patient-oriented question-answering. Our findings reveal that general-purpose LLMs frequently surpass biomedical-adapted models, while automated metrics correlate with human judgment. By releasing MeDiSumQA on PhysioNet, we aim to advance the development of LLMs to enhance patient understanding and ultimately improve care outcomes.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03298', 170)">Copy Link</button>
<div id="copy-message-170" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03323">Out-of-Distribution Detection using Synthetic Data Generation</a></h1>
<p><b>Authors:</b> Momin Abbas, Muneeza Azmat, Raya Horesh, Mikhail Yurochkin</p>
<p>Abstract: Distinguishing in- and out-of-distribution (OOD) inputs is crucial for reliable deployment of classification systems. However, OOD data is typically unavailable or difficult to collect, posing a significant challenge for accurate OOD detection. In this work, we present a method that harnesses the generative capabilities of Large Language Models (LLMs) to create high-quality synthetic OOD proxies, eliminating the dependency on any external OOD data source. We study the efficacy of our method on classical text classification tasks such as toxicity detection and sentiment classification as well as classification tasks arising in LLM development and deployment, such as training a reward model for RLHF and detecting misaligned generations. Extensive experiments on nine InD-OOD dataset pairs and various model sizes show that our approach dramatically lowers false positive rates (achieving a perfect zero in some cases) while maintaining high accuracy on in-distribution tasks, outperforming baseline methods by a significant margin.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03323', 171)">Copy Link</button>
<div id="copy-message-171" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03327">Is In-Context Universality Enough? MLPs are Also Universal In-Context</a></h1>
<p><b>Authors:</b> Anastasis Kratsios, Takashi Furuya</p>
<p>Abstract: The success of transformers is often linked to their ability to perform in-context learning. Recent work shows that transformers are universal in context, capable of approximating any real-valued continuous function of a context (a probability measure over $\mathcal{X}\subseteq \mathbb{R}^d$) and a query $x\in \mathcal{X}$. This raises the question: Does in-context universality explain their advantage over classical models? We answer this in the negative by proving that MLPs with trainable activation functions are also universal in-context. This suggests the transformer's success is likely due to other factors like inductive bias or training stability.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03327', 172)">Copy Link</button>
<div id="copy-message-172" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03332">A Mixture-Based Framework for Guiding Diffusion Models</a></h1>
<p><b>Authors:</b> Yazid Janati, Badr Moufad, Mehdi Abou El Qassime, Alain Durmus, Eric Moulines, Jimmy Olsson</p>
<p>Abstract: Denoising diffusion models have driven significant progress in the field of Bayesian inverse problems. Recent approaches use pre-trained diffusion models as priors to solve a wide range of such problems, only leveraging inference-time compute and thereby eliminating the need to retrain task-specific models on the same dataset. To approximate the posterior of a Bayesian inverse problem, a diffusion model samples from a sequence of intermediate posterior distributions, each with an intractable likelihood function. This work proposes a novel mixture approximation of these intermediate distributions. Since direct gradient-based sampling of these mixtures is infeasible due to intractable terms, we propose a practical method based on Gibbs sampling. We validate our approach through extensive experiments on image inverse problems, utilizing both pixel- and latent-space diffusion priors, as well as on source separation with an audio diffusion model. The code is available at https://www.github.com/badr-moufad/mgdm</p>
<p>URLs: <a href="https://www.github.com/badr-moufad/mgdm">https://www.github.com/badr-moufad/mgdm</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03332, https://www.github.com/badr-moufad/mgdm', 173)">Copy Link</button>
<div id="copy-message-173" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03341">Adaptive Variational Inference in Probabilistic Graphical Models: Beyond Bethe, Tree-Reweighted, and Convex Free Energies</a></h1>
<p><b>Authors:</b> Harald Leisenberger, Franz Pernkopf</p>
<p>Abstract: Variational inference in probabilistic graphical models aims to approximate fundamental quantities such as marginal distributions and the partition function. Popular approaches are the Bethe approximation, tree-reweighted, and other types of convex free energies. These approximations are efficient but can fail if the model is complex and highly interactive. In this work, we analyze two classes of approximations that include the above methods as special cases: first, if the model parameters are changed; and second, if the entropy approximation is changed. We discuss benefits and drawbacks of either approach, and deduce from this analysis how a free energy approximation should ideally be constructed. Based on our observations, we propose approximations that automatically adapt to a given model and demonstrate their effectiveness for a range of difficult problems.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03341', 174)">Copy Link</button>
<div id="copy-message-174" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03350">Optimal Task Order for Continual Learning of Multiple Tasks</a></h1>
<p><b>Authors:</b> Ziyan Li, Naoki Hiratani</p>
<p>Abstract: Continual learning of multiple tasks remains a major challenge for neural networks. Here, we investigate how task order influences continual learning and propose a strategy for optimizing it. Leveraging a linear teacher-student model with latent factors, we derive an analytical expression relating task similarity and ordering to learning performance. Our analysis reveals two principles that hold under a wide parameter range: (1) tasks should be arranged from the least representative to the most typical, and (2) adjacent tasks should be dissimilar. We validate these rules on both synthetic data and real-world image classification datasets (Fashion-MNIST, CIFAR-10, CIFAR-100), demonstrating consistent performance improvements in both multilayer perceptrons and convolutional neural networks. Our work thus presents a generalizable framework for task-order optimization in task-incremental continual learning.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03350', 175)">Copy Link</button>
<div id="copy-message-175" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03359">GHOST: Gaussian Hypothesis Open-Set Technique</a></h1>
<p><b>Authors:</b> Ryan Rabinowitz, Steve Cruz, Manuel G\"unther, Terrance E. Boult</p>
<p>Abstract: Evaluations of large-scale recognition methods typically focus on overall performance. While this approach is common, it often fails to provide insights into performance across individual classes, which can lead to fairness issues and misrepresentation. Addressing these gaps is crucial for accurately assessing how well methods handle novel or unseen classes and ensuring a fair evaluation. To address fairness in Open-Set Recognition (OSR), we demonstrate that per-class performance can vary dramatically. We introduce Gaussian Hypothesis Open Set Technique (GHOST), a novel hyperparameter-free algorithm that models deep features using class-wise multivariate Gaussian distributions with diagonal covariance matrices. We apply Z-score normalization to logits to mitigate the impact of feature magnitudes that deviate from the model's expectations, thereby reducing the likelihood of the network assigning a high score to an unknown sample. We evaluate GHOST across multiple ImageNet-1K pre-trained deep networks and test it with four different unknown datasets. Using standard metrics such as AUOSCR, AUROC and FPR95, we achieve statistically significant improvements, advancing the state-of-the-art in large-scale OSR. Source code is provided online.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03359', 176)">Copy Link</button>
<div id="copy-message-176" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03365">A Match Made in Heaven? Matching Test Cases and Vulnerabilities With the VUTECO Approach</a></h1>
<p><b>Authors:</b> Emanuele Iannone, Quang-Cuong Bui, Riccardo Scandariato</p>
<p>Abstract: Software vulnerabilities are commonly detected via static analysis, penetration testing, and fuzzing. They can also be found by running unit tests - so-called vulnerability-witnessing tests - that stimulate the security-sensitive behavior with crafted inputs. Developing such tests is difficult and time-consuming; thus, automated data-driven approaches could help developers intercept vulnerabilities earlier. However, training and validating such approaches require a lot of data, which is currently scarce. This paper introduces VUTECO, a deep learning-based approach for collecting instances of vulnerability-witnessing tests from Java repositories. VUTECO carries out two tasks: (1) the "Finding" task to determine whether a test case is security-related, and (2) the "Matching" task to relate a test case to the exact vulnerability it is witnessing. VUTECO successfully addresses the Finding task, achieving perfect precision and 0.83 F0.5 score on validated test cases in VUL4J and returning 102 out of 145 (70%) correct security-related test cases from 244 open-source Java projects. Despite showing sufficiently good performance for the Matching task - i.e., 0.86 precision and 0.68 F0.5 score - VUTECO failed to retrieve any valid match in the wild. Nevertheless, we observed that in almost all of the matches, the test case was still security-related despite being matched to the wrong vulnerability. In the end, VUTECO can help find vulnerability-witnessing tests, though the matching with the right vulnerability is yet to be solved; the findings obtained lay the stepping stone for future research on the matter.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03365', 177)">Copy Link</button>
<div id="copy-message-177" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03370">Deep Learning-Based Approach for Identification of Potato Leaf Diseases Using Wrapper Feature Selection and Feature Concatenation</a></h1>
<p><b>Authors:</b> Muhammad Ahtsam Naeem, Muhammad Asim Saleem, Muhammad Imran Sharif, Shahzad Akber, Sajjad Saleem, Zahid Akhtar, Kamran Siddique</p>
<p>Abstract: The potato is a widely grown crop in many regions of the world. In recent decades, potato farming has gained incredible traction in the world. Potatoes are susceptible to several illnesses that stunt their development. This plant seems to have significant leaf disease. Early Blight and Late Blight are two prevalent leaf diseases that affect potato plants. The early detection of these diseases would be beneficial for enhancing the yield of this crop. The ideal solution is to use image processing to identify and analyze these disorders. Here, we present an autonomous method based on image processing and machine learning to detect late blight disease affecting potato leaves. The proposed method comprises four different phases: (1) Histogram Equalization is used to improve the quality of the input image; (2) feature extraction is performed using a Deep CNN model, then these extracted features are concatenated; (3) feature selection is performed using wrapper-based feature selection; (4) classification is performed using an SVM classifier and its variants. This proposed method achieves the highest accuracy of 99% using SVM by selecting 550 features.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03370', 178)">Copy Link</button>
<div id="copy-message-178" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03373">Demystifying Long Chain-of-Thought Reasoning in LLMs</a></h1>
<p><b>Authors:</b> Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, Xiang Yue</p>
<p>Abstract: Scaling inference compute enhances reasoning in large language models (LLMs), with long chains-of-thought (CoTs) enabling strategies like backtracking and error correction. Reinforcement learning (RL) has emerged as a crucial method for developing these capabilities, yet the conditions under which long CoTs emerge remain unclear, and RL training requires careful design choices. In this study, we systematically investigate the mechanics of long CoT reasoning, identifying the key factors that enable models to generate long CoT trajectories. Through extensive supervised fine-tuning (SFT) and RL experiments, we present four main findings: (1) While SFT is not strictly necessary, it simplifies training and improves efficiency; (2) Reasoning capabilities tend to emerge with increased training compute, but their development is not guaranteed, making reward shaping crucial for stabilizing CoT length growth; (3) Scaling verifiable reward signals is critical for RL. We find that leveraging noisy, web-extracted solutions with filtering mechanisms shows strong potential, particularly for out-of-distribution (OOD) tasks such as STEM reasoning; and (4) Core abilities like error correction are inherently present in base models, but incentivizing these skills effectively for complex tasks via RL demands significant compute, and measuring their emergence requires a nuanced approach. These insights provide practical guidance for optimizing training strategies to enhance long CoT reasoning in LLMs. Our code is available at: https://github.com/eddycmu/demystify-long-cot.</p>
<p>URLs: <a href="https://github.com/eddycmu/demystify-long-cot.">https://github.com/eddycmu/demystify-long-cot.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03373, https://github.com/eddycmu/demystify-long-cot.', 179)">Copy Link</button>
<div id="copy-message-179" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03377">Energy-Efficient Flying LoRa Gateways: A Multi-Agent Reinforcement Learning Approach</a></h1>
<p><b>Authors:</b> Abdullahi Isa Ahmed, El Mehdi Amhoud</p>
<p>Abstract: With the rapid development of next-generation Internet of Things (NG-IoT) networks, the increasing number of connected devices has led to a surge in power consumption. This rise in energy demand poses significant challenges to resource availability and raises sustainability concerns for large-scale IoT deployments. Efficient energy utilization in communication networks, particularly for power-constrained IoT devices, has thus become a critical area of research. In this paper, we deployed flying LoRa gateways (GWs) mounted on unmanned aerial vehicles (UAVs) to collect data from LoRa end devices (EDs) and transmit it to a central server. Our primary objective is to maximize the global system energy efficiency (EE) of wireless LoRa networks by joint optimization of transmission power (TP), spreading factor (SF), bandwidth (W), and ED association. To solve this challenging problem, we model the problem as a partially observable Markov decision process (POMDP), where each flying LoRa GW acts as a learning agent using a cooperative Multi-Agent Reinforcement Learning (MARL) approach under centralized training and decentralized execution (CTDE). Simulation results demonstrate that our proposed method, based on the multi-agent proximal policy optimization (MAPPO) algorithm, significantly improves the global system EE and surpasses the conventional MARL schemes.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03377', 180)">Copy Link</button>
<div id="copy-message-180" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03412">Deep Reinforcement Learning-Based Optimization of Second-Life Battery Utilization in Electric Vehicles Charging Stations</a></h1>
<p><b>Authors:</b> Rouzbeh Haghighi, Ali Hassan, Van-Hai Bui, Akhtar Hussain, Wencong Su</p>
<p>Abstract: The rapid rise in electric vehicle (EV) adoption presents significant challenges in managing the vast number of retired EV batteries. Research indicates that second-life batteries (SLBs) from EVs typically retain considerable residual capacity, offering extended utility. These batteries can be effectively repurposed for use in EV charging stations (EVCS), providing a cost-effective alternative to new batteries and reducing overall planning costs. Integrating battery energy storage systems (BESS) with SLBs into EVCS is a promising strategy to alleviate system overload. However, efficient operation of EVCS with integrated BESS is hindered by uncertainties such as fluctuating EV arrival and departure times and variable power prices from the grid. This paper presents a deep reinforcement learning-based (DRL) planning framework for EV charging stations with BESS, leveraging SLBs. We employ the advanced soft actor-critic (SAC) approach, training the model on a year's worth of data to account for seasonal variations, including weekdays and holidays. A tailored reward function enables effective offline training, allowing real-time optimization of EVCS operations under uncertainty.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03412', 181)">Copy Link</button>
<div id="copy-message-181" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03435">Taking a Big Step: Large Learning Rates in Denoising Score Matching Prevent Memorization</a></h1>
<p><b>Authors:</b> Yu-Han Wu, Pierre Marion, G\'erard Biau, Claire Boyer</p>
<p>Abstract: Denoising score matching plays a pivotal role in the performance of diffusion-based generative models. However, the empirical optimal score--the exact solution to the denoising score matching--leads to memorization, where generated samples replicate the training data. Yet, in practice, only a moderate degree of memorization is observed, even without explicit regularization. In this paper, we investigate this phenomenon by uncovering an implicit regularization mechanism driven by large learning rates. Specifically, we show that in the small-noise regime, the empirical optimal score exhibits high irregularity. We then prove that, when trained by stochastic gradient descent with a large enough learning rate, neural networks cannot stably converge to a local minimum with arbitrarily small excess risk. Consequently, the learned score cannot be arbitrarily close to the empirical optimal score, thereby mitigating memorization. To make the analysis tractable, we consider one-dimensional data and two-layer neural networks. Experiments validate the crucial role of the learning rate in preventing memorization, even beyond the one-dimensional setting.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03435', 182)">Copy Link</button>
<div id="copy-message-182" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03439">Linearized Optimal Transport pyLOT Library: A Toolkit for Machine Learning on Point Clouds</a></h1>
<p><b>Authors:</b> Jun Linwu, Varun Khurana, Nicholas Karris, Alexander Cloninger</p>
<p>Abstract: The pyLOT library offers a Python implementation of linearized optimal transport (LOT) techniques and methods to use in downstream tasks. The pipeline embeds probability distributions into a Hilbert space via the Optimal Transport maps from a fixed reference distribution, and this linearization allows downstream tasks to be completed using off the shelf (linear) machine learning algorithms. We provide a case study of performing ML on 3D scans of lemur teeth, where the original questions of classification, clustering, dimension reduction, and data generation reduce to simple linear operations performed on the LOT embedded representations.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03439', 183)">Copy Link</button>
<div id="copy-message-183" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2502.03444">Masked Autoencoders Are Effective Tokenizers for Diffusion Models</a></h1>
<p><b>Authors:</b> Hao Chen, Yujin Han, Fangyi Chen, Xiang Li, Yidong Wang, Jindong Wang, Ze Wang, Zicheng Liu, Difan Zou, Bhiksha Raj</p>
<p>Abstract: Recent advances in latent diffusion models have demonstrated their effectiveness for high-resolution image synthesis. However, the properties of the latent space from tokenizer for better learning and generation of diffusion models remain under-explored. Theoretically and empirically, we find that improved generation quality is closely tied to the latent distributions with better structure, such as the ones with fewer Gaussian Mixture modes and more discriminative features. Motivated by these insights, we propose MAETok, an autoencoder (AE) leveraging mask modeling to learn semantically rich latent space while maintaining reconstruction fidelity. Extensive experiments validate our analysis, demonstrating that the variational form of autoencoders is not necessary, and a discriminative latent space from AE alone enables state-of-the-art performance on ImageNet generation using only 128 tokens. MAETok achieves significant practical improvements, enabling a gFID of 1.69 with 76x faster training and 31x higher inference throughput for 512x512 generation. Our findings show that the structure of the latent space, rather than variational constraints, is crucial for effective diffusion models. Code and trained models are released.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.03444', 184)">Copy Link</button>
<div id="copy-message-184" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2010.08891">DeepAveragers: Offline Reinforcement Learning by Solving Derived Non-Parametric MDPs</a></h1>
<p><b>Authors:</b> Aayam Shrestha, Stefan Lee, Prasad Tadepalli, Alan Fern</p>
<p>Abstract: We study an approach to offline reinforcement learning (RL) based on optimally solving finitely-represented MDPs derived from a static dataset of experience. This approach can be applied on top of any learned representation and has the potential to easily support multiple solution objectives as well as zero-shot adjustment to changing environments and goals. Our main contribution is to introduce the Deep Averagers with Costs MDP (DAC-MDP) and to investigate its solutions for offline RL. DAC-MDPs are a non-parametric model that can leverage deep representations and account for limited data by introducing costs for exploiting under-represented parts of the model. In theory, we show conditions that allow for lower-bounding the performance of DAC-MDP solutions. We also investigate the empirical behavior in a number of environments, including those with image-based observations. Overall, the experiments demonstrate that the framework can work in practice and scale to large complex offline RL problems.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2010.08891', 185)">Copy Link</button>
<div id="copy-message-185" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2110.13103">Shift of Pairwise Similarities for Data Clustering</a></h1>
<p><b>Authors:</b> Morteza Haghir Chehreghani</p>
<p>Abstract: Several clustering methods (e.g., Normalized Cut and Ratio Cut) divide the Min Cut cost function by a cluster dependent factor (e.g., the size or the degree of the clusters), in order to yield a more balanced partitioning. We, instead, investigate adding such regularizations to the original cost function. We first consider the case where the regularization term is the sum of the squared size of the clusters, and then generalize it to adaptive regularization of the pairwise similarities. This leads to shifting (adaptively) the pairwise similarities which might make some of them negative. We then study the connection of this method to Correlation Clustering and then propose an efficient local search optimization algorithm with fast theoretical convergence rate to solve the new clustering problem. In the following, we investigate the shift of pairwise similarities on some common clustering methods, and finally, we demonstrate the superior performance of the method by extensive experiments on different datasets.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2110.13103', 186)">Copy Link</button>
<div id="copy-message-186" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2302.10184">AttNS: Attention-Inspired Numerical Solving For Limited Data Scenarios</a></h1>
<p><b>Authors:</b> Zhongzhan Huang, Mingfu Liang, Shanshan Zhong, Liang Lin</p>
<p>Abstract: We propose the attention-inspired numerical solver (AttNS), a concise method that helps the generalization and robustness issues faced by the AI-Hybrid numerical solver in solving differential equations due to limited data. AttNS is inspired by the effectiveness of attention modules in Residual Neural Networks (ResNet) in enhancing model generalization and robustness for conventional deep learning tasks. Drawing from the dynamical system perspective of ResNet, we seamlessly incorporate attention mechanisms into the design of numerical methods tailored for the characteristics of solving differential equations. Our results on benchmarks, ranging from high-dimensional problems to chaotic systems, showcases AttNS consistently enhancing various numerical solvers without any intricate model crafting. Finally, we analyze AttNS experimentally and theoretically, demonstrating its ability to achieve strong generalization and robustness while ensuring the convergence of the solver. This includes requiring less data compared to other advanced methods to achieve comparable generalization errors and better prevention of numerical explosion issues when solving differential equations.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2302.10184', 187)">Copy Link</button>
<div id="copy-message-187" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2310.15318">HetGPT: Harnessing the Power of Prompt Tuning in Pre-Trained Heterogeneous Graph Neural Networks</a></h1>
<p><b>Authors:</b> Yihong Ma, Ning Yan, Jiayu Li, Masood Mortazavi, Nitesh V. Chawla</p>
<p>Abstract: Graphs have emerged as a natural choice to represent and analyze the intricate patterns and rich information of the Web, enabling applications such as online page classification and social recommendation. The prevailing "pre-train, fine-tune" paradigm has been widely adopted in graph machine learning tasks, particularly in scenarios with limited labeled nodes. However, this approach often exhibits a misalignment between the training objectives of pretext tasks and those of downstream tasks. This gap can result in the "negative transfer" problem, wherein the knowledge gained from pre-training adversely affects performance in the downstream tasks. The surge in prompt-based learning within Natural Language Processing (NLP) suggests the potential of adapting a "pre-train, prompt" paradigm to graphs as an alternative. However, existing graph prompting techniques are tailored to homogeneous graphs, neglecting the inherent heterogeneity of Web graphs. To bridge this gap, we propose HetGPT, a general post-training prompting framework to improve the predictive performance of pre-trained heterogeneous graph neural networks (HGNNs). The key is the design of a novel prompting function that integrates a virtual class prompt and a heterogeneous feature prompt, with the aim to reformulate downstream tasks to mirror pretext tasks. Moreover, HetGPT introduces a multi-view neighborhood aggregation mechanism, capturing the complex neighborhood structure in heterogeneous graphs. Extensive experiments on three benchmark datasets demonstrate HetGPT's capability to enhance the performance of state-of-the-art HGNNs on semi-supervised node classification.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2310.15318', 188)">Copy Link</button>
<div id="copy-message-188" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2312.02400">DP-SGD-Global-Adapt-V2-S: Triad Improvements of Privacy, Accuracy and Fairness via Step Decay Noise Multiplier and Step Decay Upper Clipping Threshold</a></h1>
<p><b>Authors:</b> Sai Venkatesh Chilukoti, Md Imran Hossen, Liqun Shan, Vijay Srinivas Tida, Mahathir Mohammad Bappy, Wenmeng Tian, Xiai Hei</p>
<p>Abstract: Differentially Private Stochastic Gradient Descent (DP-SGD) has become a widely used technique for safeguarding sensitive information in deep learning applications. Unfortunately, DPSGD's per-sample gradient clipping and uniform noise addition during training can significantly degrade model utility and fairness. We observe that the latest DP-SGD-Global-Adapt's average gradient norm is the same throughout the training. Even when it is integrated with the existing linear decay noise multiplier, it has little or no advantage. Moreover, we notice that its upper clipping threshold increases exponentially towards the end of training, potentially impacting the models convergence. Other algorithms, DP-PSAC, Auto-S, DP-SGD-Global, and DP-F, have utility and fairness that are similar to or worse than DP-SGD, as demonstrated in experiments. To overcome these problems and improve utility and fairness, we developed the DP-SGD-Global-Adapt-V2-S. It has a step-decay noise multiplier and an upper clipping threshold that is also decayed step-wise. DP-SGD-Global-Adapt-V2-S with a privacy budget ($\epsilon$) of 1 improves accuracy by 0.9795\%, 0.6786\%, and 4.0130\% in MNIST, CIFAR10, and CIFAR100, respectively. It also reduces the privacy cost gap ($\pi$) by 89.8332% and 60.5541% in unbalanced MNIST and Thinwall datasets, respectively. Finally, we develop mathematical expressions to compute the privacy budget using truncated concentrated differential privacy (tCDP) for DP-SGD-Global-Adapt-V2-T and DP-SGD-Global-Adapt-V2-S.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2312.02400', 189)">Copy Link</button>
<div id="copy-message-189" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2312.15230">PERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMs</a></h1>
<p><b>Authors:</b> Max Zimmer, Megi Andoni, Christoph Spiegel, Sebastian Pokutta</p>
<p>Abstract: Neural Networks can be effectively compressed through pruning, significantly reducing storage and compute demands while maintaining predictive performance. Simple yet effective methods like magnitude pruning remove less important parameters and typically require a costly retraining procedure to restore performance. However, with the rise of LLMs, full retraining has become infeasible due to memory and compute constraints. This study challenges the practice of retraining all parameters by showing that updating a small subset of highly expressive parameters can suffice to recover or even enhance performance after pruning. Surprisingly, retraining just 0.01%-0.05% of the parameters in GPT-architectures can match the performance of full retraining across various sparsity levels, significantly reducing compute and memory requirements, and enabling retraining of models with up to 30 billion parameters on a single GPU in minutes. To bridge the gap to full retraining in the high sparsity regime, we introduce two novel LoRA variants that, unlike standard LoRA, allow merging adapters back without compromising sparsity. Going a step further, we show that these methods can be applied for memory-efficient layer-wise reconstruction, significantly enhancing state-of-the-art retraining-free methods like Wanda (Sun et al., 2023) and SparseGPT (Frantar & Alistarh, 2023). Our findings present a promising alternative to avoiding retraining.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2312.15230', 190)">Copy Link</button>
<div id="copy-message-190" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2402.01528">Decoding Speculative Decoding</a></h1>
<p><b>Authors:</b> Minghao Yan, Saurabh Agarwal, Shivaram Venkataraman</p>
<p>Abstract: Speculative Decoding is a widely used technique to speed up inference for Large Language Models (LLMs) without sacrificing quality. When performing inference, speculative decoding uses a smaller draft model to generate speculative tokens and then uses the target LLM to verify those draft tokens. The speedup provided by speculative decoding heavily depends on the choice of the draft model. In this work, we perform a detailed study comprising over 350 experiments with LLaMA-65B and OPT-66B using speculative decoding and delineate the factors that affect the performance gain provided by speculative decoding. Our experiments indicate that the performance of speculative decoding depends heavily on the latency of the draft model, and the draft model's capability in language modeling does not correlate strongly with its performance in speculative decoding. Based on these insights we explore a new design space for draft models and design hardware-efficient draft models for speculative decoding. Our newly designed draft model can provide 111% higher throughput than existing draft models and our approach generalizes further to all LLaMA models (1/2/3.1) and supervised fine-tuned models.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2402.01528', 191)">Copy Link</button>
<div id="copy-message-191" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2402.02168">Enhancing Cross-domain Link Prediction via Evolution Process Modeling</a></h1>
<p><b>Authors:</b> Xuanwen Huang, Wei Chow, Yize Zhu, Yang Wang, Ziwei Chai, Chunping Wang, Lei Chen, Yang Yang</p>
<p>Abstract: This work proposes DyExpert, a dynamic graph model for cross-domain link prediction. It can explicitly model historical evolving processes to learn the evolution pattern of a specific downstream graph and subsequently make pattern-specific link predictions. DyExpert adopts a decode-only transformer and is capable of efficiently parallel training and inference by \textit{conditioned link generation} that integrates both evolution modeling and link prediction. DyExpert is trained by extensive dynamic graphs across diverse domains, comprising 6M dynamic edges. Extensive experiments on eight untrained graphs demonstrate that DyExpert achieves state-of-the-art performance in cross-domain link prediction. Compared to the advanced baseline under the same setting, DyExpert achieves an average of 11.40% improvement Average Precision across eight graphs. More impressive, it surpasses the fully supervised performance of 8 advanced baselines on 6 untrained graphs.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2402.02168', 192)">Copy Link</button>
<div id="copy-message-192" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2402.04933">Context in Public Health for Underserved Communities: A Bayesian Approach to Online Restless Bandits</a></h1>
<p><b>Authors:</b> Biyonka Liang, Lily Xu, Aparna Taneja, Milind Tambe, Lucas Janson</p>
<p>Abstract: Public health programs often provide interventions to encourage program adherence, and effectively allocating interventions is vital for producing the greatest overall health outcomes, especially in underserved communities where resources are limited. Such resource allocation problems are often modeled as restless multi-armed bandits (RMABs) with unknown underlying transition dynamics, hence requiring online reinforcement learning (RL). We present Bayesian Learning for Contextual RMABs (BCoR), an online RL approach for RMABs that novelly combines techniques in Bayesian modeling with Thompson sampling to flexibly model the complex RMAB settings present in public health program adherence problems, namely context and non-stationarity. BCoR's key strength is the ability to leverage shared information within and between arms to learn the unknown RMAB transition dynamics quickly in intervention-scarce settings with relatively short time horizons, which is common in public health applications. Empirically, BCoR achieves substantially higher finite-sample performance over a range of experimental settings, including a setting using real-world adherence data that was developed in collaboration with ARMMAN, an NGO in India which runs a large-scale maternal mHealth program, showcasing BCoR practical utility and potential for real-world deployment.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2402.04933', 193)">Copy Link</button>
<div id="copy-message-193" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2402.05971">Are we making much progress? Revisiting chemical reaction yield prediction from an imbalanced regression perspective</a></h1>
<p><b>Authors:</b> Yihong Ma, Xiaobao Huang, Bozhao Nan, Nuno Moniz, Xiangliang Zhang, Olaf Wiest, Nitesh V. Chawla</p>
<p>Abstract: The yield of a chemical reaction quantifies the percentage of the target product formed in relation to the reactants consumed during the chemical reaction. Accurate yield prediction can guide chemists toward selecting high-yield reactions during synthesis planning, offering valuable insights before dedicating time and resources to wet lab experiments. While recent advancements in yield prediction have led to overall performance improvement across the entire yield range, an open challenge remains in enhancing predictions for high-yield reactions, which are of greater concern to chemists. In this paper, we argue that the performance gap in high-yield predictions results from the imbalanced distribution of real-world data skewed towards low-yield reactions, often due to unreacted starting materials and inherent ambiguities in the reaction processes. Despite this data imbalance, existing yield prediction methods continue to treat different yield ranges equally, assuming a balanced training distribution. Through extensive experiments on three real-world yield prediction datasets, we emphasize the urgent need to reframe reaction yield prediction as an imbalanced regression problem. Finally, we demonstrate that incorporating simple cost-sensitive re-weighting methods can significantly enhance the performance of yield prediction models on underrepresented high-yield regions.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2402.05971', 194)">Copy Link</button>
<div id="copy-message-194" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2402.14645">Sparse Linear Regression and Lattice Problems</a></h1>
<p><b>Authors:</b> Aparna Gupte, Neekon Vafa, Vinod Vaikuntanathan</p>
<p>Abstract: Sparse linear regression (SLR) is a well-studied problem in statistics where one is given a design matrix $X\in\mathbb{R}^{m\times n}$ and a response vector $y=X\theta^*+w$ for a $k$-sparse vector $\theta^*$ (that is, $\|\theta^*\|_0\leq k$) and small, arbitrary noise $w$, and the goal is to find a $k$-sparse $\widehat{\theta} \in \mathbb{R}^n$ that minimizes the mean squared prediction error $\frac{1}{m}\|X\widehat{\theta}-X\theta^*\|^2_2$. While $\ell_1$-relaxation methods such as basis pursuit, Lasso, and the Dantzig selector solve SLR when the design matrix is well-conditioned, no general algorithm is known, nor is there any formal evidence of hardness in an average-case setting with respect to all efficient algorithms.
  We give evidence of average-case hardness of SLR w.r.t. all efficient algorithms assuming the worst-case hardness of lattice problems. Specifically, we give an instance-by-instance reduction from a variant of the bounded distance decoding (BDD) problem on lattices to SLR, where the condition number of the lattice basis that defines the BDD instance is directly related to the restricted eigenvalue condition of the design matrix, which characterizes some of the classical statistical-computational gaps for sparse linear regression. Also, by appealing to worst-case to average-case reductions from the world of lattices, this shows hardness for a distribution of SLR instances; while the design matrices are ill-conditioned, the resulting SLR instances are in the identifiable regime.
  Furthermore, for well-conditioned (essentially) isotropic Gaussian design matrices, where Lasso is known to behave well in the identifiable regime, we show hardness of outputting any good solution in the unidentifiable regime where there are many solutions, assuming the worst-case hardness of standard and well-studied lattice problems.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2402.14645', 195)">Copy Link</button>
<div id="copy-message-195" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2402.16105">Towards Automated Knowledge Integration From Human-Interpretable Representations</a></h1>
<p><b>Authors:</b> Katarzyna Kobalczyk, Mihaela van der Schaar</p>
<p>Abstract: A significant challenge in machine learning, particularly in noisy and low-data environments, lies in effectively incorporating inductive biases to enhance data efficiency and robustness. Despite the success of informed machine learning methods, designing algorithms with explicit inductive biases remains largely a manual process. In this work, we explore how prior knowledge represented in its native formats, e.g. in natural language, can be integrated into machine learning models in an automated manner. Inspired by the learning to learn principles of meta-learning, we consider the approach of learning to integrate knowledge via conditional meta-learning, a paradigm we refer to as informed meta-learning. We introduce and motivate theoretically the principles of informed meta-learning enabling automated and controllable inductive bias selection. To illustrate our claims, we implement an instantiation of informed meta-learning--the Informed Neural Process, and empirically demonstrate the potential benefits and limitations of informed meta-learning in improving data efficiency and generalisation.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2402.16105', 196)">Copy Link</button>
<div id="copy-message-196" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2402.18213">Multi-objective Differentiable Neural Architecture Search</a></h1>
<p><b>Authors:</b> Rhea Sanjay Sukthanker, Arber Zela, Benedikt Staffler, Samuel Dooley, Josif Grabocka, Frank Hutter</p>
<p>Abstract: Pareto front profiling in multi-objective optimization (MOO), i.e., finding a diverse set of Pareto optimal solutions, is challenging, especially with expensive objectives that require training a neural network. Typically, in MOO for neural architecture search (NAS), we aim to balance performance and hardware metrics across devices. Prior NAS approaches simplify this task by incorporating hardware constraints into the objective function, but profiling the Pareto front necessitates a computationally expensive search for each constraint. In this work, we propose a novel NAS algorithm that encodes user preferences to trade-off performance and hardware metrics, yielding representative and diverse architectures across multiple devices in just a single search run. To this end, we parameterize the joint architectural distribution across devices and multiple objectives via a hypernetwork that can be conditioned on hardware features and preference vectors, enabling zero-shot transferability to new devices. Extensive experiments involving up to 19 hardware devices and 3 different objectives demonstrate the effectiveness and scalability of our method. Finally, we show that, without any additional costs, our method outperforms existing MOO NAS methods across a broad range of qualitatively different search spaces and datasets, including MobileNetV3 on ImageNet-1k, an encoder-decoder transformer space for machine translation and a decoder-only space for language modelling.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2402.18213', 197)">Copy Link</button>
<div id="copy-message-197" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2403.02233">A Theoretical Analysis of Self-Supervised Learning for Vision Transformers</a></h1>
<p><b>Authors:</b> Yu Huang, Zixin Wen, Yuejie Chi, Yingbin Liang</p>
<p>Abstract: Self-supervised learning has become a cornerstone in computer vision, primarily divided into reconstruction-based methods like masked autoencoders (MAE) and discriminative methods such as contrastive learning (CL). Recent empirical observations reveal that MAE and CL capture different types of representations: CL tends to focus on global patterns, while MAE adeptly captures both global and subtle local information simultaneously. Despite a flurry of recent empirical investigations to shed light on this difference, theoretical understanding remains limited, especially on the dominant architecture vision transformers (ViTs). In this paper, to provide rigorous insights, we model the visual data distribution by considering two types of spatial features: dominant global features and comparatively minuscule local features, and study the impact of imbalance among these features. We analyze the training dynamics of one-layer softmax-based ViTs on both MAE and CL objectives using gradient descent. Our analysis shows that as the degree of feature imbalance varies, ViTs trained with the MAE objective effectively learn both global and local features to achieve near-optimal reconstruction, while the CL-trained ViTs favor predominantly global features, even under mild imbalance. These results provide a theoretical explanation for distinct behaviors of MAE and CL observed in empirical studies.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2403.02233', 198)">Copy Link</button>
<div id="copy-message-198" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2403.02780">Data Collaboration Analysis with Orthonormal Basis Selection and Alignment</a></h1>
<p><b>Authors:</b> Keiyu Nosaka, Yuichi Takano, Akiko Yoshise</p>
<p>Abstract: Data Collaboration (DC) analysis offers a privacy-preserving approach to multi-source machine learning by enabling participants to train a shared model without revealing their raw data. Instead, each participant shares only linearly transformed data through a non-iterative communication protocol, thereby mitigating both privacy risks and communication overhead. The core idea of DC is that while each participant obfuscates their data with a secret linear transformation (or basis), the aggregator aligns these secret bases to a chosen target basis \textit{without knowing the secret bases}. Although DC theory suggests that any target basis spanning the same subspace as the secret bases should suffice, empirical evidence reveals that the choice of target basis can substantially influence model performance. To address this discrepancy, we propose \textbf{Orthonormal DC (ODC)}, a novel framework that enforces orthonormal constraints during the basis selection and alignment phases. Unlike conventional DC -- which allows arbitrary target bases -- ODC restricts the target to orthonormal bases, rendering the specific choice of basis negligible concerning model performance. Furthermore, the alignment step in ODC reduces to the \textbf{Orthogonal Procrustes Problem}, which admits a closed-form solution with favorable computational properties. Empirical evaluations demonstrate that ODC achieves higher accuracy and improved efficiency compared to existing DC methods, aligning with our theoretical findings. Additional evaluations assess performance in non-ideal scenarios with heterogenous distributions, also showing the best overall performance for our method. These findings position ODC as a direct and effective enhancement to current DC frameworks without compromising privacy or communication overhead when orthonormality constraints are applicable.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2403.02780', 199)">Copy Link</button>
<div id="copy-message-199" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2403.03726">Diffusion on language model encodings for protein sequence generation</a></h1>
<p><b>Authors:</b> Viacheslav Meshchaninov, Pavel Strashnov, Andrey Shevtsov, Fedor Nikolaev, Nikita Ivanisenko, Olga Kardymon, Dmitry Vetrov</p>
<p>Abstract: Protein sequence design has seen significant advances through discrete diffusion and autoregressive approaches, yet the potential of continuous diffusion remains underexplored. Here, we present DiMA, a latent diffusion framework that operates on protein language model representations. Through systematic exploration of architectural choices and diffusion components, we develop a robust methodology that generalizes across multiple protein encoders ranging from 8M to 3B parameters. We demonstrate that our framework achieves consistently high performance across sequence-only (ESM-2, ESMc), dual-decodable (CHEAP), and multimodal (SaProt) representations using the same architecture and training approach. We extensively evaluate existing methods alongside DiMA using multiple metrics across two protein modalities, covering quality, diversity, novelty, and distribution matching of generated proteins. DiMA consistently produces novel, high-quality and diverse protein sequences and achieves strong results compared to baselines such as autoregressive, discrete diffusion and flow matching language models. The model demonstrates versatile functionality, supporting conditional generation tasks including protein family-generation, motif scaffolding and infilling, and fold-specific sequence design. This work provides a universal continuous diffusion framework for protein sequence generation, offering both architectural insights and practical applicability across various protein design scenarios.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2403.03726', 200)">Copy Link</button>
<div id="copy-message-200" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2404.12488">Global Counterfactual Directions</a></h1>
<p><b>Authors:</b> Bartlomiej Sobieski, Przemys{\l}aw Biecek</p>
<p>Abstract: Despite increasing progress in development of methods for generating visual counterfactual explanations, especially with the recent rise of Denoising Diffusion Probabilistic Models, previous works consider them as an entirely local technique. In this work, we take the first step at globalizing them. Specifically, we discover that the latent space of Diffusion Autoencoders encodes the inference process of a given classifier in the form of global directions. We propose a novel proxy-based approach that discovers two types of these directions with the use of only single image in an entirely black-box manner. Precisely, g-directions allow for flipping the decision of a given classifier on an entire dataset of images, while h-directions further increase the diversity of explanations. We refer to them in general as Global Counterfactual Directions (GCDs). Moreover, we show that GCDs can be naturally combined with Latent Integrated Gradients resulting in a new black-box attribution method, while simultaneously enhancing the understanding of counterfactual explanations. We validate our approach on existing benchmarks and show that it generalizes to real-world use-cases.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2404.12488', 201)">Copy Link</button>
<div id="copy-message-201" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2405.15231">Cardinality Estimation on Hyper-relational Knowledge Graphs</a></h1>
<p><b>Authors:</b> Fei Teng, Haoyang Li, Shimin Di, Lei Chen</p>
<p>Abstract: Cardinality Estimation (CE) for query is to estimate the number of results without execution, which is an effective index in query optimization. Recently, CE for queries over knowlege graph (KGs) with triple facts has achieved great success. To more precisely represent facts, current researchers propose hyper-relational KGs (HKGs) to represent a triple fact with qualifiers providing additional context to the fact. However, existing CE methods, such as sampling and summary methods over KGs, perform unsatisfactorily on HKGs due to the complexity of qualifiers. Learning-based CE methods do not utilize qualifier information to learn query representation accurately, leading to poor performance. Also, there is only one limited CE benchmark for HKG query, which is not comprehensive and only covers limited patterns. The lack of querysets over HKG also becomes a bottleneck to comprehensively investigate CE problems on HKGs. In this work, we first construct diverse and unbiased hyper-relational querysets over three popular HKGs for investigating CE. Besides, we also propose a novel qualifier-aware graph neural network (GNN) model that effectively incorporates qualifier information and adaptively combines outputs from multiple GNN layers, to accurately predict the cardinality. Our experiments demonstrate that our model outperforms all state-of-the-art CE methods over three benchmarks on popular HKGs.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2405.15231', 202)">Copy Link</button>
<div id="copy-message-202" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2405.15885">Diffusion Bridge Implicit Models</a></h1>
<p><b>Authors:</b> Kaiwen Zheng, Guande He, Jianfei Chen, Fan Bao, Jun Zhu</p>
<p>Abstract: Denoising diffusion bridge models (DDBMs) are a powerful variant of diffusion models for interpolating between two arbitrary paired distributions given as endpoints. Despite their promising performance in tasks like image translation, DDBMs require a computationally intensive sampling process that involves the simulation of a (stochastic) differential equation through hundreds of network evaluations. In this work, we take the first step in fast sampling of DDBMs without extra training, motivated by the well-established recipes in diffusion models. We generalize DDBMs via a class of non-Markovian diffusion bridges defined on the discretized timesteps concerning sampling, which share the same marginal distributions and training objectives, give rise to generative processes ranging from stochastic to deterministic, and result in diffusion bridge implicit models (DBIMs). DBIMs are not only up to 25$\times$ faster than the vanilla sampler of DDBMs but also induce a novel, simple, and insightful form of ordinary differential equation (ODE) which inspires high-order numerical solvers. Moreover, DBIMs maintain the generation diversity in a distinguished way, by using a booting noise in the initial sampling step, which enables faithful encoding, reconstruction, and semantic interpolation in image translation tasks. Code is available at https://github.com/thu-ml/DiffusionBridge.</p>
<p>URLs: <a href="https://github.com/thu-ml/DiffusionBridge.">https://github.com/thu-ml/DiffusionBridge.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2405.15885, https://github.com/thu-ml/DiffusionBridge.', 203)">Copy Link</button>
<div id="copy-message-203" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2405.19466">Active Exploration via Autoregressive Generation of Missing Data</a></h1>
<p><b>Authors:</b> Tiffany Tianhui Cai, Hongseok Namkoong, Daniel Russo, Kelly W Zhang</p>
<p>Abstract: We pose uncertainty quantification and exploration in online decision-making as a problem of training and generation from an autoregressive sequence model, an area experiencing rapid innovation. Our approach rests on viewing uncertainty as arising from missing future outcomes that would be revealed through appropriate action choices, rather than from unobservable latent parameters of the environment. This reformulation aligns naturally with modern machine learning capabilities: we can i) train generative models through next-outcome prediction rather than fit explicit priors, ii) assess uncertainty through autoregressive generation rather than parameter sampling, and iii) adapt to new information through in-context learning rather than explicit posterior updating. To showcase these ideas, we formulate a challenging meta-bandit problem where effective performance requires leveraging unstructured prior information (like text features) while exploring judiciously to resolve key remaining uncertainties. We validate our approach through both theory and experiments. Our theory establishes a reduction, showing success at offline next-outcome prediction translates to reliable online uncertainty quantification and decision-making, even with strategically collected data. Semi-synthetic experiments show our insights bear out in a news-article recommendation task, where article text can be leveraged to minimize exploration.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2405.19466', 204)">Copy Link</button>
<div id="copy-message-204" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2406.06494">Scaling Continuous Latent Variable Models as Probabilistic Integral Circuits</a></h1>
<p><b>Authors:</b> Gennaro Gala, Cassio de Campos, Antonio Vergari, Erik Quaeghebeur</p>
<p>Abstract: Probabilistic integral circuits (PICs) have been recently introduced as probabilistic models enjoying the key ingredient behind expressive generative models: continuous latent variables (LVs). PICs are symbolic computational graphs defining continuous LV models as hierarchies of functions that are summed and multiplied together, or integrated over some LVs. They are tractable if LVs can be analytically integrated out, otherwise they can be approximated by tractable probabilistic circuits (PC) encoding a hierarchical numerical quadrature process, called QPCs.
  So far, only tree-shaped PICs have been explored, and training them via numerical quadrature requires memory-intensive processing at scale. In this paper, we address these issues, and present: (i) a pipeline for building DAG-shaped PICs out of arbitrary variable decompositions, (ii) a procedure for training PICs using tensorized circuit architectures, and (iii) neural functional sharing techniques to allow scalable training. In extensive experiments, we showcase the effectiveness of functional sharing and the superiority of QPCs over traditional PCs.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2406.06494', 205)">Copy Link</button>
<div id="copy-message-205" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2406.09570">Improving Consistency Models with Generator-Augmented Flows</a></h1>
<p><b>Authors:</b> Thibaut Issenhuth, Sangchul Lee, Ludovic Dos Santos, Jean-Yves Franceschi, Chansoo Kim, Alain Rakotomamonjy</p>
<p>Abstract: Consistency models imitate the multi-step sampling of score-based diffusion in a single forward pass of a neural network. They can be learned in two ways: consistency distillation and consistency training. The former relies on the true velocity field of the corresponding differential equation, approximated by a pre-trained neural network. In contrast, the latter uses a single-sample Monte Carlo estimate of this velocity field. The related estimation error induces a discrepancy between consistency distillation and training that, we show, still holds in the continuous-time limit. To alleviate this issue, we propose a novel flow that transports noisy data towards their corresponding outputs derived from a consistency model. We prove that this flow reduces the previously identified discrepancy and the noise-data transport cost. Consequently, our method not only accelerates consistency training convergence but also enhances its overall performance. The code is available at: https://github.com/thibautissenhuth/consistency_GC.</p>
<p>URLs: <a href="https://github.com/thibautissenhuth/consistency_GC.">https://github.com/thibautissenhuth/consistency_GC.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2406.09570, https://github.com/thibautissenhuth/consistency_GC.', 206)">Copy Link</button>
<div id="copy-message-206" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2406.12814">Dissecting Adversarial Robustness of Multimodal LM Agents</a></h1>
<p><b>Authors:</b> Chen Henry Wu, Rishi Shah, Jing Yu Koh, Ruslan Salakhutdinov, Daniel Fried, Aditi Raghunathan</p>
<p>Abstract: As language models (LMs) are used to build autonomous agents in real environments, ensuring their adversarial robustness becomes a critical challenge. Unlike chatbots, agents are compound systems with multiple components taking actions, which existing LMs safety evaluations do not adequately address. To bridge this gap, we manually create 200 targeted adversarial tasks and evaluation scripts in a realistic threat model on top of VisualWebArena, a real environment for web agents. To systematically examine the robustness of agents, we propose the Agent Robustness Evaluation (ARE) framework. ARE views the agent as a graph showing the flow of intermediate outputs between components and decomposes robustness as the flow of adversarial information on the graph. We find that we can successfully break latest agents that use black-box frontier LMs, including those that perform reflection and tree search. With imperceptible perturbations to a single image (less than 5% of total web page pixels), an attacker can hijack these agents to execute targeted adversarial goals with success rates up to 67%. We also use ARE to rigorously evaluate how the robustness changes as new components are added. We find that inference-time compute that typically improves benign performance can open up new vulnerabilities and harm robustness. An attacker can compromise the evaluator used by the reflexion agent and the value function of the tree search agent, which increases the attack success relatively by 15% and 20%. Our data and code for attacks, defenses, and evaluation are at https://github.com/ChenWu98/agent-attack</p>
<p>URLs: <a href="https://github.com/ChenWu98/agent-attack">https://github.com/ChenWu98/agent-attack</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2406.12814, https://github.com/ChenWu98/agent-attack', 207)">Copy Link</button>
<div id="copy-message-207" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2406.14324">Revealing the Learning Process in Reinforcement Learning Agents Through Attention-Oriented Metrics</a></h1>
<p><b>Authors:</b> Charlotte Beylier, Simon M. Hofmann, Nico Scherf</p>
<p>Abstract: The learning process of a reinforcement learning (RL) agent remains poorly understood beyond the mathematical formulation of its learning algorithm. To address this gap, we introduce attention-oriented metrics (ATOMs) to investigate the development of an RL agent's attention during training. In a controlled experiment, we tested ATOMs on three variations of a Pong game, each designed to teach the agent distinct behaviours, complemented by a behavioural assessment. ATOMs successfully delineate the attention patterns of an agent trained on each game variation, and that these differences in attention patterns translate into differences in the agent's behaviour. Through continuous monitoring of ATOMs during training, we observed that the agent's attention developed in phases, and that these phases were consistent across game variations. Overall, we believe that ATOM could help improve our understanding of the learning processes of RL agents and better understand the relationship between attention and learning.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2406.14324', 208)">Copy Link</button>
<div id="copy-message-208" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2406.16061">PORT: Preference Optimization on Reasoning Traces</a></h1>
<p><b>Authors:</b> Salem Lahlou, Abdalgader Abubaker, Hakim Hacid</p>
<p>Abstract: Preference optimization methods have been successfully applied to improve not only the alignment of large language models (LLMs) with human values, but also specific natural language tasks such as summarization and stylistic continuations. This paper proposes using preference optimization methods on Chain-of-Thought steps in order to improve the mathematical reasoning performances of language models. While the chosen answers are obtained from datasets that include reasoning traces, we propose two complementary schemes for generating rejected answers: weak LLM prompting, and digit corruption. Our approach leads to increased accuracy on the GSM8K and AQuA-RAT mathematical reasoning benchmarks for Falcon2-11B and Mistral-7B. Additionally, the improved abilities transfer to non-mathematical tasks, including the ARC benchmark and symbolic reasoning challenges. For example, our method can lead to up to relative 8.47% and 18.73% increases in accuracy on the GSM8K and AQuA benchmarks respectively, without any extra annotations. This work suggests that the path towards better language reasoning abilities goes through spending resources on creating high-quality datasets of reasoning traces.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2406.16061', 209)">Copy Link</button>
<div id="copy-message-209" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2407.02700">A simple algorithm for output range analysis for deep neural networks</a></h1>
<p><b>Authors:</b> Helder Rojas, Nilton Rojas, Espinoza J. B., Luis Huamanchumo</p>
<p>Abstract: This paper presents a novel approach for the output range estimation problem in Deep Neural Networks (DNNs) by integrating a Simulated Annealing (SA) algorithm tailored to operate within constrained domains and ensure convergence towards global optima. The method effectively addresses the challenges posed by the lack of local geometric information and the high non-linearity inherent to DNNs, making it applicable to a wide variety of architectures, with a special focus on Residual Networks (ResNets) due to their practical importance. Unlike existing methods, our algorithm imposes minimal assumptions on the internal architecture of neural networks, thereby extending its usability to complex models. Theoretical analysis guarantees convergence, while extensive empirical evaluations-including optimization tests involving functions with multiple local minima-demonstrate the robustness of our algorithm in navigating non-convex response surfaces. The experimental results highlight the algorithm's efficiency in accurately estimating DNN output ranges, even in scenarios characterized by high non-linearity and complex constraints. For reproducibility, Python codes and datasets used in the experiments are publicly available through our GitHub repository.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2407.02700', 210)">Copy Link</button>
<div id="copy-message-210" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2407.14206">Watermark Smoothing Attacks against Language Models</a></h1>
<p><b>Authors:</b> Hongyan Chang, Hamed Hassani, Reza Shokri</p>
<p>Abstract: Watermarking is a key technique for detecting AI-generated text. In this work, we study its vulnerabilities and introduce the Smoothing Attack, a novel watermark removal method. By leveraging the relationship between the model's confidence and watermark detectability, our attack selectively smoothes the watermarked content, erasing watermark traces while preserving text quality. We validate our attack on open-source models ranging from $1.3$B to $30$B parameters on $10$ different watermarks, demonstrating its effectiveness. Our findings expose critical weaknesses in existing watermarking schemes and highlight the need for stronger defenses.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2407.14206', 211)">Copy Link</button>
<div id="copy-message-211" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2407.17754">DualFed: Enjoying both Generalization and Personalization in Federated Learning via Hierachical Representations</a></h1>
<p><b>Authors:</b> Guogang Zhu, Xuefeng Liu, Jianwei Niu, Shaojie Tang, Xinghao Wu, Jiayuan Zhang</p>
<p>Abstract: In personalized federated learning (PFL), it is widely recognized that achieving both high model generalization and effective personalization poses a significant challenge due to their conflicting nature. As a result, existing PFL methods can only manage a trade-off between these two objectives. This raises an interesting question: Is it feasible to develop a model capable of achieving both objectives simultaneously? Our paper presents an affirmative answer, and the key lies in the observation that deep models inherently exhibit hierarchical architectures, which produce representations with various levels of generalization and personalization at different stages. A straightforward approach stemming from this observation is to select multiple representations from these layers and combine them to concurrently achieve generalization and personalization. However, the number of candidate representations is commonly huge, which makes this method infeasible due to high computational costs.To address this problem, we propose DualFed, a new method that can directly yield dual representations correspond to generalization and personalization respectively, thereby simplifying the optimization task. Specifically, DualFed inserts a personalized projection network between the encoder and classifier. The pre-projection representations are able to capture generalized information shareable across clients, and the post-projection representations are effective to capture task-specific information on local clients. This design minimizes the mutual interference between generalization and personalization, thereby achieving a win-win situation. Extensive experiments show that DualFed can outperform other FL methods. Code is available at https://github.com/GuogangZhu/DualFed.</p>
<p>URLs: <a href="https://github.com/GuogangZhu/DualFed.">https://github.com/GuogangZhu/DualFed.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2407.17754, https://github.com/GuogangZhu/DualFed.', 212)">Copy Link</button>
<div id="copy-message-212" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2408.08761">Mitigating Information Loss in Tree-Based Reinforcement Learning via Direct Optimization</a></h1>
<p><b>Authors:</b> Sascha Marton, Tim Grams, Florian Vogt, Stefan L\"udtke, Christian Bartelt, Heiner Stuckenschmidt</p>
<p>Abstract: Reinforcement learning (RL) has seen significant success across various domains, but its adoption is often limited by the black-box nature of neural network policies, making them difficult to interpret. In contrast, symbolic policies allow representing decision-making strategies in a compact and interpretable way. However, learning symbolic policies directly within on-policy methods remains challenging. In this paper, we introduce SYMPOL, a novel method for SYMbolic tree-based on-POLicy RL. SYMPOL employs a tree-based model integrated with a policy gradient method, enabling the agent to learn and adapt its actions while maintaining a high level of interpretability. We evaluate SYMPOL on a set of benchmark RL tasks, demonstrating its superiority over alternative tree-based RL approaches in terms of performance and interpretability. Unlike existing methods, it enables gradient-based, end-to-end learning of interpretable, axis-aligned decision trees within standard on-policy RL algorithms. Therefore, SYMPOL can become the foundation for a new class of interpretable RL based on decision trees. Our implementation is available under: https://github.com/s-marton/sympol</p>
<p>URLs: <a href="https://github.com/s-marton/sympol">https://github.com/s-marton/sympol</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2408.08761, https://github.com/s-marton/sympol', 213)">Copy Link</button>
<div id="copy-message-213" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2408.12830">SAMBO-RL: Shifts-aware Model-based Offline Reinforcement Learning</a></h1>
<p><b>Authors:</b> Wang Luo, Haoran Li, Zicheng Zhang, Congying Han, Jiayu Lv, Tiande Guo</p>
<p>Abstract: Model-based offline reinforcement learning trains policies using pre-collected datasets and learned environment models, eliminating the need for direct real-world environment interaction. However, this paradigm is inherently challenged by distribution shift (DS). Existing methods address this issue by leveraging off-policy mechanisms and estimating model uncertainty, but they often result in inconsistent objectives and lack a unified theoretical foundation. This paper offers a comprehensive analysis that disentangles the problem into two fundamental components: model bias and policy shift. Our theoretical and empirical investigations reveal how these factors distort value estimation and restrict policy optimization. To tackle these challenges, we derive a novel Shifts-aware Reward (SAR) through a unified probabilistic inference framework, which modifies the vanilla reward to refine value learning and facilitate policy training. Building on this, we introduce Shifts-aware Model-based Offline Reinforcement Learning (SAMBO-RL), a practical framework that efficiently trains classifiers to approximate SAR for policy optimization. Empirical experiments show that SAR effectively mitigates DS, and SAMBO-RL achieves superior or comparable performance across various benchmarks, underscoring its effectiveness and validating our theoretical analysis.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2408.12830', 214)">Copy Link</button>
<div id="copy-message-214" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2409.03735">Investigating Privacy Bias in Training Data of Language Models</a></h1>
<p><b>Authors:</b> Yan Shvartzshnaider, Vasisht Duddu</p>
<p>Abstract: As LLMs are integrated into sociotechnical systems, it is crucial to examine the privacy biases they exhibit. A privacy bias refers to the skew in the appropriateness of information flows within a given context that LLMs acquire from large amounts of non-publicly available training data. This skew may either align with existing expectations or signal a symptom of systemic issues reflected in the training datasets.
  We formulate a novel research question: how can we examine privacy biases in the training data of LLMs? We present a novel approach to assess the privacy biases using a contextual integrity-based methodology to evaluate the responses from different LLMs. Our approach accounts for the sensitivity of responses across prompt variations, which hinders the evaluation of privacy biases. We investigate how privacy biases are affected by model capacities and optimizations.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2409.03735', 215)">Copy Link</button>
<div id="copy-message-215" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2409.03845">Latent Space Energy-based Neural ODEs</a></h1>
<p><b>Authors:</b> Sheng Cheng, Deqian Kong, Jianwen Xie, Kookjin Lee, Ying Nian Wu, Yezhou Yang</p>
<p>Abstract: This paper introduces novel deep dynamical models designed to represent continuous-time sequences. Our approach employs a neural emission model to generate each data point in the time series through a non-linear transformation of a latent state vector. The evolution of these latent states is implicitly defined by a neural ordinary differential equation (ODE), with the initial state drawn from an informative prior distribution parameterized by an Energy-based model (EBM). This framework is extended to disentangle dynamic states from underlying static factors of variation, represented as time-invariant variables in the latent space. We train the model using maximum likelihood estimation with Markov chain Monte Carlo (MCMC) in an end-to-end manner. Experimental results on oscillating systems, videos and real-world state sequences (MuJoCo) demonstrate that our model with the learnable energy-based prior outperforms existing counterparts, and can generalize to new dynamic parameterization, enabling long-horizon predictions.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2409.03845', 216)">Copy Link</button>
<div id="copy-message-216" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2409.16040">Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts</a></h1>
<p><b>Authors:</b> Xiaoming Shi, Shiyu Wang, Yuqi Nie, Dianqi Li, Zhou Ye, Qingsong Wen, Ming Jin</p>
<p>Abstract: Deep learning for time series forecasting has seen significant advancements over the past decades. However, despite the success of large-scale pre-training in language and vision domains, pre-trained time series models remain limited in scale and operate at a high cost, hindering the development of larger capable forecasting models in real-world applications. In response, we introduce Time-MoE, a scalable and unified architecture designed to pre-train larger, more capable forecasting foundation models while reducing inference costs. By leveraging a sparse mixture-of-experts (MoE) design, Time-MoE enhances computational efficiency by activating only a subset of networks for each prediction, reducing computational load while maintaining high model capacity. This allows Time-MoE to scale effectively without a corresponding increase in inference costs. Time-MoE comprises a family of decoder-only transformer models that operate in an auto-regressive manner and support flexible forecasting horizons with varying input context lengths. We pre-trained these models on our newly introduced large-scale data Time-300B, which spans over 9 domains and encompassing over 300 billion time points. For the first time, we scaled a time series foundation model up to 2.4 billion parameters, achieving significantly improved forecasting precision. Our results validate the applicability of scaling laws for training tokens and model size in the context of time series forecasting. Compared to dense models with the same number of activated parameters or equivalent computation budgets, our models consistently outperform them by large margin. These advancements position Time-MoE as a state-of-the-art solution for tackling real-world time series forecasting challenges with superior capability, efficiency, and flexibility.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2409.16040', 217)">Copy Link</button>
<div id="copy-message-217" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2410.01153">Text2PDE: Latent Diffusion Models for Accessible Physics Simulation</a></h1>
<p><b>Authors:</b> Anthony Zhou, Zijie Li, Michael Schneier, John R Buchanan Jr, Amir Barati Farimani</p>
<p>Abstract: Recent advances in deep learning have inspired numerous works on data-driven solutions to partial differential equation (PDE) problems. These neural PDE solvers can often be much faster than their numerical counterparts; however, each presents its unique limitations and generally balances training cost, numerical accuracy, and ease of applicability to different problem setups. To address these limitations, we introduce several methods to apply latent diffusion models to physics simulation. Firstly, we introduce a mesh autoencoder to compress arbitrarily discretized PDE data, allowing for efficient diffusion training across various physics. Furthermore, we investigate full spatio-temporal solution generation to mitigate autoregressive error accumulation. Lastly, we investigate conditioning on initial physical quantities, as well as conditioning solely on a text prompt to introduce text2PDE generation. We show that language can be a compact, interpretable, and accurate modality for generating physics simulations, paving the way for more usable and accessible PDE solvers. Through experiments on both uniform and structured grids, we show that the proposed approach is competitive with current neural PDE solvers in both accuracy and efficiency, with promising scaling behavior up to $\sim$3 billion parameters. By introducing a scalable, accurate, and usable physics simulator, we hope to bring neural PDE solvers closer to practical use.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2410.01153', 218)">Copy Link</button>
<div id="copy-message-218" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2410.05985">Asynchronous Stochastic Gradient Descent with Decoupled Backpropagation and Layer-Wise Updates</a></h1>
<p><b>Authors:</b> Cabrel Teguemne Fokam, Khaleelulla Khan Nazeer, Lukas K\"onig, David Kappel, Anand Subramoney</p>
<p>Abstract: The increasing size of deep learning models has made distributed training across multiple devices essential. However, current methods such as distributed data-parallel training suffer from large communication and synchronization overheads when training across devices, leading to longer training times as a result of suboptimal hardware utilization. Asynchronous stochastic gradient descent (ASGD) methods can improve training speed, but are sensitive to delays due to both communication and differences throughput. Moreover, the backpropagation algorithm used within ASGD workers is bottlenecked by the interlocking between its forward and backward passes. Current methods also do not take advantage of the large differences in the computation required for the forward and backward passes. Therefore, we propose an extension to ASGD called Partial Decoupled ASGD (PD-ASGD) that addresses these issues. PD-ASGD uses separate threads for the forward and backward passes, decoupling the updates and allowing for a higher ratio of forward to backward threads than the usual 1:1 ratio, leading to higher throughput. PD-ASGD also performs layer-wise (partial) model updates concurrently across multiple threads. This reduces parameter staleness and consequently improves robustness to delays. Our approach yields close to state-of-the-art results while running up to $5.95\times$ faster than synchronous data parallelism in the presence of delays, and up to $2.14\times$ times faster than comparable ASGD algorithms by achieving higher model flops utilization. We mathematically describe the gradient bias introduced by our method, establish an upper bound, and prove convergence.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2410.05985', 219)">Copy Link</button>
<div id="copy-message-219" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2410.08003">More Experts Than Galaxies: Conditionally-overlapping Experts With Biologically-Inspired Fixed Routing</a></h1>
<p><b>Authors:</b> Sagi Shaier, Francisco Pereira, Katharina von der Wense, Lawrence E Hunter, Matt Jones</p>
<p>Abstract: The evolution of biological neural systems has led to both modularity and sparse coding, which enables energy efficiency and robustness across the diversity of tasks in the lifespan. In contrast, standard neural networks rely on dense, non-specialized architectures, where all model parameters are simultaneously updated to learn multiple tasks, leading to interference. Current sparse neural network approaches aim to alleviate this issue but are hindered by limitations such as 1) trainable gating functions that cause representation collapse, 2) disjoint experts that result in redundant computation and slow learning, and 3) reliance on explicit input or task IDs that limit flexibility and scalability. In this paper we propose Conditionally Overlapping Mixture of ExperTs (COMET), a general deep learning method that addresses these challenges by inducing a modular, sparse architecture with an exponential number of overlapping experts. COMET replaces the trainable gating function used in Sparse Mixture of Experts with a fixed, biologically inspired random projection applied to individual input representations. This design causes the degree of expert overlap to depend on input similarity, so that similar inputs tend to share more parameters. This results in faster learning per update step and improved out-of-sample generalization. We demonstrate the effectiveness of COMET on a range of tasks, including image classification, language modeling, and regression, using several popular deep learning architectures.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2410.08003', 220)">Copy Link</button>
<div id="copy-message-220" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2410.08854">Hybrid LLM-DDQN based Joint Optimization of V2I Communication and Autonomous Driving</a></h1>
<p><b>Authors:</b> Zijiang Yan, Hao Zhou, Hina Tabassum, Xue Liu</p>
<p>Abstract: Large language models (LLMs) have received considerable interest recently due to their outstanding reasoning and comprehension capabilities. This work explores applying LLMs to vehicular networks, aiming to jointly optimize vehicle-to-infrastructure (V2I) communications and autonomous driving (AD) policies. We deploy LLMs for AD decision-making to maximize traffic flow and avoid collisions for road safety, and a double deep Q-learning algorithm (DDQN) is used for V2I optimization to maximize the received data rate and reduce frequent handovers. In particular, for LLM-enabled AD, we employ the Euclidean distance to identify previously explored AD experiences, and then LLMs can learn from past good and bad decisions for further improvement. Then, LLM-based AD decisions will become part of states in V2I problems, and DDQN will optimize the V2I decisions accordingly. After that, the AD and V2I decisions are iteratively optimized until convergence. Such an iterative optimization approach can better explore the interactions between LLMs and conventional reinforcement learning techniques, revealing the potential of using LLMs for network optimization and management. Finally, the simulations demonstrate that our proposed hybrid LLM-DDQN approach outperforms the conventional DDQN algorithm, showing faster convergence and higher average rewards.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2410.08854', 221)">Copy Link</button>
<div id="copy-message-221" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2410.11767">Analyzing (In)Abilities of SAEs via Formal Languages</a></h1>
<p><b>Authors:</b> Abhinav Menon, Manish Shrivastava, David Krueger, Ekdeep Singh Lubana</p>
<p>Abstract: Autoencoders have been used for finding interpretable and disentangled features underlying neural network representations in both image and text domains. While the efficacy and pitfalls of such methods are well-studied in vision, there is a lack of corresponding results, both qualitative and quantitative, for the text domain. We aim to address this gap by training sparse autoencoders (SAEs) on a synthetic testbed of formal languages. Specifically, we train SAEs on the hidden representations of models trained on formal languages (Dyck-2, Expr, and English PCFG) under a wide variety of hyperparameter settings, finding interpretable latents often emerge in the features learned by our SAEs. However, similar to vision, we find performance turns out to be highly sensitive to inductive biases of the training pipeline. Moreover, we show latents correlating to certain features of the input do not always induce a causal impact on model's computation. We thus argue that causality has to become a central target in SAE training: learning of causal features should be incentivized from the ground-up. Motivated by this, we propose and perform preliminary investigations for an approach that promotes learning of causally relevant features in our formal language setting.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2410.11767', 222)">Copy Link</button>
<div id="copy-message-222" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2410.16534">SoftSRV: Learn to Generate Targeted Synthetic Data</a></h1>
<p><b>Authors:</b> Giulia DeSalvo, Jean-Fracois Kagy, Lazaros Karydas, Afshin Rostamizadeh, Sanjiv Kumar</p>
<p>Abstract: We present a novel framework, SoftSRV, that is used to generate targeted synthetic fine-tuning data for improving task-specific model performance. Given a sample from a target distribution, our proposed framework uses a data-driven loss minimization approach to steer a frozen large language model (LLM) to generate synthetic sequences that are similar to those from the target distribution. SoftSRV provides a practical improvement over common prompt engineering approaches that rely on human-engineered prompt-templates, which can be idiosyncratic, labor-intensive to craft, and may need to be specialized per domain. We empirically evaluate our method against standard baselines guiding a large LLM to generate synthetic data to fine-tune a smaller language model on three different domains (coding, math, reasoning). We perform these evaluations without any particular specialization of the framework to each domain, emphasizing the generality of our approach. We find that SoftSRV improves upon typical prompt engineering approaches, generating targeted data that leads to fine-tuned models with significantly better task-specific performance. In addition, SoftSRV-generated data better matches the target distribution according to the MAUVE similarity metric.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2410.16534', 223)">Copy Link</button>
<div id="copy-message-223" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2410.18959">Context is Key: A Benchmark for Forecasting with Essential Textual Information</a></h1>
<p><b>Authors:</b> Andrew Robert Williams, Arjun Ashok, \'Etienne Marcotte, Valentina Zantedeschi, Jithendaraa Subramanian, Roland Riachi, James Requeima, Alexandre Lacoste, Irina Rish, Nicolas Chapados, Alexandre Drouin</p>
<p>Abstract: Forecasting is a critical task in decision-making across numerous domains. While historical numerical data provide a start, they fail to convey the complete context for reliable and accurate predictions. Human forecasters frequently rely on additional information, such as background knowledge and constraints, which can efficiently be communicated through natural language. However, in spite of recent progress with LLM-based forecasters, their ability to effectively integrate this textual information remains an open question. To address this, we introduce "Context is Key" (CiK), a time-series forecasting benchmark that pairs numerical data with diverse types of carefully crafted textual context, requiring models to integrate both modalities; crucially, every task in CiK requires understanding textual context to be solved successfully. We evaluate a range of approaches, including statistical models, time series foundation models, and LLM-based forecasters, and propose a simple yet effective LLM prompting method that outperforms all other tested methods on our benchmark. Our experiments highlight the importance of incorporating contextual information, demonstrate surprising performance when using LLM-based forecasting models, and also reveal some of their critical shortcomings. This benchmark aims to advance multimodal forecasting by promoting models that are both accurate and accessible to decision-makers with varied technical expertise. The benchmark can be visualized at https://anon-forecast.github.io/benchmark_report_dev/.</p>
<p>URLs: <a href="https://anon-forecast.github.io/benchmark_report_dev/.">https://anon-forecast.github.io/benchmark_report_dev/.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2410.18959, https://anon-forecast.github.io/benchmark_report_dev/.', 224)">Copy Link</button>
<div id="copy-message-224" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2410.20542">PaPaGei: Open Foundation Models for Optical Physiological Signals</a></h1>
<p><b>Authors:</b> Arvind Pillai, Dimitris Spathis, Fahim Kawsar, Mohammad Malekzadeh</p>
<p>Abstract: Photoplethysmography (PPG) is the leading non-invasive technique for monitoring biosignals and cardiovascular health, with widespread adoption in both clinical settings and consumer wearable devices. While machine learning models trained on PPG signals have shown promise, they tend to be task-specific and struggle with generalization. Current research is limited by the use of single-device datasets, insufficient exploration of out-of-domain generalization, and a lack of publicly available models, which hampers reproducibility. To address these limitations, we present PaPaGei, the first open foundation model for PPG signals. The model is pre-trained on over 57,000 hours of data, comprising 20 million unlabeled PPG segments from publicly available datasets. We introduce a novel representation learning approach that leverages domain knowledge of PPG signal morphology across individuals, enabling the capture of richer representations compared to traditional contrastive learning methods. We evaluate PaPaGei against state-of-the-art time-series foundation models and self-supervised learning benchmarks across 20 tasks from 10 diverse datasets, spanning cardiovascular health, sleep disorders, pregnancy monitoring, and wellbeing assessment. Our model demonstrates superior performance, improving classification and regression metrics by 6.3% and 2.9% respectively in at least 14 tasks. Notably, PaPaGei achieves these results while being more data- and parameter-efficient, outperforming models that are 70x larger. Beyond accuracy, we examine model robustness across different skin tones, establishing a benchmark for bias evaluation in future models. PaPaGei can serve as both a feature extractor and an encoder for multimodal models, opening up new opportunities for multimodal health monitoring.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2410.20542', 225)">Copy Link</button>
<div id="copy-message-225" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2410.22110">Data Generation for Hardware-Friendly Post-Training Quantization</a></h1>
<p><b>Authors:</b> Lior Dikstein, Ariel Lapid, Arnon Netzer, Hai Victor Habi</p>
<p>Abstract: Zero-shot quantization (ZSQ) using synthetic data is a key approach for post-training quantization (PTQ) under privacy and security constraints. However, existing data generation methods often struggle to effectively generate data suitable for hardware-friendly quantization, where all model layers are quantized. We analyze existing data generation methods based on batch normalization (BN) matching and identify several gaps between synthetic and real data: 1) Current generation algorithms do not optimize the entire synthetic dataset simultaneously; 2) Data augmentations applied during training are often overlooked; and 3) A distribution shift occurs in the final model layers due to the absence of BN in those layers. These gaps negatively impact ZSQ performance, particularly in hardware-friendly quantization scenarios. In this work, we propose Data Generation for Hardware-friendly quantization (DGH), a novel method that addresses these gaps. DGH jointly optimizes all generated images, regardless of the image set size or GPU memory constraints. To address data augmentation mismatches, DGH includes a preprocessing stage that mimics the augmentation process and enhances image quality by incorporating natural image priors. Finally, we propose a new distribution-stretching loss that aligns the support of the feature map distribution between real and synthetic data. This loss is applied to the model's output and can be adapted to various tasks. DGH demonstrates significant improvements in quantization performance across multiple tasks, achieving up to a 30% increase in accuracy for hardware-friendly ZSQ in both classification and object detection, often performing on par with real data.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2410.22110', 226)">Copy Link</button>
<div id="copy-message-226" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2411.06568">Meta-Learning Objectives for Preference Optimization</a></h1>
<p><b>Authors:</b> Carlo Alfano, Silvia Sapora, Jakob Nicolaus Foerster, Patrick Rebeschini, Yee Whye Teh</p>
<p>Abstract: Evaluating preference optimization (PO) algorithms on LLM alignment is a challenging task that presents prohibitive costs, noise, and several variables like model size and hyper-parameters. In this work, we show that it is possible to gain insights on the efficacy of PO algorithm on much simpler benchmarks. We design a diagnostic suite of MuJoCo tasks and datasets, which we use to systematically evaluate PO algorithms, establishing a more controlled and cheaper benchmark. We then propose a novel family of PO algorithms based on mirror descent, which we call Mirror Preference Optimization (MPO). Through evolutionary strategies, we search this class to discover algorithms specialized to specific properties of preference datasets, such as mixed-quality or noisy data. We demonstrate that our discovered PO algorithms outperform all known algorithms in the targeted MuJoCo settings. Finally, based on the insights gained from our MuJoCo experiments, we design a novel PO algorithm that significantly outperforms existing baselines in an LLM alignment task.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2411.06568', 227)">Copy Link</button>
<div id="copy-message-227" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2411.07501">LAuReL: Learned Augmented Residual Layer</a></h1>
<p><b>Authors:</b> Gaurav Menghani, Ravi Kumar, Sanjiv Kumar</p>
<p>Abstract: One of the core pillars of efficient deep learning methods is architectural improvements such as the residual/skip connection, which has led to significantly better model convergence and quality. Since then the residual connection has become ubiquitous in not just convolutional neural networks but also transformer-based architectures, the backbone of LLMs.
  In this paper we introduce \emph{Learned Augmented Residual Layer} (LAuReL) -- a novel generalization of the canonical residual connection -- with the goal to be an in-situ replacement of the latter while outperforming on both model quality and footprint metrics. Our experiments show that using \laurel can help boost performance for both vision and language models. For example, on the ResNet-50, ImageNet 1K task, it achieves $60\%$ of the gains from adding an extra layer, while only adding $0.003\%$ more parameters, and matches it while adding $2.6\times$ fewer parameters.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2411.07501', 228)">Copy Link</button>
<div id="copy-message-228" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2411.14727">HyReaL: Clustering Attributed Graph via Hyper-Complex Space Representation Learning</a></h1>
<p><b>Authors:</b> Junyang Chen, Yang Lu, Mengke Li, Cuie Yang, Yiqun Zhang, Yiu-ming Cheung</p>
<p>Abstract: Clustering complex data in the form of attributed graphs has attracted increasing attention, where powerful graph representation is a critical prerequisite. However, the well-known Over-Smoothing (OS) effect makes Graph Convolutional Networks tend to homogenize the representation of graph nodes, while the existing OS solutions focus on alleviating the homogeneity of nodes' embeddings from the aspect of graph topology information, which is inconsistent with the attributed graph clustering objective. Therefore, we introduce hyper-complex space with powerful quaternion feature transformation to enhance the representation learning of the attributes. A generalized \textbf{Hy}per-complex space \textbf{Re}present\textbf{a}tion \textbf{L}earning (\textbf{HyReaL}) model is designed to: 1) bridge arbitrary dimensional attributes to the well-developed quaternion algebra with four parts, and 2) connect the learned representations to more generalized clustering objective without being restricted to a given number of clusters $k$. The novel introduction of quaternion benefits attributed graph clustering from two aspects: 1) enhanced attribute coupling learning capability allows complex attribute information to be sufficiently exploited in clustering, and 2) stronger learning capability makes it unnecessary to stack too many graph convolution layers, naturally alleviating the OS problem. It turns out that the node representations learned by HyReaL are more discriminative and widely suit downstream clustering with different $k$s. Extensive experiments including significance tests, ablation studies, qualitative results, etc., show the superiority of HyReaL.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2411.14727', 229)">Copy Link</button>
<div id="copy-message-229" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2411.15204">Label Distribution Shift-Aware Prediction Refinement for Test-Time Adaptation</a></h1>
<p><b>Authors:</b> Minguk Jang, Hye Won Chung</p>
<p>Abstract: Test-time adaptation (TTA) is an effective approach to mitigate performance degradation of trained models when encountering input distribution shifts at test time. However, existing TTA methods often suffer significant performance drops when facing additional class distribution shifts. We first analyze TTA methods under label distribution shifts and identify the presence of class-wise confusion patterns commonly observed across different covariate shifts. Based on this observation, we introduce label Distribution shift-Aware prediction Refinement for Test-time adaptation (DART), a novel TTA method that refines the predictions by focusing on class-wise confusion patterns. DART trains a prediction refinement module during an intermediate time by exposing it to several batches with diverse class distributions using the training dataset. This module is then used during test time to detect and correct class distribution shifts, significantly improving pseudo-label accuracy for test data. Our method exhibits 5-18% gains in accuracy under label distribution shifts on CIFAR-10C, without any performance degradation when there is no label distribution shift. Extensive experiments on CIFAR, PACS, OfficeHome, and ImageNet benchmarks demonstrate DART's ability to correct inaccurate predictions caused by test-time distribution shifts. This improvement leads to enhanced performance in existing TTA methods, making DART a valuable plug-in tool.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2411.15204', 230)">Copy Link</button>
<div id="copy-message-230" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2411.16303">Understanding Generalization of Federated Learning: the Trade-off between Model Stability and Optimization</a></h1>
<p><b>Authors:</b> Dun Zeng, Zheshun Wu, Shiyu Liu, Yu Pan, Xiaoying Tang, Zenglin Xu</p>
<p>Abstract: Federated Learning (FL) is a distributed learning approach that trains machine learning models across multiple devices while keeping their local data private. However, FL often faces challenges due to data heterogeneity, leading to inconsistent local optima among clients. These inconsistencies can cause unfavorable convergence behavior and generalization performance degradation. Existing studies mainly describe this issue through \textit{convergence analysis}, focusing on how well a model fits training data, or through \textit{algorithmic stability}, which examines the generalization gap. However, neither approach precisely captures the generalization performance of FL algorithms, especially for neural networks. This paper introduces an innovative generalization dynamics analysis framework, named as Libra, for algorithm-dependent excess risk minimization, highlighting the trade-offs between model stability and optimization. Through this framework, we show how the generalization of FL algorithms is affected by the interplay of algorithmic stability and optimization. This framework applies to standard federated optimization and its advanced variants, such as server momentum. Our findings suggest that larger local steps or momentum accelerate convergence but enlarge stability, while yielding a better minimum excess risk. These insights can guide the design of future algorithms to achieve stronger generalization.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2411.16303', 231)">Copy Link</button>
<div id="copy-message-231" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2412.00087">Physics-Informed Deep Learning Model for Line-integral Diagnostics Across Fusion Devices</a></h1>
<p><b>Authors:</b> Cong Wang, Weizhe Yang, Haiping Wang, Renjie Yang, Jing Li, Zhijun Wang, Xinyao Yu, Yixiong Wei, Xianli Huang, Chenshu Hu, Zhaoyang Liu, Changqing Zou, Zhifeng Zhao</p>
<p>Abstract: Rapid reconstruction of 2D plasma profiles from line-integral measurements is important in nuclear fusion. This paper introduces a physics-informed model architecture called Onion, that can enhance the performance of models and be adapted to various backbone networks. The model under Onion incorporates physical information by a multiplication process and applies the physics-informed loss function according to the principle of line integration. Experimental results demonstrate that the additional input of physical information improves the model's ability, leading to a reduction in the average relative error E_1 between the reconstruction profiles and the target profiles by approximately 52% on synthetic datasets and about 15% on experimental datasets. Furthermore, the implementation of the Softplus activation function in the final two fully connected layers improves model performance. This enhancement results in a reduction in the E_1 by approximately 71% on synthetic datasets and about 27% on experimental datasets. The incorporation of the physics-informed loss function has been shown to correct the model's predictions, bringing the back-projections closer to the actual inputs and reducing the errors associated with inversion algorithms. Besides, we have developed a synthetic data model to generate customized line-integral diagnostic datasets and have also collected soft x-ray diagnostic datasets from EAST and HL-2A. This study achieves reductions in reconstruction errors, and accelerates the development of diagnostic surrogate models in fusion research.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2412.00087', 232)">Copy Link</button>
<div id="copy-message-232" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2412.06540">Sloth: scaling laws for LLM skills to predict multi-benchmark performance across families</a></h1>
<p><b>Authors:</b> Felipe Maia Polo, Seamus Somerstep, Leshem Choshen, Yuekai Sun, Mikhail Yurochkin</p>
<p>Abstract: Scaling laws for large language models (LLMs) predict model performance based on parameters like size and training data. However, differences in training configurations and data processing across model families lead to significant variations in benchmark performance, making it difficult for a single scaling law to generalize across all LLMs. On the other hand, training family-specific scaling laws requires training models of varying sizes for every family. In this work, we propose Skills Scaling Laws (SSLaws, pronounced as Sloth), a novel scaling law that leverages publicly available benchmark data and assumes LLM performance is driven by low-dimensional latent skills, such as reasoning and instruction following. These latent skills are influenced by computational resources like model size and training tokens but with varying efficiencies across model families. Sloth exploits correlations across benchmarks to provide more accurate and interpretable predictions while alleviating the need to train multiple LLMs per family. We present both theoretical results on parameter identification and empirical evaluations on 12 prominent benchmarks, from Open LLM Leaderboard v1/v2, demonstrating that Sloth predicts LLM performance efficiently and offers insights into scaling behaviors for complex downstream tasks and increased test-time compute.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2412.06540', 233)">Copy Link</button>
<div id="copy-message-233" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2412.10146">Investigating generalization capabilities of neural networks by means of loss landscapes and Hessian analysis</a></h1>
<p><b>Authors:</b> Nikita Gabdullin</p>
<p>Abstract: This paper studies generalization capabilities of neural networks (NNs) using new and improved PyTorch library Loss Landscape Analysis (LLA). LLA facilitates visualization and analysis of loss landscapes along with the properties of NN Hessian. Different approaches to NN loss landscape plotting are discussed with particular focus on normalization techniques showing that conventional methods cannot always ensure correct visualization when batch normalization layers are present in NN architecture. The use of Hessian axes is shown to be able to mitigate this effect, and methods for choosing Hessian axes are proposed. In addition, spectra of Hessian eigendecomposition are studied and it is shown that typical spectra exist for a wide range of NNs. This allows to propose quantitative criteria for Hessian analysis that can be applied to evaluate NN performance and assess its generalization capabilities. Generalization experiments are conducted using ImageNet-1K pre-trained models along with several models trained as part of this study. The experiment include training models on one dataset and testing on another one to maximize experiment similarity to model performance in the Wild. It is shown that when datasets change, the changes in criteria correlate with the changes in accuracy, making the proposed criteria a computationally efficient estimate of generalization ability, which is especially useful for extremely large datasets.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2412.10146', 234)">Copy Link</button>
<div id="copy-message-234" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2412.16502">Spatial-Temporal Knowledge Distillation for Takeaway Recommendation</a></h1>
<p><b>Authors:</b> Shuyuan Zhao, Wei Chen, Boyan Shi, Liyong Zhou, Shuohao Lin, Huaiyu Wan</p>
<p>Abstract: The takeaway recommendation system aims to recommend users' future takeaway purchases based on their historical purchase behaviors, thereby improving user satisfaction and boosting merchant sales. Existing methods focus on incorporating auxiliary information or leveraging knowledge graphs to alleviate the sparsity issue of user purchase sequences. However, two main challenges limit the performance of these approaches: (1) capturing dynamic user preferences on complex geospatial information and (2) efficiently integrating spatial-temporal knowledge from both graphs and sequence data with low computational costs. In this paper, we propose a novel spatial-temporal knowledge distillation model for takeaway recommendation (STKDRec) based on the two-stage training process. Specifically, during the first pre-training stage, a spatial-temporal knowledge graph (STKG) encoder is trained to extract high-order spatial-temporal dependencies and collaborative associations from the STKG. During the second spatial-temporal knowledge distillation (STKD) stage, a spatial-temporal Transformer (ST-Transformer) is employed to comprehensively model dynamic user preferences on various types of fine-grained geospatial information from a sequential perspective. Furthermore, the STKD strategy is introduced to transfer graph-based spatial-temporal knowledge to the ST-Transformer, facilitating the adaptive fusion of rich knowledge derived from both the STKG and sequence data while reducing computational overhead. Extensive experiments on three real-world datasets show that STKDRec significantly outperforms the state-of-the-art baselines.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2412.16502', 235)">Copy Link</button>
<div id="copy-message-235" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2412.20824">Isoperimetry is All We Need: Langevin Posterior Sampling for RL with Sublinear Regret</a></h1>
<p><b>Authors:</b> Emilio Jorge, Christos Dimitrakakis, Debabrota Basu</p>
<p>Abstract: Common assumptions, like linear or RKHS models, and Gaussian or log-concave posteriors over the models, do not explain practical success of RL across a wider range of distributions and models. Thus, we study how to design RL algorithms with sublinear regret for isoperimetric distributions, specifically the ones satisfying the Log-Sobolev Inequality (LSI). LSI distributions include the standard setups of RL theory, and others, such as many non-log-concave and perturbed distributions. First, we show that the Posterior Sampling-based RL (PSRL) algorithm yields sublinear regret if the data distributions satisfy LSI and some mild additional assumptions. Also, when we cannot compute or sample from an exact posterior, we propose a Langevin sampling-based algorithm design: LaPSRL. We show that LaPSRL achieves order-optimal regret and subquadratic complexity per episode. Finally, we deploy LaPSRL with a Langevin sampler -- SARAH-LD, and test it for different bandit and MDP environments. Experimental results validate the generality of LaPSRL across environments and its competitive performance with respect to the baselines.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2412.20824', 236)">Copy Link</button>
<div id="copy-message-236" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2501.07306">Variable Bregman Majorization-Minimization Algorithm and its Application to Dirichlet Maximum Likelihood Estimation</a></h1>
<p><b>Authors:</b> S\'egol\`ene Martin, Jean-Christophe Pesquet, Gabriele Steidl, Ismail Ben Ayed</p>
<p>Abstract: We propose a novel Bregman descent algorithm for minimizing a convex function that is expressed as the sum of a differentiable part (defined over an open set) and a possibly nonsmooth term. The approach, referred to as the Variable Bregman Majorization-Minimization (VBMM) algorithm, extends the Bregman Proximal Gradient method by allowing the Bregman function used in the divergence to adaptively vary at each iteration, provided it satisfies a majorizing condition on the objective function. This adaptive framework enables the algorithm to approximate the objective more precisely at each iteration, thereby allowing for accelerated convergence compared to the traditional Bregman Proximal Gradient descent. We establish the convergence of the VBMM algorithm to a minimizer under mild assumptions on the family of metrics used. Furthermore, we introduce a novel application of both the Bregman Proximal Gradient method and the VBMM algorithm to the estimation of the multidimensional parameters of a Dirichlet distribution through the maximization of its log-likelihood. Numerical experiments confirm that the VBMM algorithm outperforms existing approaches in terms of convergence speed.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2501.07306', 237)">Copy Link</button>
<div id="copy-message-237" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2501.08418">CVaR-Based Variational Quantum Optimization for User Association in Handoff-Aware Vehicular Networks</a></h1>
<p><b>Authors:</b> Zijiang Yan, Hao Zhou, Jianhua Pei, Aryan Kaushik, Hina Tabassum, Ping Wang</p>
<p>Abstract: Efficient resource allocation is essential for optimizing various tasks in wireless networks, which are usually formulated as generalized assignment problems (GAP). GAP, as a generalized version of the linear sum assignment problem, involves both equality and inequality constraints that add computational challenges. In this work, we present a novel Conditional Value at Risk (CVaR)-based Variational Quantum Eigensolver (VQE) framework to address GAP in vehicular networks (VNets). Our approach leverages a hybrid quantum-classical structure, integrating a tailored cost function that balances both objective and constraint-specific penalties to improve solution quality and stability. Using the CVaR-VQE model, we handle the GAP efficiently by focusing optimization on the lower tail of the solution space, enhancing both convergence and resilience on noisy intermediate-scale quantum (NISQ) devices. We apply this framework to a user-association problem in VNets, where our method achieves 23.5% improvement compared to the deep neural network (DNN) approach.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2501.08418', 238)">Copy Link</button>
<div id="copy-message-238" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2501.08958">Kolmogorov-Arnold Networks for Time Series Granger Causality Inference</a></h1>
<p><b>Authors:</b> Meiliang Liu, Yunfang Xu, Zijin Li, Zhengye Si, Xiaoxiao Yang, Xinyue Yang, Zhiwen Zhao</p>
<p>Abstract: We propose the Granger causality inference Kolmogorov-Arnold Networks (KANGCI), a novel architecture that extends the recently proposed Kolmogorov-Arnold Networks (KAN) to the domain of causal inference. By extracting base weights from KAN layers and incorporating the sparsity-inducing penalty and ridge regularization, KANGCI effectively infers the Granger causality from time series. Additionally, we propose an algorithm based on time-reversed Granger causality that automatically selects causal relationships with better inference performance from the original or time-reversed time series or integrates the results to mitigate spurious connectivities. Comprehensive experiments conducted on Lorenz-96, Gene regulatory networks, fMRI BOLD signals, VAR, and real-world EEG datasets demonstrate that the proposed model achieves competitive performance to state-of-the-art methods in inferring Granger causality from nonlinear, high-dimensional, and limited-sample time series.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2501.08958', 239)">Copy Link</button>
<div id="copy-message-239" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2501.10910">DeepIFSAC: Deep Imputation of Missing Values Using Feature and Sample Attention within Contrastive Framework</a></h1>
<p><b>Authors:</b> Ibna Kowsar, Shourav B. Rabbani, Yina Hou, Manar D. Samad</p>
<p>Abstract: Missing values of varying patterns and rates in real-world tabular data pose a significant challenge in developing reliable data-driven models. Existing missing value imputation methods use statistical and traditional machine learning and are ineffective when the missing rate is high and not at random. This paper explores row and column attention in tabular data as between-feature and between-sample attention in a novel framework to reconstruct missing values. The proposed method uses the CutMix data augmentation within a contrastive learning framework to improve the uncertainty of missing value estimation. The performance and generalizability of trained imputation models are evaluated on set-aside test data folds with missing values. The proposed framework outperforms nine state-of-the-art imputation methods across several missing value types and rates (10\%-50\%) on a diverse selection of twelve tabular data sets. We evaluate the quality of imputed data using real-world electronic health records with missing values, demonstrating our proposed framework's superiority to state-of-the-art statistical, machine learning, and deep imputation methods. This paper highlights the heterogeneity of tabular data sets to recommend imputation methods based on missing value types and data characteristics.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2501.10910', 240)">Copy Link</button>
<div id="copy-message-240" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2501.11689">Randomness, exchangeability, and conformal prediction</a></h1>
<p><b>Authors:</b> Vladimir Vovk</p>
<p>Abstract: This paper continues development of the functional theory of randomness, a modification of the algorithmic theory of randomness getting rid of unspecified additive constants. It introduces new kinds of confidence predictors, including randomness predictors (the most general confidence predictors based on the assumption of IID observations) and exchangeability predictors (the most general confidence predictors based on the assumption of exchangeable observations). The main result implies that both are close to conformal predictors and quantifies the difference between randomness prediction and conformal prediction.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2501.11689', 241)">Copy Link</button>
<div id="copy-message-241" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2501.13331">Qrazor: Reliable and Effortless 4-bit LLM Quantization by Significant Data Razoring</a></h1>
<p><b>Authors:</b> Dongyoung Lee, Seungkyu Choi, Ik Joon Chang</p>
<p>Abstract: Large-scale language models (LLMs) excel in language processing tasks but face deployment challenges due to high memory and computational demands. While low-bit quantization, such as 4-bit techniques, offers a potential solution, these methods often suffer from significant accuracy loss or require considerable effort for implementation such as reordering, rotation, etc. To address these challenges, we propose QRazor, a simple yet effective quantization scheme that enables 4-bit quantization of weights, activations, and KV cache in transformer-based LLMs. QRazor operates in two stages: first, quantizing data using 8 or 16-bit integers as a basis with absolute max scaling to preserve accuracy close to full-precision models, and second, compressing the quantized data to 4-bit using our significant data razoring (SDR) technique, which retains only the four most salient bits. Without any additional requirment of fine-tuning or additional training, QRazor achieves performance similar or better compared to state-of-the-art in 4-bit quantization method, surpassing Smoothquant and QLLM by over 12 points and Quarot(RTN) by more than 2.9 points in zero-shot reasoning task accuracy on the LLaMA2-7B model. Additionally, we introduce an integer-based arithmetic unit optimized for QRazor, allowing direct low-precision operations on SDR data without decompression.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2501.13331', 242)">Copy Link</button>
<div id="copy-message-242" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2501.16368">Foundation Models for CPS-IoT: Opportunities and Challenges</a></h1>
<p><b>Authors:</b> Ozan Baris, Yizhuo Chen, Gaofeng Dong, Liying Han, Tomoyoshi Kimura, Pengrui Quan, Ruijie Wang, Tianchen Wang, Tarek Abdelzaher, Mario Berg\'es, Paul Pu Liang, Mani Srivastava</p>
<p>Abstract: Methods from machine learning (ML) have transformed the implementation of Perception-Cognition-Communication-Action loops in Cyber-Physical Systems (CPS) and the Internet of Things (IoT), replacing mechanistic and basic statistical models with those derived from data. However, the first generation of ML approaches, which depend on supervised learning with annotated data to create task-specific models, faces significant limitations in scaling to the diverse sensor modalities, deployment configurations, application tasks, and operating dynamics characterizing real-world CPS-IoT systems. The success of task-agnostic foundation models (FMs), including multimodal large language models (LLMs), in addressing similar challenges across natural language, computer vision, and human speech has generated considerable enthusiasm for and exploration of FMs and LLMs as flexible building blocks in CPS-IoT analytics pipelines, promising to reduce the need for costly task-specific engineering.
  Nonetheless, a significant gap persists between the current capabilities of FMs and LLMs in the CPS-IoT domain and the requirements they must meet to be viable for CPS-IoT applications. In this paper, we analyze and characterize this gap through a thorough examination of the state of the art and our research, which extends beyond it in various dimensions. Based on the results of our analysis and research, we identify essential desiderata that CPS-IoT domain-specific FMs and LLMs must satisfy to bridge this gap. We also propose actions by CPS-IoT researchers to collaborate in developing key community resources necessary for establishing FMs and LLMs as foundational tools for the next generation of CPS-IoT systems.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2501.16368', 243)">Copy Link</button>
<div id="copy-message-243" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2501.16718">Outlier Synthesis via Hamiltonian Monte Carlo for Out-of-Distribution Detection</a></h1>
<p><b>Authors:</b> Hengzhuang Li, Teng Zhang</p>
<p>Abstract: Out-of-distribution (OOD) detection is crucial for developing trustworthy and reliable machine learning systems. Recent advances in training with auxiliary OOD data demonstrate efficacy in enhancing detection capabilities. Nonetheless, these methods heavily rely on acquiring a large pool of high-quality natural outliers. Some prior methods try to alleviate this problem by synthesizing virtual outliers but suffer from either poor quality or high cost due to the monotonous sampling strategy and the heavy-parameterized generative models. In this paper, we overcome all these problems by proposing the Hamiltonian Monte Carlo Outlier Synthesis (HamOS) framework, which views the synthesis process as sampling from Markov chains. Based solely on the in-distribution data, the Markov chains can extensively traverse the feature space and generate diverse and representative outliers, hence exposing the model to miscellaneous potential OOD scenarios. The Hamiltonian Monte Carlo with sampling acceptance rate almost close to 1 also makes our framework enjoy great efficiency. By empirically competing with SOTA baselines on both standard and large-scale benchmarks, we verify the efficacy and efficiency of our proposed HamOS.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2501.16718', 244)">Copy Link</button>
<div id="copy-message-244" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2501.18094">AlphaAdam:Asynchronous Masked Optimization with Dynamic Alpha for Selective Updates</a></h1>
<p><b>Authors:</b> Da Chang, Yu Li, Ganzhao Yuan</p>
<p>Abstract: In the training of large language models (LLMs), updating parameters more efficiently and stably has always been an important challenge. To achieve efficient parameter updates, existing methods usually achieve performance comparable to full parameter updates through methods such as low-dimensional decomposition or layer-wise selective updates. In this work, we propose AlphaAdam, an optimization framework for LLM from the perspective of intra-layer parameter updates. By decoupling parameter updates and dynamically adjusting their strength, AlphaAdam accelerates convergence and improves training stability. We construct parameter masks based on the consistency of historical momentum and gradient direction and combine them with an adaptive mask strength strategy to ensure efficient optimization and theoretical convergence guarantees, which is also applicable to most momentum-based optimizers. Extensive experiments show that AlphaAdam outperforms state-of-the-art methods such as AdamW in terms of convergence speed and computational efficiency across tasks, including GPT-2 pre-trained and fine-tuned RoBERTa and Llama-7B. Our AlphaAdam implements an optimizer enhancement framework for LLMs through intra-layer asynchronous masked adaptive updates. Our code is available in this https://github.com/MaeChd/AlphaAdam.</p>
<p>URLs: <a href="https://github.com/MaeChd/AlphaAdam.">https://github.com/MaeChd/AlphaAdam.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2501.18094, https://github.com/MaeChd/AlphaAdam.', 245)">Copy Link</button>
<div id="copy-message-245" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2501.18184">Genetic Algorithm with Border Trades (GAB)</a></h1>
<p><b>Authors:</b> Qingchuan Lyu</p>
<p>Abstract: This paper introduces a novel approach to improving Genetic Algorithms (GA) in large or complex problem spaces by incorporating new chromosome patterns in the breeding process through border trade activities. These strategies increase chromosome diversity, preventing premature convergence and enhancing the GA's ability to explore the solution space more effectively. Empirical evidence demonstrates significant improvements in convergence behavior. This approach offers a promising pathway to addressing challenges in optimizing large or complex problem domains.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2501.18184', 246)">Copy Link</button>
<div id="copy-message-246" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2502.00047">HadamRNN: Binary and Sparse Ternary Orthogonal RNNs</a></h1>
<p><b>Authors:</b> Armand Foucault (IMT, ANITI), Franck Mamalet (ANITI), Fran\c{c}ois Malgouyres (IMT)</p>
<p>Abstract: Binary and sparse ternary weights in neural networks enable faster computations and lighter representations, facilitating their use on edge devices with limited computational power. Meanwhile, vanilla RNNs are highly sensitive to changes in their recurrent weights, making the binarization and ternarization of these weights inherently challenging. To date, no method has successfully achieved binarization or ternarization of vanilla  RNN weights. We present a new approach leveraging the properties of Hadamard matrices to parameterize a subset of binary and sparse ternary orthogonal matrices. This method enables the training of orthogonal RNNs (ORNNs) with binary and sparse ternary recurrent weights, effectively creating a specific class of binary and sparse ternary vanilla RNNs. The resulting ORNNs, called HadamRNN and lock-HadamRNN, are evaluated on benchmarks such as the copy task, permuted and sequential MNIST tasks, and IMDB dataset. Despite binarization or sparse ternarization, these RNNs maintain performance levels comparable to state-of-the-art full-precision models, highlighting the effectiveness of our approach. Notably, our approach is the first solution with binary recurrent weights capable of tackling the copy task over 1000 timesteps.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.00047', 247)">Copy Link</button>
<div id="copy-message-247" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2502.00473">Weak-to-Strong Diffusion with Reflection</a></h1>
<p><b>Authors:</b> Lichen Bai, Masashi Sugiyama, Zeke Xie</p>
<p>Abstract: The goal of diffusion generative models is to align the learned distribution with the real data distribution through gradient score matching. However, inherent limitations in training data quality, modeling strategies, and architectural design lead to inevitable gap between generated outputs and real data. To reduce this gap, we propose Weak-to-Strong Diffusion (W2SD), a novel framework that utilizes the estimated difference between existing weak and strong models (i.e., weak-to-strong difference) to approximate the gap between an ideal model and a strong model. By employing a reflective operation that alternates between denoising and inversion with weak-to-strong difference, we theoretically understand that W2SD steers latent variables along sampling trajectories toward regions of the real data distribution. W2SD is highly flexible and broadly applicable, enabling diverse improvements through the strategic selection of weak-to-strong model pairs (e.g., DreamShaper vs. SD1.5, good experts vs. bad experts in MoE). Extensive experiments demonstrate that W2SD significantly improves human preference, aesthetic quality, and prompt adherence, achieving SOTA performance across various modalities (e.g., image, video), architectures (e.g., UNet-based, DiT-based, MoE), and benchmarks. For example, Juggernaut-XL with W2SD can improve with the HPSv2 winning rate up to 90% over the original results. Moreover, the performance gains achieved by W2SD markedly outweigh its additional computational overhead, while the cumulative improvements from different weak-to-strong difference further solidify its practical utility and deployability.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.00473', 248)">Copy Link</button>
<div id="copy-message-248" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2502.00620">Representations Shape Weak-to-Strong Generalization: Theoretical Insights and Empirical Predictions</a></h1>
<p><b>Authors:</b> Yihao Xue, Jiping Li, Baharan Mirzasoleiman</p>
<p>Abstract: Weak-to-Strong Generalization (W2SG), where a weak model supervises a stronger one, serves as an important analogy for understanding how humans might guide superhuman intelligence in the future. Promising empirical results revealed that a strong model can surpass its weak supervisor. While recent work has offered theoretical insights into this phenomenon, a clear understanding of the interactions between weak and strong models that drive W2SG remains elusive. We investigate W2SG through a theoretical lens and show that it can be characterized using kernels derived from the principal components of weak and strong models' internal representations. These kernels can be used to define a space that, at a high level, captures what the weak model is unable to learn but is learnable by the strong model. The projection of labels onto this space quantifies how much the strong model falls short of its full potential due to weak supervision. This characterization also provides insights into how certain errors in weak supervision can be corrected by the strong model, regardless of overfitting. Our theory has significant practical implications, providing a representation-based metric that predicts W2SG performance trends without requiring labels, as shown in experiments on molecular predictions with transformers and 5 NLP tasks involving 52 LLMs.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.00620', 249)">Copy Link</button>
<div id="copy-message-249" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2502.01208">Almost Surely Safe Alignment of Large Language Models at Inference-Time</a></h1>
<p><b>Authors:</b> Xiaotong Ji, Shyam Sundhar Ramesh, Matthieu Zimmer, Ilija Bogunovic, Jun Wang, Haitham Bou Ammar</p>
<p>Abstract: Even highly capable large language models (LLMs) can produce biased or unsafe responses, and alignment techniques, such as RLHF, aimed at mitigating this issue, are expensive and prone to overfitting as they retrain the LLM. This paper introduces a novel inference-time alignment approach that ensures LLMs generate safe responses almost surely, i.e., with a probability approaching one. We achieve this by framing the safe generation of inference-time responses as a constrained Markov decision process within the LLM's latent space. Crucially, we augment a safety state that tracks the evolution of safety constraints and enables us to demonstrate formal safety guarantees upon solving the MDP in the latent space. Building on this foundation, we propose InferenceGuard, a practical implementation that safely aligns LLMs without modifying the model weights. Empirically, we demonstrate InferenceGuard effectively balances safety and task performance, outperforming existing inference-time alignment methods in generating safe and aligned responses.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.01208', 250)">Copy Link</button>
<div id="copy-message-250" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2502.01692">Fast Direct: Query-Efficient Online Black-box Guidance for Diffusion-model Target Generation</a></h1>
<p><b>Authors:</b> Kim Yong Tan, Yueming Lyu, Ivor Tsang, Yew-Soon Ong</p>
<p>Abstract: Guided diffusion-model generation is a promising direction for customizing the generation process of a pre-trained diffusion-model to address the specific downstream tasks. Existing guided diffusion models either rely on training of the guidance model with pre-collected datasets or require the objective functions to be differentiable. However, for most real-world tasks, the offline datasets are often unavailable, and their objective functions are often not differentiable, such as image generation with human preferences, molecular generation for drug discovery, and material design. Thus, we need an \textbf{online} algorithm capable of collecting data during runtime and supporting a \textbf{black-box} objective function. Moreover, the \textbf{query efficiency} of the algorithm is also critical because the objective evaluation of the query is often expensive in the real-world scenarios. In this work, we propose a novel and simple algorithm, \textbf{Fast Direct}, for query-efficient online black-box target generation. Our Fast Direct builds a pseudo-target on the data manifold to update the noise sequence of the diffusion model with a universal direction, which is promising to perform query-efficient guided generation. Extensive experiments on twelve high-resolution ($\small {1024 \times 1024}$) image target generation tasks and six 3D-molecule target generation tasks show $\textbf{6}\times$ up to $\textbf{10}\times$ query efficiency improvement and $\textbf{11}\times$ up to $\textbf{44}\times$ query efficiency improvement, respectively. Our implementation is publicly available at: https://github.com/kimyong95/guide-stable-diffusion/tree/fast-direct</p>
<p>URLs: <a href="https://github.com/kimyong95/guide-stable-diffusion/tree/fast-direct">https://github.com/kimyong95/guide-stable-diffusion/tree/fast-direct</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.01692, https://github.com/kimyong95/guide-stable-diffusion/tree/fast-direct', 251)">Copy Link</button>
<div id="copy-message-251" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2502.01989">T-SCEND: Test-time Scalable MCTS-enhanced Diffusion Model</a></h1>
<p><b>Authors:</b> Tao Zhang, Jia-Shu Pan, Ruiqi Feng, Tailin Wu</p>
<p>Abstract: We introduce Test-time Scalable MCTS-enhanced Diffusion Model (T-SCEND), a novel framework that significantly improves diffusion model's reasoning capabilities with better energy-based training and scaling up test-time computation. We first show that na\"ively scaling up inference budget for diffusion models yields marginal gain. To address this, the training of T-SCEND consists of a novel linear-regression negative contrastive learning objective to improve the performance-energy consistency of the energy landscape, and a KL regularization to reduce adversarial sampling. During inference, T-SCEND integrates the denoising process with a novel hybrid Monte Carlo Tree Search (hMCTS), which sequentially performs best-of-N random search and MCTS as denoising proceeds. On challenging reasoning tasks of Maze and Sudoku, we demonstrate the effectiveness of T-SCEND's training objective and scalable inference method. In particular, trained with Maze sizes of up to $6\times6$, our T-SCEND solves $88\%$ of Maze problems with much larger sizes of $15\times15$, while standard diffusion completely fails. Code to reproduce the experiments can be found at https://github.com/AI4Science-WestlakeU/t_scend.</p>
<p>URLs: <a href="https://github.com/AI4Science-WestlakeU/t_scend.">https://github.com/AI4Science-WestlakeU/t_scend.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.01989, https://github.com/AI4Science-WestlakeU/t_scend.', 252)">Copy Link</button>
<div id="copy-message-252" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2502.02202">Multi-level Supervised Contrastive Learning</a></h1>
<p><b>Authors:</b> Naghmeh Ghanooni, Barbod Pajoum, Harshit Rawal, Sophie Fellenz, Vo Nguyen Le Duy, Marius Kloft</p>
<p>Abstract: Contrastive learning is a well-established paradigm in representation learning. The standard framework of contrastive learning minimizes the distance between "similar" instances and maximizes the distance between dissimilar ones in the projection space, disregarding the various aspects of similarity that can exist between two samples. Current methods rely on a single projection head, which fails to capture the full complexity of different aspects of a sample, leading to suboptimal performance, especially in scenarios with limited training data. In this paper, we present a novel supervised contrastive learning method in a unified framework called multilevel contrastive learning (MLCL), that can be applied to both multi-label and hierarchical classification tasks. The key strength of the proposed method is the ability to capture similarities between samples across different labels and/or hierarchies using multiple projection heads. Extensive experiments on text and image datasets demonstrate that the proposed approach outperforms state-of-the-art contrastive learning methods</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02202', 253)">Copy Link</button>
<div id="copy-message-253" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2502.02275">A User&#x27;s Guide to Sampling Strategies for Sliced Optimal Transport</a></h1>
<p><b>Authors:</b> Keanu Sisouk, Julie Delon, Julien Tierny</p>
<p>Abstract: This paper serves as a user's guide to sampling strategies for sliced optimal transport. We provide reminders and additional regularity results on the Sliced Wasserstein distance. We detail the construction methods, generation time complexity, theoretical guarantees, and conditions for each strategy. Additionally, we provide insights into their suitability for sliced optimal transport in theory. Extensive experiments on both simulated and real-world data offer a representative comparison of the strategies, culminating in practical recommendations for their best usage.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02275', 254)">Copy Link</button>
<div id="copy-message-254" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2502.02523">Brief analysis of DeepSeek R1 and its implications for Generative AI</a></h1>
<p><b>Authors:</b> Sarah Mercer, Samuel Spillard, Daniel P. Martin</p>
<p>Abstract: In late January 2025, DeepSeek released their new reasoning model (DeepSeek R1); which was developed at a fraction of the cost yet remains competitive with OpenAI's models, despite the US's GPU export ban. This report discusses the model, and what its release means for the field of Generative AI more widely. We briefly discuss other models released from China in recent weeks, their similarities; innovative use of Mixture of Experts (MoE), Reinforcement Learning (RL) and clever engineering appear to be key factors in the capabilities of these models. This think piece has been written to a tight timescale, providing broad coverage of the topic, and serves as introductory material for those looking to understand the model's technical advancements, as well as its place in the ecosystem. Several further areas of research are identified.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02523', 255)">Copy Link</button>
<div id="copy-message-255" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2502.02531">Deep Linear Network Training Dynamics from Random Initialization: Data, Width, Depth, and Hyperparameter Transfer</a></h1>
<p><b>Authors:</b> Blake Bordelon, Cengiz Pehlevan</p>
<p>Abstract: We theoretically characterize gradient descent dynamics in deep linear networks trained at large width from random initialization and on large quantities of random data. Our theory captures the ``wider is better" effect of mean-field/maximum-update parameterized networks as well as hyperparameter transfer effects, which can be contrasted with the neural-tangent parameterization where optimal learning rates shift with model width. We provide asymptotic descriptions of both non-residual and residual neural networks, the latter of which enables an infinite depth limit when branches are scaled as $1/\sqrt{\text{depth}}$. We also compare training with one-pass stochastic gradient descent to the dynamics when training data are repeated at each iteration. Lastly, we show that this model recovers the accelerated power law training dynamics for power law structured data in the rich regime observed in recent works.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02531', 256)">Copy Link</button>
<div id="copy-message-256" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2502.02542">OverThink: Slowdown Attacks on Reasoning LLMs</a></h1>
<p><b>Authors:</b> Abhinav Kumar, Jaechul Roh, Ali Naseh, Marzena Karpinska, Mohit Iyyer, Amir Houmansadr, Eugene Bagdasarian</p>
<p>Abstract: We increase overhead for applications that rely on reasoning LLMs-we force models to spend an amplified number of reasoning tokens, i.e., "overthink", to respond to the user query while providing contextually correct answers. The adversary performs an OVERTHINK attack by injecting decoy reasoning problems into the public content that is used by the reasoning LLM (e.g., for RAG applications) during inference time. Due to the nature of our decoy problems (e.g., a Markov Decision Process), modified texts do not violate safety guardrails. We evaluated our attack across closed-(OpenAI o1, o1-mini, o3-mini) and open-(DeepSeek R1) weights reasoning models on the FreshQA and SQuAD datasets. Our results show up to 18x slowdown on FreshQA dataset and 46x slowdown on SQuAD dataset. The attack also shows high transferability across models. To protect applications, we discuss and implement defenses leveraging LLM-based and system design approaches. Finally, we discuss societal, financial, and energy impacts of OVERTHINK attack which could amplify the costs for third-party applications operating reasoning models.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.02542', 257)">Copy Link</button>
<div id="copy-message-257" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2204.05923">An Algebraically Converging Stochastic Gradient Descent Algorithm for Global Optimization</a></h1>
<p><b>Authors:</b> Bj\"orn Engquist, Kui Ren, Yunan Yang</p>
<p>Abstract: We propose a new gradient descent algorithm with added stochastic terms for finding the global optimizers of nonconvex optimization problems. A key component in the algorithm is the adaptive tuning of the randomness based on the value of the objective function. In the language of simulated annealing, the temperature is state-dependent. With this, we prove the global convergence of the algorithm with an algebraic rate both in probability and in the parameter space. This is a significant improvement over the classical rate from using a more straightforward control of the noise term. The convergence proof is based on the actual discrete setup of the algorithm, not just its continuous limit as often done in the literature. We also present several numerical examples to demonstrate the efficiency and robustness of the algorithm for reasonably complex objective functions.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2204.05923', 258)">Copy Link</button>
<div id="copy-message-258" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2209.09404">Machine Learning-Augmented Optimization of Large Bilevel and Two-stage Stochastic Programs: Application to Cycling Network Design</a></h1>
<p><b>Authors:</b> Timothy C. Y. Chan, Bo Lin, Shoshanna Saxe</p>
<p>Abstract: A wide range of decision problems can be formulated as bilevel programs with independent followers, which as a special case include two-stage stochastic programs. These problems are notoriously difficult to solve especially when a large number of followers present. Motivated by a real-world cycling infrastructure planning application, we present a general approach to solving such problems. We propose an optimization model that explicitly considers a sampled subset of followers and exploits a machine learning model to estimate the objective values of unsampled followers. We prove bounds on the optimality gap of the generated leader decision as measured by the original objective function that considers the full follower set. We then develop follower sampling algorithms to tighten the bounds and a representation learning approach to learn follower features, which are used as inputs to the embedded machine learning model. Through numerical studies, we show that our approach generates leader decisions of higher quality compared to baselines. Finally, in collaboration with the City of Toronto, we perform a real-world case study in Toronto where we solve a cycling network design problem with over one million followers. Compared to the current practice, our approach improves Toronto's cycling accessibility by 19.2%, equivalent to $18M in potential cost savings. Our approach is being used to inform the cycling infrastructure planning in Toronto and outperforms the current practice by a large margin. It can be generalized to any decision problems that are formulated as bilevel programs with independent followers.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2209.09404', 259)">Copy Link</button>
<div id="copy-message-259" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2308.14536">Spoken Language Intelligence of Large Language Models for Language Learning</a></h1>
<p><b>Authors:</b> Linkai Peng, Baorian Nuchged, Yingming Gao</p>
<p>Abstract: People have long hoped for a conversational system that can assist in real-life situations, and recent progress on large language models (LLMs) is bringing this idea closer to reality. While LLMs are often impressive in performance, their efficacy in real-world scenarios that demand expert knowledge remains unclear. LLMs are believed to hold the most potential and value in education, especially in the development of Artificial intelligence (AI) based virtual teachers capable of facilitating language learning. Our focus is centered on evaluating the efficacy of LLMs in the realm of education, specifically in the areas of spoken language learning which encompass phonetics, phonology, and second language acquisition. We introduce a new multiple-choice question dataset to evaluate the effectiveness of LLMs in the aforementioned scenarios, including understanding and application of spoken language knowledge. In addition, we investigate the influence of various prompting techniques such as zero- and few-shot method (prepending the question with question-answer exemplars), chain-of-thought (CoT, think step-by-step), in-domain exampler and external tools (Google, Wikipedia). We conducted large-scale evaluation on popular LLMs (20 distinct models) using these methods. We achieved significant performance improvements compared to the zero-shot baseline in the practical questions reasoning (GPT-3.5, 49.1% -> 63.1%; LLaMA2-70B-Chat, 42.2% -> 48.6%). We found that models of different sizes have good understanding of concepts in phonetics, phonology, and second language acquisition, but show limitations in reasoning for real-world problems. Additionally, we also explore preliminary findings on conversational communication.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2308.14536', 260)">Copy Link</button>
<div id="copy-message-260" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2309.02705">Certifying LLM Safety against Adversarial Prompting</a></h1>
<p><b>Authors:</b> Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Aaron Jiaxun Li, Soheil Feizi, Himabindu Lakkaraju</p>
<p>Abstract: Large language models (LLMs) are vulnerable to adversarial attacks that add malicious tokens to an input prompt to bypass the safety guardrails of an LLM and cause it to produce harmful content. In this work, we introduce erase-and-check, the first framework for defending against adversarial prompts with certifiable safety guarantees. Given a prompt, our procedure erases tokens individually and inspects the resulting subsequences using a safety filter. Our safety certificate guarantees that harmful prompts are not mislabeled as safe due to an adversarial attack up to a certain size. We implement the safety filter in two ways, using Llama 2 and DistilBERT, and compare the performance of erase-and-check for the two cases. We defend against three attack modes: i) adversarial suffix, where an adversarial sequence is appended at the end of a harmful prompt; ii) adversarial insertion, where the adversarial sequence is inserted anywhere in the middle of the prompt; and iii) adversarial infusion, where adversarial tokens are inserted at arbitrary positions in the prompt, not necessarily as a contiguous block. Our experimental results demonstrate that this procedure can obtain strong certified safety guarantees on harmful prompts while maintaining good empirical performance on safe prompts. Additionally, we propose three efficient empirical defenses: i) RandEC, a randomized subsampling version of erase-and-check; ii) GreedyEC, which greedily erases tokens that maximize the softmax score of the harmful class; and iii) GradEC, which uses gradient information to optimize tokens to erase. We demonstrate their effectiveness against adversarial prompts generated by the Greedy Coordinate Gradient (GCG) attack algorithm. The code for our experiments is available at https://github.com/aounon/certified-llm-safety.</p>
<p>URLs: <a href="https://github.com/aounon/certified-llm-safety.">https://github.com/aounon/certified-llm-safety.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2309.02705, https://github.com/aounon/certified-llm-safety.', 261)">Copy Link</button>
<div id="copy-message-261" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2310.14483">Chain-of-Factors Paper-Reviewer Matching</a></h1>
<p><b>Authors:</b> Yu Zhang, Yanzhen Shen, SeongKu Kang, Xiusi Chen, Bowen Jin, Jiawei Han</p>
<p>Abstract: With the rapid increase in paper submissions to academic conferences, the need for automated and accurate paper-reviewer matching is more critical than ever. Previous efforts in this area have considered various factors to assess the relevance of a reviewer's expertise to a paper, such as the semantic similarity, shared topics, and citation connections between the paper and the reviewer's previous works. However, most of these studies focus on only one factor, resulting in an incomplete evaluation of the paper-reviewer relevance. To address this issue, we propose a unified model for paper-reviewer matching that jointly considers semantic, topic, and citation factors. To be specific, during training, we instruction-tune a contextualized language model shared across all factors to capture their commonalities and characteristics; during inference, we chain the three factors to enable step-by-step, coarse-to-fine search for qualified reviewers given a submission. Experiments on four datasets (one of which is newly contributed by us) spanning various fields such as machine learning, computer vision, information retrieval, and data mining consistently demonstrate the effectiveness of our proposed Chain-of-Factors model in comparison with state-of-the-art paper-reviewer matching methods and scientific pre-trained language models.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2310.14483', 262)">Copy Link</button>
<div id="copy-message-262" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2311.12864">OptScaler: A Collaborative Framework for Robust Autoscaling in the Cloud</a></h1>
<p><b>Authors:</b> Ding Zou, Wei Lu, Zhibo Zhu, Xingyu Lu, Jun Zhou, Xiaojin Wang, Kangyu Liu, Haiqing Wang, Kefan Wang, Renen Sun</p>
<p>Abstract: Autoscaling is a critical mechanism in cloud computing, enabling the autonomous adjustment of computing resources in response to dynamic workloads. This is particularly valuable for co-located, long-running applications with diverse workload patterns. The primary objective of autoscaling is to regulate resource utilization at a desired level, effectively balancing the need for resource optimization with the fulfillment of Service Level Objectives (SLOs). Many existing proactive autoscaling frameworks may encounter prediction deviations arising from the frequent fluctuations of cloud workloads. Reactive frameworks, on the other hand, rely on realtime system feedback, but their hysteretic nature could lead to violations of stringent SLOs. Hybrid frameworks, while prevalent, often feature independently functioning proactive and reactive modules, potentially leading to incompatibility and undermining the overall decision-making efficacy. In addressing these challenges, we propose OptScaler, a collaborative autoscaling framework that integrates proactive and reactive modules through an optimization module. The proactive module delivers reliable future workload predictions to the optimization module, while the reactive module offers a self-tuning estimator for real-time updates. By embedding a Model Predictive Control (MPC) mechanism and chance constraints into the optimization module, we further enhance its robustness. Numerical results have demonstrated the superiority of our workload prediction model and the collaborative framework, leading to over a 36% reduction in SLO violations compared to prevalent reactive, proactive, or hybrid autoscalers. Notably, OptScaler has been successfully deployed at Alipay, providing autoscaling support for the world-leading payment platform.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2311.12864', 263)">Copy Link</button>
<div id="copy-message-263" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2312.05153">Uncertainty Quantification and Propagation in Surrogate-based Bayesian Inference</a></h1>
<p><b>Authors:</b> Philipp Reiser, Javier Enrique Aguilar, Anneli Guthke, Paul-Christian B\"urkner</p>
<p>Abstract: Surrogate models are statistical or conceptual approximations for more complex simulation models. In this context, it is crucial to propagate the uncertainty induced by limited simulation budget and surrogate approximation error to predictions, inference, and subsequent decision-relevant quantities. However, quantifying and then propagating the uncertainty of surrogates is usually limited to special analytic cases or is otherwise computationally very expensive. In this paper, we propose a framework enabling a scalable, Bayesian approach to surrogate modeling with thorough uncertainty quantification, propagation, and validation. Specifically, we present three methods for Bayesian inference with surrogate models given measurement data. This is a task where the propagation of surrogate uncertainty is especially relevant, because failing to account for it may lead to biased and/or overconfident estimates of the parameters of interest. We showcase our approach in three detailed case studies for linear and nonlinear real-world modeling scenarios. Uncertainty propagation in surrogate models enables more reliable and safe approximation of expensive simulators and will therefore be useful in various fields of applications.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2312.05153', 264)">Copy Link</button>
<div id="copy-message-264" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2401.02501">A Kolmogorov metric embedding for live cell microscopy signaling patterns</a></h1>
<p><b>Authors:</b> Layton Aho, Mark Winter, Marc DeCarlo, Agne Frismantiene, Yannick Blum, Paolo Armando Gagliardi, Olivier Pertz, Andrew R. Cohen</p>
<p>Abstract: We present a metric embedding that captures spatiotemporal patterns of cell signaling dynamics in 5-D $(x,y,z,channel,time)$ live cell microscopy movies. The embedding uses a metric distance called the normalized information distance (NID) based on Kolmogorov complexity theory, an absolute measure of information content between digital objects. The NID uses statistics of lossless compression to compute a theoretically optimal metric distance between pairs of 5-D movies, requiring no a priori knowledge of expected pattern dynamics, and no training data. The cell signaling structure function (SSF) is defined using a class of metric 3-D image filters that compute at each spatiotemporal cell centroid the voxel intensity configuration of the nucleus w.r.t. the surrounding cytoplasm, or a functional output e.g. velocity. The only parameter is the expected cell radii ($\mu m$). The SSF can be optionally combined with segmentation and tracking algorithms. The resulting lossless compression pipeline represents each 5-D input movie as a single point in a metric embedding space. The utility of a metric embedding follows from Euclidean distance between any points in the embedding space approximating optimally the pattern difference, as measured by the NID, between corresponding pairs of 5-D movies. This is true throughout the embedding space, not only at points corresponding to input images. Examples are shown for synthetic data, for 2-D+time movies of ERK and AKT signaling under different oncogenic mutations in human epithelial (MCF10A) cells, for 3-D MCF10A spheroids under optogenetic manipulation of ERK, and for ERK dynamics during colony differentiation in human induced pluripotent stem cells.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2401.02501', 265)">Copy Link</button>
<div id="copy-message-265" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2401.15478">Product Manifold Representations for Learning on Biological Pathways</a></h1>
<p><b>Authors:</b> Daniel McNeela, Frederic Sala, Anthony Gitter</p>
<p>Abstract: Machine learning models that embed graphs in non-Euclidean spaces have shown substantial benefits in a variety of contexts, but their application has not been studied extensively in the biological domain, particularly with respect to biological pathway graphs. Such graphs exhibit a variety of complex network structures, presenting challenges to existing embedding approaches. Learning high-quality embeddings for biological pathway graphs is important for researchers looking to understand the underpinnings of disease and train high-quality predictive models on these networks. In this work, we investigate the effects of embedding pathway graphs in non-Euclidean mixed-curvature spaces and compare against traditional Euclidean graph representation learning models. We then train a supervised model using the learned node embeddings to predict missing protein-protein interactions in pathway graphs. We find large reductions in distortion and boosts on in-distribution edge prediction performance as a result of using mixed-curvature embeddings and their corresponding graph neural network models. However, we find that mixed-curvature representations underperform existing baselines on out-of-distribution edge prediction performance suggesting that these representations may overfit to the training graph topology. We provide our Mixed-Curvature Product Graph Convolutional Network code at https://github.com/mcneela/Mixed-Curvature-GCN and our pathway analysis code at https://github.com/mcneela/Mixed-Curvature-Pathways.</p>
<p>URLs: <a href="https://github.com/mcneela/Mixed-Curvature-GCN">https://github.com/mcneela/Mixed-Curvature-GCN</a>, <a href="https://github.com/mcneela/Mixed-Curvature-Pathways.">https://github.com/mcneela/Mixed-Curvature-Pathways.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2401.15478, https://github.com/mcneela/Mixed-Curvature-GCN, https://github.com/mcneela/Mixed-Curvature-Pathways.', 266)">Copy Link</button>
<div id="copy-message-266" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2402.12062">Causal Equal Protection as Algorithmic Fairness</a></h1>
<p><b>Authors:</b> Marcello Di Bello, Nicol\`o Cangiotti, Michele Loi</p>
<p>Abstract: By combining the philosophical literature on statistical evidence and the interdisciplinary literature on algorithmic fairness, we revisit recent objections against classification parity in light of causal analyses of algorithmic fairness and the distinction between predictive and diagnostic evidence. We focus on trial proceedings as a black-box classification algorithm in which defendants are sorted into two groups by convicting or acquitting them. We defend a novel principle, causal equal protection, that combines classification parity with the causal approach. In the do-calculus, causal equal protection requires that individuals should not be subject to uneven risks of classification error because of their protected or socially salient characteristics. The explicit use of protected characteristics, however, may be required if it equalizes these risks.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2402.12062', 267)">Copy Link</button>
<div id="copy-message-267" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2402.17911">Demonstration of Robust and Efficient Quantum Property Learning with Shallow Shadows</a></h1>
<p><b>Authors:</b> Hong-Ye Hu, Andi Gu, Swarnadeep Majumder, Hang Ren, Yipei Zhang, Derek S. Wang, Yi-Zhuang You, Zlatko Minev, Susanne F. Yelin, Alireza Seif</p>
<p>Abstract: Extracting information efficiently from quantum systems is a major component of quantum information processing tasks. Randomized measurements, or classical shadows, enable predicting many properties of arbitrary quantum states using few measurements. While random single-qubit measurements are experimentally friendly and suitable for learning low-weight Pauli observables, they perform poorly for nonlocal observables. Prepending a shallow random quantum circuit before measurements maintains this experimental friendliness, but also has favorable sample complexities for observables beyond low-weight Paulis, including high-weight Paulis and global low-rank properties such as fidelity. However, in realistic scenarios, quantum noise accumulated with each additional layer of the shallow circuit biases the results. To address these challenges, we propose the \emph{robust shallow shadows protocol}. Our protocol uses Bayesian inference to learn the experimentally relevant noise model and mitigate it in postprocessing. This mitigation introduces a bias-variance trade-off: correcting for noise-induced bias comes at the cost of a larger estimator variance. Despite this increased variance, as we demonstrate on a superconducting quantum processor, our protocol correctly recovers state properties such as expectation values, fidelity, and entanglement entropy, while maintaining a lower sample complexity compared to the random single qubit measurement scheme. We also theoretically analyze the effects of noise on sample complexity and show how the optimal choice of the shallow shadow depth varies with noise strength. This combined theoretical and experimental analysis positions the robust shallow shadow protocol as a scalable, robust, and sample-efficient protocol for characterizing quantum states on current quantum computing platforms.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2402.17911', 268)">Copy Link</button>
<div id="copy-message-268" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2403.00165">TELEClass: Taxonomy Enrichment and LLM-Enhanced Hierarchical Text Classification with Minimal Supervision</a></h1>
<p><b>Authors:</b> Yunyi Zhang, Ruozhen Yang, Xueqiang Xu, Rui Li, Jinfeng Xiao, Jiaming Shen, Jiawei Han</p>
<p>Abstract: Hierarchical text classification aims to categorize each document into a set of classes in a label taxonomy, which is a fundamental web text mining task with broad applications such as web content analysis and semantic indexing. Most earlier works focus on fully or semi-supervised methods that require a large amount of human annotated data which is costly and time-consuming to acquire. To alleviate human efforts, in this paper, we work on hierarchical text classification with a minimal amount of supervision: using the sole class name of each node as the only supervision. Recently, large language models (LLM) have shown competitive performance on various tasks through zero-shot prompting, but this method performs poorly in the hierarchical setting because it is ineffective to include the large and structured label space in a prompt. On the other hand, previous weakly-supervised hierarchical text classification methods only utilize the raw taxonomy skeleton and ignore the rich information hidden in the text corpus that can serve as additional class-indicative features. To tackle the above challenges, we propose TELEClass, Taxonomy Enrichment and LLM-Enhanced weakly-supervised hierarchical text Classification, which combines the general knowledge of LLMs and task-specific features mined from an unlabeled corpus. TELEClass automatically enriches the raw taxonomy with class-indicative features for better label space understanding and utilizes novel LLM-based data annotation and generation methods specifically tailored for the hierarchical setting. Experiments show that TELEClass can significantly outperform previous baselines while achieving comparable performance to zero-shot prompting of LLMs with drastically less inference cost.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2403.00165', 269)">Copy Link</button>
<div id="copy-message-269" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2403.14753">Learning with SASQuaTCh: a Novel Variational Quantum Transformer Architecture with Kernel-Based Self-Attention</a></h1>
<p><b>Authors:</b> Ethan N. Evans, Matthew Cook, Zachary P. Bradshaw, Margarite L. LaBorde</p>
<p>Abstract: The recent exploding growth in size of state-of-the-art machine learning models highlights a well-known issue where exponential parameter growth, which has grown to trillions as in the case of the Generative Pre-trained Transformer (GPT), leads to training time and memory requirements which limit their advancement in the near term. The predominant models use the so-called transformer network and have a large field of applicability, including predicting text and images, classification, and even predicting solutions to the dynamics of physical systems. Here we present a variational quantum circuit architecture named Self-Attention Sequential Quantum Transformer Channel (SASQuaTCh), which builds networks of qubits that perform analogous operations of the transformer network, namely the keystone self-attention operation, and leads to an exponential improvement in parameter complexity and run-time complexity over its classical counterpart. Our approach leverages recent insights from kernel-based operator learning in the context of predicting spatiotemporal systems to represent deep layers of a vision transformer network using simple gate operations and a set of multi-dimensional quantum Fourier transforms. To validate our approach, we consider image classification tasks in simulation and with hardware, where with only 9 qubits and a handful of parameters we are able to simultaneously embed and classify a grayscale image of handwritten digits with high accuracy.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2403.14753', 270)">Copy Link</button>
<div id="copy-message-270" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2405.05202">Discretely Beyond $1/e$: Guided Combinatorial Algorithms for Submodular Maximization</a></h1>
<p><b>Authors:</b> Yixin Chen, Ankur Nath, Chunli Peng, Alan Kuhnle</p>
<p>Abstract: For constrained, not necessarily monotone submodular maximization, all known approximation algorithms with ratio greater than $1/e$ require continuous ideas, such as queries to the multilinear extension of a submodular function and its gradient, which are typically expensive to simulate with the original set function. For combinatorial algorithms, the best known approximation ratios for both size and matroid constraint are obtained by a simple randomized greedy algorithm of Buchbinder et al. [9]: $1/e \approx 0.367$ for size constraint and $0.281$ for the matroid constraint in $\mathcal O (kn)$ queries, where $k$ is the rank of the matroid. In this work, we develop the first combinatorial algorithms to break the $1/e$ barrier: we obtain approximation ratio of $0.385$ in $\mathcal O (kn)$ queries to the submodular set function for size constraint, and $0.305$ for a general matroid constraint. These are achieved by guiding the randomized greedy algorithm with a fast local search algorithm. Further, we develop deterministic versions of these algorithms, maintaining the same ratio and asymptotic time complexity. Finally, we develop a deterministic, nearly linear time algorithm with ratio $0.377$.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2405.05202', 271)">Copy Link</button>
<div id="copy-message-271" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2405.07432">Nonparametric Sparse Online Learning of the Koopman Operator</a></h1>
<p><b>Authors:</b> Boya Hou, Sina Sanjari, Nathan Dahlin, Alec Koppel, Subhonmesh Bose</p>
<p>Abstract: The Koopman operator provides a powerful framework for representing the dynamics of general nonlinear dynamical systems. Data-driven techniques to learn the Koopman operator typically assume that the chosen function space is closed under system dynamics. In this paper, we study the Koopman operator via its action on the reproducing kernel Hilbert space (RKHS), and explore the mis-specified scenario where the dynamics may escape the chosen function space. We relate the Koopman operator to the conditional mean embeddings (CME) operator and then present an operator stochastic approximation algorithm to learn the Koopman operator iteratively with control over the complexity of the representation. We provide both asymptotic and finite-time last-iterate guarantees of the online sparse learning algorithm with trajectory-based sampling with an analysis that is substantially more involved than that for finite-dimensional stochastic approximation. Numerical examples confirm the effectiveness of the proposed algorithm.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2405.07432', 272)">Copy Link</button>
<div id="copy-message-272" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2405.11751">Asymptotic theory of in-context learning by linear attention</a></h1>
<p><b>Authors:</b> Yue M. Lu, Mary I. Letey, Jacob A. Zavatone-Veth, Anindita Maiti, Cengiz Pehlevan</p>
<p>Abstract: Transformers have a remarkable ability to learn and execute tasks based on examples provided within the input itself, without explicit prior training. It has been argued that this capability, known as in-context learning (ICL), is a cornerstone of Transformers' success, yet questions about the necessary sample complexity, pretraining task diversity, and context length for successful ICL remain unresolved. Here, we provide a precise answer to these questions in an exactly solvable model of ICL of a linear regression task by linear attention. We derive sharp asymptotics for the learning curve in a phenomenologically-rich scaling regime where the token dimension is taken to infinity; the context length and pretraining task diversity scale proportionally with the token dimension; and the number of pretraining examples scales quadratically. We demonstrate a double-descent learning curve with increasing pretraining examples, and uncover a phase transition in the model's behavior between low and high task diversity regimes: In the low diversity regime, the model tends toward memorization of training tasks, whereas in the high diversity regime, it achieves genuine in-context learning and generalization beyond the scope of pretrained tasks. These theoretical insights are empirically validated through experiments with both linear attention and full nonlinear Transformer architectures.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2405.11751', 273)">Copy Link</button>
<div id="copy-message-273" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2405.11831">SSAMBA: Self-Supervised Audio Representation Learning with Mamba State Space Model</a></h1>
<p><b>Authors:</b> Siavash Shams, Sukru Samet Dindar, Xilin Jiang, Nima Mesgarani</p>
<p>Abstract: Transformers have revolutionized deep learning across various tasks, including audio representation learning, due to their powerful modeling capabilities. However, they often suffer from quadratic complexity in both GPU memory usage and computational inference time, affecting their efficiency. Recently, state space models (SSMs) like Mamba have emerged as a promising alternative, offering a more efficient approach by avoiding these complexities. Given these advantages, we explore the potential of SSM-based models in audio tasks. In this paper, we introduce Self-Supervised Audio Mamba (SSAMBA), the first self-supervised, attention-free, and SSM-based model for audio representation learning. SSAMBA leverages the bidirectional Mamba to capture complex audio patterns effectively. We incorporate a self-supervised pretraining framework that optimizes both discriminative and generative objectives, enabling the model to learn robust audio representations from large-scale, unlabeled datasets. We evaluated SSAMBA on various tasks such as audio classification, keyword spotting, and speaker identification. Our results demonstrate that SSAMBA outperforms the Self-Supervised Audio Spectrogram Transformer (SSAST) in most tasks. Notably, SSAMBA is approximately 92.7% faster in batch inference speed and 95.4% more memory-efficient than SSAST for the tiny model size with an input token size of 22k. These efficiency gains, combined with superior performance, underscore the effectiveness of SSAMBA's architectural innovation, making it a compelling choice for a wide range of audio processing applications.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2405.11831', 274)">Copy Link</button>
<div id="copy-message-274" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2405.12221">Images that Sound: Composing Images and Sounds on a Single Canvas</a></h1>
<p><b>Authors:</b> Ziyang Chen, Daniel Geng, Andrew Owens</p>
<p>Abstract: Spectrograms are 2D representations of sound that look very different from the images found in our visual world. And natural images, when played as spectrograms, make unnatural sounds. In this paper, we show that it is possible to synthesize spectrograms that simultaneously look like natural images and sound like natural audio. We call these visual spectrograms images that sound. Our approach is simple and zero-shot, and it leverages pre-trained text-to-image and text-to-spectrogram diffusion models that operate in a shared latent space. During the reverse process, we denoise noisy latents with both the audio and image diffusion models in parallel, resulting in a sample that is likely under both models. Through quantitative evaluations and perceptual studies, we find that our method successfully generates spectrograms that align with a desired audio prompt while also taking the visual appearance of a desired image prompt. Please see our project page for video results: https://ificl.github.io/images-that-sound/</p>
<p>URLs: <a href="https://ificl.github.io/images-that-sound/">https://ificl.github.io/images-that-sound/</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2405.12221, https://ificl.github.io/images-that-sound/', 275)">Copy Link</button>
<div id="copy-message-275" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2406.00704">An Optimized Toolbox for Advanced Image Processing with Tsetlin Machine Composites</a></h1>
<p><b>Authors:</b> Ylva Gr{\o}nnings{\ae}ter, Halvor S. Sm{\o}rvik, Ole-Christoffer Granmo</p>
<p>Abstract: The Tsetlin Machine (TM) has achieved competitive results on several image classification benchmarks, including MNIST, K-MNIST, F-MNIST, and CIFAR-2. However, color image classification is arguably still in its infancy for TMs, with CIFAR-10 being a focal point for tracking progress. Over the past few years, TM's CIFAR-10 accuracy has increased from around 61% in 2020 to 75.1% in 2023 with the introduction of Drop Clause. In this paper, we leverage the recently proposed TM Composites architecture and introduce a range of TM Specialists that use various image processing techniques. These include Canny edge detection, Histogram of Oriented Gradients, adaptive mean thresholding, adaptive Gaussian thresholding, Otsu's thresholding, color thermometers, and adaptive color thermometers. In addition, we conduct a rigorous hyperparameter search, where we uncover optimal hyperparameters for several of the TM Specialists. The result is a toolbox that provides new state-of-the-art results on CIFAR-10 for TMs with an accuracy of 82.8%. In conclusion, our toolbox of TM Specialists forms a foundation for new TM applications and a landmark for further research on TM Composites in image analysis.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2406.00704', 276)">Copy Link</button>
<div id="copy-message-276" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2406.09630">Muharaf: Manuscripts of Handwritten Arabic Dataset for Cursive Text Recognition</a></h1>
<p><b>Authors:</b> Mehreen Saeed, Adrian Chan, Anupam Mijar, Joseph Moukarzel, Georges Habchi, Carlos Younes, Amin Elias, Chau-Wai Wong, Akram Khater</p>
<p>Abstract: We present the Manuscripts of Handwritten Arabic~(Muharaf) dataset, which is a machine learning dataset consisting of more than 1,600 historic handwritten page images transcribed by experts in archival Arabic. Each document image is accompanied by spatial polygonal coordinates of its text lines as well as basic page elements. This dataset was compiled to advance the state of the art in handwritten text recognition (HTR), not only for Arabic manuscripts but also for cursive text in general. The Muharaf dataset includes diverse handwriting styles and a wide range of document types, including personal letters, diaries, notes, poems, church records, and legal correspondences. In this paper, we describe the data acquisition pipeline, notable dataset features, and statistics. We also provide a preliminary baseline result achieved by training convolutional neural networks using this data.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2406.09630', 277)">Copy Link</button>
<div id="copy-message-277" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2406.16535">Token-based Decision Criteria Are Suboptimal in In-context Learning</a></h1>
<p><b>Authors:</b> Hakaze Cho, Yoshihiro Sakai, Mariko Kato, Kenshiro Tanaka, Akira Ishii, Naoya Inoue</p>
<p>Abstract: In-Context Learning (ICL) typically utilizes classification criteria from output probabilities of manually selected label tokens. However, we argue that such token-based classification criteria lead to suboptimal decision boundaries, despite delicate calibrations through translation and constrained rotation applied. To address this problem, we propose Hidden Calibration, which renounces token probabilities and uses the nearest centroid classifier on the LM's last hidden states. In detail, we assign the label of the nearest centroid previously estimated from a calibration set to the test sample as the predicted label. Our experiments on 6 models and 10 classification datasets indicate that Hidden Calibration consistently outperforms current token-based baselines by about 20%~50%, achieving a strong state-of-the-art in ICL. Our further analysis demonstrates that Hidden Calibration finds better classification criteria with less inter-class overlap, and LMs provide linearly separable intra-class clusters with the help of demonstrations, which supports Hidden Calibration and gives new insights into the principle of ICL. Our official code implementation can be found at https://github.com/hc495/Hidden_Calibration.</p>
<p>URLs: <a href="https://github.com/hc495/Hidden_Calibration.">https://github.com/hc495/Hidden_Calibration.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2406.16535, https://github.com/hc495/Hidden_Calibration.', 278)">Copy Link</button>
<div id="copy-message-278" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2407.11784">Data-Juicer Sandbox: A Feedback-Driven Suite for Multimodal Data-Model Co-development</a></h1>
<p><b>Authors:</b> Daoyuan Chen, Haibin Wang, Yilun Huang, Ce Ge, Yaliang Li, Bolin Ding, Jingren Zhou</p>
<p>Abstract: The emergence of multimodal large models has advanced artificial intelligence, introducing unprecedented levels of performance and functionality. However, optimizing these models remains challenging due to historically isolated paths of model-centric and data-centric developments, leading to suboptimal outcomes and inefficient resource utilization. In response, we present a new sandbox suite tailored for integrated data-model co-development. This sandbox provides a feedback-driven experimental platform, enabling cost-effective iteration and guided refinement of both data and models. Our proposed ``Probe-Analyze-Refine'' workflow, validated through practical use cases on multimodal tasks such as image-text pre-training with CLIP, image-to-text generation with LLaVA-like models, and text-to-video generation with DiT-based models, yields transferable and notable performance boosts, such as topping the VBench leaderboard. Extensive experiments also uncover fruitful insights into the interplay between data quality, diversity, model behavior, and computational costs. All codes, datasets, and models are open-sourced to foster future research and applications that would otherwise be infeasible due to the lack of a dedicated co-development infrastructure.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2407.11784', 279)">Copy Link</button>
<div id="copy-message-279" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2407.14111">A Mirror Descent-Based Algorithm for Corruption-Tolerant Distributed Gradient Descent</a></h1>
<p><b>Authors:</b> Shuche Wang, Vincent Y. F. Tan</p>
<p>Abstract: Distributed gradient descent algorithms have come to the fore in modern machine learning, especially in parallelizing the handling of large datasets that are distributed across several workers. However, scant attention has been paid to analyzing the behavior of distributed gradient descent algorithms in the presence of adversarial corruptions instead of random noise. In this paper, we formulate a novel problem in which adversarial corruptions are present in a distributed learning system. We show how to use ideas from (lazy) mirror descent to design a corruption-tolerant distributed optimization algorithm. Extensive convergence analysis for (strongly) convex loss functions is provided for different choices of the stepsize. We carefully optimize the stepsize schedule to accelerate the convergence of the algorithm, while at the same time amortizing the effect of the corruption over time. Experiments based on linear regression, support vector classification, and softmax classification on the MNIST dataset corroborate our theoretical findings.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2407.14111', 280)">Copy Link</button>
<div id="copy-message-280" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2407.20959">Learning Ordinality in Semantic Segmentation</a></h1>
<p><b>Authors:</b> Ricardo P. M. Cruz, Rafael Cristino, Jaime S. Cardoso</p>
<p>Abstract: Semantic segmentation consists of predicting a semantic label for each image pixel. While existing deep learning approaches achieve high accuracy, they often overlook the ordinal relationships between classes, which can provide critical domain knowledge (e.g., the pupil lies within the iris, and lane markings are part of the road). This paper introduces novel methods for spatial ordinal segmentation that explicitly incorporate these inter-class dependencies. By treating each pixel as part of a structured image space rather than as an independent observation, we propose two regularization terms and a new metric to enforce ordinal consistency between neighboring pixels. Two loss regularization terms and one metric are proposed for structural ordinal segmentation, which penalizes predictions of non-ordinal adjacent classes. Five biomedical datasets and multiple configurations of autonomous driving datasets demonstrate the efficacy of the proposed methods. Our approach achieves improvements in ordinal metrics and enhances generalization, with up to a 15.7% relative increase in the Dice coefficient. Importantly, these benefits come without additional inference time costs. This work highlights the significance of spatial ordinal relationships in semantic segmentation and provides a foundation for further exploration in structured image representations.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2407.20959', 281)">Copy Link</button>
<div id="copy-message-281" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2408.05534">Can LLMs Replace Manual Annotation of Software Engineering Artifacts?</a></h1>
<p><b>Authors:</b> Toufique Ahmed, Premkumar Devanbu, Christoph Treude, Michael Pradel</p>
<p>Abstract: Experimental evaluations of software engineering innovations, e.g., tools and processes, often include human-subject studies as a component of a multi-pronged strategy to obtain greater generalizability of the findings. However, human-subject studies in our field are challenging, due to the cost and difficulty of finding and employing suitable subjects, ideally, professional programmers with varying degrees of experience. Meanwhile, large language models (LLMs) have recently started to demonstrate human-level performance in several areas. This paper explores the possibility of substituting costly human subjects with much cheaper LLM queries in evaluations of code and code-related artifacts. We study this idea by applying six state-of-the-art LLMs to ten annotation tasks from five datasets created by prior work, such as judging the accuracy of a natural language summary of a method or deciding whether a code change fixes a static analysis warning. Our results show that replacing some human annotation effort with LLMs can produce inter-rater agreements equal or close to human-rater agreement. To help decide when and how to use LLMs in human-subject studies, we propose model-model agreement as a predictor of whether a given task is suitable for LLMs at all, and model confidence as a means to select specific samples where LLMs can safely replace human annotators. Overall, our work is the first step toward mixed human-LLM evaluations in software engineering.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2408.05534', 282)">Copy Link</button>
<div id="copy-message-282" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2408.15408">A physics-encoded Fourier neural operator approach for surrogate modeling of divergence-free stress fields in solids</a></h1>
<p><b>Authors:</b> Mohammad S. Khorrami, Pawan Goyal, Jaber R. Mianroodi, Bob Svendsen, Peter Benner, Dierk Raabe</p>
<p>Abstract: The purpose of the current work is the development of a so-called physics-encoded Fourier neural operator (PeFNO) for surrogate modeling of the quasi-static equilibrium stress field in solids. Rather than accounting for constraints from physics in the loss function as done in the (now standard) physics-informed approach, the physics-encoded approach incorporates or "encodes" such constraints directly into the network or operator architecture. As a result, in contrast to the physics-informed approach in which only training is physically constrained, both training and output are physically constrained in the physics-encoded approach. For the current constraint of divergence-free stress, a novel encoding approach based on a stress potential is proposed.
  As a "proof-of-concept" example application of the proposed PeFNO, a heterogeneous polycrystalline material consisting of isotropic elastic grains subject to uniaxial extension is considered. Stress field data for training are obtained from the numerical solution of a corresponding boundary-value problem for quasi-static mechanical equilibrium. This data is also employed to train an analogous physics-guided FNO (PgFNO) and physics-informed FNO (PiFNO) for comparison. As confirmed by this comparison and as expected on the basis of their differences, the output of the trained PeFNO is significantly more accurate in satisfying mechanical equilibrium than the output of either the trained PgFNO or the trained PiFNO.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2408.15408', 283)">Copy Link</button>
<div id="copy-message-283" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2409.07510">Still More Shades of Null: An Evaluation Suite for Responsible Missing Value Imputation</a></h1>
<p><b>Authors:</b> Falaah Arif Khan, Denys Herasymuk, Nazar Protsiv, Julia Stoyanovich</p>
<p>Abstract: Data missingness is a practical challenge of sustained interest to the scientific community. In this paper, we present Shades-of-NULL, an evaluation suite for responsible missing value imputation. Our work is novel in two ways (i) we model realistic and socially-salient missingness scenarios that go beyond Rubin's classic Missing Completely at Random (MCAR), Missing At Random (MAR) and Missing Not At Random (MNAR) settings, to include multi-mechanism missingness (when different missingness patterns co-exist in the data) and missingness shift (when the missingness mechanism changes between training and test) (ii) we evaluate imputers holistically, based on imputation quality and imputation fairness, as well as on the predictive performance, fairness and stability of the models that are trained and tested on the data post-imputation. We use Shades-of-NULL to conduct a large-scale empirical study involving 29,736 experimental pipelines, and find that while there is no single best-performing imputation approach for all missingness types, interesting trade-offs arise between predictive performance, fairness and stability, based on the combination of missingness scenario, imputer choice, and the architecture of the predictive model. We make Shades-of-NULL publicly available, to enable researchers to rigorously evaluate missing value imputation methods on a wide range of metrics in plausible and socially meaningful scenarios.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2409.07510', 284)">Copy Link</button>
<div id="copy-message-284" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2409.14557">Exploiting Exogenous Structure for Sample-Efficient Reinforcement Learning</a></h1>
<p><b>Authors:</b> Jia Wan, Sean R. Sinclair, Devavrat Shah, Martin J. Wainwright</p>
<p>Abstract: We study Exo-MDPs, a structured class of Markov Decision Processes (MDPs) where the state space is partitioned into exogenous and endogenous components. Exogenous states evolve stochastically, independent of the agent's actions, while endogenous states evolve deterministically based on both state components and actions. Exo-MDPs are useful for applications including inventory control, portfolio management, and ride-sharing. Our first result is structural, establishing a representational equivalence between the classes of discrete MDPs, Exo-MDPs, and discrete linear mixture MDPs. Specifically, any discrete MDP can be represented as an Exo-MDP, and the transition and reward dynamics can be written as linear functions of the exogenous state distribution, showing that Exo-MDPs are instances of linear mixture MDPs. For unobserved exogenous states, we prove a regret upper bound of $O(H^{3/2}d\sqrt{K})$ over $K$ trajectories of horizon $H$, with $d$ as the size of the exogenous state space, and establish nearly-matching lower bounds. Our findings demonstrate how Exo-MDPs decouple sample complexity from action and endogenous state sizes, and we validate our theoretical insights with experiments on inventory control.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2409.14557', 285)">Copy Link</button>
<div id="copy-message-285" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2409.16295">Efficient Training of Self-Supervised Speech Foundation Models on a Compute Budget</a></h1>
<p><b>Authors:</b> Andy T. Liu, Yi-Cheng Lin, Haibin Wu, Stefan Winkler, Hung-yi Lee</p>
<p>Abstract: Despite their impressive success, training foundation models remains computationally costly. This paper investigates how to efficiently train speech foundation models with self-supervised learning (SSL) under a limited compute budget. We examine critical factors in SSL that impact the budget, including model architecture, model size, and data size. Our goal is to make analytical steps toward understanding the training dynamics of speech foundation models. We benchmark SSL objectives in an entirely comparable setting and find that other factors contribute more significantly to the success of SSL. Our results show that slimmer model architectures outperform common small architectures under the same compute and parameter budget. We demonstrate that the size of the pre-training data remains crucial, even with data augmentation during SSL training, as performance suffers when iterating over limited data. Finally, we identify a trade-off between model size and data size, highlighting an optimal model size for a given compute budget.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2409.16295', 286)">Copy Link</button>
<div id="copy-message-286" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2410.01506">Learnable Expansion of Graph Operators for Multi-Modal Feature Fusion</a></h1>
<p><b>Authors:</b> Dexuan Ding, Lei Wang, Liyun Zhu, Tom Gedeon, Piotr Koniusz</p>
<p>Abstract: In computer vision tasks, features often come from diverse representations, domains (e.g., indoor and outdoor), and modalities (e.g., text, images, and videos). Effectively fusing these features is essential for robust performance, especially with the availability of powerful pre-trained models like vision-language models. However, common fusion methods, such as concatenation, element-wise operations, and non-linear techniques, often fail to capture structural relationships, deep feature interactions, and suffer from inefficiency or misalignment of features across domains or modalities. In this paper, we shift from high-dimensional feature space to a lower-dimensional, interpretable graph space by constructing relationship graphs that encode feature relationships at different levels, e.g., clip, frame, patch, token, etc. To capture deeper interactions, we use graph power expansions and introduce a learnable graph fusion operator to combine these graph powers for more effective fusion. Our approach is relationship-centric, operates in a homogeneous space, and is mathematically principled, resembling element-wise relationship score aggregation via multilinear polynomials. We demonstrate the effectiveness of our graph-based fusion method on video anomaly detection, showing strong performance across multi-representational, multi-modal, and multi-domain feature fusion tasks.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2410.01506', 287)">Copy Link</button>
<div id="copy-message-287" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2410.13284">Learning to Route LLMs with Confidence Tokens</a></h1>
<p><b>Authors:</b> Yu-Neng Chuang, Helen Zhou, Prathusha Kameswara Sarma, Parikshit Gopalan, John Boccio, Sara Bolouki, Xia Hu</p>
<p>Abstract: Large language models (LLMs) have demonstrated impressive performance on several tasks and are increasingly deployed in real-world applications. However, especially in high-stakes settings, it becomes vital to know when the output of an LLM may be unreliable. Depending on whether an answer is trustworthy, a system can then choose to route the question to another expert, or otherwise fall back on a safe default behavior. In this work, we study the extent to which LLMs can reliably indicate confidence in their answers, and how this notion of confidence can translate into downstream accuracy gains. We propose Self-REF, a lightweight training strategy to teach LLMs to express confidence in whether their answers are correct in a reliable manner. Self-REF introduces confidence tokens into the LLM, from which a confidence score can be extracted. Compared to conventional approaches such as verbalizing confidence and examining token probabilities, we demonstrate empirically that confidence tokens show significant improvements in downstream routing and rejection learning tasks.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2410.13284', 288)">Copy Link</button>
<div id="copy-message-288" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2410.14159">Assessing Open-world Forgetting in Generative Image Model Customization</a></h1>
<p><b>Authors:</b> H\'ector Laria, Alex Gomez-Villa, Kai Wang, Bogdan Raducanu, Joost van de Weijer</p>
<p>Abstract: Recent advances in diffusion models have significantly enhanced image generation capabilities. However, customizing these models with new classes often leads to unintended consequences that compromise their reliability. We introduce the concept of open-world forgetting to characterize the vast scope of these unintended alterations. Our work presents the first systematic investigation into open-world forgetting in diffusion models, focusing on semantic and appearance drift of representations. Using zero-shot classification, we demonstrate that even minor model adaptations can lead to significant semantic drift affecting areas far beyond newly introduced concepts, with accuracy drops of up to 60% on previously learned concepts. Our analysis of appearance drift reveals substantial changes in texture and color distributions of generated content. To address these issues, we propose a functional regularization strategy that effectively preserves original capabilities while accommodating new concepts. Through extensive experiments across multiple datasets and evaluation metrics, we demonstrate that our approach significantly reduces both semantic and appearance drift. Our study highlights the importance of considering open-world forgetting in future research on model customization and finetuning methods.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2410.14159', 289)">Copy Link</button>
<div id="copy-message-289" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2410.20724">Simple Is Effective: The Roles of Graphs and Large Language Models in Knowledge-Graph-Based Retrieval-Augmented Generation</a></h1>
<p><b>Authors:</b> Mufei Li, Siqi Miao, Pan Li</p>
<p>Abstract: Large Language Models (LLMs) demonstrate strong reasoning abilities but face limitations such as hallucinations and outdated knowledge. Knowledge Graph (KG)-based Retrieval-Augmented Generation (RAG) addresses these issues by grounding LLM outputs in structured external knowledge from KGs. However, current KG-based RAG frameworks still struggle to optimize the trade-off between retrieval effectiveness and efficiency in identifying a suitable amount of relevant graph information for the LLM to digest. We introduce SubgraphRAG, extending the KG-based RAG framework that retrieves subgraphs and leverages LLMs for reasoning and answer prediction. Our approach innovatively integrates a lightweight multilayer perceptron with a parallel triple-scoring mechanism for efficient and flexible subgraph retrieval while encoding directional structural distances to enhance retrieval effectiveness. The size of retrieved subgraphs can be flexibly adjusted to match the query's need and the downstream LLM's capabilities. This design strikes a balance between model complexity and reasoning power, enabling scalable and generalizable retrieval processes. Notably, based on our retrieved subgraphs, smaller LLMs like Llama3.1-8B-Instruct deliver competitive results with explainable reasoning, while larger models like GPT-4o achieve state-of-the-art accuracy compared with previous baselines -- all without fine-tuning. Extensive evaluations on the WebQSP and CWQ benchmarks highlight SubgraphRAG's strengths in efficiency, accuracy, and reliability by reducing hallucinations and improving response grounding.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2410.20724', 290)">Copy Link</button>
<div id="copy-message-290" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2411.04586">On the Inherent Robustness of One-Stage Object Detection against Out-of-Distribution Data</a></h1>
<p><b>Authors:</b> Aitor Martinez-Seras, Javier Del Ser, Aitzol Olivares-Rad, Alain Andres, Pablo Garcia-Bringas</p>
<p>Abstract: Robustness is a fundamental aspect for developing safe and trustworthy models, particularly when they are deployed in the open world. In this work we analyze the inherent capability of one-stage object detectors to robustly operate in the presence of out-of-distribution (OoD) data. Specifically, we propose a novel detection algorithm for detecting unknown objects in image data, which leverages the features extracted by the model from each sample. Differently from other recent approaches in the literature, our proposal does not require retraining the object detector, thereby allowing for the use of pretrained models. Our proposed OoD detector exploits the application of supervised dimensionality reduction techniques to mitigate the effects of the curse of dimensionality on the features extracted by the model. Furthermore, it utilizes high-resolution feature maps to identify potential unknown objects in an unsupervised fashion. Our experiments analyze the Pareto trade-off between the performance detecting known and unknown objects resulting from different algorithmic configurations and inference confidence thresholds. We also compare the performance of our proposed algorithm to that of logits-based post-hoc OoD methods, as well as possible fusion strategies. Finally, we discuss on the competitiveness of all tested methods against state-of-the-art OoD approaches for object detection models over the recently published Unknown Object Detection benchmark. The obtained results verify that the performance of avant-garde post-hoc OoD detectors can be further improved when combined with our proposed algorithm.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2411.04586', 291)">Copy Link</button>
<div id="copy-message-291" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2411.07463">MSEG-VCUQ: Multimodal SEGmentation with Enhanced Vision Foundation Models, Convolutional Neural Networks, and Uncertainty Quantification for High-Speed Video Phase Detection Data</a></h1>
<p><b>Authors:</b> Chika Maduabuchi, Ericmoore Jossou, Matteo Bucci</p>
<p>Abstract: High-speed video (HSV) phase detection (PD) segmentation is crucial for monitoring vapor, liquid, and microlayer phases in industrial processes. While CNN-based models like U-Net have shown success in simplified shadowgraphy-based two-phase flow (TPF) analysis, their application to complex HSV PD tasks remains unexplored, and vision foundation models (VFMs) have yet to address the complexities of either shadowgraphy-based or PD TPF video segmentation. Existing uncertainty quantification (UQ) methods lack pixel-level reliability for critical metrics like contact line density and dry area fraction, and the absence of large-scale, multimodal experimental datasets tailored to PD segmentation further impedes progress. To address these gaps, we propose MSEG-VCUQ. This hybrid framework integrates U-Net CNNs with the transformer-based Segment Anything Model (SAM) to achieve enhanced segmentation accuracy and cross-modality generalization. Our approach incorporates systematic UQ for robust error assessment and introduces the first open-source multimodal HSV PD datasets. Empirical results demonstrate that MSEG-VCUQ outperforms baseline CNNs and VFMs, enabling scalable and reliable PD segmentation for real-world boiling dynamics.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2411.07463', 292)">Copy Link</button>
<div id="copy-message-292" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2411.16598">DiffBreak: Breaking Diffusion-Based Purification with Adaptive Attacks</a></h1>
<p><b>Authors:</b> Andre Kassis, Urs Hengartner, Yaoliang Yu</p>
<p>Abstract: Diffusion-based purification (DBP) has emerged as a cornerstone defense against adversarial examples (AEs), widely regarded as robust due to its use of diffusion models (DMs) that project AEs onto the natural data distribution. However, contrary to prior assumptions, we theoretically prove that adaptive gradient-based attacks nullify this foundational claim, effectively targeting the DM rather than the classifier and causing purified outputs to align with adversarial distributions. This surprising discovery prompts a reassessment of DBP's robustness, revealing it stems from critical flaws in backpropagation techniques used so far for attacking DBP. To address these gaps, we introduce DiffBreak, a novel and reliable gradient library for DBP, which exposes how adaptive attacks drastically degrade its robustness. In stricter majority-vote settings, where classifier decisions aggregate predictions over multiple purified inputs, DBP retains partial robustness to traditional norm-bounded AEs due to its stochasticity disrupting adversarial alignment. However, we propose a novel adaptation of a recent optimization method against deepfake watermarking, crafting systemic adversarial perturbations that defeat DBP even under these conditions, ultimately challenging its viability as a defense without improvements.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2411.16598', 293)">Copy Link</button>
<div id="copy-message-293" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2412.02403">3D Face Reconstruction From Radar Images</a></h1>
<p><b>Authors:</b> Valentin Braeutigam, Vanessa Wirth, Ingrid Ullmann, Christian Sch\"u{\ss}ler, Martin Vossiek, Matthias Berking, Bernhard Egger</p>
<p>Abstract: The 3D reconstruction of faces gains wide attention in computer vision and is used in many fields of application, for example, animation, virtual reality, and even forensics. This work is motivated by monitoring patients in sleep laboratories. Due to their unique characteristics, sensors from the radar domain have advantages compared to optical sensors, namely penetration of electrically non-conductive materials and independence of light. These advantages of radar signals unlock new applications and require adaptation of 3D reconstruction frameworks. We propose a novel model-based method for 3D reconstruction from radar images. We generate a dataset of synthetic radar images with a physics-based but non-differentiable radar renderer. This dataset is used to train a CNN-based encoder to estimate the parameters of a 3D morphable face model. Whilst the encoder alone already leads to strong reconstructions of synthetic data, we extend our reconstruction in an Analysis-by-Synthesis fashion to a model-based autoencoder. This is enabled by learning the rendering process in the decoder, which acts as an object-specific differentiable radar renderer. Subsequently, the combination of both network parts is trained to minimize both, the loss of the parameters and the loss of the resulting reconstructed radar image. This leads to the additional benefit, that at test time the parameters can be further optimized by finetuning the autoencoder unsupervised on the image loss. We evaluated our framework on generated synthetic face images as well as on real radar images with 3D ground truth of four individuals.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2412.02403', 294)">Copy Link</button>
<div id="copy-message-294" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2412.02529">Active learning of neural population dynamics using two-photon holographic optogenetics</a></h1>
<p><b>Authors:</b> Andrew Wagenmaker, Lu Mi, Marton Rozsa, Matthew S. Bull, Karel Svoboda, Kayvon Daie, Matthew D. Golub, Kevin Jamieson</p>
<p>Abstract: Recent advances in techniques for monitoring and perturbing neural populations have greatly enhanced our ability to study circuits in the brain. In particular, two-photon holographic optogenetics now enables precise photostimulation of experimenter-specified groups of individual neurons, while simultaneous two-photon calcium imaging enables the measurement of ongoing and induced activity across the neural population. Despite the enormous space of potential photostimulation patterns and the time-consuming nature of photostimulation experiments, very little algorithmic work has been done to determine the most effective photostimulation patterns for identifying the neural population dynamics. Here, we develop methods to efficiently select which neurons to stimulate such that the resulting neural responses will best inform a dynamical model of the neural population activity. Using neural population responses to photostimulation in mouse motor cortex, we demonstrate the efficacy of a low-rank linear dynamical systems model, and develop an active learning procedure which takes advantage of low-rank structure to determine informative photostimulation patterns. We demonstrate our approach on both real and synthetic data, obtaining in some cases as much as a two-fold reduction in the amount of data required to reach a given predictive power. Our active stimulation design method is based on a novel active learning procedure for low-rank regression, which may be of independent interest.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2412.02529', 295)">Copy Link</button>
<div id="copy-message-295" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2412.15188">LMFusion: Adapting Pretrained Language Models for Multimodal Generation</a></h1>
<p><b>Authors:</b> Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, Lili Yu</p>
<p>Abstract: We present LMFusion, a framework for empowering pretrained text-only large language models (LLMs) with multimodal generative capabilities, enabling them to understand and generate both text and images in arbitrary sequences. LMFusion leverages existing Llama-3's weights for processing texts autoregressively while introducing additional and parallel transformer modules for processing images with diffusion. During training, the data from each modality is routed to its dedicated modules: modality-specific feedforward layers, query-key-value projections, and normalization layers process each modality independently, while the shared self-attention layers allow interactions across text and image features. By freezing the text-specific modules and only training the image-specific modules, LMFusion preserves the language capabilities of text-only LLMs while developing strong visual understanding and generation abilities. Compared to methods that pretrain multimodal generative models from scratch, our experiments demonstrate that, LMFusion improves image understanding by 20% and image generation by 3.6% using only 50% of the FLOPs while maintaining Llama-3's language capabilities. We also demonstrate that this framework can adapt existing vision-language models with multimodal generation ability. Overall, this framework not only leverages existing computational investments in text-only LLMs but also enables the parallel development of language and vision capabilities, presenting a promising direction for efficient multimodal model development.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2412.15188', 296)">Copy Link</button>
<div id="copy-message-296" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2412.17920">Causal Composition Diffusion Model for Closed-loop Traffic Generation</a></h1>
<p><b>Authors:</b> Haohong Lin, Xin Huang, Tung Phan-Minh, David S. Hayden, Huan Zhang, Ding Zhao, Siddhartha Srinivasa, Eric M. Wolff, Hongge Chen</p>
<p>Abstract: Simulation is critical for safety evaluation in autonomous driving, particularly in capturing complex interactive behaviors. However, generating realistic and controllable traffic scenarios in long-tail situations remains a significant challenge. Existing generative models suffer from the conflicting objective between user-defined controllability and realism constraints, which is amplified in safety-critical contexts. In this work, we introduce the Causal Compositional Diffusion Model (CCDiff), a structure-guided diffusion framework to address these challenges. We first formulate the learning of controllable and realistic closed-loop simulation as a constrained optimization problem. Then, CCDiff maximizes controllability while adhering to realism by automatically identifying and injecting causal structures directly into the diffusion process, providing structured guidance to enhance both realism and controllability. Through rigorous evaluations on benchmark datasets and in a closed-loop simulator, CCDiff demonstrates substantial gains over state-of-the-art approaches in generating realistic and user-preferred trajectories. Our results show CCDiff's effectiveness in extracting and leveraging causal structures, showing improved closed-loop performance based on key metrics such as collision rate, off-road rate, FDE, and comfort.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2412.17920', 297)">Copy Link</button>
<div id="copy-message-297" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2501.04871">RieszBoost: Gradient Boosting for Riesz Regression</a></h1>
<p><b>Authors:</b> Kaitlyn J. Lee, Alejandro Schuler</p>
<p>Abstract: Answering causal questions often involves estimating linear functionals of conditional expectations, such as the average treatment effect or the effect of a longitudinal modified treatment policy. By the Riesz representation theorem, these functionals can be expressed as the expected product of the conditional expectation of the outcome and the Riesz representer, a key component in doubly robust estimation methods. Traditionally, the Riesz representer is estimated indirectly by deriving its explicit analytical form, estimating its components, and substituting these estimates into the known form (e.g., the inverse propensity score). However, deriving or estimating the analytical form can be challenging, and substitution methods are often sensitive to practical positivity violations, leading to higher variance and wider confidence intervals. In this paper, we propose a novel gradient boosting algorithm to directly estimate the Riesz representer without requiring its explicit analytical form. This method is particularly suited for tabular data, offering a flexible, nonparametric, and computationally efficient alternative to existing methods for Riesz regression. Through simulation studies, we demonstrate that our algorithm performs on par with or better than indirect estimation techniques across a range of functionals, providing a user-friendly and robust solution for estimating causal quantities.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2501.04871', 298)">Copy Link</button>
<div id="copy-message-298" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2501.09683">Rough kernel hedging</a></h1>
<p><b>Authors:</b> Nicola Muca Cirone, Cristopher Salvi</p>
<p>Abstract: Building on the functional-analytic framework of operator-valued kernels and un-truncated signature kernels, we propose a scalable, provably convergent signature-based algorithm for a broad class of high-dimensional, path-dependent hedging problems. We make minimal assumptions about market dynamics by modelling them as general geometric rough paths, yielding a fully model-free approach. Furthermore, through a representer theorem, we provide theoretical guarantees on the existence and uniqueness of a global minimum for the resulting optimization problem and derive an analytic solution under highly general loss functions. Similar to the popular deep hedging approach, but in a more rigorous fashion, our method can also incorporate additional features via the underlying operator-valued kernel, such as trading signals, news analytics, and past hedging decisions, closely aligning with true machine-learning practice.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2501.09683', 299)">Copy Link</button>
<div id="copy-message-299" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2501.11280">Empirical Bayes Estimation for Lasso-Type Regularizers: Analysis of Automatic Relevance Determination</a></h1>
<p><b>Authors:</b> Tsukasa Yoshida, Kazuho Watanabe</p>
<p>Abstract: This paper focuses on linear regression models with non-conjugate sparsity-inducing regularizers such as lasso and group lasso. Although empirical Bayes approach enables us to estimate the regularization parameter, little is known on the properties of the estimators. In particular, there are many unexplained aspects regarding the specific conditions under which the mechanism of automatic relevance determination (ARD) occurs. In this paper, we derive the empirical Bayes estimators for the group lasso regularized linear regression models with a limited number of parameters. It is shown that the estimators diverge under a certain condition, giving rise to the ARD mechanism. We also prove that empirical Bayes methods can produce ARD mechanism in general regularized linear regression models and clarify the conditions under which models such as ridge, lasso, and group lasso can produce ARD mechanism.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2501.11280', 300)">Copy Link</button>
<div id="copy-message-300" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2501.12359">Measured Hockey-Stick Divergence and its Applications to Quantum Pufferfish Privacy</a></h1>
<p><b>Authors:</b> Theshani Nuradha, Vishal Singh, Mark M. Wilde</p>
<p>Abstract: The hockey-stick divergence is a fundamental quantity characterizing several statistical privacy frameworks that ensure privacy for classical and quantum data. In such quantum privacy frameworks, the adversary is allowed to perform all possible measurements. However, in practice, there are typically limitations to the set of measurements that can be performed. To this end, here, we comprehensively analyze the measured hockey-stick divergence under several classes of practically relevant measurement classes. We prove several of its properties, including data processing and convexity. We show that it is efficiently computable by semi-definite programming for some classes of measurements and can be analytically evaluated for Werner and isotropic states. Notably, we show that the measured hockey-stick divergence characterizes optimal privacy parameters in the quantum pufferfish privacy framework. With this connection and the developed technical tools, we enable methods to quantify and audit privacy for several practically relevant settings. Lastly, we introduce the measured hockey-stick divergence of channels and explore its applications in ensuring privacy for channels.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2501.12359', 301)">Copy Link</button>
<div id="copy-message-301" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2501.13397">ExLM: Rethinking the Impact of [MASK] Tokens in Masked Language Models</a></h1>
<p><b>Authors:</b> Kangjie Zheng, Junwei Yang, Siyue Liang, Bin Feng, Zequn Liu, Wei Ju, Zhiping Xiao, Ming Zhang</p>
<p>Abstract: Masked Language Models (MLMs) have achieved remarkable success in many self-supervised representation learning tasks. MLMs are trained by randomly masking portions of the input sequences with [MASK] tokens and learning to reconstruct the original content based on the remaining context. This paper explores the impact of [MASK] tokens on MLMs. Analytical studies show that masking tokens can introduce the corrupted semantics problem, wherein the corrupted context may convey multiple, ambiguous meanings. This problem is also a key factor affecting the performance of MLMs on downstream tasks. Based on these findings, we propose a novel enhanced-context MLM, ExLM. Our approach expands [MASK] tokens in the input context and models the dependencies between these expanded states. This enhancement increases context capacity and enables the model to capture richer semantic information, effectively mitigating the corrupted semantics problem during pre-training. Experimental results demonstrate that ExLM achieves significant performance improvements in both text modeling and SMILES modeling tasks. Further analysis confirms that ExLM enriches semantic representations through context enhancement, and effectively reduces the semantic multimodality commonly observed in MLMs.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2501.13397', 302)">Copy Link</button>
<div id="copy-message-302" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2501.13554">One-Prompt-One-Story: Free-Lunch Consistent Text-to-Image Generation Using a Single Prompt</a></h1>
<p><b>Authors:</b> Tao Liu, Kai Wang, Senmao Li, Joost van de Weijer, Fahad Shahbaz Khan, Shiqi Yang, Yaxing Wang, Jian Yang, Ming-Ming Cheng</p>
<p>Abstract: Text-to-image generation models can create high-quality images from input prompts. However, they struggle to support the consistent generation of identity-preserving requirements for storytelling. Existing approaches to this problem typically require extensive training in large datasets or additional modifications to the original model architectures. This limits their applicability across different domains and diverse diffusion model configurations. In this paper, we first observe the inherent capability of language models, coined context consistency, to comprehend identity through context with a single prompt. Drawing inspiration from the inherent context consistency, we propose a novel training-free method for consistent text-to-image (T2I) generation, termed "One-Prompt-One-Story" (1Prompt1Story). Our approach 1Prompt1Story concatenates all prompts into a single input for T2I diffusion models, initially preserving character identities. We then refine the generation process using two novel techniques: Singular-Value Reweighting and Identity-Preserving Cross-Attention, ensuring better alignment with the input description for each frame. In our experiments, we compare our method against various existing consistent T2I generation approaches to demonstrate its effectiveness through quantitative metrics and qualitative assessments. Code is available at https://github.com/byliutao/1Prompt1Story.</p>
<p>URLs: <a href="https://github.com/byliutao/1Prompt1Story.">https://github.com/byliutao/1Prompt1Story.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2501.13554, https://github.com/byliutao/1Prompt1Story.', 303)">Copy Link</button>
<div id="copy-message-303" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2501.14070">Expanding on the BRIAR Dataset: A Comprehensive Whole Body Biometric Recognition Resource at Extreme Distances and Real-World Scenarios (Collections 1-4)</a></h1>
<p><b>Authors:</b> Gavin Jager, David Cornett III, Gavin Glenn, Deniz Aykac, Christi Johnson, Robert Zhang, Ryan Shivers, David Bolme, Laura Davies, Scott Dolvin, Nell Barber, Joel Brogan, Nick Burchfield, Carl Dukes, Andrew Duncan, Regina Ferrell, Austin Garrett, Jim Goddard, Jairus Hines, Bart Murphy, Sean Pharris, Brandon Stockwell, Leanne Thompson, Matthew Yohe</p>
<p>Abstract: The state-of-the-art in biometric recognition algorithms and operational systems has advanced quickly in recent years providing high accuracy and robustness in more challenging collection environments and consumer applications. However, the technology still suffers greatly when applied to non-conventional settings such as those seen when performing identification at extreme distances or from elevated cameras on buildings or mounted to UAVs. This paper summarizes an extension to the largest dataset currently focused on addressing these operational challenges, and describes its composition as well as methodologies of collection, curation, and annotation.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2501.14070', 304)">Copy Link</button>
<div id="copy-message-304" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2501.14739">Reproduction Research of FSA-Benchmark</a></h1>
<p><b>Authors:</b> Joshua Ludolf, Yesmin Reyna-Hernandez, Matthew Trevino</p>
<p>Abstract: In the current landscape of big data, the reliability and performance of storage systems are essential to the success of various applications and services. as data volumes continue to grow exponentially, the complexity and scale of the storage infrastructures needed to manage this data also increase. a significant challenge faced by data centers and storage systems is the detection and management of fail-slow disks that experience a gradual decline in performance before ultimately failing. Unlike outright disk failures, fail-slow conditions can go undetected for prolonged periods, leading to considerable impacts on system performance and user experience.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2501.14739', 305)">Copy Link</button>
<div id="copy-message-305" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2501.14744">FSTA-SNN:Frequency-based Spatial-Temporal Attention Module for Spiking Neural Networks</a></h1>
<p><b>Authors:</b> Kairong Yu, Tianqing Zhang, Hongwei Wang, Qi Xu</p>
<p>Abstract: Spiking Neural Networks (SNNs) are emerging as a promising alternative to Artificial Neural Networks (ANNs) due to their inherent energy efficiency. Owing to the inherent sparsity in spike generation within SNNs, the in-depth analysis and optimization of intermediate output spikes are often neglected. This oversight significantly restricts the inherent energy efficiency of SNNs and diminishes their advantages in spatiotemporal feature extraction, resulting in a lack of accuracy and unnecessary energy expenditure. In this work, we analyze the inherent spiking characteristics of SNNs from both temporal and spatial perspectives. In terms of spatial analysis, we find that shallow layers tend to focus on learning vertical variations, while deeper layers gradually learn horizontal variations of features. Regarding temporal analysis, we observe that there is not a significant difference in feature learning across different time steps. This suggests that increasing the time steps has limited effect on feature learning. Based on the insights derived from these analyses, we propose a Frequency-based Spatial-Temporal Attention (FSTA) module to enhance feature learning in SNNs. This module aims to improve the feature learning capabilities by suppressing redundant spike features.The experimental results indicate that the introduction of the FSTA module significantly reduces the spike firing rate of SNNs, demonstrating superior performance compared to state-of-the-art baselines across multiple datasets.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2501.14744', 306)">Copy Link</button>
<div id="copy-message-306" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2501.15753">Scale-Insensitive Neural Network Significance Tests</a></h1>
<p><b>Authors:</b> Hasan Fallahgoul</p>
<p>Abstract: This paper develops a scale-insensitive framework for neural network significance testing, substantially generalizing existing approaches through three key innovations. First, we replace metric entropy calculations with Rademacher complexity bounds, enabling the analysis of neural networks without requiring bounded weights or specific architectural constraints. Second, we weaken the regularity conditions on the target function to require only Sobolev space membership $H^s([-1,1]^d)$ with $s > d/2$, significantly relaxing previous smoothness assumptions while maintaining optimal approximation rates. Third, we introduce a modified sieve space construction based on moment bounds rather than weight constraints, providing a more natural theoretical framework for modern deep learning practices. Our approach achieves these generalizations while preserving optimal convergence rates and establishing valid asymptotic distributions for test statistics. The technical foundation combines localization theory, sharp concentration inequalities, and scale-insensitive complexity measures to handle unbounded weights and general Lipschitz activation functions. This framework better aligns theoretical guarantees with contemporary deep learning practice while maintaining mathematical rigor.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2501.15753', 307)">Copy Link</button>
<div id="copy-message-307" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2501.15828">Hybrid Quantum Neural Networks with Amplitude Encoding: Advancing Recovery Rate Predictions</a></h1>
<p><b>Authors:</b> Ying Chen, Paul Griffin, Paolo Recchia, Lei Zhou, Hongrui Zhang</p>
<p>Abstract: Recovery rate prediction plays a pivotal role in bond investment strategies, enhancing risk assessment, optimizing portfolio allocation, improving pricing accuracy, and supporting effective credit risk management. However, forecasting faces challenges like high-dimensional features, small sample sizes, and overfitting. We propose a hybrid Quantum Machine Learning model incorporating Parameterized Quantum Circuits (PQC) within a neural network framework. PQCs inherently preserve unitarity, avoiding computationally costly orthogonality constraints, while amplitude encoding enables exponential data compression, reducing qubit requirements logarithmically. Applied to a global dataset of 1,725 observations (1996-2023), our method achieved superior accuracy (RMSE 0.228) compared to classical neural networks (0.246) and quantum models with angle encoding (0.242), with efficient computation times. This work highlights the potential of hybrid quantum-classical architectures in advancing recovery rate forecasting.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2501.15828', 308)">Copy Link</button>
<div id="copy-message-308" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2501.16489">Nonparametric Sparse Online Learning of the Koopman Operator</a></h1>
<p><b>Authors:</b> Boya Hou, Sina Sanjari, Nathan Dahlin, Alec Koppel, Subhonmesh Bose</p>
<p>Abstract: The Koopman operator provides a powerful framework for representing the dynamics of general nonlinear dynamical systems. Data-driven techniques to learn the Koopman operator typically assume that the chosen function space is closed under system dynamics. In this paper, we study the Koopman operator via its action on the reproducing kernel Hilbert space (RKHS), and explore the mis-specified scenario where the dynamics may escape the chosen function space. We relate the Koopman operator to the conditional mean embeddings (CME) operator and then present an operator stochastic approximation algorithm to learn the Koopman operator iteratively with control over the complexity of the representation. We provide both asymptotic and finite-time last-iterate guarantees of the online sparse learning algorithm with trajectory-based sampling with an analysis that is substantially more involved than that for finite-dimensional stochastic approximation. Numerical examples confirm the effectiveness of the proposed algorithm.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2501.16489', 309)">Copy Link</button>
<div id="copy-message-309" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2501.19054">Text-to-CAD Generation Through Infusing Visual Feedback in Large Language Models</a></h1>
<p><b>Authors:</b> Ruiyu Wang, Yu Yuan, Shizhao Sun, Jiang Bian</p>
<p>Abstract: Creating Computer-Aided Design (CAD) models requires significant expertise and effort. Text-to-CAD, which converts textual descriptions into CAD parametric sequences, is crucial in streamlining this process. Recent studies have utilized ground-truth parametric sequences, known as sequential signals, as supervision to achieve this goal. However, CAD models are inherently multimodal, comprising parametric sequences and corresponding rendered visual objects. Besides,the rendering process from parametric sequences to visual objects is many-to-one. Therefore, both sequential and visual signals are critical for effective training. In this work, we introduce CADFusion, a framework that uses Large Language Models (LLMs) as the backbone and alternates between two training stages: the sequential learning (SL) stage and the visual feedback (VF) stage. In the SL stage, we train LLMs using ground-truth parametric sequences, enabling the generation of logically coherent parametric sequences. In the VF stage, we reward parametric sequences that render into visually preferred objects and penalize those that do not, allowing LLMs to learn how rendered visual objects are perceived and evaluated. These two stages alternate throughout the training, ensuring balanced learning and preserving benefits of both signals. Experiments demonstrate that CADFusion significantly improves performance, both qualitatively and quantitatively.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2501.19054', 310)">Copy Link</button>
<div id="copy-message-310" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2501.19060">Contrast-Aware Calibration for Fine-Tuned CLIP: Leveraging Image-Text Alignment</a></h1>
<p><b>Authors:</b> Song-Lin Lv, Yu-Yang Chen, Zhi Zhou, Yu-Feng Li, Lan-Zhe Guo</p>
<p>Abstract: Vision-language models (VLMs), such as CLIP, have demonstrated exceptional generalization capabilities and can quickly adapt to downstream tasks through prompt fine-tuning. Unfortunately, in classification tasks involving non-training classes, known as open-vocabulary setting, fine-tuned VLMs often overfit to train classes, resulting in a misalignment between confidence scores and actual accuracy on unseen classes, which significantly undermines their reliability in real-world deployments. Existing confidence calibration methods typically require training parameters or analyzing features from the training dataset, restricting their ability to generalize unseen classes without corresponding train data. Moreover, VLM-specific calibration methods rely solely on text features from train classes as calibration indicators, which inherently limits their ability to calibrate train classes. To address these challenges, we propose an effective multimodal calibration method Contrast-Aware Calibration (CAC). Building on the original CLIP's zero-shot adaptability and the conclusion from empirical analysis that poor intra-class and inter-class discriminative ability on unseen classes is the root cause, we calculate calibration weights based on the contrastive difference between the original and fine-tuned CLIP. This method not only adapts to calibrating unseen classes but also overcomes the limitations of previous VLM calibration methods that could not calibrate train classes. In experiments involving 11 datasets with 5 fine-tuning methods, CAC consistently achieved the best calibration effect on both train and unseen classes without sacrificing accuracy and inference speed.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2501.19060', 311)">Copy Link</button>
<div id="copy-message-311" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2501.19407">Algorithmic Inheritance: Surname Bias in AI Decisions Reinforces Intergenerational Inequality</a></h1>
<p><b>Authors:</b> Pat Pataranutaporn, Nattavudh Powdthavee, Pattie Maes</p>
<p>Abstract: Surnames often convey implicit markers of social status, wealth, and lineage, shaping perceptions in ways that can perpetuate systemic biases and intergenerational inequality. This study is the first of its kind to investigate whether and how surnames influence AI-driven decision-making, focusing on their effects across key areas such as hiring recommendations, leadership appointments, and loan approvals. Using 72,000 evaluations of 600 surnames from the United States and Thailand, two countries with distinct sociohistorical contexts and surname conventions, we classify names into four categories: Rich, Legacy, Normal, and phonetically similar Variant groups. Our findings show that elite surnames consistently increase AI-generated perceptions of power, intelligence, and wealth, which in turn influence AI-driven decisions in high-stakes contexts. Mediation analysis reveals perceived intelligence as a key mechanism through which surname biases influence AI decision-making process. While providing objective qualifications alongside surnames mitigates most of these biases, it does not eliminate them entirely, especially in contexts where candidate credentials are low. These findings highlight the need for fairness-aware algorithms and robust policy measures to prevent AI systems from reinforcing systemic inequalities tied to surnames, an often-overlooked bias compared to more salient characteristics such as race and gender. Our work calls for a critical reassessment of algorithmic accountability and its broader societal impact, particularly in systems designed to uphold meritocratic principles while counteracting the perpetuation of intergenerational privilege.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2501.19407', 312)">Copy Link</button>
<div id="copy-message-312" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2502.00094">AIN: The Arabic INclusive Large Multimodal Model</a></h1>
<p><b>Authors:</b> Ahmed Heakl, Sara Ghaboura, Omkar Thawkar, Fahad Shahbaz Khan, Hisham Cholakkal, Rao Muhammad Anwer, Salman Khan</p>
<p>Abstract: Amid the swift progress of large language models (LLMs) and their evolution into large multimodal models (LMMs), significant strides have been made in high-resource languages such as English and Chinese. While Arabic LLMs have seen notable progress, Arabic LMMs remain largely unexplored, often narrowly focusing on a few specific aspects of the language and visual understanding. To bridge this gap, we introduce AIN-the Arabic Inclusive Multimodal Model-designed to excel across diverse domains. AIN is an English-Arabic bilingual LMM designed to excel in English and Arabic, leveraging carefully constructed 3.6 million high-quality Arabic-English multimodal data samples. AIN demonstrates state-of-the-art Arabic performance, while also possessing strong English-language visual capabilities. On the recent CAMEL-Bench benchmark comprising 38 sub-domains including, multi-image understanding, complex visual perception, handwritten document understanding, video understanding, medical imaging, plant diseases, and remote sensing-based land use understanding, our AIN demonstrates strong performance with the 7B model outperforming GPT-4o by an absolute gain of 3.4% averaged over eight domains and 38 sub-domains. AIN's superior capabilities position it as a significant step toward empowering Arabic speakers with advanced multimodal generative AI tools across diverse applications.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.00094', 313)">Copy Link</button>
<div id="copy-message-313" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2502.01627">A Poisson Process AutoDecoder for X-ray Sources</a></h1>
<p><b>Authors:</b> Yanke Song, Victoria Ashley Villar, Juan Rafael Martinez-Galarza, Steven Dillmann</p>
<p>Abstract: X-ray observing facilities, such as the Chandra X-ray Observatory and the eROSITA, have detected millions of astronomical sources associated with high-energy phenomena. The arrival of photons as a function of time follows a Poisson process and can vary by orders-of-magnitude, presenting obstacles for common tasks such as source classification, physical property derivation, and anomaly detection. Previous work has either failed to directly capture the Poisson nature of the data or only focuses on Poisson rate function reconstruction. In this work, we present Poisson Process AutoDecoder (PPAD). PPAD is a neural field decoder that maps fixed-length latent features to continuous Poisson rate functions across energy band and time via unsupervised learning. PPAD reconstructs the rate function and yields a representation at the same time. We demonstrate the efficacy of PPAD via reconstruction, regression, classification and anomaly detection experiments using the Chandra Source Catalog.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.01627', 314)">Copy Link</button>
<div id="copy-message-314" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2502.01697">BARE: Combining Base and Instruction-Tuned Language Models for Better Synthetic Data Generation</a></h1>
<p><b>Authors:</b> Alan Zhu, Parth Asawa, Jared Quincy Davis, Lingjiao Chen, Boris Hanin, Ion Stoica, Joseph E. Gonzalez, Matei Zaharia</p>
<p>Abstract: As the demand for high-quality data in model training grows, researchers and developers are increasingly generating synthetic data to tune and train LLMs. A common assumption about synthetic data is that sampling from instruct-tuned models is sufficient; however, these models struggle to produce diverse outputs-a key requirement for generalization. Despite various prompting methods, in this work we show that achieving meaningful diversity from instruct-tuned models remains challenging. In contrast, we find base models without post-training exhibit greater diversity, but are less capable at instruction following and hence of lower quality. Leveraging this insight, we propose Base-Refine (BARE), a synthetic data generation method that combines the diversity of base models with the quality of instruct-tuned models through a two-stage process. With minimal few-shot examples and curation, BARE generates diverse and high-quality datasets, improving downstream task performance. We show that fine-tuning with as few as 1,000 BARE-generated samples can reach performance comparable to the best similarly sized models on LiveCodeBench tasks. Furthermore, fine-tuning with BARE-generated data achieves a 101% improvement over instruct-only data on GSM8K and a 18.4% improvement over SOTA methods on RAFT.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.01697', 315)">Copy Link</button>
<div id="copy-message-315" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2502.01706">Comply: Learning Sentences with Complex Weights inspired by Fruit Fly Olfaction</a></h1>
<p><b>Authors:</b> Alexei Figueroa, Justus Westerhoff, Golzar Atefi, Dennis Fast, Benjamin Winter, Felix Alexader Gers, Alexander L\"oser, Wolfang Nejdl</p>
<p>Abstract: Biologically inspired neural networks offer alternative avenues to model data distributions. FlyVec is a recent example that draws inspiration from the fruit fly's olfactory circuit to tackle the task of learning word embeddings. Surprisingly, this model performs competitively even against deep learning approaches specifically designed to encode text, and it does so with the highest degree of computational efficiency. We pose the question of whether this performance can be improved further. For this, we introduce Comply. By incorporating positional information through complex weights, we enable a single-layer neural network to learn sequence representations. Our experiments show that Comply not only supersedes FlyVec but also performs on par with significantly larger state-of-the-art models. We achieve this without additional parameters. Comply yields sparse contextual representations of sentences that can be interpreted explicitly from the neuron weights.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.01706', 316)">Copy Link</button>
<div id="copy-message-316" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2502.01976">CITER: Collaborative Inference for Efficient Large Language Model Decoding with Token-Level Routing</a></h1>
<p><b>Authors:</b> Wenhao Zheng, Yixiao Chen, Weitong Zhang, Souvik Kundu, Yun Li, Zhengzhong Liu, Eric P. Xing, Hongyi Wang, Huaxiu Yao</p>
<p>Abstract: Large language models have achieved remarkable success in various tasks but suffer from high computational costs during inference, limiting their deployment in resource-constrained applications. To address this issue, we propose a novel CITER (\textbf{C}ollaborative \textbf{I}nference with \textbf{T}oken-l\textbf{E}vel \textbf{R}outing) framework that enables efficient collaboration between small and large language models (SLMs & LLMs) through a token-level routing strategy. Specifically, CITER routes non-critical tokens to an SLM for efficiency and routes critical tokens to an LLM for generalization quality. We formulate router training as a policy optimization, where the router receives rewards based on both the quality of predictions and the inference costs of generation. This allows the router to learn to predict token-level routing scores and make routing decisions based on both the current token and the future impact of its decisions. To further accelerate the reward evaluation process, we introduce a shortcut which significantly reduces the costs of the reward estimation and improving the practicality of our approach. Extensive experiments on five benchmark datasets demonstrate that CITER reduces the inference costs while preserving high-quality generation, offering a promising solution for real-time and resource-constrained applications.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.01976', 317)">Copy Link</button>
<div id="copy-message-317" class="copy-message"></div>
</div>

    </div>
    </body>
    