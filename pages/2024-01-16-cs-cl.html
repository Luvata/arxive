<!DOCTYPE html>
<html>
<head>
<title>2024-01-16-cs-cl</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2401.06183">End to end Hindi to English speech conversion using Bark, mBART and a finetuned XLSR Wav2Vec2. (arXiv:2401.06183v1 [eess.AS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Tathe_A/0/1/0/all/0/1">Aniket Tathe</a>, <a href="http://arxiv.org/find/eess/1/au:+Kamble_A/0/1/0/all/0/1">Anand Kamble</a>, <a href="http://arxiv.org/find/eess/1/au:+Kumbharkar_S/0/1/0/all/0/1">Suyash Kumbharkar</a>, <a href="http://arxiv.org/find/eess/1/au:+Bhandare_A/0/1/0/all/0/1">Atharva Bhandare</a>, <a href="http://arxiv.org/find/eess/1/au:+Mitra_A/0/1/0/all/0/1">Anirban C. Mitra</a></p>
<p>Speech has long been a barrier to effective communication and connection,
persisting as a challenge in our increasingly interconnected world. This
research paper introduces a transformative solution to this persistent obstacle
an end-to-end speech conversion framework tailored for Hindi-to-English
translation, culminating in the synthesis of English audio. By integrating
cutting-edge technologies such as XLSR Wav2Vec2 for automatic speech
recognition (ASR), mBART for neural machine translation (NMT), and a
Text-to-Speech (TTS) synthesis component, this framework offers a unified and
seamless approach to cross-lingual communication. We delve into the intricate
details of each component, elucidating their individual contributions and
exploring the synergies that enable a fluid transition from spoken Hindi to
synthesized English audio.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06194">CrisisKAN: Knowledge-infused and Explainable Multimodal Attention Network for Crisis Event Classification. (arXiv:2401.06194v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1">Shubham Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Saini_N/0/1/0/all/0/1">Nandini Saini</a>, <a href="http://arxiv.org/find/cs/1/au:+Kundu_S/0/1/0/all/0/1">Suman Kundu</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1">Debasis Das</a></p>
<p>Pervasive use of social media has become the emerging source for real-time
information (like images, text, or both) to identify various events. Despite
the rapid growth of image and text-based event classification, the
state-of-the-art (SOTA) models find it challenging to bridge the semantic gap
between features of image and text modalities due to inconsistent encoding.
Also, the black-box nature of models fails to explain the model's outcomes for
building trust in high-stakes situations such as disasters, pandemic.
Additionally, the word limit imposed on social media posts can potentially
introduce bias towards specific events. To address these issues, we proposed
CrisisKAN, a novel Knowledge-infused and Explainable Multimodal Attention
Network that entails images and texts in conjunction with external knowledge
from Wikipedia to classify crisis events. To enrich the context-specific
understanding of textual information, we integrated Wikipedia knowledge using
proposed wiki extraction algorithm. Along with this, a guided cross-attention
module is implemented to fill the semantic gap in integrating visual and
textual data. In order to ensure reliability, we employ a model-specific
approach called Gradient-weighted Class Activation Mapping (Grad-CAM) that
provides a robust explanation of the predictions of the proposed model. The
comprehensive experiments conducted on the CrisisMMD dataset yield in-depth
analysis across various crisis-specific tasks and settings. As a result,
CrisisKAN outperforms existing SOTA methodologies and provides a novel view in
the domain of explainable multimodal event classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06201">EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction. (arXiv:2401.06201v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1">Siyu Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1">Kaitao Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiangjie Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1">Xu Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yongliang Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Kan_R/0/1/0/all/0/1">Ren Kan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dongsheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1">Deqing Yang</a></p>
<p>To address intricate real-world tasks, there has been a rising interest in
tool utilization in applications of large language models (LLMs). To develop
LLM-based agents, it usually requires LLMs to understand many tool functions
from different tool documentation. But these documentations could be diverse,
redundant or incomplete, which immensely affects the capability of LLMs in
using tools. To solve this, we introduce EASYTOOL, a framework transforming
diverse and lengthy tool documentation into a unified and concise tool
instruction for easier tool usage. EasyTool purifies essential information from
extensive tool documentation of different sources, and elaborates a unified
interface (i.e., tool instruction) to offer standardized tool descriptions and
functionalities for LLM-based agents. Extensive experiments on multiple
different tasks demonstrate that EasyTool can significantly reduce token
consumption and improve the performance of tool utilization in real-world
scenarios. Our code will be available at
\url{https://github.com/microsoft/JARVIS/} in the future.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06210">Learning Unsupervised Semantic Document Representation for Fine-grained Aspect-based Sentiment Analysis. (arXiv:2401.06210v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1">Hao-Ming Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1">Pu-Jen Cheng</a></p>
<p>Document representation is the core of many NLP tasks on machine
understanding. A general representation learned in an unsupervised manner
reserves generality and can be used for various applications. In practice,
sentiment analysis (SA) has been a challenging task that is regarded to be
deeply semantic-related and is often used to assess general representations.
Existing methods on unsupervised document representation learning can be
separated into two families: sequential ones, which explicitly take the
ordering of words into consideration, and non-sequential ones, which do not
explicitly do so. However, both of them suffer from their own weaknesses. In
this paper, we propose a model that overcomes difficulties encountered by both
families of methods. Experiments show that our model outperforms
state-of-the-art methods on popular SA datasets and a fine-grained aspect-based
SA by a large margin.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06233">LEGOBench: Leaderboard Generation Benchmark for Scientific Models. (arXiv:2401.06233v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Shruti Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Alam_S/0/1/0/all/0/1">Shoaib Alam</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1">Mayank Singh</a></p>
<p>The ever-increasing volume of paper submissions makes it difficult to stay
informed about the latest state-of-the-art research. To address this challenge,
we introduce LEGOBench, a benchmark for evaluating systems that generate
leaderboards. LEGOBench is curated from 22 years of preprint submission data in
arXiv and more than 11,000 machine learning leaderboards in the PapersWithCode
portal. We evaluate the performance of four traditional graph-based ranking
variants and three recently proposed large language models. Our preliminary
results show significant performance gaps in automatic leaderboard generation.
The code is available on https://github.com/lingo-iitgn/LEGOBench and the
dataset is hosted on
https://osf.io/9v2py/?view_only=6f91b0b510df498ba01595f8f278f94c .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06301">Misconfidence-based Demonstration Selection for LLM In-Context Learning. (arXiv:2401.06301v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Shangqing Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chao Zhang</a> (Georgia Institute of Technology)</p>
<p>In-context learning with large language models (LLMs) excels at adapting to
various tasks rapidly. However, its success hinges on carefully selecting
demonstrations, which remains an obstacle in practice. Current approaches to
this problem either rely on hard-to-acquire external supervision or require
frequent interactions with LLMs, resulting in high costs. We propose a new
method called In-Context Reflection (ICR) to overcome these challenges. ICR
strategically selects demonstrations to reduce the discrepancy between the
LLM's outputs and the actual input-output mappings. Specifically, ICR starts
with a random set of initial demonstrations, then iteratively refines it. In
each step, it analyzes a pool of candidate examples and identifies the ones
most likely to challenge the LLM's current understanding, measured by a new
metric called misconfidence. These most confusing examples are then selected to
replace the less informative demonstrations in the current set. Our
comprehensive evaluation across five diverse datasets encompassing 13 subtasks
shows the efficacy of ICR. Compared to existing methods, ICR achieves an
average performance boost of 4%, while demonstrating remarkable cross-task
generalization capabilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06310">Beyond the Surface: A Global-Scale Analysis of Visual Stereotypes in Text-to-Image Generation. (arXiv:2401.06310v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jha_A/0/1/0/all/0/1">Akshita Jha</a>, <a href="http://arxiv.org/find/cs/1/au:+Prabhakaran_V/0/1/0/all/0/1">Vinodkumar Prabhakaran</a>, <a href="http://arxiv.org/find/cs/1/au:+Denton_R/0/1/0/all/0/1">Remi Denton</a>, <a href="http://arxiv.org/find/cs/1/au:+Laszlo_S/0/1/0/all/0/1">Sarah Laszlo</a>, <a href="http://arxiv.org/find/cs/1/au:+Dave_S/0/1/0/all/0/1">Shachi Dave</a>, <a href="http://arxiv.org/find/cs/1/au:+Qadri_R/0/1/0/all/0/1">Rida Qadri</a>, <a href="http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1">Chandan K. Reddy</a>, <a href="http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1">Sunipa Dev</a></p>
<p>Recent studies have highlighted the issue of stereotypical depictions for
people of different identity groups in Text-to-Image (T2I) model generations.
However, these existing approaches have several key limitations, including a
noticeable lack of coverage of global identity groups in their evaluation, and
the range of their associated stereotypes. Additionally, they often lack a
critical distinction between inherently visual stereotypes, such as
`underweight' or `sombrero', and culturally dependent stereotypes like
`attractive' or `terrorist'. In this work, we address these limitations with a
multifaceted approach that leverages existing textual resources to ground our
evaluation of geo-cultural stereotypes in the generated images from T2I models.
We employ existing stereotype benchmarks to identify and evaluate visual
stereotypes at a global scale, spanning 135 nationality-based identity groups.
We demonstrate that stereotypical attributes are thrice as likely to be present
in images of these identities as compared to other attributes. We further
investigate how disparately offensive the depictions of generated images are
for different nationalities. Finally, through a detailed case study, we reveal
how the 'default' representations of all identity groups have a stereotypical
appearance. Moreover, for the Global South, images across different attributes
are visually similar, even when explicitly prompted otherwise. CONTENT WARNING:
Some examples may contain offensive stereotypes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06320">Zero-shot Generative Large Language Models for Systematic Review Screening Automation. (arXiv:2401.06320v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Scells_H/0/1/0/all/0/1">Harrisen Scells</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_S/0/1/0/all/0/1">Shengyao Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1">Martin Potthast</a>, <a href="http://arxiv.org/find/cs/1/au:+Koopman_B/0/1/0/all/0/1">Bevan Koopman</a>, <a href="http://arxiv.org/find/cs/1/au:+Zuccon_G/0/1/0/all/0/1">Guido Zuccon</a></p>
<p>Systematic reviews are crucial for evidence-based medicine as they
comprehensively analyse published research findings on specific questions.
Conducting such reviews is often resource- and time-intensive, especially in
the screening phase, where abstracts of publications are assessed for inclusion
in a review. This study investigates the effectiveness of using zero-shot large
language models~(LLMs) for automatic screening. We evaluate the effectiveness
of eight different LLMs and investigate a calibration technique that uses a
predefined recall threshold to determine whether a publication should be
included in a systematic review. Our comprehensive evaluation using five
standard test collections shows that instruction fine-tuning plays an important
role in screening, that calibration renders LLMs practical for achieving a
targeted recall, and that combining both with an ensemble of zero-shot models
saves significant screening time compared to state-of-the-art approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06321">Multi-Task Learning for Front-End Text Processing in TTS. (arXiv:2401.06321v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1">Wonjune Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hinsvark_A/0/1/0/all/0/1">Arthur Hinsvark</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1">Qing He</a></p>
<p>We propose a multi-task learning (MTL) model for jointly performing three
tasks that are commonly solved in a text-to-speech (TTS) front-end: text
normalization (TN), part-of-speech (POS) tagging, and homograph disambiguation
(HD). Our framework utilizes a tree-like structure with a trunk that learns
shared representations, followed by separate task-specific heads. We further
incorporate a pre-trained language model to utilize its built-in lexical and
contextual knowledge, and study how to best use its embeddings so as to most
effectively benefit our multi-task model. Through task-wise ablations, we show
that our full model trained on all three tasks achieves the strongest overall
performance compared to models trained on individual or sub-combinations of
tasks, confirming the advantages of our MTL framework. Finally, we introduce a
new HD dataset containing a balanced number of sentences in diverse contexts
for a variety of homographs and their pronunciations. We demonstrate that
incorporating this dataset into training significantly improves HD performance
over only using a commonly used, but imbalanced, pre-existing dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06327">Learning from Semi-Factuals: A Debiased and Semantic-Aware Framework for Generalized Relation Discovery. (arXiv:2401.06327v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiaxin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lingling Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1">Tianlin Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1">Wenjun Wu</a></p>
<p>We introduce a novel task, called Generalized Relation Discovery (GRD), for
open-world relation extraction. GRD aims to identify unlabeled instances in
existing pre-defined relations or discover novel relations by assigning
instances to clusters as well as providing specific meanings for these
clusters. The key challenges of GRD are how to mitigate the serious model
biases caused by labeled pre-defined relations to learn effective relational
representations and how to determine the specific semantics of novel relations
during classifying or clustering unlabeled instances. We then propose a novel
framework, SFGRD, for this task to solve the above issues by learning from
semi-factuals in two stages. The first stage is semi-factual generation
implemented by a tri-view debiased relation representation module, in which we
take each original sentence as the main view and design two debiased views to
generate semi-factual examples for this sentence. The second stage is
semi-factual thinking executed by a dual-space tri-view collaborative relation
learning module, where we design a cluster-semantic space and a class-index
space to learn relational semantics and relation label indices, respectively.
In addition, we devise alignment and selection strategies to integrate two
spaces and establish a self-supervised learning loop for unlabeled data by
doing semi-factual thinking across three views. Extensive experimental results
show that SFGRD surpasses state-of-the-art models in terms of accuracy by
2.36\% $\sim$5.78\% and cosine similarity by 32.19\%$\sim$ 84.45\% for relation
label index and relation semantic quality, respectively. To the best of our
knowledge, we are the first to exploit the efficacy of semi-factuals in
relation extraction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06373">How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs. (arXiv:2401.06373v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1">Yi Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1">Hongpeng Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jingwen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1">Diyi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1">Ruoxi Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1">Weiyan Shi</a></p>
<p>Most traditional AI safety research has approached AI models as machines and
centered on algorithm-focused attacks developed by security experts. As large
language models (LLMs) become increasingly common and competent, non-expert
users can also impose risks during daily interactions. This paper introduces a
new perspective to jailbreak LLMs as human-like communicators, to explore this
overlooked intersection between everyday language interaction and AI safety.
Specifically, we study how to persuade LLMs to jailbreak them. First, we
propose a persuasion taxonomy derived from decades of social science research.
Then, we apply the taxonomy to automatically generate interpretable persuasive
adversarial prompts (PAP) to jailbreak LLMs. Results show that persuasion
significantly increases the jailbreak performance across all risk categories:
PAP consistently achieves an attack success rate of over $92\%$ on Llama 2-7b
Chat, GPT-3.5, and GPT-4 in $10$ trials, surpassing recent algorithm-focused
attacks. On the defense side, we explore various mechanisms against PAP and,
found a significant gap in existing defenses, and advocate for more fundamental
mitigation for highly interactive LLMs
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06382">What should I say? -- Interacting with AI and Natural Language Interfaces. (arXiv:2401.06382v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Adkins_M/0/1/0/all/0/1">Mark Adkins</a></p>
<p>As Artificial Intelligence (AI) technology becomes more and more prevalent,
it becomes increasingly important to explore how we as humans interact with AI.
The Human-AI Interaction (HAI) sub-field has emerged from the Human-Computer
Interaction (HCI) field and aims to examine this very notion. Many interaction
patterns have been implemented without fully understanding the changes in
required cognition as well as the cognitive science implications of using these
alternative interfaces that aim to be more human-like in nature. Prior research
suggests that theory of mind representations are crucial to successful and
effortless communication, however very little is understood when it comes to
how theory of mind representations are established when interacting with AI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06394">Adaptive Data Augmentation for Aspect Sentiment Quad Prediction. (arXiv:2401.06394v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenyuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xinghua Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1">Shiyao Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1">Kun Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xuebin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tingwen Liu</a></p>
<p>Aspect sentiment quad prediction (ASQP) aims to predict the quad sentiment
elements for a given sentence, which is a critical task in the field of
aspect-based sentiment analysis. However, the data imbalance issue has not
received sufficient attention in ASQP task. In this paper, we divide the issue
into two-folds, quad-pattern imbalance and aspect-category imbalance, and
propose an Adaptive Data Augmentation (ADA) framework to tackle the imbalance
issue. Specifically, a data augmentation process with a condition function
adaptively enhances the tail quad patterns and aspect categories, alleviating
the data imbalance in ASQP. Following previous studies, we also further explore
the generative framework for extracting complete quads by introducing the
category prior knowledge and syntax-guided decoding target. Experimental
results demonstrate that data augmentation for imbalance in ASQP task can
improve the performance, and the proposed ADA method is superior to naive data
oversampling.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06398">An approach for mistranslation removal from popular dataset for Indic MT Task. (arXiv:2401.06398v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1">Sudhansu Bala Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodrigues_L/0/1/0/all/0/1">Leo Raphael Rodrigues</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishra_T/0/1/0/all/0/1">Tapas Kumar Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Patra_B/0/1/0/all/0/1">Bidyut Kr. Patra</a></p>
<p>The conversion of content from one language to another utilizing a computer
system is known as Machine Translation (MT). Various techniques have come up to
ensure effective translations that retain the contextual and lexical
interpretation of the source language. End-to-end Neural Machine Translation
(NMT) is a popular technique and it is now widely used in real-world MT
systems. Massive amounts of parallel datasets (sentences in one language
alongside translations in another) are required for MT systems. These datasets
are crucial for an MT system to learn linguistic structures and patterns of
both languages during the training phase. One such dataset is Samanantar, the
largest publicly accessible parallel dataset for Indian languages (ILs). Since
the corpus has been gathered from various sources, it contains many incorrect
translations. Hence, the MT systems built using this dataset cannot perform to
their usual potential. In this paper, we propose an algorithm to remove
mistranslations from the training corpus and evaluate its performance and
efficiency. Two Indic languages (ILs), namely, Hindi (HIN) and Odia (ODI) are
chosen for the experiment. A baseline NMT system is built for these two ILs,
and the effect of different dataset sizes is also investigated. The quality of
the translations in the experiment is evaluated using standard metrics such as
BLEU, METEOR, and RIBES. From the results, it is observed that removing the
incorrect translation from the dataset makes the translation quality better. It
is also noticed that, despite the fact that the ILs-English and English-ILs
systems are trained using the same corpus, ILs-English works more effectively
across all the evaluation metrics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06400">Generalizing Visual Question Answering from Synthetic to Human-Written Questions via a Chain of QA with a Large Language Model. (arXiv:2401.06400v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1">Taehee Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1">Yeongjae Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Shin_H/0/1/0/all/0/1">Heejun Shin</a>, <a href="http://arxiv.org/find/cs/1/au:+Jo_Y/0/1/0/all/0/1">Yohan Jo</a>, <a href="http://arxiv.org/find/cs/1/au:+Shin_D/0/1/0/all/0/1">Dongmyung Shin</a></p>
<p>Visual question answering (VQA) is a task where an image is given, and a
series of questions are asked about the image. To build an efficient VQA
algorithm, a large amount of QA data is required which is very expensive.
Generating synthetic QA pairs based on templates is a practical way to obtain
data. However, VQA models trained on those data do not perform well on complex,
human-written questions. To address this issue, we propose a new method called
{\it chain of QA for human-written questions} (CoQAH). CoQAH utilizes a
sequence of QA interactions between a large language model and a VQA model
trained on synthetic data to reason and derive logical answers for
human-written questions. We tested the effectiveness of CoQAH on two types of
human-written VQA datasets for 3D-rendered and chest X-ray images and found
that it achieved state-of-the-art accuracy in both types of data. Notably,
CoQAH outperformed general vision-language models, VQA models, and medical
foundation models with no finetuning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06401">DevEval: Evaluating Code Generation in Practical Software Projects. (arXiv:2401.06401v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jia Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1">Ge Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yunfei Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yongmin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1">Zhi Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1">Hao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Huanyu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1">Kaibo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lecheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1">Zheng Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lanshen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1">Jiazheng Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xuanming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1">Yihong Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yuqi Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_B/0/1/0/all/0/1">Bin Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Mengfei Yang</a></p>
<p>How to evaluate Large Language Models (LLMs) in code generation is an open
question. Many benchmarks have been proposed but are inconsistent with
practical software projects, e.g., unreal program distributions, insufficient
dependencies, and small-scale project contexts. Thus, the capabilities of LLMs
in practical projects are still unclear. In this paper, we propose a new
benchmark named DevEval, aligned with Developers' experiences in practical
projects. DevEval is collected through a rigorous pipeline, containing 2,690
samples from 119 practical projects and covering 10 domains. Compared to
previous benchmarks, DevEval aligns to practical projects in multiple
dimensions, e.g., real program distributions, sufficient dependencies, and
enough-scale project contexts. We assess five popular LLMs on DevEval (e.g.,
gpt-4, gpt-3.5-turbo, CodeLLaMa, and StarCoder) and reveal their actual
abilities in code generation. For instance, the highest Pass@1 of gpt-3.5-turbo
only is 42 in our experiments. We also discuss the challenges and future
directions of code generation in practical projects. We open-source DevEval and
hope it can facilitate the development of code generation in practical
projects.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06408">AboutMe: Using Self-Descriptions in Webpages to Document the Effects of English Pretraining Data Filters. (arXiv:2401.06408v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lucy_L/0/1/0/all/0/1">Li Lucy</a>, <a href="http://arxiv.org/find/cs/1/au:+Gururangan_S/0/1/0/all/0/1">Suchin Gururangan</a>, <a href="http://arxiv.org/find/cs/1/au:+Soldaini_L/0/1/0/all/0/1">Luca Soldaini</a>, <a href="http://arxiv.org/find/cs/1/au:+Strubell_E/0/1/0/all/0/1">Emma Strubell</a>, <a href="http://arxiv.org/find/cs/1/au:+Bamman_D/0/1/0/all/0/1">David Bamman</a>, <a href="http://arxiv.org/find/cs/1/au:+Klein_L/0/1/0/all/0/1">Lauren Klein</a>, <a href="http://arxiv.org/find/cs/1/au:+Dodge_J/0/1/0/all/0/1">Jesse Dodge</a></p>
<p>Large language models' (LLMs) abilities are drawn from their pretraining
data, and model development begins with data curation. However, decisions
around what data is retained or removed during this initial stage is
under-scrutinized. In our work, we ground web text, which is a popular
pretraining data source, to its social and geographic contexts. We create a new
dataset of 10.3 million self-descriptions of website creators, and extract
information about who they are and where they are from: their topical
interests, social roles, and geographic affiliations. Then, we conduct the
first study investigating how ten "quality" and English language identification
(langID) filters affect webpages that vary along these social dimensions. Our
experiments illuminate a range of implicit preferences in data curation: we
show that some quality classifiers act like topical domain filters, and langID
can overlook English content from some regions of the world. Overall, we hope
that our work will encourage a new line of research on pretraining data
curation practices and its social implications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06416">Mission: Impossible Language Models. (arXiv:2401.06416v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kallini_J/0/1/0/all/0/1">Julie Kallini</a>, <a href="http://arxiv.org/find/cs/1/au:+Papadimitriou_I/0/1/0/all/0/1">Isabel Papadimitriou</a>, <a href="http://arxiv.org/find/cs/1/au:+Futrell_R/0/1/0/all/0/1">Richard Futrell</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahowald_K/0/1/0/all/0/1">Kyle Mahowald</a>, <a href="http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1">Christopher Potts</a></p>
<p>Chomsky and others have very directly claimed that large language models
(LLMs) are equally capable of learning languages that are possible and
impossible for humans to learn. However, there is very little published
experimental evidence to support such a claim. Here, we develop a set of
synthetic impossible languages of differing complexity, each designed by
systematically altering English data with unnatural word orders and grammar
rules. These languages lie on an impossibility continuum: at one end are
languages that are inherently impossible, such as random and irreversible
shuffles of English words, and on the other, languages that may not be
intuitively impossible but are often considered so in linguistics, particularly
those with rules based on counting word positions. We report on a wide range of
evaluations to assess the capacity of GPT-2 small models to learn these
uncontroversially impossible languages, and crucially, we perform these
assessments at various stages throughout training to compare the learning
process for each language. Our core finding is that GPT-2 struggles to learn
impossible languages when compared to English as a control, challenging the
core claim. More importantly, we hope our approach opens up a productive line
of inquiry in which different LLM architectures are tested on a variety of
impossible languages in an effort to learn more about how LLMs can be used as
tools for these cognitive and typological investigations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06431">From Automation to Augmentation: Large Language Models Elevating Essay Scoring Landscape. (arXiv:2401.06431v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Changrong Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1">Wenxing Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Sean Xin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kunpeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yufang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1">Qi Fu</a></p>
<p>Receiving immediate and personalized feedback is crucial for second-language
learners, and Automated Essay Scoring (AES) systems are a vital resource when
human instructors are unavailable. This study investigates the effectiveness of
Large Language Models (LLMs), specifically GPT-4 and fine-tuned GPT-3.5, as
tools for AES. Our comprehensive set of experiments, conducted on both public
and private datasets, highlights the remarkable advantages of LLM-based AES
systems. They include superior accuracy, consistency, generalizability, and
interpretability, with fine-tuned GPT-3.5 surpassing traditional grading
models. Additionally, we undertake LLM-assisted human evaluation experiments
involving both novice and expert graders. One pivotal discovery is that LLMs
not only automate the grading process but also enhance the performance of human
graders. Novice graders when provided with feedback generated by LLMs, achieve
a level of accuracy on par with experts, while experts become more efficient
and maintain greater consistency in their assessments. These results underscore
the potential of LLMs in educational technology, paving the way for effective
collaboration between humans and AI, ultimately leading to transformative
learning experiences through AI-generated feedback.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06443">BOK-VQA: Bilingual Outside Knowledge-based Visual Question Answering via Graph Representation Pretraining. (arXiv:2401.06443v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1">Minjun Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1">Seungwoo Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1">Youhan Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Jang_H/0/1/0/all/0/1">Haneol Jang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_K/0/1/0/all/0/1">Kyungtae Lim</a></p>
<p>The current research direction in generative models, such as the recently
developed GPT4, aims to find relevant knowledge information for multimodal and
multilingual inputs to provide answers. Under these research circumstances, the
demand for multilingual evaluation of visual question answering (VQA) tasks, a
representative task of multimodal systems, has increased. Accordingly, we
propose a bilingual outside-knowledge VQA (BOK-VQA) dataset in this study that
can be extended to multilingualism. The proposed data include 17K images, 17K
question-answer pairs for both Korean and English and 280K instances of
knowledge information related to question-answer content. We also present a
framework that can effectively inject knowledge information into a VQA system
by pretraining the knowledge information of BOK-VQA data in the form of graph
embeddings. Finally, through in-depth analysis, we demonstrated the actual
effect of the knowledge information contained in the constructed training data
on VQA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06461">Between Lines of Code: Unraveling the Distinct Patterns of Machine and Human Programmers. (arXiv:2401.06461v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yuling Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_C/0/1/0/all/0/1">Chengcheng Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1">Xiaodong Gu</a></p>
<p>Large language models have catalyzed an unprecedented wave in code
generation. While achieving significant advances, they blur the distinctions
between machine-and human-authored source code, causing integrity and
authenticity issues of software artifacts. Previous methods such as DetectGPT
have proven effective in discerning machine-generated texts, but they do not
identify and harness the unique patterns of machine-generated code. Thus, its
applicability falters when applied to code. In this paper, we carefully study
the specific patterns that characterize machine and human-authored code.
Through a rigorous analysis of code attributes such as length, lexical
diversity, and naturalness, we expose unique pat-terns inherent to each source.
We particularly notice that the structural segmentation of code is a critical
factor in identifying its provenance. Based on our findings, we propose a novel
machine-generated code detection method called DetectCodeGPT, which improves
DetectGPT by capturing the distinct structural patterns of code. Diverging from
conventional techniques that depend on external LLMs for perturbations,
DetectCodeGPT perturbs the code corpus by strategically inserting spaces and
newlines, ensuring both efficacy and efficiency. Experiment results show that
our approach significantly outperforms state-of-the-art techniques in detecting
machine-generated code.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06466">PersianMind: A Cross-Lingual Persian-English Large Language Model. (arXiv:2401.06466v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rostami_P/0/1/0/all/0/1">Pedram Rostami</a>, <a href="http://arxiv.org/find/cs/1/au:+Salemi_A/0/1/0/all/0/1">Ali Salemi</a>, <a href="http://arxiv.org/find/cs/1/au:+Dousti_M/0/1/0/all/0/1">Mohammad Javad Dousti</a></p>
<p>Large language models demonstrate remarkable proficiency in various
linguistic tasks and have extensive knowledge across various domains. Although
they perform best in English, their ability in other languages is notable too.
In contrast, open-source models, such as LLaMa, are primarily trained on
English datasets, resulting in poor performance in non-English languages. In
this paper, we introduce PersianMind, an open-source bilingual large language
model which demonstrates comparable performance to closed-source GPT-3.5-turbo
in the Persian language. By expanding LLaMa2's vocabulary with 10,000 Persian
tokens and training it on a dataset comprising nearly 2 billion Persian tokens,
we show that our approach preserves the model's English knowledge and employs
transfer learning to excel at transferring task knowledge from one language to
another.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06468">Adapting Large Language Models for Document-Level Machine Translation. (arXiv:2401.06468v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1">Minghao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1">Thuy-Trang Vu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1">Lizhen Qu</a>, <a href="http://arxiv.org/find/cs/1/au:+Foster_G/0/1/0/all/0/1">George Foster</a>, <a href="http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1">Gholamreza Haffari</a></p>
<p>Large language models (LLMs) have made significant strides in various natural
language processing (NLP) tasks. Recent research shows that the
moderately-sized LLMs often outperform their larger counterparts after
task-specific fine-tuning. In this work, we delve into the process of adapting
LLMs to specialize in document-level machine translation (DocMT) for a specific
language pair. Firstly, we explore how prompt strategies affect downstream
translation performance. Then, we conduct extensive experiments with two
fine-tuning methods, three LLM backbones, and 18 translation tasks across nine
language pairs. Our findings indicate that in some cases, these specialized
models even surpass GPT-4 in translation performance, while they still
significantly suffer from the off-target translation issue in others, even if
they are exclusively fine-tuned on bilingual parallel documents. Furthermore,
we provide an in-depth analysis of these LLMs tailored for DocMT, exploring
aspects such as translation errors, the scaling law of parallel documents,
out-of-domain generalization, and the impact of zero-shot crosslingual
transfer. The findings of this research not only shed light on the strengths
and limitations of LLM-based DocMT models but also provide a foundation for
future research in DocMT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06469">Batch-ICL: Effective, Efficient, and Order-Agnostic In-Context Learning. (arXiv:2401.06469v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kaiyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_A/0/1/0/all/0/1">Ang Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuhan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ha_H/0/1/0/all/0/1">Hansen Ha</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1">Tao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1">Rui Yan</a></p>
<p>In this paper, by treating in-context learning (ICL) as a meta-optimization
process, we explain why LLMs are sensitive to the order of ICL examples. This
understanding leads us to the development of Batch-ICL, an effective,
efficient, and order-agnostic inference algorithm for ICL. Differing from the
standard N-shot learning approach, Batch-ICL employs $N$ separate 1-shot
forward computations and aggregates the resulting meta-gradients. These
aggregated meta-gradients are then applied to a zero-shot learning to generate
the final prediction. This batch processing approach renders the LLM agnostic
to the order of ICL examples. Through extensive experiments and analysis, we
demonstrate that Batch-ICL consistently outperforms most permutations of
example sequences. In some cases, it even exceeds the performance of the
optimal order for standard ICL, all while reducing the computational resources
required. Furthermore, we develop a novel variant of Batch-ICL featuring
multiple "epochs" of meta-optimization. This variant implicitly explores
permutations of ICL examples, further enhancing ICL performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06477">Kun: Answer Polishment for Chinese Self-Alignment with Instruction Back-Translation. (arXiv:2401.06477v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1">Tianyu Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1">Shuyue Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1">Xingwei Qu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jiawei Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Weixu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1">Xinrun Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1">Chenghua Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1">Wenhao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wenhu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1">Jie Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Ge Zhang</a></p>
<p>In this paper, we introduce Kun, a novel approach for creating high-quality
instruction-tuning datasets for large language models (LLMs) without relying on
manual annotations. Adapting a self-training algorithm based on instruction
back-translation and answer polishment, Kun leverages unlabelled data from
diverse sources such as Wudao, Wanjuan, and SkyPile to generate a substantial
dataset of over a million Chinese instructional data points. This approach
significantly deviates from traditional methods by using a self-curation
process to refine and select the most effective instruction-output pairs. Our
experiments with the 6B-parameter Yi model across various benchmarks
demonstrate Kun's robustness and scalability. Our method's core contributions
lie in its algorithmic advancement, which enhances data retention and clarity,
and its innovative data generation approach that substantially reduces the
reliance on costly and time-consuming manual annotations. This methodology
presents a scalable and efficient solution for improving the
instruction-following capabilities of LLMs, with significant implications for
their application across diverse fields. The code and dataset can be found at
https://github.com/Zheng0428/COIG-Kun
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06495">An investigation of structures responsible for gender bias in BERT and DistilBERT. (arXiv:2401.06495v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Leteno_T/0/1/0/all/0/1">Thibaud Leteno</a>, <a href="http://arxiv.org/find/cs/1/au:+Gourru_A/0/1/0/all/0/1">Antoine Gourru</a>, <a href="http://arxiv.org/find/cs/1/au:+Laclau_C/0/1/0/all/0/1">Charlotte Laclau</a>, <a href="http://arxiv.org/find/cs/1/au:+Gravier_C/0/1/0/all/0/1">Christophe Gravier</a></p>
<p>In recent years, large Transformer-based Pre-trained Language Models (PLM)
have changed the Natural Language Processing (NLP) landscape, by pushing the
performance boundaries of the state-of-the-art on a wide variety of tasks.
However, this performance gain goes along with an increase in complexity, and
as a result, the size of such models (up to billions of parameters) represents
a constraint for their deployment on embedded devices or short-inference time
tasks. To cope with this situation, compressed models emerged (e.g.
DistilBERT), democratizing their usage in a growing number of applications that
impact our daily lives. A crucial issue is the fairness of the predictions made
by both PLMs and their distilled counterparts. In this paper, we propose an
empirical exploration of this problem by formalizing two questions: (1) Can we
identify the neural mechanism(s) responsible for gender bias in BERT (and by
extension DistilBERT)? (2) Does distillation tend to accentuate or mitigate
gender bias (e.g. is DistilBERT more prone to gender bias than its uncompressed
version, BERT)? Our findings are the following: (I) one cannot identify a
specific layer that produces bias; (II) every attention head uniformly encodes
bias; except in the context of underrepresented classes with a high imbalance
of the sensitive attribute; (III) this subset of heads is different as we
re-fine tune the network; (IV) bias is more homogeneously produced by the heads
in the distilled model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06509">AntEval: Quantitatively Evaluating Informativeness and Expressiveness of Agent Social Interactions. (arXiv:2401.06509v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1">Yuanzhi Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1">Linchao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yi Yang</a></p>
<p>While Large Language Models (LLMs) based agents have successfully mimicked
human behaviors in various scenarios, the realm of complex, multi-character
social interactions within extended contexts remains underexplored. The
challenge is compounded by privacy concerns, making it difficult to capture and
utilize intricate real-life interactions. More importantly, the absence of
quantitative evaluation methods hampers the pursuit of high-quality agent
interactions, often leading to interactions that are limited in informativeness
and expressiveness, characterized by superficial small talk without clear
intentions. In this work, we leverage the rules of Tabletop Role-Playing Games
(TRPG) to create an environment conducive to complex, context-rich
interactions, emphasizing informativeness and expressiveness. This virtual
setting alleviates privacy concerns and motivates agents to engage in
meaningful, high-quality interactions as part of their in-game objectives. To
assess these interactions, we introduce the Agent interaction Evaluation
framework (AntEval), targeting the qualitative evaluation of interaction
informativeness and expressiveness. Specifically, we propose two novel
evaluation metrics: Information Exchanging Precision (IEP) and Interaction
Expressiveness Gap (IEG). These metrics are designed to assess interactions in
scenarios focused on information exchange and intention expression,
respectively. Our experimental results demonstrate the effectiveness of these
metrics in evaluating interaction quality. Notably, we identify significant
areas for improvement in LLMs regarding social interactions, as highlighted by
our metrics. We believe AntEval will guide further exploration in complex agent
interactions, bringing them closer to emulating real human behavior and
enhancing their integration and utility in real-world applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06526">MetaHate: A Dataset for Unifying Efforts on Hate Speech Detection. (arXiv:2401.06526v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Piot_P/0/1/0/all/0/1">Paloma Piot</a>, <a href="http://arxiv.org/find/cs/1/au:+Martin_Rodilla_P/0/1/0/all/0/1">Patricia Mart&#xed;n-Rodilla</a>, <a href="http://arxiv.org/find/cs/1/au:+Parapar_J/0/1/0/all/0/1">Javier Parapar</a></p>
<p>Hate speech represents a pervasive and detrimental form of online discourse,
often manifested through an array of slurs, from hateful tweets to defamatory
posts. As such speech proliferates, it connects people globally and poses
significant social, psychological, and occasionally physical threats to
targeted individuals and communities. Current computational linguistic
approaches for tackling this phenomenon rely on labelled social media datasets
for training. For unifying efforts, our study advances in the critical need for
a comprehensive meta-collection, advocating for an extensive dataset to help
counteract this problem effectively. We scrutinized over 60 datasets,
selectively integrating those pertinent into MetaHate. This paper offers a
detailed examination of existing collections, highlighting their strengths and
limitations. Our findings contribute to a deeper understanding of the existing
datasets, paving the way for training more robust and adaptable models. These
enhanced models are essential for effectively combating the dynamic and complex
nature of hate speech in the digital realm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06532">INTERS: Unlocking the Power of Large Language Models in Search with Instruction Tuning. (arXiv:2401.06532v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yutao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Peitian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chenghao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yifei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_B/0/1/0/all/0/1">Binyu Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1">Zhicheng Dou</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1">Ji-Rong Wen</a></p>
<p>Large language models (LLMs) have demonstrated impressive capabilities in
various natural language processing tasks. Despite this, their application to
information retrieval (IR) tasks is still challenging due to the infrequent
occurrence of many IR-specific concepts in natural language. While prompt-based
methods can provide task descriptions to LLMs, they often fall short in
facilitating comprehensive understanding and execution of IR tasks, thereby
limiting LLMs' applicability. To address this gap, in this work, we explore the
potential of instruction tuning to enhance LLMs' proficiency in IR tasks. We
introduce a novel instruction tuning dataset, INTERS, encompassing 21 tasks
across three fundamental IR categories: query understanding, document
understanding, and query-document relationship understanding. The data are
derived from 43 distinct datasets with manually written templates. Our
empirical results reveal that INTERS significantly boosts the performance of
various publicly available LLMs, such as LLaMA, Mistral, and Phi, in
search-related tasks. Furthermore, we conduct a comprehensive analysis to
ascertain the effects of base model selection, instruction design, volume of
instructions, and task variety on performance. We make our dataset and the
models fine-tuned on it publicly accessible at https://github.com/DaoD/INTERS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06541">Medical Dialogue Generation via Intuitive-then-Analytical Differential Diagnosis. (arXiv:2401.06541v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Kaishuai Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_W/0/1/0/all/0/1">Wenjun Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1">Yi Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wenjie Li</a></p>
<p>Medical dialogue systems have attracted growing research attention as they
have the potential to provide rapid diagnoses, treatment plans, and health
consultations. In medical dialogues, a proper diagnosis is crucial as it
establishes the foundation for future consultations. Clinicians typically
employ both intuitive and analytic reasoning to formulate a differential
diagnosis. This reasoning process hypothesizes and verifies a variety of
possible diseases and strives to generate a comprehensive and rigorous
diagnosis. However, recent studies on medical dialogue generation have
overlooked the significance of modeling a differential diagnosis, which hinders
the practical application of these systems. To address the above issue, we
propose a medical dialogue generation framework with the
Intuitive-then-Analytic Differential Diagnosis (IADDx). Our method starts with
a differential diagnosis via retrieval-based intuitive association and
subsequently refines it through a graph-enhanced analytic procedure. The
resulting differential diagnosis is then used to retrieve medical knowledge and
guide response generation. Experimental results on two datasets validate the
efficacy of our method. Besides, we demonstrate how our framework assists both
clinicians and patients in understanding the diagnostic process, for instance,
by producing intermediate results and graph-based diagnosis paths.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06561">Intention Analysis Prompting Makes Large Language Models A Good Jailbreak Defender. (arXiv:2401.06561v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuqi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1">Liang Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lefei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1">Dacheng Tao</a></p>
<p>Aligning large language models (LLMs) with human values, particularly in the
face of stealthy and complex jailbreaks, presents a formidable challenge. In
this study, we present a simple yet highly effective defense strategy, i.e.,
Intention Analysis Prompting (IAPrompt). The principle behind is to trigger
LLMs' inherent self-correct and improve ability through a two-stage process: 1)
essential intention analysis, and 2) policy-aligned response. Notably, IAPrompt
is an inference-only method, thus could enhance the safety of LLMs without
compromising their helpfulness. Extensive experiments on SAP200 and DAN
benchmarks across Vicuna, ChatGLM, MPT, DeepSeek, and GPT-3.5 show that
IAPrompt could consistently and significantly reduce the harmfulness in
response (averagely -46.5% attack success rate) and maintain the general
helpfulness. Further analyses present some insights into how our method works.
To facilitate reproducibility, We release our code and scripts at:
https://github.com/alphadl/SafeLLM_with_IntentionAnalysis
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06568">Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation. (arXiv:2401.06568v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xu Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhirui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1">Xiang Geng</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1">Yichao Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiajun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Shujian Huang</a></p>
<p>Large Language Models (LLMs) have achieved remarkable results in the machine
translation evaluation task, yet there remains a gap in knowledge regarding how
they utilize the provided data to conduct evaluations. This study aims to
explore how LLMs leverage source and reference information in evaluating
translations, with the ultimate goal of better understanding the working
mechanism of LLMs. To this end, we design the controlled experiments across
various input modes and model types, and employ both coarse-grained and
fine-grained prompts to discern the utility of source versus reference
information. Surprisingly, we find that reference information significantly
enhances the evaluation accuracy, while source information sometimes is
counterproductive, indicating a lack of cross-lingual capability when using
LLMs to evaluate translations. We further conduct a meta-evaluation for
translation error detection of LLMs, observing a similar phenomenon. These
findings also suggest a potential research direction for LLMs that fully
exploits the cross-lingual capability of LLMs to achieve better performance in
machine translation evaluation tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06583">Mapping Transformer Leveraged Embeddings for Cross-Lingual Document Representation. (arXiv:2401.06583v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tashu_T/0/1/0/all/0/1">Tsegaye Misikir Tashu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kontos_E/0/1/0/all/0/1">Eduard-Raul Kontos</a>, <a href="http://arxiv.org/find/cs/1/au:+Sabatelli_M/0/1/0/all/0/1">Matthia Sabatelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Valdenegro_Toro_M/0/1/0/all/0/1">Matias Valdenegro-Toro</a></p>
<p>Recommendation systems, for documents, have become tools to find relevant
content on the Web. However, these systems have limitations when it comes to
recommending documents in languages different from the query language, which
means they might overlook resources in non-native languages. This research
focuses on representing documents across languages by using Transformer
Leveraged Document Representations (TLDRs) that are mapped to a cross-lingual
domain. Four multilingual pre-trained transformer models (mBERT, mT5 XLM
RoBERTa, ErnieM) were evaluated using three mapping methods across 20 language
pairs representing combinations of five selected languages of the European
Union. Metrics like Mate Retrieval Rate and Reciprocal Rank were used to
measure the effectiveness of mapped TLDRs compared to non-mapped ones. The
results highlight the power of cross-lingual representations achieved through
pre-trained transformers and mapping approaches suggesting a promising
direction for expanding beyond language connections, between two specific
languages.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06591">Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained Evaluation. (arXiv:2401.06591v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Seongyun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Seungone Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1">Sue Hyun Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1">Geewook Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1">Minjoon Seo</a></p>
<p>Assessing long-form responses generated by Vision-Language Models (VLMs) is
challenging. It not only requires checking whether the VLM follows the given
instruction but also verifying whether the text output is properly grounded on
the given image. Inspired by the recent approach of evaluating LMs with LMs, in
this work, we propose to evaluate VLMs with VLMs. For this purpose, we present
a new feedback dataset called the Perception Collection, encompassing 15K
customized score rubrics that users might care about during assessment. Using
the Perception Collection, we train Prometheus-Vision, the first open-source
VLM evaluator model that can understand the user-defined score criteria during
evaluation. Prometheus-Vision shows the highest Pearson correlation with human
evaluators and GPT-4V among open-source models, showing its effectiveness for
transparent and accessible evaluation of VLMs. We open-source our code,
dataset, and model at https://github.com/kaistAI/prometheus-vision
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06603">Mutual Enhancement of Large Language and Reinforcement Learning Models through Bi-Directional Feedback Mechanisms: A Case Study. (arXiv:2401.06603v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1">Shangding Gu</a></p>
<p>Large Language Models (LLMs) have demonstrated remarkable capabilities for
reinforcement learning (RL) models, such as planning and reasoning
capabilities. However, the problems of LLMs and RL model collaboration still
need to be solved. In this study, we employ a teacher-student learning
framework to tackle these problems, specifically by offering feedback for LLMs
using RL models and providing high-level information for RL models with LLMs in
a cooperative multi-agent setting. Within this framework, the LLM acts as a
teacher, while the RL model acts as a student. The two agents cooperatively
assist each other through a process of recursive help, such as "I help you help
I help." The LLM agent supplies abstract information to the RL agent, enabling
efficient exploration and policy improvement. In turn, the RL agent offers
feedback to the LLM agent, providing valuable, real-time information that helps
generate more useful tokens. This bi-directional feedback loop promotes
optimization, exploration, and mutual improvement for both agents, enabling
them to accomplish increasingly challenging tasks. Remarkably, we propose a
practical algorithm to address the problem and conduct empirical experiments to
evaluate the effectiveness of our method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06620">TransliCo: A Contrastive Learning Framework to Address the Script Barrier in Multilingual Pretrained Language Models. (arXiv:2401.06620v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yihong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1">Chunlan Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1">Haotian Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1">Hinrich Sch&#xfc;tze</a></p>
<p>There are 293 scripts representing over 7,000 languages in the written form.
Due to various reasons, many closely related languages use different scripts,
which poses difficulty for multilingual pretrained language models (mPLMs) in
learning crosslingual knowledge through lexical overlap. As a result, mPLMs
present a script barrier: representations from different scripts are located in
different subspaces, which is a strong indicator of why crosslingual transfer
involving languages of different scripts shows sub-optimal performance. To
address this problem, we propose a simple framework TransliCo that contains
Transliteration Contrastive Modeling (TCM) to fine-tune an mPLM by contrasting
sentences in its training data and their transliterations in a unified script
(Latn, in our case), which ensures uniformity in the representation space for
different scripts. Using Glot500-m, an mPLM pretrained on over 500 languages,
as our source model, we find-tune it on a small portion (5\%) of its training
data, and refer to the resulting model as Furina. We show that Furina not only
better aligns representations from distinct scripts but also outperforms the
original Glot500-m on various crosslingual transfer tasks. Additionally, we
achieve consistent improvement in a case study on the Indic group where the
languages are highly related but use different scripts. We make our code and
models publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06628">OOP: Object-Oriented Programming Evaluation Benchmark for Large Language Models. (arXiv:2401.06628v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1">Liang Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1">Li Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1">Yong Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1">Bo Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1">Dacheng Tao</a></p>
<p>Advancing automated programming necessitates robust and comprehensive code
generation benchmarks, yet current evaluation frameworks largely neglect
object-oriented programming (OOP) in favor of functional programming (FP),
e.g., HumanEval and MBPP. To address this, our study introduces a pioneering
OOP-focused benchmark, featuring 431 Python programs that encompass essential
OOP concepts and features like classes and encapsulation methods. We propose a
novel evaluation metric, pass@o, tailored for OOP, enhancing traditional pass@k
measures. Our evaluation of 23 leading large language models (LLMs), including
both general and code-specialized models, reveals three key insights: 1) pass@o
offers a more relevant and comprehensive assessment for OOP code generation; 2)
Despite excelling in FP, code-specialized LLMs like WizardCoder lag in OOP
compared to models like ChatGPT; 3) The poor performance of all advanced LLMs
on our OOP benchmark highlights a critical need for improvements in this field.
Our benchmark and scripts are publicly released at:
https://github.com/alphadl/OOP-eval.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06640">Experimental Contexts Can Facilitate Robust Semantic Property Inference in Language Models, but Inconsistently. (arXiv:2401.06640v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Misra_K/0/1/0/all/0/1">Kanishka Misra</a>, <a href="http://arxiv.org/find/cs/1/au:+Ettinger_A/0/1/0/all/0/1">Allyson Ettinger</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahowald_K/0/1/0/all/0/1">Kyle Mahowald</a></p>
<p>Recent zero-shot evaluations have highlighted important limitations in the
abilities of language models (LMs) to perform meaning extraction. However, it
is now well known that LMs can demonstrate radical improvements in the presence
of experimental contexts such as in-context examples and instructions. How well
does this translate to previously studied meaning-sensitive tasks? We present a
case-study on the extent to which experimental contexts can improve LMs'
robustness in performing property inheritance -- predicting semantic properties
of novel concepts, a task that they have been previously shown to fail on. Upon
carefully controlling the nature of the in-context examples and the
instructions, our work reveals that they can indeed lead to non-trivial
property inheritance behavior in LMs. However, this ability is inconsistent:
with a minimal reformulation of the task, some LMs were found to pick up on
shallow, non-semantic heuristics from their inputs, suggesting that the
computational principles of semantic property inference are yet to be mastered
by LMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06643">Effects of diversity incentives on sample diversity and downstream model performance in LLM-based text augmentation. (arXiv:2401.06643v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cegin_J/0/1/0/all/0/1">Jan Cegin</a>, <a href="http://arxiv.org/find/cs/1/au:+Pecher_B/0/1/0/all/0/1">Branislav Pecher</a>, <a href="http://arxiv.org/find/cs/1/au:+Simko_J/0/1/0/all/0/1">Jakub Simko</a>, <a href="http://arxiv.org/find/cs/1/au:+Srba_I/0/1/0/all/0/1">Ivan Srba</a>, <a href="http://arxiv.org/find/cs/1/au:+Bielikova_M/0/1/0/all/0/1">Maria Bielikova</a>, <a href="http://arxiv.org/find/cs/1/au:+Brusilovsky_P/0/1/0/all/0/1">Peter Brusilovsky</a></p>
<p>The latest generative large language models (LLMs) have found their
application in data augmentation tasks, where small numbers of text samples are
LLM-paraphrased and then used to fine-tune the model. However, more research is
needed to assess how different prompts, seed data selection strategies,
filtering methods, or model settings affect the quality of paraphrased data
(and downstream models). In this study, we investigate three text diversity
incentive methods well established in crowdsourcing: taboo words, hints by
previous outlier solutions, and chaining on previous outlier solutions. Using
these incentive methods as part of instructions to LLMs augmenting text
datasets, we measure their effects on generated texts' lexical diversity and
downstream model performance. We compare the effects over 5 different LLMs and
6 datasets. We show that diversity is most increased by taboo words, while
downstream model performance is highest when previously created paraphrases are
used as hints.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06659">WisdoM: Improving Multimodal Sentiment Analysis by Fusing Contextual World Knowledge. (arXiv:2401.06659v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenbin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1">Liang Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1">Li Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1">Yong Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1">Han Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1">Dacheng Tao</a></p>
<p>Sentiment analysis is rapidly advancing by utilizing various data modalities
(e.g., text, image). However, most previous works relied on superficial
information, neglecting the incorporation of contextual world knowledge (e.g.,
background information derived from but beyond the given image and text pairs)
and thereby restricting their ability to achieve better multimodal sentiment
analysis. In this paper, we proposed a plug-in framework named WisdoM, designed
to leverage contextual world knowledge induced from the large vision-language
models (LVLMs) for enhanced multimodal sentiment analysis. WisdoM utilizes a
LVLM to comprehensively analyze both images and corresponding sentences,
simultaneously generating pertinent context. To reduce the noise in the
context, we also introduce a training-free Contextual Fusion mechanism.
Experimental results across diverse granularities of multimodal sentiment
analysis tasks consistently demonstrate that our approach has substantial
improvements (brings an average +1.89 F1 score among five advanced methods)
over several state-of-the-art methods. Code will be released.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06665">PolyTOPS: Reconfigurable and Flexible Polyhedral Scheduler. (arXiv:2401.06665v1 [cs.DC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Consolaro_G/0/1/0/all/0/1">Gianpietro Consolaro</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Razanajato_H/0/1/0/all/0/1">Harenome Razanajato</a>, <a href="http://arxiv.org/find/cs/1/au:+Lossing_N/0/1/0/all/0/1">Nelson Lossing</a>, <a href="http://arxiv.org/find/cs/1/au:+Tchoulak_N/0/1/0/all/0/1">Nassim Tchoulak</a>, <a href="http://arxiv.org/find/cs/1/au:+Susungi_A/0/1/0/all/0/1">Adilla Susungi</a>, <a href="http://arxiv.org/find/cs/1/au:+Alves_A/0/1/0/all/0/1">Artur Cesar Araujo Alves</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Renwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Barthou_D/0/1/0/all/0/1">Denis Barthou</a>, <a href="http://arxiv.org/find/cs/1/au:+Ancourt_C/0/1/0/all/0/1">Corinne Ancourt</a>, <a href="http://arxiv.org/find/cs/1/au:+Bastoul_C/0/1/0/all/0/1">Cedric Bastoul</a></p>
<p>Polyhedral techniques have been widely used for automatic code optimization
in low-level compilers and higher-level processes. Loop optimization is central
to this technique, and several polyhedral schedulers like Feautrier, Pluto, isl
and Tensor Scheduler have been proposed, each of them targeting a different
architecture, parallelism model, or application scenario. The need for
scenario-specific optimization is growing due to the heterogeneity of
architectures. One of the most critical cases is represented by NPUs (Neural
Processing Units) used for AI, which may require loop optimization with
different objectives. Another factor to be considered is the framework or
compiler in which polyhedral optimization takes place. Different scenarios,
depending on the target architecture, compilation environment, and application
domain, may require different kinds of optimization to best exploit the
architecture feature set.
</p>
<p>We introduce a new configurable polyhedral scheduler, PolyTOPS, that can be
adjusted to various scenarios with straightforward, high-level configurations.
This scheduler allows the creation of diverse scheduling strategies that can be
both scenario-specific (like state-of-the-art schedulers) and kernel-specific,
breaking the concept of a one-size-fits-all scheduler approach. PolyTOPS has
been used with isl and CLooG as code generators and has been integrated in
MindSpore AKG deep learning compiler. Experimental results in different
scenarios show good performance: a geomean speedup of 7.66x on MindSpore (for
the NPU Ascend architecture) hybrid custom operators over isl scheduling, a
geomean speedup up to 1.80x on PolyBench on different multicore architectures
over Pluto scheduling. Finally, some comparisons with different
state-of-the-art tools are presented in the PolyMage scenario.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06683">DQNC2S: DQN-based Cross-stream Crisis event Summarizer. (arXiv:2401.06683v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cambrin_D/0/1/0/all/0/1">Daniele Rege Cambrin</a>, <a href="http://arxiv.org/find/cs/1/au:+Cagliero_L/0/1/0/all/0/1">Luca Cagliero</a>, <a href="http://arxiv.org/find/cs/1/au:+Garza_P/0/1/0/all/0/1">Paolo Garza</a></p>
<p>Summarizing multiple disaster-relevant data streams simultaneously is
particularly challenging as existing Retrieve&amp;Re-ranking strategies suffer from
the inherent redundancy of multi-stream data and limited scalability in a
multi-query setting. This work proposes an online approach to crisis timeline
generation based on weak annotation with Deep Q-Networks. It selects on-the-fly
the relevant pieces of text without requiring neither human annotations nor
content re-ranking. This makes the inference time independent of the number of
input queries. The proposed approach also incorporates a redundancy filter into
the reward function to effectively handle cross-stream content overlaps. The
achieved ROUGE and BERTScore results are superior to those of best-performing
models on the CrisisFACTS 2022 benchmark.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06687">Proximal Causal Inference With Text Data. (arXiv:2401.06687v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jacob M. Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_R/0/1/0/all/0/1">Rohit Bhattacharya</a>, <a href="http://arxiv.org/find/cs/1/au:+Keith_K/0/1/0/all/0/1">Katherine A. Keith</a></p>
<p>Recent text-based causal methods attempt to mitigate confounding bias by
including unstructured text data as proxies of confounding variables that are
partially or imperfectly measured. These approaches assume analysts have
supervised labels of the confounders given text for a subset of instances, a
constraint that is not always feasible due to data privacy or cost. Here, we
address settings in which an important confounding variable is completely
unobserved. We propose a new causal inference method that splits pre-treatment
text data, infers two proxies from two zero-shot models on the separate splits,
and applies these proxies in the proximal g-formula. We prove that our
text-based proxy method satisfies identification conditions required by the
proximal g-formula while other seemingly reasonable proposals do not. We
evaluate our method in synthetic and semi-synthetic settings and find that it
produces estimates with low bias. This combination of proximal causal inference
and zero-shot classifiers is novel (to our knowledge) and expands the set of
text-specific causal methods available to practitioners.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06688">Don&#x27;t Rank, Combine! Combining Machine Translation Hypotheses Using Quality Estimation. (arXiv:2401.06688v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vernikos_G/0/1/0/all/0/1">Giorgos Vernikos</a>, <a href="http://arxiv.org/find/cs/1/au:+Popescu_Belis_A/0/1/0/all/0/1">Andrei Popescu-Belis</a></p>
<p>Neural machine translation systems estimate probabilities of target sentences
given source sentences, yet these estimates may not align with human
preferences. This work introduces QE-fusion, a method utilizing a quality
estimation metric (QE) that better correlates with human judgments to
synthesize improved translations. QE-fusion leverages a candidate pool sampled
from a model, combining spans from different candidates using QE metrics such
as CometKiwi. We compare QE-fusion against beam search and recent reranking
techniques, such as Minimum Bayes Risk decoding or QE-reranking. Our method
consistently improves translation quality in terms of COMET and BLEURT scores
when applied to large language models (LLMs) used for translation (PolyLM,
XGLM, Llama2, and Mistral) and to multilingual translation models (NLLB), over
five language pairs. Notably, QE-fusion exhibits larger improvements for LLMs
due to their ability to generate diverse outputs. We demonstrate that our
approach generates novel translations in over half of the cases and
consistently outperforms other methods across varying numbers of candidates
(5-200). Furthermore, we empirically establish that QE-fusion scales linearly
with the number of candidates in the pool. QE-fusion proves effective in
enhancing LLM-based translation without the need for costly retraining of LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06692">An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models. (arXiv:2401.06692v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bhatt_G/0/1/0/all/0/1">Gantavya Bhatt</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yifang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1">Arnav M. Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jifan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Truong_S/0/1/0/all/0/1">Sang T. Truong</a>, <a href="http://arxiv.org/find/cs/1/au:+Mussmann_S/0/1/0/all/0/1">Stephen Mussmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yinglun Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bilmes_J/0/1/0/all/0/1">Jeffrey Bilmes</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1">Simon S. Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Jamieson_K/0/1/0/all/0/1">Kevin Jamieson</a>, <a href="http://arxiv.org/find/cs/1/au:+Ash_J/0/1/0/all/0/1">Jordan T. Ash</a>, <a href="http://arxiv.org/find/cs/1/au:+Nowak_R/0/1/0/all/0/1">Robert D. Nowak</a></p>
<p>Supervised finetuning (SFT) on instruction datasets has played a crucial role
in achieving the remarkable zero-shot generalization capabilities observed in
modern large language models (LLMs). However, the annotation efforts required
to produce high quality responses for instructions are becoming prohibitively
expensive, especially as the number of tasks spanned by instruction datasets
continues to increase. Active learning is effective in identifying useful
subsets of samples to annotate from an unlabeled pool, but its high
computational cost remains a barrier to its widespread applicability in the
context of LLMs. To mitigate the annotation cost of SFT and circumvent the
computational bottlenecks of active learning, we propose using experimental
design. Experimental design techniques select the most informative samples to
label, and typically maximize some notion of uncertainty and/or diversity. In
our work, we implement a framework that evaluates several existing and novel
experimental design techniques and find that these methods consistently yield
significant gains in label efficiency with little computational overhead. On
generative tasks, our methods achieve the same generalization performance with
only $50\%$ of annotation cost required by random sampling.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06706">Multi-Candidate Speculative Decoding. (arXiv:2401.06706v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Sen Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Shujian Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1">Xinyu Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiajun Chen</a></p>
<p>Large language models have shown impressive capabilities across a variety of
NLP tasks, yet their generating text autoregressively is time-consuming. One
way to speed them up is speculative decoding, which generates candidate
segments (a sequence of tokens) from a fast draft model that is then verified
in parallel by the target model. However, the acceptance rate of candidate
tokens receives limitations from several factors, such as the model, the
dataset, and the decoding setup. This paper proposes sampling multiple
candidates from a draft model and then organising them in batches for
verification. We design algorithms for efficient multi-candidate verification
while maintaining the distribution of the target model. Our approach shows
significant improvements in acceptance rates on multiple datasets and models,
consistently outperforming standard speculative decoding.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06709">Reliability Analysis of Psychological Concept Extraction and Classification in User-penned Text. (arXiv:2401.06709v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Garg_M/0/1/0/all/0/1">Muskan Garg</a>, <a href="http://arxiv.org/find/cs/1/au:+Sathvik_M/0/1/0/all/0/1">MSVPJ Sathvik</a>, <a href="http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1">Amrit Chadha</a>, <a href="http://arxiv.org/find/cs/1/au:+Raza_S/0/1/0/all/0/1">Shaina Raza</a>, <a href="http://arxiv.org/find/cs/1/au:+Sohn_S/0/1/0/all/0/1">Sunghwan Sohn</a></p>
<p>The social NLP research community witness a recent surge in the computational
advancements of mental health analysis to build responsible AI models for a
complex interplay between language use and self-perception. Such responsible AI
models aid in quantifying the psychological concepts from user-penned texts on
social media. On thinking beyond the low-level (classification) task, we
advance the existing binary classification dataset, towards a higher-level task
of reliability analysis through the lens of explanations, posing it as one of
the safety measures. We annotate the LoST dataset to capture nuanced textual
cues that suggest the presence of low self-esteem in the posts of Reddit users.
We further state that the NLP models developed for determining the presence of
low self-esteem, focus more on three types of textual cues: (i) Trigger: words
that triggers mental disturbance, (ii) LoST indicators: text indicators
emphasizing low self-esteem, and (iii) Consequences: words describing the
consequences of mental disturbance. We implement existing classifiers to
examine the attention mechanism in pre-trained language models (PLMs) for a
domain-specific psychology-grounded task. Our findings suggest the need of
shifting the focus of PLMs from Trigger and Consequences to a more
comprehensive explanation, emphasizing LoST indicators while determining low
self-esteem in Reddit posts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06712">Few-Shot Detection of Machine-Generated Text using Style Representations. (arXiv:2401.06712v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Soto_R/0/1/0/all/0/1">Rafael Rivera Soto</a>, <a href="http://arxiv.org/find/cs/1/au:+Koch_K/0/1/0/all/0/1">Kailin Koch</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1">Aleem Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Barry Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Bishop_M/0/1/0/all/0/1">Marcus Bishop</a>, <a href="http://arxiv.org/find/cs/1/au:+Andrews_N/0/1/0/all/0/1">Nicholas Andrews</a></p>
<p>The advent of instruction-tuned language models that convincingly mimic human
writing poses a significant risk of abuse. For example, such models could be
used for plagiarism, disinformation, spam, or phishing. However, such abuse may
be counteracted with the ability to detect whether a piece of text was composed
by a language model rather than a human. Some previous approaches to this
problem have relied on supervised methods trained on corpora of confirmed human
and machine-written documents. Unfortunately, model under-specification poses
an unavoidable challenge for neural network-based detectors, making them
brittle in the face of data shifts, such as the release of further language
models producing still more fluent text than the models used to train the
detectors. Other previous approaches require access to the models that may have
generated a document in question at inference or detection time, which is often
impractical. In light of these challenges, we pursue a fundamentally different
approach not relying on samples from language models of concern at training
time. Instead, we propose to leverage representations of writing style
estimated from human-authored text. Indeed, we find that features effective at
distinguishing among human authors are also effective at distinguishing human
from machine authors, including state of the art large language models like
Llama 2, ChatGPT, and GPT-4. Furthermore, given a handful of examples composed
by each of several specific language models of interest, our approach affords
the ability to predict which model generated a given document.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06715">Reframing Tax Law Entailment as Analogical Reasoning. (arXiv:2401.06715v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1">Xinrui Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Ming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Weir_N/0/1/0/all/0/1">Nathaniel Weir</a>, <a href="http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1">Benjamin Van Durme</a>, <a href="http://arxiv.org/find/cs/1/au:+Holzenberger_N/0/1/0/all/0/1">Nils Holzenberger</a></p>
<p>Statutory reasoning refers to the application of legislative provisions to a
series of case facts described in natural language. We re-frame statutory
reasoning as an analogy task, where each instance of the analogy task involves
a combination of two instances of statutory reasoning. This increases the
dataset size by two orders of magnitude, and introduces an element of
interpretability. We show that this task is roughly as difficult to Natural
Language Processing models as the original task. Finally, we come back to
statutory reasoning, solving it with a combination of a retrieval mechanism and
analogy models, and showing some progress on prior comparable work.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06730">Relying on the Unreliable: The Impact of Language Models&#x27; Reluctance to Express Uncertainty. (arXiv:2401.06730v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1">Kaitlyn Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1">Jena D. Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1">Xiang Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1">Maarten Sap</a></p>
<p>As natural language becomes the default interface for human-AI interaction,
there is a critical need for LMs to appropriately communicate uncertainties in
downstream applications. In this work, we investigate how LMs incorporate
confidence about their responses via natural language and how downstream users
behave in response to LM-articulated uncertainties. We examine publicly
deployed models and find that LMs are unable to express uncertainties when
answering questions even when they produce incorrect responses. LMs can be
explicitly prompted to express confidences, but tend to be overconfident,
resulting in high error rates (on average 47%) among confident responses. We
test the risks of LM overconfidence by running human experiments and show that
users rely heavily on LM generations, whether or not they are marked by
certainty. Lastly, we investigate the preference-annotated datasets used in
RLHF alignment and find that humans have a bias against texts with uncertainty.
Our work highlights a new set of safety harms facing human-LM interactions and
proposes design recommendations and mitigating strategies moving forward.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06742">Using Natural Language Inference to Improve Persona Extraction from Dialogue in a New Domain. (arXiv:2401.06742v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+DeLucia_A/0/1/0/all/0/1">Alexandra DeLucia</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1">Mengjie Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Maeda_Y/0/1/0/all/0/1">Yoshinori Maeda</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoda_M/0/1/0/all/0/1">Makoto Yoda</a>, <a href="http://arxiv.org/find/cs/1/au:+Yamada_K/0/1/0/all/0/1">Keiichi Yamada</a>, <a href="http://arxiv.org/find/cs/1/au:+Wakaki_H/0/1/0/all/0/1">Hiromi Wakaki</a></p>
<p>While valuable datasets such as PersonaChat provide a foundation for training
persona-grounded dialogue agents, they lack diversity in conversational and
narrative settings, primarily existing in the "real" world. To develop dialogue
agents with unique personas, models are trained to converse given a specific
persona, but hand-crafting these persona can be time-consuming, thus methods
exist to automatically extract persona information from existing
character-specific dialogue. However, these persona-extraction models are also
trained on datasets derived from PersonaChat and struggle to provide
high-quality persona information from conversational settings that do not take
place in the real world, such as the fantasy-focused dataset, LIGHT. Creating
new data to train models on a specific setting is human-intensive, thus
prohibitively expensive. To address both these issues, we introduce a natural
language inference method for post-hoc adapting a trained persona extraction
model to a new setting. We draw inspiration from the literature of dialog
natural language inference (NLI), and devise NLI-reranking methods to extract
structured persona information from dialogue. Compared to existing persona
extraction models, our method returns higher-quality extracted persona and
requires less human annotation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06751">The Unreasonable Effectiveness of Easy Training Data for Hard Tasks. (arXiv:2401.06751v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hase_P/0/1/0/all/0/1">Peter Hase</a>, <a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1">Mohit Bansal</a>, <a href="http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1">Peter Clark</a>, <a href="http://arxiv.org/find/cs/1/au:+Wiegreffe_S/0/1/0/all/0/1">Sarah Wiegreffe</a></p>
<p>How can we train models to perform well on hard test data when hard training
data is by definition difficult to label correctly? This question has been
termed the scalable oversight problem and has drawn increasing attention as
language models have continually improved. In this paper, we present the
surprising conclusion that current language models often generalize relatively
well from easy to hard data, even performing as well as "oracle" models trained
on hard data. We demonstrate this kind of easy-to-hard generalization using
simple training methods like in-context learning, linear classifier heads, and
QLoRA for seven different measures of datapoint hardness, including six
empirically diverse human hardness measures (like grade level) and one
model-based measure (loss-based). Furthermore, we show that even if one cares
most about model performance on hard data, it can be better to collect and
train on easy data rather than hard data, since hard data is generally noisier
and costlier to collect. Our experiments use open models up to 70b in size and
four publicly available question-answering datasets with questions ranging in
difficulty from 3rd grade science questions to college level STEM questions and
general-knowledge trivia. We conclude that easy-to-hard generalization in LMs
is surprisingly strong for the tasks studied, suggesting the scalable oversight
problem may be easier than previously thought. Our code is available at
https://github.com/allenai/easy-to-hard-generalization
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06752">Stylometry Analysis of Multi-authored Documents for Authorship and Author Style Change Detection. (arXiv:2401.06752v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zamir_M/0/1/0/all/0/1">Muhammad Tayyab Zamir</a>, <a href="http://arxiv.org/find/cs/1/au:+Ayub_M/0/1/0/all/0/1">Muhammad Asif Ayub</a>, <a href="http://arxiv.org/find/cs/1/au:+Gul_A/0/1/0/all/0/1">Asma Gul</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmad_N/0/1/0/all/0/1">Nasir Ahmad</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmad_K/0/1/0/all/0/1">Kashif Ahmad</a></p>
<p>In recent years, the increasing use of Artificial Intelligence based text
generation tools has posed new challenges in document provenance,
authentication, and authorship detection. However, advancements in stylometry
have provided opportunities for automatic authorship and author change
detection in multi-authored documents using style analysis techniques. Style
analysis can serve as a primary step toward document provenance and
authentication through authorship detection. This paper investigates three key
tasks of style analysis: (i) classification of single and multi-authored
documents, (ii) single change detection, which involves identifying the point
where the author switches, and (iii) multiple author-switching detection in
multi-authored documents. We formulate all three tasks as classification
problems and propose a merit-based fusion framework that integrates several
state-of-the-art natural language processing (NLP) algorithms and weight
optimization techniques. We also explore the potential of special characters,
which are typically removed during pre-processing in NLP applications, on the
performance of the proposed methods for these tasks by conducting extensive
experiments on both cleaned and raw datasets. Experimental results demonstrate
significant improvements over existing solutions for all three tasks on a
benchmark dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06760">Navigating the Metrics Maze: Reconciling Score Magnitudes and Accuracies. (arXiv:2401.06760v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kocmi_T/0/1/0/all/0/1">Tom Kocmi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zouhar_V/0/1/0/all/0/1">Vil&#xe9;m Zouhar</a>, <a href="http://arxiv.org/find/cs/1/au:+Federmann_C/0/1/0/all/0/1">Christian Federmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Post_M/0/1/0/all/0/1">Matt Post</a></p>
<p>Ten years ago a single metric, BLEU, governed progress in machine translation
research. For better or worse, there is no such consensus today, and
consequently it is difficult for researchers to develop and retain the kinds of
heuristic intuitions about metric deltas that drove earlier research and
deployment decisions. This paper investigates the "dynamic range" of a number
of modern metrics in an effort to provide a collective understanding of the
meaning of differences in scores both within and among metrics; in other words,
we ask what point difference X in metric Y is required between two systems for
humans to notice? We conduct our evaluation on a new large dataset, ToShip23,
using it to discover deltas at which metrics achieve system-level differences
that are meaningful to humans, which we measure by pairwise system accuracy. We
additionally show that this method of establishing delta-accuracy is more
stable than the standard use of statistical p-values in regards to testset
size. Where data size permits, we also explore the effect of metric deltas and
accuracy across finer-grained features such as translation direction, domain,
and system closeness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06761">APAR: LLMs Can Do Auto-Parallel Auto-Regressive Decoding. (arXiv:2401.06761v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Mingdao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1">Aohan Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bowen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Peng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jie Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1">Yuxiao Dong</a></p>
<p>The massive adoption of large language models (LLMs) demands efficient
deployment strategies. However, the auto-regressive decoding process, which is
fundamental to how most LLMs generate text, poses challenges to achieve
efficient serving. In this work, we introduce a parallel auto-regressive
generation method. By instruct-tuning on general domain data that contains
hierarchical structures, we enable LLMs to independently plan their generation
process and perform auto-parallel auto-regressive (APAR) generation,
significantly reducing the number of generation steps. APAR alone can achieve
up to 2x speed-up, and when combined with speculative decoding, the speed-up
can reach up to 4x. In addition, APAR reduces the key-value cache consumption
and attention computation during generation. This leads to a throughput
increase of 20-70% and a latency reduce of 20-35% in high-throughput scenarios,
compared to state-of-the-art serving frameworks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06766">Mind Your Format: Towards Consistent Evaluation of In-Context Learning Improvements. (arXiv:2401.06766v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Voronov_A/0/1/0/all/0/1">Anton Voronov</a>, <a href="http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1">Lena Wolf</a>, <a href="http://arxiv.org/find/cs/1/au:+Ryabinin_M/0/1/0/all/0/1">Max Ryabinin</a></p>
<p>Large language models demonstrate a remarkable capability for learning to
solve new tasks from a few examples. The prompt template, or the way the input
examples are formatted to obtain the prompt, is an important yet often
overlooked aspect of in-context learning. In this work, we conduct a
comprehensive study of the template format's influence on the in-context
learning performance. We evaluate the impact of the prompt template across
models (from 770M to 70B parameters) and 4 standard classification datasets. We
show that a poor choice of the template can reduce the performance of the
strongest models and inference methods to a random guess level. More
importantly, the best templates do not transfer between different setups and
even between models of the same family. Our findings show that the currently
prevalent approach to evaluation, which ignores template selection, may give
misleading results due to different templates in different works. As a first
step towards mitigating this issue, we propose Template Ensembles that
aggregate model predictions across several templates. This simple test-time
augmentation boosts average performance while being robust to the choice of
random set of templates.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06769">Machine Translation Models are Zero-Shot Detectors of Translation Direction. (arXiv:2401.06769v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wastl_M/0/1/0/all/0/1">Michelle Wastl</a>, <a href="http://arxiv.org/find/cs/1/au:+Vamvas_J/0/1/0/all/0/1">Jannis Vamvas</a>, <a href="http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1">Rico Sennrich</a></p>
<p>Detecting the translation direction of parallel text has applications for
machine translation training and evaluation, but also has forensic applications
such as resolving plagiarism or forgery allegations. In this work, we explore
an unsupervised approach to translation direction detection based on the simple
hypothesis that
$p(\text{translation}|\text{original})&gt;p(\text{original}|\text{translation})$,
motivated by the well-known simplification effect in translationese or
machine-translationese. In experiments with massively multilingual machine
translation models across 20 translation directions, we confirm the
effectiveness of the approach for high-resource language pairs, achieving
document-level accuracies of 82-96% for NMT-produced translations, and 60-81%
for human translations, depending on the model used. Code and demo are
available at https://github.com/ZurichNLP/translation-direction-detection
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2106.06147">NAAQA: A Neural Architecture for Acoustic Question Answering. (arXiv:2106.06147v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abdelnour_J/0/1/0/all/0/1">Jerome Abdelnour</a>, <a href="http://arxiv.org/find/cs/1/au:+Rouat_J/0/1/0/all/0/1">Jean Rouat</a>, <a href="http://arxiv.org/find/cs/1/au:+Salvi_G/0/1/0/all/0/1">Giampiero Salvi</a></p>
<p>The goal of the Acoustic Question Answering (AQA) task is to answer a
free-form text question about the content of an acoustic scene. It was inspired
by the Visual Question Answering (VQA) task. In this paper, based on the
previously introduced CLEAR dataset, we propose a new benchmark for AQA, namely
CLEAR2, that emphasizes the specific challenges of acoustic inputs. These
include handling of variable duration scenes, and scenes built with elementary
sounds that differ between training and test set. We also introduce NAAQA, a
neural architecture that leverages specific properties of acoustic inputs. The
use of 1D convolutions in time and frequency to process 2D spectro-temporal
representations of acoustic content shows promising results and enables
reductions in model complexity. We show that time coordinate maps augment
temporal localization capabilities which enhance performance of the network by
~17 percentage points. On the other hand, frequency coordinate maps have little
influence on this task. NAAQA achieves 79.5% of accuracy on the AQA task with
~4 times fewer parameters than the previously explored VQA model. We evaluate
the perfomance of NAAQA on an independent data set reconstructed from DAQA. We
also test the addition of a MALiMo module in our model on both CLEAR2 and DAQA.
We provide a detailed analysis of the results for the different question types.
We release the code to produce CLEAR2 as well as NAAQA to foster research in
this newly emerging machine learning task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.03050">State-of-the-art generalisation research in NLP: A taxonomy and review. (arXiv:2210.03050v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hupkes_D/0/1/0/all/0/1">Dieuwke Hupkes</a>, <a href="http://arxiv.org/find/cs/1/au:+Giulianelli_M/0/1/0/all/0/1">Mario Giulianelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Dankers_V/0/1/0/all/0/1">Verna Dankers</a>, <a href="http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1">Mikel Artetxe</a>, <a href="http://arxiv.org/find/cs/1/au:+Elazar_Y/0/1/0/all/0/1">Yanai Elazar</a>, <a href="http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1">Tiago Pimentel</a>, <a href="http://arxiv.org/find/cs/1/au:+Christodoulopoulos_C/0/1/0/all/0/1">Christos Christodoulopoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Lasri_K/0/1/0/all/0/1">Karim Lasri</a>, <a href="http://arxiv.org/find/cs/1/au:+Saphra_N/0/1/0/all/0/1">Naomi Saphra</a>, <a href="http://arxiv.org/find/cs/1/au:+Sinclair_A/0/1/0/all/0/1">Arabella Sinclair</a>, <a href="http://arxiv.org/find/cs/1/au:+Ulmer_D/0/1/0/all/0/1">Dennis Ulmer</a>, <a href="http://arxiv.org/find/cs/1/au:+Schottmann_F/0/1/0/all/0/1">Florian Schottmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Batsuren_K/0/1/0/all/0/1">Khuyagbaatar Batsuren</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1">Kaiser Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Sinha_K/0/1/0/all/0/1">Koustuv Sinha</a>, <a href="http://arxiv.org/find/cs/1/au:+Khalatbari_L/0/1/0/all/0/1">Leila Khalatbari</a>, <a href="http://arxiv.org/find/cs/1/au:+Ryskina_M/0/1/0/all/0/1">Maria Ryskina</a>, <a href="http://arxiv.org/find/cs/1/au:+Frieske_R/0/1/0/all/0/1">Rita Frieske</a>, <a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1">Ryan Cotterell</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1">Zhijing Jin</a></p>
<p>The ability to generalise well is one of the primary desiderata of natural
language processing (NLP). Yet, what 'good generalisation' entails and how it
should be evaluated is not well understood, nor are there any evaluation
standards for generalisation. In this paper, we lay the groundwork to address
both of these issues. We present a taxonomy for characterising and
understanding generalisation research in NLP. Our taxonomy is based on an
extensive literature review of generalisation research, and contains five axes
along which studies can differ: their main motivation, the type of
generalisation they investigate, the type of data shift they consider, the
source of this data shift, and the locus of the shift within the modelling
pipeline. We use our taxonomy to classify over 400 papers that test
generalisation, for a total of more than 600 individual experiments.
Considering the results of this review, we present an in-depth analysis that
maps out the current state of generalisation research in NLP, and we make
recommendations for which areas might deserve attention in the future. Along
with this paper, we release a webpage where the results of our review can be
dynamically explored, and which we intend to update as new NLP generalisation
studies are published. With this work, we aim to take steps towards making
state-of-the-art generalisation testing the new status quo in NLP.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.07772">A Comprehensive Evaluation of Neural SPARQL Query Generation from Natural Language Questions. (arXiv:2304.07772v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Diallo_P/0/1/0/all/0/1">Papa Abdou Karim Karou Diallo</a>, <a href="http://arxiv.org/find/cs/1/au:+Reyd_S/0/1/0/all/0/1">Samuel Reyd</a>, <a href="http://arxiv.org/find/cs/1/au:+Zouaq_A/0/1/0/all/0/1">Amal Zouaq</a></p>
<p>In recent years, the field of neural machine translation (NMT) for SPARQL
query generation has witnessed significant growth. Incorporating the copy
mechanism with traditional encoder-decoder architectures and using pre-trained
encoder-decoders and large language models have set new performance benchmarks.
This paper presents various experiments that replicate and expand upon recent
NMT-based SPARQL generation studies, comparing pre-trained language models
(PLMs), non-pre-trained language models (NPLMs), and large language models
(LLMs), highlighting the impact of question annotation and the copy mechanism
and testing various fine-tuning methods using LLMs. In particular, we provide a
systematic error analysis of the models and test their generalization ability.
Our study demonstrates that the copy mechanism yields significant performance
enhancements for most PLMs and NPLMs. Annotating the data is pivotal to
generating correct URIs, with the "tag-within" strategy emerging as the most
effective approach. Additionally, our findings reveal that the primary source
of errors stems from incorrect URIs in SPARQL queries that are sometimes
replaced with hallucinated URIs when using base models. This does not happen
using the copy mechanism, but it sometimes leads to selecting wrong URIs among
candidates. Finally, the performance of the tested LLMs fell short of achieving
the desired outcomes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.01393">Assessing the Importance of Frequency versus Compositionality for Subword-based Tokenization in NMT. (arXiv:2306.01393v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wolleb_B/0/1/0/all/0/1">Benoist Wolleb</a>, <a href="http://arxiv.org/find/cs/1/au:+Silvestri_R/0/1/0/all/0/1">Romain Silvestri</a>, <a href="http://arxiv.org/find/cs/1/au:+Vernikos_G/0/1/0/all/0/1">Giorgos Vernikos</a>, <a href="http://arxiv.org/find/cs/1/au:+Dolamic_L/0/1/0/all/0/1">Ljiljana Dolamic</a>, <a href="http://arxiv.org/find/cs/1/au:+Popescu_Belis_A/0/1/0/all/0/1">Andrei Popescu-Belis</a></p>
<p>Subword tokenization is the de facto standard for tokenization in neural
language models and machine translation systems. Three advantages are
frequently cited in favor of subwords: shorter encoding of frequent tokens,
compositionality of subwords, and ability to deal with unknown words. As their
relative importance is not entirely clear yet, we propose a tokenization
approach that enables us to separate frequency (the first advantage) from
compositionality. The approach uses Huffman coding to tokenize words, by order
of frequency, using a fixed amount of symbols. Experiments with CS-DE, EN-FR
and EN-DE NMT show that frequency alone accounts for 90%-95% of the scores
reached by BPE, hence compositionality has less importance than previously
thought.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01163">Improving Language Plasticity via Pretraining with Active Forgetting. (arXiv:2307.01163v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yihong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Marchisio_K/0/1/0/all/0/1">Kelly Marchisio</a>, <a href="http://arxiv.org/find/cs/1/au:+Raileanu_R/0/1/0/all/0/1">Roberta Raileanu</a>, <a href="http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1">David Ifeoluwa Adelani</a>, <a href="http://arxiv.org/find/cs/1/au:+Stenetorp_P/0/1/0/all/0/1">Pontus Stenetorp</a>, <a href="http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1">Sebastian Riedel</a>, <a href="http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1">Mikel Artetxe</a></p>
<p>Pretrained language models (PLMs) are today the primary model for natural
language processing. Despite their impressive downstream performance, it can be
difficult to apply PLMs to new languages, a barrier to making their
capabilities universally accessible. While prior work has shown it possible to
address this issue by learning a new embedding layer for the new language,
doing so is both data and compute inefficient. We propose to use an active
forgetting mechanism during pretraining, as a simple way of creating PLMs that
can quickly adapt to new languages. Concretely, by resetting the embedding
layer every K updates during pretraining, we encourage the PLM to improve its
ability of learning new embeddings within a limited number of updates, similar
to a meta-learning effect. Experiments with RoBERTa show that models pretrained
with our forgetting mechanism not only demonstrate faster convergence during
language adaptation but also outperform standard ones in a low-data regime,
particularly for languages that are distant from English.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06857">Lightweight reranking for language model generations. (arXiv:2307.06857v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1">Siddhartha Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xiaofei Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Deoras_A/0/1/0/all/0/1">Anoop Deoras</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_B/0/1/0/all/0/1">Bing Xiang</a></p>
<p>Large Language Models (LLMs) can exhibit considerable variation in the
quality of their sampled outputs. Reranking and selecting the best generation
from the sampled set is a popular way of obtaining strong gains in generation
quality. In this paper, we present a novel approach for reranking LLM
generations. Unlike other techniques that might involve additional inferences
or training a specialized reranker, our approach relies on easy to compute
pairwise statistics between the generations that have minimal compute overhead.
We show that our approach can be formalized as an extension of self-consistency
and analyze its performance in that framework, theoretically as well as via
simulations. We show strong improvements for selecting the best k generations
for code generation tasks as well as robust improvements for the best
generation for the tasks of autoformalization, summarization, and translation.
While our approach only assumes black-box access to LLMs, we show that
additional access to token probabilities can improve performance even further.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.09311">Lip Reading for Low-resource Languages by Learning and Combining General Speech Knowledge and Language-specific Knowledge. (arXiv:2308.09311v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1">Minsu Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Yeo_J/0/1/0/all/0/1">Jeong Hun Yeo</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1">Jeongsoo Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ro_Y/0/1/0/all/0/1">Yong Man Ro</a></p>
<p>This paper proposes a novel lip reading framework, especially for
low-resource languages, which has not been well addressed in the previous
literature. Since low-resource languages do not have enough video-text paired
data to train the model to have sufficient power to model lip movements and
language, it is regarded as challenging to develop lip reading models for
low-resource languages. In order to mitigate the challenge, we try to learn
general speech knowledge, the ability to model lip movements, from a
high-resource language through the prediction of speech units. It is known that
different languages partially share common phonemes, thus general speech
knowledge learned from one language can be extended to other languages. Then,
we try to learn language-specific knowledge, the ability to model language, by
proposing Language-specific Memory-augmented Decoder (LMDecoder). LMDecoder
saves language-specific audio features into memory banks and can be trained on
audio-text paired data which is more easily accessible than video-text paired
data. Therefore, with LMDecoder, we can transform the input speech units into
language-specific audio features and translate them into texts by utilizing the
learned rich language knowledge. Finally, by combining general speech knowledge
and language-specific knowledge, we can efficiently develop lip reading models
even for low-resource languages. Through extensive experiments using five
languages, English, Spanish, French, Italian, and Portuguese, the effectiveness
of the proposed method is evaluated.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12604">PromptMRG: Diagnosis-Driven Prompts for Medical Report Generation. (arXiv:2308.12604v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1">Haibo Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Che_H/0/1/0/all/0/1">Haoxuan Che</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yi Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hao Chen</a></p>
<p>Automatic medical report generation (MRG) is of great research value as it
has the potential to relieve radiologists from the heavy burden of report
writing. Despite recent advancements, accurate MRG remains challenging due to
the need for precise clinical understanding and disease identification.
Moreover, the imbalanced distribution of diseases makes the challenge even more
pronounced, as rare diseases are underrepresented in training data, making
their diagnostic performance unreliable. To address these challenges, we
propose diagnosis-driven prompts for medical report generation (PromptMRG), a
novel framework that aims to improve the diagnostic accuracy of MRG with the
guidance of diagnosis-aware prompts. Specifically, PromptMRG is based on
encoder-decoder architecture with an extra disease classification branch. When
generating reports, the diagnostic results from the classification branch are
converted into token prompts to explicitly guide the generation process. To
further improve the diagnostic accuracy, we design cross-modal feature
enhancement, which retrieves similar reports from the database to assist the
diagnosis of a query image by leveraging the knowledge from a pre-trained CLIP.
Moreover, the disease imbalanced issue is addressed by applying an adaptive
logit-adjusted loss to the classification branch based on the individual
learning status of each disease, which overcomes the barrier of text decoder's
inability to manipulate disease distributions. Experiments on two MRG
benchmarks show the effectiveness of the proposed method, where it obtains
state-of-the-art clinical efficacy performance on both datasets. The code is
available at https://github.com/jhb86253817/PromptMRG.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.04027">TIDE: Textual Identity Detection for Evaluating and Augmenting Classification and Language Models. (arXiv:2309.04027v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Klu_E/0/1/0/all/0/1">Emmanuel Klu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sethi_S/0/1/0/all/0/1">Sameer Sethi</a></p>
<p>Machine learning models can perpetuate unintended biases from unfair and
imbalanced datasets. Evaluating and debiasing these datasets and models is
especially hard in text datasets where sensitive attributes such as race,
gender, and sexual orientation may not be available. When these models are
deployed into society, they can lead to unfair outcomes for historically
underrepresented groups. In this paper, we present a dataset coupled with an
approach to improve text fairness in classifiers and language models. We create
a new, more comprehensive identity lexicon, TIDAL, which includes 15,123
identity terms and associated sense context across three demographic
categories. We leverage TIDAL to develop an identity annotation and
augmentation tool that can be used to improve the availability of identity
context and the effectiveness of ML fairness techniques. We evaluate our
approaches using human contributors, and additionally run experiments focused
on dataset and model debiasing. Results show our assistive annotation technique
improves the reliability and velocity of human-in-the-loop processes. Our
dataset and methods uncover more disparities during evaluation, and also
produce more fair models during remediation. These approaches provide a
practical path forward for scaling classifier and generative model fairness in
real-world settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.13018">Dynamic ASR Pathways: An Adaptive Masking Approach Towards Efficient Pruning of A Multilingual ASR Model. (arXiv:2309.13018v2 [eess.AS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Xie_J/0/1/0/all/0/1">Jiamin Xie</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_K/0/1/0/all/0/1">Ke Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Guo_J/0/1/0/all/0/1">Jinxi Guo</a>, <a href="http://arxiv.org/find/eess/1/au:+Tjandra_A/0/1/0/all/0/1">Andros Tjandra</a>, <a href="http://arxiv.org/find/eess/1/au:+Shangguan_Y/0/1/0/all/0/1">Yuan Shangguan</a>, <a href="http://arxiv.org/find/eess/1/au:+Sari_L/0/1/0/all/0/1">Leda Sari</a>, <a href="http://arxiv.org/find/eess/1/au:+Wu_C/0/1/0/all/0/1">Chunyang Wu</a>, <a href="http://arxiv.org/find/eess/1/au:+Jia_J/0/1/0/all/0/1">Junteng Jia</a>, <a href="http://arxiv.org/find/eess/1/au:+Mahadeokar_J/0/1/0/all/0/1">Jay Mahadeokar</a>, <a href="http://arxiv.org/find/eess/1/au:+Kalinli_O/0/1/0/all/0/1">Ozlem Kalinli</a></p>
<p>Neural network pruning offers an effective method for compressing a
multilingual automatic speech recognition (ASR) model with minimal performance
loss. However, it entails several rounds of pruning and re-training needed to
be run for each language. In this work, we propose the use of an adaptive
masking approach in two scenarios for pruning a multilingual ASR model
efficiently, each resulting in sparse monolingual models or a sparse
multilingual model (named as Dynamic ASR Pathways). Our approach dynamically
adapts the sub-network, avoiding premature decisions about a fixed sub-network
structure. We show that our approach outperforms existing pruning methods when
targeting sparse monolingual models. Further, we illustrate that Dynamic ASR
Pathways jointly discovers and trains better sub-networks (pathways) of a
single multilingual model by adapting from different sub-network
initializations, thereby reducing the need for language-specific pruning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.12489">MedAI Dialog Corpus (MEDIC): Zero-Shot Classification of Doctor and AI Responses in Health Consultations. (arXiv:2310.12489v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ojo_O/0/1/0/all/0/1">Olumide E. Ojo</a>, <a href="http://arxiv.org/find/cs/1/au:+Adebanji_O/0/1/0/all/0/1">Olaronke O. Adebanji</a>, <a href="http://arxiv.org/find/cs/1/au:+Gelbukh_A/0/1/0/all/0/1">Alexander Gelbukh</a>, <a href="http://arxiv.org/find/cs/1/au:+Calvo_H/0/1/0/all/0/1">Hiram Calvo</a>, <a href="http://arxiv.org/find/cs/1/au:+Feldman_A/0/1/0/all/0/1">Anna Feldman</a></p>
<p>Zero-shot classification enables text to be classified into classes not seen
during training. In this study, we examine the efficacy of zero-shot learning
models in classifying healthcare consultation responses from Doctors and AI
systems. The models evaluated include BART, BERT, XLM, XLM-R and DistilBERT.
The models were tested on three different datasets based on a binary and
multi-label analysis to identify the origins of text in health consultations
without any prior corpus training. According to our findings, the zero-shot
language models show a good understanding of language generally, but has
limitations when trying to classify doctor and AI responses to healthcare
consultations. This research provides a foundation for future research in the
field of medical text classification by informing the development of more
accurate methods of classifying text written by Doctors and AI systems in
health consultations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.14403">O3D: Offline Data-driven Discovery and Distillation for Sequential Decision-Making with Large Language Models. (arXiv:2310.14403v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1">Yuchen Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yanchao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1">Mengda Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Madhushani_U/0/1/0/all/0/1">Udari Madhushani</a>, <a href="http://arxiv.org/find/cs/1/au:+Vann_J/0/1/0/all/0/1">Jared Vann</a>, <a href="http://arxiv.org/find/cs/1/au:+Garg_D/0/1/0/all/0/1">Deepeka Garg</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganesh_S/0/1/0/all/0/1">Sumitra Ganesh</a></p>
<p>Recent advancements in large language models (LLMs) have exhibited promising
performance in solving sequential decision-making problems. By imitating
few-shot examples provided in the prompts (i.e., in-context learning), an LLM
agent can interact with an external environment and complete given tasks
without additional training. However, such few-shot examples are often
insufficient to generate high-quality solutions for complex and long-horizon
tasks, while the limited context length cannot consume larger-scale
demonstrations. To this end, we propose an offline learning framework that
utilizes offline data at scale (e.g, logs of human interactions) to facilitate
the in-context learning performance of LLM agents. We formally define
LLM-powered policies with both text-based approaches and code-based approaches.
We then introduce an Offline Data-driven Discovery and Distillation (O3D)
framework to improve LLM-powered policies without finetuning. O3D automatically
discovers reusable skills and distills generalizable knowledge across multiple
tasks based on offline interaction data, advancing the capability of solving
downstream tasks. Empirical results under two interactive decision-making
benchmarks (ALFWorld and WebShop) demonstrate that O3D can notably enhance the
decision-making capabilities of LLMs through the offline discovery and
distillation process, and consistently outperform baselines across various LLMs
with both text-based-policy and code-based-policy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.18152">Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs. (arXiv:2310.18152v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1">Yijian Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Ziwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1">Wenwu Zhu</a></p>
<p>Text-attributed graphs (TAGs) are prevalent on the web and research over TAGs
such as citation networks, e-commerce networks and social networks has
attracted considerable attention in the web community. Recently, large language
models (LLMs) have demonstrated exceptional capabilities across a wide range of
tasks. However, the existing works focus on harnessing the potential of LLMs
solely relying on prompts to convey graph structure information to LLMs, thus
suffering from insufficient understanding of the complex structural
relationships within TAGs. To address this problem, in this paper we present
the Disentangled Graph-Text Learner (DGTL) model, which is able to enhance the
reasoning and predicting capabilities of LLMs for TAGs. Our proposed DGTL model
incorporates graph structure information through tailored disentangled graph
neural network (GNN) layers, enabling LLMs to capture the intricate
relationships hidden in text-attributed graphs from multiple structural
factors. Furthermore, DGTL operates with frozen pre-trained LLMs, reducing
computational costs and allowing much more flexibility in combining with
different LLM models. Experimental evaluations demonstrate the effectiveness of
the proposed DGTL model on achieving superior or comparable performance over
state-of-the-art baselines. Additionally, we also demonstrate that our DGTL
model can offer natural language explanations for predictions, thereby
significantly enhancing model interpretability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08640">Multistage Collaborative Knowledge Distillation from Large Language Models for Semi-Supervised Sequence Generation. (arXiv:2311.08640v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jiachen Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Wenlong Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Drozdov_A/0/1/0/all/0/1">Andrew Drozdov</a>, <a href="http://arxiv.org/find/cs/1/au:+Rozonoyer_B/0/1/0/all/0/1">Benjamin Rozonoyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Sultan_M/0/1/0/all/0/1">Md Arafat Sultan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jay-Yoon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1">Mohit Iyyer</a>, <a href="http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1">Andrew McCallum</a></p>
<p>We study semi-supervised sequence generation tasks where labeled data are too
scarce to effectively finetune a model and at the same time few-shot prompting
of a large language model (LLM) has suboptimal performance. This happens when a
task, such as parsing, is expensive to annotate and also unfamiliar to a
pretrained LLM. In this paper, we present a discovery that student models
distilled from an in-context learned LLM can often generalize better than their
teacher on such tasks. Leveraging this finding, we present a new method --
multistage collaborative knowledge distillation from an LLM (MCKD) -- for such
tasks. MCKD first few-shot prompts an LLM to produce pseudolabels for unlabeled
data. At each intermediate knowledge distillation (KD) stage, a new pair of
students is trained on disjoint partitions of the pseudolabeled data. Each
student then produces new and improved pseudolabels for its unseen partition to
be used in the next stage of distillation. We demonstrate the advantage of
multistage cross-partition labeling on several syntactic and semantic parsing
tasks. On CRAFT biomedical parsing, for example, 3-stage MCKD with 50 labeled
examples outperforms the prompted LLM and vanilla KD by 7.5% and 3.7% parsing
F1, respectively, and matches the performance of supervised finetuning with 500
examples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17492">Mergen: The First Manchu-Korean Machine Translation Model Trained on Augmented Data. (arXiv:2311.17492v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1">Jean Seo</a>, <a href="http://arxiv.org/find/cs/1/au:+Byun_S/0/1/0/all/0/1">Sungjoo Byun</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1">Minha Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Sangah Lee</a></p>
<p>The Manchu language, with its roots in the historical Manchurian region of
Northeast China, is now facing a critical threat of extinction, as there are
very few speakers left. In our efforts to safeguard the Manchu language, we
introduce Mergen, the first-ever attempt at a Manchu-Korean Machine Translation
(MT) model. To develop this model, we utilize valuable resources such as the
Manwen Laodang(a historical book) and a Manchu-Korean dictionary. Due to the
scarcity of a Manchu-Korean parallel dataset, we expand our data by employing
word replacement guided by GloVe embeddings, trained on both monolingual and
parallel texts. Our approach is built around an encoder-decoder neural machine
translation model, incorporating a bi-directional Gated Recurrent Unit (GRU)
layer. The experiments have yielded promising results, showcasing a significant
enhancement in Manchu-Korean translation, with a remarkable 20-30 point
increase in the BLEU score.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14890">NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes. (arXiv:2312.14890v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1">Lizhou Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1">Wenyue Hua</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lingyao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1">Haoyang Ling</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yongfeng Zhang</a></p>
<p>Complex reasoning ability is one of the most important features of current
LLMs, which has also been leveraged to play an integral role in complex
decision-making tasks. Therefore, the investigation into the reasoning
capabilities of Large Language Models (LLMs) is critical: numerous benchmarks
have been established to assess the reasoning abilities of LLMs. However,
current benchmarks are inadequate in offering a rigorous evaluation of the full
extent of reasoning abilities that LLMs are capable of achieving. They are also
prone to the risk of overfitting, as these benchmarks, being publicly
accessible and static, allow models to potentially tailor their responses to
specific benchmark metrics, thereby inflating their performance. Addressing
these limitations, our research introduces a new benchmark, named NPHardEval.
This benchmark is designed to evaluate the reasoning abilities of LLMs across a
broad spectrum of 900 algorithmic questions, extending up to the NP-Hard
complexity class. These questions are meticulously chosen to represent a wide
range of complexity class below the NP-hard complexity class, offering a
rigorous measure of the reasoning ability of LLMs. Through this study, we shed
light on the current state of reasoning in LLMs, providing an objective and
rigorous perspective through the comparison of LLMs' performance across complex
classes. Moreover, this benchmark is designed with a dynamic update mechanism,
where the datapoints are refreshed on a monthly basis. Such regular updates
play a crucial role in mitigating the risk of LLMs overfitting to the
benchmark, promoting a more accurate and reliable assessment of their reasoning
capabilities. The benchmark dataset and code of NPHardEval are available at
https://github.com/casmlab/NPHardEval.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.17279">Stateful Conformer with Cache-based Inference for Streaming Automatic Speech Recognition. (arXiv:2312.17279v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Noroozi_V/0/1/0/all/0/1">Vahid Noroozi</a>, <a href="http://arxiv.org/find/cs/1/au:+Majumdar_S/0/1/0/all/0/1">Somshubra Majumdar</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1">Ankur Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Balam_J/0/1/0/all/0/1">Jagadeesh Balam</a>, <a href="http://arxiv.org/find/cs/1/au:+Ginsburg_B/0/1/0/all/0/1">Boris Ginsburg</a></p>
<p>In this paper, we propose an efficient and accurate streaming speech
recognition model based on the FastConformer architecture. We adapted the
FastConformer architecture for streaming applications through: (1) constraining
both the look-ahead and past contexts in the encoder, and (2) introducing an
activation caching mechanism to enable the non-autoregressive encoder to
operate autoregressively during inference. The proposed model is thoughtfully
designed in a way to eliminate the accuracy disparity between the train and
inference time which is common for many streaming models. Furthermore, our
proposed encoder works with various decoder configurations including
Connectionist Temporal Classification (CTC) and RNN-Transducer (RNNT) decoders.
Additionally, we introduced a hybrid CTC/RNNT architecture which utilizes a
shared encoder with both a CTC and RNNT decoder to boost the accuracy and save
computation. We evaluate the proposed model on LibriSpeech dataset and a
multi-domain large scale dataset and demonstrate that it can achieve better
accuracy with lower latency and inference time compared to a conventional
buffered streaming model baseline. We also showed that training a model with
multiple latencies can achieve better accuracy than single latency models while
it enables us to support multiple latencies with a single model. Our
experiments also showed the hybrid architecture would not only speedup the
convergence of the CTC decoder but also improves the accuracy of streaming
models compared to single decoder models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01055">LLaMA Beyond English: An Empirical Study on Language Capability Transfer. (arXiv:2401.01055v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jun Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhihao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1">Luhui Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1">Tao Gui</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xuanjing Huang</a></p>
<p>In recent times, substantial advancements have been witnessed in large
language models (LLMs), exemplified by ChatGPT, showcasing remarkable
proficiency across a range of complex tasks. However, many mainstream LLMs
(e.g. LLaMA) are pretrained on English-dominant corpus, which limits their
performance in other non-English languages. In this paper, we focus on how to
effectively transfer the capabilities of language generation and following
instructions to a non-English language. To answer this question, we conduct an
extensive empirical investigation based on LLaMA, accumulating over 1440 GPU
hours. We analyze the impact of key factors such as vocabulary extension,
further pretraining, and instruction tuning on transfer. To accurately assess
the model's level of knowledge, we employ four widely used standardized testing
benchmarks: C-Eval, MMLU, AGI-Eval, and GAOKAO-Bench. Furthermore, a
comprehensive evaluation of the model's response quality is conducted,
considering aspects such as accuracy, fluency, informativeness, logical
coherence, and harmlessness, based on LLM-Eval, a benchmarks consisting
instruction tasks from 17 diverse categories. Our evaluation results
demonstrate that comparable performance to state-of-the-art transfer models can
be achieved with less than 1% of the pretraining data, both in terms of
knowledge alignment and response quality. Furthermore, the experimental
outcomes across the thirteen low-resource languages also exhibit similar
trends. We anticipate that the conclusions revealed by the experiments will aid
the community in developing non-English LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01623">Can AI Be as Creative as Humans?. (arXiv:2401.01623v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haonan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1">James Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Mozer_M/0/1/0/all/0/1">Michael Mozer</a>, <a href="http://arxiv.org/find/cs/1/au:+Goyal_A/0/1/0/all/0/1">Anirudh Goyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Lamb_A/0/1/0/all/0/1">Alex Lamb</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Linjun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1">Weijie J Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1">Zhun Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1">Michael Qizhe Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Brown_H/0/1/0/all/0/1">Hannah Brown</a>, <a href="http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1">Kenji Kawaguchi</a></p>
<p>Creativity serves as a cornerstone for societal progress and innovation. With
the rise of advanced generative AI models capable of tasks once reserved for
human creativity, the study of AI's creative potential becomes imperative for
its responsible development and application. In this paper, we provide a
theoretical answer to the question of whether AI can be creative. We prove in
theory that AI can be as creative as humans under the condition that AI can fit
the existing data generated by human creators. Therefore, the debate on AI's
creativity is reduced into the question of its ability of fitting a massive
amount of data. To arrive at this conclusion, this paper first addresses the
complexities in defining creativity by introducing a new concept called
Relative Creativity. Instead of trying to define creativity universally, we
shift the focus to whether AI can match the creative abilities of a
hypothetical human. This perspective draws inspiration from the Turing Test,
expanding upon it to address the challenges and subjectivities inherent in
assessing creativity. This methodological shift leads to a statistically
quantifiable assessment of AI's creativity, which we term Statistical
Creativity. This concept allows for comparisons of AI's creative abilities with
those of specific human groups, and facilitates the theoretical findings of
AI's creative potential. Building on this foundation, we discuss the
application of statistical creativity in prompt-conditioned autoregressive
models, providing a practical means for evaluating creative abilities of
contemporary AI models, such as Large Language Models (LLMs). In addition to
defining and analyzing creativity, we introduce an actionable training
guideline, effectively bridging the gap between theoretical quantification of
creativity and practical model training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.04531">MERA: A Comprehensive LLM Evaluation in Russian. (arXiv:2401.04531v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fenogenova_A/0/1/0/all/0/1">Alena Fenogenova</a>, <a href="http://arxiv.org/find/cs/1/au:+Chervyakov_A/0/1/0/all/0/1">Artem Chervyakov</a>, <a href="http://arxiv.org/find/cs/1/au:+Martynov_N/0/1/0/all/0/1">Nikita Martynov</a>, <a href="http://arxiv.org/find/cs/1/au:+Kozlova_A/0/1/0/all/0/1">Anastasia Kozlova</a>, <a href="http://arxiv.org/find/cs/1/au:+Tikhonova_M/0/1/0/all/0/1">Maria Tikhonova</a>, <a href="http://arxiv.org/find/cs/1/au:+Akhmetgareeva_A/0/1/0/all/0/1">Albina Akhmetgareeva</a>, <a href="http://arxiv.org/find/cs/1/au:+Emelyanov_A/0/1/0/all/0/1">Anton Emelyanov</a>, <a href="http://arxiv.org/find/cs/1/au:+Shevelev_D/0/1/0/all/0/1">Denis Shevelev</a>, <a href="http://arxiv.org/find/cs/1/au:+Lebedev_P/0/1/0/all/0/1">Pavel Lebedev</a>, <a href="http://arxiv.org/find/cs/1/au:+Sinev_L/0/1/0/all/0/1">Leonid Sinev</a>, <a href="http://arxiv.org/find/cs/1/au:+Isaeva_U/0/1/0/all/0/1">Ulyana Isaeva</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolomeytseva_K/0/1/0/all/0/1">Katerina Kolomeytseva</a>, <a href="http://arxiv.org/find/cs/1/au:+Moskovskiy_D/0/1/0/all/0/1">Daniil Moskovskiy</a>, <a href="http://arxiv.org/find/cs/1/au:+Goncharova_E/0/1/0/all/0/1">Elizaveta Goncharova</a>, <a href="http://arxiv.org/find/cs/1/au:+Savushkin_N/0/1/0/all/0/1">Nikita Savushkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Mikhailova_P/0/1/0/all/0/1">Polina Mikhailova</a>, <a href="http://arxiv.org/find/cs/1/au:+Dimitrov_D/0/1/0/all/0/1">Denis Dimitrov</a>, <a href="http://arxiv.org/find/cs/1/au:+Panchenko_A/0/1/0/all/0/1">Alexander Panchenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Markov_S/0/1/0/all/0/1">Sergei Markov</a></p>
<p>Over the past few years, one of the most notable advancements in AI research
has been in foundation models (FMs), headlined by the rise of language models
(LMs). As the models' size increases, LMs demonstrate enhancements in
measurable aspects and the development of new qualitative features. However,
despite researchers' attention and the rapid growth in LM application, the
capabilities, limitations, and associated risks still need to be better
understood. To address these issues, we introduce an open Multimodal Evaluation
of Russian-language Architectures (MERA), a new instruction benchmark for
evaluating foundation models oriented towards the Russian language. The
benchmark encompasses 21 evaluation tasks for generative models in 11 skill
domains and is designed as a black-box test to ensure the exclusion of data
leakage. The paper introduces a methodology to evaluate FMs and LMs in zero-
and few-shot fixed instruction settings that can be extended to other
modalities. We propose an evaluation methodology, an open-source code base for
the MERA assessment, and a leaderboard with a submission system. We evaluate
open LMs as baselines and find that they are still far behind the human level.
We publicly release MERA to guide forthcoming research, anticipate
groundbreaking model features, standardize the evaluation procedure, and
address potential societal drawbacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.04679">RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation. (arXiv:2401.04679v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nikdan_M/0/1/0/all/0/1">Mahdi Nikdan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tabesh_S/0/1/0/all/0/1">Soroush Tabesh</a>, <a href="http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1">Dan Alistarh</a></p>
<p>We investigate parameter-efficient fine-tuning (PEFT) methods that can
provide good accuracy under limited computational and memory budgets in the
context of large language models (LLMs). We present a new PEFT method called
Robust Adaptation (RoSA) inspired by robust principal component analysis (PCA)
that jointly trains $\textit{low-rank}$ and $\textit{highly-sparse}$ components
on top of a set of fixed pretrained weights to efficiently approximate the
performance of a full-fine-tuning (FFT) solution. Across a series of
challenging generative tasks such as grade-school math and SQL query
generation, which require fine-tuning for good performance, we show that RoSA
outperforms both LoRA and pure sparse fine-tuning, at the same parameter
budget. We provide system support for RoSA to complement the training
algorithm, specifically in the form of sparse GPU kernels which enable memory-
and computationally-efficient training. Our code will be made available at
https://github.com/IST-DASLab/RoSA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05566">Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training. (arXiv:2401.05566v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hubinger_E/0/1/0/all/0/1">Evan Hubinger</a>, <a href="http://arxiv.org/find/cs/1/au:+Denison_C/0/1/0/all/0/1">Carson Denison</a>, <a href="http://arxiv.org/find/cs/1/au:+Mu_J/0/1/0/all/0/1">Jesse Mu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lambert_M/0/1/0/all/0/1">Mike Lambert</a>, <a href="http://arxiv.org/find/cs/1/au:+Tong_M/0/1/0/all/0/1">Meg Tong</a>, <a href="http://arxiv.org/find/cs/1/au:+MacDiarmid_M/0/1/0/all/0/1">Monte MacDiarmid</a>, <a href="http://arxiv.org/find/cs/1/au:+Lanham_T/0/1/0/all/0/1">Tamera Lanham</a>, <a href="http://arxiv.org/find/cs/1/au:+Ziegler_D/0/1/0/all/0/1">Daniel M. Ziegler</a>, <a href="http://arxiv.org/find/cs/1/au:+Maxwell_T/0/1/0/all/0/1">Tim Maxwell</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1">Newton Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Jermyn_A/0/1/0/all/0/1">Adam Jermyn</a>, <a href="http://arxiv.org/find/cs/1/au:+Askell_A/0/1/0/all/0/1">Amanda Askell</a>, <a href="http://arxiv.org/find/cs/1/au:+Radhakrishnan_A/0/1/0/all/0/1">Ansh Radhakrishnan</a>, <a href="http://arxiv.org/find/cs/1/au:+Anil_C/0/1/0/all/0/1">Cem Anil</a>, <a href="http://arxiv.org/find/cs/1/au:+Duvenaud_D/0/1/0/all/0/1">David Duvenaud</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganguli_D/0/1/0/all/0/1">Deep Ganguli</a>, <a href="http://arxiv.org/find/cs/1/au:+Barez_F/0/1/0/all/0/1">Fazl Barez</a>, <a href="http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1">Jack Clark</a>, <a href="http://arxiv.org/find/cs/1/au:+Ndousse_K/0/1/0/all/0/1">Kamal Ndousse</a>, <a href="http://arxiv.org/find/cs/1/au:+Sachan_K/0/1/0/all/0/1">Kshitij Sachan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sellitto_M/0/1/0/all/0/1">Michael Sellitto</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_M/0/1/0/all/0/1">Mrinank Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+DasSarma_N/0/1/0/all/0/1">Nova DasSarma</a>, <a href="http://arxiv.org/find/cs/1/au:+Grosse_R/0/1/0/all/0/1">Roger Grosse</a>, <a href="http://arxiv.org/find/cs/1/au:+Kravec_S/0/1/0/all/0/1">Shauna Kravec</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1">Yuntao Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Witten_Z/0/1/0/all/0/1">Zachary Witten</a>, <a href="http://arxiv.org/find/cs/1/au:+Favaro_M/0/1/0/all/0/1">Marina Favaro</a>, <a href="http://arxiv.org/find/cs/1/au:+Brauner_J/0/1/0/all/0/1">Jan Brauner</a>, <a href="http://arxiv.org/find/cs/1/au:+Karnofsky_H/0/1/0/all/0/1">Holden Karnofsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Christiano_P/0/1/0/all/0/1">Paul Christiano</a>, <a href="http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1">Samuel R. Bowman</a>, <a href="http://arxiv.org/find/cs/1/au:+Graham_L/0/1/0/all/0/1">Logan Graham</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaplan_J/0/1/0/all/0/1">Jared Kaplan</a>, <a href="http://arxiv.org/find/cs/1/au:+Mindermann_S/0/1/0/all/0/1">S&#xf6;ren Mindermann</a>, <a href="http://arxiv.org/find/cs/1/au:+Greenblatt_R/0/1/0/all/0/1">Ryan Greenblatt</a>, <a href="http://arxiv.org/find/cs/1/au:+Shlegeris_B/0/1/0/all/0/1">Buck Shlegeris</a>, <a href="http://arxiv.org/find/cs/1/au:+Schiefer_N/0/1/0/all/0/1">Nicholas Schiefer</a>, <a href="http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1">Ethan Perez</a></p>
<p>Humans are capable of strategically deceptive behavior: behaving helpfully in
most situations, but then behaving very differently in order to pursue
alternative objectives when given the opportunity. If an AI system learned such
a deceptive strategy, could we detect it and remove it using current
state-of-the-art safety training techniques? To study this question, we
construct proof-of-concept examples of deceptive behavior in large language
models (LLMs). For example, we train models that write secure code when the
prompt states that the year is 2023, but insert exploitable code when the
stated year is 2024. We find that such backdoor behavior can be made
persistent, so that it is not removed by standard safety training techniques,
including supervised fine-tuning, reinforcement learning, and adversarial
training (eliciting unsafe behavior and then training to remove it). The
backdoor behavior is most persistent in the largest models and in models
trained to produce chain-of-thought reasoning about deceiving the training
process, with the persistence remaining even when the chain-of-thought is
distilled away. Furthermore, rather than removing backdoors, we find that
adversarial training can teach models to better recognize their backdoor
triggers, effectively hiding the unsafe behavior. Our results suggest that,
once a model exhibits deceptive behavior, standard techniques could fail to
remove such deception and create a false impression of safety.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05883">Generative Deduplication For Socia Media Data Selection. (arXiv:2401.05883v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xianming Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jing Li</a></p>
<p>Social media data is plagued by the redundancy problem caused by its noisy
nature, leading to increased training time and model bias. To address this
issue, we propose a novel approach called generative deduplication. It aims to
remove duplicate text from noisy social media data and mitigate model bias. By
doing so, it can improve social media language understanding performance and
save training time. Extensive experiments demonstrate that the proposed
generative deduplication can effectively reduce training samples while
improving performance. This evidence suggests the effectiveness of generative
deduplication and its importance in social media language understanding.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05949">Universal Vulnerabilities in Large Language Models: In-context Learning Backdoor Attacks. (arXiv:2401.05949v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1">Shuai Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_M/0/1/0/all/0/1">Meihuizi Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Tuan_L/0/1/0/all/0/1">Luu Anh Tuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1">Jinming Wen</a></p>
<p>In-context learning, a paradigm bridging the gap between pre-training and
fine-tuning, has demonstrated high efficacy in several NLP tasks, especially in
few-shot settings. Unlike traditional fine-tuning methods, in-context learning
adapts pre-trained models to unseen tasks without updating any parameters.
Despite being widely applied, in-context learning is vulnerable to malicious
attacks. In this work, we raise security concerns regarding this paradigm. Our
studies demonstrate that an attacker can manipulate the behavior of large
language models by poisoning the demonstration context, without the need for
fine-tuning the model. Specifically, we have designed a new backdoor attack
method, named ICLAttack, to target large language models based on in-context
learning. Our method encompasses two types of attacks: poisoning demonstration
examples and poisoning prompts, which can make models behave in accordance with
predefined intentions. ICLAttack does not require additional fine-tuning to
implant a backdoor, thus preserving the model's generality. Furthermore, the
poisoned examples are correctly labeled, enhancing the natural stealth of our
attack method. Extensive experimental results across several language models,
ranging in size from 1.3B to 40B parameters, demonstrate the effectiveness of
our attack method, exemplified by a high average attack success rate of 95.0%
across the three datasets on OPT models. Our findings highlight the
vulnerabilities of language models, and we hope this work will raise awareness
of the possible security threats associated with in-context learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06071">LEGO:Language Enhanced Multi-modal Grounding Model. (arXiv:2401.06071v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhaowei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1">Qi Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1">Hang Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1">Yiqing Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_Q/0/1/0/all/0/1">Qi Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1">Ran Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1">Junting Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zefeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Vu_V/0/1/0/all/0/1">Van Tu Vu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zhida Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tao Wang</a></p>
<p>Multi-modal large language models have demonstrated impressive performance
across various tasks in different modalities. However, existing multi-modal
models primarily emphasize capturing global information within each modality
while neglecting the importance of perceiving local information across
modalities. Consequently, these models lack the ability to effectively
understand the fine-grained details of input data, limiting their performance
in tasks that require a more nuanced understanding. To address this limitation,
there is a compelling need to develop models that enable fine-grained
understanding across multiple modalities, thereby enhancing their applicability
to a wide range of tasks. In this paper, we propose LEGO, a language enhanced
multi-modal grounding model. Beyond capturing global information like other
multi-modal models, our proposed model excels at tasks demanding a detailed
understanding of local information within the input. It demonstrates precise
identification and localization of specific regions in images or moments in
videos. To achieve this objective, we design a diversified dataset construction
pipeline, resulting in a multi-modal, multi-granularity dataset for model
training. The code, dataset, and demo of our model can be found at https:
//github.com/lzw-lzw/LEGO.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06102">Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models. (arXiv:2401.06102v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ghandeharioun_A/0/1/0/all/0/1">Asma Ghandeharioun</a>, <a href="http://arxiv.org/find/cs/1/au:+Caciularu_A/0/1/0/all/0/1">Avi Caciularu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pearce_A/0/1/0/all/0/1">Adam Pearce</a>, <a href="http://arxiv.org/find/cs/1/au:+Dixon_L/0/1/0/all/0/1">Lucas Dixon</a>, <a href="http://arxiv.org/find/cs/1/au:+Geva_M/0/1/0/all/0/1">Mor Geva</a></p>
<p>Inspecting the information encoded in hidden representations of large
language models (LLMs) can explain models' behavior and verify their alignment
with human values. Given the capabilities of LLMs in generating
human-understandable text, we propose leveraging the model itself to explain
its internal representations in natural language. We introduce a framework
called Patchscopes and show how it can be used to answer a wide range of
questions about an LLM's computation. We show that prior interpretability
methods based on projecting representations into the vocabulary space and
intervening on the LLM computation can be viewed as instances of this
framework. Moreover, several of their shortcomings such as failure in
inspecting early layers or lack of expressivity can be mitigated by
Patchscopes. Beyond unifying prior inspection techniques, Patchscopes also
opens up new possibilities such as using a more capable model to explain the
representations of a smaller model, and unlocks new applications such as
self-correction in multi-hop reasoning.
</p>
</p>
</div>

    </div>
    </body>
    